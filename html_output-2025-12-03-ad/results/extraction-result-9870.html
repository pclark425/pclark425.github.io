<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9870 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9870</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9870</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-72d5bd1095f9719742212f81a2a3790583e95796</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/72d5bd1095f9719742212f81a2a3790583e95796" target="_blank">Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work uses controlled nearest neighbor sampling over citation graph embeddings for contrastive learning to learn continuous similarity, to sample hard-to-learn negatives and positives, and to avoid collisions between negative and positive samples by controlling the sampling margin between them.</p>
                <p><strong>Paper Abstract:</strong> Learning scientific document representations can be substantially improved through contrastive learning objectives, where the challenge lies in creating positive and negative training samples that encode the desired similarity semantics. Prior work relies on discrete citation relations to generate contrast samples. However, discrete citations enforce a hard cut-off to similarity. This is counter-intuitive to similarity-based learning and ignores that scientific papers can be very similar despite lacking a direct citation - a core problem of finding related research. Instead, we use controlled nearest neighbor sampling over citation graph embeddings for contrastive learning. This control allows us to learn continuous similarity, to sample hard-to-learn negatives and positives, and also to avoid collisions between negative and positive samples by controlling the sampling margin between them. The resulting method SciNCL outperforms the state-of-the-art on the SciDocs benchmark. Furthermore, we demonstrate that it can train (or tune) language models sample-efficiently and that it can be combined with recent training-efficient methods. Perhaps surprisingly, even training a general-domain language model this way outperforms baselines pretrained in-domain.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9870.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9870.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciDocs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciDocs benchmark (Scientific Document Representation Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task benchmark for evaluating document embeddings on scientific papers, including classification, user-activity prediction, citation prediction, and recommendation tasks; embeddings are used without downstream fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SPECTER: Document-level Representation Learning using Citation-informed Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>SciBERT (and other encoder baselines: BERT, BioBERT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>SciBERT: a domain-adapted Transformer-based masked language model pretrained on scientific text; other baselines include BERT (general-domain) and BioBERT (biomedical). Models are used to produce CLS token representations for documents.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific document understanding / NLP for scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluate frozen document embeddings on downstream tasks (classification, user activity prediction, citation prediction, recommendation) using distance-based retrieval and supervised metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific metrics: F1 for classification; MAP and nDCG for user-activity and citation prediction; P@1 and mDCG for recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SciDocs benchmark: contains MeSH and MAG labels for classification, co-view/co-read logs for user activity, direct and co-citation labels, and recommendation gold data.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as the primary evaluation: SciNCL achieved average score 81.8 (w/ replicated SPECTER data, with leakage) vs SPECTER 80.0; without leakage SciNCL avg 81.1 vs SPECTER 79.4. Task-level: SciNCL outperformed baselines on 9/12 metrics in w/ leakage setup; Oracle SciDocs (upper bound with gold triplets) yields avg 83.0.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SciDocs measures performance of embeddings only (no task-specific fine-tuning); some leakage between SPECTER training data and SciDocs required careful dataset variants; task distribution and overfitting can affect scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Oracle SciDocs uses gold labels as upper bound to contrast self-supervised triplet selection; no human-expert evaluation of generated scientific theories is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use SciDocs' multi-task evaluation to measure general embedding quality across classification, retrieval, and recommendation tasks; treat Oracle SciDocs as an approximate upper bound.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9870.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9870.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TripletMarginLoss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triplet margin loss (Schroff et al., FaceNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive objective that trains embeddings by pulling query-positive pairs together and pushing query-negative pairs apart with a margin (here L2 distance and margin ξ=1).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Facenet: A unified embedding for face recognition and clustering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>SciBERT (triplet-finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>SciBERT encoder is fine-tuned with the triplet margin loss to produce document embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Representation learning for scientific documents (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Optimize triplet loss on constructed (query, positive, negative) triplets and evaluate resulting embeddings on SciDocs tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Downstream task metrics (F1, MAP, nDCG, P@1) of the embeddings produced by LLM after triplet training.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Training triplets derived from citation-based sampling over S2ORC citation graph; evaluated on SciDocs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Triplet loss with SciNCL sampling yields state-of-the-art SDR performance (avg 81.8) and outperforms alternatives like student-teacher or joint losses in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Triplet training effectiveness depends strongly on triplet mining (positives/negatives); collisions between positive and negative labels can degrade learning; margin and sample difficulty must be tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Triplet loss provides an automatic self-supervised alternative to manually labeled similarity, approximated by Oracle SciDocs for upper bound comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Careful triplet mining (hard positives/negatives with controlled margin) is crucial; using multiple positives per query (c+=5) as in SPECTER/SciNCL is effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9870.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9870.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciNCL KNN Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighborhood contrastive sampling with citation graph embeddings (SciNCL KNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sampling positive and negative examples for triplet training from continuous citation-graph embedding neighborhoods (KNN), enabling controlled sample difficulty and avoidance of positive-negative collisions via sample-induced margin (k+ and k_hard^- hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>SciBERT (SciNCL fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>SciBERT encoder fine-tuned with triplet loss whose triplets are generated by KNN over citation graph embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific document representation / citation-aware contrastive learning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate triplets by selecting positives from close neighbor ranges (k+ − c+; k+) and hard negatives from farther neighbor ranges (k_hard^- − c_hard^-; k_hard^-), train with triplet loss, and evaluate embeddings on SciDocs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Average downstream SciDocs metrics and task-specific metrics (F1, MAP, nDCG, P@1); also measure collision counts between positive/negative labels and validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Triplets drawn from S2ORC-derived citation graph embeddings; evaluated on SciDocs benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Best setting found: k+ = 25 for positives, k_hard^- = 4000 for hard negatives, filtered random for easy negatives. This sampling led to an avg. SciDocs score of 81.8 (w/ leakage), outperforming SPECTER by 1.8 points.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires training citation graph embeddings and KNN retrieval; hyperparameters (k+, k_hard^-) must be tuned; too small sampling-induced margin increases collisions and degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Replaces discrete citation selection (SPECTER) with continuous similarity-based sampling, reducing label collisions and producing harder, more informative negatives than random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use citation-graph KNN sampling to control positive/negative difficulty; tune k+ around ~25 and set k_hard^- in the thousands (2000–4000 range) while avoiding overlap region to prevent collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9870.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9870.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Collision / Sample-induced margin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collision measure and sample-induced margin (k+ vs k_hard^-)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantify and control instances where the same (query, candidate) pair appears both as a positive and a negative in training; sample-induced margin (the gap between positive and negative sampling ranges) controls collision rate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>SciBERT (SciNCL training)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>SciBERT fine-tuned with triplets whose collision behavior is measured and controlled through KNN hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Contrastive representation learning for scientific documents</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Count collisions (pairs labeled positive and negative across triplets) as a function of k+ and k_hard^-, analyze relation to validation/test performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Number of collisions per training set and effect on downstream SciDocs validation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>S2ORC-derived triplets and SciDocs validation/test</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Empirical relation: collisions increase as the sample-induced margin shrinks; collisions go to zero when k_hard^- > 1000; performance degrades when k+ is large or k_hard^- is small because of collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Measuring collisions requires tracking pair occurrences across triplets; avoiding collisions reduces available hard negatives if margin is too conservative.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Shows that strict discrete citation-based positive/negative selection (SPECTER) can induce collisions whereas continuous KNN sampling with margin reduces them.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Maintain a sufficiently large sample-induced margin (e.g., choose k_hard^- above ~1000 and tune k+) to avoid collisions and noisy contrastive labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9870.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9870.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Metrics (embedding tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Downstream evaluation metrics: F1, MAP, nDCG, P@1, mDCG</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard retrieval and classification metrics used to evaluate quality of document embeddings on classification, user-activity, citation prediction, and recommendation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>SciBERT (embeddings evaluated across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Embeddings produced by SciBERT (and other models) evaluated using these metrics across SciDocs tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Information retrieval and classification in scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute F1 for classification tasks; compute Mean Average Precision (MAP) and normalized Discounted Cumulative Gain (nDCG) for retrieval-style tasks; compute Precision@1 and mean DCG for recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>F1 measures classification quality; MAP and nDCG measure ranking/retrieval quality; P@1 and mDCG measure top-recommendation effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SciDocs tasks (MAG/MeSH classification, co-view/co-read, citation/co-citation prediction, recommendations).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SciNCL: improved F1 on MeSH (88.7 vs SPECTER 86.4) and competitive MAG F1 (81.4 vs SPECTER 82.0); gains most pronounced in user-activity and citation tasks (MAP/nDCG).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some tasks exhibited overfitting depending on training regime; metric choice reflects different aspects of embedding utility and should be considered jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Metrics are automated proxies for relevance and classification quality; Oracle SciDocs provides an upper-bound using gold labels.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Evaluate embeddings across multiple complementary metrics and tasks to capture generalization; report averages over multiple random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9870.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9870.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph embedding link-prediction metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph embedding evaluation metrics: MRR, Hits@1, Hits@10, AUC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metrics used to evaluate citation graph embeddings on link-prediction: Mean Reciprocal Rank (MRR), Hits at k, and Area Under ROC Curve (AUC).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>n/a (graph embedding evaluation for citation model)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Graph embeddings (BigGraph) trained on S2ORC citation graph; these embeddings are used to mine triplets for LLM fine-tuning but are not themselves LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Graph representation learning for citation networks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Train BigGraph embedding variants and evaluate on held-out 1% link prediction test set using MRR, Hits@1/10 and AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Higher MRR and Hits indicate better link prediction; AUC measures binary edge prediction performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>S2ORC citation graph (trained on 99% leave-1% for link prediction evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Dot-product distance with larger dimensions performed best (e.g., dim=768: MRR 95.12, Hits@1 93.22, Hits@10 97.77, AUC 98.74), guiding choice of embedding settings for SciNCL sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Graph embedding training requires compute and disk; link-prediction performance does not directly guarantee downstream SciDocs performance though correlated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides automated quantitative assessment of graph embeddings; no human evaluation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer dot-product objective and larger embedding dimensionality when compute/resources permit; validate link-prediction as proxy for neighborhood quality used in triplet mining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9870.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9870.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Student-Teacher Similarity Loss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Student-teacher loss via MSE between text and graph similarity matrices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attempted alternative transfer method where the language model (student) is trained to match pairwise cosine similarities of citation graph embeddings (teacher) using an MSE loss over similarity matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>SciBERT (student)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>SciBERT trained to match pairwise similarity structure of citation graph embeddings via MSE loss instead of (or in addition to) triplet loss.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-modal transfer from graph embeddings to text encoders</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute cosine similarity matrices S_Text and S_Graph for batch, minimize MSE(S_Text, S_Graph); evaluate resulting embeddings on SciDocs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>SciDocs downstream metrics (avg score, task metrics) and overfitting behavior of training/validation loss.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Citation graph (teacher) and SciDocs evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Student-teacher approach performed poorly: MSE-only student-teacher yielded avg SciDocs score 64.7 (close to SciBERT baseline), joint loss (triplet + student-teacher) achieved 80.5, worse than triplet-only SciNCL (81.8). Overfitting to citation similarity observed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Approach suffered rapid training loss collapse (overfitting) and poor validation performance; computation of full similarity matrices is expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Shows that direct mimicry of graph similarity is less effective than carefully mined triplet supervision for this task in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer contrastive triplet mining over naive student-teacher MSE transfer; if used, combine cautiously and monitor overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9870.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9870.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BitFit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BitFit: training only bias terms (parameter-efficient fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient fine-tuning method that only updates bias parameters of the Transformer model while freezing the rest; drastically reduces tunable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>SciBERT (BitFit fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>SciBERT fine-tuned with BitFit (only biases updated), resulting in training ~0.1% of parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Efficient fine-tuning for NLP models on scientific data</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Train using triplet loss while updating only bias terms; evaluate embeddings on SciDocs tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same SciDocs metrics: average across tasks and per-task metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SciDocs benchmark for evaluation; training uses SciNCL triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BitFit achieved avg SciDocs score 81.2 (−0.5 vs full SciNCL 81.8), demonstrating strong performance with minimal parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May be limited for some tasks (recommendation score slightly lower), but enables training larger models on same hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides a compute-efficient alternative to full fine-tuning while retaining most performance gains from SciNCL sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Consider BitFit for resource-limited environments when combined with strong triplet mining like SciNCL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPECTER: Document-level Representation Learning using Citation-informed Transformers <em>(Rating: 2)</em></li>
                <li>Facenet: A unified embedding for face recognition and clustering <em>(Rating: 2)</em></li>
                <li>A metric learning reality check <em>(Rating: 2)</em></li>
                <li>What makes for good views for contrastive learning? <em>(Rating: 2)</em></li>
                <li>A theoretical analysis of contrastive unsupervised representation learning <em>(Rating: 2)</em></li>
                <li>Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval <em>(Rating: 1)</em></li>
                <li>PyTorch-BigGraph: A Largescale Graph Embedding System <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9870",
    "paper_id": "paper-72d5bd1095f9719742212f81a2a3790583e95796",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "SciDocs",
            "name_full": "SciDocs benchmark (Scientific Document Representation Benchmark)",
            "brief_description": "A multi-task benchmark for evaluating document embeddings on scientific papers, including classification, user-activity prediction, citation prediction, and recommendation tasks; embeddings are used without downstream fine-tuning.",
            "citation_title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers",
            "mention_or_use": "use",
            "llm_name": "SciBERT (and other encoder baselines: BERT, BioBERT, etc.)",
            "llm_description": "SciBERT: a domain-adapted Transformer-based masked language model pretrained on scientific text; other baselines include BERT (general-domain) and BioBERT (biomedical). Models are used to produce CLS token representations for documents.",
            "scientific_domain": "Scientific document understanding / NLP for scientific literature",
            "evaluation_method": "Evaluate frozen document embeddings on downstream tasks (classification, user activity prediction, citation prediction, recommendation) using distance-based retrieval and supervised metrics.",
            "evaluation_criteria": "Task-specific metrics: F1 for classification; MAP and nDCG for user-activity and citation prediction; P@1 and mDCG for recommendation.",
            "benchmark_or_dataset": "SciDocs benchmark: contains MeSH and MAG labels for classification, co-view/co-read logs for user activity, direct and co-citation labels, and recommendation gold data.",
            "results_summary": "Used as the primary evaluation: SciNCL achieved average score 81.8 (w/ replicated SPECTER data, with leakage) vs SPECTER 80.0; without leakage SciNCL avg 81.1 vs SPECTER 79.4. Task-level: SciNCL outperformed baselines on 9/12 metrics in w/ leakage setup; Oracle SciDocs (upper bound with gold triplets) yields avg 83.0.",
            "limitations_or_challenges": "SciDocs measures performance of embeddings only (no task-specific fine-tuning); some leakage between SPECTER training data and SciDocs required careful dataset variants; task distribution and overfitting can affect scores.",
            "comparison_to_human_or_traditional": "Oracle SciDocs uses gold labels as upper bound to contrast self-supervised triplet selection; no human-expert evaluation of generated scientific theories is performed.",
            "recommendations_or_best_practices": "Use SciDocs' multi-task evaluation to measure general embedding quality across classification, retrieval, and recommendation tasks; treat Oracle SciDocs as an approximate upper bound.",
            "uuid": "e9870.0",
            "source_info": {
                "paper_title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "TripletMarginLoss",
            "name_full": "Triplet margin loss (Schroff et al., FaceNet)",
            "brief_description": "A contrastive objective that trains embeddings by pulling query-positive pairs together and pushing query-negative pairs apart with a margin (here L2 distance and margin ξ=1).",
            "citation_title": "Facenet: A unified embedding for face recognition and clustering",
            "mention_or_use": "use",
            "llm_name": "SciBERT (triplet-finetuned)",
            "llm_description": "SciBERT encoder is fine-tuned with the triplet margin loss to produce document embeddings.",
            "scientific_domain": "Representation learning for scientific documents (NLP)",
            "evaluation_method": "Optimize triplet loss on constructed (query, positive, negative) triplets and evaluate resulting embeddings on SciDocs tasks.",
            "evaluation_criteria": "Downstream task metrics (F1, MAP, nDCG, P@1) of the embeddings produced by LLM after triplet training.",
            "benchmark_or_dataset": "Training triplets derived from citation-based sampling over S2ORC citation graph; evaluated on SciDocs.",
            "results_summary": "Triplet loss with SciNCL sampling yields state-of-the-art SDR performance (avg 81.8) and outperforms alternatives like student-teacher or joint losses in this study.",
            "limitations_or_challenges": "Triplet training effectiveness depends strongly on triplet mining (positives/negatives); collisions between positive and negative labels can degrade learning; margin and sample difficulty must be tuned.",
            "comparison_to_human_or_traditional": "Triplet loss provides an automatic self-supervised alternative to manually labeled similarity, approximated by Oracle SciDocs for upper bound comparison.",
            "recommendations_or_best_practices": "Careful triplet mining (hard positives/negatives with controlled margin) is crucial; using multiple positives per query (c+=5) as in SPECTER/SciNCL is effective.",
            "uuid": "e9870.1",
            "source_info": {
                "paper_title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "SciNCL KNN Sampling",
            "name_full": "Neighborhood contrastive sampling with citation graph embeddings (SciNCL KNN)",
            "brief_description": "Sampling positive and negative examples for triplet training from continuous citation-graph embedding neighborhoods (KNN), enabling controlled sample difficulty and avoidance of positive-negative collisions via sample-induced margin (k+ and k_hard^- hyperparameters).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "SciBERT (SciNCL fine-tuning)",
            "llm_description": "SciBERT encoder fine-tuned with triplet loss whose triplets are generated by KNN over citation graph embeddings.",
            "scientific_domain": "Scientific document representation / citation-aware contrastive learning",
            "evaluation_method": "Generate triplets by selecting positives from close neighbor ranges (k+ − c+; k+) and hard negatives from farther neighbor ranges (k_hard^- − c_hard^-; k_hard^-), train with triplet loss, and evaluate embeddings on SciDocs.",
            "evaluation_criteria": "Average downstream SciDocs metrics and task-specific metrics (F1, MAP, nDCG, P@1); also measure collision counts between positive/negative labels and validation performance.",
            "benchmark_or_dataset": "Triplets drawn from S2ORC-derived citation graph embeddings; evaluated on SciDocs benchmark.",
            "results_summary": "Best setting found: k+ = 25 for positives, k_hard^- = 4000 for hard negatives, filtered random for easy negatives. This sampling led to an avg. SciDocs score of 81.8 (w/ leakage), outperforming SPECTER by 1.8 points.",
            "limitations_or_challenges": "Requires training citation graph embeddings and KNN retrieval; hyperparameters (k+, k_hard^-) must be tuned; too small sampling-induced margin increases collisions and degrades performance.",
            "comparison_to_human_or_traditional": "Replaces discrete citation selection (SPECTER) with continuous similarity-based sampling, reducing label collisions and producing harder, more informative negatives than random sampling.",
            "recommendations_or_best_practices": "Use citation-graph KNN sampling to control positive/negative difficulty; tune k+ around ~25 and set k_hard^- in the thousands (2000–4000 range) while avoiding overlap region to prevent collisions.",
            "uuid": "e9870.2",
            "source_info": {
                "paper_title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Collision / Sample-induced margin",
            "name_full": "Collision measure and sample-induced margin (k+ vs k_hard^-)",
            "brief_description": "Quantify and control instances where the same (query, candidate) pair appears both as a positive and a negative in training; sample-induced margin (the gap between positive and negative sampling ranges) controls collision rate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "SciBERT (SciNCL training)",
            "llm_description": "SciBERT fine-tuned with triplets whose collision behavior is measured and controlled through KNN hyperparameters.",
            "scientific_domain": "Contrastive representation learning for scientific documents",
            "evaluation_method": "Count collisions (pairs labeled positive and negative across triplets) as a function of k+ and k_hard^-, analyze relation to validation/test performance.",
            "evaluation_criteria": "Number of collisions per training set and effect on downstream SciDocs validation metrics.",
            "benchmark_or_dataset": "S2ORC-derived triplets and SciDocs validation/test",
            "results_summary": "Empirical relation: collisions increase as the sample-induced margin shrinks; collisions go to zero when k_hard^- &gt; 1000; performance degrades when k+ is large or k_hard^- is small because of collisions.",
            "limitations_or_challenges": "Measuring collisions requires tracking pair occurrences across triplets; avoiding collisions reduces available hard negatives if margin is too conservative.",
            "comparison_to_human_or_traditional": "Shows that strict discrete citation-based positive/negative selection (SPECTER) can induce collisions whereas continuous KNN sampling with margin reduces them.",
            "recommendations_or_best_practices": "Maintain a sufficiently large sample-induced margin (e.g., choose k_hard^- above ~1000 and tune k+) to avoid collisions and noisy contrastive labels.",
            "uuid": "e9870.3",
            "source_info": {
                "paper_title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Evaluation Metrics (embedding tasks)",
            "name_full": "Downstream evaluation metrics: F1, MAP, nDCG, P@1, mDCG",
            "brief_description": "Standard retrieval and classification metrics used to evaluate quality of document embeddings on classification, user-activity, citation prediction, and recommendation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "SciBERT (embeddings evaluated across tasks)",
            "llm_description": "Embeddings produced by SciBERT (and other models) evaluated using these metrics across SciDocs tasks.",
            "scientific_domain": "Information retrieval and classification in scientific literature",
            "evaluation_method": "Compute F1 for classification tasks; compute Mean Average Precision (MAP) and normalized Discounted Cumulative Gain (nDCG) for retrieval-style tasks; compute Precision@1 and mean DCG for recommendations.",
            "evaluation_criteria": "F1 measures classification quality; MAP and nDCG measure ranking/retrieval quality; P@1 and mDCG measure top-recommendation effectiveness.",
            "benchmark_or_dataset": "SciDocs tasks (MAG/MeSH classification, co-view/co-read, citation/co-citation prediction, recommendations).",
            "results_summary": "SciNCL: improved F1 on MeSH (88.7 vs SPECTER 86.4) and competitive MAG F1 (81.4 vs SPECTER 82.0); gains most pronounced in user-activity and citation tasks (MAP/nDCG).",
            "limitations_or_challenges": "Some tasks exhibited overfitting depending on training regime; metric choice reflects different aspects of embedding utility and should be considered jointly.",
            "comparison_to_human_or_traditional": "Metrics are automated proxies for relevance and classification quality; Oracle SciDocs provides an upper-bound using gold labels.",
            "recommendations_or_best_practices": "Evaluate embeddings across multiple complementary metrics and tasks to capture generalization; report averages over multiple random seeds.",
            "uuid": "e9870.4",
            "source_info": {
                "paper_title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Graph embedding link-prediction metrics",
            "name_full": "Graph embedding evaluation metrics: MRR, Hits@1, Hits@10, AUC",
            "brief_description": "Metrics used to evaluate citation graph embeddings on link-prediction: Mean Reciprocal Rank (MRR), Hits at k, and Area Under ROC Curve (AUC).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "n/a (graph embedding evaluation for citation model)",
            "llm_description": "Graph embeddings (BigGraph) trained on S2ORC citation graph; these embeddings are used to mine triplets for LLM fine-tuning but are not themselves LLMs.",
            "scientific_domain": "Graph representation learning for citation networks",
            "evaluation_method": "Train BigGraph embedding variants and evaluate on held-out 1% link prediction test set using MRR, Hits@1/10 and AUC.",
            "evaluation_criteria": "Higher MRR and Hits indicate better link prediction; AUC measures binary edge prediction performance.",
            "benchmark_or_dataset": "S2ORC citation graph (trained on 99% leave-1% for link prediction evaluation).",
            "results_summary": "Dot-product distance with larger dimensions performed best (e.g., dim=768: MRR 95.12, Hits@1 93.22, Hits@10 97.77, AUC 98.74), guiding choice of embedding settings for SciNCL sampling.",
            "limitations_or_challenges": "Graph embedding training requires compute and disk; link-prediction performance does not directly guarantee downstream SciDocs performance though correlated.",
            "comparison_to_human_or_traditional": "Provides automated quantitative assessment of graph embeddings; no human evaluation reported.",
            "recommendations_or_best_practices": "Prefer dot-product objective and larger embedding dimensionality when compute/resources permit; validate link-prediction as proxy for neighborhood quality used in triplet mining.",
            "uuid": "e9870.5",
            "source_info": {
                "paper_title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Student-Teacher Similarity Loss",
            "name_full": "Student-teacher loss via MSE between text and graph similarity matrices",
            "brief_description": "An attempted alternative transfer method where the language model (student) is trained to match pairwise cosine similarities of citation graph embeddings (teacher) using an MSE loss over similarity matrices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "SciBERT (student)",
            "llm_description": "SciBERT trained to match pairwise similarity structure of citation graph embeddings via MSE loss instead of (or in addition to) triplet loss.",
            "scientific_domain": "Cross-modal transfer from graph embeddings to text encoders",
            "evaluation_method": "Compute cosine similarity matrices S_Text and S_Graph for batch, minimize MSE(S_Text, S_Graph); evaluate resulting embeddings on SciDocs.",
            "evaluation_criteria": "SciDocs downstream metrics (avg score, task metrics) and overfitting behavior of training/validation loss.",
            "benchmark_or_dataset": "Citation graph (teacher) and SciDocs evaluation.",
            "results_summary": "Student-teacher approach performed poorly: MSE-only student-teacher yielded avg SciDocs score 64.7 (close to SciBERT baseline), joint loss (triplet + student-teacher) achieved 80.5, worse than triplet-only SciNCL (81.8). Overfitting to citation similarity observed.",
            "limitations_or_challenges": "Approach suffered rapid training loss collapse (overfitting) and poor validation performance; computation of full similarity matrices is expensive.",
            "comparison_to_human_or_traditional": "Shows that direct mimicry of graph similarity is less effective than carefully mined triplet supervision for this task in practice.",
            "recommendations_or_best_practices": "Prefer contrastive triplet mining over naive student-teacher MSE transfer; if used, combine cautiously and monitor overfitting.",
            "uuid": "e9870.6",
            "source_info": {
                "paper_title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "BitFit",
            "name_full": "BitFit: training only bias terms (parameter-efficient fine-tuning)",
            "brief_description": "A parameter-efficient fine-tuning method that only updates bias parameters of the Transformer model while freezing the rest; drastically reduces tunable parameters.",
            "citation_title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "mention_or_use": "use",
            "llm_name": "SciBERT (BitFit fine-tuning)",
            "llm_description": "SciBERT fine-tuned with BitFit (only biases updated), resulting in training ~0.1% of parameters.",
            "scientific_domain": "Efficient fine-tuning for NLP models on scientific data",
            "evaluation_method": "Train using triplet loss while updating only bias terms; evaluate embeddings on SciDocs tasks.",
            "evaluation_criteria": "Same SciDocs metrics: average across tasks and per-task metrics.",
            "benchmark_or_dataset": "SciDocs benchmark for evaluation; training uses SciNCL triplets.",
            "results_summary": "BitFit achieved avg SciDocs score 81.2 (−0.5 vs full SciNCL 81.8), demonstrating strong performance with minimal parameter updates.",
            "limitations_or_challenges": "May be limited for some tasks (recommendation score slightly lower), but enables training larger models on same hardware.",
            "comparison_to_human_or_traditional": "Provides a compute-efficient alternative to full fine-tuning while retaining most performance gains from SciNCL sampling.",
            "recommendations_or_best_practices": "Consider BitFit for resource-limited environments when combined with strong triplet mining like SciNCL.",
            "uuid": "e9870.7",
            "source_info": {
                "paper_title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers",
            "rating": 2
        },
        {
            "paper_title": "Facenet: A unified embedding for face recognition and clustering",
            "rating": 2
        },
        {
            "paper_title": "A metric learning reality check",
            "rating": 2
        },
        {
            "paper_title": "What makes for good views for contrastive learning?",
            "rating": 2
        },
        {
            "paper_title": "A theoretical analysis of contrastive unsupervised representation learning",
            "rating": 2
        },
        {
            "paper_title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
            "rating": 1
        },
        {
            "paper_title": "PyTorch-BigGraph: A Largescale Graph Embedding System",
            "rating": 1
        }
    ],
    "cost": 0.016401,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings</h1>
<p>Malte Ostendorff ${ }^{1,2}$, Nils Rethmeier ${ }^{1,3}$, Isabelle Augenstein ${ }^{3}$, Bela Gipp ${ }^{2}$, Georg Rehm ${ }^{1}$<br>${ }^{1}$ DFKI GmbH, ${ }^{2}$ University of Göttingen, ${ }^{3}$ University of Copenhagen<br>${ }^{1}$ first.lastname@dfki.de, ${ }^{2}$ lastname@uni-goettingen.de, 2 lastname@di.ku.dk</p>
<h4>Abstract</h4>
<p>Learning scientific document representations can be substantially improved through contrastive learning objectives, where the challenge lies in creating positive and negative training samples that encode the desired similarity semantics. Prior work relies on discrete citation relations to generate contrast samples. However, discrete citations enforce a hard cutoff to similarity. This is counter-intuitive to similarity-based learning and ignores that scientific papers can be very similar despite lacking a direct citation - a core problem of finding related research. Instead, we use controlled nearest neighbor sampling over citation graph embeddings for contrastive learning. This control allows us to learn continuous similarity, to sample hard-to-learn negatives and positives, and also to avoid collisions between negative and positive samples by controlling the sampling margin between them. The resulting method SciNCL outperforms the state-of-theart on the SciDocs benchmark. Furthermore, we demonstrate that it can train (or tune) language models sample-efficiently and that it can be combined with recent training-efficient methods. Perhaps surprisingly, even training a general-domain language model this way outperforms baselines pretrained in-domain.</p>
<h2>1 Introduction</h2>
<p>Large pretrained language models (LLMs) achieve state-of-the-art results through fine-tuning on many NLP tasks (Rogers et al., 2020). However, the sentence or document embeddings derived from LLMs are of lesser quality compared to simple baselines like GloVe (Reimers and Gurevych, 2019), as their embedding space suffers from being anisotropic, i.e. poorly defined in some areas (Li et al., 2020).</p>
<p>One approach that has recently gained attention is the combination of LLMs with contrastive finetuning to improve the semantic textual similarity between document representations (Wu et al., 2020; Gao et al., 2021). These contrastive methods learn
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Starting from a query paper $\star$ in a citation graph embedding space. Hard positives $\oplus$ are citation graph embeddings that are sampled from a similar (close) context of $\star$, but are not so close that their gradients collapse easily. Hard (to classify) negatives $\equiv$ (red band) are close to positives (green band) up to a sampling induced margin. Easy negatives $\equiv$ are very dissimilar (distant) from the query paper $\star$.
to distinguish between pairs of similar and dissimilar texts (positive and negative samples). As recent works show (Tian et al., 2020b; Rethmeier and Augenstein, 2022b,a; Shorten et al., 2021), the selection of these positive and negative samples is crucial for efficient contrastive learning.</p>
<p>This paper focusses on learning scientific document representations (SDRs). The core distinguishing feature of this domain is the presence of citation information that complement the textual information. The current state-of-the-art SPECTER by Cohan et al. (2020) uses citation information to generate positive and negative samples for contrastive fine-tuning of a SciBERT language model (Beltagy et al., 2019). SPECTER relies on 'citations by the query paper' as a discrete signal for similarity, i.e., positive samples are cited by the query while negative ones are not cited.</p>
<p>However, SPECTER's use of citations has its</p>
<p>pitfalls. Considering only one citation direction may cause positive and negative samples to collide since a paper pair could be treated as a positive and negative instance simultaneously. Also, relying on a single citation as a discrete similarity signal is subject to noise, e.g., citations may reflect politeness and policy rather than semantic similarity (Pasternack, 1969) or related papers lack a direct citation (Gipp and Beel, 2009). This discrete cutoff to similarity is counter-intuitive to (continuous) similarity-based learning.</p>
<p>Instead, the generation of non-colliding contrastive samples should be based on a continuous similarity function that allows us to find semantically similar papers, even without direct citations. With SciNCL, we address these issues by generating contrastive samples based on citation embeddings. The citation embeddings, which incorporate the full citation graph, provide a continuous, undirected, and less noisy similarity signal that allows the generations of arbitrary difficult-to-learn positive and negative samples.</p>
<h2>Contributions:</h2>
<ul>
<li>We propose neighborhood contrastive learning for scientific document representations with citation graph embeddings (SciNCL) based on contrastive learning theory insights.</li>
<li>We sample positive (similar) and negative (dissimilar) papers from the $k$ nearest neighbors in the citation graph embedding space, such that positives and negatives do not collide but are also hard to learn.</li>
<li>We compare against the state-of-the-art approach SPECTER (Cohan et al., 2020) and other strong methods on the SciDocs benchmark and find that SciNCL outperforms SPECTER on average and on 9 of 12 metrics.</li>
<li>Finally, we demonstrate that with SciNCL, using only $1 \%$ of the triplets for training, starting with a general-domain language model, or training only the bias terms of the model is sufficient to outperform the baselines.</li>
<li>Our code and models are publicly available. ${ }^{1}$</li>
</ul>
<h2>2 Related Work</h2>
<p>Contrastive Learning pulls representations of similar data points (positives) closer together, while representations of dissimilar documents (negatives) are pushed apart. A common contrastive objective</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>is the triplet loss (Schroff et al., 2015) that Cohan et al. (2020) used for scientific document representation learning, as we describe below. However, as Musgrave et al. (2020) and Rethmeier and Augenstein (2022b) point out, contrastive objectives work best when specific requirements are respected. (Req. 1) Views of the same data should introduce new information, i.e. the mutual information between views should be minimized (Tian et al., 2020b). We use citation graph embeddings to generate contrast label information that supplements text-based similarity. (Req. 2) For training time and sample efficiency, negative samples should be hard to classify, but should also not collide with positives (Saunshi et al., 2019). (Req. 3) Recent works like Musgrave et al. (2020) and Khosla et al. (2020) use multiple positives. However, positives need to be consistently close to each other (Wang and Isola, 2020), since positives and negatives may otherwise collide, e.g., Cohan et al. (2020) consider only 'citations by the query' as similarity signal and not 'citations to the query'. Such unidirectional similarity does not guarantee that a negative paper (not cited by the query) may cite the query paper and thus could cause collisions, the more we sample (Appendix F.10). Our method treats both citing and being cited as positives (Req. 2), while it also generates hard negatives and hard positives (Req. $2+3)$. Hard negatives are close to but do not overlap positives (red band in Fig. 1). Hard positives are close, but not trivially close to the query document (green band in Fig. 1). The sample induced margin (space between red and green band in Fig. 1) ensures that contrast samples do not collide.</p>
<p>Triplet Mining remains a challenge in NLP due to the discrete nature of language which makes data augmentation less trivial as compared to computer vision (Gao et al., 2021). Examples for augmentation strategies are translation, word deletion, or word reordering (Fang et al., 2020; Wu et al., 2020). Positives and negatives can be sampled based on the sentence position within a document (Giorgi et al., 2021). Gao et al. (2021) utilize supervised entailment datasets for the triplet generation. Language- and text-independent approaches are also applied. Kim et al. (2021) use intermediate BERT hidden state for positive sampling and Wu et al. (2021) add noise to representations to obtain negative samples. Xiong et al. (2020) present an approach similar to SciNCL where they sample hard negatives from the k nearest neighbors in the</p>
<p>embedding space derived from the previous model checkpoint. While Xiong et al. rely only on textual data, SciNCL integrates also citation information which are especially valuable in the scientific context as Cohan et al. (2020) have shown.</p>
<p>Scientific Document Representations based on Transformers (Vaswani et al., 2017) and pretrained on domain-specific text dominate today's scientific document processing. There are SciBERT (Beltagy et al., 2019), BioBERT (Lee et al., 2019) and SciGPT2 (Luu et al., 2021), to name a few. Recent works modify these domain LLMs to support cite-worthiness detection (Wright and Augenstein, 2021), document similarity (Ostendorff et al., 2020) or fact checking (Wadden et al., 2020).</p>
<p>Aside from text, citations are a valuable signal for the similarity of research papers. Paper (node) representations can be learned using the citation graph (Wu et al., 2019; Perozzi et al., 2014; Grover and Leskovec, 2016). Especially for recommendations of papers or citations, hybrid combinations of text and citation features are often employed (Han et al., 2018; Jeong et al., 2020; Brochier et al., 2019; Yang et al., 2015; Holm et al., 2022).</p>
<p>Closest to SciNCL are Citeomatic (Bhagavatula et al., 2018) and SPECTER (Cohan et al., 2020). While Citeomatic relies on bag-of-words for its textual features, SPECTER is based on SciBERT. Both leverage citations to learn a triplet-based document embedding model, whereby positive samples are papers cited in the query. Easy negatives are random papers not cited by the query. Hard negatives are citations of citations - papers referenced in positive citations of the query, but are not cited directly by it. Citeomatic also uses a second type of hard negatives, which are the nearest neighbors of a query that are not cited by it.</p>
<p>Unlike our approach, Citeomatic does not use the neighborhood of citation embeddings, but instead relies on the actual document embeddings from the previous epoch. Despite being related to SciNCL, the sampling approaches employed in Citeomatic and SPECTER do not account for the pitfalls of using discrete citations as signal for paper similarity. Our work addresses this issue.</p>
<p>Cross-Modal Transfer. SciNCL transfers knowledge across modalities, i.e., from citations into a language model. According to Cohan et al. (2020), SciNCL can be considered as a "citation-informed Transformer". This cross-modal
transfer learning is applied for various modalities (see Kaur et al. (2021) for an overview): text-toimage (Socher et al., 2013), RGB-to-depth image (Tian et al., 2020a), or graph-to-image (Wang et al., 2018). While the aforementioned methods incorporate cross-modal knowledge through joint loss functions or latent representations, SciNCL transfers knowledge through the contrastive sample selection, which we found superior to the direct transfer approach (Appendix F.9).</p>
<h2>3 Methodology</h2>
<p>Our goal is to learn citation-informed representations for scientific documents. To do so we sample three document representation vectors and learn their similarity. For a given query paper vector $\boldsymbol{d}^{Q}$, we sample a positive (similar) paper vector $\boldsymbol{d}^{+}$and a negative (dissimilar) paper vector $\boldsymbol{d}^{-}$. This produces a 'query, positive, negative' triplet $\left(\boldsymbol{d}^{Q}, \boldsymbol{d}^{+}, \boldsymbol{d}^{-}\right)$-represented by $(\star, \diamond, \infty)$ in Fig. 1. To learn paper similarity, we need to define three components: (§3.1) how to calculate document vectors $\boldsymbol{d}$ for the loss over triplets $\mathcal{L}$; (§3.2) how citations provide similarity between papers; and (§3.3) how negative and positive papers $\left(\boldsymbol{d}^{-}, \boldsymbol{d}^{+}\right)$are sampled as (dis-)similar documents from the neighborhood of a query paper $\boldsymbol{d}^{Q}$.</p>
<h3>3.1 Contrastive Learning Objective</h3>
<p>Given the textual content of a document $d$ (paper), the goal is to derive a dense vector representation $\boldsymbol{d}$ that best encodes the document information and can be used in downstream tasks. A Transformer language model $f$ (SciBERT; Beltagy et al. (2019)) encodes documents $d$ into vector representations $f(d)=\boldsymbol{d}$. The input to the language model is the title and abstract separated by the [SEP] token. ${ }^{2}$ The final layer hidden state of the [CLS] token is then used as a document representation $f(d)=\boldsymbol{d}$.</p>
<p>Training with a masked language modeling objectives alone has been shown to produce suboptimal document representations (Li et al., 2020; Gao et al., 2021). Thus, similar to the SDR state-of-the-art method SPECTER (Cohan et al., 2020), we continue training the SciBERT model (Beltagy et al., 2019) using a self-supervised triplet margin loss (Schroff et al., 2015):</p>
<p>$$
\mathcal{L}=\max \left{\left|\boldsymbol{d}^{Q}-\boldsymbol{d}^{+}\right|<em 2="2">{2}-\left|\boldsymbol{d}^{Q}-\boldsymbol{d}^{-}\right|</em>+\xi, 0\right}
$$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Here, $\xi$ is a slack term ( $\xi=1$ as in SPECTER) and $|\Delta \boldsymbol{d}|_{2}$ is the $L^{2}$ norm, used as a distance function. However, the SPECTER sampling method has significant drawbacks. We will describe these issues and our contrastive learning theory guided improvements in detail below in $\S 3.2$.</p>
<h3>3.2 Citation Neighborhood Sampling</h3>
<p>Compared to the textual content of a paper, citations provide an outside view on a paper and its relation to the scientific literature (Elkiss et al., 2008), which is why citations are traditionally used as a similarity measure in library science (Kessler, 1963; Small, 1973). However, using citations as a discrete similarity signal, as done in Cohan et al. (2020), has its pitfalls. Their method defines papers cited by the query as positives, while paper citing the query could be treated as negatives. This means that positive and negative learning information collides between citation directions, which Saunshi et al. (2019) have shown to deteriorate performance. Furthermore, a cited paper can have a low similarity with the citing paper given the many motivations a citation can have (Teufel et al., 2006). Likewise, a similar paper might not be cited.</p>
<p>To overcome these limitations, we learn citation embeddings first and then use the citation neighborhood around a given query paper $d^{Q}$ to construct similar (positive) and dissimilar (negative) samples for contrast by using the $k$ nearest neighbors. This builds on the intuition that nodes connected by edges should be close to each other in the embedding space (Perozzi et al., 2014). Using citation embeddings allows us to: (1) sample paper similarity on a continuous scale, which makes it possible to: (2) define hard to learn positives, as well as (3) hard or easy to learn negatives. Points (2-3) are important in making contrastive learning efficient as will describe below in $\S 3.3$.</p>
<h3>3.3 Positives and Negatives Sampling</h3>
<p>Positive samples: $d^{+}$should be semantically similar to the query paper $d^{Q}$, i.e. sampled close to the query embedding $\boldsymbol{d}^{Q}$. Additionally, as Wang and Isola (2020) find, positives should be sampled from comparable locations (distances from the query) in embedding space and be dissimilar enough from the query embedding, to avoid gradient collapse (zero gradients). Therefore, we sample $c^{+}$positive (similar) papers from a close neighborhood around query embedding $\boldsymbol{d}^{Q}\left(k^{+}-c^{+}, k^{+}\right]$, i.e. the green band in Fig. 1. When sampling with</p>
<p>KNN search, we use a small $k^{+}$to find positives and later analyze the impact of $k^{+}$in Fig. 2.</p>
<p>Negative samples: can be divided into easy and hard $\rightleftharpoons$ negative samples (light and dark red in Fig. 1). Sampling more hard negatives is known to improve contrastive learning (Bucher et al., 2016; Wu et al., 2017). However, we make sure to sample hard negatives (red band in Fig. 1) such that they are close to potential positives but do not collide with positives (green band), by using a tunable 'sampling induced margin'. We do so, since Saunshi et al. (2019) showed that sampling a larger number of hard negatives only improves performance if the negatives do not collide with positive samples, since collisions make the learning signal noisy. That is, in the margin between hard negatives and positives we expect positives and negatives to collide, thus we avoid sampling from this region. To generate a diverse self-supervised citation similarity signal for contrastive SDR learning, we also sample easy negatives that are farther from the query than hard negatives. For negatives, the $k^{-}$should be large when sampling via KNN to ensure samples are dissimilar from the query paper.</p>
<h3>3.4 Sampling Strategies</h3>
<p>As described in $\S 3.2$ and $\S 3.3$, our approach improves upon the method by Cohan et al. (2020). Therefore, we reuse their sampling parameters ( 5 triplets per query paper) and then further optimize our methods' hyperparameters. For example, to train the triplet loss, we generate the same amount of $\left(\boldsymbol{d}^{Q}, \boldsymbol{d}^{+}, \boldsymbol{d}^{-}\right)$triplets per query paper as SPECTER (Cohan et al., 2020). To be precise, this means we generate $c^{+}=5$ positives (as explained in $\S 3.3$ ). We also generate 5 negatives, three easy negatives $c_{\text {easy }}^{-}=3$ and two hard negatives $c_{\text {hard }}^{-}=2$, as described in $\S 3.3$.</p>
<p>Below, we describe three strategies (I-III) for sampling triplets. These either sample neighboring papers from citation embeddings (I), by random sampling (II), or using both strategies (III). For each strategy, let $c^{\prime}$ be the number of samples for either positives $c^{+}$, easy negatives $c_{\text {easy }}^{-}$, or hard negatives $c_{\text {hard }}^{-}$.</p>
<p>Citation Graph Embeddings: We train a graph embedding model $f_{c}$ on citations extracted from the Semantic Scholar Open Research Corpus (S2ORC; Lo et al., 2020) to get citation embeddings $C$. We utilize PyTorch BigGraph (Lerer et al., 2019),</p>
<p>which allows for training on large graphs with modest hardware requirements. The resulting graph embeddings perform well using the default training settings from Lerer et al. (2019), but given more computational resources, careful tuning may produce even better-performing embeddings. Nonetheless, we conducted a narrow parameter search based on link prediction - see Appendix D.
(I) K-nearest neighbors (KNN): Assuming a given citation embedding model $f_{c}$ and a search index (e.g., FAISS $\S 4.3$ ), we run $K N N\left(f_{c}\left(d^{Q}\right), C\right)$ and take $c^{\prime}$ samples from a range of the $\left(k-c^{\prime}, k\right]$ nearest neighbors around the query paper $d^{Q}$ with its neighbors $N=\left{n_{1}, n_{2}, n_{3}, \ldots\right}$, whereby neighbor $n_{i}$ is the $i$-th nearest neighbor in the citation embedding space. For instance, for $c^{\prime}=3$ and $k=10$ the corresponding samples would be the three neighbors descending from the tenth neighbor: $n_{8}, n_{9}$, and $n_{10}$. To reduce computing effort, we sample the neighbors $N$ only once via $\left[0 ; \max \left(k^{+}, k_{\text {hard }}^{-}\right)\right]$, and then generate triplets by range-selection in $N$; i.e. positives $=\left(k^{+}-\right.$ $\left.c^{+} ; k^{+}\right]$, and hard negatives $=\left(k_{\text {hard }}^{-}-c_{\text {hard }}^{-} ; k_{\text {hard }}^{-}\right]$.
(II) Random sampling: Sample any $c^{\prime}$ papers without replacement from the corpus.
(III) Filtered random: Like (II) but excluding the papers that are retrieved by KNN, i.e., all neighbors within the largest $k$ are excluded. This is ana$\log$ to SPECTER's approach of selecting random candidates that are not cited by the query.</p>
<p>The KNN sampling introduces the hyperparameter $k$ that allows for the controlled sampling of positives or negatives with different difficulty (from easy to hard depending on $k$ ). Specifically, in Fig. 1 the hyperparameter $k$ defines the tunable sample induced margin between positives and negatives, as well as the width and position of the positive sample band (green) and negative sample band (red) around the query sample. Besides the strategies above, we experiment with similarity threshold, k-means clustering and sorted random sampling, neither of which performs well (Appendix F).</p>
<h2>4 Experiments</h2>
<p>In the following, we introduce our experiments including the data sets and implementation details.</p>
<h3>4.1 Evaluation Dataset</h3>
<p>We evaluate on the SciDocs benchmark (Cohan et al., 2020). A key difference to other benchmarks is that embeddings are the input to the individual tasks without explicit fine-tuning. The SciDocs benchmark consists of the following four tasks:</p>
<p>Document classification (CLS) with Medical Subject Headings (MeSH) (Lipscomb, 2000) and Microsoft Academic Graph labels (MAG) (Sinha et al., 2015). Co-views and co-reads (USR) prediction based on the L2 distance between embeddings. Direct and co-citation (CITE) prediction based on the L2 distance between the embeddings. Recommendations (REC) generation based on embeddings and paper metadata.</p>
<h3>4.2 Training Datasets</h3>
<p>The experiments mainly compare SciNCL against SPECTER on the SciDocs benchmark. However, we found $40.5 \%$ of SciDocs's papers leaking into SPECTER's training data (the leakage affects only the unsupervised paper data but not the gold labels - see Appendix B). To be transparent about this leakage, we train SciNCL on two datasets:</p>
<p>SPECTER replication (w/ leakage): We replicate SPECTER's training data including its leakage. Unfortunately, SPECTER provides neither citation data nor a mapping to S2ORC, which our citation embeddings are based on. We successfully map $96.2 \%$ of SPECTER's query papers and $83.3 \%$ of the corpus from which positives and negatives are sampled to S2ORC. To account for the missing papers, we randomly sample papers from S2ORC (without the SciDocs papers) such that the absolute number of papers is identical with SPECTER.</p>
<p>S2ORC subset (w/o leakage): We select a random subset from S2ORC that does not contain any of the mapped SciDocs papers. This avoids SPECTER's leakage, but also makes the scores reported in Cohan et al. (2020) less comparable. We successfully map $98.6 \%$ of the SciDocs papers to S2ORC. Thus, only the remaining $1.4 \%$ of the SciDocs papers could leak into this training set.</p>
<p>The details of the dataset creation are described in Appendix A and C. Both training sets yield 684K triplets (same count as SPECTER). Also, the ratio of training triplets per query remains the same (§3.4). Our citation embedding model is trained on the S2ORC citation graph. In w/ leakage, we include all SPECTER papers even if they are part</p>
<p>of SciDocss, the remaining SciDocss papers are excluded ( 52.5 nodes and 463 M edges). In w/o leakage, all mapped SciDoCS papers are excluded ( 52.4 M nodes and 447 M edges) such that we avoid leakage also for the citation embedding model.</p>
<h3>4.3 Model Training and Implementation</h3>
<p>We replicate the training setup from SPECTER as closely as possible. We implement SciNCL using Huggingface Transformers (Wolf et al., 2020), initialize the model with SciBERT's weights (Beltagy et al., 2019), and train via the triplet loss (Equation 3.1). The optimizer is Adam with weight decay (Kingma and Ba, 2015; Loshchilov and Hutter, 2019) and learning rate $\lambda=2^{-5}$. To explore the effect of computing efficient fine-tuning we also train a BitFit model (Ben Zaken et al., 2022) with $\lambda=1^{-4}$ (§7.2). We train SciNCL on two NVIDIA GeForce RTX 6000 (24G) for 2 epochs (approx. 24 hours of training time) with batch size 8 and gradient accumulation for an effective batch size of 32 (same as SPECTER). The graph embedding training is performed on an Intel Xeon Gold 6230 CPU with 60 cores and takes approx. 6 hours. The KNN strategy is implemented with FAISS (Johnson et al., 2021) using a flat index (exhaustive search) and takes less than 30 min for indexing and retrieval of the triplets.</p>
<h3>4.4 Baseline Methods</h3>
<p>We compare against the following baselines (details in Appendix E): USE (Cer et al., 2018), BERT (Devlin et al., 2019), BioBERT (Lee et al., 2019), SciBERT (Beltagy et al., 2019), CiteBERT (Wright and Augenstein, 2021), DeCLUTR (Giorgi et al., 2021), the graph-convolution approach SGC (Wu et al., 2019), Citeomatic (Bhagavatula et al., 2018), and SPECTER (Cohan et al., 2020).</p>
<p>Also, we compare against Oracle SciDocs which is identical to SciNCL except that its triplets are generated based on SciDocs's validation and test set using their gold labels. For example, papers with the same MAG labels are positives and papers with different labels are negatives. Similarly, the ground truth of the other tasks is used, i.e., clicked recommendations are considered as positives etc. In total, this procedure creates 106 K training triplets for Oracle SciDocs. Moreover, we under-sample triplets from the classification tasks to ensure a balanced triplet distribution over the tasks. Accordingly, Oracle SciDocs represents an estimate for the performance upper bound that can
be achieved with the current setting (triplet margin loss and SciBERT encoder).</p>
<h2>5 Overall Results</h2>
<p>Tab. 1 shows the results, comparing SciNCL with the best validation performance against the baselines. With replicated SPECTER training data (w/ leakage), SciNCL achieves an average performance of 81.8 across all metrics, which is a 1.8 point absolute improvement over SPECTER (the next-best baseline). When trained without leakage, the improvement of SciNCL over SPECTER is consistent with 1.7 points but generally lower ( 79.4 avg. score). In the following, we refer to the results obtained through training on the replicated SPECTER data (w/ leakage) if not otherwise mentioned.</p>
<p>We find the best validation performance based on SPECTER's data when positives and hard negative are sampled with KNN , whereby positives are $k^{+}=25$, and hard negatives are $k_{\text {hard }}^{-}=4000$ (§6). Easy negatives are generated through filtered random sampling. SciNCL's scores are reported as mean over ten random seeds (seed $\in[0,9]$ ).</p>
<p>For MAG classification, SPECTER achieves the best result with 82.0 F1 followed by SciNCL with 81.4 F1 (-0.6 points). For MeSH classification, SciNCL yields the highest score with 88.7 F1 (+2.3 compared to SPECTER). Both classification tasks have in common that the chosen training settings lead to over-fitting. Changing the training by using only $1 \%$ training data, SciNCL yields 82.2 F1@MAG (Tab. 2). In all user activity and citation tasks, SciNCL yields higher scores than all baselines. Moreover, SciNCL outperforms SGC on direct citation prediction, where SGC outperforms SPECTER in terms of nDCG. On the recommender task, SPECTER yields the best $\mathrm{P} @ 1$ with 20.0, whereas SciNCL achieves 19.3 P@1 (in terms of nDCG SciNCL and SPECTER are on par).</p>
<p>When training SPECTER and SciNCL without leakage, SciNCL outperforms SPECTER even in 11 of 12 metrics and is on par in the other metric. This suggests that SciNCL's hyperparameters have a low corpus dependency since they were only optimized on the corpus with leakage.</p>
<p>Regarding the LLM baselines, we observe that the general-domain BERT, with a score of 63.4, outperforms the domain-specific BERT variants, namely SciBERT (59.6) and BioBERT (58.8). LLMs without citations or contrastive objectives yield generally poor results. This emphasizes the</p>
<p>| Task $\rightarrow$ | Classification | User activity prediction | Citation prediction | Recomm. | Avg. |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Subtask $\rightarrow$ | MAG | MeSH | Co-View | Co-Read | Cite | Co-Cite |  |
| Model $\downarrow /$ Metric $\rightarrow$ | F1 | F1 | MAP | nDCG | MAP | nDCG | MAP | nDCG | MAP | nDCG | mDCG | P@1 |  |
| Oracle SciDocs $\dagger$ | 87.1 | 94.8 | 87.2 | 93.5 | 88.7 | 94.6 | 92.3 | 96.8 | 91.4 | 96.4 | 53.8 | 19.4 | 83.0 |
| USE (2018) | 80.0 | 83.9 | 77.2 | 88.1 | 76.5 | 88.1 | 76.6 | 89.0 | 78.3 | 89.8 | 53.7 | 19.6 | 75.1 |
| Citeomatic<em> (2018) | 67.1 | 75.7 | 81.1 | 90.2 | 80.5 | 90.2 | 86.3 | 94.1 | 84.4 | 92.8 | 52.5 | 17.3 | 76.0 |
| SGC</em> (2019) | 76.8 | 82.7 | 77.2 | 88.0 | 75.7 | 87.5 | 91.6 | 96.2 | 84.1 | 92.5 | 52.7 | 18.2 | 76.9 |
| BERT (2019) | 79.9 | 74.3 | 59.9 | 78.3 | 57.1 | 76.4 | 54.3 | 75.1 | 57.9 | 77.3 | 52.1 | 18.1 | 63.4 |
| SciBERT<em> (2019) | 79.7 | 80.7 | 50.7 | 73.1 | 47.7 | 71.1 | 48.3 | 71.7 | 49.7 | 72.6 | 52.1 | 17.9 | 59.6 |
| BioBERT (2019) | 77.2 | 73.0 | 53.3 | 74.0 | 50.6 | 72.2 | 45.5 | 69.0 | 49.4 | 71.8 | 52.0 | 17.9 | 58.8 |
| CiteBERT (2021) | 78.8 | 74.8 | 53.2 | 73.6 | 49.9 | 71.3 | 45.0 | 67.9 | 50.3 | 72.1 | 51.6 | 17.0 | 58.8 |
| DeCLUTR (2021) | 81.2 | 88.0 | 63.4 | 80.6 | 60.0 | 78.6 | 57.2 | 77.4 | 62.9 | 80.9 | 52.0 | 17.4 | 66.6 |
| SPECTER</em> (2020) | 82.0 | 86.4 | 83.6 | 91.5 | 84.5 | 92.4 | 88.3 | 94.9 | 88.1 | 94.8 | 53.9 | 20.0 | 80.0 |
| Replicated SPECTER training data (w/ leakage): |  |  |  |  |  |  |  |  |  |  |  |  |  |
| SciNCL (ours) | 81.4 | 88.7 | 85.3 | 92.3 | 87.5 | 93.9 | 93.6 | 97.3 | 91.6 | 96.4 | 53.9 | 19.3 | 81.8 |
| $\pm \sigma$ w/ ten seeds | .449 | .422 | .128 | .08 | .162 | .118 | .104 | .054 | .099 | .066 | .203 | .356 | .064 |
| Random S2ORC training data (w/o leakage): |  |  |  |  |  |  |  |  |  |  |  |  |  |
| SPECTER | 81.3 | 88.4 | 83.1 | 91.3 | 84.0 | 92.1 | 86.2 | 93.9 | 87.8 | 94.7 | 52.2 | 17.5 | 79.4 |
| SciNCL (ours) | 81.3 | 89.4 | 84.3 | 91.8 | 85.6 | 92.8 | 91.4 | 96.3 | 90.1 | 95.7 | 54.3 | 19.9 | 81.1 |</p>
<p>Table 1: Results on the SciDocs test set. With replicated SPECTER training data, SciNCL surpasses the previous best avg. score by 1.8 points and also outperforms the baselines in 9 of 12 task metrics. Our scores are reported as mean and standard deviation $\sigma$ over ten random seeds. With training data randomly sampled from S2ORC, SciNCL outperforms SPECTER in terms of avg. score with 1.7 points. The scores with * are from Cohan et al. (2020). Oracle SciDocs $\dagger$ is the upper bound of the performance with triplets from SciDocs's data.
anisotropy problem of embeddings directly extracted from current LLMs and highlights the advantage of combining text and citation information.</p>
<p>In summary, we show that SciNCL's triplet selection leads on average to a performance improvement on SciDocss, with most gains being observed for user activity and citation tasks. The gain from 80.0 to 81.8 is particularly notable given that even Oracle SciDocs yields with 83.0 an only marginally higher avg. score despite using test and validation data from SciDocs for the triplet selection. Appendix H shows examples of paper triplets.</p>
<h2>6 Impact of Sample Difficulty</h2>
<p>In this section, we present the optimization of SciNCL's sampling strategy (§3.3). We optimize the sampling for positives and hard or easy negatives with partial grid search on a random sample of $10 \%$ of the replicated SPECTER training data (sampling based on queries). Our experiments show that optimizations on this subset correlate with the entire dataset. The validation scores in Fig. 2 and 3 are reported as the mean over three random seeds.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results on the validation set w.r.t. positive sampling with KNN when using $10 \%$ training data.</p>
<h3>6.1 Positive Samples</h3>
<p>Fig. 2 shows the avg. scores on the SciDocss validation set depending on the selection of positives with the KNN strategy. We only change $k^{+}$, while negative sampling remains fixed to its best setting (§6.2). The performance is relatively stable for $k^{+}&lt;100$ with peak at $k^{+}=25$, for $k^{+}&gt;100$ the performance declines as $k^{+}$increases. Wang and Isola (2020) state that positive samples should be semantically similar to each other, but not too</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results on the validation set w.r.t. hard negative sampling with KNN using 10% training data.</p>
<p>Similar to the query. For example, at k⁺=5, positives may be a bit "too easy" to learn, such that they produce less informative gradients than the optimal setting k⁺=25. Similarly, making k⁺ too large leads to the <em>sampling induced margin</em> being too small, such that <em>positives collide with negative samples</em>, which creates contrastive label noise that degrades performance (Saunshi et al., 2019).</p>
<p>Another observation is the standard deviation σ: One would expect σ to be independent of k⁺ since random seeds affect only the negatives. However, positives and negatives interact with each other through the triplet margin loss. Therefore, σ is also affected by k⁺. To account for the interaction of positives and negatives, one could sample simultaneously based on the distance to the query and the distance of positives and negatives to each other.</p>
<h3>6.2 Hard Negative Samples</h3>
<p>Fig. 3 presents the validation results for different k⁻<sub>hard</sub> given the best setting for positives (k⁺=25). The performance increases with increasing k⁻<sub>hard</sub> until a plateau between 2000&lt;k⁻<sub>hard</sub> &lt;4000 with a peak at k⁻<sub>hard</sub> = 4000. This plateau can also be observed in the test set, where k⁻<sub>hard</sub> = 3000 yields a marginally lower score of 81.7 (Tab. 2). For k⁻<sub>hard</sub> &gt; 4000, the performance starts to decline again. This suggests that for large k⁻<sub>hard</sub> the samples are not "hard enough" which confirms the findings of Cohan et al. (2020).</p>
<h3>6.3 Easy Negative Samples</h3>
<p>Filtered random sampling of easy negatives yields the best validation performance compared pure random sampling (Tab. 2). However, the performance difference is marginal. When rounded to one decimal, their average test scores are identical. The</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Number of collisions w.r.t. size of the sample induced margin as defined through k⁺ and k⁻<sub>hard</sub>.</p>
<p>marginal difference is caused by the large corpus size and the resulting small probability of randomly sampling one paper from the KNN results. But without filtering, the effect of random seeds increases, since we find a higher standard deviation compared to the one with filtering.</p>
<p>As a potential way to decrease randomness, we experiment with other approaches like k-means clustering but find that they decrease the performance (Appendix F).</p>
<h3>6.4 Collisions</h3>
<p>Similar to SPECTER, SciNCL's sampling based on graph embeddings could cause collisions when selecting positives and negatives from regions close to each other. To avoid this, we rely on a sample induced margin that is defined by the hyperparameter k⁺ and k⁻<sub>hard</sub> (distance between red and green band in Fig. 1). When the margin gets too small, positives and negatives are more likely to collide. A collision occurs when the paper pair (dq, dx) is contained in the training data as positive and as negative sample at the same time. Fig. 4 demonstrates the relation between the number of collisions and the size of the sample induced margin. The number of collisions increases when the sample induced margin gets smaller. The opposite is the case when the margin is large enough (k⁻<sub>hard</sub> &gt; 1000), i.e., then the number of collisions goes to zero. This relation also affects the evaluation performance as Fig. 2 and Fig. 3 show. Namely, for large k⁺ or small k⁻<sub>hard</sub> SciNCL's performance declines and approaches SPECTER's performance.</p>
<h3>7 Ablation Analysis</h3>
<p>Next, we evaluate the impact of language model initialization and number of parameters and triples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CLS</th>
<th style="text-align: center;">USR</th>
<th style="text-align: center;">CITE</th>
<th style="text-align: center;">REC</th>
<th style="text-align: center;">Avg.</th>
<th style="text-align: center;">$\Delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SciNCL</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">$\mathbf{9 4 . 7}$</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">$\mathbf{8 1 . 8}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SPECTER</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">-1.8</td>
</tr>
<tr>
<td style="text-align: left;">$k_{\text {hard }}^{+}=2000$</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">$\mathbf{9 4 . 7}$</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">-0.2</td>
</tr>
<tr>
<td style="text-align: left;">$k_{\text {hard }}^{-}=3000$</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">-0.1</td>
</tr>
<tr>
<td style="text-align: left;">easy neg. w/ random</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">$\mathbf{9 4 . 7}$</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">$\mathbf{8 1 . 8}$</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">undirected citations</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">$\mathbf{9 4 . 7}$</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">-0.1</td>
</tr>
<tr>
<td style="text-align: left;">Init. w/ BERT-Base</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">-0.6</td>
</tr>
<tr>
<td style="text-align: left;">Init. w/ BERT-Large</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">-0.4</td>
</tr>
<tr>
<td style="text-align: left;">Init. w/ BioBERT</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">$\mathbf{3 7 . 7}$</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">-0.4</td>
</tr>
<tr>
<td style="text-align: left;">1\% training data</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">-1.0</td>
</tr>
<tr>
<td style="text-align: left;">10\% training data</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">-0.6</td>
</tr>
<tr>
<td style="text-align: left;">BitFit training</td>
<td style="text-align: center;">$\mathbf{8 5 . 8}$</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">-0.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Ablations. Numbers are averages over tasks of the SciDocs test set, average score over all metrics, and rounded absolute difference to SciNCL.</p>
<h3>7.1 Initial Language Models</h3>
<p>Tab. 2 shows the effect of initializing the model weights not with SciBERT but with general-domain LLMs (BERT-Base and BERT-Large) or with BioBERT. The initialization with other LLMs decreases the performance. However, the decline is marginal (BERT-Base -0.6, BERT-Large -0.4, BioBERT -0.4) and all LLMs outperform the SPECTER baseline. For the recommendation task, in which SPECTER is superior over SciNCL, BioBERT outperforms SPECTER. This indicates that the improved triplet mining of SciNCL has a greater domain adaption effect than pretraining on domain-specific literature. Given that pretraining of LLMs requires a magnitude more resources than the fine-tuning with SciNCL, our approach can be a solution for resource-limited use cases.</p>
<h3>7.2 Data and Computing Efficiency</h3>
<p>The last three rows of Tab. 2 show the results regarding data and computing efficiency. When keeping the citation graph unchanged but training the language model with only $10 \%$ of the original triplets, SciNCL still yields a score of 81.1 (-0.6). Even with only $1 \%$ ( 6840 triplets), SciNCL achieves a score of 80.8 that is 1.0 points less than with $100 \%$ but still 0.8 points more than the SPECTER baseline. With this textual sample efficiency, one could manually create triplets or use existing supervised datasets as in Gao et al. (2021).</p>
<p>Lastly, we evaluate BitFit training (Ben Zaken et al., 2022), which only trains the bias terms of the model while freezing all other parameters. This corresponds to training only $0.1 \%$ of the original
parameters. With BitFit, SciNCL yields a considerable score of 81.2 ( -0.5 points). As a result, SciNCL could be trained on the same hardware with even larger (general-domain) language models (§7.1).</p>
<h2>8 Conclusion</h2>
<p>We present a novel approach for contrastive learning of scientific document embeddings that addresses the challenge of selecting informative positive and negative samples. By leveraging citation graph embeddings for sample generation, SciNCL achieves a score of 81.8 on the SciDocss benchmark, a 1.8 point improvement over the previous best method SPECTER. This is purely achieved by introducing tunable sample difficulty and avoiding collisions between positive and negative samples, while existing LLM and data setups can be reused. This improvement over SPECTER can be also observed when excluding the SciDocs papers during training (see w/o leakage in Tab. 1). Furthermore, SciNCL's improvement from 80.0 to 81.8 is particularly notable given that even oracle triplets, which are generated with SciDocs's test and validation data, yield with 83.0 only a marginally higher score.</p>
<p>Our work highlights the importance of sample generation in a contrastive learning setting. We show that language model training with $1 \%$ of triplets is sufficient to outperform SPECTER, whereas the remaining $99 \%$ provide only 1.0 additional points ( 80.8 to 81.8 ). This sample efficiency is achieved by adding reasonable effort for sample generation, i.e., graph embedding training and KNN search. We also demonstrate that in-domain LLM pretraining (like SciBERT) is beneficial, while general-domain LLMs can achieve comparable performance and even outperform SPECTER. This indicates that controlling sample difficulty and avoiding collisions is more effective than indomain pretraining, especially in scenarios where training an LLM from scratch is infeasible.</p>
<h2>9 Limitations</h2>
<p>SciNCL's strategy of selecting positive and negative samples requires additional computational resources for training the graph embedding model, performing the KNN search, and optimizing the hyperparameters $k^{+}, k_{\text {hard }}^{-}$(§4.3). While some of the compute resources are offset by the sampleefficient language model training (§7.2), we still</p>
<p>consider the increased compute effort as the major limitation of the SciNCL method.</p>
<p>Especially the training of the graph embedding model accounts for most of the additional compute effort. This is also the reason for us providing only a shallow of evaluation of the graph embeddings (Appendix D). For example, we did not evaluate the effect of different graph embeddings on the actual SciDocs performance. Moreover, evaluations with smaller subsets of the S2ORC citation graph are missing. Such evaluations could indicate whether also less citation data can be sufficient, which would lower the compute requirements but would make SciNCL also applicable in domains where less graph data is available.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Christian Schulze and his team for providing the compute infrastructure that made our experiments possible. The research presented in this article is partially funded by the German Federal Ministry of Education and Research (BMBF) through the projects QURATOR (Rehm et al., 2020) (Unternehmen Region, Wachstumskern, no. 03WKDA1A) and PANQURA (no. 03COV03E).</p>
<h2>References</h2>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3613-3618, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1-9, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Chandra Bhagavatula, Sergey Feldman, Russell Power, and Waleed Ammar. 2018. Content-based citation recommendation. NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference, $1: 238-251$.</p>
<p>Robin Brochier, Adrien Guille, and Julien Velcin. 2019. Global Vectors for Node Representations. In The</p>
<p>World Wide Web Conference on - WWW '19, volume 2, pages 2587-2593, New York, New York, USA. ACM Press.</p>
<p>Maxime Bucher, Stéphane Herbin, and Frédéric Jurie. 2016. Hard negative mining for metric learning based zero-shot classification. In Computer Vision - ECCV 2016 Workshops, pages 524-531, Cham. Springer International Publishing.</p>
<p>Daniel Matthew Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal sentence encoder. arXiv:1803.11175.</p>
<p>Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural Scaffolds for Citation Intent Classification in Scientific Publications. In Proceedings of the 2019 Conference of the North, volume 1, pages 3586-3596, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020. SPECTER: Document-level Representation Learning using Citation-informed Transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270-2282, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Aaron Elkiss, Siwei Shen, Anthony Fader, Güneş Erkan, David States, and Dragomir Radev. 2008. Blind men and elephants: What do citation summaries tell us about a research article? Journal of the American Society for Information Science and Technology, 59(1):51-62.</p>
<p>Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. 2020. CERT: Contrastive Self-supervised Learning for Language Understanding. arXiv:2005.12766, pages 1-16.</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2021. DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. In Proceedings of the 59th Annual Meeting of the Association</p>
<p>for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 879-895, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Bela Gipp and J Beel. 2009. Citation Proximity Analysis (CPA) - A new approach for identifying related work based on Co-Citation Analysis. Birger Larsen and Jacqueline Leta, editors, Proceedings of the 12th International Conference on Scientometrics and Informetrics (ISSI'09), 2(July):571-575.</p>
<p>Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16, pages 855-864, New York, New York, USA. ACM Press.</p>
<p>Jialong Han, Yan Song, Wayne Xin Zhao, Shuming Shi, and Haisong Zhang. 2018. hyperdoc2vec: Distributed Representations of Hypertext Documents. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 2384-2394, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Andreas Nugaard Holm, Barbara Plank, Dustin Wright, and Isabelle Augenstein. 2022. Longitudinal Citation Prediction using Temporal Graph Neural Networks. In AAAI 2022 Workshop on Scientific Document Understanding (SDU 2022).</p>
<p>Chanwoo Jeong, Sion Jang, Hyuna Shin, Eunjeong Lucy Park, and Sungchul Choi. 2020. A context-aware citation recommendation model with bert and graph convolutional networks. Scientometrics, pages $1-16$.</p>
<p>Jeff Johnson, Matthijs Douze, and Herve Jegou. 2021. Billion-Scale Similarity Search with GPUs. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Parminder Kaur, Husanbir Singh Pannu, and Avleen Kaur Malhi. 2021. Comparative analysis on cross-modal information retrieval: A review. Computer Science Review, 39:100336.
M. M. Kessler. 1963. Bibliographic coupling between scientific papers. American Documentation, 14(1):10-25.</p>
<p>Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. In Advances in Neural Information Processing Systems, volume 33, pages 18661-18673. Curran Associates, Inc.</p>
<p>Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021. Self-guided contrastive learning for BERT sentence representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers), pages 2528-2540, Online. Association for Computational Linguistics.</p>
<p>Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A method for stochastic optimization. 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, pages $1-15$.</p>
<p>Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, pages 1-8.</p>
<p>Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt, Abhijit Bose, and Alex Peysakhovich. 2019. PyTorch-BigGraph: A Largescale Graph Embedding System. In Proceedings of The Conference on Systems and Machine Learning.</p>
<p>Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. 2020. On the Sentence Embeddings from Pre-trained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9119-9130, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Carolyn E. Lipscomb. 2000. Medical subject headings (mesh). Bulletin of the Medical Library Association, 88 3:265-6.</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. 7th International Conference on Learning Representations, ICLR 2019.</p>
<p>Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle Lo, Isabel Cachola, and Noah A. Smith. 2021. Explaining Relationships Between Scientific Documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2130-2144, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Kevin Musgrave, Serge J. Belongie, and Ser-Nam Lim. 2020. A metric learning reality check. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXV, pages 681-699.</p>
<p>Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, and Georg Rehm. 2020. Aspect-based Document Similarity for Research Papers. In Proceedings of the 28th International Conference on Computational Linguistics (COLING 2020).</p>
<p>Simon Pasternack. 1969. The scientific enterprise: Public knowledge. an essay concerning the social dimension of science. Science, 164(3880):669-670.</p>
<p>Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14, pages 701-710, New York, New York, USA. ACM Press.</p>
<p>Georg Rehm, Peter Bourgonje, Stefanie Hegele, Florian Kintzel, Julán Moreno Schneider, Malte Ostendorff, Karolina Zaczynska, Armin Berger, Stefan Grill, Sören Räuchle, Jens Rauenbusch, Lisa Rutenburg, André Schmidt, Mikka Wild, Henry Hoffmann, Julian Fink, Sarah Schulz, Jurica Seva, Joachim Quantz, Joachim Böttger, Josefine Matthey, Rolf Fricke, Jan Thomsen, Adrian Paschke, Jamal Al Qundus, Thomas Hoppe, Naouel Karam, Frauke Weichhardt, Christian Fillies, Clemens Neudecker, Mike Gerber, Kai Labusch, Vahid Rezanezhad, Robin Schaefer, David Zellhöfer, Daniel Siewert, Patrick Bunk, Lydia Pintscher, Elena Aleynikova, and Franziska Heine. 2020. QURATOR: Innovative Technologies for Content and Data Curation. In Proceedings of QURATOR 2020 - The conference for intelligent content solutions, Berlin, Germany. CEUR Workshop Proceedings, Volume 2535. 20/21 January 2020.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3980-3990, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Nils Rethmeier and Isabelle Augenstein. 2022a. LongTail Zero and Few-Shot Learning via Contrastive Pretraining on and for Small Data. Computer Sciences; Mathematics Forum, 3(1).</p>
<p>Nils Rethmeier and Isabelle Augenstein. 2022b. A primer on contrastive pretraining in language processing: Methods, lessons learned \&amp; perspectives. ACM Comput. Surv.</p>
<p>Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertology: What we know about how BERT works. Trans. Assoc. Comput. Linguistics, 8:842-866.</p>
<p>Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. 2019. A theoretical analysis of contrastive unsupervised representation learning. In ICML, volume 97 of $P M L R$. PMLR.</p>
<p>Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A unified embedding for face recognition and clustering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR</p>
<p>2015, Boston, MA, USA, June 7-12, 2015, pages 815-823.</p>
<p>Connor Shorten, Taghi M. Khoshgoftaar, and Borko Furht. 2021. Text data augmentation for deep learning. J. Big Data, 8(1):101.</p>
<p>Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Paul Hsu, and Kuansan Wang. 2015. An overview of microsoft academic service (mas) and applications. Proceedings of the 24th International Conference on World Wide Web.</p>
<p>Henry Small. 1973. Co-citation in the scientific literature: A new measure of the relationship between two documents. Journal of the American Society for Information Science, 24(4):265-269.</p>
<p>Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. 2013. Zero-shot learning through cross-modal transfer. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.</p>
<p>Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006. Automatic classification of citation function. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing - EMNLP '06, page 103, Morristown, NJ, USA. Association for Computational Linguistics.</p>
<p>Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020a. Contrastive representation distillation. In International Conference on Learning Representations.</p>
<p>Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. 2020b. What makes for good views for contrastive learning? In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000-6010.</p>
<p>David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or Fiction: Verifying Scientific Claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534-7550, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9929-9939. PMLR.</p>
<p>X. Wang, Yufei Ye, and Abhinav Kumar Gupta. 2018. Zero-shot recognition via semantic embeddings and knowledge graphs. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $6857-6866$.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45.</p>
<p>Dustin Wright and Isabelle Augenstein. 2021. CiteWorth: Cite-worthiness detection for improved scientific document understanding. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 1796-1807, Online. Association for Computational Linguistics.</p>
<p>Chao-yuan Wu, R. Manmatha, Alexander J Smola, and Philipp Krahenbuhl. 2017. Sampling Matters in Deep Embedding Learning. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2859-2867. IEEE.</p>
<p>Felix Wu, Tianyi Zhang, Amauri Holanda de Souza, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger. 2019. Simplifying Graph Convolutional Networks. In Proceedings of the 36th International Conference on Machine Learning, volume 6861-6871, pages 815-826. PMLR.</p>
<p>Xing Wu, Chaochen Gao, Liangjun Zang, Jizhong Han, Zhongyuan Wang, and Songlin Hu. 2021. Smoothed Contrastive Learning for Unsupervised Sentence Embedding. arXiv:2109.04321.</p>
<p>Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020. CLEAR: Contrastive Learning for Sentence Representation. arXiv:2012.15466.</p>
<p>Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. In International Conference on Learning Representations, pages 1-16.</p>
<p>Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y. Chang. 2015. Network representation learning with rich text information. In Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI'15, page 2111-2117. AAAI Press.</p>
<h2>A Mapping to S2ORC</h2>
<p>Table 3: Mapping to S2ORC citation graph</p>
<table>
<thead>
<tr>
<th style="text-align: left;">S2ORC mapping</th>
<th style="text-align: left;">Success rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SciDocs papers</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">- with S2ORC IDs</td>
<td style="text-align: left;">$220,815 / 223,932(98.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">- in S2ORC graph</td>
<td style="text-align: left;">$197,811 / 223,932(88.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">SPECTER papers</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">- with S2ORC IDs</td>
<td style="text-align: left;">$311,094 / 311,860(99.7 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">- in S2ORC graph</td>
<td style="text-align: left;">$260,014 / 311,860(83.3 \%)$</td>
</tr>
</tbody>
</table>
<p>Neither the SPECTER training data nor the SciDocs test data comes with a mapping to the S2ORC dataset, which we use for the training of the citation embedding model. However, to replicate SPECTER's training data and to avoid leakage of SciDocs test data such a mapping is needed. Therefore, we try to map the papers to S2ORC based on PDF hashes and exact title matches. The remaining paper metadata is collected through the Semantic Scholar API. Tab. 3 summarizes the outcome of mapping procedure. Failed mappings can be attributed to papers being unavailable through the Semantic Scholar API (e.g., retracted papers) or papers not being part of S2ORC citation graph.</p>
<h2>B SPECTER-SciDocs Leakage</h2>
<p>When replicating SPECTER (Cohan et al., 2020), we found a substantial overlap between the papers $^{3}$ used during the model training and the papers from their SciDocs benchmark ${ }^{4}$. In both datasets, papers are associated with Semantic Scholar IDs. Thus, no custom ID mapping as in Appendix A is required to identify papers that leak from training to test data. From the 311,860 unique papers used in SPECTER's training data, we find 79,201 papers $(25.4 \%)$ in the test set of SciDoCS and 79,609 papers ( $25.5 \%$ ) in its validation set. When combining test and validation set, there is a total overlap of 126,176 papers ( $40.5 \%$ ). However, this overlap affects only the 'unsupervised' paper metadata (title, abstract, citations, etc.) and not the gold labels used in SciDoCS (e.g., MAG labels or clicked recommendations).</p>
<h2>C Dataset Creation</h2>
<p>As describe in $\S 4.2$, we conduct our experiments on two datasets. Both datasets rely on the cita-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion graph of S2ORC (Lo et al., 2020). More specifically, S2ORC with the version identifier 20200705v1 is used. The full citation graph consists of 52.6 M nodes (papers) and 467 M edges (citations). Tab. 4 presents statistics on the datasets and their overlap with SPECTER and SciDoCS. The steps to reproduce both datasets are:</p>
<p>Replicated SPECTER (w/ leakage) In order to replicate SPECTER's training data and do not increase the leakage, we exclude all SciDoCS papers which are not used by SPECTER from the S2ORC citation graph. This means that apart from the 110,538 SPECTER papers not a single other SciDoCS paper is included. The resulting citation graph has 52.5 M nodes and 463 M edges and is used for training the citation graph embeddings.</p>
<p>For the SciNCL triplet selection, we also replicate SPECTER's query papers and its corpus from which positive and negatives are sampled. Our mapping and the underlying citation graph allows us to use 227,869 of 248,007 SPECTER's papers for training. Regarding query papers, we use 131,644 of 136,820 SPECTER's query papers. To align the number training triplets with the one from SPECTER, additional papers are randomly sampled from the filtered citation graph.</p>
<p>Random S2ORC subset (w/o leakage) To avoid leakage, we exclude all successfully mapped SciDoCS papers from the S2ORC citation graph. After filtering the graph has 52.3 nodes and 447M edges. The citation graph embedding model is trained on this graph.</p>
<p>Next, we reproduce triplet selection from SPECTER. Any random 136,820 query papers are selected from the filtered graph. For each query, we generate five positives (cited by the query), two hard negatives (citation of citation), and three random nodes from the filtered S2ORC citation graphs. This sampling produces 684,100 training triplets with 680,967 unique papers IDs (more compared to the replicated SPECTER dataset). Based on these triplets the SPECTER model for this dataset is trained with the same model settings and hyperparameters as SciNCL (second last row in Tab. 1).</p>
<p>Lastly, the SciNCL triplets are generated based on the citation graph embeddings of the same 680,967 unique papers IDs, i.e, the FAISS index contains only these papers and not the remaining S2ORC papers. Also, the same 136,820 query papers are used.</p>
<p>Table 4: Statistics for our two datasets and their overlap with SPECTER and SciDocs respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Replicated <br> SPECTER <br> (w/ leakage)</th>
<th style="text-align: center;">Random <br> S2ORC subset <br> (w/o leakage)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training triplets</td>
<td style="text-align: center;">684,100</td>
<td style="text-align: center;">684,100</td>
</tr>
<tr>
<td style="text-align: left;">Unique paper IDs</td>
<td style="text-align: center;">248,007</td>
<td style="text-align: center;">680,967</td>
</tr>
<tr>
<td style="text-align: left;">- in SPECTER</td>
<td style="text-align: center;">227,869</td>
<td style="text-align: center;">9,182</td>
</tr>
<tr>
<td style="text-align: left;">- in SciDocs</td>
<td style="text-align: center;">110,538</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">- in SciDocs</td>
<td style="text-align: center;">110,538</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">and in SPECTER</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Query paper IDs</td>
<td style="text-align: center;">136,820</td>
<td style="text-align: center;">136,820</td>
</tr>
<tr>
<td style="text-align: left;">- in SciDocs</td>
<td style="text-align: center;">69,306</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">- in SPECTER queries</td>
<td style="text-align: center;">131,644</td>
<td style="text-align: center;">463</td>
</tr>
<tr>
<td style="text-align: left;">Citation graph</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- Nodes</td>
<td style="text-align: center;">$52,526,134$</td>
<td style="text-align: center;">$52,373,977$</td>
</tr>
<tr>
<td style="text-align: left;">- Edges</td>
<td style="text-align: center;">$463,697,639$</td>
<td style="text-align: center;">$447,697,727$</td>
</tr>
</tbody>
</table>
<h2>D Graph Embedding Evaluation</h2>
<p>To evaluate the underlying citation graph embeddings, we experiment with a few of BigGraph's hyperparameters. We trained embeddings with different dimensions $d={128,512,768}$ and different distance measures (cosine similarity and dot product) on $99 \%$ of the data and test the remaining $1 \%$ on the link prediction task. An evaluation of the graph embeddings with SciDocs is not possible since we could not map the papers used in SciDocs to the S2ORC corpus. All variations are trained for 20 epochs, margin $m=0.15$, and learning rate $\lambda=0.1$ (based on the recommended settings by Lerer et al. (2019)).</p>
<p>Table 5: Link prediction performance of BigGraph embeddings trained on S2ORC citation graph with different dimensions and distance measures.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dim.</th>
<th style="text-align: center;">Dist.</th>
<th style="text-align: center;">MRR</th>
<th style="text-align: center;">Hits@1</th>
<th style="text-align: center;">Hits@10</th>
<th style="text-align: center;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">Cos.</td>
<td style="text-align: center;">54.09</td>
<td style="text-align: center;">43.39</td>
<td style="text-align: center;">75.21</td>
<td style="text-align: center;">85.75</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">Dot</td>
<td style="text-align: center;">89.75</td>
<td style="text-align: center;">85.84</td>
<td style="text-align: center;">96.13</td>
<td style="text-align: center;">97.70</td>
</tr>
<tr>
<td style="text-align: center;">512</td>
<td style="text-align: center;">Dot</td>
<td style="text-align: center;">94.60</td>
<td style="text-align: center;">92.47</td>
<td style="text-align: center;">97.64</td>
<td style="text-align: center;">98.64</td>
</tr>
<tr>
<td style="text-align: center;">768</td>
<td style="text-align: center;">Dot</td>
<td style="text-align: center;">95.12</td>
<td style="text-align: center;">93.22</td>
<td style="text-align: center;">97.77</td>
<td style="text-align: center;">98.74</td>
</tr>
</tbody>
</table>
<p>Tab. 5 shows the link prediction performance measured in MRR, Hits@1, Hits@10, and AUC. Dot product is substantially better than cosine similarity as distance measure. Also, there is a positive correlation between the performance and the size of the embeddings. The larger the embedding size the better link prediction performance. Graph embeddings with $d=768$ were the largest possible size given our compute resources (available disk space
was the limiting factor).</p>
<h2>E Baseline Details</h2>
<p>If not otherwise mentioned, all BERT variations are used in their base-uncased versions.</p>
<p>The weights for BERT (bert-base-uncased), BioBERT (biobert-base-cased-v1.2), CiteBERT (citebert), DeCLUTR (declutr-sci-base) are taken from Huggingface Hub ${ }^{5}$. We use Universal Sentence Encoder (USE) from Tensorflow Hub ${ }^{6}$. For Oracle SciDocs, we use the SciNCL implementation and under-sample the triplets from the classification tasks to ensure a balanced triplet distribution over the tasks. The SPECTER version for the random S2ORC training data (w/o leakage) is also trained with the SciNCL implementation. Please see Cohan et al. (2020) for additional baseline methods and their implementation details.</p>
<h2>F Negative Results</h2>
<p>We investigated additional sampling strategies and model modification of which none led to a significant performance improvement.</p>
<h2>F. 1 Undirected Citations</h2>
<p>Our graph embedding model considers citations as directed edges by default. We also train a SciNCL model with undirected citations by first converting a single edge $(a, b)$ into the two edges $(a, b)$ and $(b, a)$. This approach yields a slightly worse performance ( 81.7 avg. score; -0.1 points) and, therefore, was discarded for the final experiments.</p>
<h2>F. 2 KNN with interval large than $c$</h2>
<p>Our best results are achieved with KNN where the size of the neighbor interval $\left(k-c^{\prime} ; k\right]$ is equal to the number of samples $c^{\prime}$ that the strategy should generate. In addition to this, we also experimented with large intervals, e.g., $(1000 ; 2000]$, from which $c^{\prime}$ papers are randomly sampled. This approach yields comparable results but suffers from a larger effect of randomness and is therefore more difficult to optimize.</p>
<h2>F. 3 K-Means Cluster for Easy Negatives</h2>
<p>Easy negatives are supposed to be far away from the query. Random sampling from a large corpus ensures this as our results show. As an alternative</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>approach, we tried k-means clustering whereby we selected easy negatives from the centroid that has a given distance to the query's centroid. However, this decreased the performance.</p>
<h2>F. 4 Sampling with Similarity Threshold</h2>
<p>As alternative to KNN, we select samples based on cosine similarity in the citation embedding space. Take $c^{\prime}$ papers that are within the similarity threshold $t$ of a query paper $d^{Q}$ such that $s\left(f_{c}\left(d^{Q}\right), f_{c}\left(d_{i}\right)\right)&lt;t$, where $s$ is the cosine similarity function.</p>
<p>For example, given the similarity scores $S={0.9,0.8,0.7,0.1}$ (ascending order, the higher the similarity is the closer the candidate embedding to the query embedding is) with $c^{\prime}=2$ and $t=0.5$, the two candidates with the largest similarity scores and larger than the threshold would be 0.8 and 0.7 . The corresponding papers would be selected as samples. While the positive threshold $t^{+}$should close to 1 , the negative threshold $t^{-}$ should be small to ensure samples are dissimilar from $d^{Q}$. However, the empirical results suggest that this strategy is inferior compared to KNN.</p>
<h2>F. 5 Hard Negatives with Similarity Threshold</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Results on the validation set w.r.t. hard negative sampling with SIM using $10 \%$ training data.</p>
<p>Selecting hard negatives based on the similarity threshold yields a test score of 81.7 ( -0.1 points). Fig. 5 show the validation results for different similarity thresholds. A similar pattern as in Fig. 3 can be seen. When the negatives are closer to the query paper (larger similarity threshold $t$ ), the validation score decreases.</p>
<h2>F. 6 Positives with Similarity Threshold</h2>
<p>Positive sampling with SIM performs poorly since even for small $t^{+}&lt;0.5$ many query papers do not have any neighbors within this similarity threshold (more than $40 \%$ ). Solving this issue would require changing the set of query papers which we omit for comparability to SPECTER.</p>
<h2>F. 7 Sorted Random</h2>
<p>Simple random sampling does not ensure if a sample is far or close to the query. To integrate a distance measure in the random sampling, we first sample $n$ candidates, then order the candidates according to their distance to the query, and lastly select the $c^{\prime}$ candidates that are the closest or furthest to the query as samples.</p>
<h2>F. 8 Mask Language Modeling</h2>
<p>Giorgi et al. (2021) show that combining a contrastive loss with a mask language modeling loss can improve text representation learning. However, in our experiments a combined function decreases the performance on ScIDocs, probably due to the effects found by (Li et al., 2020).</p>
<h2>F. 9 Student-Teacher Learning</h2>
<p>Student-teacher learning is effective in related work on cross-modal knowledge transfer (Kaur et al., 2021; Tian et al., 2020a). We also try to adopt this approach for our experiments, whereby the Transformer language model is the student, and the citation graph embedding model is the teacher. By directly learning from the citation embeddings, we could circumvent the positive and negative sampling needed for triplet loss learning, which introduces unwanted issues like collisions. Given a batch of document representations derived from text $D_{\text {Text }}$ (through the language model) and the citation graph representations for the same documents $D_{\text {Graph }}$, we compute the pairwise cosine similarity for both sets $S_{\text {Text }}$ and $S_{\text {Graph }}$. To transfer the knowledge from the citation embeddings into the language model, we devise the studentteacher loss $\mathcal{L}_{S T}$ based on a mean-squared-error loss (MSE) such that the difference between the cosine similarities is minimized:</p>
<p>$$
\mathcal{L}<em _Text="{Text" _text="\text">{S T}=\operatorname{MSE}\left(S</em>\right)
$$}}, S_{\text {Graph }</p>
<p>Despite the promising results from Tian et al. (2020a), the student-teacher approach performs poorly in our experiments. We attribute this the</p>
<p>overfitting to the citation data (the training loss approaches zero after a few steps while the validation loss remains high). The model trained with $\mathcal{L}_{S T}$ yields only a ScIDocs average score of 64.7, slightly better than SciBERT but substantially worse than SciNCL with triplet loss.</p>
<p>Additionally, we experiment with a joint loss that is the sum of triplet margin loss $\mathcal{L}<em S="S" T="T">{\text {Triplet }}$ (see $\S 3.1)$ and the student-teacher loss $\mathcal{L}</em>$ :</p>
<p>$$
\mathcal{L}<em _Triplet="{Triplet" _text="\text">{\text {Joint }}=\mathcal{L}</em>
$$}}+\mathcal{L}_{S T</p>
<p>Training with the joint loss $\mathcal{L}<em _Triplet="{Triplet" _text="\text">{\text {Joint }}$ achieves an average score of 80.5 . Even though the joint loss is not subject to overfitting, its ScIDocs performance is slightly worse than the triplet loss $\mathcal{L}</em>$ alone. Given this outcome and that the computation of the cosine similarities adds additional complexity, we discard the student-teacher approach for the final experiments.}</p>
<h2>F. 10 SPECTER \&amp; Bidirectional Citations</h2>
<p>SPECTER (Cohan et al., 2020) relies on unidirectional citations for their sampling strategy. While papers cited by the query paper are considered as positives samples, those citing the query paper (opposite citation direction) could be negative samples. We see this use of citations as a conceptional flaw in their sampling strategy.</p>
<p>To test the actual effect on the resulting document representation, we first replicate the original unidirectional sampling strategy from SPECTER with our training data (see w/ leakage in §4.2). The resulting SPECTER model achieves an average score of 79.0 on ScIDocs. ${ }^{7}$ When changing the sampling strategy from unidirectional to bidirectional ('citations to the query' are also treated as a signal for similarity), we observe an improvement of +0.4 points to 79.4 . Consequently, the use of unidirectional citations is not only a conceptional issue but also degrades learning performance.</p>
<h2>G Task-specific Results</h2>
<p>Fig. 6 and 7 present the validation performance like in $\S 6$ but on a task-level and not as an average over all tasks. The plots show that the optimal $k^{+}$and $k_{\text {hard }}^{-}$values are partially task dependent.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>H Examples</h2>
<p>Tab. 6 lists three examples of query papers with their corresponding positive and negative samples. The complete set of triplets that we use during training is available in our code repository ${ }^{1}$.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Task-level validation performance w.r.t. $k^{+}$with KNN strategy using $10 \%$ training data.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Task-level validation performance w.r.t. $k_{\text {hard }}^{-}$with KNN strategy using $10 \%$ training data.</p>
<p>Table 6: Example query papers with their positive and negative samples.</p>
<p>Query: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p>
<ul>
<li>A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</li>
<li>Looking for ELMo's Friends: Sentence-Level Pretraining Beyond Language Modeling
Positives: $\quad$ GLUE : A MultiTask Benchmark and Analysis Platform for Natural Language Understanding</li>
<li>Dissecting Contextual Word Embeddings: Architecture and Representation</li>
<li>Universal Transformers</li>
<li>Planning for decentralized control of multiple robots under uncertainty</li>
<li>Graph-Based Relational Data Visualization</li>
</ul>
<p>Negatives: $\quad$ Linked Stream Data Processing</p>
<ul>
<li>Topic Modeling Using Distributed Word Embeddings</li>
<li>Adversarially-Trained Normalized Noisy-Feature Auto-Encoder for Text Generation</li>
</ul>
<p>Query: BioBERT: a pre-trained biomedical language representation model for biomedical text mining</p>
<ul>
<li>Exploring Word Embedding for Drug Name Recognition</li>
<li>A neural joint model for entity and relation extraction from biomedical text</li>
</ul>
<p>Positives: $\quad$ Event Detection with Hybrid Neural Architecture</p>
<ul>
<li>Improving chemical disease relation extraction with rich features and weakly labeled data</li>
<li>GLUE : A MultiTask Benchmark and Analysis Platform for Natural Language Understanding</li>
<li>Weakly Supervised Facial Attribute Manipulation via Deep Adversarial Network</li>
<li>Applying the Clique Percolation Method to analyzing cross-market branch banking ...</li>
</ul>
<p>Negatives: $\quad$ Perpetual environmentally powered sensor networks</p>
<ul>
<li>Labelling strategies for hierarchical multi-label classification techniques</li>
<li>Domain Aware Neural Dialog System</li>
</ul>
<p>Query: A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</p>
<ul>
<li>Content-based citation analysis: The next generation of citation analysis</li>
<li>ScisummNet: A Large Annotated Dataset and Content-Impact Models for Scientific Paper ...</li>
</ul>
<p>Positives: $\quad$ Citation Block Determination Using Textual Coherence</p>
<ul>
<li>Discourse Segmentation Of Multi-Party Conversation</li>
<li>Argumentative Zoning for Improved Citation Indexing</li>
<li>Adaptive Quantization for Hashing: An Information-Based Approach to Learning ...</li>
<li>Trap Design for Vibratory Bowl Feeders</li>
</ul>
<p>Negatives: $\quad$ Software system for the Mars 2020 mission sampling and caching testbeds</p>
<ul>
<li>Applications of Rhetorical Structure Theory</li>
<li>Text summarization for Malayalam documents - An experience</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ The difference to the scores reported in Cohan et al. (2020) is due to the difference in the underlying training data.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
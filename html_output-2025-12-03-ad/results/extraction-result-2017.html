<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2017 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2017</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2017</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-47.html">extraction-schema-47</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <p><strong>Paper ID:</strong> paper-277501774</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.01445v2.pdf" target="_blank">Compositional-ARC: Assessing Systematic Generalization in Abstract Spatial Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Systematic generalization refers to the capacity to understand and generate novel combinations from known components. Despite recent progress by large language models (LLMs) across various domains, these models often fail to extend their knowledge to novel compositional scenarios, revealing notable limitations in systematic generalization. There has been an ongoing debate about whether neural networks possess the capacity for systematic generalization, with recent studies suggesting that meta-learning approaches designed for compositionality can significantly enhance this ability. However, these insights have largely been confined to linguistic problems, leaving their applicability to other tasks an open question. In this study, we extend meta-learning for compositionality to the domain of abstract spatial reasoning. To this end, we introduce $\textit{Compositional-ARC}-$a dataset designed to evaluate the capacity of models to systematically generalize from known geometric transformations (e.g., translation, rotation) of abstract two-dimensional objects to novel combinations of these transformations (e.g., translation+rotation). Our results show that a small transformer-based encoder-decoder model, trained via meta-learning for compositionality, can systematically generalize to previously unseen transformation compositions. Notably, despite having only 5.7M parameters, this model significantly outperforms state-of-the-art LLMs$-$including o3-mini, GPT-4o, and Gemini 2.0 Flash, which fail to exhibit similar systematic behavior$-$and performs on par with the winning model of the ARC prize 2024, an 8B-parameter LLM trained via test-time training. Our findings highlight the effectiveness of meta-learning in promoting systematicity beyond linguistic tasks, suggesting a promising direction toward more robust and generalizable models.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2017.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2017.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLC-Compositional-ARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Learning for Compositionality applied to Compositional-ARC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer encoder-decoder (5.7M parameters) trained via meta-learning on dynamically changing visual interpretation grammars (Compositional-ARC) to test systematic generalization from primitive geometric transformations and level-1 compositions to unseen level-2 compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>abstract spatial reasoning on 10x10 grid tasks (ARC-style)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Transformer encoder-decoder trained via Meta-Learning for Compositionality (MLC)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>Episodes present study examples that progress from primitive mappings to level-1 compositions; systematicity evaluation provides primitives + level-1 examples and requires generalization to unseen level-2 (deeper) compositions. Training uses dynamically changing interpretation grammars (no single fixed mapping across dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Primitive transformations are included within each episode: for the 'Systematicity' setup each episode provides 6 primitive examples (two per indicator). Primitives are not trained separately as isolated pre-training; they are presented as part of the episode few-shot study set. The dataset contains five basic geometric primitive types and three indicator types (shape, color, neighbor).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Up to 3 primitives composed (level-2 compositions combine three indicators/primitives; level-1 combine two indicators).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Large diversity via 100,000 generated episodes (training split ~82,908 episodes) with uniformly sampled transformation triplets mapping indicators to transformations; per-episode random assignment of specific shapes, colors, and indicator objects; multiple spatial placements and valid-transformation checks ensure varied contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>High performance on setups where target composition examples are present in the study examples: exact-match accuracy on the 3-shot (examples include final composition) was 99.92% (single split, Table 1) and averaged 98.78% ± 1.99% across four data splits (Table 2). Color/shape accuracies ~100%/~99%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Performance on novel (out-of-distribution) level-2 compositions: single-split reported exact-match 78.26% (Table 1); averaged across four independently generated splits: 86.73% ± 6.03% exact-match (Table 2). Color/shape accuracies for systematicity were ~99.36% and ~87.55% (mean across splits).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Approximate gap between near-training compositions (3-shot) and novel level-2 compositions: ~99.92% -> ~86.73% (mean) = ~13.2 percentage points (single-split example shows 99.92% -> 78.26% = ~21.7 pp). The paper also reports a baseline seq2seq gap of >99% train vs 0% OOD (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Depth sensitivity observed: presence of intermediate (level-1) compositions substantially aids generalization; removing intermediate compositions causes a severe drop (see ablation: exact-match from 86.73% ±6.03% -> 21.01% ±19.07%), indicating a large, non-linear degradation when models must directly compose three primitives without intermediate composition practice.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Compared to a basic seq2seq trained on static grammar (no episodic MLC curriculum): seq2seq fits training (>99% accuracy) but fails to generalize to OOD level-2 compositions (0.0% test), while MLC achieves high OOD generalization (86.73% mean). This shows the MLC curriculum greatly outperforms static end-to-end training for compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Meta-learning over episodes with dynamically varying interpretation grammars (a curriculum presenting primitives and level-1 compositions within episodes) enables strong systematic generalization to unseen level-2 compositions (mean exact-match ~86.7%), whereas standard static seq2seq training fails (0% OOD). Auxiliary objectives (copy task) and examples of intermediate compositions are critical: removing the copy task or removing level-1 examples substantially reduces OOD performance (copy task ablation: 86.73% -> 69.05%; no level-1: 86.73% -> 21.01%).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - provides empirical evidence that a curriculum-like meta-learning procedure and exposure to primitives plus intermediate compositions improve compositional generalization, and that naive static training yields a large generalization gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2017.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2017.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablations-MLC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation studies on MLC (copy task, primitives, level-1 compositions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic ablations quantify the contribution of auxiliary objectives and curriculum components: removal of the auxiliary copy task, primitive examples, or intermediate (level-1) compositions and their effect on systematic generalization performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>abstract spatial reasoning (Compositional-ARC), systematicity task</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Same MLC transformer encoder-decoder; ablated variants</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>Same episodic presentation but with specific components removed per ablation: (i) no auxiliary copy task, (ii) no primitive transformations in study examples, (iii) no level-1 transformation compositions in study examples.</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Ablations directly remove primitives or level-1 examples from the episode study sets rather than changing separate pretraining; primitives in normal setup are 6 per systematicity episode (two per indicator).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Level-2 (3-primitives) generalization targeted in all conditions; ablations test effect of removing lower-depth exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Same episode generation but with constrained study-example types according to ablation (e.g., no primitives or no level-1), so compositional diversity reduced for that ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Not separately reported per ablation for 'trained' compositions; ablations primarily reported for OOD systematicity performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Reported exact-match (mean ± std across splits): - no copy task: 69.05% ± 9.23%; - no primitives: 75.27% ± 12.95%; - no level-1 compositions: 21.01% ± 19.07%. Baseline (full MLC systematicity mean): 86.73% ± 6.03%.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Ablations quantify gap impact: removing level-1 compositions causes an absolute drop of ~65.7 pp (86.73% -> 21.01%) in exact-match OOD performance (mean values).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Ablations show that the ability to compose deeper (three primitives) relies strongly on exposure to shallower intermediate compositions; without level-1 practice performance collapses, indicating super-linear difficulty increase with depth when intermediate practice is absent.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Ablated curricula (especially 'no level-1') perform much worse than the full MLC curriculum (21.01% vs 86.73% mean), demonstrating the curriculum components are causal contributors to improved generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The auxiliary copy objective and inclusion of primitives and level-1 compositions are important for systematic generalization: removing the copy task or primitives moderately reduces OOD performance, while removing intermediate (level-1) examples produces a catastrophic drop, implying that practicing intermediate compositions is essential to generalize to deeper composed operations.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - indicates that curriculum components (intermediate compositions and auxiliary training signals) materially improve compositional generalization and mitigate the generalization gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2017.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2017.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static-Seq2Seq-Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Basic sequence-to-sequence model trained on static visual grammar (no episodic/meta-learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer encoder-decoder trained on a fixed visual grammar (static mapping between indicators and transformations) using individual input-output pairs; fits training but fails to generalize to unseen level-2 compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>abstract spatial reasoning (Compositional-ARC-like controlled dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Transformer encoder-decoder trained on static mapping (basic seq2seq)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>No curriculum; dataset contains independent input-output pairs with a fixed grammar across the dataset (no episodes/few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Training comprised 1,300 grid pairs including primitives and compositions but under a single fixed mapping; no episodes or few-shot study-query structure.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Included level-2 compositions in the test set that were not observed during training (i.e., OOD combinations of primitives), but training did include some level-2 compositions consistent with the fixed grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Limited: 1,300 static examples with a fixed mapping and less variability than the episodic MLC generation procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Fits training data with >99% accuracy (training fit reported >99%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Generalization to out-of-distribution level-2 compositions: 0.0% exact-match accuracy on OOD test (explicitly reported).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Training >99% vs OOD test 0.0% → an absolute generalization gap of ~99+ percentage points, demonstrating near-total failure to generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Shows inability to generalize to deeper compositions when trained on static mappings; suggests that standard end-to-end training does not acquire composition rules transferable to novel compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Direct comparison: static seq2seq (0% OOD) vs MLC (86.73% mean OOD) demonstrates the substantial benefit of the episodic meta-learning curriculum over static training.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>A standard seq2seq model trained on a static mapping achieves near-perfect training accuracy but zero OOD generalization to new compositions, highlighting that generic supervised training without episodic/curriculum structure does not produce systematic compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - underscores the claim that curriculum/meta-learning approaches are necessary to close the compositional generalization gap, and that naive end-to-end training produces a large gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2017.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2017.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Comparisons-Systematicity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance of general-purpose and domain-specific LLMs on Compositional-ARC systematicity and 3-shot tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of several LLMs (GPT-4o, Gemini 2.0 Flash, o3-mini) and domain/adapted models (Llama-3.2-3B-ReARC, Mistral-NeMO-Minitron-8B-Full) showing that general-purpose LLMs largely fail at systematic compositional generalization while domain-specific and meta-learned small MLC model succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>abstract spatial reasoning (Compositional-ARC) prompted in text-only and multimodal variants</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>GPT-4o, Gemini 2.0 Flash, o3-mini (general-purpose); Llama-3.2-3B-ReARC and Mistral-NeMO-Minitron-8B-Full (ARC-specialized domain models)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>No curriculum applied for general-purpose LLMs in these evaluations; they were prompted in few-shot or systematicity configurations (text-only or text+image). Domain-specific models use fine-tuning and data-augmentation techniques (and optional test-time training) described by their authors.</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Evaluated on the same tasks: 3-shot (final composition shown) and Systematicity (primitive + level-1 study examples, test on unseen level-2 compositions).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Models were evaluated across the Compositional-ARC test episodes (8,546 episodes per test split); domain models use augmented inference and training corpora from ARC-like datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>3-Shot exact-match accuracies (single-split): GPT-4o 22.28%, Gemini 2.0 Flash 30.08%, o3-mini (low) 64.04%; domain-specific: Llama-3.2-3B-ReARC 85.85%, Mistral-NeMO-Minitron-8B-Full 95.71%; MLC 99.92%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Systematicity exact-match (single-split): GPT-4o 0.99%, Gemini 2.0 Flash 2.66%, o3-mini 0.53%; domain-specific with test-time training: Llama-3.2-3B-ReARC + TTT 73.70% (or 99.10% depending on reported variant in table), Mistral-NeMO + TTT 78.20%; MLC 78.26% (single-split) / mean 86.73% across splits.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>General-purpose LLMs show very large gaps between their ability to use direct examples (3-shot) and to infer novel compositions: e.g., GPT-4o 3-shot 22.28% → systematicity 0.99% (drop ~21.3 pp); o3-mini 3-shot 64.04% → systematicity 0.53% (drop ~63.5 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>General-purpose LLMs perform especially poorly when required to compose primitives into novel level-2 combinations; domain-adapted models with ARC-specific tuning and TTT fare better, but still may require test-time adaptation to approach MLC performance.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Domain-specialized models with training/augmentation and test-time training substantially outperform general-purpose LLMs on systematicity; MLC (meta-learning curriculum) outperforms general-purpose LLMs and matches or exceeds domain-specific models without test-time training.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>General-purpose LLMs largely fail at compositional generalization in this visual abstract reasoning domain; domain-adapted models and meta-learning curricula close the gap. Test-time training and domain-specific augmentations help but MLC achieves strong OOD generalization with a small model and without test-time training.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>nuances - shows that large general-purpose models do not automatically achieve systematic compositionality, and that specialized curricula or domain adaptations (including meta-learning) are effective remedies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human-like systematic generalization through a metalearning neural network <em>(Rating: 2)</em></li>
                <li>Compositional generalization through meta sequence-to-sequence learning <em>(Rating: 2)</em></li>
                <li>The llm architect: Solving the arc challenge is a matter of perspective <em>(Rating: 2)</em></li>
                <li>COGS: A compositional generalization challenge based on semantic interpretation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2017",
    "paper_id": "paper-277501774",
    "extraction_schema_id": "extraction-schema-47",
    "extracted_data": [
        {
            "name_short": "MLC-Compositional-ARC",
            "name_full": "Meta-Learning for Compositionality applied to Compositional-ARC",
            "brief_description": "A transformer encoder-decoder (5.7M parameters) trained via meta-learning on dynamically changing visual interpretation grammars (Compositional-ARC) to test systematic generalization from primitive geometric transformations and level-1 compositions to unseen level-2 compositions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "abstract spatial reasoning on 10x10 grid tasks (ARC-style)",
            "agent_or_model_name": "Transformer encoder-decoder trained via Meta-Learning for Compositionality (MLC)",
            "curriculum_structure": "Episodes present study examples that progress from primitive mappings to level-1 compositions; systematicity evaluation provides primitives + level-1 examples and requires generalization to unseen level-2 (deeper) compositions. Training uses dynamically changing interpretation grammars (no single fixed mapping across dataset).",
            "primitive_training_details": "Primitive transformations are included within each episode: for the 'Systematicity' setup each episode provides 6 primitive examples (two per indicator). Primitives are not trained separately as isolated pre-training; they are presented as part of the episode few-shot study set. The dataset contains five basic geometric primitive types and three indicator types (shape, color, neighbor).",
            "composition_depth_range": "Up to 3 primitives composed (level-2 compositions combine three indicators/primitives; level-1 combine two indicators).",
            "compositional_diversity_description": "Large diversity via 100,000 generated episodes (training split ~82,908 episodes) with uniformly sampled transformation triplets mapping indicators to transformations; per-episode random assignment of specific shapes, colors, and indicator objects; multiple spatial placements and valid-transformation checks ensure varied contexts.",
            "performance_trained_compositions": "High performance on setups where target composition examples are present in the study examples: exact-match accuracy on the 3-shot (examples include final composition) was 99.92% (single split, Table 1) and averaged 98.78% ± 1.99% across four data splits (Table 2). Color/shape accuracies ~100%/~99%.",
            "performance_novel_compositions": "Performance on novel (out-of-distribution) level-2 compositions: single-split reported exact-match 78.26% (Table 1); averaged across four independently generated splits: 86.73% ± 6.03% exact-match (Table 2). Color/shape accuracies for systematicity were ~99.36% and ~87.55% (mean across splits).",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Approximate gap between near-training compositions (3-shot) and novel level-2 compositions: ~99.92% -&gt; ~86.73% (mean) = ~13.2 percentage points (single-split example shows 99.92% -&gt; 78.26% = ~21.7 pp). The paper also reports a baseline seq2seq gap of &gt;99% train vs 0% OOD (see separate entry).",
            "composition_depth_scaling": "Depth sensitivity observed: presence of intermediate (level-1) compositions substantially aids generalization; removing intermediate compositions causes a severe drop (see ablation: exact-match from 86.73% ±6.03% -&gt; 21.01% ±19.07%), indicating a large, non-linear degradation when models must directly compose three primitives without intermediate composition practice.",
            "curriculum_vs_baseline_comparison": "Compared to a basic seq2seq trained on static grammar (no episodic MLC curriculum): seq2seq fits training (&gt;99% accuracy) but fails to generalize to OOD level-2 compositions (0.0% test), while MLC achieves high OOD generalization (86.73% mean). This shows the MLC curriculum greatly outperforms static end-to-end training for compositional generalization.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Meta-learning over episodes with dynamically varying interpretation grammars (a curriculum presenting primitives and level-1 compositions within episodes) enables strong systematic generalization to unseen level-2 compositions (mean exact-match ~86.7%), whereas standard static seq2seq training fails (0% OOD). Auxiliary objectives (copy task) and examples of intermediate compositions are critical: removing the copy task or removing level-1 examples substantially reduces OOD performance (copy task ablation: 86.73% -&gt; 69.05%; no level-1: 86.73% -&gt; 21.01%).",
            "supports_or_challenges_theory": "supports - provides empirical evidence that a curriculum-like meta-learning procedure and exposure to primitives plus intermediate compositions improve compositional generalization, and that naive static training yields a large generalization gap.",
            "uuid": "e2017.0"
        },
        {
            "name_short": "Ablations-MLC",
            "name_full": "Ablation studies on MLC (copy task, primitives, level-1 compositions)",
            "brief_description": "Systematic ablations quantify the contribution of auxiliary objectives and curriculum components: removal of the auxiliary copy task, primitive examples, or intermediate (level-1) compositions and their effect on systematic generalization performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "abstract spatial reasoning (Compositional-ARC), systematicity task",
            "agent_or_model_name": "Same MLC transformer encoder-decoder; ablated variants",
            "curriculum_structure": "Same episodic presentation but with specific components removed per ablation: (i) no auxiliary copy task, (ii) no primitive transformations in study examples, (iii) no level-1 transformation compositions in study examples.",
            "primitive_training_details": "Ablations directly remove primitives or level-1 examples from the episode study sets rather than changing separate pretraining; primitives in normal setup are 6 per systematicity episode (two per indicator).",
            "composition_depth_range": "Level-2 (3-primitives) generalization targeted in all conditions; ablations test effect of removing lower-depth exposure.",
            "compositional_diversity_description": "Same episode generation but with constrained study-example types according to ablation (e.g., no primitives or no level-1), so compositional diversity reduced for that ablation.",
            "performance_trained_compositions": "Not separately reported per ablation for 'trained' compositions; ablations primarily reported for OOD systematicity performance.",
            "performance_novel_compositions": "Reported exact-match (mean ± std across splits): - no copy task: 69.05% ± 9.23%; - no primitives: 75.27% ± 12.95%; - no level-1 compositions: 21.01% ± 19.07%. Baseline (full MLC systematicity mean): 86.73% ± 6.03%.",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Ablations quantify gap impact: removing level-1 compositions causes an absolute drop of ~65.7 pp (86.73% -&gt; 21.01%) in exact-match OOD performance (mean values).",
            "composition_depth_scaling": "Ablations show that the ability to compose deeper (three primitives) relies strongly on exposure to shallower intermediate compositions; without level-1 practice performance collapses, indicating super-linear difficulty increase with depth when intermediate practice is absent.",
            "curriculum_vs_baseline_comparison": "Ablated curricula (especially 'no level-1') perform much worse than the full MLC curriculum (21.01% vs 86.73% mean), demonstrating the curriculum components are causal contributors to improved generalization.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": false,
            "key_findings_summary": "The auxiliary copy objective and inclusion of primitives and level-1 compositions are important for systematic generalization: removing the copy task or primitives moderately reduces OOD performance, while removing intermediate (level-1) examples produces a catastrophic drop, implying that practicing intermediate compositions is essential to generalize to deeper composed operations.",
            "supports_or_challenges_theory": "supports - indicates that curriculum components (intermediate compositions and auxiliary training signals) materially improve compositional generalization and mitigate the generalization gap.",
            "uuid": "e2017.1"
        },
        {
            "name_short": "Static-Seq2Seq-Baseline",
            "name_full": "Basic sequence-to-sequence model trained on static visual grammar (no episodic/meta-learning)",
            "brief_description": "A transformer encoder-decoder trained on a fixed visual grammar (static mapping between indicators and transformations) using individual input-output pairs; fits training but fails to generalize to unseen level-2 compositions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "abstract spatial reasoning (Compositional-ARC-like controlled dataset)",
            "agent_or_model_name": "Transformer encoder-decoder trained on static mapping (basic seq2seq)",
            "curriculum_structure": "No curriculum; dataset contains independent input-output pairs with a fixed grammar across the dataset (no episodes/few-shot).",
            "primitive_training_details": "Training comprised 1,300 grid pairs including primitives and compositions but under a single fixed mapping; no episodes or few-shot study-query structure.",
            "composition_depth_range": "Included level-2 compositions in the test set that were not observed during training (i.e., OOD combinations of primitives), but training did include some level-2 compositions consistent with the fixed grammar.",
            "compositional_diversity_description": "Limited: 1,300 static examples with a fixed mapping and less variability than the episodic MLC generation procedure.",
            "performance_trained_compositions": "Fits training data with &gt;99% accuracy (training fit reported &gt;99%).",
            "performance_novel_compositions": "Generalization to out-of-distribution level-2 compositions: 0.0% exact-match accuracy on OOD test (explicitly reported).",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Training &gt;99% vs OOD test 0.0% → an absolute generalization gap of ~99+ percentage points, demonstrating near-total failure to generalize.",
            "composition_depth_scaling": "Shows inability to generalize to deeper compositions when trained on static mappings; suggests that standard end-to-end training does not acquire composition rules transferable to novel compositions.",
            "curriculum_vs_baseline_comparison": "Direct comparison: static seq2seq (0% OOD) vs MLC (86.73% mean OOD) demonstrates the substantial benefit of the episodic meta-learning curriculum over static training.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "A standard seq2seq model trained on a static mapping achieves near-perfect training accuracy but zero OOD generalization to new compositions, highlighting that generic supervised training without episodic/curriculum structure does not produce systematic compositional generalization.",
            "supports_or_challenges_theory": "supports - underscores the claim that curriculum/meta-learning approaches are necessary to close the compositional generalization gap, and that naive end-to-end training produces a large gap.",
            "uuid": "e2017.2"
        },
        {
            "name_short": "LLM-Comparisons-Systematicity",
            "name_full": "Performance of general-purpose and domain-specific LLMs on Compositional-ARC systematicity and 3-shot tasks",
            "brief_description": "Evaluation of several LLMs (GPT-4o, Gemini 2.0 Flash, o3-mini) and domain/adapted models (Llama-3.2-3B-ReARC, Mistral-NeMO-Minitron-8B-Full) showing that general-purpose LLMs largely fail at systematic compositional generalization while domain-specific and meta-learned small MLC model succeed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "abstract spatial reasoning (Compositional-ARC) prompted in text-only and multimodal variants",
            "agent_or_model_name": "GPT-4o, Gemini 2.0 Flash, o3-mini (general-purpose); Llama-3.2-3B-ReARC and Mistral-NeMO-Minitron-8B-Full (ARC-specialized domain models)",
            "curriculum_structure": "No curriculum applied for general-purpose LLMs in these evaluations; they were prompted in few-shot or systematicity configurations (text-only or text+image). Domain-specific models use fine-tuning and data-augmentation techniques (and optional test-time training) described by their authors.",
            "primitive_training_details": null,
            "composition_depth_range": "Evaluated on the same tasks: 3-shot (final composition shown) and Systematicity (primitive + level-1 study examples, test on unseen level-2 compositions).",
            "compositional_diversity_description": "Models were evaluated across the Compositional-ARC test episodes (8,546 episodes per test split); domain models use augmented inference and training corpora from ARC-like datasets.",
            "performance_trained_compositions": "3-Shot exact-match accuracies (single-split): GPT-4o 22.28%, Gemini 2.0 Flash 30.08%, o3-mini (low) 64.04%; domain-specific: Llama-3.2-3B-ReARC 85.85%, Mistral-NeMO-Minitron-8B-Full 95.71%; MLC 99.92%.",
            "performance_novel_compositions": "Systematicity exact-match (single-split): GPT-4o 0.99%, Gemini 2.0 Flash 2.66%, o3-mini 0.53%; domain-specific with test-time training: Llama-3.2-3B-ReARC + TTT 73.70% (or 99.10% depending on reported variant in table), Mistral-NeMO + TTT 78.20%; MLC 78.26% (single-split) / mean 86.73% across splits.",
            "generalization_gap_measured": true,
            "generalization_gap_value": "General-purpose LLMs show very large gaps between their ability to use direct examples (3-shot) and to infer novel compositions: e.g., GPT-4o 3-shot 22.28% → systematicity 0.99% (drop ~21.3 pp); o3-mini 3-shot 64.04% → systematicity 0.53% (drop ~63.5 pp).",
            "composition_depth_scaling": "General-purpose LLMs perform especially poorly when required to compose primitives into novel level-2 combinations; domain-adapted models with ARC-specific tuning and TTT fare better, but still may require test-time adaptation to approach MLC performance.",
            "curriculum_vs_baseline_comparison": "Domain-specialized models with training/augmentation and test-time training substantially outperform general-purpose LLMs on systematicity; MLC (meta-learning curriculum) outperforms general-purpose LLMs and matches or exceeds domain-specific models without test-time training.",
            "adaptive_curriculum_used": null,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "General-purpose LLMs largely fail at compositional generalization in this visual abstract reasoning domain; domain-adapted models and meta-learning curricula close the gap. Test-time training and domain-specific augmentations help but MLC achieves strong OOD generalization with a small model and without test-time training.",
            "supports_or_challenges_theory": "nuances - shows that large general-purpose models do not automatically achieve systematic compositionality, and that specialized curricula or domain adaptations (including meta-learning) are effective remedies.",
            "uuid": "e2017.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human-like systematic generalization through a metalearning neural network",
            "rating": 2
        },
        {
            "paper_title": "Compositional generalization through meta sequence-to-sequence learning",
            "rating": 2
        },
        {
            "paper_title": "The llm architect: Solving the arc challenge is a matter of perspective",
            "rating": 2
        },
        {
            "paper_title": "COGS: A compositional generalization challenge based on semantic interpretation",
            "rating": 1
        }
    ],
    "cost": 0.017908749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>COMPOSITIONAL-ARC: ASSESSING SYSTEMATIC GENERALIZATION IN ABSTRACT SPATIAL REASONING
25 Sep 2025</p>
<p>Philipp Mondorf p.mondorf@lmu.de 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>Shijia Zhou zhou.shijia@lmu.de 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>Monica Riedler 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Barbara Plank b.plank@lmu.de 
MaiNLP
Center for Information and Language Processing
LMU Munich
Germany</p>
<p>Munich Center for Machine Learning (MCML)
MunichGermany</p>
<p>COMPOSITIONAL-ARC: ASSESSING SYSTEMATIC GENERALIZATION IN ABSTRACT SPATIAL REASONING
25 Sep 2025EAF312043528FE02F14CEEE2B0CD0A09arXiv:2504.01445v2[cs.AI]
Systematic generalization refers to the capacity to understand and generate novel combinations from known components.Despite recent progress by large language models (LLMs) across various domains, these models often fail to extend their knowledge to novel compositional scenarios, revealing notable limitations in systematic generalization.There has been an ongoing debate about whether neural networks possess the capacity for systematic generalization, with recent studies suggesting that meta-learning approaches designed for compositionality can significantly enhance this ability.However, these insights have largely been confined to linguistic problems, leaving their applicability to other tasks an open question.In this study, we extend meta-learning for compositionality to the domain of abstract spatial reasoning.To this end, we introduce Compositional-ARC-a dataset designed to evaluate the capacity of models to systematically generalize from known geometric transformations (e.g., translation, rotation) of abstract two-dimensional objects to novel combinations of these transformations (e.g., translation+rotation).Our results show that a small transformer-based encoder-decoder model, trained via meta-learning for compositionality, can systematically generalize to previously unseen transformation compositions.Notably, despite having only 5.7M parameters, this model significantly outperforms state-of-the-art LLMs-including o3-mini, GPT-4o, and Gemini 2.0 Flash, which fail to exhibit similar systematic behavior-and performs on par with the winning model of the ARC prize 2024, an 8B-parameter LLM trained via test-time training.Our findings highlight the effectiveness of meta-learning in promoting systematicity beyond linguistic tasks, suggesting a promising direction toward more robust and generalizable models.</p>
<p>INTRODUCTION</p>
<p>A fundamental aspect of human cognition is the ability to systematically generalize from known components to novel combinations (Marcus, 2003;Lake et al., 2017).This capacity is particularly evident in language, where an infinite number of new sentences can be constructed and interpreted by extracting meaning from previously acquired expressions and rules (Chomsky, 2002;Szabó, 2012).Similarly, our spatial perception relies on systematic generalization, enabling individuals to compose learned spatial principles into novel configurations (Zhou et al., 2024;Dautriche &amp; Chemla, 2025).For instance, once a person understands how to translate and rotate an object, they can apply these transformations in combination-translating and rotating the object simultaneously-even if they have never encountered such a composed transformation before (Fife et al., 2019).</p>
<p>Despite its central role in human cognition, systematic generalization remains a significant challenge in artificial intelligence (Lake &amp; Baroni, 2018;Loula et al., 2018;Hupkes et al., 2020).While large language models have recently demonstrated notable progress across various domains (OpenAI, 2024;Guo et al., 2025), they often fail to combine acquired knowledge in novel scenarios, demonstrating notable difficulties with systematic generalization (Dziri et al., 2023;Ismayilzada et al., 2025;Gendron et al., 2024).The question of whether neural networks can achieve systematicity has been the subject of extensive debate (Fodor &amp; Pylyshyn, 1988;Brakel &amp; Frank, 2009;Calvo &amp; Symons, 2014, inter alia).Recent research by Lake &amp; Baroni (2023) demonstrates that a transformer- based encoder-decoder model, trained via meta-learning for compositionality (MLC), can achieve human-like systematic generalization in processing instructions expressed in a pseudolanguage.By training the model to combine basic units of pseudolanguage into novel sequences over a stream of dynamically changing grammars, Lake &amp; Baroni (2023) show that this model can effectively generalize to previously unseen compositions of language (see Section 2 for further details).While this approach presents a promising direction for addressing systematicity in neural networks, its applicability beyond linguistic contexts remains an open question.</p>
<p>In this study, we extend the MLC framework proposed by Lake &amp; Baroni (2023) to the domain of abstract spatial reasoning.Inspired by the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), we introduce Compositional-ARC-a new dataset for assessing systematic generalization in abstract spatial reasoning.Compositional-ARC presents examples of basic geometric transformations (e.g., translation, rotation) applied to abstract two-dimensional objects and tests generalization to previously unseen compositions (e.g., translation+rotation; see Figure 1).Using MLC, we train a small encoder-decoder model on samples from Compositional-ARC and demonstrate that it can systematically generalize to unseen transformation compositions.To the best of our knowledge, this is the first application of MLC to abstract spatial reasoning.In summary, our contributions are:</p>
<p>2 BACKGROUND: META-LEARNING FOR COMPOSITIONALITY When learning a new language, humans rely on their ability to recombine known words and expressions to interpret novel sentences (Chomsky et al., 1976;De Beule &amp; Bergen, 2006).For instance, someone who understands the meanings of "cats drink water" and "dogs like to play" will typically also understand the meanings of "dogs drink water" and "cats like to play" (Hinzen et al., 2012).</p>
<p>Whether language models possess a comparable degree of systematicity remains an open question, as current models, including large language models, still struggle with tests of systematic generalization (Ismayilzada et al., 2025;Dziri et al., 2023).To address these limitations, Lake &amp; Baroni (2023) propose meta-learning for compositionality (MLC), a framework designed to model human-like systematic generalization in learning pseudolanguage instructions.Through a series of experiments, the authors show that models trained via MLC can achieve levels of systematicity comparable to those of humans when interpreting previously unseen pseudolanguage inputs.</p>
<p>Task setup.In their study, Lake &amp; Baroni (2023) examine few-shot compositional tasks in which instructions, represented as sequences of pseudowords (e.g., "dax," "lug," "fep"), must be mapped to corresponding sequences of abstract symbols (see Figure 2 for an example).To understand the meaning of such instructions, an interpretation grammar needs to be deduced from a limited number of study examples.This grammar maps pseudowords to their symbolic representation through a set of compositional rewrite rules.For instance, if "dax" corresponds to a green circle, "dax fep" to three green circles, and "zup" to a red circle, then "zup fep" would denote three red circles.Importantly, the examples are designed to be highly systematic, progressing from primitive mappings to more complex compositions.The core challenge lies in the ability to generalize systematically, i.e., to reuse and combine components from the study examples (left side of Figure 2) to generate correct outputs for novel query instructions (right side of Figure 2).</p>
<p>Algorithmic approach.To achieve systematic generalization in the instruction-learning task, Lake &amp; Baroni ( 2023 After training the model over a set of 100,000 distinct interpretation grammars, it demonstrates the capacity to generalize to previously unseen instructions and grammars.For specific details regarding training procedures, we refer to the original paper (Lake &amp; Baroni, 2023).</p>
<p>While Lake &amp; Baroni (2023) also evaluate MLC on COGS (Kim &amp; Linzen, 2020) and SCAN (Lake &amp; Baroni, 2018), which test systematic lexical generalization to novel word combinations, their  experiments are confined to the linguistic domain.In the following section, we propose Compositional-ARC to show how MLC can be extended to support systematic generalization in abstract spatial reasoning, demonstrating its potential beyond linguistic tasks.</p>
<p>METHOD</p>
<p>COMPOSITIONAL-ARC</p>
<p>To test systematicity in abstract spatial reasoning, we leverage the closure property of combined geometric transformations, where the composition of two valid transformations-such as translation, rotation, and reflection-yields another valid geometric transformation (Brannan et al., 2011).</p>
<p>Drawing inspiration from the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), we design a task in which abstract objects, defined in a two-dimensional grid environment, are subjected to basic geometric transformations and their compositions (see Figure 1 for examples).We use fixed-size 10×10 grids, each of which can be represented as a two-dimensional array of integers, where different values correspond to distinct colors.We use integers from 0 to 9, with 0 denoting a black background and the remaining integers mapping to unique colors (see Appendix A.1 for more details).Objects are defined based on color connectivity; that is, each object comprises a group of connected cells sharing the same color.Connectivity is determined by the Moore neighborhood (Bays, 2010), meaning that cells are considered connected if they are directly or diagonally adjacent.Each grid contains either one or two objects.A transformation is represented as a pair of grids, with the input grid displaying the objects before, and the output grid showing them after the geometric transformation.</p>
<p>Each transformation affects only one of the objects in the grid.For example, in Figure 1a, a single L-shaped yellow object is translated one step downward.In Figure 1c, a square blue object in the bottom-right expands toward the neighboring top row.Objects never occlude one another nor extend beyond the boundaries of the 10 × 10 grids.</p>
<p>We limit our dataset to five basic geometric transformations and their compositions: i) translations, ii) rotations, iii) reflections, iv) extensions, and v) color changes.For our experiments, we further constrain the configurations of these transformations to establish a controlled setup.Translations are limited to movements of one cell to the right or one cell downward.Rotations are restricted to 90 degrees clockwise or counterclockwise around the top-left corner of the object.We consider horizontal and vertical reflections across the object's central axis.Extensions mean that the object grows in a certain direction, and are limited to neighboring cells either leftward or upward.Color changes are restricted to changing the object's color to either red or orange.For detailed definitions of each transformation, please refer to Appendix A.2.</p>
<p>To signal which objects undergo which transformations, we consider three types of indicators: i) shapebased transformations, which affect objects of a particular shape; ii) color-based transformations, which affect all objects of a specific color; and iii) neighbor-based transformations, where objects are transformed when a second, indicator object is present.For instance, in Figure 1, all L-shaped objects (similar to the object in Figure 1a) undergo a one-step downward translation.All green objects undergo a horizontal reflection, and any object sharing a grid with the gray diagonal object (e.g., as seen in Figure 1c) expands into the neighboring top row.This indicator-based approach enables the definition of transformation compositions.For example, objects that are both L-shaped and green undergo a one-step downward translation together with a horizontal reflection (see Figure 1d for an example).We also define different levels of composition: level 1 combines two indicators (e.g., when an object matches the indicated shape and color, but lacks a the proximity to a neighboring object, as illustrated in Figure 1d), while level 2 combines all three indicators, specifying the object's shape, color, and proximity to an indicator object (see Figure 1g).</p>
<p>To test systematicity, we present few-shot examples of primitive transformations and their level-1 compositions, and evaluate models on previously unseen level-2 compositions.For instance, in Figure 3, models are asked to infer the correct transformation for a previously unseen level-2 composition of indicators, given a set of 12 study examples illustrating primitive transformations and their level-1 compositions.Conceptually, our setup is similar to the few-shot compositional task introduced by Lake &amp; Baroni (2023) (see Section 2), but it replaces the lexical interpretation grammar with a visual interpretation grammar.Specifically, models need to infer which indicator maps to which transformation, and how to compose them to deduce the correct final transformation.For a detailed description of how we algorithmically generate dataset samples, please refer to Appendix A.3.  2  2 2 2  2 2   2 2 2  2 2 2  2   5  5   6  6 6   4 4  4  4 4  4   6  6 6   5  5 5 5   4  4 4 4   2  2  2  2 2 2  2 2   2 2 2  2 2 2  2   5 5  5 5   6  6 6   4 4 4  4 4  4 4 4  4</p>
<p>META-LEARNING FOR COMPOSITIONALITY IN ABSTRACT SPATIAL REASONING</p>
<p>To systematically generalize from known geometric transformations to previously unseen transformation compositions, we extend the meta-learning for compositionality (Lake &amp; Baroni, 2023) framework described in Section 2. As in the original MLC approach, we train a transformer-based encoder-decoder model on a dataset of dynamically changing interpretation grammars.However, instead of mapping pseudolinguistic instructions to sequences of abstract symbols, we consider a visual interpretation grammar that associates visual indicators (object shape, color, or proximity to an indicator object) with specific geometric transformations, as described in Section 3.1.An episode in Compositional-ARC is defined as a set of study examples that illustrate the underlying grammar, along with query inputs for which the correct outputs must be inferred.For instance, the episode in Figure 3  Encoding and positional embedding.Each episode is presented to the model as a sequence of input-output grid pairs (study examples), followed by a query input grid, for which the model must generate the corresponding output grid (see Figure 3).To encode the two-dimensional grids, we divide each 10 × 10 grid into 2 × 2 patches (left to right, top to bottom), yielding 25 patches per grid (Dosovitskiy et al., 2021).Each patch is mapped to a unique embedding vector.Since each grid cell can take integer values from 0 to 9, a 2 × 2 patch can yield up to 10,000 distinct configurations, resulting in 10,000 possible embedding vectors.Two special tokens, | and →, are introduced to mark the boundaries between study examples and the input-output grids, respectively.The decoder vocabulary comprises two additional tokens for the start and end of a sequence (SOS and EOS).To encode positional information, we use standard learnable 1D positional embeddings that capture the order of grid pairs, as well as a second set of learnable 2D positional embeddings applied to grid patches.These 2D embeddings are decomposed into separate row and column components, which are added to each patch embedding to capture two-dimensional spatial information.</p>
<p>Training procedure.The model is trained on a large set of episodes, each defined by a unique visual interpretation grammar.In each episode, the model is provided with a sequence of study examples and tasked with predicting the output grid for a given input query (see Figure 3).Following Lake &amp; Baroni (2023), we include an auxiliary copy task during training, in which the model must also reproduce the output grids of each study example.We employ a model with three layers each in the encoder and decoder, eight attention heads per layer, input and hidden embeddings of size 128, a feedforward hidden size of 768, and GELU (Hendrycks &amp; Gimpel, 2016)   We generate 100,000 episodes, each comprising three few-shot examples for the "3-Shot" task, 12 systematic study examples for the "Systematicity" setup, and ten query input-output grid pairs demonstrating the final level-2 transformation composition.Each episode is characterized by a unique visual interpretation grammar.For instance, in one episode, yellow objects are translated downward by a single cell, while in another, yellow objects are reflected horizontally.To train our encoder-decoder model via MLC, we split the data into 82,908 training, 8,546 validation and 8,546 test episodes.Importantly, the data splits are constructed such that the geometric transformations involved in the final query level-2 compositions differ between the training and evaluation sets.</p>
<p>For instance, while the model is trained on basic transformations and a series of transformation compositions (e.g., translation+rotation+reflection), it is tested out-of-distribution on compositions not seen during training (e.g., translation+rotation+extension).For comprehensive statistics of the dataset splits, please refer to Table 5 in the Appendix.</p>
<p>LARGE LANGUAGE MODELS</p>
<p>General-purpose LLMs.In addition to the model trained via MLC, we evaluate three state-of-theart general-purpose LLMs on the test set of our proposed dataset: o3-mini (low) (OpenAI, 2025), GPT-4o (Achiam et al., 2023), and Gemini 2.0 Flash (DeepMind, 2024).To textually prompt the models for a given episode, we represent grids as two-dimensional arrays, consistent with prior work (Moskvichev et al., 2023).We also test a multimodal setup in which both an image of the study examples and the input query are provided alongside the text prompt.Domain-specific LLMs.We further consider two LLMs specifically tailored to ARC-style data: (i) Llama-3.2-3B-ReARC,fine-tuned on the re-ARC dataset (Hodel, 2024)-an extension of 1,000 additional generated examples per sample in ARC-and (ii) Mistral-NeMO-Minitron-8B-Full, trained on a broad range of ARC-style data, including re-ARC, Concept-ARC (Moskvichev et al., 2023), and ARC-Heavy (Li et al., 2025).These models were proposed by Franzen et al. (2024) and placed 1st in the ARC prize 2024. 1 Note that in addition to fine-tuning, these models use an ARC-customized tokenizer, extensive data augmentation during training and inference, a generation procedure that leverages depth-first search to produce multiple solution candidates, and a refined candidate-selection step.The authors also employ test-time training (TTT), which further fine-tunes models on the fewshot input-output grid pairs from the final test set.We use both models with their default parameters.</p>
<p>For additional details, please refer to the original paper (Franzen et al., 2024) or Appendix C.2.</p>
<p>EVALUATION METRICS</p>
<p>To evaluate the quality of the generated output grids, we use three different metrics: i) exact match accuracy, ii) color accuracy, and iii) shape accuracy.Exact match accuracy requires that a prediction is accurate only if every cell matches the target grid.Color accuracy checks whether predicted objects match target colors, ignoring shape and location.Shape accuracy checks whether predicted objects match target shapes, ignoring color and location.Formal definitions are provided in Appendix C.1.</p>
<p>RESULTS</p>
<p>In Table 1, we report the performance of the model trained via MLC, alongside the LLMs we evaluate on the two task setups, as described in Section 4.1.</p>
<p>Standard few-shot learning task.We begin by examining model performance on the "3-Shot" task, where models are given three input-output examples illustrating the final transformation composition (see Figure 4 in the Appendix).Despite this guidance and the relatively simple transformations involved, general-purpose LLMs such as GPT-4o and Gemini 2.0 Flash struggle with the task: GPT-4o reaches an accuracy of only 22.28%, while Gemini 2.0 Flash performs slightly better at 30.08%.The long-chain-of-thought model o3-mini achieves a modest accuracy of 64.04%.In contrast, domain-specific models such as Llama-3.2-3B-ReARC, and Mistral-NeMO-Minitron-8B-Full perform significantly better.While Llama-3.2-3B-ReARC achieves an accuracy of 85.85%, Mistral-NeMO-Minitron-8B-Full reaches up to 95.71%.Note that we do not employ test-time training in this setup, as it would contradict the out-of-distribution test setup described in Section 4.1.Notably, the 5.7M-parameter encoder-decoder model trained via MLC outperforms both general-purpose and domain-specific LLMs, with an accuracy of 99.92%, despite having only a fraction of the parameters.</p>
<p>We further find that all models predict object color nearly perfectly.For GPT-4o and Gemini 2.0 Flash, we observe that shape accuracy is significantly higher than exact match accuracy.This discrepancy suggests that while these models are often able to predict the correct shape and color of an object, they frequently fail to accurately predict its final position.Interestingly, both models show lower accuracy when visual input is added to the textual prompt, likely due to modality alignment challenges (Masry et al., 2025) or limitations in leveraging the visual content for reasoning.</p>
<p>Systematicity task.In the "Systematicity" task, models are asked to infer the correct final transformation composition from a set of study examples that represent more basic, decomposed transformations (see Figure 3 for an example).As shown in Table 1, all general-purpose LLMs perform poorly on this task.For instance, GPT-4o achieves an accuracy of 0.99%, while Gemini 2.0 Flash reaches 2.66%.Interestingly, o3-mini, the best-performing general-purpose model on the "3-Shot" task, performs worst in this setting, with an accuracy of only 0.53%.</p>
<p>CONSISTENCY ACROSS DATA SPLITS</p>
<p>To ensure that the strong performance of MLC, as reported in Table 1, is not the result of a favorable data split, we train and evaluate the model on three additional, independently generated data splits for each task configuration-resulting in four distinct models per task setup.Detailed descriptions of these data splits are provided in Table 5 in the Appendix.Table 2 summarizes the average accuracy and corresponding standard deviation across all four splits.For the standard three-shot learning task, MLC consistently achieves high accuracy, with a mean of 98.78% and a standard deviation of 1.99%.Similarly, for the systematicity task, the model demonstrates robust generalization, achieving an even higher average accuracy than on the initial data split, with a mean of 86.73%.</p>
<p>Ablation studies.To gain deeper insights into the factors influencing model performance, we conduct a series of ablation studies.First, we evaluate the impact of removing the auxiliary copy task from the training objective-a setup in which the model is trained not only to predict the output grid for a given input query but also to reproduce the output grid of each study example (refer to Section 3.2).Removing this auxiliary task results in a notable decrease in accuracy from 86.73% ± 6.03% to 69.05% ± 9.23%.This decline underscores the importance of the copy task in promoting systematic generalization, aligning with the findings of Lake &amp; Baroni (2023).Subsequently, we assess the role of study examples in model performance.Removing primitive transformations from the study examples (see Figure 3) results in a moderate reduction in performance, with an average accuracy of 75.27% ± 12.95%.This suggests that examples involving only level-1 transformation compositions are, to some extent, sufficient for allowing the model to generalize to more complex level-2 compositions.However, removing level-1 transformation compositions leads to a severe performance degradation, reducing accuracy to 21.01% ± 19.07%.We hypothesize that this is due to the increased complexity of composing three primitive operations directly into a level-2 transformation, as opposed to building on intermediate level-1 compositions.</p>
<p>In conclusion, our experiments highlight the potential of MLC beyond linguistic tasks, demonstrating its capacity for systematic generalization in abstract spatial reasoning.</p>
<p>RELATED WORK</p>
<p>Meta-learning.Meta-learning aims to improve a model's ability to adapt to novel tasks by leveraging experience over multiple training episodes (Thrun &amp; Pratt, 1998;Hospedales et al., 2022).It has been successfully applied to various tasks, such as few-shot learning (Mishra et al., 2018), continual learning (Javed &amp; White, 2019;Lee et al., 2023;Irie et al., 2025), and reinforcement learning (Duan et al., 2016;Wang et al., 2017;Mishra et al., 2018).Related to our work, meta-learning has been used to improve systematic generalization.Lake &amp; Baroni (2018) showed that traditional sequence-tosequence models struggle with compositional skills, but incorporating meta-learning can significantly improve performance (Lake, 2019;Conklin et al., 2021).Recent work argues that giving models the opportunity to practice skills via meta-learning is crucial for addressing challenges such as systematic generalization, among others (Irie et al., 2025).Our method builds on meta-learning strategies inspired by Lake &amp; Baroni (2023), extending them to the domain of abstract spatial reasoning.</p>
<p>ARC-like puzzles.The abstraction and reasoning corpus (ARC) (Chollet, 2019) is a benchmark designed to evaluate a model's capacity to generalize to novel scenarios with limited to no prior knowledge.Based on a set of few-shot examples, models are required to infer transformations of abstract objects or patterns within two-dimensional grids.Unlike ARC, which encompasses a broad range of complex transformations, our work deliberately narrows the scope to the five fundamental geometric transformations described in Section 3.1, focusing instead on the aspect of systematicity.Several ARC variants have been proposed, including 1D-ARC (Xu et al., 2024), Mini-ARC (Kim et al., 2022), ConceptARC (Moskvichev et al., 2023) and MC-LARC (Shin et al., 2024).However, to the best of our knowledge, Compositional-ARC is the first to focus on compositional generalization.</p>
<p>CONCLUSION</p>
<p>In this work, we extend the meta-learning for compositionality framework proposed by Lake &amp; Baroni (2023) to the domain of abstract spatial reasoning.</p>
<p>A DATASET</p>
<p>In this work, we present Compositional-ARC, a dataset designed to study systematicity in abstract spatial reasoning.As outlined in Section 3.1, Compositional-ARC evaluates a model's capacity to systematically generalize learned geometric transformations (e.g., translation, rotation) of twodimensional objects to novel compositions of these transformations (e.g., translation+rotation).The subsequent sections offer a detailed description of the dataset, including formal definitions of the grid-based environment and the set of transformations it includes.</p>
<p>A.1 GRID SETUP</p>
<p>We define the structure of the 10 × 10 grid environment and the notion of objects within it.Each grid is represented as a matrix X ∈ N 10×10 , where each element corresponds to a cell with a discrete color value.Objects are defined based on color connectivity using the Moore neighborhood (Bays, 2010).</p>
<p>Definition 1 (Grid &amp; Object).Let X ∈ N 10×10 be a matrix with rows i and columns j , referred to as a grid, where each element X ij ∈ {0, . . ., 9}.The value X ij = 0 represents a background cell, and values X ij ∈ {1, . . ., 9} represent object colors.</p>
<p>An object is a set of coordinates O ⊆ {0, . . ., 9} 2 such that each (i, j) ∈ O satisfies X ij = c, and the elements in O form a single connected component.</p>
<p>Two elements X ij and X kl are considered connected if:
max(|i − k|, |j − l|) ≤ 1
We define the following color mapping: 0 → black, 1 → red, 2 → orange, 3 → yellow, 4 → green, 5 → blue, 6 → purple, 7 → pink, 8 → cyan, and 9 → gray.</p>
<p>A.2 GEOMETRIC TRANSFORMATIONS</p>
<p>We formally define the five basic geometric transformations used in our dataset: translation, rotation, reflection, extension, and color change.Each transformation operates on objects within the grid environment as defined in Appendix A.1.A transformation is considered valid if all transformed coordinates lie within the grid bounds and do not overlap with existing objects in the original grid.</p>
<p>Translation.Moves an object by one cell along a specified direction (downward or rightward).A formal definition is given in the text box below.</p>
<p>Definition 2 (Translation).Let O ⊆ {0, . . ., 9} 2 be an object in a grid X ∈ N 10×10 , and let v = (v 1 , v 2 ) ∈ {(1, 0), (0, 1)} be the translation direction (downward or rightward).</p>
<p>The translated object is:
T trans,v (O) = {(i + v 1 , j + v 2 ) | (i, j) ∈ O}
The translation is valid if:
∀(i ′ , j ′ ) ∈ T trans,v (O), 0 ≤ i ′ , j ′ &lt; 10, X i ′ j ′ = 0
Rotation.Rotates an object 90 • clockwise or counterclockwise around the top-left of its bounding box.A more formal definition is given in the text box below.</p>
<p>Definition 3 (Rotation).Let O ⊆ {0, . . ., 9} 2 be a set of grid cells with row-column coordinates (i, j).Let i 0 = min (i,j)∈O i and j 0 = min (i,j)∈O j.We set the pivot P = (i 0 , j 0 ) as the top-left of the bounding box.</p>
<p>For each (i, j) ∈ O, we specify the offset from the pivot as:
(∆i, ∆j) = (i − i min , j − j min ).
We define a rotation by ±90 • as:
R +90 • (∆i, ∆j) = (∆j, −∆i), R −90 • (∆i, ∆j) = (−∆j, ∆i),
where +90 • is clockwise and −90 • is counterclockwise under the row-down convention.</p>
<p>Given a 90 • rotation, either clockwise or counterclockwise, the rotated object is:
T rot,±90 • (O) = ( i min + ∆i, j min + ∆j ) (i, j) ∈ O .
The rotation is valid if:
∀(i ′ , j ′ ) ∈ T rot,θ (O), 0 ≤ i ′ , j ′ &lt; 10, x i ′ j ′ = 0
Reflection.Reflects an object across its vertical or horizontal axis, reversing the relative positions of its coordinates while preserving overall structure.</p>
<p>Definition 4 (Reflection).Let O ⊆ {0, . . ., 9} 2 be an object in a grid X ∈ N 10×10 , and let d ∈ {horizontal, vertical} indicate the axis of reflection.</p>
<p>Let:
i min = min{i | (i, j) ∈ O}, i max = max{i | (i, j) ∈ O} j min = min{j | (i, j) ∈ O}, j max = max{j | (i, j) ∈ O}
Then the reflected object is:
T ref,d (O) = {(i max − (i − i min ), j) | (i, j) ∈ O} if d = horizontal {(i, j max − (j − j min )) | (i, j) ∈ O} if d = vertical
The reflection is valid if:
∀(i ′ , j ′ ) ∈ T ref,d (O), 0 ≤ i ′ , j ′ &lt; 10, X i ′ j ′ = 0
Extension.Adds a new cell in the upward or leftward direction for each coordinate in the object.</p>
<p>Definition 5 (Extension).Let O ⊆ {0, . . ., 9} 2 be an object in a grid X ∈ N 10×10 , with color c &gt; 0. Let d ∈ {up, left} indicate the extension direction.</p>
<p>Let the set of new cells adjacent to the object in direction d be:
N d (O) = {(i − 1, j) / ∈ O | (i, j) ∈ O, i &gt; 0, x i−1,j = 0} if d = up {(i, j − 1) / ∈ O | (i, j) ∈ O, j &gt; 0, x i,j−1 = 0} if d = left
Then the extended object is:
T ext,d (O) = O ∪ N d (O)
The extension is valid if:
∀(i ′ , j ′ ) ∈ N d (O), 0 ≤ i ′ , j ′ &lt; 10, , X i ′ j ′ = 0 All new cells (i ′ , j ′ ) ∈ N d (O)
are assigned the color of the original object:
X ′ i ′ j ′ = c
Color change.Alters the color of an object to either red or orange, without changing its structure or position.</p>
<p>Definition 6 (Color Change).Let O ⊆ {0, . . ., 9} 2 be an object in a grid X ∈ N 10×10 , with color c &gt; 0. Let c ′ ∈ {1, 2} be the new color (representing red or orange).</p>
<p>The resulting grid X ′ is given by:
X ′ ij = c ′ if (i, j) ∈ O X ij otherwise A.3 DATASET GENERATION
To generate episodes that comprise primitive transformations, level-1 transformation compositions, and level-2 transformation compositions, we developed a script that systematically generates the corresponding input-output grid pairs for each transformation.The complete code repository for data generation is publicly available at: https://github.com/mainlp/C-ARC.In the following, we provide a brief overview of the procedure used to generate input-output grid pairs for each sample within an episode.As detailed in Section 3.1 and Appendix A.2, we consider five basic geometric transformations, along with three types of transformation indicators: shape-based, color-based, and neighbor-based.These allow us to define a total of ten distinct transformation triplets, each mapping the indicators to corresponding transformations (e.g., shape-based: translation, color-based: reflection, neighbor-based: extension).For each episode, a transformation triplet is uniformly sampled from this set to define the visual interpretation grammar of the episode.Once the transformations are determined, we randomly assign a specific shape for the shape-based transformation, a specific color for the color-based transformation, and an indicator object for the neighbor-based transformation.Importantly, the indicator object is constrained to neither share the shape associated with the shapebased transformation nor the color linked to the color-based transformation.</p>
<p>Using these specifications, we generate input-output grid pairs representing primitive, level-1, and level-2 transformations.For each transformation mapping, we randomly place an object on a 10 × 10 grid, ensuring it possesses the designated shape, color, and/or proximity to the indicator object as required.The specified transformation is then applied to this object.If the resulting transformed object remains within the grid bounds and does not overlap with any other object, the corresponding input-output grid pair is accepted as a valid sample for the episode.Otherwise, a new object location is sampled and the process is repeated until a valid pair is obtained.Finally, we make sure that each</p>
<p>B TRAINING DETAILS</p>
<p>As outlined in Section 3.2, we use a transformer-based encoder-decoder model trained using MLC to predict the correct output grid for a given input query, given a set of study examples.Specifically, we generate a dataset of 100,000 episodes and split it into train, validation and test sets (for more information see Section 4.1 and Table 5).The model is optimized using cross-entropy loss, averaged over the predicted patch embeddings, as described in Section 3.2.To place greater emphasis on non-background regions, patches corresponding exclusively to black 2 × 2 cells are down-weighted by a factor of 0.2 during loss computation.</p>
<p>Each episode includes a collection of study examples and queries.In the standard few-shot learning task (Section 4.1), the model receives three input-output grid pairs, along with the input query.For the systematicity task, 12 systematic study examples are provided.In both tasks, the model is required to predict the correct output grid for ten distinct input queries.</p>
<p>Training is conducted over 200 epochs with a batch size of 200 for the standard few-shot learning task (i.e., 200 • 10 = 2000 queries per batch), and over 300 epochs with the same batch size for the systematicity task.A learning rate of 0.01 is used in both cases.Following the approach of Lake &amp; Baroni (2023), we apply a warm-up phase during the first episode, beginning with a learning rate of 1 × 10 −4 , followed by a linear decay to 5 × 10 −4 over the course of training.Additional hyperparameter settings are provided in Section B.1 and summarized in Table 3.  [2,3,4], and number of decoder layers ∈ 2, 3, 4.</p>
<p>For the hyperparamter search, the model is trained for 40 epochs on the systematicity task and evaluated on its corresponding validation set.Across 25 independent runs, we select the configuration that achieves the highest validation accuracy.The final hyperparameter settings, presented in Table 3, are employed consistently across both task setups.</p>
<p>B.2 IMPLEMENTATION DETAILS</p>
<p>All experiments were conducted using PyTorch (Paszke et al., 2019) as the primary development framework.Comprehensive details regarding supporting software and versioning are available in our code repository.Experiments were executed on NVIDIA A100 and H200 GPUs.Training models with MLC on the standard three-shot learning task over 200 epochs required approximately 40 GPU hours on a single A100 GPU.For the systematicity experiments with 12 study examples, training over 300 epochs on the designated dataset consumed roughly 100 GPU hours on a single H200 GPU.</p>
<p>C EXPERIMENT DETAILS</p>
<p>This section provide further details regarding our experimental setup.Specifically, Section C.1 presents formal definitions of the evaluation metrics used to assess the performance of the models studied in this work, while Section C.2 outlines additional information on how we interact with API-based LLMs.</p>
<p>C.1 EVALUATION METRICS</p>
<p>As described in Section 4.3, we use three different evaluation metrics to assess model performance in this study: i) exact match accuracy, ii) color accuracy, and iii) shape accuracy.These metrics are formally defined based on the grid-based environment X and the concept of an object O, as specified in Definition 1.</p>
<p>Let X target , X pred ∈ N 10×10 denote the target and predicted grids, respectively.Each cell X target ij (or X pred ij</p>
<p>) contains an integer in 0, . . ., 9, where 0 represents the background and values from 1 to 9 correspond to cells occupied by colored objects.The set of objects-defined as maximal connected cells of a consistent color under the Moore neighborhood (see Section 3.1)-extracted from X target and X pred are denoted P(X target ) and P(X pred ), respectively.For each object in grid O ∈ P(X), we assign a color label c(O) ∈ 1, . . ., 9 and define its normalized shape as follows:
S(O) = {(i − i min , j − j min ) : (i, j) ∈ O},(1)
where i min = min{i : (i, j) ∈ O} and j min = min{j : (i, j) ∈ O}.</p>
<p>(
)2
This transformation "anchors" the object to the top-left corner by translating it to a coordinate system with its minimum row and column indices set to zero.</p>
<p>Accuracy.The exact match accuracy evaluates whether the predicted grid X pred is identical to the target grid X target on a cell-by-cell basis:
Accuracy(X pred , X target ) = 1, if X pred ij = X target ij ∀ (i, j) ∈ {0, . . . , 9} 2 , 0, otherwise.(3)
In other words, this metric yields a value of 1 if and only if the entire predicted grid matches the target grid exactly, i.e., X target = X pred .The mean accuracy over the dataset D is then defined as:
Accuracy = 1 |D| (X pred ,X target )∈D Accuracy(X pred , X target )(4)
Color accuracy.Color accuracy assesses whether the predicted grid contains the same number of objects of each color as the target grid, irrespective of their locations or shapes.For a given color c ∈ 1, . . ., 9, let m(c, X) = {O ∈ P(X) : c(O) = c} .</p>
<p>(5) denote the number of objects of color c in grid X.Then, color accuracy is defined as:
Color Accuracy(X pred , X target ) = 1 ∀ c ∈ {1, . . . , 9} : m(c, X pred ) = m(c, X target ) , (6)
where 1• is the indicator function, returning 1 if the condition is satisfied for all colors and 0 otherwise.The mean color accuracy over the dataset D is given by:
Color Accuracy = 1 |D| (X pred ,X target )∈D
Color Accuracy(X pred , X target ) (7)</p>
<p>Shape accuracy.Shape accuracy measures the agreement in object shapes between the predicted and target grids, independent of color and position.For each object in a grid O ∈ P(X), we consider its normalized shape S(O) as defined in Equation 1.The count of objects with a specific normalized shape s in grid X is given by:
n(s, X) = {O ∈ P(X) : S(O) = s} . (8)
Accordingly, shape accuracy is defined as:
Shape Accuracy(X pred , X target ) = 1 ∀ s : n(s, X pred ) = n(s, X target ) .(9)
That is, the predicted grid X pred has perfect shape accuracy if the number of objects corresponding to each normalized shape is identical to that in the target grid X target .Finally, the mean shape accuracy over the dataset D is given by:
Shape Accuracy = 1 |D| (X pred ,X target )∈D
Shape Accuracy(X pred , X target ) (10)</p>
<p>C.2 MODEL INFORMATION</p>
<p>General-purpose LLMs.As described in Section 4.2, we evaluate three different generalpurpose LLMs on Compositional-ARC.Specifically, we assess the performance of o3mini (OpenAI, 2025) (version o3-mini-2025-01-312 ), GPT-4o (Achiam et al., 2023) (version gpt-4o-2024-08-063 ), and Gemini 2.0 Flash (DeepMind, 2024) (version gemini-2.0-flash-0014).All models are accessed via their respective batch APIs, enabling us to process multiple samples per request.Unless otherwise specified, we employ the default API settings.For GPT-4o and o3-mini, this corresponds to a temperature and top p value of 1.0.5 Due to financial constraints, the o3-mini model is configured with a "low" reasoning effort.For Gemini 2.0 Flash, the provider does not disclose default parameter settings.</p>
<p>Prompts.The complete set of prompts used in our evaluations is presented in Figures 11 through 14.</p>
<p>To ensure consistency and facilitate meaningful comparisons, we apply the same prompts across all models.The standard few-shot learning prompt appears in Figure 11, while the prompt used for the systematicity task is shown in Figure 13.For Gemini 2.0 Flash, we add the instruction: "Do not generate any code to solve the task" to the output requirements, as the model otherwise does not adhere to the required output format.As outlined in Section 4.2, we additionally evaluate GPT-4o and Gemini 2.0 Flash in a multimodal configuration, in which both an image of the study examples and the input query are provided alongside the text prompt (text+image).The multimodal prompt for the few-shot learning task is shown in Figure 12, with the accompanying image illustrated in Figure 9.The corresponding multimodal prompt for the systematicity task is depicted in Figure 14, with the associated image presented in Figure 10.For the textual prompts, we represent grids as two-dimensional arrays, consistent with prior work (Moskvichev et al., 2023)).For instance, the final query input grid in Figure 4 would be represented as:</p>
<p>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 0, 0, 0, 0, 0, 0, 0, 0], [5, 5, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0]] Model responses are parsed using regular expressions to identify the expression "output:", followed by a two-dimensional array of the form "[[. ..]]", as specified in the input prompt.If a response does not contain this pattern, it is excluded from further analysis and omitted from accuracy computations.Table 4 summarizes the proportion of valid responses for each model.</p>
<p>Domain-specific LLMs.As mentioned in Section 4.2, we also evaluate two LLMs proposed by Franzen et al. (2024) that are specifically tailored to ARC-style data: (i) Llama-3.2-3B-ReARC(version Llama-3.2-3B-ARChitects-ReArc-bnb-4bit 6 ) and (ii) Mistral-NeMO-Minitron-8B-Full (version Mistral-NeMo-Minitron-8B-ARChitects-Full-bnb-4bit 7 ).We use the original code 8 provided by the authors to run their models on Compositional-ARC, with default parameters.This means that the models perform augmented inference on the test set with rotations and transpositions over all symmetries, in addition to color permutations and example shuffling.Candidate pruning is further applied with a minimum probability of 0.1.For models evaluated with test-time training, we follow the authors' one-epoch LoRA adaptation on the study examples of the test data repeated 48 times with the same augmentations described before.LoRA targets the attention and MLP modules, as well as the embeddings, with r = 64, α = 16, and dropout set to 0. The models are trained with a batch size of 16, gradient accumulation set to 1, a cosine learning rate of 1 × 10 −4 (with 1 × 10 −5 for embeddings), and a warmup ratio of 0.25.The resulting weights are then used for inference with the same default settings as described earlier.</p>
<p>D ADDITIONAL RESULTS</p>
<p>In this section, we present additional results for the experiments conducted in this study.First, we present additional qualitative results related to the model predictions on the standard few-shot learning and the systematicity task.Figures 4 through 6 illustrate representative episodes from the standard few-shot learning task.Model predictions are shown adjacent to each query, with results for GPT-4o and Gemini 2.0 Flash corresponding to text-only prompts.Across all three episodes, the model trained using MLC consistently predicts the correct output grid.In contrast, GPT-4o and Gemini 2.0 Flash frequently fail to identify the correct transformation-either misrepresenting the shape of the transformed object or incorrectly predicting its final position.Notably, o3-mini successfully predicts the correct output for the episodes in Figures 5 and 6, but fails on the example in Figure 4.</p>
<p>Figures 7 and 8 highlight episodes from the systematicity task.As shown, all general-purpose LLMs fail to produce accurate transformations, often misplacing the transformed object within the grid.In contrast, the model trained via MLC consistently predicts the correct transformation.</p>
<p>Response rates.As outlined in Section C.2, the general-purpose LLMs we evaluate are instructed to present their final output grid predictions using the keyword "output:", followed by a two-dimensional array of size 10 × 10 in the format "[[. ..]]".Responses that do not conform to this expected pattern are excluded from subsequent analyses and are not included in accuracy calculations.Table 4 provides an overview of the proportion of valid responses for each model.In the standard few-shot learning setting, all models demonstrate very high valid response rates, exceeding 99%.However, in the systematicity task, a slight decrease in valid responses is observed for Gemini 2.0 Flash when additional visual input (text+image) is introduced, with the rate falling to 94.09%.More significantly, GPT-4o exhibits a notable drop in valid response rate to 77.24% under multimodal conditions.We hypothesize that this decline may be attributed to the increased context length resulting from the additional image input.</p>
<p>Training on static data.In addition to the model trained via MLC on a stream of dynamically changing visual interpretation grammars, as described in Section 3.2, we adopt the approach of Lake (2019) and train a transformer-based encoder-decoder on a dataset governed by a fixed visual grammar (referred to as basic seq2seq).This means that the indicator-transformation mappings are static across the whole dataset.For instance, if yellow object translates one step downward, then this applies to all data samples across the dataset.Instead of episodes with few-shot examples, this dataset comprises individual input-output grid pairs, where the objective is to predict the output grid corresponding to a given input grid.This more closely resembles a standard training approach.</p>
<p>We construct a dataset of 1,300 grid pairs, partitioned into</p>
<p>E USE OF AI ASSISTANTS</p>
<p>We used GitHub Copilot for parts of the project's code, and ChatGPT for minor language revisions.### Task Description: You must solve an abstract visual reasoning task by identifying geometric transformations (e.g., rotation, translation, color changes, etc.) applied to objects within a 10x10 grid.
1 1 1 2 2 1 1 1 2 2 1 1 1 2 2 1 1 2 1 2 1 1 1 1 1 2 2 1 1 1 2 1 2 Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid 1 1 1 2 2 1 1 1 1 2 2 Input Grid Output Grid Query Target 1 1 1 1 2 2 1 1 1 2 2 1 1 1 1 1 2 2 1 1 1 1 2 2 1 1 1 1 1 2 2Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid Input Grid Output Grid
To infer the correct geometric transformation, you are given a series of 3 pairs of input-output examples.Each example pair consists of:</p>
<p>• An input grid: a 10x10 list of lists (2d array), where each element is an integer (0-9).</p>
<p>• A corresponding output grid: a 10x10 list of lists (2d array) that has undergone a transformation based on a specific geometric rule.</p>
<p>For the prediction you need to understand the transformations displayed in the provided examples and apply them to the final input grid.</p>
<h3>Your Task:</h3>
<ol>
<li>
<p>Analyze the example pairs to infer the transformation rules applied to each input grid.</p>
</li>
<li>
<p>Identify how these transformations are applied to generate the output grids.</p>
</li>
<li>
<p>Apply the deduced transformations to the final input grid.4. Output the correctly transformed 10x10 grid.</p>
</li>
</ol>
<h3>Output Requirements:</h3>
<p>• Return only the final output grid.</p>
<p>• Do not include any extra text, explanations, or comments.</p>
<p>• The output must be formatted exactly as: 'output: [[...]]'</p>
<p>• The output grid must be a 10x10 list of lists containing only integers between 0 and 9 (inclusive).</p>
<p>• Do not include unnecessary line breaks or additional text beyond the specified format.</p>
<h3>Input Format: You will receive the following data: ### Task Description: You must solve an abstract visual reasoning task by identifying geometric transformations (e.g., rotation, translation, color changes, etc.) applied to objects within a 10x10 grid.</h3>
<p>To infer the correct geometric transformation, you are given a series of 3 pairs of input-output examples.Each example pair consists of:</p>
<p>• An input grid: a 10x10 list of lists (2d array), where each element is an integer (0-9).</p>
<p>• A corresponding output grid: a 10x10 list of lists (2d array) that has undergone a transformation based on a specific geometric rule.</p>
<p>For the prediction you need to understand the transformations displayed in the provided examples and apply them to the final input grid.</p>
<h3>Your Task:</h3>
<ol>
<li>
<p>Analyze the example pairs to infer the transformation rules applied to each input grid.</p>
</li>
<li>
<p>Identify how these transformations are applied to generate the output grids.</p>
</li>
<li>
<p>Apply the deduced transformations to the final input grid.4. Output the correctly transformed 10x10 grid.</p>
</li>
</ol>
<h3>Output Requirements:</h3>
<p>• Return only the final output grid.</p>
<p>• Do not include any extra text, explanations, or comments.</p>
<p>• The output must be formatted exactly as: 'output: [[...]]'</p>
<p>• The output grid must be a 10x10 list of lists containing only integers between 0 and 9 (inclusive).</p>
<p>• Do not include unnecessary line breaks or additional text beyond the specified format.</p>
<h3>Input Format: You will receive the following data: ### Task Description: You must solve an abstract visual reasoning task by identifying geometric transformations (e.g., rotation, translation, color changes, etc.) applied to objects within a 10x10 grid.</h3>
<p>To infer the correct geometric transformation, you are given a series of 12 pairs of input-output examples.Each example pair consists of:</p>
<p>• An input grid: a 10x10 list of lists (2d array), where each element is an integer (0-9).</p>
<p>• A corresponding output grid: a 10x10 list of lists (2d array) that has undergone a transformation based on a specific geometric rule.</p>
<p>The first 6 example pairs demonstrate primitive transformations based on the object's color, shape, or the presence of an additional object.For instance, objects of a certain color within the 10x10 input grid might undergo a translation, while objects of a certain shape (distinct numerical pattern) are being rotated.</p>
<p>The latter 6 example pairs involve composite transformations, meaning multiple transformations are applied simultaneously.For instance, for objects that have the appropriate color and shape, both a translation and rotation are applied simultaneously.</p>
<p>For the final prediction you need to understand and further combine the transformations displayed in the provided examples and apply them to the final input grid.</p>
<h3>Your Task:</h3>
<ol>
<li>
<p>Analyze the example pairs to infer the transformation rules applied to each input grid.</p>
</li>
<li>
<p>Identify how these transformations might combine to generate the output grids.</p>
</li>
<li>
<p>Apply the deduced transformations to the final input grid.4. Output the correctly transformed 10x10 grid.</p>
</li>
</ol>
<h3>Output Requirements:</h3>
<p>• Return only the final output grid.</p>
<p>• Do not include any extra text, explanations, or comments.</p>
<p>• The output must be formatted exactly as: 'output: [[...]]'</p>
<p>• The output grid must be a 10x10 list of lists containing only integers between 0 and 9 (inclusive).</p>
<p>• Do not include unnecessary line breaks or additional text beyond the specified format.</p>
<h3>Input Format: You will receive the following data: ### Task Description: You must solve an abstract visual reasoning task by identifying geometric transformations (e.g., rotation, translation, color changes, etc.) applied to objects within a 10x10 grid.</h3>
<p>To infer the correct geometric transformation, you are given a series of 12 pairs of input-output examples.Each example pair consists of:</p>
<p>• An input grid: a 10x10 list of lists (2d array), where each element is an integer (0-9).• A corresponding output grid: a 10x10 list of lists (2d array) that has undergone a transformation based on a specific geometric rule.</p>
<p>The first 6 example pairs demonstrate primitive transformations based on the object's color, shape, or the presence of an additional object.For instance, objects of a certain color within the 10x10 input grid might undergo a translation, while objects of a certain shape (distinct numerical pattern) are being rotated.</p>
<p>The latter 6 example pairs involve composite transformations, meaning multiple transformations are applied simultaneously.For instance, for objects that have the appropriate color and shape, both a translation and rotation are applied simultaneously.</p>
<p>For the final prediction you need to understand and further combine the transformations displayed in the provided examples and apply them to the final input grid.</p>
<h3>Your Task:</h3>
<ol>
<li>
<p>Analyze the example pairs to infer the transformation rules applied to each input grid.</p>
</li>
<li>
<p>Identify how these transformations might combine to generate the output grids.</p>
</li>
<li>
<p>Apply the deduced transformations to the final input grid.4. Output the correctly transformed 10x10 grid.</p>
</li>
</ol>
<h3>Output Requirements:</h3>
<p>• Return only the final output grid.</p>
<p>• Do not include any extra text, explanations, or comments.</p>
<p>• The output must be formatted exactly as: 'output: [[...]]'</p>
<p>• The output grid must be a 10x10 list of lists containing only integers between 0 and 9 (inclusive).</p>
<p>• Do not include unnecessary line breaks or additional text beyond the specified format.</p>
<h3>Input Format: You will receive the following data:</h3>
<p>Figure 1 :
1
Figure 1: A conceptual overview of the data in Compositional-ARC.Primitive transformations refer to basic geometric transformations (e.g., translation, reflection, extension) based on an object's (a) shape, (b) color, or (c) proximity to a neighboring object.Pairs of these indicators, such as (d) shape+color, (e) shape+neighbor, or (f) color+neighbor, can be combined to form level-1 transformation compositions.Finally, all three indicators can be combined to form level-2 transformation compositions, based on the object's (g) shape+color+neighbor.</p>
<p>) train a transformer-based encoder-decoder model through meta-learning for compositionality.The key idea is to train the model on a dataset of dynamically changing interpretation grammars, where the mappings from input sequences to output symbols differ across training samples.This forces the model to rely on the information conveyed in the study examples to infer the appropriate grammar of a given sample, rather than memorizing static input-output mappings across the dataset.This flexibility enables the model to adjust to novel scenarios governed by new sets of examples and rules.Moreover, the compositional structure of both study examples and queries encourages the model to internalize mechanisms for composing elements presented in the study examples.</p>
<p>Figure 2 :
2
Figure 2: An example of the few-shot instruction learning task adapted from Lake &amp; Baroni (2023).Study instructions illustrate the mapping of pseudolanguage expressions to abstract symbols.On the right, query instructions and their target responses are shown.</p>
<p>Figure 3 :
3
Figure 3: An episode from Compositional-ARC.Given a set of study examples with primitive transformations and level-1 transformation compositions, models must predict the output grid for a previously unseen level-2 transformation composition.Model predictions are presented to the right.</p>
<p>Figure 4
4
in the Appendix.This task evaluates the model's ability to infer geometric transformations from a limited set of examples.The second setup, denoted as "Systematicity," focuses on compositional generalization and differs from the first in the type of few-shot examples presented.As mentioned in Section 3.1, the idea is to test whether models can infer novel compositions from known geometric transformations.To this end, we replace the level-2 few-shot examples with a set of primitive transformations plus level-1 transformation compositions, and query the model to predict the previously unseen level-2 transformation composition, as illustrated in Figure3.Specifically, we present six primitive transformations-two examples for each indicator (shape-based, color-based, neighbor-based)-and six level-1 transformation compositions, two examples for each level-1 indicator composition (shape+color, shape+neighbor, color+neighbor).</p>
<p>B. 1 HYPERPARAMETERS
1
To identify suitable hyperparameters for model training, we conduct Bayesian search over a predefined range of values: learning rate ∈ [1 × 10 −2 , 1 × 10 −3 , 1 × 10 −4 ], final learning rate after linear decay ∈ [1 × 10 −4 , 5 × 10 −4 ], dropout rate ∈ [0.0, 0.1, 0.2], gradient accumulation over k ∈[1,2] batches, cell color perturbation probability p noise ∈ [0.0, 0.01, 0.001], feedforward hidden dimension ∈ [512, 768], loss weighting for background (all-black) patches ∈ [0.2, 0.4, 1.0], number of encoder layers ∈</p>
<p>Figure 4 :
4
Figure 4: An example of the few-shot learning task.Models are provided with three study examples that demonstrate the transformation that needs to be inferred for the final input grid.Model predictions are displayed to the right.</p>
<p>Figure 5 :
5
Figure 5: A second example of the few-shot learning task.Models are provided with three study examples that demonstrate the transformation that needs to be inferred for the final input grid.Model predictions are displayed to the right.</p>
<p>Figure 6 :
6
Figure 6: A third example of the few-shot learning task.Models are provided with three study examples that demonstrate the transformation that needs to be inferred for the final input grid.Model predictions are displayed to the right.</p>
<p>Figure 7 :
7
Figure 7: An episode from the systematicity task.Given a set of study examples comprising primitive transformations and level-1 transformation compositions, models are asked to predict the output grid for a previously unseen level-2 transformation composition.Predictions of different models are presented to the right.</p>
<p>Figure 9 :
9
Figure 9: An exemplary visual input used in the multimodal prompt for the 3-shot learning task.</p>
<p>Figure 10 :
10
Figure 10: An exemplary visual input used in the multimodal prompt for the systematicity task.</p>
<p>1 .
1
Figure 11: The prompt used for the few-shot experiment when instructing LLMs in (text-only) mode.Text enclosed in sharp brackets &lt; . . .&gt; is replaced by the actual examples.</p>
<p>1 .
1
Figure 12: The prompt used for the few-shot experiment when instructing LLMs in (text+image) mode.Text enclosed in sharp brackets &lt; . . .&gt; is replaced by the actual examples.Additionally, the model is provided with the image in Figure 9.</p>
<p>1 .
1
Figure 13: The prompt used for the systematicity experiment when instructing LLMs in (text-only) mode.Text enclosed in sharp brackets &lt; . . .&gt; is replaced by the actual examples.</p>
<p>1 .
1
Figure 14: The prompt used for the systematicity experiment when instructing LLMs in (text+image) mode.Text enclosed in sharp brackets &lt; . . .&gt; is replaced by the actual examples.Additionally, the model is provided with the image in Figure 10.</p>
<p>Table 1 :
1
Comparison of model performance across the two different task setups.We report exact match accuracy, color accuracy, and shape accuracy as described in Section 4.3).
ModelExact Match Accuracy [%]Color Accuracy [%]Shape Accuracy [%]GPT-4o22.2899.6757.02+ image19.4299.7554.56Gemini 2.0 Flash30.0899.9252.343-Shot+ image o3-mini (low)17.19 64.0499.79 99.8935.86 68.74Llama-3.2-3B-ReARC85.8598.5786.05Mistral-NeMO-Minitron-8B-Full95.7199.8596.78MLC (ours)99.92100.0099.92GPT-4o0.9999.239.82+ image0.8697.947.50SystematicityGemini 2.0 Flash + image o3-mini (low) Llama-3.2-3B-ReARC + test-time training2.66 2.05 0.53 0.87 73.7099.68 99.28 99.10 99.94 100.0012.81 9.60 5.65 2.54 86.88Mistral-NeMO-Minitron-8B-Full0.7099.999.75+ test-time training78.20100.0088.26MLC (ours)78.2697.8880.49
Due to financial constraints, each model is evaluated on a single test query for each of the 8,546 episodes in the test set.All textual and visual prompts, specific model versions, and decoding parameters are detailed in Appendix C.2.</p>
<p>Table 2 :
2
Average accuracy and standard deviation across the four different data splits.For the systematicity task, we ablate different components of the training procedure to assess their individual contributions and overall impact.
ModelExact Match Accuracy [%]Color Accuracy [%]Shape Accuracy [%]MLC (3-Shot)98.78 ± 1.99100.00 ± 0.0098.79 ± 1.98MLC (Systematicity)86.73 ± 6.0399.36 ± 0.7087.55 ± 5.45-no copy task69.05 ± 9.2399.43 ± 0.3870.60 ± 9.23-no primitives75.27 ± 12.9599.56 ± 0.5076.92 ± 11.23-no level-1 compositions21.01 ± 19.0794.72 ± 7.4123.03 ± 19.08</p>
<p>STATEMENTTo ensure the reproducibility of our work, we make all code publicly available at: https://github.com/mainlp/C-ARC.This enables users to reproduce the data described in Section 3.1 and train models via MLC for the task, as outlined in Section 3.2.Details about the training procedures and hyperparameters are provided in Section 3.2 and Appendix B. Specifics on prompts, model versions, and decoding parameters are given in Appendix C.2.Further details about the datasets can be found in Section 3.1, Section 4.1, and Appendix A. Finally, Appendix B.2 outlines the software and computational resources used for model training.</p>
<p>To this end, we introduce Compositional-ARC-a novel dataset designed to evaluate systematicity in this field.Our experiments demonstrate that models trained via MLC can systematically generalize to novel compositions of geometric transformations.Moreover, a small MLC model outperforms state-of-the-art general-purpose LLMs on Compositional-ARC, and performs on par with domain-specific LLMs trained via test-time training.Our findings suggest that MLC presents a promising direction for enabling systematic generalization in language models across diverse domains.REPRODUCIBILITY</p>
<p>Table 3 :
3
Hyperparameter configuration for models trained via MLC.
ParameterValueParameterValuenumber layers in decoder3learning rate after training5 × 10 −4number layers in decoder3dropout0.0number of attention heads 8weight decay0.01hidden dimension128noise probability0.001feedforward hidden size768gradient accumulation over k batches 2learning rate0.01background patch loss weight0.2episode follows a unique grammar, i.e., that no two combinations of shape, color, and indicatorobjects correspond to the same set of transformations within the dataset.A.4 DATASET STATISTICS</p>
<p>Table 5
5
presents detailed statistics for the datasets used in this study.As outlined in Section 5.1, we train and evaluate models via MLC across four distinct dataset splits to mitigate the influence of randomness in the data split process.The table includes the number of training, validation, and test samples for each split.Additionally, it provides information on the query transformation compositions present in the training and test sets, along with the frequency of each basic geometric transformation within the train dataset.</p>
<p>Table 4 :
4
The proportion of valid responses generated by the different models reported for the standard three-shot learning task and the systematicity task.For general-purpose LLMs, valid responses must contain the string "output:", followed by a two-dimensional 10 × 10 array of the form "[[. ..]]".
ModelValid Responses (3-Shot)Valid Responses (Systematicity)GPT-4o99.95%99.40%+ image99.80%77.24%Gemini 2.0 Flash99.92%99.74%+ image99.51%94.09%o3-mini (low)100%100%Llama-3.2-3B-ReARC100%100%+ test-time training-100%Mistral-NeMO-Minitron-8B-Full100%100%+ test-time training-100%MLC (ours)100%100%</p>
<p>1,260 training samples, 20 validation samples, and 20 test samples.Samples represent primitive transformations, as well as level-1 and level-2 transformation compositions.As with our other experiments, the test set includes level-2 transformation compositions that were not observed during training-only their constituent components and level-1 compositions were seen during training.For instance, the test set might include transformations composed of shape-based downward translation, color-based horizontal reflection, and neighbor-based upward extension.However, only their decomposed elements have been shown during training.
The model is trained for 200 epochs on the dataset using the parameters specified in Section B.While it successfully fits the training data (with an accuracy of over 99%), it fails to generalize tothe out-of-distribution test set, achieving a test accuracy of 0.0%. This demonstrates that traditionalmodel training, sample by sample, does not encourage systematic generalization to unseen composi-tions. Instead, systematicity requires a training procedure with examples over dynamically varyinginterpretation grammars, as described in Section 3.2.</p>
<p>Table 5 :
5
Summary of dataset statistics across different dataset splits, each determined by a distinct random seed.Listed are the number of episodes in the training, validation, and test sets.Additionally, the final query transformation compositions (level 2) are reported for both the training and evaluation datasets.The rightmost column details the frequency of each basic geometric transformation present in the training dataset.
Data SplitNo. EpisodesQuery TransformationsBasic TransformationsSetNo.Type CompositionTransformationFreq.Train 82908translation+reflection+coloringred coloring35828Val8546reflection+rotation+extensionorange coloring35819Test8546translation+reflection+rotationdown translation23398seed 1860Traintranslation+rotation+coloring reflection+coloring+extension reflection+rotation+coloringright translation leftward extension upward extension27021 22140 21806translation+coloring+extensioncw. rotation19551rotation+coloring+extensionccw. rotation19394Testtranslation+rotation+extension translation+reflection+extensionhorizontal reflection 21967 vertical reflection 21800Train 83481translation+rotation+extensionred coloring27603Val8259translation+reflection+rotationorange coloring27525Test8260reflection+rotation+extensiondown translation31385seed 1870Trainreflection+coloring+extension translation+reflection+extension translation+rotation+coloringright translation leftward extension upward extension36126 26501 25913translation+reflection+coloringcw. rotation15421translation+coloring+extensionccw. rotation15283Testrotation+coloring+extension reflection+rotation+coloringhorizontal reflection 22366 vertical reflection 22320Train 80035translation+coloring+extensionred coloring25850Val9982translation+rotation+extensionorange coloring25832Test9983translation+rotation+coloringdown translation31385seed 1880Trainreflection+rotation+extension translation+reflection+coloring translation+reflection+extensionright translation leftward extension upward extension36126 24821 24147translation+reflection+rotationcw. rotation19734rotation+coloring+extensionccw. rotation19594Testreflection+rotation+coloring reflection+coloring+extensionhorizontal reflection 16331 vertical reflection 16285Train 80557translation+coloring+extensionred coloring30227Val9721translation+reflection+rotationorange coloring30255Test9722rotation+coloring+extensiondown translation23279seed 1890Traintranslation+reflection+coloring reflection+rotation+extension translation+reflection+extensionright translation leftward extension upward extension24789 26483 26277reflection+coloring+extensioncw. rotation13949reflection+rotation+coloringccw. rotation13831Testtranslation+rotation+coloring translation+rotation+extensionhorizontal reflection 26329 vertical reflection 26252
https://arcprize.org/competitions/2024
https://platform.openai.com/docs/models/o3-mini
https://platform.openai.com/docs/models/gpt-4o
https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash
https://platform.openai.com/docs/api-reference/chat/create
https://huggingface.co/da-fr/Llama-3.2-3B-ARChitects-ReArc-bnb-4bit
https://huggingface.co/da-fr/Mistral-NeMo-Minitron-8B-ARChitects-Full-bnb-4bit
https://github.com/da-fr/arc-prize-2024
ACKNOWLEDGMENTSWe express our gratitude to the members of the MaiNLP lab for their invaluable feedback.Furthermore, we thank the anonymous reviewers for their insightful comments and suggestions.We gratefully acknowledge that experiments involving API calls to GPT-4o and o3-mini were supported by a compute grant from OpenAI.The authors also acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) under the NHR project b217dd.NHR funding is provided by federal and Bavarian state authorities.NHR@FAU hardware is partially funded by the German Research Foundation (DFG) -440719683.Finally, we acknowledge the support for BP through the ERC Consolidator Grant DIALECT 101043235.Query TargetInput Grid Output GridMLC (ours) Llama
We introduce Compositional-ARC-a novel dataset, inspired by ARC (Chollet, 2019), that evaluates systematic generalization in abstract spatial reasoning. The dataset includes examples of basic geometric transformations applied to abstract two-dimensional objects and tests generalization to unseen transformation compositions. see Figure 1</p>
<p>We demonstrate that MLC enables transformer-based models to generalize to unseen compositions of geometric transformations, demonstrating its potential beyond linguistic tasks. </p>
<p>We show that a 5.7M-parameter encoder-decoder model trained via MLC significantly outperforms state-of-the-art general-purpose LLMs such as o3-mini. Achiam, GPT-4o. OpenAI, 2025. 2023), and Gemini 2.0 Flash (DeepMind, 2024which fail to exhibit comparable systematic behavior on Compositional-ARC</p>
<p>We find that the same MLC model performs on par with the winning model of the ARC Prize 2024, an 8B-parameter LLM trained via test-time training. Franzen, 2024</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Introduction to Cellular Automata and Conway's Game of Life. Carter Bays, 10.1007/978-1-84996-217-9_12010SpringerLondon, London</p>
<p>Strong systematicity in sentence processing by simple recurrent networks. Philémon Brakel, Stefan Frank, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society200931</p>
<p>The architecture of cognition: Rethinking Fodor and Pylyshyn's systematicity challenge. Matthew F David A Brannan, Jeremy J Esplen, Gray, Geometry, Paco Calvo and John Symons. MIT Press2011. 2014</p>
<p>On the measure of intelligence. Chollet Franc, 2019</p>
<p>Noam Chomsky et al. Reflections on language. Noam Chomsky, 2002. 1976Mouton de GruyterTemple Smith LondonSyntactic structures</p>
<p>Meta-learning to compositionally generalize. Henry Conklin, Bailin Wang, Kenny Smith, Ivan Titov, doi: 10.18653Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>URL. </p>
<p>Evidence for compositional abilities in one-yearold infants. Isabelle Dautriche, Emmanuel Chemla, 10.1038/s44271-025-00222-9Communications Psychology. 2731-912131372025</p>
<p>On the emergence of compositionality. Joachim De, Beule , Benjamin K Bergen, The Evolution of Language. World Scientific2006</p>
<p>Gemini 2.0 flash. Google Deepmind, 2024</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 2021</p>
<p>Rl 2 : Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, Pieter Abbeel, arXiv:1611.027792016arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, ( Xiang, ) Lorraine, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>A learning progression for geometric transformations. H James, Kofi Fife, Malcolm James, Bauer, 10.1002/ets2.12236ETS Research Report Series. 201912019</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, org/10.1016/0010-0277(88)90031-5Cognition. 0010-02772811988</p>
<p>The llm architect: Solving the arc challenge is a matter of perspective. Jan Daniel Franzen, David Disselhoff, Hartmann, 2024</p>
<p>Large language models are not strong abstract reasoners. Gaël Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie, 10.24963/ijcai.2024/693Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI '24. the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI '242024</p>
<p>Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, 10.1038/s41586-025-09422-zNature. 1476-46876458081Sep 2025</p>
<p>Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). 2016arXiv preprint</p>
<p>The Oxford Handbook of Compositionality. Wolfram Hinzen, Edouard Machery, Markus Werning, 10.1093/oxfordhb/9780199541072.001.0001Oxford University Press</p>
<p>Addressing the abstraction and reasoning corpus via procedural example generation. Michael Hodel, arXiv:2404.073532024arXiv preprint</p>
<p>Meta-Learning in Neural Networks: A Survey. Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey, 10.1109/TPAMI.2021.3079209IEEE Transactions on Pattern Analysis &amp; Machine Intelligence. 1939-35394409September 2022</p>
<p>Compositionality decomposed: How do neural networks generalise. Dieuwke Hupkes, Verna Dankers, Mathijs Mul, Elia Bruni, Journal of Artificial Intelligence Research. 672020</p>
<p>Metalearning continual learning algorithms. Kazuki Irie, Róbert Csordás, Jürgen Schmidhuber, Transactions on Machine Learning Research. 2835-88562025</p>
<p>Evaluating morphological compositional generalization in large language models. Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Duygu Ataman, Lonneke Van Der, Plas, 10.18653/v1/2025.naacl-long.59Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Luis Chiruzzo, Alan Ritter, Lu Wang, the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language TechnologiesAlbuquerque, New MexicoAssociation for Computational LinguisticsApril 20251</p>
<p>Meta-learning representations for continual learning. Khurram Javed, Martha White, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>COGS: A compositional generalization challenge based on semantic interpretation. Najoung Kim, Tal Linzen, 10.18653/v1/2020.emnlp-main.731Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>Playgrounds for abstraction and reasoning. Subin Kim, Prin Phunyaphibarn, Donghyun Ahn, Sundong Kim, NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI). 2022</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. Brenden Lake, Marco Baroni, Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy, Andreas Krause, the 35th International Conference on Machine LearningPMLR10-15 Jul 201880</p>
<p>Compositional generalization through meta sequence-to-sequence learning. M Brenden, Lake, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Human-like systematic generalization through a metalearning neural network. M Brenden, Marco Lake, Baroni, 10.1038/s41586-023-06668-3Nature. 1476-468762379852023</p>
<p>Building machines that learn and think like people. M Brenden, Lake, D Tomer, Joshua B Ullman, Samuel J Tenenbaum, Gershman, 10.1017/S0140525X16001837Behavioral and Brain Sciences. 40e2532017</p>
<p>Recasting continual learning as sequence modeling. Soochan Lee, Jaehyeon Son, Gunhee Kim, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Combining induction and transduction for abstract reasoning. Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M Dunn, Hao Tang, Wei-Long Zheng, Yewen Pu, Kevin Ellis, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Rearranging the familiar: Testing compositional generalization in recurrent networks. João Loula, Marco Baroni, Brenden Lake, 10.18653/v1/W18-5413Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Tal Linzen, Grzegorz Chrupała, Afra Alishahi, the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsNovember 2018</p>
<p>Alignvlm: Bridging vision and language latent spaces for multimodal understanding. Marcus ; Gary, Ahmed Masry, Juan A Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Kalkunte Akshay, Abhay Suresh, Xiangru Puri, Pierre-André Jian, Sathwik Noël, Marco Tejaswi Madhusudhan, Bang Pedersoli, Nicolas Liu, Yoshua Chapados, Enamul Bengio, Hoque, Pal, Issam H. Laradji, David Vazquez, Perouz Taslakian, Spandana Gella, and Sai Rajeswar2003. 2025MIT pressThe algebraic mind: Integrating connectionism and cognitive science</p>
<p>A simple neural attentive metalearner. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel, International Conference on Learning Representations. 2018</p>
<p>The conceptARC benchmark: Evaluating understanding and generalization in the ARC domain. Arsenii Kirillovich Moskvichev, Victor Vikram Odouard, Melanie Mitchell, Transactions on Machine Learning Research. 2023</p>
<p>Openai o3-mini system card. Openai, 2024. January 2025Openai o1 system card</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>From generation to selection: Findings of converting analogical problem-solving into multiple-choice questions. Donghyeon Shin, Seungpil Lee, Klea , Lena Kovacec, Sundong Kim, 10.18653/v1/2024.findings-emnlp.392Findings of the Association for Computational Linguistics: EMNLP 2024. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, Miami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>The case for compositionality. Zoltán Gendler, Szabó , The Oxford Handbook of Compositionality. Markus Werning, Wolfram Hinzen, Edouard Machery, Oxford University Press2012</p>
<p>Learning to Learn: Introduction and Overview. Sebastian Thrun, Lorien Pratt, 10.1007/978-1-4615-5529-2_11998Springer USBoston, MA</p>
<p>Learning to reinforcement learn. Jane Wang, Zeb Kurth-Nelson, Hubert Soyer, Joel Leibo, Dhruva Tirumala, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society201739</p>
<p>LLMs and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, Elias Boutros Khalil, Transactions on Machine Learning Research. 2835-88562024</p>
<p>Compositional diversity in visual concept learning. Yanli Zhou, Reuben Feinman, Brenden M Lake, 10.1016/j.cognition.2023.105711.URLhttps://www.sciencedirect.com/science/article/pii/S0010027723003451Cognition. 0010-02772441057112024</p>            </div>
        </div>

    </div>
</body>
</html>
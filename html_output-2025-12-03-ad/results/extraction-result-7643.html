<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7643 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7643</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7643</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-7670d88173786e054b72c32580676fa3fed0c364</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7670d88173786e054b72c32580676fa3fed0c364" target="_blank">Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is found that reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMs' inability to fully leverage the cross-lingual capability when evaluating translations.</p>
                <p><strong>Paper Abstract:</strong> This study investigates how Large Language Models (LLMs) leverage source and reference data in machine translation evaluation task, aiming to better understand the mechanisms behind their remarkable performance in this task. We design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. We find that reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMs' inability to fully leverage the cross-lingual capability when evaluating translations. Further analysis of the fine-grained evaluation and fine-tuning experiments show similar results. These findings also suggest a potential research direction for LLMs that fully exploits the cross-lingual capability of LLMs to achieve better performance in machine translation evaluation tasks.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7643.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7643.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMBA-SQM R-T vs T/S-T/S-R-T (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMBA single-question metric (GEMBA-SQM) input-mode comparison for GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Coarse-grained translation-quality score prediction using the GEMBA-SQM prompt with different input modes (T, S-T, R-T, S-R-T). The study finds that including a human reference (R-T) yields the best system-level accuracy and segment-level correlations for GPT-3.5, while adding the source sometimes degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (chat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed API chat model (gpt-3.5-turbo-0613) — an instruction-following autoregressive transformer (chat interface).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>chat / closed (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Coarse-grained translation quality score prediction (WMT22 MQM test)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given translation (and optionally source and/or reference), predict a continuous quality score (0–100) aligned to MQM-derived human quality scores; report system-level accuracy and segment-level Kendall's tau / Pearson rho.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction prompt (GEMBA-SQM) with variants that include different input fields: T (translation only), S-T (source + translation), R-T (reference + translation), S-R-T (source + reference + translation).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>GEMBA-SQM: 0–100 scoring scale; when present the prompt contains labeled lines like 'src: "..."', 'reference: "..."', 'translation: "..."'; single-shot (zero in-context examples) instruction; used as a single exchange to the chat model (gpt-3.5-turbo-0613).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>system-level accuracy; segment-level Kendall's τ; Pearson ρ</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T: acc=0.759; τ=0.181; ρ=0.153. S-T: acc=0.876; τ=0.212; ρ=0.242. R-T: acc=0.891; τ=0.284; ρ=0.280. S-R-T: acc=0.876; τ=0.255; ρ=0.285.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T mode: acc=0.759; τ=0.181; ρ=0.153.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>R-T vs T: +0.132 absolute in accuracy (0.759→0.891); τ +0.103 absolute (0.181→0.284); ρ +0.127 absolute (0.153→0.280). S-T and S-R-T improve accuracy over T but R-T is best overall.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>WMT22 MQM test set (En-De, Zh-En, En-Ru aggregated); GEMBA-SQM prompt template; model: gpt-3.5-turbo-0613; metrics computed with PERM-BOTH resampling (1000 runs, p=0.05) as in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Segment-level Kendall's τ for R-T marked as significantly highest among input modes (asterisk in table); significance assessed using PERM-BOTH (1000 resamples, p=0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7643.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7643.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMBA-SQM R-T superiority (Llama2-70B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMBA-SQM input-mode comparison for Llama2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Coarse-grained score prediction with the GEMBA-SQM prompt on Llama2-70B-Chat shows the R-T input mode (reference + translation) gives the best system accuracy and segment correlations; including the source does not further improve and can reduce some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open Llama2 family chat-style decoder-only transformer fine-tuned for instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Coarse-grained translation quality score prediction (WMT22 MQM test)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above: predict a continuous translation quality score given translation and optionally source/reference; compare input modes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>GEMBA-SQM natural-language prompt with labeled fields (T, S-T, R-T, S-R-T).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same GEMBA-SQM prompt structure; chat model used in a single-shot scoring call; presence/absence of source/reference toggled per mode.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>system-level accuracy; Kendall's τ; Pearson ρ</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T: acc=0.737; τ=0.148; ρ=0.105. S-T: acc=0.807; τ=0.126; ρ=0.123. R-T: acc=0.887*; τ=0.241*; ρ=0.221*. S-R-T: acc=0.843; τ=0.167; ρ=0.180.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T mode: acc=0.737; τ=0.148.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>R-T vs T: +0.150 absolute accuracy (0.737→0.887); τ +0.093 absolute (0.148→0.241). R-T is marked significantly higher among input modes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>WMT22 MQM test set; GEMBA-SQM prompt; Llama2-70B-Chat; evaluation with same metrics and statistical test (PERM-BOTH).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>R-T values marked with asterisks in tables indicating significantly highest among the four modes (PERM-BOTH, 1000 runs, p=0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7643.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7643.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoMQM R-T vs T (GPT-3.5) - fine-grained</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoMQM prompt input-mode comparison for GPT-3.5-turbo (fine-grained MQM error scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using the AutoMQM prompt (few-shot demonstration + structured output), GPT-3.5 performs best when given the reference (R-T) vs translation-only or source+translation; the reference substantially improves MQM score prediction accuracy and correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (chat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed API chat model (gpt-3.5-turbo-0613).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>chat / closed (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fine-grained error detection and MQM score prediction (AutoMQM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify error spans and categories and compute MQM scores derived from predicted error annotations; report system-level accuracy and Kendall's τ / Pearson ρ for MQM scores.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>AutoMQM prompt: few-shot (4 in-context examples from WMT21), structured output listing error spans/categories/severity; input modes T, S-T, R-T, S-R-T determine which fields are shown in the demo and test prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot structured output</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>AutoMQM uses 4 stratified in-context demonstrations; prompt indicates labeled fields for source/reference/translation depending on mode; output must follow error-list format; base models used greedy decoding for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>system-level accuracy; Kendall's τ; Pearson ρ (MQM score meta-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across two language pairs (En-De, Zh-En): T: acc=0.757; τ=0.221; ρ=0.283. S-T: acc=0.751; τ=0.150; ρ=0.222. R-T: acc=0.858; τ=0.275; ρ=0.331. S-R-T: acc=0.769; τ=0.284; ρ=0.349.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T mode: acc=0.757; τ=0.221; ρ=0.283.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>R-T vs T: +0.101 absolute accuracy (0.757→0.858); τ +0.054 absolute (0.221→0.275); ρ +0.048 absolute (0.283→0.331).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>200 sampled source sentences from WMT22; in-context demos from WMT21 (4 examples); GPT-3.5 chat used for AutoMQM experiments; span/category outputs converted to MQM weighted score.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Paper notes R-T is often the best; some scores in tables are bold/asterisked to indicate significance among input modes (same PERM-BOTH test setup referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7643.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7643.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Span meta-eval: AutoMQM (GPT-3.5) – format impact on span F1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoMQM span-level meta-evaluation showing effect of input-mode on span detection (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Span precision/recall/F1 and major-error metrics are higher when a reference is provided (R-T) compared to T-only for GPT-3.5; adding the source (S-R-T) does not consistently improve span F1 beyond R-T.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (chat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed chat model (gpt-3.5-turbo-0613) used with AutoMQM prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>chat / closed (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Error-span detection (AutoMQM) — span-level meta-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict error spans (word-level) and compute span precision (SP), span recall (SR), span F1 (SF1) and major-error F1 (MF1); compare input modes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>AutoMQM few-shot structured error-list prompt with T, S-T, R-T, S-R-T variants.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot structured output</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>AutoMQM uses 4 in-context demonstrations; models produce structured lists of spans and categories; evaluation uses set overlap over token indices to compute SP/SR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>span precision (SP), span recall (SR), span F1 (SF1), major F1 (MF1), MCC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T: SP=0.162 / SR=0.375 / SF1=0.227; MP/MR/MF1 = 0.122/0.155/0.136; MCC=0.153. R-T: SP=0.239 / SR=0.378 / SF1=0.293; MP/MR/MF1 = 0.202/0.344/0.254; MCC=0.208.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T mode span metrics (see above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>R-T vs T: SF1 +0.066 absolute (0.227→0.293); MP +0.080 absolute (0.122→0.202); MF1 +0.118 absolute (0.136→0.254).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>AutoMQM prompt on sampled WMT22 subsets; GPT-3.5 chat; greedy generation for base-model variants as noted for other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Reported tables highlight best values in bold; no numeric p-values for span-level differences reported in the span table itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7643.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7643.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Log-probability input-mode experiment (various models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Log-probability scoring of translation text under different prompt contexts (T, S-T, R-T, S-R-T)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instead of asking models to output a score, authors compute the log-probability of the translation tokens conditioned on different contexts (T-only, with source, with reference, with both) and use that as a metric — R-T context gives superior correlations and system accuracy, and T-only log-prob remains non-trivial (better than random).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple open models (Llama2-70B-Chat, Llama2-13B-Chat, Llama2-7B-Chat, Mistral-7B-Instruct, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open decoder-only / instruct models in chat and base variants; base models concatenated inputs with '=' and used token log-probabilities; chat models used same GEMBA prompt and computed log-prob for translation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B, 13B, 70B chat variants; Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Translation-quality scoring via log-probability (WMT22)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compute sum log-probability of translation tokens conditioned on prompt context; treat this score as a metric and evaluate system-level accuracy and segment-level correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Text-only prompt contexts varying which fields appear (translation only vs source/reference combinations); no explicit instruction for numeric scoring — metric is computed from model token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt context (scoring by conditional likelihood)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For chat models: GEMBA-like instruction kept but log-prob of translation tokens measured; for base models: simple concatenation 'source=reference=translation' or just 'translation'; greedy/standard decoding not relevant as log-prob computed; evaluated across same WMT22 test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>system-level accuracy; Kendall's τ; Pearson ρ</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (Llama2-70B-Chat): T acc=0.701; τ=0.176. S-T acc=0.485; τ=0.168. R-T acc=0.730; τ=0.246. S-R-T acc=0.544; τ=0.196. General finding: R-T typically yields the best correlations and system accuracy across models; T-only log-prob gives above-random signal.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T-only log-prob (example: Llama2-70B-Chat T acc=0.701; τ=0.176).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>R-T vs T (Llama2-70B-Chat): acc +0.029 absolute (0.701→0.730); τ +0.070 absolute (0.176→0.246). In many models R-T outperforms S-T and S-R-T; T-only still beats random.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Log-prob computed for chat/base models as described; test set = WMT22 MQM; models include chat and base Llama2 variants and Mistral; authors note limited scaling effect (greater model size did not reliably increase metric performance).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Paper notes differences and calls out patterns; specific significance markers used elsewhere, but log-prob table entries are presented without per-cell p-values in that table.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7643.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7643.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned Llama2-7B (S-T / R-T / S-R-T)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned Llama2-7B evaluation across input modes after supervised MQM fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama2-7B fine-tuned on MQM-style supervised data still shows better performance when given reference (R-T) than when given source (S-T) or all three (S-R-T); naive fine-tuning does not eliminate the negative/neutral effect of the source.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B (base) fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open Llama2-7B base model fine-tuned for 3 epochs on WMT21 MQM-style supervised data using an Alpaca-like instruction template; batch size 128, LR 2e-5 decayed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fine-grained MQM evaluation and coarse score prediction (WMT22 test after WMT21-based fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>After instruction-style fine-tuning on MQM labels, evaluate system-level accuracy and segment-level correlations when prompted with S-T, R-T, or S-R-T inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>AutoMQM-like instruction template used for training and evaluation; training samples randomly assigned to one of the three modes (S-T, R-T, S-R-T) during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / training data distribution</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Training set combined En-De and Zh-En WMT21 MQM samples; random assignment of mode per sample; one fine-tuned checkpoint evaluated on WMT22; also a down-sampled training variant used to reduce No-error dominance for En-De.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>system-level accuracy; Kendall's τ; Pearson ρ</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Fine-tuned Llama2-7B (main): S-T acc=0.832; τ=0.072; ρ=0.080. R-T acc=0.847; τ=0.139*; ρ=0.199*. S-R-T acc=0.847; τ=0.114; ρ=0.145. (Down-sampled training variant shows similar pattern with R-T having significantly better τ on some splits.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>S-T mode (fine-tuned): acc=0.832; τ=0.072.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>R-T vs S-T: acc +0.015 absolute (0.832→0.847); τ +0.067 absolute (0.072→0.139). R-T τ marked significant on aggregated comparisons in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fine-tuned for 3 epochs; LR 2e-5 decayed; batch size 128; training samples randomly assigned to input modes; evaluation on WMT22.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Values starred in Table 10 indicate significantly better τ/ρ for R-T in some comparisons (paper uses same PERM-BOTH resampling test framework).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7643.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7643.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shapley analysis of source vs reference contributions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shapley-value quantification of the incremental contribution of source and reference to evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors compute Shapley values over the input parts (source and reference) to quantify each part's marginal contribution; results show reference consistently contributes more positively, while source contributions are small and sometimes negative (i.e., source can hurt performance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple evaluated models (GPT-3.5-turbo, Llama2-7B/13B/70B-Chat, Mistral-7B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various chat and base models evaluated in the study; Shapley values computed from the four input-mode scores S_T, S_ST, S_RT, S_SRT per model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B, 13B, 70B, GPT-3.5 chat)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Coarse-grained score prediction & fine-grained error detection (Shapley meta-analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compute Shapley value for source and reference parts using the formula in Appendix D to attribute their contribution to system-level accuracy and Kendall's τ.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Analytic post-hoc attribution: use scores from the four input modes to compute marginal contributions (Shapley), not a prompt-format change at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>analysis method / attribution of input components</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Shapley_src = ((S_ST - S_T) + (S_SRT - S_RT)) / 2; Shapley_ref = ((S_RT - S_T) + (S_SRT - S_ST)) / 2. Values reported per model and per language pair.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Shapley contributions (units = change in system-level accuracy or Kendall's τ)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples: GPT-3.5-turbo Shapley (system-accuracy): src=+0.051; ref=+0.066. Llama2-13B src=-0.067 (negative), ref=+0.043. Llama2-7B ref contribution often larger than src (e.g., ref=+0.159, src=-0.030 for Llama2-7B-Chat in score task table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Interpretation: Reference inclusion provides the largest positive marginal contribution across models; source inclusion provides small positive, neutral, or negative contributions (i.e., adding source can decrease performance).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Shapley values computed from meta-evaluation scores across four input modes per model and per language pair; details and formula in Appendix D.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Shapley values reported numerically; significance not directly attached to Shapley table, but mode-level significance tested elsewhere using PERM-BOTH.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7643.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7643.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt template difference (AutoMQM vs GEMBA-SQM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of AutoMQM (few-shot structured) vs GEMBA-SQM (score prompt) prompt templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares two prompt styles: GEMBA-SQM (single-shot numeric scoring prompt) and AutoMQM (few-shot, structured error-span/category output). For GPT-3.5, AutoMQM yields stronger fine-grained MQM performance than GEMBA-SQM, while Llama2 models do not consistently benefit from AutoMQM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo and Llama2 series (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed GPT-3.5 chat; open Llama2 chat/base variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (GPT-3.5 chat; Llama2 7B/13B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fine-grained MQM error detection and coarse score prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how prompt style (GEMBA-SQM vs AutoMQM) affects quality of MQM-style outputs (scores, spans, categories) for different models.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Two different prompt styles: GEMBA-SQM (single-shot continuous 0–100 score prompt) vs AutoMQM (4 in-context examples, structured errors output).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot vs zero-shot format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>AutoMQM uses 4 in-context demonstrations sampled from WMT21 with rejection criteria; outputs are structured lists of error spans and categories; GEMBA-SQM asks for a single score on a labelled 0–100 scale; format differences tested across models and input modes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>system-level accuracy; Kendall's τ; span F1; category F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: For GPT-3.5 in AutoMQM (R-T) MQM score acc=0.858 (see AutoMQM results), AutoMQM outperforms GEMBA-SQM for GPT-3.5 in this study; for Llama2 series, AutoMQM did not consistently outperform GEMBA-SQM.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GEMBA-SQM prompt for the same model/task (numbers provided in separate tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>GPT-3.5: AutoMQM outperforms GEMBA-SQM on score metrics in this paper; for Llama2 models the trend is mixed (no consistent improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>AutoMQM uses few-shot demos (4); GEMBA-SQM is zero-shot numeric instruction; experiments use same evaluation sets (WMT21/WMT22 splits) where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Paper reports per-table bests and marks significant mode-level differences with asterisks; the prompt-style comparison is presented descriptively (no single p-value comparing prompts across all models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation <em>(Rating: 2)</em></li>
                <li>COMET-22: unbabel-ist 2022 submission for the metrics shared task <em>(Rating: 2)</em></li>
                <li>GEMBAMQM: detecting translation quality error spans with GPT-4 <em>(Rating: 2)</em></li>
                <li>INSTRUCTSCORE: towards explainable text generation evaluation with automatic feedback <em>(Rating: 1)</em></li>
                <li>Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7643",
    "paper_id": "paper-7670d88173786e054b72c32580676fa3fed0c364",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "GEMBA-SQM R-T vs T/S-T/S-R-T (GPT-3.5)",
            "name_full": "GEMBA single-question metric (GEMBA-SQM) input-mode comparison for GPT-3.5-turbo",
            "brief_description": "Coarse-grained translation-quality score prediction using the GEMBA-SQM prompt with different input modes (T, S-T, R-T, S-R-T). The study finds that including a human reference (R-T) yields the best system-level accuracy and segment-level correlations for GPT-3.5, while adding the source sometimes degrades performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (chat)",
            "model_description": "Closed API chat model (gpt-3.5-turbo-0613) — an instruction-following autoregressive transformer (chat interface).",
            "model_size": "chat / closed (GPT-3.5 family)",
            "task_name": "Coarse-grained translation quality score prediction (WMT22 MQM test)",
            "task_description": "Given translation (and optionally source and/or reference), predict a continuous quality score (0–100) aligned to MQM-derived human quality scores; report system-level accuracy and segment-level Kendall's tau / Pearson rho.",
            "problem_format": "Natural-language instruction prompt (GEMBA-SQM) with variants that include different input fields: T (translation only), S-T (source + translation), R-T (reference + translation), S-R-T (source + reference + translation).",
            "format_category": "prompt style / input modality",
            "format_details": "GEMBA-SQM: 0–100 scoring scale; when present the prompt contains labeled lines like 'src: \"...\"', 'reference: \"...\"', 'translation: \"...\"'; single-shot (zero in-context examples) instruction; used as a single exchange to the chat model (gpt-3.5-turbo-0613).",
            "performance_metric": "system-level accuracy; segment-level Kendall's τ; Pearson ρ",
            "performance_value": "T: acc=0.759; τ=0.181; ρ=0.153. S-T: acc=0.876; τ=0.212; ρ=0.242. R-T: acc=0.891; τ=0.284; ρ=0.280. S-R-T: acc=0.876; τ=0.255; ρ=0.285.",
            "baseline_performance": "T mode: acc=0.759; τ=0.181; ρ=0.153.",
            "performance_change": "R-T vs T: +0.132 absolute in accuracy (0.759→0.891); τ +0.103 absolute (0.181→0.284); ρ +0.127 absolute (0.153→0.280). S-T and S-R-T improve accuracy over T but R-T is best overall.",
            "experimental_setting": "WMT22 MQM test set (En-De, Zh-En, En-Ru aggregated); GEMBA-SQM prompt template; model: gpt-3.5-turbo-0613; metrics computed with PERM-BOTH resampling (1000 runs, p=0.05) as in paper.",
            "statistical_significance": "Segment-level Kendall's τ for R-T marked as significantly highest among input modes (asterisk in table); significance assessed using PERM-BOTH (1000 resamples, p=0.05).",
            "uuid": "e7643.0",
            "source_info": {
                "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GEMBA-SQM R-T superiority (Llama2-70B-Chat)",
            "name_full": "GEMBA-SQM input-mode comparison for Llama2-70B-Chat",
            "brief_description": "Coarse-grained score prediction with the GEMBA-SQM prompt on Llama2-70B-Chat shows the R-T input mode (reference + translation) gives the best system accuracy and segment correlations; including the source does not further improve and can reduce some metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-70B-Chat",
            "model_description": "Open Llama2 family chat-style decoder-only transformer fine-tuned for instruction following.",
            "model_size": "70B parameters",
            "task_name": "Coarse-grained translation quality score prediction (WMT22 MQM test)",
            "task_description": "Same as above: predict a continuous translation quality score given translation and optionally source/reference; compare input modes.",
            "problem_format": "GEMBA-SQM natural-language prompt with labeled fields (T, S-T, R-T, S-R-T).",
            "format_category": "prompt style / input modality",
            "format_details": "Same GEMBA-SQM prompt structure; chat model used in a single-shot scoring call; presence/absence of source/reference toggled per mode.",
            "performance_metric": "system-level accuracy; Kendall's τ; Pearson ρ",
            "performance_value": "T: acc=0.737; τ=0.148; ρ=0.105. S-T: acc=0.807; τ=0.126; ρ=0.123. R-T: acc=0.887*; τ=0.241*; ρ=0.221*. S-R-T: acc=0.843; τ=0.167; ρ=0.180.",
            "baseline_performance": "T mode: acc=0.737; τ=0.148.",
            "performance_change": "R-T vs T: +0.150 absolute accuracy (0.737→0.887); τ +0.093 absolute (0.148→0.241). R-T is marked significantly higher among input modes.",
            "experimental_setting": "WMT22 MQM test set; GEMBA-SQM prompt; Llama2-70B-Chat; evaluation with same metrics and statistical test (PERM-BOTH).",
            "statistical_significance": "R-T values marked with asterisks in tables indicating significantly highest among the four modes (PERM-BOTH, 1000 runs, p=0.05).",
            "uuid": "e7643.1",
            "source_info": {
                "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "AutoMQM R-T vs T (GPT-3.5) - fine-grained",
            "name_full": "AutoMQM prompt input-mode comparison for GPT-3.5-turbo (fine-grained MQM error scoring)",
            "brief_description": "Using the AutoMQM prompt (few-shot demonstration + structured output), GPT-3.5 performs best when given the reference (R-T) vs translation-only or source+translation; the reference substantially improves MQM score prediction accuracy and correlations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (chat)",
            "model_description": "Closed API chat model (gpt-3.5-turbo-0613).",
            "model_size": "chat / closed (GPT-3.5 family)",
            "task_name": "Fine-grained error detection and MQM score prediction (AutoMQM)",
            "task_description": "Identify error spans and categories and compute MQM scores derived from predicted error annotations; report system-level accuracy and Kendall's τ / Pearson ρ for MQM scores.",
            "problem_format": "AutoMQM prompt: few-shot (4 in-context examples from WMT21), structured output listing error spans/categories/severity; input modes T, S-T, R-T, S-R-T determine which fields are shown in the demo and test prompt.",
            "format_category": "prompt style / few-shot structured output",
            "format_details": "AutoMQM uses 4 stratified in-context demonstrations; prompt indicates labeled fields for source/reference/translation depending on mode; output must follow error-list format; base models used greedy decoding for generation.",
            "performance_metric": "system-level accuracy; Kendall's τ; Pearson ρ (MQM score meta-evaluation)",
            "performance_value": "Across two language pairs (En-De, Zh-En): T: acc=0.757; τ=0.221; ρ=0.283. S-T: acc=0.751; τ=0.150; ρ=0.222. R-T: acc=0.858; τ=0.275; ρ=0.331. S-R-T: acc=0.769; τ=0.284; ρ=0.349.",
            "baseline_performance": "T mode: acc=0.757; τ=0.221; ρ=0.283.",
            "performance_change": "R-T vs T: +0.101 absolute accuracy (0.757→0.858); τ +0.054 absolute (0.221→0.275); ρ +0.048 absolute (0.283→0.331).",
            "experimental_setting": "200 sampled source sentences from WMT22; in-context demos from WMT21 (4 examples); GPT-3.5 chat used for AutoMQM experiments; span/category outputs converted to MQM weighted score.",
            "statistical_significance": "Paper notes R-T is often the best; some scores in tables are bold/asterisked to indicate significance among input modes (same PERM-BOTH test setup referenced).",
            "uuid": "e7643.2",
            "source_info": {
                "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Span meta-eval: AutoMQM (GPT-3.5) – format impact on span F1",
            "name_full": "AutoMQM span-level meta-evaluation showing effect of input-mode on span detection (GPT-3.5)",
            "brief_description": "Span precision/recall/F1 and major-error metrics are higher when a reference is provided (R-T) compared to T-only for GPT-3.5; adding the source (S-R-T) does not consistently improve span F1 beyond R-T.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (chat)",
            "model_description": "Closed chat model (gpt-3.5-turbo-0613) used with AutoMQM prompt.",
            "model_size": "chat / closed (GPT-3.5 family)",
            "task_name": "Error-span detection (AutoMQM) — span-level meta-evaluation",
            "task_description": "Predict error spans (word-level) and compute span precision (SP), span recall (SR), span F1 (SF1) and major-error F1 (MF1); compare input modes.",
            "problem_format": "AutoMQM few-shot structured error-list prompt with T, S-T, R-T, S-R-T variants.",
            "format_category": "prompt style / few-shot structured output",
            "format_details": "AutoMQM uses 4 in-context demonstrations; models produce structured lists of spans and categories; evaluation uses set overlap over token indices to compute SP/SR.",
            "performance_metric": "span precision (SP), span recall (SR), span F1 (SF1), major F1 (MF1), MCC",
            "performance_value": "T: SP=0.162 / SR=0.375 / SF1=0.227; MP/MR/MF1 = 0.122/0.155/0.136; MCC=0.153. R-T: SP=0.239 / SR=0.378 / SF1=0.293; MP/MR/MF1 = 0.202/0.344/0.254; MCC=0.208.",
            "baseline_performance": "T mode span metrics (see above).",
            "performance_change": "R-T vs T: SF1 +0.066 absolute (0.227→0.293); MP +0.080 absolute (0.122→0.202); MF1 +0.118 absolute (0.136→0.254).",
            "experimental_setting": "AutoMQM prompt on sampled WMT22 subsets; GPT-3.5 chat; greedy generation for base-model variants as noted for other experiments.",
            "statistical_significance": "Reported tables highlight best values in bold; no numeric p-values for span-level differences reported in the span table itself.",
            "uuid": "e7643.3",
            "source_info": {
                "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Log-probability input-mode experiment (various models)",
            "name_full": "Log-probability scoring of translation text under different prompt contexts (T, S-T, R-T, S-R-T)",
            "brief_description": "Instead of asking models to output a score, authors compute the log-probability of the translation tokens conditioned on different contexts (T-only, with source, with reference, with both) and use that as a metric — R-T context gives superior correlations and system accuracy, and T-only log-prob remains non-trivial (better than random).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple open models (Llama2-70B-Chat, Llama2-13B-Chat, Llama2-7B-Chat, Mistral-7B-Instruct, etc.)",
            "model_description": "Open decoder-only / instruct models in chat and base variants; base models concatenated inputs with '=' and used token log-probabilities; chat models used same GEMBA prompt and computed log-prob for translation.",
            "model_size": "various (7B, 13B, 70B chat variants; Mistral-7B)",
            "task_name": "Translation-quality scoring via log-probability (WMT22)",
            "task_description": "Compute sum log-probability of translation tokens conditioned on prompt context; treat this score as a metric and evaluate system-level accuracy and segment-level correlations.",
            "problem_format": "Text-only prompt contexts varying which fields appear (translation only vs source/reference combinations); no explicit instruction for numeric scoring — metric is computed from model token probabilities.",
            "format_category": "input modality / prompt context (scoring by conditional likelihood)",
            "format_details": "For chat models: GEMBA-like instruction kept but log-prob of translation tokens measured; for base models: simple concatenation 'source=reference=translation' or just 'translation'; greedy/standard decoding not relevant as log-prob computed; evaluated across same WMT22 test sets.",
            "performance_metric": "system-level accuracy; Kendall's τ; Pearson ρ",
            "performance_value": "Example (Llama2-70B-Chat): T acc=0.701; τ=0.176. S-T acc=0.485; τ=0.168. R-T acc=0.730; τ=0.246. S-R-T acc=0.544; τ=0.196. General finding: R-T typically yields the best correlations and system accuracy across models; T-only log-prob gives above-random signal.",
            "baseline_performance": "T-only log-prob (example: Llama2-70B-Chat T acc=0.701; τ=0.176).",
            "performance_change": "R-T vs T (Llama2-70B-Chat): acc +0.029 absolute (0.701→0.730); τ +0.070 absolute (0.176→0.246). In many models R-T outperforms S-T and S-R-T; T-only still beats random.",
            "experimental_setting": "Log-prob computed for chat/base models as described; test set = WMT22 MQM; models include chat and base Llama2 variants and Mistral; authors note limited scaling effect (greater model size did not reliably increase metric performance).",
            "statistical_significance": "Paper notes differences and calls out patterns; specific significance markers used elsewhere, but log-prob table entries are presented without per-cell p-values in that table.",
            "uuid": "e7643.4",
            "source_info": {
                "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Fine-tuned Llama2-7B (S-T / R-T / S-R-T)",
            "name_full": "Fine-tuned Llama2-7B evaluation across input modes after supervised MQM fine-tuning",
            "brief_description": "Llama2-7B fine-tuned on MQM-style supervised data still shows better performance when given reference (R-T) than when given source (S-T) or all three (S-R-T); naive fine-tuning does not eliminate the negative/neutral effect of the source.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B (base) fine-tuned",
            "model_description": "Open Llama2-7B base model fine-tuned for 3 epochs on WMT21 MQM-style supervised data using an Alpaca-like instruction template; batch size 128, LR 2e-5 decayed.",
            "model_size": "7B parameters (fine-tuned)",
            "task_name": "Fine-grained MQM evaluation and coarse score prediction (WMT22 test after WMT21-based fine-tuning)",
            "task_description": "After instruction-style fine-tuning on MQM labels, evaluate system-level accuracy and segment-level correlations when prompted with S-T, R-T, or S-R-T inputs.",
            "problem_format": "AutoMQM-like instruction template used for training and evaluation; training samples randomly assigned to one of the three modes (S-T, R-T, S-R-T) during fine-tuning.",
            "format_category": "prompt style / training data distribution",
            "format_details": "Training set combined En-De and Zh-En WMT21 MQM samples; random assignment of mode per sample; one fine-tuned checkpoint evaluated on WMT22; also a down-sampled training variant used to reduce No-error dominance for En-De.",
            "performance_metric": "system-level accuracy; Kendall's τ; Pearson ρ",
            "performance_value": "Fine-tuned Llama2-7B (main): S-T acc=0.832; τ=0.072; ρ=0.080. R-T acc=0.847; τ=0.139*; ρ=0.199*. S-R-T acc=0.847; τ=0.114; ρ=0.145. (Down-sampled training variant shows similar pattern with R-T having significantly better τ on some splits.)",
            "baseline_performance": "S-T mode (fine-tuned): acc=0.832; τ=0.072.",
            "performance_change": "R-T vs S-T: acc +0.015 absolute (0.832→0.847); τ +0.067 absolute (0.072→0.139). R-T τ marked significant on aggregated comparisons in tables.",
            "experimental_setting": "Fine-tuned for 3 epochs; LR 2e-5 decayed; batch size 128; training samples randomly assigned to input modes; evaluation on WMT22.",
            "statistical_significance": "Values starred in Table 10 indicate significantly better τ/ρ for R-T in some comparisons (paper uses same PERM-BOTH resampling test framework).",
            "uuid": "e7643.5",
            "source_info": {
                "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Shapley analysis of source vs reference contributions",
            "name_full": "Shapley-value quantification of the incremental contribution of source and reference to evaluation metrics",
            "brief_description": "The authors compute Shapley values over the input parts (source and reference) to quantify each part's marginal contribution; results show reference consistently contributes more positively, while source contributions are small and sometimes negative (i.e., source can hurt performance).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple evaluated models (GPT-3.5-turbo, Llama2-7B/13B/70B-Chat, Mistral-7B-Instruct)",
            "model_description": "Various chat and base models evaluated in the study; Shapley values computed from the four input-mode scores S_T, S_ST, S_RT, S_SRT per model.",
            "model_size": "various (7B, 13B, 70B, GPT-3.5 chat)",
            "task_name": "Coarse-grained score prediction & fine-grained error detection (Shapley meta-analysis)",
            "task_description": "Compute Shapley value for source and reference parts using the formula in Appendix D to attribute their contribution to system-level accuracy and Kendall's τ.",
            "problem_format": "Analytic post-hoc attribution: use scores from the four input modes to compute marginal contributions (Shapley), not a prompt-format change at inference.",
            "format_category": "analysis method / attribution of input components",
            "format_details": "Shapley_src = ((S_ST - S_T) + (S_SRT - S_RT)) / 2; Shapley_ref = ((S_RT - S_T) + (S_SRT - S_ST)) / 2. Values reported per model and per language pair.",
            "performance_metric": "Shapley contributions (units = change in system-level accuracy or Kendall's τ)",
            "performance_value": "Examples: GPT-3.5-turbo Shapley (system-accuracy): src=+0.051; ref=+0.066. Llama2-13B src=-0.067 (negative), ref=+0.043. Llama2-7B ref contribution often larger than src (e.g., ref=+0.159, src=-0.030 for Llama2-7B-Chat in score task table).",
            "baseline_performance": null,
            "performance_change": "Interpretation: Reference inclusion provides the largest positive marginal contribution across models; source inclusion provides small positive, neutral, or negative contributions (i.e., adding source can decrease performance).",
            "experimental_setting": "Shapley values computed from meta-evaluation scores across four input modes per model and per language pair; details and formula in Appendix D.",
            "statistical_significance": "Shapley values reported numerically; significance not directly attached to Shapley table, but mode-level significance tested elsewhere using PERM-BOTH.",
            "uuid": "e7643.6",
            "source_info": {
                "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Prompt template difference (AutoMQM vs GEMBA-SQM)",
            "name_full": "Comparison of AutoMQM (few-shot structured) vs GEMBA-SQM (score prompt) prompt templates",
            "brief_description": "The paper compares two prompt styles: GEMBA-SQM (single-shot numeric scoring prompt) and AutoMQM (few-shot, structured error-span/category output). For GPT-3.5, AutoMQM yields stronger fine-grained MQM performance than GEMBA-SQM, while Llama2 models do not consistently benefit from AutoMQM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo and Llama2 series (various sizes)",
            "model_description": "Closed GPT-3.5 chat; open Llama2 chat/base variants.",
            "model_size": "various (GPT-3.5 chat; Llama2 7B/13B/70B)",
            "task_name": "Fine-grained MQM error detection and coarse score prediction",
            "task_description": "Evaluate how prompt style (GEMBA-SQM vs AutoMQM) affects quality of MQM-style outputs (scores, spans, categories) for different models.",
            "problem_format": "Two different prompt styles: GEMBA-SQM (single-shot continuous 0–100 score prompt) vs AutoMQM (4 in-context examples, structured errors output).",
            "format_category": "prompt style / few-shot vs zero-shot format",
            "format_details": "AutoMQM uses 4 in-context demonstrations sampled from WMT21 with rejection criteria; outputs are structured lists of error spans and categories; GEMBA-SQM asks for a single score on a labelled 0–100 scale; format differences tested across models and input modes.",
            "performance_metric": "system-level accuracy; Kendall's τ; span F1; category F1",
            "performance_value": "Example: For GPT-3.5 in AutoMQM (R-T) MQM score acc=0.858 (see AutoMQM results), AutoMQM outperforms GEMBA-SQM for GPT-3.5 in this study; for Llama2 series, AutoMQM did not consistently outperform GEMBA-SQM.",
            "baseline_performance": "GEMBA-SQM prompt for the same model/task (numbers provided in separate tables).",
            "performance_change": "GPT-3.5: AutoMQM outperforms GEMBA-SQM on score metrics in this paper; for Llama2 models the trend is mixed (no consistent improvement).",
            "experimental_setting": "AutoMQM uses few-shot demos (4); GEMBA-SQM is zero-shot numeric instruction; experiments use same evaluation sets (WMT21/WMT22 splits) where applicable.",
            "statistical_significance": "Paper reports per-table bests and marks significant mode-level differences with asterisks; the prompt-style comparison is presented descriptively (no single p-value comparing prompts across all models).",
            "uuid": "e7643.7",
            "source_info": {
                "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2
        },
        {
            "paper_title": "The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation",
            "rating": 2
        },
        {
            "paper_title": "COMET-22: unbabel-ist 2022 submission for the metrics shared task",
            "rating": 2
        },
        {
            "paper_title": "GEMBAMQM: detecting translation quality error spans with GPT-4",
            "rating": 2
        },
        {
            "paper_title": "INSTRUCTSCORE: towards explainable text generation evaluation with automatic feedback",
            "rating": 1
        },
        {
            "paper_title": "Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust",
            "rating": 1
        }
    ],
    "cost": 0.02309775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation</h1>
<p>Xu Huang ${ }^{1 *}$ Zhirui Zhang ${ }^{2 \dagger}$ Xiang Geng ${ }^{1}$<br>Yichao Du ${ }^{3}$ Jiajun Chen ${ }^{1}$ Shujian Huang ${ }^{1 \dagger}$<br>${ }^{1}$ National Key Laboratory for Novel Software Technology, Nanjing University<br>${ }^{2}$ Tencent AI Lab ${ }^{3}$ University of Science and Technology of China<br>${ }^{1}{x u h u a n g, g x} @ s m a i l . n j u . e d u . c n,{c h e n j j, h u a n g s j} @ n j u . e d u . c n$<br>${ }^{2}$ zrustc11@gmail.com ${ }^{3}$ duyichao@mail.ustc.edu.cn</p>
<h4>Abstract</h4>
<p>This study investigates how Large Language Models (LLMs) leverage source and reference data in machine translation evaluation task, aiming to better understand the mechanisms behind their remarkable performance in this task. We design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. We find that reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMs' inability to fully leverage the cross-lingual capability when evaluating translations. Further analysis of the fine-grained evaluation and finetuning experiments show similar results. These findings also suggest a potential research direction for LLMs that fully exploits the crosslingual capability of LLMs to achieve better performance in machine translation evaluation tasks.</p>
<h2>1 Introduction</h2>
<p>The last decade has witnessed significant development in Neural Machine Translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018). As the quality of machine translations has been improved, it becomes more challenging and critical for automatic translation evaluation. The recent study (Freitag et al., 2022) calls for stopping using BLEU (Papineni et al., 2002), a traditional metric, as it is not reliable for highquality translations and has a lower correlation with human judgements. Neural metrics based on pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020), such as COMET (Rei et al., 2020), show a higher correlation with human judgments. However, these neural</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>${ }^{\dagger}$ Corresponding authors</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Mode</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Kendall's $\tau$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">All LPs</td>
<td style="text-align: center;">En-De</td>
<td style="text-align: center;">Zh-En</td>
<td style="text-align: center;">En-Ru</td>
</tr>
<tr>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.195</td>
</tr>
<tr>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.219</td>
</tr>
<tr>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 8 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 8 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 5 3}$</td>
</tr>
<tr>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.211</td>
</tr>
</tbody>
</table>
<p>Table 1: The system-level accuracy and segment-level Kendall's $\tau$ correlation of ChatGPT when using different inputs.
metrics only provide a score lacking interpretability and still exhibit robustness issues hard to detect (Yan et al., 2023). Recently, Large Language Models (LLMs) (OpenAI, 2023; Touvron et al., 2023; Wang et al., 2023b) have also been used as translation evaluators. GEMBA (Kocmi and Federmann, 2023b) presents that GPT-4 can achieve state-of-the-art performance in system-level assessment. While LLMs show remarkable performance in translation evaluation tasks, the reasons underlying their success have not been thoroughly investigated.</p>
<p>In this paper, we take the further step to investigate how LLMs leverage source and reference information in evaluating translation in both coarsegrained and fine-grained settings, bringing better understanding of the working mechanism of LLMs. Four input modes are defined, each of which exposes different information to LLMs. These include Translation-only (T), Source-Translation (S-T), Reference-Translation (R-T) and Source-Reference-Translation (S-R-T) modes. We first instruct both open and closed LLMs to predict coarsegrained quality scores using GEMBA prompt, but given different information, namely sources and references. While references significantly improve the system-level accuracy and segment-level correlations, we surprisingly find that the source information is sometimes counterproductive. For</p>
<p>example, as shown in Table 1, ChatGPT ${ }^{1}$ achieves the best performance in the R-T mode, while S-RT mode leads to performance degradation. This indicates LLMs' inability to fully leverage crosslingual capabilities when evaluating translation sentences, despite ChatGPT's impressive performance in multi-lingual or translation tasks.</p>
<p>In addition to the superficial score prediction, we further explore the fine-grained error detection to better understand the cross-lingual ability. Our fine-grained experiments confirm the above observations, where we follow AutoMQM (Fernandes et al., 2023) method to predict error spans and categories for translation sentences and study the performance of LLMs with different input modes. We also conduct a comprehensive meta-evaluation for the error span and error category, alongside executing a critical error detection task. The findings suggest that LLMs struggle to fully utilize the source information for translation evaluation.</p>
<p>Lastly, we examine the effect of fine-tuning, which makes deeper modifications to the model, with Multidimensional Quality Metrics (MQM) data (Freitag et al., 2021a). Although fine-tuning can greatly improve the model's performance of translation evaluation, the negative impact of source sentence still exists. These experimental results reveal the limitation of current LLMs on machine translation evaluation tasks and suggest a potential research direction that fully exploits the cross-lingual capability of LLMs to achieve better performance.</p>
<p>Overall, our main contributions are as follows:</p>
<ul>
<li>To the best of our knowledge, we are the first to explore the working mechanism of LLMs in evaluating translation by testing the importance of sources and reference information.</li>
<li>We conduct extensive experiments to discern the utility of source versus reference information through various aspects, suggesting that LLMs are unable to fully utilize the crosslingual capability to evaluate translation sentences.</li>
<li>We provide an in-depth analysis of translation error detection. Our code and data would be released for the research community to promote the development of LLMs in automatic translation estimation tasks. ${ }^{2}$</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>2 Related Work</h2>
<p>Automatic Translation Evaluation. Automatic evaluation has been a crucial and tough problem along with the development of machine translation. Traditional metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovic, 2015) rely heavily on n-gram matching algorithms. Despite their past success, they fail to keep pace with the increasing performance of translation models that generate semantically correct translations.</p>
<p>Neural metrics leverage the meaningful representations of pre-trained language models to evaluate translations. BertScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) calculate similarity using these representations, while generationbased methods like Prism (Thompson and Post, 2020) and BartScore (Yuan et al., 2021) assess text quality through generation tasks, conditioned on sources or references. Learned metrics such as COMET (Rei et al., 2020), BLEURT (Sellam et al., 2020), UniTE (Wan et al., 2022) and xCOMET (Guerreiro et al., 2023) apply neural networks to predict quality scores in a supervised manner. Particularly, UniTE suggests that the interaction between source and hypothesis may have an adverse effect. Although UniTE performs best with the S-R-T mode, our work shows this is not always the case for LLMs.</p>
<p>LLMs that can follow the instructions of evaluation tasks are also used to evaluate translations. GEMBA (Kocmi and Federmann, 2023b) shows that GPT-4, when asked to directly generate a quality score, is the state-of-the-art translation evaluator at that time in the system-level assessment. EAPrompt (Lu et al., 2023), AutoMQM (Fernandes et al., 2023), and GEMBA-MQM (Kocmi and Federmann, 2023a) endeavor to instruct LLMs in detecting translation errors meticulously and labelling the error category and severity. Despite their remarkable performance, the reasons underlying their success have not been thoroughly investigated.</p>
<p>LLM-based Text Evaluation. Evaluating text quality is a challenging problem even for humans. LLMs with broad world knowledge and expertise in linguistics, like ChatGPT, have been used as natural language evaluators. GPTScore (Fu et al., 2023) utilizes the conditional probability for text evaluation. Other works (Wang et al., 2023a; Liu et al., 2023a; Wang et al., 2023c; Chan et al., 2023;</p>
<table>
<thead>
<tr>
<th>Score the following translation from {src_lang} to {tgt_lang} with respect to</th>
</tr>
</thead>
<tbody>
<tr>
<td>the human reference on a continuous scale from 0 to 100 that starts on "No</td>
</tr>
<tr>
<td>meaning preserved", goes through "Some meaning preserved", then "Most meaning</td>
</tr>
<tr>
<td>preserved and few grammar mistakes", up to "Perfect meaning and grammar".</td>
</tr>
<tr>
<td>{src_lang} source: "{source}"</td>
</tr>
<tr>
<td>{tgt_lang} human reference: "{reference}"</td>
</tr>
<tr>
<td>{tgt_lang} translation: "{translation}"</td>
</tr>
<tr>
<td>Score (0-100):</td>
</tr>
</tbody>
</table>
<p>Figure 1: The GEMBA-SQM prompt template. The green parts will be included in the prompt if the reference information is given. Similarly, the red part will be in the prompt if the source is given. Detailed prompts can be found in Appendix A.</p>
<p>Liu et al., 2023b) prompt ChatGPT to generate evaluations for natural texts using various techniques. InstructScore (Xu et al., 2023) proposes an explainable metric by fine-tuning the Llama-7B with data generated from GPT-4 and self-generated outputs, proving the feasibility of using open, smaller models for evaluation purposes. This study primarily focuses on employing LLMs for assessing translations, with a major emphasis on the cross-lingual ability. However, the insights gained from our research may extend to other NLG evaluation tasks. LLMs demonstrate a greater ability to capitalize on reference information, while they may face challenges in effectively utilizing task input like source information.</p>
<h2>3 Coarse-grained Score Prediction</h2>
<p>We first investigate how LLMs leverage the source and reference information in conducting coarsegrained evaluations of translation quality via score prediction. We adopt the GEMBA-SQM prompt template <em>Kocmi and Federmann (2023b)</em>, as shown in Figure 1. The inclusion of source and reference information in the prompt varies based on the selected input mode. For instance, in the R-T mode, the source text in red is omitted from the prompt. We assess the efficacy of LLMs across four distinct input modes, examining their impact on the model’s performance.</p>
<h3>3.1 Experimental Setup</h3>
<p>Data. We use the test set from WMT22 Metric Shared Task <em>Freitag et al. (2022)</em> which contains the MQM annotated data for three translation directions: En-De, Zh-En, and En-Ru. Reference A (refA) is used as the standard reference. The quality of references can affect the performance of reference-based metrics (see Appendix C). The golden quality score is calculated from the MQM ratings annotated by humans. The weighting scheme of each error severity and category can be found in Freitag et al. (2021a).</p>
<p>Models. We evaluate both the closed model GPT-3.5-turbo and open models, including the Llama2-Chat series <em>Touvron et al. (2023)</em> and Mistral-7B-Instruct <em>Jiang et al. (2023)</em>. We only consider the chat version of these models because base models without alignment may not follow instructions according to our preliminary study. All of these models possess a certain level of translation ability in the specified language pairs <em>Zhu et al. (2023)</em>.</p>
<p>Evaluation Metrics. Following <em>Kocmi and Federmann (2023b)</em>, we use the system-level accuracy and segment-level Kendall’s $\tau$ correlation as our primary evaluation metrics, complemented by the Pearson correlation $\rho$. We use the PERM-BOTH hypothesis test with 1000 resampling runs and p=0.05 <em>Deutsch et al. (2021)</em>.</p>
<h3>3.2 Results</h3>
<p>Table 2 demonstrates the main results of the metaevaluation of the coarse-grained translation quality score, in which we include COMET-22 <em>Rei et al. (2022)</em>, BLEU and chrF as baselines.</p>
<p>One of the most surprising findings is that the RT mode is the most effective among the four modes in most cases. The R-T mode surpasses other modes in system-level accuracy and segment-level correlations across models, particularly excelling with strong models like GPT-3.5 and Llama2-70B-Chat. This suggests that the reference information can significantly enhance the evaluation accuracy, but the source information has little or no impact on the translation evaluation task. On the other hand, the numerous lower scores in S-T mode compared to T mode also indicate that LLMs do not fully utilize their cross-lingual capability and may even be confused by the source in this task. We further</p>
<table>
<thead>
<tr>
<th>Model</th>
<th></th>
<th>Mode</th>
<th>All LPs</th>
<th>En-De</th>
<th></th>
<th>Zh-En</th>
<th></th>
<th>En-Ru</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Acc.</td>
<td>$\tau$</td>
<td>$\rho$</td>
<td>$\tau$</td>
<td>$\rho$</td>
<td>$\tau$</td>
<td>$\rho$</td>
</tr>
<tr>
<td>GPT-3.5-turbo</td>
<td>T</td>
<td></td>
<td>0.759</td>
<td>0.181</td>
<td>0.153</td>
<td>0.228</td>
<td>0.157</td>
<td>0.195</td>
<td>0.169</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td></td>
<td>0.876</td>
<td>0.212</td>
<td>0.242</td>
<td>0.220</td>
<td>0.219</td>
<td>0.219</td>
<td>0.186</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td></td>
<td>0.891</td>
<td>0.284*</td>
<td>0.280</td>
<td>0.286*</td>
<td>0.230</td>
<td>0.253*</td>
<td>0.217*</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td></td>
<td>0.876</td>
<td>0.255</td>
<td>0.285</td>
<td>0.274</td>
<td>0.248*</td>
<td>0.211</td>
<td>0.196</td>
</tr>
<tr>
<td>Llama2-7B-Chat</td>
<td>T</td>
<td></td>
<td>0.620</td>
<td>0.052</td>
<td>0.036</td>
<td>0.156</td>
<td>0.195</td>
<td>0.042</td>
<td>0.054</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td></td>
<td>0.599</td>
<td>-0.010</td>
<td>-0.037</td>
<td>0.093</td>
<td>0.121</td>
<td>0.008</td>
<td>0.003</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td></td>
<td>0.788</td>
<td>0.217*</td>
<td>0.200*</td>
<td>0.284</td>
<td>0.260</td>
<td>0.213</td>
<td>0.177</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td></td>
<td>0.748</td>
<td>0.187</td>
<td>0.173</td>
<td>0.290</td>
<td>0.277*</td>
<td>0.222</td>
<td>0.196*</td>
</tr>
<tr>
<td>Llama2-13B-Chat</td>
<td>T</td>
<td></td>
<td>0.675</td>
<td>0.000</td>
<td>0.003</td>
<td>0.034</td>
<td>0.03</td>
<td>0.032</td>
<td>0.029</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td></td>
<td>0.591</td>
<td>0.041</td>
<td>0.028</td>
<td>0.056</td>
<td>0.041</td>
<td>0.084</td>
<td>0.038</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td></td>
<td>0.701</td>
<td>0.107</td>
<td>0.100</td>
<td>0.104*</td>
<td>0.097*</td>
<td>0.108</td>
<td>0.105</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td></td>
<td>0.650</td>
<td>0.108</td>
<td>0.109</td>
<td>0.053</td>
<td>0.055</td>
<td>0.108</td>
<td>0.102</td>
</tr>
<tr>
<td>Llama2-70B-Chat</td>
<td>T</td>
<td></td>
<td>0.737</td>
<td>0.148</td>
<td>0.105</td>
<td>0.215</td>
<td>0.177</td>
<td>0.220</td>
<td>0.145</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td></td>
<td>0.807</td>
<td>0.126</td>
<td>0.123</td>
<td>0.194</td>
<td>0.153</td>
<td>0.134</td>
<td>0.126</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td></td>
<td>0.887*</td>
<td>0.241*</td>
<td>0.221*</td>
<td>0.271*</td>
<td>0.228*</td>
<td>0.222</td>
<td>0.160*</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td></td>
<td>0.843</td>
<td>0.167</td>
<td>0.180</td>
<td>0.250</td>
<td>0.197</td>
<td>0.178</td>
<td>0.103</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>T</td>
<td></td>
<td>0.726</td>
<td>0.108</td>
<td>0.079</td>
<td>0.232</td>
<td>0.211</td>
<td>0.228*</td>
<td>0.160</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td></td>
<td>0.646</td>
<td>0.063</td>
<td>0.052</td>
<td>0.238</td>
<td>0.190</td>
<td>0.180</td>
<td>0.131</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td></td>
<td>0.796</td>
<td>0.123</td>
<td>0.119</td>
<td>0.228</td>
<td>0.213</td>
<td>0.158</td>
<td>0.118</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td></td>
<td>0.770</td>
<td>0.157*</td>
<td>0.143*</td>
<td>0.237</td>
<td>0.228*</td>
<td>0.170</td>
<td>0.146</td>
</tr>
<tr>
<td>COMET-22</td>
<td></td>
<td>/</td>
<td>0.839</td>
<td>0.368</td>
<td>0.512</td>
<td>0.428</td>
<td>0.585</td>
<td>0.400</td>
<td>0.469</td>
</tr>
<tr>
<td>BLEU</td>
<td></td>
<td>/</td>
<td>0.708</td>
<td>0.169</td>
<td>0.193</td>
<td>0.145</td>
<td>0.175</td>
<td>0.140</td>
<td>0.160</td>
</tr>
<tr>
<td>chrF</td>
<td></td>
<td>/</td>
<td>0.734</td>
<td>0.214</td>
<td>0.231</td>
<td>0.147</td>
<td>0.154</td>
<td>0.168</td>
<td>0.168</td>
</tr>
</tbody>
</table>
<p>Table 2: The system-level accuracy and segment-level Kendall’s $\tau$ and Pearson $\rho$ correlations of different models with different input modes on WMT22 test set. Bold scores indicate the highest values, while asterisks mark the significantly highest among the four input modes. The underlined S-T mode scores are significantly lower than the T mode scores.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Part</th>
<th>Acc.</th>
<th>En-De $\tau$</th>
<th>Zh-En $\tau$</th>
<th>En-Ru $\tau$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5-turbo</td>
<td>src</td>
<td>0.051</td>
<td>0.001</td>
<td>-0.010</td>
<td>-0.009</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.066</td>
<td>0.073</td>
<td>0.056</td>
<td>0.025</td>
</tr>
<tr>
<td>Llama2-7B-Chat</td>
<td>src</td>
<td>-0.030</td>
<td>-0.046</td>
<td>-0.028</td>
<td>-0.012</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.159</td>
<td>0.181</td>
<td>0.163</td>
<td>0.193</td>
</tr>
<tr>
<td>Llama2-13B-Chat</td>
<td>src</td>
<td>-0.067</td>
<td>0.021</td>
<td>-0.014</td>
<td>0.026</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.043</td>
<td>0.087</td>
<td>0.034</td>
<td>0.050</td>
</tr>
<tr>
<td>Llama2-70B-Chat</td>
<td>src</td>
<td>0.013</td>
<td>-0.048</td>
<td>-0.021</td>
<td>-0.065</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.093</td>
<td>0.067</td>
<td>0.056</td>
<td>0.023</td>
</tr>
<tr>
<td>Mistral-7B-Instruct</td>
<td>src</td>
<td>-0.053</td>
<td>-0.005</td>
<td>0.008</td>
<td>-0.018</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.097</td>
<td>0.055</td>
<td>-0.002</td>
<td>-0.040</td>
</tr>
</tbody>
</table>
<p>Table 3: The Shapley values that quantify the impact of the source and reference parts on the system-level accuracy and Kendall’s $\tau$ correlations in the score prediction task across different language pairs.
calculate the Shapley values <em>Shapley (1953)</em> that assess the contributions of the source and reference part, as shown in Table 3. Higher number means more positive influence, and vice versa. A positive number means that the information has a positive effect while a negative number means a negative effect. The way to calculate Shapley Values can be found in Appendix D. The reference parts contribute more than the source parts, which can even have negative impacts.</p>
<p>There is another unexpected observation that the T mode can achieve much better performance than a random guess. We posit that LLMs evaluate translations solely based on fluency, which is positively correlated to the translation quality. Our further analysis by log-probability in Appendix E supports this hypothesis.</p>
<p>Compared to baseline metrics, our findings align with those of <em>Kocmi and Federmann (2023b)</em>. LLMs are better at system-level evaluation but are inferior at segment-level correlations than COMET22. Metrics based on strong LLMs outperform both BLEU and chrF. COMET-22 is built upon the pre-trained encoder language model XLM-R <em>Conneau et al. (2020)</em> and it is an ensemble between several models with different input. It’s sequence tagger performs better with the S-R-T mode at the segment level. We suspect that the underlying mechanism of encoder-only models may differ from decoder-only models. Moreover, COMET is fine-tuned with a mount of task-specific data including both the source and the reference. This may also enhance the performance of COMET on this task.</p>
<table>
<thead>
<tr>
<th>{src_lang} source: "{source_i}"</th>
<th>x N</th>
</tr>
</thead>
<tbody>
<tr>
<td>{tgt_lang} human reference: "{reference_i}"</td>
<td></td>
</tr>
<tr>
<td>{tgt_lang} translation: "{translation_i}"</td>
<td></td>
</tr>
<tr>
<td>{error span1}-{error_category1}/{error_severity1}; {error_span2}-...</td>
<td></td>
</tr>
<tr>
<td>{src lang} source: "{source}"</td>
<td></td>
</tr>
<tr>
<td>{tgt lang} human reference: "{reference}"</td>
<td></td>
</tr>
<tr>
<td>{tgt lang} translation: "{translation}"</td>
<td></td>
</tr>
<tr>
<td>{errors:}</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 2: The AutoMQM prompt template. The green parts and red parts are included according to whether the related information is given. The yellow part is also determined by the input mode. The text in the shaded area is an in-context demonstration, followed by the test sample. Detailed prompts can be found in Appendix B.</p>
<h2>4 Fine-grained Error Detection</h2>
<p>While coarse-grained scoring methods have demonstrated their potential, recent innovations like AutoMQM and GEMBA-MQM are eliciting the capabilities of LLMs through the use of specialized prompts, leading to more refined and interpretable results. We further dive into studying how well LLMs leverage the different information with finegrained methods.</p>
<p>We adopt the AutoMQM prompt template [fernandes2023automated], as illustrated in Figure 2. The content in the yellow part varies depending on the input mode. Our assessment encompasses three perspectives: MQM scores, error spans, and error categories, hence offering a comprehensive diagnosis of the model’s predictions.</p>
<h3>4.1 Experimental Setup</h3>
<p>Data. We sample a portion of the WMT22 test set as our test set due to limited budgets (see Appendix F). Specifically, we uniformly sample 200 source sentences and all corresponding system outputs from the test set. There are 16 systems with MQM scores in the En-De and Zh-En directions, resulting in a total of 3200 samples for each direction. Following [fernandes2023automated], the in-context demonstrations are sampled from the data in WMT21 Metric Shared Task [freitag2021metr]. The number of in-context demonstrations is 4 and stratified sampling with a set of rejection criteria is used. Since there are no MQM ratings for the En-Ru direction in the WMT21 dataset, we only assess the other two directions.</p>
<p>Models. We evaluate the GPT-3.5-turbo and the Llama2 base series. In our preliminary study, the Llama2 chat models cannot follow the output format in this prompt. Therefore, we decide to assess the base models only. All models in this experiment generate text using greedy decoding.</p>
<p>Meta Evaluation. Based on the identified error categories and severity, we compute an MQM score for each sample according to Google’s MQM error weighting [freitag2021m2]. Since we do not predict sub-categories, we only assign a score of $-5$ for a major error and $-1$ for a minor error. We adopt the previous metrics to evaluate the MQM scores.</p>
<p>We also assess the quality of the identified error spans. Similar to [fernandes2023automated], we calculate the precision, recall, F1 score, and Matthews Correlation Coefficient (MCC) for the predicted error spans. In particular, given the gold error spans $S=\left{e_{1}, \ldots, e_{n}\right}, e_{j}=\left{w_{i}, w_{i+1}, \ldots\right}$ denotes each error span containing the wrong words, where $w_{i}$ is the $i$-th word in the sentence. The span of each error is $P\left(e_{j}\right)=\left{i \mid w_{i} \in e_{j}\right}$. Then we count the span overlap based on the set $P(S)=\bigcup_{j=1}^{n} P\left(e_{j}\right)$. The span precision (SP) and span recall (SR) of the predicted error spans $\hat{S}$ are defined as follows:</p>
<p>$$
\begin{aligned}
&amp; \mathrm{SP}=\frac{|P(S) \cap P(\hat{S})|}{|P(\hat{S})|} \
&amp; \mathrm{SR}=\frac{|P(S) \cap P(\hat{S})|}{|P(S)|}
\end{aligned}
$$</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Mode</th>
<th>2 LPs</th>
<th>En-De</th>
<th>Zh-En</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Acc.</td>
<td>$\tau$</td>
<td>$\rho$</td>
</tr>
<tr>
<td>AutoMQM</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3.5-turbo</td>
<td>T</td>
<td>0.757</td>
<td>0.221</td>
<td>0.283</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.751</td>
<td>0.150</td>
<td>0.222</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.858</td>
<td>0.275</td>
<td>0.331</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.769</td>
<td>0.284</td>
<td>0.349</td>
</tr>
<tr>
<td>Llama2-7B</td>
<td>T</td>
<td>0.556</td>
<td>0.077</td>
<td>0.111</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.592</td>
<td>0.071</td>
<td>0.073</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.527</td>
<td>0.077</td>
<td>0.102</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.533</td>
<td>0.063</td>
<td>0.075</td>
</tr>
<tr>
<td>Llama2-13B</td>
<td>T</td>
<td>0.544</td>
<td>0.078</td>
<td>0.110</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.515</td>
<td>0.063</td>
<td>0.060</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.533</td>
<td>0.083</td>
<td>0.086</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.562</td>
<td>0.049</td>
<td>0.036</td>
</tr>
<tr>
<td>Llama2-70B</td>
<td>T</td>
<td>0.586</td>
<td>0.134</td>
<td>0.182</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.633</td>
<td>0.135</td>
<td>0.206</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.627</td>
<td>0.200</td>
<td>0.270</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.669</td>
<td>0.200</td>
<td>0.237</td>
</tr>
<tr>
<td>Mistral-7B</td>
<td>T</td>
<td>0.444</td>
<td>0.109</td>
<td>0.136</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.538</td>
<td>0.088</td>
<td>0.102</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.604</td>
<td>0.143</td>
<td>0.185</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.586</td>
<td>0.108</td>
<td>0.112</td>
</tr>
<tr>
<td>GEMBA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3.5-turbo</td>
<td>T</td>
<td>0.728</td>
<td>0.264</td>
<td>0.272</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.852</td>
<td>0.247</td>
<td>0.226</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.852</td>
<td>0.273</td>
<td>0.290</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.828</td>
<td>0.284</td>
<td>0.299</td>
</tr>
<tr>
<td>Llama2-70B-Chat</td>
<td>T</td>
<td>0.698</td>
<td>0.150</td>
<td>0.114</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.775</td>
<td>0.161</td>
<td>0.117</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.828</td>
<td>0.262</td>
<td>0.222</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.769</td>
<td>0.198</td>
<td>0.194</td>
</tr>
<tr>
<td>COMET-22</td>
<td>/</td>
<td>0.852</td>
<td>0.398</td>
<td>0.515</td>
</tr>
<tr>
<td>BLEU</td>
<td>/</td>
<td>0.556</td>
<td>0.167</td>
<td>0.212</td>
</tr>
<tr>
<td>chrF</td>
<td>/</td>
<td>0.592</td>
<td>0.217</td>
<td>0.267</td>
</tr>
</tbody>
</table>
<p>Table 4: The system-level accuracy and segment-level Kendall’s $\tau$ and Pearson $\rho$ correlations of AutoMQM with different models. All of the models use the AutoMQM prompt except the last five. The highest scores of different input modes of each model are in bold.</p>
<p>The span F1 score (SF1) is the harmonic mean of SP and SR. Since major errors contribute most to the quality score, we calculate the major precision (MP) and major recall (MR) as follows:</p>
<p>$\mathrm{MP}=\frac{\left|P\left(S_{m a j}\right) \cap P\left(\hat{S}<em a="a" j="j" m="m">{m a j}\right)\right|}{\left|P\left(\hat{S}</em>$ (3)
$\mathrm{MR}=\frac{\left|P\left(S_{m a j}\right) \cap P\left(\hat{S}}\right)\right|<em a="a" j="j" m="m">{m a j}\right)\right|}{\left|P\left(S</em>$ (4)}\right)\right|</p>
<p>where $S_{m a j} \subseteq S$ is the subset only containing major errors, and major F1 (MF1) score is the harmonic mean. Note that our MR is slightly different from Fernandes et al.'s (2023) MR, which takes into account both minor and major prediction errors. In this way, we can better evaluate the performance of predicting the major errors.</p>
<p>In addition, we calculate the precision, recall, and F1 score for the error category. Specifically, let $\operatorname{Cat}(e)$ denote the error category, and $\operatorname{Cat}(S)=$ $\left(\operatorname{Cat}\left(e_{1}\right), \ldots, \operatorname{Cat}\left(e_{n}\right)\right)$ denote the gold labels. The function $\operatorname{Count}(S, c)$ calculates the count of occurrences of category $c$ within $\operatorname{Cat}(S)$. The precision and recall of the category $c$ are defined as:</p>
<p>$$
\begin{aligned}
&amp; \mathrm{P}<em _mathrm_c="\mathrm{c">{\mathrm{c}}=\frac{\min (\operatorname{Count}(S, c), \operatorname{Count}(\hat{S}, c))}{\operatorname{Count}(\hat{S}, c)} \
&amp; \mathrm{R}</em>
\end{aligned}
$$}}=\frac{\min (\operatorname{Count}(S, c), \operatorname{Count}(\hat{S}, c))}{\operatorname{Count}(S, c)</p>
<p>And the $\mathrm{F} 1_{c}$ score is the harmonic mean of the precision and recall. Here we ignore the sub-categories since AutoMQM does not predict sub-categories. Note that these three scores only consider the error categories and do not necessitate the correct identification of error positions for simplicity.</p>
<h3>4.2 Results</h3>
<p>Score Meta-evaluation. Table 4 shows that GPT3.5, Llama2-70B, and Mistral-7B achieve the best or second-best score with the R-T mode, suggesting a limitation in their ability to employ cross-lingual capabilities for this task. However, while the T mode appears to yield the strong results for weak models such as Llama2-13B and Llama2-7B, it is important to note that their overall performance remains substantially low. This leads to the hypothesis that these weak models may not fully understand the task, thereby failing to effectively identify errors. Nevertheless, the contribution of the reference is much larger than that of the source in most cases, as shown in Table 6. These results also indicate the limitation of cross-lingual capabilities of LLMs to evaluate translations.</p>
<p>Besides, we also find something different with the conclusions of Fernandes et al. (2023). The AutoMQM prompt outperforms the GEMBA-SQM prompt when using GPT-3.5-Turbo, which is consistent with the previous work using PaLM2 (Anil et al., 2023), but this trend does not extend to the Llama2 series (the 7B and 13B models have a similar phenomenon). Due to the lack of training details on PaLM2, we speculate that PaLM2 may have a larger model scale and enhanced multilingual capabilities than the Llama2 series.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Mode</th>
<th>SP / SR / SF1</th>
<th>MP / MR / MF1</th>
<th>MCC</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5-turbo</td>
<td>T</td>
<td>0.162 / 0.375 / 0.227</td>
<td>0.122 / 0.155 / 0.136</td>
<td>0.153</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.237 / 0.207 / 0.221</td>
<td>0.192 / 0.192 / 0.192</td>
<td>0.150</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.239 / 0.378 / 0.293</td>
<td>0.202 / 0.344 / 0.254</td>
<td>0.208</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.214 / 0.354 / 0.267</td>
<td>0.179 / 0.348 / 0.236</td>
<td>0.180</td>
</tr>
<tr>
<td>Llama2-7B</td>
<td>T</td>
<td>0.110 / 0.520 / 0.181</td>
<td>0.056 / 0.414 / 0.098</td>
<td>0.057</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.085 / 0.329 / 0.135</td>
<td>0.041 / 0.243 / 0.070</td>
<td>0.061</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.112 / 0.309 / 0.165</td>
<td>0.056 / 0.219 / 0.090</td>
<td>0.045</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.092 / 0.260 / 0.136</td>
<td>0.048 / 0.201 / 0.077</td>
<td>0.056</td>
</tr>
<tr>
<td>Llama2-13B</td>
<td>T</td>
<td>0.113 / 0.604 / 0.191</td>
<td>0.055 / 0.503 / 0.100</td>
<td>0.079</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.084 / 0.448 / 0.141</td>
<td>0.037 / 0.351 / 0.067</td>
<td>0.051</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.119 / 0.433 / 0.186</td>
<td>0.064 / 0.391 / 0.110</td>
<td>0.071</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.098 / 0.405 / 0.158</td>
<td>0.049 / 0.360 / 0.086</td>
<td>0.053</td>
</tr>
<tr>
<td>Llama2-70B</td>
<td>T</td>
<td>0.107 / 0.665 / 0.185</td>
<td>0.056 / 0.646 / 0.106</td>
<td>0.065</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.104 / 0.592 / 0.177</td>
<td>0.058 / 0.541 / 0.101</td>
<td>0.072</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.124 / 0.631 / 0.207</td>
<td>0.071 / 0.576 / 0.127</td>
<td>0.109</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.121 / 0.659 / 0.204</td>
<td>0.072 / 0.577 / 0.128</td>
<td>0.111</td>
</tr>
<tr>
<td>Mistral-7B</td>
<td>T</td>
<td>0.108 / 0.679 / 0.186</td>
<td>0.054 / 0.639 / 0.099</td>
<td>0.069</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.101 / 0.569 / 0.171</td>
<td>0.051 / 0.546 / 0.094</td>
<td>0.056</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.108 / 0.537 / 0.179</td>
<td>0.056 / 0.524 / 0.101</td>
<td>0.058</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.105 / 0.545 / 0.177</td>
<td>0.052 / 0.520 / 0.094</td>
<td>0.055</td>
</tr>
</tbody>
</table>
<p>Table 5: The results of span meta-evaluation. All of the scores are micro-averaged across two language directions. The highest F1 scores and MCC are in bold.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Part</th>
<th>Acc.</th>
<th>En-De $\tau$</th>
<th>Zh-En $\tau$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5-turbo</td>
<td>src</td>
<td>-0.047</td>
<td>-0.031</td>
<td>0.009</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.059</td>
<td>0.094</td>
<td>0.079</td>
</tr>
<tr>
<td>Llama2-7B</td>
<td>src</td>
<td>0.021</td>
<td>-0.010</td>
<td>-0.026</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>-0.044</td>
<td>-0.004</td>
<td>0.006</td>
</tr>
<tr>
<td>Llama2-13B</td>
<td>src</td>
<td>0.000</td>
<td>-0.025</td>
<td>-0.010</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.018</td>
<td>-0.004</td>
<td>-0.010</td>
</tr>
<tr>
<td>Llama2-70B</td>
<td>src</td>
<td>0.045</td>
<td>0.001</td>
<td>0.032</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.039</td>
<td>0.066</td>
<td>0.088</td>
</tr>
<tr>
<td>Mistral-7B</td>
<td>src</td>
<td>0.038</td>
<td>-0.028</td>
<td>-0.003</td>
</tr>
<tr>
<td></td>
<td>ref</td>
<td>0.104</td>
<td>0.027</td>
<td>0.006</td>
</tr>
</tbody>
</table>
<p>Table 6: The Shapley values in the error detection task across different language pairs.</p>
<p>Span Meta-evaluation. The results of the span meta-evaluation presented in Table 5 demonstrate a similar pattern to the score meta-evaluation. GPT-3.5-Turbo and Llama2-70B have better performance when using the R-T mode, while small models are not stable. Overall, the performance of the R-T mode still surpasses that of both the S-T and S-R-T modes. This suggests that the limitations in the cross-lingual capabilities of LLMs also exist in word-level translation evaluation tasks.</p>
<p>Unexpectedly, it appears that identifying major errors poses a greater challenge. The MF1 scores are consistently lower than the corresponding SF1 scores across all tested models. All of these models exhibit an apparently low level of performance, suggesting substantial room for progress in the error span prediction.</p>
<p>Category Meta-evaluation. Table 7 presents the outcomes of the category meta-evaluation. When scrutinizing the F1 scores across the models for each designated category, the overall performance is very poor despite the simplification. Notably, the scores of the Accuracy category surpass those of other categories, with GPT-3.5 demonstrating a relative advantage over the Llama2 models, particularly when provided with a reference.</p>
<p>The AutoMQM prompt lacks explicit definitions for each category, requiring the models to infer the meaning of each from the demonstrations. The Accuracy category predominates all other categories except the No-Error category. This prevalence likely biases the model towards a more frequent prediction of Accuracy errors. The remaining categories exhibit diminished F1 scores, which are attributed to the models’ limited understanding of the inherent semantics associated with each category due to their low frequency. This pattern persists irrespective of different models or input modes.
"No-Error" is a special category, as it is mutually exclusive with other error categories. For analytical simplicity, it is treated analogously to a category, with F1 scores computed accordingly. In this regard, GPT-3.5 exhibits a pronounced competence in identifying error-free samples in stark contrast to the Llama2 models. Weak models exhibit a propensity for overestimating the presence of errors.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Mode</th>
<th>Accuracy</th>
<th>Fluency</th>
<th>Terminology</th>
<th>Style</th>
<th>Locale</th>
<th>No-Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5-turbo</td>
<td>T</td>
<td>0.31/0.25/0.28</td>
<td>0.21/0.14/0.17</td>
<td>0.03/0.05/0.04</td>
<td>0.13/0.29/0.18</td>
<td>0.00/0.00/0.00</td>
<td>0.57/0.80/0.67</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.45/0.19/0.26</td>
<td>0.27/0.06/0.10</td>
<td>0.04/0.02/0.02</td>
<td>0.15/0.05/0.07</td>
<td>0.00/0.00/0.00</td>
<td>0.54/0.94/0.69</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.43/0.41/0.42</td>
<td>0.27/0.09/0.13</td>
<td>0.03/0.04/0.03</td>
<td>0.17/0.17/0.17</td>
<td>0.00/0.00/0.00</td>
<td>0.60/0.84/0.70</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.43/0.41/0.42</td>
<td>0.25/0.08/0.13</td>
<td>0.04/0.05/0.05</td>
<td>0.18/0.17/0.18</td>
<td>0.00/0.00/0.00</td>
<td>0.61/0.81/0.69</td>
</tr>
<tr>
<td>Llama2-7B</td>
<td>T</td>
<td>0.20/0.66/0.31</td>
<td>0.16/0.09/0.12</td>
<td>0.01/0.11/0.03</td>
<td>0.12/0.12/0.12</td>
<td>0.05/0.04/0.05</td>
<td>0.57/0.04/0.08</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.21/0.65/0.32</td>
<td>0.16/0.08/0.10</td>
<td>0.01/0.11/0.02</td>
<td>0.12/0.12/0.12</td>
<td>0.02/0.04/0.03</td>
<td>0.54/0.06/0.10</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.24/0.50/0.32</td>
<td>0.19/0.08/0.12</td>
<td>0.01/0.08/0.02</td>
<td>0.12/0.10/0.11</td>
<td>0.03/0.02/0.02</td>
<td>0.56/0.22/0.32</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.22/0.52/0.31</td>
<td>0.16/0.06/0.08</td>
<td>0.02/0.11/0.03</td>
<td>0.10/0.08/0.09</td>
<td>0.02/0.02/0.02</td>
<td>0.54/0.21/0.30</td>
</tr>
<tr>
<td>Llama2-13B</td>
<td>T</td>
<td>0.20/0.34/0.25</td>
<td>0.13/0.47/0.20</td>
<td>0.01/0.04/0.01</td>
<td>0.09/0.20/0.12</td>
<td>0.00/0.00/0.00</td>
<td>0.61/0.03/0.05</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.20/0.39/0.27</td>
<td>0.12/0.40/0.19</td>
<td>0.01/0.06/0.02</td>
<td>0.08/0.14/0.11</td>
<td>0.00/0.00/0.00</td>
<td>0.51/0.03/0.05</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.25/0.34/0.29</td>
<td>0.16/0.39/0.22</td>
<td>0.02/0.06/0.03</td>
<td>0.11/0.15/0.13</td>
<td>0.00/0.00/0.00</td>
<td>0.55/0.14/0.22</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.24/0.37/0.29</td>
<td>0.15/0.37/0.21</td>
<td>0.01/0.05/0.02</td>
<td>0.10/0.10/0.10</td>
<td>0.00/0.00/0.00</td>
<td>0.53/0.13/0.20</td>
</tr>
<tr>
<td>Llama2-70B</td>
<td>T</td>
<td>0.17/0.49/0.26</td>
<td>0.12/0.36/0.18</td>
<td>0.01/0.08/0.02</td>
<td>0.08/0.08/0.08</td>
<td>0.00/0.00/0.00</td>
<td>0.65/0.03/0.05</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.19/0.53/0.28</td>
<td>0.12/0.34/0.18</td>
<td>0.01/0.08/0.02</td>
<td>0.09/0.11/0.10</td>
<td>0.02/0.04/0.03</td>
<td>0.70/0.08/0.15</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.22/0.56/0.32</td>
<td>0.13/0.30/0.18</td>
<td>0.02/0.06/0.03</td>
<td>0.11/0.13/0.12</td>
<td>0.05/0.02/0.03</td>
<td>0.70/0.14/0.23</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.23/0.54/0.32</td>
<td>0.12/0.36/0.18</td>
<td>0.01/0.04/0.01</td>
<td>0.10/0.13/0.11</td>
<td>0.02/0.02/0.02</td>
<td>0.74/0.13/0.22</td>
</tr>
<tr>
<td>Mistral-7B</td>
<td>T</td>
<td>0.16/0.48/0.24</td>
<td>0.11/0.38/0.17</td>
<td>0.02/0.03/0.02</td>
<td>0.08/0.13/0.10</td>
<td>0.05/0.04/0.04</td>
<td>0.63/0.03/0.06</td>
</tr>
<tr>
<td></td>
<td>S-T</td>
<td>0.17/0.56/0.26</td>
<td>0.12/0.31/0.17</td>
<td>0.00/0.00/0.00</td>
<td>0.07/0.12/0.09</td>
<td>0.04/0.02/0.03</td>
<td>0.54/0.04/0.07</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.20/0.52/0.29</td>
<td>0.14/0.34/0.20</td>
<td>0.00/0.00/0.00</td>
<td>0.08/0.10/0.09</td>
<td>0.04/0.02/0.03</td>
<td>0.58/0.10/0.16</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.20/0.53/0.29</td>
<td>0.14/0.32/0.20</td>
<td>0.00/0.00/0.00</td>
<td>0.09/0.09/0.09</td>
<td>0.05/0.02/0.03</td>
<td>0.60/0.09/0.16</td>
</tr>
</tbody>
</table>
<p>Table 7: The results of the category evaluation. The numbers in each cell are in the format of $P_{c} / R_{c} / F 1_{c}$, where $c$ is the category in the column header. Locale stands for the Locale Convention error category.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>SP</th>
<th>SR</th>
<th>SF1</th>
<th>accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5-turbo</td>
<td>0.186</td>
<td>0.626</td>
<td>0.287</td>
<td>0.553</td>
</tr>
<tr>
<td>Llama2-7B</td>
<td>0.109</td>
<td>0.296</td>
<td>0.160</td>
<td>0.293</td>
</tr>
<tr>
<td>Llama2-13B</td>
<td>0.137</td>
<td>0.500</td>
<td>0.215</td>
<td>0.513</td>
</tr>
<tr>
<td>Llama2-70B</td>
<td>0.147</td>
<td>0.836</td>
<td>0.250</td>
<td>0.753</td>
</tr>
</tbody>
</table>
<p>Table 8: The results of critical error detection. Each experiment is run with three different random seeds.</p>
<h3>4.3 Critical Error Detection</h3>
<p>To better understand the cross-lingual ability of LLMs, we investigate whether they can detect the critical translation errors that are easy to discover. We extract 50 samples from the test set of WMT22’s Critical Error Detection Task (Zerva et al., 2022). Specifically, we only use the “BAD” samples from the En-De subset and manually label one critical error span for each sample. The samples with omission errors are excluded, keeping the addition errors, named entity errors, negation errors and number errors. We use the AutoMQM prompt with the S-T mode to determine whether LLMs can utilize the source information to identify the critical error spans. SP, SR, SF1 and accuracy are used to measure the performance. The accuracy here is calculated as the ratio of how many critical error spans are completely identified.</p>
<p>The results are demonstrated in Table 8. Strong models like GPT-3.5 and Llama2-70B can identify most errors. However, the precision is very low, indicating that they tend to over-predict errors. On the other hand, there remains a noticeable probability, exceeding 25%, that they may overlook crucial information in the source. This suggests that LLMs cannot fully utilize the source information, leading to the failure of error detection. There some cases shown in Figure 3.</p>
<table>
<thead>
<tr>
<th>Direction</th>
<th>#S-T</th>
<th>#R-T</th>
<th>#S-R-T</th>
<th>Total</th>
<th>No-error%</th>
</tr>
</thead>
<tbody>
<tr>
<td>En-De</td>
<td>2940</td>
<td>2993</td>
<td>3026</td>
<td>8959</td>
<td>58.7%</td>
</tr>
<tr>
<td>Zh-En</td>
<td>3342</td>
<td>3204</td>
<td>3204</td>
<td>9750</td>
<td>29.3%</td>
</tr>
<tr>
<td>En-De</td>
<td>1712</td>
<td>1736</td>
<td>1812</td>
<td>5260</td>
<td>29.7%</td>
</tr>
</tbody>
</table>
<p>Table 9: The statistics of the training set. #S-T/#R-T/#S-R-T is the number of samples in this mode after random assignment. No-error% is the No-error rate of samples. En-De${}^{\dagger}$ is the down-sampled subset.</p>
<h2>5 Fine-tuning LLMs with MQM data</h2>
<p>We further investigate the effect of fine-tuning an open LLM with task-specific data to determine if it can eliminate the above limitation.</p>
<h3>5.1 Experimental Setup</h3>
<p>In this experiment, we integrate the En-De and Zh-En samples from the WMT21 dataset to form the supervised training set, and employ the WMT22 dataset as the test set. The organization of the training samples adheres to the Alpaca (Taori et al., 2023) instruction template, where the instruction</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Mode</th>
<th>All LPs</th>
<th>En-De</th>
<th></th>
<th>Zh-En</th>
<th></th>
<th>En-Ru</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Acc.</td>
<td>$\tau$</td>
<td>$\rho$</td>
<td>$\tau$</td>
<td>$\rho$</td>
<td>$\tau$</td>
<td>$\rho$</td>
</tr>
<tr>
<td>Fine-tuned Llama2-7B</td>
<td>S-T</td>
<td>0.832</td>
<td>0.072</td>
<td>0.080</td>
<td>0.368</td>
<td>0.453</td>
<td>0.181</td>
<td>0.221</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.847</td>
<td>0.139*</td>
<td>0.199*</td>
<td>0.407</td>
<td>0.492</td>
<td>0.259*</td>
<td>0.319*</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.847</td>
<td>0.114</td>
<td>0.145</td>
<td>0.401</td>
<td>0.488</td>
<td>0.228</td>
<td>0.289</td>
</tr>
<tr>
<td>Fine-tuned Llama2-7B ${ }^{\dagger}$</td>
<td>S-T</td>
<td>0.828</td>
<td>0.153</td>
<td>0.161</td>
<td>0.366</td>
<td>0.455</td>
<td>0.208</td>
<td>0.236</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>0.818</td>
<td>0.229*</td>
<td>0.218*</td>
<td>0.412*</td>
<td>0.506</td>
<td>0.278*</td>
<td>0.300</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>0.828</td>
<td>0.199</td>
<td>0.201</td>
<td>0.403</td>
<td>0.503</td>
<td>0.242</td>
<td>0.285</td>
</tr>
<tr>
<td>GEMBA-Llama2-7B-Chat</td>
<td>R-T</td>
<td>0.788</td>
<td>0.217</td>
<td>0.200</td>
<td>0.284</td>
<td>0.260</td>
<td>0.213</td>
<td>0.177</td>
</tr>
<tr>
<td>GEMBA-GPT-3.5-turbo</td>
<td>R-T</td>
<td>0.891</td>
<td>0.284</td>
<td>0.280</td>
<td>0.286</td>
<td>0.230</td>
<td>0.253</td>
<td>0.217</td>
</tr>
</tbody>
</table>
<p>Table 10: The system-level accuracy and segment-level Kendall’s $\tau$ and Pearson $\rho$ correlations of the fine-tuned Llama2. Fine-tuned Llama2-7B ${ }^{\dagger}$ uses the down-sampled training set. Starred values are significantly better than those of the other two input modes.
and the input parts are identical with the AutoMQM prompt. Regarding the output part, our format mirrors that of InstructScore (Xu et al., 2023), with the exception of the explanation component. ${ }^{4}$ Note that we also ignore the error sub-category here. To accommodate the three input modes, i.e. the S-T, R-T and S-R-T mode, each training sample is randomly assigned with one mode. The statistics of the training set in can be found in Table 9. We fine-tune the Llama2-7B base model for 3 epochs, using a decayed learning rate of $2 \mathrm{e}-5$ and a batch size of 128 .</p>
<h3>5.2 Results</h3>
<p>We have some interesting findings in Table 10. Firstly, the performance of the R-T mode remains significantly superior to that of the other two modes, indicating that the model still cannot make full use of the source information after naive fine-tuning. Secondly, The overall performance of the finetuned Llama2 is stronger than GEMBA-Llama2-7B-Chat, proving the effectiveness of further finetuning for this task. However, the distribution of the training data is crucial. The fine-tuned model outperforms GPT-3.5-turbo on Zh-En segment-level correlations with the proper data distribution. On the contrary, the performance on En-De direction degrades due to the extremely imbalanced En-De training data, where samples with No-error dominates, as shown in Table 9. To mitigate the problem of unbalanced distribution, we down-sample the No-error samples in the En-De corpus and keep about $30 \%$ No-error samples. The Zh-En samples remain unchanged. Fine-tuning with more balanced data can effectively enhance the En-De segment-level correlations, as shown in Table 10.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The change of En-De data also brings benefits to other directions. Lastly, it is noteworthy that the evaluation capability, to some extent, can be transferred to the language pair not encountered in the fine-tuning stage. The fine-tuned model achieves even higher correlations without seeing any En-Ru samples, compared to GEMBA-Llama2-7B-Chat.</p>
<h2>6 Conclusion</h2>
<p>We empirically analyze how well LLMs incorporate the source and reference information for translation evaluation, comparing the effectiveness of open and closed LLMs through prompting and finetuning. Our results reveal their limitations in fully exploiting the cross-lingual capability for the task, with the inclusion of source information even occasionally proving detrimental to performance. Furthermore, our work contributes a detailed metaevaluation of spans and categories with the finegrained evaluation method, along with the critical error detection task. These findings not only furnish insights into the current capabilities and limitations of LLMs in translation evaluation, but also establish a foundational basis for subsequent scholarly endeavors. In the future, we would like to extend these analyses to other NLG evaluation tasks.</p>
<h2>7 Limitations</h2>
<p>We discuss the limitations and future research directions of our work in this section.</p>
<ul>
<li>In experiments, we mainly use the prompts from the previous works (Fernandes et al., 2023; Kocmi and Federmann, 2023b). These prompts are may not the best prompt that can fully elicit the ability of LLMs on this task. It's important to note that our conclusion may</li>
</ul>
<p>not apply to all prompts. However, the current popular prompts that simply ask LLMs to predict scores or fine-grained errors can be negatively affected by the source. Designing prompts that can better elicit the cross-lingual capability of LLMs is a topic for future research.</p>
<ul>
<li>We do not evaluate other closed LLMs like GPT-4 due to the limited resources. The tokens consumed in our experiments are recorded in the Appendix F. We leave assessing additional LLMs with more test data as future work.</li>
<li>We do not dive into how to better fine-tune the open model. More carefully designed training data or pipelines may bring greater improvement for this task.</li>
<li>In this work, we only focus on the translation evaluation task which is a sub-field of NLG evaluation tasks. Future research should focus on extending these analyses to other NLG evaluation tasks.</li>
</ul>
<h2>Acknowledgements</h2>
<p>We would like to thank the anonymous reviewers for their insightful comments. Shujian Huang and Zhirui Zhang are the corresponding authors. This work is supported by National Science Foundation of China (No. 62376116, 62176120), the Liaoning Provincial Research Foundation for Basic Research (No. 2022-KF-26-02), research project of Nanjing University-China Mobile Joint Institute.</p>
<h2>References</h2>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El Shafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin Cherry, Christopher A. ChoquetteChoo, Aakanksha Chowdhery, C Crépy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, M. C. D'iaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag, Xavier García, Sebastian Gehrmann, Lucas González, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou,</p>
<p>Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report. ArXiv, abs/2305.10403.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, pages 65-72. Association for Computational Linguistics.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shan Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. ArXiv, abs/2308.07201.</p>
<p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8440-8451. Association for Computational Linguistics.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. 2021. A statistical analysis of summarization evaluation metrics using resampling methods. Transactions of the Association for Computational Linguistics, 9:11321146.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for</p>
<p>Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics.</p>
<p>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. ArXiv, abs/2308.07286.</p>
<p>Markus Freitag, George F. Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Trans. Assoc. Comput. Linguistics, 9:1460-1474.</p>
<p>Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation, pages 578-628, Singapore. Association for Computational Linguistics.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George F. Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022, pages 46-68. Association for Computational Linguistics.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George F. Foster, Alon Lavie, and Ondrej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021, pages 733-774. Association for Computational Linguistics.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. ArXiv, abs/2302.04166.</p>
<p>Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luísa Coheur, Pierre Colombo, and André Martins. 2023. xcomet: Transparent machine translation evaluation through fine-grained error detection. ArXiv, abs/2310.10482.</p>
<p>Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. 2018. Achieving human parity on automatic chinese to english news translation. arXiv preprint arXiv:1803.05567.</p>
<p>Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv, abs/2310.06825.</p>
<p>Tom Kocmi and Christian Federmann. 2023a. GEMBAMQM: detecting translation quality error spans with GPT-4. ArXiv, abs/2310.13988.</p>
<p>Tom Kocmi and Christian Federmann. 2023b. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, EAMT 2023, Tampere, Finland, 12-15 June 2023, pages 193-203. European Association for Machine Translation.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: Nlg evaluation using gpt-4 with better human alignment. ArXiv, abs/2303.16634.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. ArXiv, abs/1907.11692.</p>
<p>Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2023b. Calibrating llmbased evaluator. ArXiv, abs/2309.13308.</p>
<p>Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt. ArXiv, abs/2303.13809.</p>
<p>OpenAI. 2023. GPT-4 technical report. ArXiv, abs/2303.08774.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>Maja Popovic. 2015. chrf: character n-gram f-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lisbon, Portugal, pages 392-395. The Association for Computer Linguistics.</p>
<p>Ricardo Rei, José G. C. de Souza, Duarte M. Alves, Chrysoula Zerva, Ana C. Farinha, Taisiya Glushkova, Alon Lavie, Luísa Coheur, and André F. T. Martins. 2022. COMET-22: unbabel-ist 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation, WMT</p>
<p>2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022, pages 578-585. Association for Computational Linguistics.</p>
<p>Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 2685-2702. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. BLEURT: learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7881-7892. Association for Computational Linguistics.</p>
<p>Lloyd S. Shapley. 1953. A value for n-person games. Contributions to the Theory of Games, pages 307317.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 90-121. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural</p>
<p>Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek F. Wong, and Lidia S. Chao. 2022. Unite: Unified translation evaluation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8117-8127. Association for Computational Linguistics.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study. ArXiv, abs/2303.04048.</p>
<p>Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023b. Document-level machine translation with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 16646-16661. Association for Computational Linguistics.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023c. Large language models are not fair evaluators. ArXiv, abs/2305.17926.</p>
<p>Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D. Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, Benjamin Newman, Pang Wei Koh, Allyson Ettinger, and Yejin Choi. 2023. The generative AI paradox: "what it can create, it may not understand". ArXiv, abs/2311.00059.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. 2023. INSTRUCTSCORE: towards explainable text generation evaluation with automatic feedback. ArXiv, abs/2305.14282.</p>
<p>Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Mingxuan Wang. 2023. BLEURT has universal translations: An analysis of automatic metrics by minimum risk training. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5428-5443. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 27263-27277.</p>
<p>Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orǎsan, Marina Fomicheva, André F. T. Martins, and</p>
<p>Lucia Specia. 2022. Findings of the WMT 2022 shared task on quality estimation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 69-99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 563-578. Association for Computational Linguistics.</p>
<p>Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation with large language models: Empirical results and analysis. ArXiv, abs/2304.04675.</p>
<h2>A GEMBA-SQM Prompts with Four Input Modes</h2>
<h2>T mode</h2>
<p>Score the following translation from {src_lang} to {tgt_lang} on a continuous scale from 0 to 100 that starts on "No meaning preserved", goes through "Some meaning preserved", then "Most meaning preserved and few grammar mistakes", up to "Perfect meaning and grammar".
{tgt_lang} translation: "{translation}" Score (0-100):</p>
<h2>S-T mode</h2>
<p>Score the following translation from {src_lang} to {tgt_lang} on a continuous scale from 0 to 100 that starts on "No meaning preserved", goes through "Some meaning preserved", then "Most meaning preserved and few grammar mistakes", up to "Perfect meaning and grammar".
{src_lang} source: "{source}"
{tgt_lang} translation: "{translation}" Score (0-100):</p>
<h2>R-T mode</h2>
<p>Score the following translation from {src_lang} to {tgt_lang} with respect to the human reference on a continuous scale from 0 to 100 that starts on "No meaning preserved", goes through "Some
meaning preserved", then "Most meaning preserved and few grammar mistakes", up to "Perfect meaning and grammar".
{tgt_lang} human reference: "{reference}"
{tgt_lang} translation: "{translation}" Score (0-100):</p>
<h2>S-R-T mode</h2>
<p>Score the following translation from {src_lang} to {tgt_lang} with respect to the human reference on a continuous scale from 0 to 100 that starts on "No meaning preserved", goes through "Some meaning preserved", then "Most meaning preserved and few grammar mistakes", up to "Perfect meaning and grammar".
{src_lang} source: "{source}"
{tgt_lang} human reference: "{reference}"
{tgt_lang} translation: "{translation}" Score (0-100):</p>
<h2>B AutoMQM Prompts with Four Input Modes</h2>
<h2>T mode</h2>
<p>Identify the major and minor errors in this translation. Note that Major errors refer to actual translation or grammatical errors, and Minor errors refer to smaller imperfections, and purely subjective opinions about the translation.
{tgt_lang} translation: "{translation;}" Errors: ...
{tgt_lang} translation: "{translation}" Errors:</p>
<h2>S-T mode</h2>
<p>Based on the given source, identify the major and minor errors in this translation. Note that Major errors refer to actual translation or grammatical errors, and Minor errors refer to smaller imperfections, and purely subjective opinions about the translation.
{src_lang} source: "{source;}"
{tgt_lang} translation: "{translation;}" Errors: ...
{src_lang} source: "{source}"
{tgt_lang} translation: "{translation}" Errors:</p>
<h2>R-T mode</h2>
<p>Based on the given reference, identify the major and minor errors in this translation. Note that Major errors refer to actual translation or grammatical errors, and Minor errors refer to smaller imperfections, and</p>
<table>
<thead>
<tr>
<th>purely subjective opinions about the</th>
</tr>
</thead>
<tbody>
<tr>
<td>translation.</td>
</tr>
<tr>
<td>(tgt_lang) human reference:</td>
</tr>
<tr>
<td>" ${$ reference $_{i}$ "</td>
</tr>
<tr>
<td>(tgt_lang) translation: " ${$ translation $_{i}$ "</td>
</tr>
<tr>
<td>Errors: ...</td>
</tr>
<tr>
<td>(tgt_lang) human reference:</td>
</tr>
<tr>
<td>" ${$ reference $_{i}$ "</td>
</tr>
<tr>
<td>(tgt_lang) translation: " ${$ translation $_{i}$ "</td>
</tr>
<tr>
<td>Errors:</td>
</tr>
</tbody>
</table>
<h2>S-R-T mode</h2>
<div class="codehilite"><pre><span></span><code>Based on the given source and reference,
identify the major and minor errors
in this translation. Note that Major
errors refer to actual translation or
grammatical errors, and Minor errors
refer to smaller imperfections, and
purely subjective opinions about the
translation.
(src_lang) source: &quot;{source}_{i}&quot;
(tgt_lang) human reference:
&quot;{reference}_{i}&quot;
(tgt_lang) translation: &quot;{translation}_{i}&quot;
Errors: ...
(src_lang) source: &quot;{source}&quot;
(tgt_lang) human reference:
&quot;{reference}&quot;
(tgt_lang) translation: &quot;{translation}&quot;
Errors:
</code></pre></div>

<h2>C Effects of Reference Quality</h2>
<p>According to the results of WMT23 Metrics Shared Task (Freitag et al., 2023), poor human-generated reference translations can dramatically hurt the performance and reliability of the reference-based metrics. Here we perform a simple experiment to confirm this conclusion. We extract all of the samples whose MQM score of refA is less than or equal to 2.0 from the WMT22 Zh-En test set, and finally get 5488 samples with 343 different sources Then we evaluate the performance of GEMBA-SQM-GPT-3.5-turbo and GEMBA-SQM-Llama2-70B-Chat on this test set. The results are shown in Table 11. The gap between S-T and R-T/S-R-T gets much smaller. Sometimes S-T is even better than R-T. Consequently, we believe that the low-quality references have a negative impact on reference-based methods.</p>
<h2>D Shapley Values Calculation</h2>
<p>We denote the meta-evaluation scores of each input mode as $S_{T}, S_{S T}, S_{R T}$ and $S_{S R T}$. The Shapley Value of the source part is</p>
<p>$$
\text { Shapley }<em S="S" T="T">{s r c}=\frac{\left(S</em>
$$}-S_{T}\right)+\left(S_{S R T}-S_{R T}\right)}{2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Mode</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;">$\tau$</th>
<th style="text-align: center;">$\rho$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.129</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.142</td>
<td style="text-align: center;">0.149</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.142</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.183</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-70B-Chat</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.189</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.102</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.147</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.135</td>
<td style="text-align: center;">0.108</td>
</tr>
</tbody>
</table>
<p>Table 11: The performance of different models using different input modes on the test set with inaccurate references.</p>
<p>Similarly, the Shapley Value of the reference is</p>
<p>$$
\text { Shapley }<em R="R" T="T">{r e f}=\frac{\left(S</em>
$$}-S_{T}\right)+\left(S_{S R T}-S_{S T}\right)}{2</p>
<h2>E Analysis by Log-Probability</h2>
<p>We hypothesize that the non-trivial outcomes observed when employing the T mode may be attributed to the LLMs basing their scoring on the quality of the translation sentence provided that the translation is semantically similar to the source. We measure the quality of a sentence using logprobability. Moreover, drawing inspiration from generation-based methods, we also calculate the log-probability of the translation as a scoring metric when providing either the source, the reference, or a combination of both.</p>
<p>In this experiment, we only test the open models including both chat and base versions since the log-probability of ChatGPT is inaccessible at that time. We adopt the same prompt as above (Figure 1) for the chat models and just compute the vanilla log-probability of the translation part. As for the base models, considering they may be confused about the instruction, we only use the equal sign " $=$ " to concatenate the source, reference, and translation sentences. For example, the prompt template of the S-R-T mode is " ${$ source $}={$ reference $}={$ translation $}$ ", and that of the T mode is simply the translation sentence " ${$ translation $}$ ". The log-probability of the translation sentence is computed as follows:</p>
<p>$$
P(\mathbf{t})=\sum_{i=1}^{N} \log p\left(t_{i} \mid \mathbf{c}, \mathbf{t}_{&lt;i}\right)
$$</p>
<p>where $\mathbf{t}$ is the tokens in the translation sentence of length $N$, and $\mathbf{c}$ is the context before the translation in the prompt, such as the instruction, the source, and reference information.</p>
<p>The test set, models, and metrics are identical to those used in the coarse-grained score prediction experiments, except that we add the base models.</p>
<h3>E.1 Results</h3>
<p>As presented in Table 13, we have similar observations to the previous experiment. The superiority of the R-T mode is more prominent in this experiment, irrespective of the model type and size. This also corroborates that even powerful large language models cannot utilize the source information effectively in the translation evaluation task. The performance of the T mode which only computes the translation's log-probability, remains significantly higher than random guess. The system-level accuracy of the T mode even exceeds the S-T and S-R-T mode by a large margin. These findings provide strong support for our hypothesis, suggesting that it is plausible for models to offer a relatively accurate score solely based on the quality of the translation sentence.</p>
<p>In this table, we also observe that the performance of each metric does not scale up well with the model size, regardless of further alignment. The system-level accuracy of models within the same base or chat series is comparable, with the 7B model even slightly outperforming the 70B model. Meanwhile, some of the segment-level correlations, like the correlations of T and S-T mode, are slightly increasing as the model size up. However, the slope is very gradual. We speculate that scaling may bring little benefit to the inherently deficient discriminate capability of auto-regressive language models, which is pertinent to the Generative AI Paradox (West et al., 2023).</p>
<p>When comparing Table 2 and Table 13, a peculiar phenomenon is observed that the segment-level correlations of log-probability are much higher than those of the score prediction method, whereas the system-level accuracy is significantly lower. We leave the reason behind as future work.</p>
<h2>F ChatGPT Token Usage</h2>
<p>We record the ChatGPT token usage and cost in Table 12.</p>
<table>
<thead>
<tr>
<th>Prompt</th>
<th>Input Mode</th>
<th>LP</th>
<th>Samples</th>
<th>Tokens</th>
<th>Cost(\$)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GEMBA</td>
<td>S-T</td>
<td>En-De</td>
<td>22725</td>
<td>2860k</td>
<td>5.72</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zh-En</td>
<td>26340</td>
<td>4030k</td>
<td>8.06</td>
</tr>
<tr>
<td></td>
<td></td>
<td>En-Ru</td>
<td>23326</td>
<td>3340k</td>
<td>6.68</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>En-De</td>
<td>22847</td>
<td>3970k</td>
<td>7.94</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zh-En</td>
<td>26399</td>
<td>5280k</td>
<td>10.56</td>
</tr>
<tr>
<td></td>
<td></td>
<td>En-Ru</td>
<td>24058</td>
<td>5010k</td>
<td>10.02</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>En-De</td>
<td>22738</td>
<td>3340k</td>
<td>6.68</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zh-En</td>
<td>26676</td>
<td>3830k</td>
<td>7.66</td>
</tr>
<tr>
<td></td>
<td></td>
<td>En-Ru</td>
<td>23841</td>
<td>4330k</td>
<td>8.66</td>
</tr>
<tr>
<td></td>
<td>T</td>
<td>En-De</td>
<td>22719</td>
<td>2240k</td>
<td>4.48</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zh-En</td>
<td>27454</td>
<td>2660k</td>
<td>5.32</td>
</tr>
<tr>
<td></td>
<td></td>
<td>En-Ru</td>
<td>23260</td>
<td>2700k</td>
<td>5.40</td>
</tr>
<tr>
<td></td>
<td>Total</td>
<td></td>
<td>292383</td>
<td>43590k</td>
<td>87.18</td>
</tr>
<tr>
<td>AutoMQM</td>
<td>S-T</td>
<td>En-De</td>
<td>3200</td>
<td>2450k</td>
<td>4.90</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zh-En</td>
<td>3200</td>
<td>2700k</td>
<td>5.40</td>
</tr>
<tr>
<td></td>
<td>S-R-T</td>
<td>En-De</td>
<td>3200</td>
<td>3470k</td>
<td>6.94</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zh-En</td>
<td>3200</td>
<td>3520k</td>
<td>7.04</td>
</tr>
<tr>
<td></td>
<td>R-T</td>
<td>En-De</td>
<td>3200</td>
<td>2810k</td>
<td>5.62</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zh-En</td>
<td>3200</td>
<td>2360k</td>
<td>4.72</td>
</tr>
<tr>
<td></td>
<td>T</td>
<td>En-De</td>
<td>3200</td>
<td>1800k</td>
<td>3.60</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zh-En</td>
<td>3200</td>
<td>1550k</td>
<td>3.10</td>
</tr>
<tr>
<td></td>
<td>Total</td>
<td></td>
<td>25600</td>
<td>20660k</td>
<td>41.32</td>
</tr>
</tbody>
</table>
<p>Table 12: ChatGPT token usage in the experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Mode</th>
<th style="text-align: center;">All LPs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">En-De</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Zh-En</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">En-Ru</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-70B-Chat</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.267</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.252</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.331</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">0.294</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-13B-Chat</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.262</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.233</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.317</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.283</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-7B-Chat</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.675</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.253</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.221</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.752</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.310</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.278</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.260</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.258</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.331</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.316</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-70B</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.282</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.258</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.328</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.279</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-13B</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.272</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.242</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.319</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.525</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.276</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-7B</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.264</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.255</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.322</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.282</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.267</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-T</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.249</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R-T</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.333</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-R-T</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.296</td>
</tr>
<tr>
<td style="text-align: center;">COMET-22</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">0.839</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.469</td>
</tr>
<tr>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.160</td>
</tr>
</tbody>
</table>
<p>Table 13: Results of log-probability as a metric on WMT22 test set.</p>
<h1>Source:</h1>
<p>The authorities, particularly in Europe, have lost control of the situation.</p>
<h2>Reference:</h2>
<p>Die Regierungen haben die Kontrolle über die Lage verloren, insbesondere in Europa.</p>
<h2>Translation:</h2>
<p>Die Regierungen haben die Kontrolle über die Lage verloren, insbesondere in Griechenland.</p>
<h2>Output:</h2>
<p>'insbesondere in Griechenland' - major/accuracy</p>
<h2>Source:</h2>
<p>The absence of an effective soft power component undercut the strategic response to terrorism.</p>
<h2>Reference:</h2>
<p>Das Fehlen einer effektiven Komponente von Soft Power unterhöhlte die strategische Reaktion gegenüber dem Terrorismus.</p>
<h2>Translation:</h2>
<p>Das Fehlen einer effektiven Komponente von Soft Power unterhöhlte die strategische und operative Ausrichtung Reaktion gegenüber dem Terrorismus.</p>
<h2>Output:</h2>
<p>No error</p>
<h2>Source:</h2>
<p>This led to what the scholar Moncef Djaziri described as a division of formal and informal authority.</p>
<h2>Reference:</h2>
<p>Das führte zu dem, was der Gelehrte Moncef Djaziri als Trennung der formalen und der nicht formalen Autorität bezeichnete.</p>
<h2>Translation:</h2>
<p>Das führte zu dem, was der Gelehrte Heino Barth als Trennung der formalen und der nicht formalen Autorität bezeichnete.</p>
<h2>Output:</h2>
<p>No error</p>
<p>Figure 3: Cases of GPT-3.5's outputs. The texts in red are critical errors. Up: The model identifies the named entity error successfully. Middle: The model fails to detect the addition error. Bottom: The model fails to detect the named entity error.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ In our preliminary experiments, we used the output format of AutoMQM, but the results were terrible.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
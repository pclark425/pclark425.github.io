<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-830 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-830</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-830</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-264491096</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.pandl-1.5.pdf" target="_blank">Symbolic Planning and Code Generation for Grounded Dialogue</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code’s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system’s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e830.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e830.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Planning and Code-generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular grounded dialogue agent that uses an LLM to translate partner utterances into executable Python 'meaning functions' that call a domain-specific perceptual API, executes those functions to produce symbolic interpretations, maintains a Bayesian belief over possible worlds, and performs explicit symbolic planning by optimizing expected information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SPC</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SPC is a three-part system (read, plan, write). The Reader uses an LLM (GPT-4) to generate Python meaning functions that call a manually-defined perceptual grounding API (predicate functions such as is_small(d), is_grey(d), is_below(d, prev_dots)). Executing the generated code yields a distribution over grounded symbolic actions/configurations p(x|u) (categorical over configurations). The Planner maintains a Bayesian belief p(z) over worlds (which dots are present in the partner's view) and uses a symbolic partner model p(x|z) and p(x|y,z) (heuristic models) to compute expected information gain; it selects the next symbolic action y* that maximizes expected reduction in entropy and uses a templated writer to produce the natural-language utterance. SPC approximates marginalization in belief updates by selecting the most likely interpretation x* (argmax p(x|u)) for efficiency. The system also includes prompt decomposition (dialogue-act classification, reference prediction, constraint/code-fragment generation, then composing full code) to speed up reading.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ONECOMMON (ONECOM-MON, hardest partition)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A collaborative reference game where two players (agent and partner) see different but overlapping views of an underlying set of dots and must, through dialogue, identify one dot common to both views. The context is abstract dot-based imagery requiring spatial and attribute grounding; partial observability arises because each player only sees a subset of dots.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>LLM code-generation model (GPT-4, specifically gpt-4-0613) to synthesize Python meaning functions; a Python execution environment/runtime to execute generated code; a manually engineered, domain-specific perceptual grounding API (Python functions such as is_small, is_grey, is_below, and functions to compute geometric properties like circumcircle radius); prompt templates and few-shot examples used as tooling for the reading step.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured symbolic outputs (Config objects / bitvectors over dots representing configurations), categorical probability distributions over configurations p(x|u), boolean predicate results from perceptual API calls, numeric geometric measures (e.g., circumcircle radius used to score compactness), and text tokens (LLM-generated code and prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A Bayesian belief distribution p(z) over discrete world configurations (z in {0,1}^{|D|}, indicating which dots are in the partner's view). The prior encodes spatial contiguity using a minimum spanning tree-based energy (closer dots are more likely to share state). The belief is stored symbolically (explicit probability over configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Reading produces p(x|u) by executing the generated meaning function which enumerates valid configurations consistent with perceptual predicates. The formal update is p(z|u) = sum_x p(z|x) p(x|u), but SPC approximates this marginalization by taking a point estimate x* = argmax_x p(x|u) (they choose the most compact valid configuration in ONECOM-MON) and computing p(z|u) ∝ p(x*|z) p(z). Prior p(z) is defined via an MST-based energy favoring contiguous shared dots. The system marginalizes over referenced previous-turn configurations when producing p(x|u) during code execution.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Symbolic, model-based planning optimizing expected information gain (entropy reduction) over the discrete belief p(z). The expectation uses a heuristic partner-response model p(x_y | y, z) to simulate possible replies. The agent selects y* = argmax_y [ H[z|u] - E_{x_y | y}[ H[z | u, y, x_y] ] ]. A heuristic end condition is used: end the game (exploit) when the probability of task success exceeds a threshold θ (0.8).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Human evaluation success: 68.8% task success on the hardest ONECOM-MON partition (4 shared dots); self-play success: 84.0% on same partition. Average turns in human eval: 7.77 turns (SPC).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using LLM-generated executable code to ground utterances into symbolic actions plus explicit Bayesian belief tracking and symbolic planning (expected information gain) substantially improves performance on a partially observable grounded dialogue task. Executable code + perceptual API provides accurate, compositional grounding (p(x|u)) which is directly incorporated into beliefs; approximating the marginal via x* is an efficient practical choice. SPC is more conservative (longer dialogues) but more successful and more responsive to skilled human partners than a prior supervised pipeline baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Planning and Code Generation for Grounded Dialogue', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e830.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e830.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CICERO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CICERO (Diplomacy system referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior system that applies symbolic planning in a dialogue-and-strategy game (Diplomacy); it relies on supervised training data and symbolic planning for negotiation and coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Humanlevel play in the game of diplomacy by combining language models with strategic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CICERO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as the closest related work: a system that utilizes symbolic planning for the game of Diplomacy and requires a supervised dataset for training. No implementation details or tool-usage specifics are given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Diplomacy (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Described as a dialogue and strategy game that requires negotiation and coordination between players. The paper does not provide further environment-specific details.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Symbolic planning (stated in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CICERO is cited to motivate symbolic planning in strategic dialogue tasks; unlike SPC, CICERO relies on supervised data, whereas SPC leverages code LLMs and a small perceptual API to reduce supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Planning and Code Generation for Grounded Dialogue', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e830.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e830.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIPERGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vipergpt: Visual inference via python execution for reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced grounded code-generation approach that uses Python execution to perform visual inference by having an LLM generate code which calls perception libraries; cited as an example of grounded code-generation systems that do not require task-specific training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vipergpt: Visual inference via python execution for reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VIPERGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as prior work where an LLM generates Python code to call lower-level perception libraries to perform visual inference; specific architecture and belief/tracking details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>LLM-to-Python code generation and execution; perception libraries (as described generically in the citation).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured outputs from executed code and perceptual API calls (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to support the claim that grounded code generation can be used to interpret language and call perceptual functions without task-specific supervised training; offers a precedent for SPC's use of executable code for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Planning and Code Generation for Grounded Dialogue', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Humanlevel play in the game of diplomacy by combining language models with strategic reasoning <em>(Rating: 2)</em></li>
                <li>Vipergpt: Visual inference via python execution for reasoning <em>(Rating: 2)</em></li>
                <li>Code as policies: Language model programs for embodied control <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Reference-centric models for grounded collaborative dialogue <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-830",
    "paper_id": "paper-264491096",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "SPC",
            "name_full": "Symbolic Planning and Code-generation",
            "brief_description": "A modular grounded dialogue agent that uses an LLM to translate partner utterances into executable Python 'meaning functions' that call a domain-specific perceptual API, executes those functions to produce symbolic interpretations, maintains a Bayesian belief over possible worlds, and performs explicit symbolic planning by optimizing expected information gain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SPC",
            "agent_description": "SPC is a three-part system (read, plan, write). The Reader uses an LLM (GPT-4) to generate Python meaning functions that call a manually-defined perceptual grounding API (predicate functions such as is_small(d), is_grey(d), is_below(d, prev_dots)). Executing the generated code yields a distribution over grounded symbolic actions/configurations p(x|u) (categorical over configurations). The Planner maintains a Bayesian belief p(z) over worlds (which dots are present in the partner's view) and uses a symbolic partner model p(x|z) and p(x|y,z) (heuristic models) to compute expected information gain; it selects the next symbolic action y* that maximizes expected reduction in entropy and uses a templated writer to produce the natural-language utterance. SPC approximates marginalization in belief updates by selecting the most likely interpretation x* (argmax p(x|u)) for efficiency. The system also includes prompt decomposition (dialogue-act classification, reference prediction, constraint/code-fragment generation, then composing full code) to speed up reading.",
            "environment_name": "ONECOMMON (ONECOM-MON, hardest partition)",
            "environment_description": "A collaborative reference game where two players (agent and partner) see different but overlapping views of an underlying set of dots and must, through dialogue, identify one dot common to both views. The context is abstract dot-based imagery requiring spatial and attribute grounding; partial observability arises because each player only sees a subset of dots.",
            "is_partially_observable": true,
            "external_tools_used": "LLM code-generation model (GPT-4, specifically gpt-4-0613) to synthesize Python meaning functions; a Python execution environment/runtime to execute generated code; a manually engineered, domain-specific perceptual grounding API (Python functions such as is_small, is_grey, is_below, and functions to compute geometric properties like circumcircle radius); prompt templates and few-shot examples used as tooling for the reading step.",
            "tool_output_types": "Structured symbolic outputs (Config objects / bitvectors over dots representing configurations), categorical probability distributions over configurations p(x|u), boolean predicate results from perceptual API calls, numeric geometric measures (e.g., circumcircle radius used to score compactness), and text tokens (LLM-generated code and prompts).",
            "belief_state_mechanism": "A Bayesian belief distribution p(z) over discrete world configurations (z in {0,1}^{|D|}, indicating which dots are in the partner's view). The prior encodes spatial contiguity using a minimum spanning tree-based energy (closer dots are more likely to share state). The belief is stored symbolically (explicit probability over configurations).",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Reading produces p(x|u) by executing the generated meaning function which enumerates valid configurations consistent with perceptual predicates. The formal update is p(z|u) = sum_x p(z|x) p(x|u), but SPC approximates this marginalization by taking a point estimate x* = argmax_x p(x|u) (they choose the most compact valid configuration in ONECOM-MON) and computing p(z|u) ∝ p(x*|z) p(z). Prior p(z) is defined via an MST-based energy favoring contiguous shared dots. The system marginalizes over referenced previous-turn configurations when producing p(x|u) during code execution.",
            "planning_approach": "Symbolic, model-based planning optimizing expected information gain (entropy reduction) over the discrete belief p(z). The expectation uses a heuristic partner-response model p(x_y | y, z) to simulate possible replies. The agent selects y* = argmax_y [ H[z|u] - E_{x_y | y}[ H[z | u, y, x_y] ] ]. A heuristic end condition is used: end the game (exploit) when the probability of task success exceeds a threshold θ (0.8).",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": "Human evaluation success: 68.8% task success on the hardest ONECOM-MON partition (4 shared dots); self-play success: 84.0% on same partition. Average turns in human eval: 7.77 turns (SPC).",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Using LLM-generated executable code to ground utterances into symbolic actions plus explicit Bayesian belief tracking and symbolic planning (expected information gain) substantially improves performance on a partially observable grounded dialogue task. Executable code + perceptual API provides accurate, compositional grounding (p(x|u)) which is directly incorporated into beliefs; approximating the marginal via x* is an efficient practical choice. SPC is more conservative (longer dialogues) but more successful and more responsive to skilled human partners than a prior supervised pipeline baseline.",
            "uuid": "e830.0",
            "source_info": {
                "paper_title": "Symbolic Planning and Code Generation for Grounded Dialogue",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CICERO",
            "name_full": "CICERO (Diplomacy system referenced)",
            "brief_description": "Referenced prior system that applies symbolic planning in a dialogue-and-strategy game (Diplomacy); it relies on supervised training data and symbolic planning for negotiation and coordination.",
            "citation_title": "Humanlevel play in the game of diplomacy by combining language models with strategic reasoning",
            "mention_or_use": "mention",
            "agent_name": "CICERO",
            "agent_description": "Mentioned as the closest related work: a system that utilizes symbolic planning for the game of Diplomacy and requires a supervised dataset for training. No implementation details or tool-usage specifics are given in this paper.",
            "environment_name": "Diplomacy (as referenced)",
            "environment_description": "Described as a dialogue and strategy game that requires negotiation and coordination between players. The paper does not provide further environment-specific details.",
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Symbolic planning (stated in related work)",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "CICERO is cited to motivate symbolic planning in strategic dialogue tasks; unlike SPC, CICERO relies on supervised data, whereas SPC leverages code LLMs and a small perceptual API to reduce supervision.",
            "uuid": "e830.1",
            "source_info": {
                "paper_title": "Symbolic Planning and Code Generation for Grounded Dialogue",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "VIPERGPT",
            "name_full": "Vipergpt: Visual inference via python execution for reasoning",
            "brief_description": "A referenced grounded code-generation approach that uses Python execution to perform visual inference by having an LLM generate code which calls perception libraries; cited as an example of grounded code-generation systems that do not require task-specific training data.",
            "citation_title": "Vipergpt: Visual inference via python execution for reasoning",
            "mention_or_use": "mention",
            "agent_name": "VIPERGPT",
            "agent_description": "Mentioned as prior work where an LLM generates Python code to call lower-level perception libraries to perform visual inference; specific architecture and belief/tracking details are not provided in this paper.",
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": "LLM-to-Python code generation and execution; perception libraries (as described generically in the citation).",
            "tool_output_types": "Structured outputs from executed code and perceptual API calls (not detailed in this paper).",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": null,
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited to support the claim that grounded code generation can be used to interpret language and call perceptual functions without task-specific supervised training; offers a precedent for SPC's use of executable code for grounding.",
            "uuid": "e830.2",
            "source_info": {
                "paper_title": "Symbolic Planning and Code Generation for Grounded Dialogue",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Humanlevel play in the game of diplomacy by combining language models with strategic reasoning",
            "rating": 2,
            "sanitized_title": "humanlevel_play_in_the_game_of_diplomacy_by_combining_language_models_with_strategic_reasoning"
        },
        {
            "paper_title": "Vipergpt: Visual inference via python execution for reasoning",
            "rating": 2,
            "sanitized_title": "vipergpt_visual_inference_via_python_execution_for_reasoning"
        },
        {
            "paper_title": "Code as policies: Language model programs for embodied control",
            "rating": 2,
            "sanitized_title": "code_as_policies_language_model_programs_for_embodied_control"
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Reference-centric models for grounded collaborative dialogue",
            "rating": 2,
            "sanitized_title": "referencecentric_models_for_grounded_collaborative_dialogue"
        }
    ],
    "cost": 0.01377425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Symbolic Planning and Code Generation for Grounded Dialogue</p>
<p>Justin T Chiu 
Wenting Zhao 
Derek Chen 
Saujas Vaduguru svadugur@andrew.cmu.edu 
Alexander M Rush arush@cornell.edu 
Cornell Tech 
Daniel Fried dfried@cs.cmu.edu </p>
<p>Cornell Tech</p>
<p>Cornell University</p>
<p>Columbia University</p>
<p>Carnegie Mellon University</p>
<p>Carnegie Mellon University</p>
<p>Symbolic Planning and Code Generation for Grounded Dialogue
DE20BC08A529E499058567B1AC3DEC37
Large language models (LLMs) excel at processing and generating both text and code.However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding.We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution.Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding.The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response.We evaluate our system's performance on the demanding ONECOMMON dialogue task, involving collaborative reference resolution on abstract images of scattered dots.Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.</p>
<p>Introduction</p>
<p>Success in grounded task-oriented dialogue requires intentional communication guided by strategic planning (Cohen and Perrault, 1979;Traum, 1994;Walker et al., 2004;Rieser and Lemon, 2009;Bakhtin et al., 2022, inter alia).Dialogue agents must read partner utterances, update their beliefs, then make a plan that furthers their goal.These plans must take into account both dialogue history and grounding, such as in an image.In end-toend systems based solely on large language models (LLMs), this process is implicit and therefore difficult to control, requiring extra supervision (Christiano et al., 2023) or expensive search (Lu et al., 2022) to improve.While recent work has taken steps to rectify implicit reasoning via planning in language space, where intermediate steps are generated by an LLM (Wei et al., 2022), there is no SPC: Do you see a pair of dots, where the top left dot is medium-sized and grey and the bottom right dot is medium-sized and dark Human:</p>
<p>No SPC:</p>
<p>Do you see a pair of dots, where the top dot is small-sized and light and the bottom dot is medium-sized and grey Human:</p>
<p>Yes I see them.Is there a small grey dot above the small light dot?SPC:</p>
<p>No. Is there a large size and dark color dot to the left and below those?Human:</p>
<p>Yes and there is a small grey dot below them as well for me.SPC:</p>
<p>Yes. Let's select the small size and light color one.SPC: <select> Human: <select> Figure 1: An example grounded dialogue from ONECOMMON.Our dialogue agent, SPC, and a human partner have different but overlapping circular views of a shared board.The agent and partner must collaborate through dialogue in order to find and select a shared dot.ONECOMMON demands careful, grounded reasoning.</p>
<p>guarantee that these approaches result in plans that further task progress.Additionally, planning in language space is expensive, requiring inference in an LLM (Yarats and Lewis, 2017;Guez et al., 2012).Rather than implicit or heuristic reasoning, we are interested in explicit reasoning and planning over symbolic actions.Symbolic actions are controllable by construction, allowing system designers to easily build in task-specific knowledge (He et al., 2018;Bakhtin et al., 2022).This controlla-bility is crucial for obtaining task-specific success using general tools, even with LLMs.</p>
<p>We provide an example from ONECOMMON, a particularly challenging grounded dialogue game (Udagawa and Aizawa, 2019).The goal of ONECOMMON is to, through dialogue, identify one dot in common with your partner, who has an overlapping but different view of an underlying set of dots, illustrated in Figure 1.The challenge in ONECOMMON is grounding the contextual spatial relationships described in language to dots.</p>
<p>Recent work has utilized code-generation for grounded language understanding (Dídac et al., 2023).In particular, they translate natural language questions to code as an intermediate representation, then execute that code to obtain an answer.Code has a couple appealing properties as an intermediate representation: First, modern language models are trained on a mixture of code and natural language, affording them the capability of, with some accuracy, translating between the two (Chen et al., 2021).Second, code acts as a compositional knowledge representation.This allows code-generation systems to perform grounded compositional reasoning, provided a library of Python functions that perform grounding (Liang et al., 2022).We present a system, Symbolic Planning and Code-generation (SPC), that reads by translating partner utterances into code and plans based on symbolic reasoning over what to say next.Code as a compositional knowledge representation closely mirrors the compositional nature of utterances, which are composed of grounded parts.SPC plans by optimizing expected information gain, which has been shown to be effective at building a key aspect of collaborative dialogue: common ground (Yu et al., 2019;White et al., 2021;Chiu et al., 2022).Symbolic planning allows SPC to explicitly and efficiently optimize for task success while taking advantage of task-specific properties.</p>
<p>We evaluate our SPC system on the most challenging subset of the ONECOMMON task, comparing our system to the previous state-of-the-art supervised system for the task (Fried et al., 2021).In both evaluations with human partners and automated self-play evaluations, we find that our approach substantially outperforms the previous stateof-the-art in task accuracy, improving from 56% to 69% accuracy, and obtains comparable task accuracy to human-human pairs on average.</p>
<p>Related Work</p>
<p>Prior work on collaborative reference games focuses on building common ground (He et al., 2017;Haber et al., 2019;Khani et al., 2018).Prior work by Fried et al. (2021) implements an approximation of pragmatic reasoning on ONECOMMON, but plans in language space and utilizes supervised models for mapping language to symbols.Khani et al. (2018) plan in symbolic space, but without natural language.We plan in symbolic space and map from language to symbols via code generation.</p>
<p>Dialogue systems have a long history of reasoning with symbolic actions.When available, symbolic actions have been found to improve the performance of dialogue systems, especially in the setting of grounded dialogue (Winograd, 1971;Young, 2006;He et al., 2018;Andreas et al., 2020;Bakhtin et al., 2022).The closest work to ours is CICERO, which utilizes symbolic planning in a system for DIPLOMACY, a dialogue and strategy game that requires negotiation and coordination between players (Bakhtin et al., 2022).CICERO requires a supervised dataset to train their system.We use code LLMs which require minimal supervision beyond constructing a small perceptual grounding API.</p>
<p>Planning in dialogue systems has recently eschewed symbolic actions in favor of planning directly in text, where systems either perform rollouts, tree-search, or other forms of intermediate reasoning in language.This allows system designers to avoid manually defining symbolic actions (Yarats and Lewis, 2017;Jang et al., 2020;Gandhi et al., 2023).However, the accuracy of languagespace planners is still low in many settings (Fried et al., 2021;Valmeekam et al., 2023).We focus on symbolic planning, where planning is defined in a space that ensures accuracy and controllability.</p>
<p>With the recent progress in large language modeling, code generation for modular grounded systems has quickly gained interest.Grounded code generation systems do not require task-specific training data, making them cheap to apply.A body of work utilizes a large language model for instruction following by generating Python code that makes calls to lower-level perception libraries (Liang et al., 2022;Dídac et al., 2023;Gupta and Kembhavi, 2022;Gao et al., 2023).This extends prior work on executable semantic parsing (Liang, 2016;Johnson et al., 2017;Cheng et al., 2018) with large language models.Concurrent work has also utilized code-generation to interpret language, integrated with symbolic reasoning (Wong et al., 2023).We apply these advances to the setting of grounded task-oriented dialogue, where code generation grounds language to symbolic actions for use in explicit planning.</p>
<p>Overview: Reference Games</p>
<p>Collaborative reference games pair an agent and a partner in order to build common ground through natural language dialogue (Haber et al., 2019;Khani et al., 2018;He et al., 2017;Udagawa and Aizawa, 2019).Mirroring realistic scenarios, many reference games are also partially observable, where the agent and partner have different perspectives, and so they must resolve ambiguity.</p>
<p>ONECOMMON (Udagawa and Aizawa, 2019), as shown in Figure 1, is a reference game that exemplifies two challenges: grounding and planning.In ONECOMMON, the agent and partner see different but overlapping views of a set of dots, and the goal is to find and select one dot common to both players' views.Grounding in ONECOMMON is particularly difficult due to the dot-based visual context, which requires abstract spatial reasoning.Planning is complicated by the partial observability caused by differing perspectives, which require agents to use complex referring expressions in order to avoid ambiguity. 1We focus on ONECOMMON due to its simplicity and difficulty.</p>
<p>Our approach to grounded reference games separates symbolic reasoning from language, allowing explicit steering.Our system, Symbolic Planning and Code-generation (SPC), breaks down a turn into three procedures: reading, planning, and writing.Reading and writing convert from language to symbols and vice versa, while planning reasons in purely symbolic space.</p>
<p>The agent maintains a belief distribution over possible worlds, z, representing task-specific unknowns.The goal of dialogue is to gain information about z until the agent is confident enough to end the game.At each turn, the agent reads the partner's utterance u, converting it into a symbolic action, p(x|u).This symbolic action potentially builds upon the action x ′ of a previous utterance, u ′ .The agent then plans in symbolic space.The system uses reasoning to update its belief state, p(z|u) =</p>
<p>x p(z|x)p(x|u), then produces a response y * of what to say next, which it describes in language to the partner.There is additionally a templated write module for generating a response from y * described in Appendix C.</p>
<p>In ONECOMMON, given a set of dots D, the state z ∈ {0, 1} |D| represents which dots the agent believes are contained (1) and not contained (0) in the partner's view, illustrated in Figure 3.We call a set of dots a configuration.The action representation of partner, x and x ′ , and agent utterances, y * , alike is also a configuration in {0, 1} |D| , as well as any answers or confirmations to previous questions.</p>
<p>Reading: From Language to Symbols</p>
<p>Reading in SPC requires interpreting utterances to a grounded symbolic action, which in turn facilitates the planning stage.Consider the following exchange:</p>
<p>Agent:</p>
<p>Do you see a triangle of dark dots?Partner: Yes, is there a small grey one below it?</p>
<p>Reading has several challenges.First, reading requires grounding utterances in context, e.g. the shapes and relations.Second, utterances are compositional.For example, the partner utterance builds on top of the previous utterance through coreference.Finally, a reading system must act quickly, as real-time dialogue systems require reasonable response times.</p>
<p>Code Generation</p>
<p>In SPC, reading is implemented as code generation.Given a dialogue, we generate Python code2 which is then used as a meaning function to produce a distribution over all valid interpretations of the utterance's symbolic action (Figure 2).The code calls perceptual library functions with grounded semantics, drawn from a task-specific API.This perceptual library allows the system to both ground elements of the utterance and compositionally build upon previous utterances.Consider the following abbreviated example, based on ONECOMMON: The code in the meaning function is imperative, but represents a set of declarative constraints representing p(x|u). 3The meaning function for the partner turn, turn(prev_configs), takes as input the distribution over symbolic actions of a previous turn, p(x ′ ), and yields a set of possible interpretations of the current turn, p(x|u) = x ′ p(x|u, x ′ )p(x ′ ). 4ecause utterances can have multiple valid interpretations due to ambiguity, prev_configs represents a distribution. 5ithin turn, we consider all valid configurations while marginalizing over x ′ , i.e. interpretations in prev_configs.For each interpretation, each dot is considered.If the new dot satisfies the semantics of the utterance, checked step-by-step via grounded perceptual library functions such as is_small(d), then it is a valid interpretation of the current utterance and is used to create a new Config.</p>
<p>The perceptual library functions are drawn from a manually-defined library.For ONECOMMON, we define these functions using domain-specific knowledge:
def is_small(d): return d.size &lt; -0.3
The perceptual library for ONECOMMON can be found here.</p>
<p>Prompting</p>
<p>Reading is implemented with large language model (LLM) code generation.While LLMs can generate accurate code, full code specifications (Section 4.1) are lengthy and therefore too slow to generate for real-time use.We break down code generation into four steps, where some steps do not require any calls to an LLM.Decreasing the number of output tokens guarantees a speedup, assuming consistent latency.See the code for details on the code LLM and prompts we use. 6ialogue Act: Classify partner utterances as one of three dialogue acts: Start a NEW line of questioning, ask a FOLLOW-UP question, END the dialogue.Reference: Predict which previous turn x ′ the utterance is following up on, if any: Agent:</p>
<p>Do you see a triangle?Partner: Yes, is there a small grey dot below it?dialogue act: follow-up refer: turn 1</p>
<p>The system grounds the dots mentioned in the previous turn: agent_configs, which is stored by the system.This allows referring to other turns besides the previous.</p>
<p>Constraint Generation: Predict the new dots mentioned in the partner utterance alongside code fragments that express the semantics, without the boilerplate code, in the example above:</p>
<p>Partner: Yes, is there a small grey one below it?
1 new dot is_small(dot) is_grey(dot) is_below(dot, prev_dots)
Compose: Finally, we utilize a template to compose all of this information back into the full code representation for execution.</p>
<p>Planning: From Symbols to Responses</p>
<p>To perform well in collaborative reference games, it is essential to build common ground quickly and accurately by carefully reasoning about what information has been gathered so far, as well as what to say next.SPC addresses these desiderata by planning in symbolic space, over the symbolic actions produced by reading.</p>
<p>We have two challenges: First, to incorporate the new information from the partner's utterance while accounting for task-specific grounding as well as dialogue history.Second, given this new information, the system must decide either to end the game or how to improve the probability of success.</p>
<p>Planning requires us to model the actions of the partner given the shared state.To do this we need task specific models of our partner, p(x | z), and our partner's reponse to us, p(x|z, y).In ONECOM-MON, we model both of these by a heuristic function considering set overlap and dot proximity, described in Appendix D.</p>
<p>Belief update</p>
<p>Starting from a prior over the previous belief p(z), we incorporate probabilistic evidence from the utterance p(x|u).This requires marginalizing over all valid symbolic actions x from the reading step.In practice, p(x|u) is sparse, and symbols x with non-zero support are very similar.We therefore approximate this marginalization with a point esti-mate:
p(z|u) = x p(z|x)p(x|u) = x p(x|z)p(z) p(x) p(x|u) ≈ x p(x|z)p(z) p(x) 1(x = x * ) ∝ p(x * | z)p(z),(1)
where x * = argmax x p(x|u).</p>
<p>We give an example of this process in Figure 3.In this case, a 'big light dot next to a big dark one' could have two valid interpretations, the big light dot and the black dot to the left, or the other black dot to the right.We approximate this distribution with the most likely interpretation x * .In ONECOM-MON, we use the most compact7 as x * , yielding the black dot on the left.The belief state is then updated to p(z|u), shown in Figure 3 (center).</p>
<p>Planning</p>
<p>Given the updated belief, SPC then plans its next action.The challenge here is to ensure task success, e.g.finding one dot in common.This requires both exploring by building common ground, then exploiting that knowledge to win the game.</p>
<p>We formalize exploration as the expected information gain, a quantity that codifies how much the agent can expect to learn about possible worlds z after taking an action (Lindley, 1956).That action then elicits a response from the partner, providing information about the uncertain world state.For example, if the agent has asked about a set of dots and already received a 'no', then asking further questions about those dots would not reduce uncertainty.</p>
<p>Formally, we optimize
y * = argmax y H[z|u] − E xy|y [H[z | u, y, x y ]] ,
(2) where H[z|u] is the entropy of the current belief 8 and H[z | u, y, x y ] the entropy of the posterior distribution.This second term is the key part of the objective.Assuming that we take action y, the expectation considers all hypothetical future Figure 3: Overview of Planning.Partner utterances are interpreted by a meaning function generated by a code LLM (read), producing a distribution over valid symbolic interpretations, p(x|u).This is used to symbolically update the belief state, p(z|u), increasing the probability of worlds (shared dots) that are consistent with x.This belief state is used to symbolically plan the agent's next utterance, y * , by optimizing the expected information gain, which is described to the partner (write).</p>
<p>partner responses x y .We are penalized if after seeing these responses, we are still uncertain about the common ground z.This objective therefore encourages actions that reduce uncertainty. 9PC chooses to exploit and end the game with the following heuristic: If the system is confident in success, i.e. the probability of task success is greater than hyperparameter θ (set to 0.8), SPC ends the game.</p>
<p>Experimental Setup</p>
<p>We conduct two evaluations of SPC on the ONECOMMON task.We compare to the state-ofthe-art baseline system of Fried et al. (2021), which we refer to as Imitate.Imitate is a pipelined system, where each part is fully supervised.Imitate uses a neural representation of dialogue history in combination with a neural-CRF reference resolution module to understand grounded language.In order to generate, Imitate relies on a pragmatic planning procedure, which plans in a mixture of symbolic and language space, prioritizing descriptions of dots that are easily understood.</p>
<p>We first perform human evaluation, evaluating the task success of systems when paired with human partners.This setting is challenging, requiring the system to handle both the linguistically diverse utterances and a range of strategies of human partners.We recruit 19 workers from Amazon's Mechanical Turk to play with one of three partners: SPC, the most successful past system for the task (Fried et al., 2021), or another human.We pay $15 per hour, with $1.00 per game at an average of 4 minutes per game.We additionally give a bonus of $0.15 for every game.We use 100 visual contexts from the most difficult10 partition of ONECOM-MON.We pay workers $1.00 per game, with a $0.15 bonus if they win.We collect 287 completed dialogues in total, where both players selected a dot.</p>
<p>We secondarily evaluate systems in self-play, where systems are paired with a copy of themselves.This isolates strategic efficiency by ensuring the agent's partner has the same skill as the agent.The 200 games share the same contexts across systems.</p>
<p>We include an additional system in self-play, GPT4 2-shot11 , which gets two full human dialogues as examples.Each human dialogue example starts with a description of the context the agent sees.The full prompts can be viewed here.</p>
<p>Parameterization For code generation, during the reading phase we use GPT-4 12 (OpenAI, 2023)</p>
<p>Results</p>
<p>Human evaluation In human evaluation, SPC obtains substantially higher task accuracy than the baseline model of Fried et al. (2021), and is comparable to human performance on average.This demonstrates that the combination of symbolic information-gain planning and code-generation in SPC is more effective than the baseline's languagespace planning objective and supervised reference resolution.</p>
<p>We see a more nuanced story when conducting a skill-based analysis of the human evaluation results, presented in Figure 4.A worker's skill is given by their average success rate with other human partners.The x-axis of the graph, the minimum success rate, increasingly filters workers from left to right: the left side of the graph shows all workers, while the far right shows only those workers who won nearly all of their human-human games.Skilled human partners have a higher success rate with other humans, as opposed to when partnered with SPC.Additionally, the success rate of SPC improves with human skill, while the success rate of human partners with the baseline system, Imitate, remains relatively constant across skill levels, implying that SPC is more responsive than the baseline to strategies used by humans.</p>
<p>SPC also takes more turns on average than both the baseline and human-human games.We hypothesize that this difference is caused by shorter human partner responses to the system, and therefore less information shared by the human partner.</p>
<p>In Table 2, we confirm that the average and median number of words per human utterance are significantly lower for humans partnered with SPC than any other agent type.</p>
<p>Self-play Similarly to human evaluation, SPC outperforms the baseline Imitate system in selfplay as shown in Table 3.Compared to the baseline, SPC takes more turns on average, but has a higher success rate.We attribute both the longer games and higher success to symbolic planning, which ensures conservative playing.Interestingly, SPC self-play takes fewer turns on average than SPChuman pairings.We hypothesize that this is due to both copies of SPC communicating a consistent amount of information every turn.This also highlights the importance of human evaluation, which evaluates with a large population of partners.</p>
<p>We also find that GPT4 2-shot performs poorly in self-play.We attribute this to overly-agreeable responses, where the agents choose a dot without thorough verification or reasoning.This occurs despite the much longer dialogues, in comparison to all other agent types.</p>
<p>Analysis</p>
<p>Qualitative analysis We present a qualitative example of a dialogue between SPC and a human partner in Figure 5.This long dialogue illustrates the benefits of explicit belief updates and planning:</p>
<p>The conversation starts off with many rejections, which the agent uses to deduce the shared configurations.Eventually, a common set of dots is found and the game ends in a success.Without explicit planning, it would have been unlikely for SPC to have succeeded at the end of the conversation.</p>
<p>Reading speed analysis We perform a speed ablation of the code-generation prompt in SPC.SPC uses a sequence of steps for reading, involving dialogue act classification, code fragment generation, and composing the full code representation based on the output of these steps.We compare this to a prompt that generates the full meaning function.</p>
<p>We evaluate both of these prompts in a reading task, where the goal is to read utterances generated by SPC and recover the underlying plans, measured by accuracy.In Table 4, we see that both styles of prompts have similar similar accuracy, but the sequential, decomposed approach is much faster due to shorter outputs.</p>
<p>Conclusion</p>
<p>We present Symbolic Planning and Codegeneration (SPC), a method that approaches grounded task-oriented dialogue by separating symbolic reasoning from language.Our approach uses an LLM to generate executable code functions which represent the meaning of utterances, mapping from language to symbolic actions.We then symbolically track task progress using Bayesian reasoning, and explicitly plan the best actions to take next using an information gain objective.Despite using minimal supervision, beyond a taskspecific API and few-shot examples, our approach substantially outperforms the state-of-the-art system for the ONECOMMON task in both human eval-uations and automatic self-play evaluations.</p>
<p>Our work contrasts with recent work on planning in language space, which reasons implicitly (Wei et al., 2022;Yarats and Lewis, 2017;Guez et al., 2012).While less flexible than language reasoning, symbolic reasoning is both interpretable and modular.Future work should seek to improve the flexibility of symbolic reasoning (Wong et al., 2023).</p>
<p>Our work also represents a first step toward using general-purpose code as a representation for downstream dialogue and interaction tasks.Future work might explore code-based representations that afford more flexible interaction with people, e.g., representing a broader range of user actions, both linguistic and grounded, to construct broadly useful interactive systems.An ideal system would be able to synthesize these representations with minimal manual intervention.</p>
<p>Limitations</p>
<p>Our system performs code execution given human input, opening our system to several risks, such as code injection and unauthorized access.Future work must strive to integrate code execution capabilities in a secure manner.</p>
<p>Our approach also requires the manual engineering of a domain-specific API, as well as a symbolic representation.Future work should seek to alleviate the amount of manual engineering in order to improve flexibility.We hope that methods in program synthesis can provide a solution.</p>
<p>B Prompt ablation</p>
<p>We present an additional experiment on how the choice of few-shot examples affects the code constraint generation prompt, which is a key component of the reading step.The code constraints express the relationships between the mentioned dots, e.g.whether they form a triangle or their relative positions, shapes, and colors.We take the first human utterance from 20 games in human evaluation and examine whether the parsed answer changes when the prompt examples are changed.The 15 examples in the constraint generation prompt were labeled by hand.Since we cannot sample another 15 examples, we instead sub-sample 5 random examples out of 15 for a 5-shot prompt.We report the average agreement between 5-shot prompts and the original 15-shot prompt across 5 trials: 99%, with a standard deviation of 2%.This implies the constraint generation prompt is not sensitive to prompt example choice at the 5-shot level and prompts could be further optimized.</p>
<p>We perform the same experiment with 5 trials of 1-shot prompts and see an average agreement rate of 34% with a standard deviation of 42%.This implies that given a single example, the prompt example matters.</p>
<p>We also find that a zero-shot prompt is unable to generate output in the correct format.</p>
<p>C Writing</p>
<p>We utilize three templates for writing, one for each dialogue act.START: Do you see a pair of dots, where the {position} dot is {size}-sized and {color} and the {position} dot is {size}-sized and {color}?FOLLOW-UP: Is there a {size} size and {color} color dot {position} those?SELECT: Let's select the {size} size and {color} color one.<selection></p>
<p>D Parameterization</p>
<p>We give the parameterization of the belief prior, p(z) for ONECOMMON.</p>
<p>Our goal in designing the prior is to ensure that the closer dots are, the more likely they are to be of the same state: either all shared or not.This reflects the contiguity of ONECOMMON perspectives.</p>
<p>The prior is given by
p(z) ∝ exp(f (z)),(3)
where f (z) is given the sum of the edges of a minimum spanning tree for the dots in z.The weights of this spanning tree are determined by the rank of how close the dots are to each other.The edge between the nearest neighbor of a dot and the dot itself gets assigned a weight of 0, the 2nd nearest neighbor a weight of 1, and so on.</p>
<p>E Relation to prior work in semantic parsing and dialogue state tracking</p>
<p>Prior work in semantic parsing for dialogue state tracking, such as in SMCalFlow (Andreas et al., 2020), does not ground in a visual context and also requires strategic, collaborative planning due to OneCommon's symmetric roles.Agents must both give and request information strategically.This type of strategic reasoning is not explored in prior works in semantic parsing and dialogue state tracking.Our technical contribution is unifying grounded language understanding and strategic symbolic reasoning with code generation.In particular, the reading phase of SPC was designed for spatial reasoning in OneCommon.</p>
<p>Figure 2 :
2
Figure2: Overview of Reading.The generated meaning function for utterance u takes the previous symbolic action distribution p(x ′ ) from a prior turn and yields the interpretations p(x|u), using code as a compositional representation (Section 4).</p>
<p>Figure 5 :
5
Figure 5: An example dialogue between SPC and a human partner.</p>
<p>Table 1 :
1
(Udagawa and Aizawa, 2019), average number of turns, and total number of games between agents and human partners on the hardest setting of ONECOM-MON, with 4 shared dots.†indicatesstatistics from the ONECOMMON dataset(Udagawa and Aizawa, 2019).
AgentSuccess Turns GamesSPC68.8%7.7796Imitate55.6%6.61117Human67.6%5.0374Human  †65.8%4.972,189The symbolic actions in ONECOMMON consist ofsets of dots and confirmations, while the beliefover symbolic states, p(z), captures which dot con-figurations are shared and is designed to accountfor dot proximity. Further details on the prior aregiven in Appendix D. The symbolic partner models,
p(x | z) and p(x | y, z), are drawn from Chiu et al. (2022), and incorporate a similar bias based on dot proximity.</p>
<p>Table 2 :
2
The average and median number of words per utterance by human partners for different agent types in human evaluation.
90% 100%Human Imitate SPCSuccess Rate60% 70% 80%50%0% 20% 40% 60% 80% 100% Minimum Human-Human SuccessFigure 4: Success rate of the different agent types withhuman partners, with progressive filtering of humanpartners by their success rate along the x-axis. Shadedregions give standard errors.AgentAvg |u| Median |u|SPC6.954Imitate9.628Human15.0614</p>
<p>Table 3 :
3
(Udagawa and Aizawa, 2019)ent agents in 200 selfplay games on the hardest setting of ONECOMMON, with 4 shared dots.A higher success rate is better.The human performance is from the ONECOMMON dataset(Udagawa and Aizawa, 2019).
AgentSuccess Avg # turnsSPC84.0%4.83Imitate63.5%3.31GPT4 2-shot19.0%9.26Human  †65.8%4.97Prompt styleAcc Time (s) LenSPC86.7%536Full84.0%18 176</p>
<dl>
<dt>Table 4 :</dt>
<dt>4</dt>
<dt>The average accuracy, speed, and output length (number of tokens) for the sequential and full code generation methods in our benchmark reading task.</dt>
<dd>
<p>Do you see a pair of dots, where the bottom dot is medium-sized and dark and the top dot is small-sized and light Human: No. Do you see a pair of medium sized dots, close together, one is dark grey the other light grey.The light grey one is slightly above and the left of the dark one.SPC: No. Do you see a pair of dots, where the left dot is medium-sized and dark and the right dot is medium-sized and light Human: No, do you see a lone medium sized grey dot?SPC: No. Do you see a pair of dots, where the left dot is medium-sized and light and the right dot is small-sized and grey Human: No. do you see a pair where the right one is medium and grey and the left one is smaller and lighter.The smaller one is slightly below the medium sized one.
SPC: Yes. Is there a medium size and light color dotto the right and below those?Human: YesSPC: Let's select the medium size and grey color one.Human: <select>SPC: <select>
SPC</p>
</dd>
</dl>
<p>Denis Yarats and Mike Lewis.2017.Hierarchical text generation and planning for strategic dialogue.CoRR, abs/1712.05846.
Steve Young. 2006. Using pomdps for dialog manage-ment. In 2006 IEEE Spoken Language TechnologyWorkshop, pages 8-13.Lili Yu, Howard Chen, Sida I. Wang, Yoav Artzi, andTao Lei. 2019. Interactive classification by askinginformative questions. CoRR, abs/1911.03598.A Prompt detailsAll prompts rely on few-shot prompting. Refor-mat has 5 few-shot examples, Classify has twodialogues with 15-turns total, Confirm has 9 exam-ples, and Understand has two dialogues with 15turns total. All examples were based loosely on 10examples from the human-human games collectedin OneCommon by Udagawa and Aizawa (2019).The same prompts were used in every context. Thefull prompts can be found here.
The contexts in ONECOMMON were constructed to make referring expressions challenging and context-dependent. For example, if the agent sees only light dots, a relatively 'dark' dot for the agent may not be considered dark at all by the partner.ONECOMMON is an ideal testbed for pragmatic methods that reason about contextual meaning. While our approach does not address pragmatics, we hope future work will.
We target Python as our code representation since it is well-understood by large language models. However, in principle, our system could target other languages such as Prolog or SQL.
In ONECOMMON, the distribution over symbolic actions p(x|u) is represented as represented as a categorical distribution over configurations with probabilities based on the size of the circumcircle.
  4  The symbolic action of a previous turn x ′ may also depend on other previous utterances u ′ . For simplicity, we omit that in the notation
.5  SPC is able to intentionally produce ambiguous descriptions if that improves task success, as illustrated in this example.
We release the code here.
We define the compactness of a configuration as the radius of the circumcircle. An ideal approximation would take into account more context, such as the relative sizes.
The belief entropy H[z|u] in the definition of information gain is constant with respect to the plan x, and can be dropped from the objective.
The distribution p(xy|y) = z p(xy|y, z)p(z) also uses the partner response model p(xy|y, z).
The number of shared dots is four.
We do not include GPT4 2-shot in human evaluation, as its self-play evaluation is very poor.
Specifically gpt-4-0613.
AcknowledgementsWe thank Vivian Chen, Sanjiban Choudhury, Ge Gao, Omer Gul, Sedrick Keh, Woojeong Kim, Celine Lee, Jack Morris, Chenran Ning, Jacob Sharf, Alane Suhr, Nicholas Tomlin, Anne Wu, and Jiawei Zhou for discussions, game-playing, and feedback at various points in the process.We also thank the Mechanical Turkers of Turker Nation for their efforts in game-playing.JC is supported by NSF #2242302.AMR is supported by a Sloan Fellowship and NSF CAREER #2037519.SV and DF were supported by gifts from Google and from Autodesk Research.
Aleksandr Nisnevich. Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan Deloach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H Lin, Ilya Lintsbakh, Andy Mcgovern ; Subhro, Jesse Roy, Beth Rusak, Div Short, Ben Slomin, Stephon Snyder, Yu Striplin, Zachary Su, Sam Tellman, Andrei Thomson, Izabela Vorobev, Jason Andrew Witoszko, Abby Wolfe, Yuchen Wray, Alexander Zhang, Zotov, 2020. 2009.11423Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth,Task-oriented dialogue as dataflow synthesis. CoRR, abs/</p>
<p>Humanlevel play in the game of diplomacy by combining language models with strategic reasoning. Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra, Science. 37866242022</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Josh Achiam, Vedant Misra, Felipe Petroski Such. Jan Leike,Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code</p>
<p>Learning an executable neural semantic parser. Jianpeng Cheng, Siva Reddy, Vijay Saraswat, Mirella Lapata, 2018</p>
<p>Modeling perspective-dependent ambiguity in collaborative dialogue. Justin T Chiu, Wenting Zhao, Daniel Fried, Alexander M Rush, The Third Wordplay: When Language Meets Games Workshop. 2022</p>
<p>Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, Deep reinforcement learning from human preferences. 2023</p>
<p>Elements of a plan-based theory of speech acts. Philip R Cohen, C Raymond Perrault, 1979Cognitive Science</p>
<p>Vipergpt: Visual inference via python execution for reasoning. Surís Dídac, Sachit Menon, Carl Vondrick, arXiv:2303.081282023arXiv preprint</p>
<p>Reference-centric models for grounded collaborative dialogue. Daniel Fried, Justin T Chiu, Dan Klein, Proceedings of EMNLP. EMNLP2021</p>
<p>Strategic reasoning with language models. Kanishk Gandhi, Dorsa Sadigh, Noah D Goodman, 2023</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, Pal: Program-aided language models. 2023</p>
<p>Efficient bayes-adaptive reinforcement learning using sample-based search. Arthur Guez, David Silver, Peter Dayan, CoRR, abs/1205.31092012</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, 2022</p>
<p>The photobook dataset: Building common ground through visually-grounded dialogue. Janosch Haber, Tim Baumgärtner, Ece Takmaz, Lieke Gelderloos, Elia Bruni, Raquel Fernández, CoRR, abs/1906.015302019</p>
<p>Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings. He He, Anusha Balakrishnan, Mihail Eric, Percy Liang, CoRR, abs/1704.071302017</p>
<p>Decoupling strategy and generation in negotiation dialogues. He He, Derek Chen, Anusha Balakrishnan, Percy Liang, CoRR, abs/1808.096372018</p>
<p>Bayes-adaptive monte-carlo planning and learning for goal-oriented dialogues. Youngsoo Jang, Jongmin Lee, Kee-Eung Kim, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020. February 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</p>
<p>Inferring and executing programs for visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, Ross Girshick, 2017</p>
<p>Planning, inference and pragmatics in sequential language games. Fereshte Khani, Noah Goodman, Percy Liang, 10.1162/tacl_a_00037Transactions of the Association for Computational Linguistics. 62018</p>
<p>Learning executable semantic parsers for natural language understanding. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, arXivpreprintarXiv:2209.077532022. 2016Percy LiangCode as policies: Language model programs for embodied control</p>
<p>On a Measure of the Information Provided by an Experiment. D V Lindley, 10.1214/aoms/1177728069The Annals of Mathematical Statistics. 195627</p>
<p>Neurologic a*esque decoding: Constrained text generation with lookahead heuristics. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Le Ronan, Lianhui Bras, Youngjae Qin, Rowan Yu, Noah A Zellers, Yejin Smith, Choi, 2022</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Natural language generation as planning under uncertainty for spoken dialogue systems. Verena Rieser, Oliver Lemon, Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009). the 12th Conference of the European Chapter of the ACL (EACL 2009)Athens, GreeceAssociation for Computational Linguistics2009</p>
<p>A computational theory of grounding in natural language conversation. David R Traum, 1994Rochester University Department of Computer ScienceTechnical report</p>
<p>A natural language corpus of common grounding under continuous and partially-observable context. Takuma Udagawa, Akiko Aizawa, 10.1609/aaai.v33i01.33017120Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>On the planning abilities of large language models (a critical investigation with a proposed benchmark). Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, Subbarao Kambhampati, 2023</p>
<p>Generation and evaluation of user tailored responses in multimodal dialogue. M A Walker, S J Whittaker, Stent, Maloor, Moore, Johnston, Vasireddy, Cognitive Science. 2852004</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, CoRR, abs/2201.119032022</p>
<p>Open-domain clarification question generation without question examples. Julia White, Gabriel Poesia, Robert Hawkins, Dorsa Sadigh, Noah Goodman, 10.18653/v1/2021.emnlp-main.44Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Procedures as a representation for data in a computer program for understanding natural language. Terry Winograd, 1971</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, K Vikash, Jacob Mansinghka, Joshua B Andreas, Tenenbaum, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
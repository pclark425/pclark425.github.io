<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8620 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8620</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8620</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276318032</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.09589v1.pdf" target="_blank">Logical forms complement probability in understanding language model (and human) performance</a></p>
                <p><strong>Paper Abstract:</strong> With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question. This work conducts a systematic investigation of LLMs' ability to perform logical reasoning in natural language. We introduce a controlled dataset of hypothetical and disjunctive syllogisms in propositional and modal logic and use it as the testbed for understanding LLM performance. Our results lead to novel insights in predicting LLM behaviors: in addition to the probability of input (Gonen et al., 2023; McCoy et al., 2024), logical forms should be considered as orthogonal factors. In addition, we show similarities and differences between the logical reasoning performances of humans and LLMs by comparing LLM and human behavioral results.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8620.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8620.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Syllogisms (propositional+modal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controlled dataset of hypothetical and disjunctive syllogisms in propositional and alethic modal logic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic, controlled benchmark of 24 logical-form templates (4 argument-form variants × theorem/fallacy × 3 modalities) instantiated with 1000 independent natural-language interpretations each (24k examples) to evaluate strict propositional and modal reasoning under natural-language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Yes/No natural-language questions derived from logical sequents: disjunctive syllogism and two hypothetical-syllogism variants (modus ponens, modus tollens) and corresponding fallacies, each instantiated under three modalities (none, possibility 3, necessity 2). Ground truth determined by formal entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Synthetic template-based translation of modal/propositional logical forms into disambiguated English; zero-shot prompting; probability-based 'soft accuracy' metric (relative p(Yes|s) vs p(No|s)); mixed-effects statistical analysis; mirror (nonsense-word) dataset for perplexity control; human behavioral comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Dataset used as primary benchmark (24,000 examples), not a model — enables per-model soft-accuracy reporting and breakdowns by modality and argument form.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Provides controlled comparisons across models and controlled ablations (modality, argument form, 'mirror' nonsensical interpretations) rather than an external baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Synthetic and limited to English; interpretations are controlled but still simplified relative to full natural language contexts; some modality interactions (necessity distribution) are confounded and require careful interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Logical form (argument structure and modality) materially affects LLM performance and complements probability/perplexity measures; the dataset exposes modality-dependent biases and argument-form-specific weaknesses (e.g., modus tollens).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8620.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B (open-weight LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight transformer-based large language model evaluated zero-shot on the synthetic syllogism benchmark using probability-based yes/no scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mistral-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Openly released transformer LLM (autoregressive); evaluated in zero-shot prompting mode; probability outputs for tokens used to compute a soft Yes/No accuracy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Atomic propositional sequents and modal variants (possibility/necessity) mapped to natural-language yes/no questions testing entailment vs non-entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; evaluation via token-probability-based soft accuracy (p(Yes) vs p(No)); mixed-effects regression to analyze factors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.645 (from Table 1); performed better on possibility modality than necessity; struggled on modus tollens and affirming-consequent fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed several smaller Llama-2 models (e.g., Llama-2-7B) and underperformed some larger / higher-ranked models (e.g., Mistral-8x7b, Llama-3-70B); mixed-effects model accounts for per-model perplexity random effects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower accuracy on necessity-modality questions and on modus tollens; exhibits affirmation/rejection biases depending on modality; not perfect on atomic logical sequents.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Confirms paper's main claim that logical-form factors (modality and argument form) impact performance beyond input perplexity; perplexity negatively but weakly correlates with accuracy (ρ = −0.09).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8620.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A top-ranked Mistral variant evaluated zero-shot on the syllogism benchmark using probability-based scoring, achieving the highest soft accuracy among open models reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mistral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight transformer model (Mistral family variant); evaluated in zero-shot with probability outputs for the Yes/No tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>approx. 8×7B (as named)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same controlled yes/no entailment questions derived from propositional and modal logic templates.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; probability-based soft accuracy; mixed-effects analysis for modality/argument form effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.724 (Table 1) — highest among open models reported; better on possibility than necessity; still struggles on specific forms (modus tollens, affirming consequent).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Top open-model performer, outperforming Mistral-7B and many Llama-2 variants; commercial models (OpenAI/Gemini) reported higher greedy-decoding scores but are not directly comparable due to different evaluation (greedy vs probability-based).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exhibited modality-dependent biases (lower on necessity), and weaknesses on modus tollens and some fallacy items; performance still far from perfect on atomic logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High-performing open model but confirms that model size/rank is not the sole determinant — logical form and modality remain important complementary predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8620.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter Llama-2 variant evaluated zero-shot using the paper's probability-based metric, showing relatively low soft accuracy on the controlled logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive transformer (Llama-2 family) evaluated in zero-shot; token probabilities for 'Yes'/'No' used to compute soft accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary entailment questions from logical sequents, including modal variants.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; probability-based soft accuracy evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.335 (Table 1) — among the lower-performing evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperformed larger Llama-2 variants and other higher-ranked open models (e.g., Mistral-8x7b, Llama-3-70B).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Particularly low overall accuracy; expected to struggle more on necessity modality and on argument forms like modus tollens/fallacies.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Consistent with paper's observation that model rank/size correlates with performance but logical-form factors still explain residual variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8620.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter Llama-2 variant evaluated zero-shot with the paper's probability-based metric, showing intermediate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer Llama-2 family; evaluated zero-shot using relative token-probability Yes/No scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Yes/No entailment questions testing propositional and modal sequents.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; probability-based soft accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.513 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Better than Llama-2-7B, worse than larger or higher-ranked models like Llama-3-70B and Mistral-8x7b.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Modality-dependent biases (lower on necessity), and specific argument-form weaknesses remain.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Shows gains with scale but still affected by logical form and modality; perplexity partially predicts but is insufficient alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8620.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter Llama-2 model evaluated zero-shot using the probability-based soft accuracy metric; shows substantially better performance than smaller Llama-2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-2-70b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer from the Llama-2 family; evaluated zero-shot with token-probability Yes/No scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Controlled natural-language yes/no entailment questions from modal and propositional logic templates.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; probability-based evaluation; mixed-effects analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.611 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Stronger than Llama-2 smaller variants; lower than top open models (Mistral-8x7b) and some Llama-3 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still shows modality and argument-form weaknesses (necessity lower than possibility; difficulties on modus tollens/fallacies).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Scale improves raw accuracy but does not eliminate logical-form dependent errors or modality biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8620.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter Llama-3 model evaluated zero-shot with probability-based soft accuracy, achieving mid-to-strong performance among open models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-3-8b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3 family autoregressive transformer; evaluated zero-shot with token-probability scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Yes/No logical entailment questions derived from propositional and modal templates.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot; probability-based soft accuracy evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.565 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed some Llama-2 variants; below top open performers such as Mistral-8x7b and Llama-3-70B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Modal rejection bias on necessity and argument-form-specific weaknesses remain.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Again supports that model architecture/scale interact with logical-form factors — improvements do not fully address modality/argument-form failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8620.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter Llama-3 model evaluated zero-shot with the probability-based metric; among the top open models for the task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-3-70b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Llama-3 autoregressive transformer; evaluated zero-shot using relative Yes/No token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary entailment judgments from controlled propositional/modal templates.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; probability-based soft accuracy metric; mixed-effects analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.714 (Table 1) — high among open models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Near top of open-model leaderboard (below Mistral-8x7b in this paper); commercial models reported higher greedy-decoding scores but are not directly comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still exhibits modality-dependent rejection for necessity and struggles on modus tollens and selected fallacy items.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large-capacity Llama-3 gives strong performance but does not remove logical-form and modality effects; probability alone insufficient to explain per-item variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8620.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>yi-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>yi-34b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 34-billion-parameter open model evaluated zero-shot on the syllogism benchmark with moderate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>yi-34b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open autoregressive transformer model (~34B parameters); evaluated zero-shot with token-probability-based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Yes/No entailment questions testing propositional/modal inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; soft accuracy using conditional token probabilities; mixed-effects analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.518 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Intermediate performance relative to other open models (worse than Llama-3-70B/Mistral-8x7b, similar to Llama-2-13B).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Shows same qualitative weaknesses: lower on necessity, struggle on modus tollens and fallacy items.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Reinforces cross-model patterns: modality and argument form are consistent predictors across diverse architectures and sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8620.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Phi-family model evaluated zero-shot on the controlled logical reasoning dataset; shows moderate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>phi-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Member of the Phi model family (autoregressive transformer); evaluated zero-shot with token-probability soft accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified in paper (Phi family)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary entailment questions derived from propositional and modal logic templates.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; probability-based soft accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.532 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Comparable to mid-ranked models (e.g., Llama-2-13B, yi-34b); below top open models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Modal and argument-form weaknesses similar to other models; necessitation rule shows especially low accuracy across many models.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Phi models follow same qualitative trends: modality and argument-form effects persist across model families.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8620.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller Phi-family model evaluated zero-shot; achieved relatively strong performance for its class.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>phi-3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small/compact Phi-family autoregressive transformer; zero-shot evaluation using token-probability soft scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mini (unspecified exact parameter count)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Yes/No entailment tasks derived from propositional/modal logical templates.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; probability-based soft accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Soft accuracy (Acc_soft) ≈ 0.690 (Table 1) — relatively strong for a 'mini' model.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed many larger Llama-2 variants; below top open performers like Mistral-8x7b.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still subject to modality-dependent biases and argument-form weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Indicates that training/data/architecture differences (not just raw parameter count) importantly affect logical reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8620.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI-o1 (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI-o1 (commercial model, greedy-decoding reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial OpenAI model evaluated greedily on a 2,000-sample subset (reference only); greedy-decoding scores reported but not directly comparable to probability-based soft accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-o1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial LLM (OpenAI family); paper could not obtain conditional token-probabilities, so evaluation used greedy-decoding on a 2,000-sample subset for reference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal) — subset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Greedy-decoding yes/no evaluation on a random 2,000-sample subset of the same controlled tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Greedy decoding (rather than conditional-probability soft scoring); reported for reference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy accuracy ≈ 0.926 on the subset (Table 1), but not directly comparable to Acc_soft metrics used for open models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Higher greedy-decoding accuracy than most open models per the reported subset; evaluation modality differs (so direct comparisons are cautioned).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not evaluated under the probability-based protocol; small subset and greedy-decoding make results not directly comparable to the main reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Commercial/closed models may obtain higher greedy-decode accuracy on sampled subsets, but the paper stresses the importance of using conditional probability measures and careful comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8620.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Pro (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5-Pro (commercial model, greedy-decoding reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial Google (Gemini) family model evaluated greedily on a subset for reference; greedy-decoding scores reported but not directly comparable to the probability-based protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial LLM (Gemini family); conditional probabilities not available to authors, so greedy-decoding on a subset used as a reference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal) — subset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Greedy-decoded yes/no answers on a 2,000-sample subset used as a reference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Greedy decoding; used for reference only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy accuracy ≈ 0.859 on the reported subset (Table 1); not directly comparable to Acc_soft.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reportedly lower than OpenAI-o1 on the subset but higher than many open models; direct comparisons limited by evaluation differences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not evaluated with probability-based protocol; reference-only subset limits generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Highlights evaluation-protocol dependence: greedy-decode scores can differ from probability-based soft-accuracy outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8620.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8620.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human participants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human behavioral participants (Prolific sample)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human participants (n=710) performed the same yes/no syllogism tasks to provide behavioral baselines and comparisons with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>human participants (Prolific)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adult fluent English speakers recruited via Prolific; responses are binary (Yes/No) with response-time collection; data used to compare human vs LLM reasoning patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothetical and disjunctive syllogisms (propositional + alethic modal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same controlled natural-language yes/no entailment prompts as used with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Behavioral experiment with binary responses, randomized key mapping, and mixed-effects logistic regression to analyze modality and argument-form effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy ≈ 0.595 (Table 1); humans showed the ordering 3 (possibility) ≻ ∅ (propositional) ≻ 2 (necessity) similar to LLMs, and highest accuracy on modus ponens.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LLMs sometimes achieved higher benchmark performance than humans on the dataset, but humans and models share some argument-form preferences while differing in modality biases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sample size is moderate; synthetic dataset and English-only limit ecological validity; humans did not show the same strong rejection bias on necessity that LLMs did.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Humans and LLMs exhibit both similarities (argument-form preferences) and differences (LLM modality biases), underscoring that LLMs are not straightforward human models and cautioning against overgeneralization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logical forms complement probability in understanding language model (and human) performance', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Typicality-aware perplexity and its relation to evaluation of language model behavior (Gonen et al., 2023) <em>(Rating: 2)</em></li>
                <li>Embers of autoregression show how large language models are shaped by the problem they are trained to solve (McCoy et al., 2024) <em>(Rating: 2)</em></li>
                <li>Systematic testing of three language models reveals low language accuracy, absence of response stability, and a yes-response bias (Dentella et al., 2023) <em>(Rating: 2)</em></li>
                <li>A systematic comparison of syllogistic reasoning in humans and language models (Eisape et al., 2024) <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language (Clark et al., 2021) <em>(Rating: 1)</em></li>
                <li>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models (Parmar et al., 2024) <em>(Rating: 2)</em></li>
                <li>Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples (Saparov et al., 2023) <em>(Rating: 2)</em></li>
                <li>A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models (Wan et al., 2024) <em>(Rating: 2)</em></li>
                <li>Conditional and Modal Reasoning in Large Language Models (Holliday & Mandelkern, 2024) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8620",
    "paper_id": "paper-276318032",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Syllogisms (propositional+modal)",
            "name_full": "Controlled dataset of hypothetical and disjunctive syllogisms in propositional and alethic modal logic",
            "brief_description": "A synthetic, controlled benchmark of 24 logical-form templates (4 argument-form variants × theorem/fallacy × 3 modalities) instantiated with 1000 independent natural-language interpretations each (24k examples) to evaluate strict propositional and modal reasoning under natural-language prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Yes/No natural-language questions derived from logical sequents: disjunctive syllogism and two hypothetical-syllogism variants (modus ponens, modus tollens) and corresponding fallacies, each instantiated under three modalities (none, possibility 3, necessity 2). Ground truth determined by formal entailment.",
            "method_or_approach": "Synthetic template-based translation of modal/propositional logical forms into disambiguated English; zero-shot prompting; probability-based 'soft accuracy' metric (relative p(Yes|s) vs p(No|s)); mixed-effects statistical analysis; mirror (nonsense-word) dataset for perplexity control; human behavioral comparison.",
            "performance": "Dataset used as primary benchmark (24,000 examples), not a model — enables per-model soft-accuracy reporting and breakdowns by modality and argument form.",
            "baseline_comparison": "Provides controlled comparisons across models and controlled ablations (modality, argument form, 'mirror' nonsensical interpretations) rather than an external baseline.",
            "limitations_or_failures": "Synthetic and limited to English; interpretations are controlled but still simplified relative to full natural language contexts; some modality interactions (necessity distribution) are confounded and require careful interpretation.",
            "insights_or_conclusions": "Logical form (argument structure and modality) materially affects LLM performance and complements probability/perplexity measures; the dataset exposes modality-dependent biases and argument-form-specific weaknesses (e.g., modus tollens).",
            "uuid": "e8620.0",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral-7B (open-weight LLM)",
            "brief_description": "An open-weight transformer-based large language model evaluated zero-shot on the synthetic syllogism benchmark using probability-based yes/no scoring.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "mistral-7b",
            "model_description": "Openly released transformer LLM (autoregressive); evaluated in zero-shot prompting mode; probability outputs for tokens used to compute a soft Yes/No accuracy metric.",
            "model_size": "7B",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Atomic propositional sequents and modal variants (possibility/necessity) mapped to natural-language yes/no questions testing entailment vs non-entailment.",
            "method_or_approach": "Zero-shot prompting; evaluation via token-probability-based soft accuracy (p(Yes) vs p(No)); mixed-effects regression to analyze factors.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.645 (from Table 1); performed better on possibility modality than necessity; struggled on modus tollens and affirming-consequent fallacies.",
            "baseline_comparison": "Outperformed several smaller Llama-2 models (e.g., Llama-2-7B) and underperformed some larger / higher-ranked models (e.g., Mistral-8x7b, Llama-3-70B); mixed-effects model accounts for per-model perplexity random effects.",
            "limitations_or_failures": "Lower accuracy on necessity-modality questions and on modus tollens; exhibits affirmation/rejection biases depending on modality; not perfect on atomic logical sequents.",
            "insights_or_conclusions": "Confirms paper's main claim that logical-form factors (modality and argument form) impact performance beyond input perplexity; perplexity negatively but weakly correlates with accuracy (ρ = −0.09).",
            "uuid": "e8620.1",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mistral-8x7B",
            "name_full": "Mistral-8x7b",
            "brief_description": "A top-ranked Mistral variant evaluated zero-shot on the syllogism benchmark using probability-based scoring, achieving the highest soft accuracy among open models reported.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "mistral-8x7b",
            "model_description": "Open-weight transformer model (Mistral family variant); evaluated in zero-shot with probability outputs for the Yes/No tokens.",
            "model_size": "approx. 8×7B (as named)",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Same controlled yes/no entailment questions derived from propositional and modal logic templates.",
            "method_or_approach": "Zero-shot prompting; probability-based soft accuracy; mixed-effects analysis for modality/argument form effects.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.724 (Table 1) — highest among open models reported; better on possibility than necessity; still struggles on specific forms (modus tollens, affirming consequent).",
            "baseline_comparison": "Top open-model performer, outperforming Mistral-7B and many Llama-2 variants; commercial models (OpenAI/Gemini) reported higher greedy-decoding scores but are not directly comparable due to different evaluation (greedy vs probability-based).",
            "limitations_or_failures": "Exhibited modality-dependent biases (lower on necessity), and weaknesses on modus tollens and some fallacy items; performance still far from perfect on atomic logical reasoning.",
            "insights_or_conclusions": "High-performing open model but confirms that model size/rank is not the sole determinant — logical form and modality remain important complementary predictors.",
            "uuid": "e8620.2",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama 2 (7B)",
            "brief_description": "A 7-billion-parameter Llama-2 variant evaluated zero-shot using the paper's probability-based metric, showing relatively low soft accuracy on the controlled logical tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-2-7b",
            "model_description": "Open-source autoregressive transformer (Llama-2 family) evaluated in zero-shot; token probabilities for 'Yes'/'No' used to compute soft accuracy.",
            "model_size": "7B",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Binary entailment questions from logical sequents, including modal variants.",
            "method_or_approach": "Zero-shot prompting; probability-based soft accuracy evaluation.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.335 (Table 1) — among the lower-performing evaluated models.",
            "baseline_comparison": "Underperformed larger Llama-2 variants and other higher-ranked open models (e.g., Mistral-8x7b, Llama-3-70B).",
            "limitations_or_failures": "Particularly low overall accuracy; expected to struggle more on necessity modality and on argument forms like modus tollens/fallacies.",
            "insights_or_conclusions": "Consistent with paper's observation that model rank/size correlates with performance but logical-form factors still explain residual variability.",
            "uuid": "e8620.3",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-2-13B",
            "name_full": "Llama 2 (13B)",
            "brief_description": "A 13B-parameter Llama-2 variant evaluated zero-shot with the paper's probability-based metric, showing intermediate performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-2-13b",
            "model_description": "Autoregressive transformer Llama-2 family; evaluated zero-shot using relative token-probability Yes/No scoring.",
            "model_size": "13B",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Yes/No entailment questions testing propositional and modal sequents.",
            "method_or_approach": "Zero-shot prompting; probability-based soft accuracy.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.513 (Table 1).",
            "baseline_comparison": "Better than Llama-2-7B, worse than larger or higher-ranked models like Llama-3-70B and Mistral-8x7b.",
            "limitations_or_failures": "Modality-dependent biases (lower on necessity), and specific argument-form weaknesses remain.",
            "insights_or_conclusions": "Shows gains with scale but still affected by logical form and modality; perplexity partially predicts but is insufficient alone.",
            "uuid": "e8620.4",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-2-70B",
            "name_full": "Llama 2 (70B)",
            "brief_description": "A 70-billion-parameter Llama-2 model evaluated zero-shot using the probability-based soft accuracy metric; shows substantially better performance than smaller Llama-2 variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-2-70b",
            "model_description": "Large autoregressive transformer from the Llama-2 family; evaluated zero-shot with token-probability Yes/No scoring.",
            "model_size": "70B",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Controlled natural-language yes/no entailment questions from modal and propositional logic templates.",
            "method_or_approach": "Zero-shot prompting; probability-based evaluation; mixed-effects analysis.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.611 (Table 1).",
            "baseline_comparison": "Stronger than Llama-2 smaller variants; lower than top open models (Mistral-8x7b) and some Llama-3 variants.",
            "limitations_or_failures": "Still shows modality and argument-form weaknesses (necessity lower than possibility; difficulties on modus tollens/fallacies).",
            "insights_or_conclusions": "Scale improves raw accuracy but does not eliminate logical-form dependent errors or modality biases.",
            "uuid": "e8620.5",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3-8B",
            "name_full": "Llama 3 (8B)",
            "brief_description": "An 8B-parameter Llama-3 model evaluated zero-shot with probability-based soft accuracy, achieving mid-to-strong performance among open models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-3-8b",
            "model_description": "Llama-3 family autoregressive transformer; evaluated zero-shot with token-probability scoring.",
            "model_size": "8B",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Yes/No logical entailment questions derived from propositional and modal templates.",
            "method_or_approach": "Zero-shot; probability-based soft accuracy evaluation.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.565 (Table 1).",
            "baseline_comparison": "Outperformed some Llama-2 variants; below top open performers such as Mistral-8x7b and Llama-3-70B.",
            "limitations_or_failures": "Modal rejection bias on necessity and argument-form-specific weaknesses remain.",
            "insights_or_conclusions": "Again supports that model architecture/scale interact with logical-form factors — improvements do not fully address modality/argument-form failure modes.",
            "uuid": "e8620.6",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3-70B",
            "name_full": "Llama 3 (70B)",
            "brief_description": "A 70B-parameter Llama-3 model evaluated zero-shot with the probability-based metric; among the top open models for the task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama-3-70b",
            "model_description": "Large Llama-3 autoregressive transformer; evaluated zero-shot using relative Yes/No token probabilities.",
            "model_size": "70B",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Binary entailment judgments from controlled propositional/modal templates.",
            "method_or_approach": "Zero-shot prompting; probability-based soft accuracy metric; mixed-effects analysis.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.714 (Table 1) — high among open models.",
            "baseline_comparison": "Near top of open-model leaderboard (below Mistral-8x7b in this paper); commercial models reported higher greedy-decoding scores but are not directly comparable.",
            "limitations_or_failures": "Still exhibits modality-dependent rejection for necessity and struggles on modus tollens and selected fallacy items.",
            "insights_or_conclusions": "Large-capacity Llama-3 gives strong performance but does not remove logical-form and modality effects; probability alone insufficient to explain per-item variability.",
            "uuid": "e8620.7",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "yi-34B",
            "name_full": "yi-34b",
            "brief_description": "A 34-billion-parameter open model evaluated zero-shot on the syllogism benchmark with moderate performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "yi-34b",
            "model_description": "Open autoregressive transformer model (~34B parameters); evaluated zero-shot with token-probability-based scoring.",
            "model_size": "34B",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Yes/No entailment questions testing propositional/modal inferences.",
            "method_or_approach": "Zero-shot prompting; soft accuracy using conditional token probabilities; mixed-effects analysis.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.518 (Table 1).",
            "baseline_comparison": "Intermediate performance relative to other open models (worse than Llama-3-70B/Mistral-8x7b, similar to Llama-2-13B).",
            "limitations_or_failures": "Shows same qualitative weaknesses: lower on necessity, struggle on modus tollens and fallacy items.",
            "insights_or_conclusions": "Reinforces cross-model patterns: modality and argument form are consistent predictors across diverse architectures and sizes.",
            "uuid": "e8620.8",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Phi-2",
            "name_full": "Phi-2",
            "brief_description": "A Phi-family model evaluated zero-shot on the controlled logical reasoning dataset; shows moderate performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "phi-2",
            "model_description": "Member of the Phi model family (autoregressive transformer); evaluated zero-shot with token-probability soft accuracy.",
            "model_size": "unspecified in paper (Phi family)",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Binary entailment questions derived from propositional and modal logic templates.",
            "method_or_approach": "Zero-shot prompting; probability-based soft accuracy.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.532 (Table 1).",
            "baseline_comparison": "Comparable to mid-ranked models (e.g., Llama-2-13B, yi-34b); below top open models.",
            "limitations_or_failures": "Modal and argument-form weaknesses similar to other models; necessitation rule shows especially low accuracy across many models.",
            "insights_or_conclusions": "Phi models follow same qualitative trends: modality and argument-form effects persist across model families.",
            "uuid": "e8620.9",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Phi-3-mini",
            "name_full": "Phi-3-mini",
            "brief_description": "A smaller Phi-family model evaluated zero-shot; achieved relatively strong performance for its class.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "phi-3-mini",
            "model_description": "Small/compact Phi-family autoregressive transformer; zero-shot evaluation using token-probability soft scoring.",
            "model_size": "mini (unspecified exact parameter count)",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Yes/No entailment tasks derived from propositional/modal logical templates.",
            "method_or_approach": "Zero-shot prompting; probability-based soft accuracy.",
            "performance": "Soft accuracy (Acc_soft) ≈ 0.690 (Table 1) — relatively strong for a 'mini' model.",
            "baseline_comparison": "Outperformed many larger Llama-2 variants; below top open performers like Mistral-8x7b.",
            "limitations_or_failures": "Still subject to modality-dependent biases and argument-form weaknesses.",
            "insights_or_conclusions": "Indicates that training/data/architecture differences (not just raw parameter count) importantly affect logical reasoning performance.",
            "uuid": "e8620.10",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "OpenAI-o1 (ref)",
            "name_full": "OpenAI-o1 (commercial model, greedy-decoding reference)",
            "brief_description": "A commercial OpenAI model evaluated greedily on a 2,000-sample subset (reference only); greedy-decoding scores reported but not directly comparable to probability-based soft accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI-o1",
            "model_description": "Commercial LLM (OpenAI family); paper could not obtain conditional token-probabilities, so evaluation used greedy-decoding on a 2,000-sample subset for reference.",
            "model_size": "not specified",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal) — subset",
            "reasoning_task_description": "Greedy-decoding yes/no evaluation on a random 2,000-sample subset of the same controlled tasks.",
            "method_or_approach": "Greedy decoding (rather than conditional-probability soft scoring); reported for reference.",
            "performance": "Greedy accuracy ≈ 0.926 on the subset (Table 1), but not directly comparable to Acc_soft metrics used for open models.",
            "baseline_comparison": "Higher greedy-decoding accuracy than most open models per the reported subset; evaluation modality differs (so direct comparisons are cautioned).",
            "limitations_or_failures": "Not evaluated under the probability-based protocol; small subset and greedy-decoding make results not directly comparable to the main reported metrics.",
            "insights_or_conclusions": "Commercial/closed models may obtain higher greedy-decode accuracy on sampled subsets, but the paper stresses the importance of using conditional probability measures and careful comparability.",
            "uuid": "e8620.11",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Gemini-1.5-Pro (ref)",
            "name_full": "Gemini-1.5-Pro (commercial model, greedy-decoding reference)",
            "brief_description": "A commercial Google (Gemini) family model evaluated greedily on a subset for reference; greedy-decoding scores reported but not directly comparable to the probability-based protocol.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5-Pro",
            "model_description": "Commercial LLM (Gemini family); conditional probabilities not available to authors, so greedy-decoding on a subset used as a reference.",
            "model_size": "not specified",
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal) — subset",
            "reasoning_task_description": "Greedy-decoded yes/no answers on a 2,000-sample subset used as a reference.",
            "method_or_approach": "Greedy decoding; used for reference only.",
            "performance": "Greedy accuracy ≈ 0.859 on the reported subset (Table 1); not directly comparable to Acc_soft.",
            "baseline_comparison": "Reportedly lower than OpenAI-o1 on the subset but higher than many open models; direct comparisons limited by evaluation differences.",
            "limitations_or_failures": "Not evaluated with probability-based protocol; reference-only subset limits generalization.",
            "insights_or_conclusions": "Highlights evaluation-protocol dependence: greedy-decode scores can differ from probability-based soft-accuracy outcomes.",
            "uuid": "e8620.12",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Human participants",
            "name_full": "Human behavioral participants (Prolific sample)",
            "brief_description": "Human participants (n=710) performed the same yes/no syllogism tasks to provide behavioral baselines and comparisons with LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "human participants (Prolific)",
            "model_description": "Adult fluent English speakers recruited via Prolific; responses are binary (Yes/No) with response-time collection; data used to compare human vs LLM reasoning patterns.",
            "model_size": null,
            "reasoning_task_name": "Hypothetical and disjunctive syllogisms (propositional + alethic modal)",
            "reasoning_task_description": "Same controlled natural-language yes/no entailment prompts as used with LLMs.",
            "method_or_approach": "Behavioral experiment with binary responses, randomized key mapping, and mixed-effects logistic regression to analyze modality and argument-form effects.",
            "performance": "Average accuracy ≈ 0.595 (Table 1); humans showed the ordering 3 (possibility) ≻ ∅ (propositional) ≻ 2 (necessity) similar to LLMs, and highest accuracy on modus ponens.",
            "baseline_comparison": "LLMs sometimes achieved higher benchmark performance than humans on the dataset, but humans and models share some argument-form preferences while differing in modality biases.",
            "limitations_or_failures": "Sample size is moderate; synthetic dataset and English-only limit ecological validity; humans did not show the same strong rejection bias on necessity that LLMs did.",
            "insights_or_conclusions": "Humans and LLMs exhibit both similarities (argument-form preferences) and differences (LLM modality biases), underscoring that LLMs are not straightforward human models and cautioning against overgeneralization.",
            "uuid": "e8620.13",
            "source_info": {
                "paper_title": "Logical forms complement probability in understanding language model (and human) performance",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Typicality-aware perplexity and its relation to evaluation of language model behavior (Gonen et al., 2023)",
            "rating": 2,
            "sanitized_title": "typicalityaware_perplexity_and_its_relation_to_evaluation_of_language_model_behavior_gonen_et_al_2023"
        },
        {
            "paper_title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve (McCoy et al., 2024)",
            "rating": 2,
            "sanitized_title": "embers_of_autoregression_show_how_large_language_models_are_shaped_by_the_problem_they_are_trained_to_solve_mccoy_et_al_2024"
        },
        {
            "paper_title": "Systematic testing of three language models reveals low language accuracy, absence of response stability, and a yes-response bias (Dentella et al., 2023)",
            "rating": 2,
            "sanitized_title": "systematic_testing_of_three_language_models_reveals_low_language_accuracy_absence_of_response_stability_and_a_yesresponse_bias_dentella_et_al_2023"
        },
        {
            "paper_title": "A systematic comparison of syllogistic reasoning in humans and language models (Eisape et al., 2024)",
            "rating": 2,
            "sanitized_title": "a_systematic_comparison_of_syllogistic_reasoning_in_humans_and_language_models_eisape_et_al_2024"
        },
        {
            "paper_title": "Transformers as soft reasoners over language (Clark et al., 2021)",
            "rating": 1,
            "sanitized_title": "transformers_as_soft_reasoners_over_language_clark_et_al_2021"
        },
        {
            "paper_title": "LogicBench: Towards systematic evaluation of logical reasoning ability of large language models (Parmar et al., 2024)",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models_parmar_et_al_2024"
        },
        {
            "paper_title": "Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples (Saparov et al., 2023)",
            "rating": 2,
            "sanitized_title": "testing_the_general_deductive_reasoning_capacity_of_large_language_models_using_ood_examples_saparov_et_al_2023"
        },
        {
            "paper_title": "A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models (Wan et al., 2024)",
            "rating": 2,
            "sanitized_title": "a_b_b_a_triggering_logical_reasoning_failures_in_large_language_models_wan_et_al_2024"
        },
        {
            "paper_title": "Conditional and Modal Reasoning in Large Language Models (Holliday & Mandelkern, 2024)",
            "rating": 2,
            "sanitized_title": "conditional_and_modal_reasoning_in_large_language_models_holliday_mandelkern_2024"
        }
    ],
    "cost": 0.0189945,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logical forms complement probability in understanding language model (and human) performance
13 Feb 2025</p>
<p>Yixuan Wang yixuanwang@uchicago.edu 
Freda Shi </p>
<p>University of Chicago</p>
<p>University of Waterloo Vector Institute
Canada</p>
<p>CIFAR AI Chair</p>
<p>Logical forms complement probability in understanding language model (and human) performance
13 Feb 20257C26C2F92F446DEC7929DA886FDAC03DarXiv:2502.09589v1[cs.CL]
With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question.This work conducts a systematic investigation of LLMs' ability to perform logical reasoning in natural language.We introduce a controlled dataset of hypothetical and disjunctive syllogisms in propositional and modal logic and use it as the testbed for understanding LLM performance.Our results lead to novel insights in predicting LLM behaviors: in addition to the probability of input(Gonen et al., 2023;McCoy et al., 2024), logical forms should be considered as orthogonal factors.In addition, we show similarities and differences between the logical reasoning performances of humans and LLMs by comparing LLM and human behavioral results.</p>
<p>Introduction</p>
<p>Logical reasoning is a fundamental aspect of building AI systems for reliable decision-making (Kautz et al., 1992, inter alia)-given a set of premises, an AI system should be able to deduce valid conclusions.With the advent of large language models (LLMs; Touvron et al., 2023;Jiang et al., 2023; AI@Meta, 2024, inter alia), there has been a surge of interest in using these models to assist planning and decision-making (Huang et al., 2022, inter alia); therefore, understanding the logical reasoning capabilities becomes crucial in understanding the reliability and potential of LLMs in planning.While recent work has shown that LLMs exhibit decent performance on logical reasoning problems (Liu et al., 2020;Ontanon et al., 2022;Wan et al., 2024, inter alia), there is still a lack of fine-grained understanding of the logical formsamong many argument forms presented in natural language (Shieber, 1993), do LLMs perform equally well, or do they exhibit preferences for certain argument forms?Do more complex components of logical forms, such as modalities, matter for LLM performance?</p>
<p>In this work, we investigate the logical reasoning capabilities of LLMs by assessing their performance on different logical forms.We curate a dataset of hypothetical syllogisms and disjunctive syllogisms in propositional and modal logic, where some logical forms are semantically equivalent to others.These forms are then interpreted with meaningful real-world statements that mirror the reasoning in daily communication, or phrases that consist of nonsensical words (Figure 2).</p>
<p>We conduct a series of controlled experiments to analyze the performance of a set of LLMs on these questions.Although our findings generally align with those by Gonen et al. (2023) and McCoy et al. (2024), who suggest that LLMs excel on examples with high probability, our results indicate that logical form, including but not limited to modalities and argument forms, is a crucial complementary factor in predicting the performance of LLMs (Figure 1).Additionally, with meaningful real-world interpretations, we find that: 1. LLMs are still far from being perfect in atomiclevel propositional and modal logic reasoning.2. LLMs prefer an affirmative answer under the modality of possibility, whereas they prefer a negative answer under the modality of necessity.3.In line with the recent results on categorical syllogisms (Eisape et al., 2024), we verify on hypothetical and disjunctive syllogisms that LLMs achieve better performance on certain logical forms that humans perform well.However, some logical forms receive favor from LLMs, while the phenomena lack support from human intuition or human behavioral data.This paper is structured as follows.After reviewing related literature ( §2), we describe the dataset synthesis process ( §3).We report the LLM reasoning results on the synthesized dataset ( §4) and ground the LLM reasoning performance to human reasoning performance ( §5).We conclude by discussing the implications of our results and the limitations of this work ( §6).</p>
<p>Related Work</p>
<p>Logical reasoning benchmarks.Prior LLM logical reasoning benchmarks (Liu et al., 2020;Han et al., 2022, inter alia) focus on complex, multihop reasoning problems with manually annotated problems, making cross-problem comparisons challenging.Recent work has introduced benchmarks with synthesized natural-language questions using predefined logical formulas and substitution rules (Saparov and He, 2022;Saparov et al., 2023;Parmar et al., 2024;Wan et al., 2024, inter alia).Compared to them, our work uniquely incorporates modal logic, which has been largely unexplored in existing benchmarks-while Holliday and Mandelkern (2024) present a case study, our approach offers two key advances: controlled knowledge bias in logic interpretations ( §3.3) and a more rigorous statistical evaluation framework ( §4.1).</p>
<p>Propositional and modal logic reasoning in language models.Recent work has explored training and finetuning language models specifically for logical reasoning (Clark et al., 2021;Hahn et al., 2021;Tafjord et al., 2022).Our work differs in two key aspects: we evaluate general-purpose language models through prompting, a cost-efficient setup that has been widely adopted in recent years, and we focus on propositional and alethic modal logic rather than temporal (Hahn et al., 2021) or epistemic (Sileo and Lernould, 2023) logic.1 Additionally, unlike studies comparing LLM and human performance on categorical syllogisms (Eisape et al., 2024, inter alia), 2 we focus on hypothetical and disjunctive syllogisms with considerations of modality.</p>
<p>Human logic reasoning.Research on human reasoning capabilities has informed studies of LLM logical reasoning.Eisape et al. (2024) compared LLM syllogistic reasoning with human behavior results (Ragni et al., 2019) under the framework of the Mental Models Theory (Johnson-Laird, 1983).Lampinen et al. (2024) found similar content effects in human and LLM reasoning, supporting the need to control for common-sense knowledge in benchmarks ( §3.2).Belem et al. (2024) studied human and LLM perception of uncertainty at a lexical level.Compared to them, we focus on the modal logic reasoning process and contribute new behavioral data.</p>
<p>Dataset</p>
<p>We curate a dataset of natural-language multichoice questions to measure the logical inference performance of LLMs (Figure 2).First, we use both propositional and modal logical forms as templates ( §3.1).Then, we assign meanings (e.g., real-world interpretations or phrases with nonsensical words) to each propositional variable in the templates, and translate templates into naturallanguage Yes/No questions ( §3.2).</p>
<p>Background: Propositional and Modal Logic</p>
<p>Propositional logic studies the relation between propositions.In this framework, each proposition is typically represented by a variable, and multiple propositions combine with logical connectives (e.g., ∨ and →) to form compound propositions.</p>
<p>In propositional logic, a proposition can be evaluated as either true or false; however, this system can be overly simplistic when dealing with the complexity of real-world events.Consider the statement Alice is not eating., while it is true in a world where Alice is not eating, it may become false in a hypothetical possible world where Alice is indeed eating.This idea, known as possible world semantics (Kripke, 1959), provides a framework for expressing more nuanced statements about event possibilities, such as Alice may be eating and Alice must be eating.The former statement can be understood as there exists a possible world where Alice is eating, and the latter can be understood as in all possible worlds, Alice is eating. 3ormal modal logic (Kripke, 1963) follows this idea and extends propositional logic to reason about the necessity and possibility of events.A normal modal logic system L can be formally described in the Backus-Naur form as
L : φ := p | ¬φ | 2φ | 3φ | φ ∨ φ | φ ∧ φ | φ → φ,(1)
where p is a propositional variable that serves as an atom in L, ¬ is the negation operator, 2 is the necessity operator (must), 3 is the possibility operator (may), ∨ is logical disjunction (or), ∧ is logical conjunction (and), and → is the logical implication operator (if...then).φ denotes the syntactic category of a formula in L. The right-hand side of Eq. ( 1) describes all possible logical formulas under the system L: for example, if φ ∈ L, the rules imply that ¬φ ∈ L, 2φ ∈ L, and so on.Following the convention in logic, the operator precedence is {¬, 2, 3} ≻ {∨, ∧} ≻ {→}.Indeed, the operators (¬, 2, →) forms a functional complete set of operators under L. Suppose φ and ψ are variables that represent logical formulas.The logical or (∨) and logical and (∧) operators can be rewritten with logical not (¬) and logical implication (→), as follows:
φ ∨ ψ ⇔ ¬φ → ψ, (2) φ ∧ ψ ⇔ ¬ (φ → ¬ψ) .
Possibility operator 3 can also be derived from the necessity operator.</p>
<p>3φ ⇔ ¬2¬φ</p>
<p>(3) Deduction and sequent.Given a formula set Γ as premises, if a deduction to a conclusion φ exists using axiom schemata and inference rules under the normal modal logic, we say the premises infer the conclusion, and the deduction can be represented as a logic sequent Γ ⊢ φ.If a formula set Γ do not infer the conclusion, we denote it as Γ ⊬ φ and call it a non-entailment.</p>
<p>Translating Logic to Natural Language</p>
<p>An interpretation is a mapping from propositional variables to concrete meanings.For example, under the interpretation that p is "Jane is eating apples" and q is "John is eating oranges", the logical formula p ∨ q becomes "Jane is eating apples or John is eating oranges."Choices of interpretation, i.e., the concrete content of the sentence, should not affect the underlying logical reasoning process.However, in natural-language utterances, reasoning can be influenced by various confounding factors.Knowledge bias is a common pitfall.For example, given the logical form {2p → 2¬q, 2p} ⊢ 2¬q, regardless of p's interpretation, if we interpret ¬q := "Cats are not animals" then the conclusion will be "It is certain that cats are not animals."But common-sense knowledge suggests that "it is certain that cats are animals" (2q), which logically contradicts the existing premise set.4Such bias will complicate logical reasoning (Lampinen et al., 2024) and should be avoided in data curation.Besides, each variable should have independent interpretation, as detailed in Appendix B.1.</p>
<p>After being assigned interpretations, each logical form is further articulated as a yes-no question on whether the conclusion can be inferred from the premises.To mitigate the ambiguity in natural language, we design heuristic rules to translate logic forms into less ambiguous English, which are detailed in Appendix B.1.For the exact wordings we</p>
<p>Logic Content</p>
<p>Figure 2: The data synthesis pipeline: for each variable in logic forms ( §3.1) we assign meanings to them to obtain the natural language question-answering pairs ( §3.2). used, see Table A1 in Appendices.If a valid deduction exists (⊢) for the logical form, the ground truth answer is Yes, otherwise No.The answer is solely determined by the logical form and is independent of the interpretation.</p>
<p>Involved Logical Forms</p>
<p>Translated logical forms can have varying degrees of naturalness.For example, the necessitation rule {φ} ⊢ 2φ, which translates to "φ is true; therefore, it is certain that φ is true," appears to be unnatural due to redundancy. 5 Based on the relationship between ∨ and → in Eq. ( 2), we use hypothetical and disjunctive syllogisms with four basic variants:
{φ ∨ ψ, ¬φ} ⊢ ψ, (∨ L ) {¬φ → ψ, ¬φ} ⊢ ψ, (→ L ; modus ponens) {φ ∨ ψ, ¬ψ} ⊢ φ, (∨ R ) {¬φ → ψ, ¬ψ} ⊢ φ.
(→ R ; modus tollens) Despite the semantic similarity, these logical forms translate to different natural-language questions.For example, taking the interpretations of φ := Jane is watching a show and ψ := John is reading a book, ∨ L translates to Consider the following statements: Jane is watching a show or John is reading a book.Jane isn't watching a show.Question: Based on these statements, can we infer that John is reading a book?</p>
<p>With the same interpretation, → L translates to 5 Nevertheless, we report the experiment results on necessitation rule in Appendix C.1.</p>
<p>Consider the following statements:</p>
<p>If Jane isn't watching a show, then John is reading a book.Jane isn't watching a show.Question: Based on these statements, can we infer that John is reading a book?</p>
<p>According to the commutativity of disjunction operator, we group ∨ L and ∨ R together as disjunctive syllogism, alongside two hypothetical syllogism groups, modus ponens (→ L ) and modus tollens (→ R ).All the logical forms shown above are valid sequents with ground-truth answer Yes.To balance the dataset, we introduce some logic fallacies that generate questions with ground-truth label No.By flipping the second premises and the conclusions, we obtain the following fallacies:
{φ ∨ ψ, ψ} ⊬ ¬φ, (∨ L ⊬ ) {¬φ → ψ, ψ} ⊬ ¬φ, (→ L ⊬ ) {φ ∨ ψ, φ} ⊬ ¬ψ, (∨ R ⊬ ) {¬φ → ψ, φ} ⊬ ¬ψ, (→ R ⊬ )
where ∨ L ⊬ and ∨ R ⊬ are grouped as affirming the disjunction, → L ⊬ and → R ⊬ corresponds to affirming the consequent and denying the antecedent, respectively.In our dataset, we require the formulas φ and ψ to the form of Mp and Mq, where p and q are propositional variables, each assigned with an interpretation.Both variables are constrained under the same modality M, which can be necessity (2), possibility (3) or no modality (∅).Pairing with four rules and theorem-fallacy variations, we have a total of 3 × 4 × 2 = 24 forms.</p>
<p>Involved Logic Interpretations</p>
<p>For logic interpretations, we generate a set of verb phrases by prompting the CodeLlama 2 model (Rozière et al., 2024), and select 204 of them manually.and combine them with top-200 popular baby names in the US into subject-verb-object pairs, 6 such as (Ray, make, a pizza).We randomly generate 1000 interpretations with two pairs each.The same set of interpretations is applied to variables p, q in each logic sequent's natural langauge template.In total, there are 24 × 1000 = 24000 question, with samples shown in Table A1.</p>
<p>Experiment</p>
<p>Metrics and Investigated Models</p>
<p>Hu and Levy ( 2023) have suggested that the standard approach of greedily decoding yes-no strings (Dentella et al., 2023) may underestimate the competence of a language model; therefore, we adopt a probability-based metric to evaluate the model performance.In our evaluation protocol, the predicted likelihood of the tokens Yes and No, conditioned on the prompt s-denoted as p(Yes | s) and p(No | s), respectively-serve as the soft labels for yes-no answers.The soft accuracy p on the single example with ground-truth answer y ∈ {Yes, No} is defined as the relative probability of y:
p = p(No | s)1[y = No] + p(Yes | s)1[y = Yes] p(No | s) + p(Yes | s) ,
where 1[•] is the indicator function that returns 1 if the condition is true and 0 otherwise.This relative probability can also be viewed as the confidence score of the model on the ground-truth answer.The soft accuracy Acc soft of a model on the entire dataset D is defined as the average soft accuracy over all examples,
Acc soft = 1 |D| |D| i=1 pi .
We use a zero-shot setting to investigate the general performance of the models' logical inference capabilities-while adding detailed instructions or few-shot demonstrations may increase the absolute performance, they are at the cost of introducing possibly undesired confounding factors or behaviors, such as simply copy-pasting the answers in the examples.We evaluate on the following models with opensourced weights: mistral-7b-v0.2 and -8x7b (Jiang et al., 2023(Jiang et al., , 2024)); llama-2-7b, -13b and -70b (Touvron et al., 2023); 3.1 version of llama-3-8b and  4), along with their 95% confidence intervals.</p>
<p>-70b (AI@Meta, 2024); yi-34b (01.AI, 2024); phi-2 and phi-3-mini (Microsoft, 2023, 2024).7</p>
<p>Results: Performance w.r.t. Logical Forms</p>
<p>We evaluate the aforementioned models with the probability-based protocol (Table 1).Generally, models that rank higher in the leaderboard also achieve higher soft accuracy on our dataset.The break-down accuracies on modalities and argument forms reveal that:</p>
<ol>
<li>(Modality) All models consistently perform better on the possibility (3) than necessity (2) or plain propositional logic.</li>
</ol>
<p>(Argument Forms)</p>
<p>The pattern is more diverse, yet most of the models struggle the most on modus tollens (→ R ⊢ ) within logic sequents (i.e., questions with ground-truth answers Yes), and affirming the consequent (→ L ⊬ ) within fallacies.</p>
<p>Analysis on Logic Sequents</p>
<p>To systematically analyze the effect on model performance of each factor of interest, as well as crossvalidating the observations above, we fit a linear mixed-effects model (Raudenbush, 2002) to the soft accuracy data on valid logic sequents (i.e. with ground truth of Yes) across different LLMs and  (Fourrier et al., 2024).Each argument form category denotes the union of the fine-grained categories specified in the superscripts and subscripts-for example, ∨ L,R ⊢ denotes the entire disjunctive syllogism group.Boldfaced values indicate the row-wise maximum for each factor.Note that due to technical limitation of commercial LLMs, results from OpenAI-o1 (OpenAI, 2024) and Gemini-1.5-pro(Team et al., 2024) are greedy-decoding based evaluation on 2,000 random samples that serve as references, and are therefore not directly comparable to other probability based evaluations.Human results are detailed in §5.
Overall Leaderboard Modality Argument Form Model (Rank) (Rank) ∅ 2 3 ∨ L,R ⊢ → L ⊢ → R ⊢ ∨ L,R ⊬ → L ⊬ → R ⊬ mistral-</p>
<p>Hypothesis</p>
<p>p-value propositional &lt; may &lt; 0.001 must &lt; propositional &lt; 0.001 must &lt; may &lt; 0.001 disjunctive &lt; modus ponens &lt; 0.001 modus tollens &lt; modus ponens &lt; 0.001 modus tollens &lt; disjunctive &lt; 0.001</p>
<p>Table 2: Hypothesis testing results on the effect of logical form factors on soft accuracy (Figure 3).logical forms,
Acc soft ∼ Modality + ArgForm + Perplexity + (1 + Perplexity | LLM),(4)
with the linear fixed effects of (i.) modality, (ii.) argument form, and (iii.)input perplexity.Individual probability, coupled with a constant term, is modeled as a random effect to account for potential model-specific biases.Here, Perplexity denotes the perplexity of the input text (x 1 x 2 . . .x N ), which is defined as the exponential of the token-wise average negative log-likelihood of the text given a specific language model:
Perplexity = exp − 1 N N i=1 log p(x i | x &lt;i )
The mixed-effects model yields a marginal R 2 of 0.342 and a conditional R 2 of 0.543, suggesting a reasonable predictive power.The likelihood ratio test on the full regression model vs. the null regression model without each of the fixed effects yields a significant result (p &lt; 0.001), suggesting the importance of all these factors in determining the model performance.2024), we find a negative correlation between perplexity and soft accuracy (p &lt; 0.001); however, the correlation between them is weak (ρ = −0.09),which suggests the necessity of the complementary factors below in predicting LLM performance.</p>
<p>For different modalities and argument forms, we estimate their marginal means on soft accuracy (Figure 3), and perform pairwise hypothesis testing on the estimated coefficients (Table 2).The results generally align with the general observations on the full dataset.The only exception is that modus ponens (→ L ), instead of disjunctive syllogism (∨), appears to be the easiest argument form (i.e., the one with the highest soft accuracy) among all.</p>
<p>Random effects.We analyze the per-LLM random effects on the soft accuracy (Figure 4).All the model-specific mixed effects of perplexity are negative, suggesting the negative correlation between perplexity and soft accuracy is consistent across models (Figure 4a).While the intercept random effects are not perfectly aligned with the Intercept random effects (i.e., constant term per model on soft accuracy), with the model performance rank (Table 1) annotated in parentheses.</p>
<p>model performance-since the perplexity random effects may introduce confounding factors-higherranked models generally tend to have higher intercept random effects (Figure 4b), which crossvalidates the general performance ranking.</p>
<p>Extended Analysis on the Negative Perplexity-Performance Correlation</p>
<p>We further investigate the negative correlation between perplexity and model performance through a controlled experiment: we create a mirror dataset of the same size, keeping all the logical formulas while interpreting them with nonsensical words.For example, the formula 3(φ ∨ ψ) may be interpreted as it's possible that Neva is balaring a montery or Lucille is sweeling prandates, where the underlined words and phrases are nonsensical.</p>
<p>Intuitively, the perplexity of the problems in this mirror dataset should be much higher than that of the primary dataset problems ( §3) under any reasonably trained language model.We analyze the correlation between perplexity and model performance (Figure 5).As desired, the perplexity of problems with nonsensical words are indeed much higher than that of the primary dataset (≈ 20 − 30 vs. ≈ 10).The significant portion of horizontal and inclined lines in the figures again suggests that perplexity is not a reliable predictor of model performance.Meanwhile, the overall parallelism of the lines echos our results that logical forms are important factors for such prediction.</p>
<p>The Affirmation Bias over Modalities</p>
<p>One key argument of Dentella et al. (2023) is that large language models exhibit a bias towards affirming the claim, i.e., answering Yes more frequently than No.</p>
<p>We investigate this phenomenon by fitting a mixed-effects model
P (Yes | s) P (Yes | s) + P (No | s) ∼ Modality + ArgForm + Perplexity +(1 + Perplexity | LLM),(5)
which has the same structure as Eq. ( 4), except the dependent variable being the relative probability of answering Yes conditioned on input text s.</p>
<p>We present the estimated marginal means of the factors in the mixed-effects model (Figure 6).While our results confirm the affirmation bias on propositional logic, such bias is slightly less pronounced on the possibility modality (3, around 0.03), and the models even show a bias towards rejecting claims under the necessity modality (2).</p>
<p>Human Experiments</p>
<p>LLMs are trained on text produced by humans and are able to generate plausible text; therefore, there have been interests in using LLMs as human models (Eisape et al., 2024;Misra and Kim, 2024, inter alia).Following this line of work, we conduct a human behavioral experiment to ground the LLM reasoning behavior.Using samples from our primary dataset, we collected 710 responses from adults fluent in English through Prolific. 8More experiment details can be found in Appendix A.2.</p>
<p>The average human accuracy on each group is shown in the last row of Table 1.9 Aligned with our LLM results ( §4), on modalities the overall human results also show an accuracy order of (3 ≻ ∅ ≻ 2), and on argument forms, modus ponens (→ L ) is the most accurately answered pattern.</p>
<p>To further investigate the interactions of logic factors, we fit a generalized linear mixed-effects model (Bates et al., 2015) to verify the effect of modality and argument forms on human logic reasoning accuracy (Eq.( 6) and Figure 7).where Acc is the binary accuracy of human responses, and Rt is the response time.The generalized mixed-effects model yields a marginal R 2 of 0.121 yet a 0.419 conditional R 2 , indicating a diverse response pattern across participants.The likelihood ratio test on the full model against the null model shows that only the effect of argument form is significant (χ 2 (2) = 25.6, p &lt; 0.001).However, in accordance with the overall performance, we find modus ponens (→ L ) has a significantly higher effect than other two valid argument forms.This confirms that logical forms can also have a significant impact on human reasoning accuracy, which is consistent with the LLM results, although the effect sizes are not the same.</p>
<p>Conclusion and Discussion</p>
<p>We present an analysis of hypothetical and disjunctive syllogisms on propositional and modal logic and systematically analyze the LLM performance on the dataset.Our analysis provides novel insights on explaining and predicting LLM performance: in addition to the perplexity or probability of the input  6), along with their 95% confidence intervals.</p>
<p>text, the underlying logic forms play an important role in determining the performance of LLMs.In addition, we compare the behaviors of LLMs and humans using the same data through human behavioral experiments.We discuss the implications of our results as follows.</p>
<p>Probability in language models.Probability and, relatedly, perplexity are often used as intrinsic evaluation metrics for language models.While Gonen et al. ( 2023) and McCoy et al. (2024) show that probability and perplexity, to some extent, correlates with LLM performance, literature in program synthesis with LLMs shows little correlation between probability and execution-based evaluation results (Li et al., 2022;Shi et al., 2022).This work does not necessarily contradict either line, but rather provides complementary factors for analyzing LLM performance.We argue that probability may have become an overloaded term in analyzing LLMs.Low probability may be due to one or more of the following nonexhaustive reasons: (1) the sentence is out of context, (2) the sentence is in ungrammatical language, or (3) the sentence is grammatical but has semantically awkward content (e.g., the mirror dataset in §4.2.2), (4) the sentence contains rare content while being reasonable.We hypothesize that the probability of language models may not be essentially able to capture all these nuanced differences, and call for encoding and decoding algorithms-such as Meister et al. (2023)-that can better decompose the probability into finer-grained components.</p>
<p>Comparing humans and LLMs.What is our goal for building LLMs?Is it to achieve better performance on practical tasks or to build a more human-like model?Our results, together with Eisape et al. ( 2024), suggest that these two goals may not be perfectly aligned by revealing a mixture of similarity and discrepancy between LLMs and humans-for example, while LLMs exhibit higher benchmark performance than humans on our dataset and show the same argument form preferences with humans (Figures 3 and 7), they also show systematic biases that humans do not have (e.g., disfavoring the necessity modality, §4.2.3).While there has been positive evidence of using LLMs as human models in psycholinguistic studies (Misra and Kim, 2024, inter alia), our results reaffirm that we should execute such approaches cautiously.</p>
<p>On the relation between modality and performance.Our results show that there is a significant difference in performance between necessity and possibility modalities, with the former much lower than the latter (Table 1).Part of the reason for this is that LLMs have a significant tendency to say "No" to necessity modality (Figure 6).</p>
<p>On the one hand, our results extend the conclusion of Dentella et al. (2023), who found that LLMs generally respond positively-LLM behaviors may be significantly affected by finer-grained factors, including but not necessarily limited to the logical modality of the input.On the other hand, while LLMs systematically tend to answer "No" to questions in necessity modality, we do not find related evidence in human experiments, which leads to us hypothesize that such rejection bias comes from either the model architecture or the training strategies, such as the reinforcement learning with human feedback (RLHF; Ouyang et al., 2022) protocol.We leave this as an open question for future research.</p>
<p>Modal logic and theory of mind.Modality, in principle, encodes mental states and beliefs.The reasoning of beliefs also resonates with the theory of mind (Premack and Woodruff, 1978;Baron-Cohen et al., 1985, inter alia) and machine theory of mind (Rabinowitz et al., 2018;Ma et al., 2023, inter alia).Following the effort by Sileo and Lernould (2023) that uses epistemic modal logic to model the machine theory of mind, our work assesses the behaviors of LLMs on alethic modal logic, distantly revealing the future potential of LLMs in achieving the theory of mind.</p>
<p>Limitations</p>
<p>This work comes with two major limitations: 1.While we have verified that our data has a low perplexity (9.82 ± 2.47 under mistra-7b; much lower than that of the data by Wan et al. (2024), 25.44), and, therefore, are similar enough to natural language utterances, the synthetic language cannot fully substitute natural language in daily life.Our dataset and analysis are not comprehensive enough to cover many nuanced examples that may appear in real communication, especially when context-dependent understanding is crucial to conveying communication goals.</p>
<ol>
<li>Despite more than 7,000 languages worldwide, as a first step, our material only covers English.This narrow focus is due to the languages the authors are proficient in and the coverage of the language models.We acknowledge the importance of extending the scope of this work to a more comprehensive set of languages and leave the extension as an immediate follow-up step.</li>
</ol>
<p>In addition, the sample size of human experiments is somewhat limited.We leave more comprehensive human behavioral data collection and analysis to future work. in the atomic logical interpretations, reviewed by all the authors.In addition, we have ensured that all participants are paid a fair wage through the Prolific platform.Instructions and consent forms delivered to the participants can be found in the Appendix A.2.The institutional ethics review board has approved the data collection process.</p>
<p>We do not foresee risk beyond the minimal risk posed by LLM evaluation work.We acknowledge that using LLMs in real-world scenarios could significantly impact human behaviors, raising the need for model transparency, safety, security, and interpretability.This work contributes to the understanding of We will open-source the synthetic logical reasoning dataset upon publication.</p>
<p>A Additional Experiment Details</p>
<p>A.1 LLM Experiment Details All LLMs used are obtained from Hugging Face checkpoints.Time and compute power requirements vary, the largest llama-3-70b model takes around 2 hours on NVIDIA A6000 GPU to obtain all results in §4.</p>
<p>A.2 Human Experiment Details</p>
<p>Participant consent.We use the following language to obtain consent from participants, where our institution name is replaced with the Anonymous Institution to protect the anonymity of submission.</p>
<p>This study is part of a scientific research project at the Anonymous Institution.Your decision to complete this study is voluntary.There is no way for us to identify you.The only information we will have, in addition to your responses, is the demographic information you provided to Prolific and the time at which you completed the survey.The results of the research may be presented at scientific meetings or published in scientific journals.Clicking on the button below indicates that you are at least 18 years of age and agree to complete this study voluntarily.Press the button below to start the experiment.</p>
<p>Participant instructions.We use keys F and J, which are roughly symmetric on a standard English keyboard, to collect participant responses.Half of the participants see the following instruction:</p>
<p>In this study, you will be presented with two statements followed by a question.Your task is to answer either Yes or No to the question, based on the information provided in the statements.Please respond quickly and accurately by pressing "F" for Yes, and "J" for No.</p>
<p>To mitigate the possible bias introduced by the dominant hand, we have the other half of the participants see instruction with reversed keys: In this study, you will be presented with two statements followed by a question.Your task is to answer either Yes or No to the question, based on the information provided in the statements.Please respond quickly and accurately by pressing "F" for No, and "J" for Yes.</p>
<p>Participant wage.We offer participants an hourly wage of 1.5 times Prolific's minimum wage.The duration is determined by the median completion time among all participants.</p>
<p>B Extra Details of the Dataset B.1 Considerations in Translating Logical</p>
<p>Form to Natural Language</p>
<p>During the interpretation process, another key point is to assign independent interpretations to variables.Deciding the dependency also involves common sense knowledge.For example, consider the premises ¬p → q and q.If we interpret p := This group of rules and fallacies comes from the fact that the necessity modality 2 not distributive to disjunction, i.e. 2(φ ∨ ψ) ⊬ 2φ ∨ 2ψ (Xiang, 2019, Ex. 5).In contrast, the possibility modality 3 is distributive to disjunction.This particular case could have served as a material to test the LLM's knowledge of the asymmetry between the two modalities, yet in §4.2.3 we showed that there is a bias towards rejection on the necessity modality.As the false case of the disjunction is on the necessity modality, this bias confounds the experiment.</p>
<p>We fit a linear mixed-effects model similar to Eq. (4) to the data,
Acc soft ∼ Modality × ArgForm + Perplexity + (1 + Perplexity | LLM),
with an interaction term between the modality and argument form.On the theorem form compared to the base form, the necessity modality 2 has a 0.173 higher estimated marginal means with p &lt; 0.0001 significance, yet the possibility modality 3 has a 0.071 lower estimated marginal means.On the spurious form compared to the base form, the 2 has a 0.312 higher means, and the 3 has no significant difference.On both forms, 3 ≻ 2 in terms of accuracy still holds at a slight margin of 0.110 and 0.047 respectively.To verify whether on 2 the performance increase on spurious form is due to the rejection bias, we fit a linear mixed-effects model with the relative probability of answering Yes as dependent variable.Results show that on spurious form compared to the base form, of 2's tendency to answer Yes is only 0.060 lower, indicating the rejection bias of the base form is still present.Therefore, we hypothesize that the LLM's performance on recognizing the fallacy of necessity distribution over disjunction is hindered by the rejection bias on the necessity modality.</p>
<p>Figure 1 :
1
Figure1: Illustration of the fact that perplexity does not serve as a reliable indicator of logical reasoning performance; and therefore, so does probability.The distributions of the probabilities assigned to the ground-truth answer (i.e., soft accuracy; Y-axis) by Llama-3-70B are plotted against the perplexity of the corresponding example question (X-axis) and grouped by (a) modality, (b) argument forms, and (c) logic interpretation content.Each group consists of 20 randomly selected examples with other factors controlled.</p>
<p>6Figure 3 :
3
Figure 3: Estimated marginal means of logical form factors in the mixed-effects model of Eq. (4), along with their 95% confidence intervals.</p>
<p>Fixed effects .
effects
In line with Gonen et al. (2023) and McCoy et al. (</p>
<p>Figure 4 :
4
Figure 4: Illustration of per-model random effects on soft accuracy in the mixed-effects model of Eq. (4) with 99.9% confidence intervals.(a) Mixed effects (i.e., the sum of fixed and random effects) of perplexity.(b) Intercept random effects (i.e., constant term per model on soft accuracy), with the model performance rank (Table1) annotated in parentheses.</p>
<p>logit(Acc) ∼ Modality + ArgForm + Rt + (1 + Rt | ParticipantID), (6)</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Correlation between mean perplexity and mean confidence score on each logic sequent.Each point represents an average over a group of 1000 prompts that share the same underlying logic sequent.Two connected dots share the same logic formula.</p>
<p>Figure 7 :
7
Figure 7: Estimated marginal means of logical form factors in the generalized mixed-effects model of Eq. (6), along with their 95% confidence intervals.</p>
<p>It is true that φ or it is true that ψ.It isn't true that φ.Is it true that ψ?Consider the following statements: It is true that Jane is watching a show or . . . it is true that John is reading a book.It isn't true that Jane is watching a show.Question: Based on these statements, can we infer . . .
{φ ∨ ψ, ¬φ} ⊢ ψFormTemplateWord PairsInterpretationthat John is reading a book?Answer:(Jane, watch, show) (John, read, book)φ = Jane is watching a show ψ = John is reading a book</p>
<p>Table 1 :
1
Overall and break-down accuracies of different models, as well as their HuggingFace OpenLLM Leaderboard performance and relative ranking
7b0.645 (4)0.145 (7)0.464 0.496 0.974 0.877 0.663 0.280 0.434 0.653 0.939mistral-8x7b0.724 (1)0.193 (5)0.698 0.601 0.874 0.963 0.873 0.023 0.757 0.648 0.813llama-2-7b0.335 (10) 0.094 (10) 0.262 0.207 0.538 0.444 0.147 0.315 0.208 0.451 0.468llama-2-13b0.513 (9)0.110 (9)0.488 0.362 0.688 0.418 0.581 0.393 0.631 0.436 0.591llama-2-70b0.611 (5)0.127 (8)0.616 0.471 0.746 0.446 0.845 0.518 0.775 0.389 0.694llama-3-8b0.565 (6)0.239 (3)0.598 0.460 0.639 0.526 0.470 0.332 0.664 0.625 0.716llama-3-70b0.714 (2)0.362 (1)0.745 0.554 0.843 0.606 0.773 0.515 0.882 0.661 0.788yi-34b0.518 (8)0.226 (4)0.457 0.413 0.683 0.346 0.498 0.205 0.685 0.638 0.737phi-20.532 (7)0.155 (6)0.469 0.456 0.673 0.670 0.757 0.522 0.365 0.402 0.510phi-3-mini0.690 (3)0.272 (2)0.657 0.536 0.877 0.839 0.974 0.475 0.664 0.462 0.604OpenAI-o10.926 N/A N/AN/A 1.000 0.773 1.000 0.895 1.000 0.775 0.919 1.000 1.000Gemini-1.5-Pro 0.859 N/A N/AN/A 0.831 0.748 0.997 1.000 1.000 0.919 0.661 0.991 0.638human0.595 N/A N/AN/A 0.589 0.566 0.640 0.691 0.901 0.628 0.594 0.225 0.411</p>
<p>Table A2 :
A2
Overall accuracy of the necessitation rule and its modality variants on each model.
∅23mistral-7b0.9980.8850.999mistral-8x7b0.9570.5400.987llama-2-7b0.7680.0130.920llama-2-13b0.3680.0040.829llama-2-70b0.5110.0510.834llama-3-8b0.3980.2250.783llama-3-70b0.6740.3840.794yi-34b0.9600.3820.999phi-20.8140.2260.892phi-3-mini0.9920.9250.994Modality Argument Form Logical Form∅baseφ ∨ ψ, ¬φ ⊢ ψ2base2φ ∨ 2ψ, ¬2φ ⊢ 2ψ2theorem2(φ ∨ ψ), 2¬φ ⊢ 2ψ2spurious2(φ ∨ ψ), ¬2φ ⊬ 2ψ3base3φ ∨ 3ψ, ¬3φ ⊢ 3ψ3theorem3(φ ∨ ψ), 3¬φ ⊢ 3ψ3spurious3(φ ∨ ψ), ¬3φ ⊢ 3ψ</p>
<p>Table A3 :
A3
Logical forms and their ground truth to study the distribution of modalities.Only the spurious form of the necessity modality (marked by underline) has a ground truth of false.
It's certain that if Freddy is not goingshopping, then Coy is making dinner.(theorem) It's certain that Freddy is not going shop-ping.(spurious) It's uncertain whether Freddy is goingshopping.Can we infer that it's certain that Coy ismaking dinner?
We acknowledge that any logic that involves non-truthfunctional operators, including but not limited to first-order logic, temporal logic, and epistemic logic, can be viewed as a modal logic; however, we adopt the most restrictive sense of modal logic(Ballarin
, 2023) and use it interchangeably with alethic modal logic.2 We refer readers toZong and Lin (2024) for a more comprehensive review of categorical syllogisms.
The possible world semantics, therefore, connects the notion of necessity and possibility to the universal and existential quantification (∀, ∃) under first-order logic.
This confounding factor affects the examples in Table18ofHan et al. (2022).
Our evaluation protocol technically requires the conditional probabilities of specified answers given a prompt, which are not supported by most commercial models; however, we report the greedy-decoding accuracy of these models on a sample subset for reference.
https://prolific.com
Human responses are binary classes, so correct and incorrect responses are coded as 1 and 0, respectively.
AcknowledgementsWe thank Yudong Li for his help in setting up the Gemini and OpenAI API for the experiments.This work was supported in part by a Google PhD Fellowship and a Canada CIFAR AI Chair award to FS, as well as NSERC RGPIN-2024-04395.Ethics StatementWhile this work involves human logical reasoning experiments, we have ensured that (1) the data are generated procedurally following templates listed in the paper and (2) there is no harmful content "Jane is inside the house" and q := "Jane is out" to proposition variables p and q, the two variables are possibly not independent.According to common sense, "Jane is not inside the house" (¬p) correlates with or is even equivalent to "Jane is out" (q).Logically, {¬p → q, q} ⊬ ¬p; however, with the extra premise ¬p ↔ q given by common sense, people may conclude that ¬p. 10  Besides, natural language is ambiguous-one sentence in natural language can come from multiple logical forms under the same interpretation.We use present tense and progressive aspect to encourage a reading of imaginary ongoing events, corresponding to the alethic modality.Such events are less likely to induce LLM's or human's individual bias, as they are unrelated to factual knowledge or moral judgements.Also, we always use two full verb phrases, ruling out sentences like "Jane is eating apples or oranges," so the two events are less likely to be mutually exclusive.In this way, we can reduce the ambiguity of the questions in our dataset.B.2 Data SamplesAll logic forms and corresponding natural language sentences can be found in TableA1.The exact prompt format is as follows: We report the results on the necessitation rule and its variants here, as these rules are obscure and verbose to be articulated in natural language:Its natural language form is as follows:10 This confounding factor affects the examples in Figure10ofHolliday and Mandelkern (2024).Jane is watching a show.(2) Can we infer that it's certain that Jane is watching a show?(3) Can we infer that it's possible that Jane is watching a show?(∅) Can we infer that Jane is watching a show?All three variants are paired with 1000 logic interpretations.As they are all rules of inference, the ground truth answer is always Yes. Overall accuracy is shown in TableA2, where across all LLMs, the necessitation rule has the lowest accuracy.This echoes the necessity modality's tendency to be rejected discussed in §4.2.3.We further fit a linear mixed-effects model similar to Eq. (4), except that the argument form effect is now constant across all data points.The mixedeffects model yields a marginal R 2 of 0.391 and a conditional R 2 of 0.745.Estimated marginal means shows that the accuracy on ∅ is 0.171 less than 3, but 0.371 higher than 2, with both differences significant at p &lt; 0.0001.This further suggests that modality serves as an important factor on logic reasoning performance.C.2 Extra Experiment: Distribution of ModalitiesBesides the necessitation rule, distribution axiom is the other fundamental axiom in normal modal logic.It can be transformed into the rule shown in Eq. (A1), and plugging in the definition of ∨ in Eq. (2) gives the rule shown in Eq. (A2).Notice that Eq. (A2) closely resembles rule ∨ L 's variant with necessity, as shown in Eq. (A3), except the different scope of the necessity operator and the position of the negation operator.Moving the negation operator out of the necessity operator will result in a fallacy (Eq.A4).We say (A2) to (A4) are of argument form theorem, base and spurious, respectively.See TableA3for the logical forms and their ground truth we used to study the distribution of modalities.The natural language form is as follows:
Yi: Open Foundation Models by 01. A I , 10.48550/arXiv.2403.04652The Llama 3 Herd of Models. Ai Ai@meta, 2024. 2024</p>
<p>Modern origins of modal logic. Roberta Ballarin, The Stanford Encyclopedia of Philosophy, Fall 2023 edition. Edward N Zalta, Uri Nodelman, 2023Metaphysics Research Lab, Stanford University</p>
<p>Does the autistic child have a "theory of mind. Simon Baron-Cohen, Alan M Leslie, Uta Frith, 10.1016/0010-0277(85)90022-8Cognition. 2111985</p>
<p>Fitting Linear Mixed-Effects Models Using lme4. Douglas Bates, Martin Mächler, Ben Bolker, Steve Walker, 10.18637/jss.v067.i01Journal of Statistical Software. 1672015</p>
<p>Catarina G Belem, Markelle Kelly, Mark Steyvers, Sameer Singh, Padhraic Smyth, 10.48550/arXiv.2407.15814Perceptions of Linguistic Uncertainty by Language Models and Humans. 2024</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, IJCAI. 2021</p>
<p>Systematic testing of three language models reveals low language accuracy, absence of response stability, and a yes-response bias. Vittoria Dentella, Fritz Günther, Evelina Leivada, Proceedings of the National Academy of Sciences. 12051e23095831202023</p>
<p>A systematic comparison of syllogistic reasoning in humans and language models. Tiwalayo Eisape, Michael Tessler, Ishita Dasgupta, Fei Sha, Sjoerd Steenkiste, Tal Linzen, NAACL. 2024</p>
<p>Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, Thomas Wolf, Open llm leaderboard v2. 2024</p>
<p>Demystifying prompts in language models via perplexity estimation. Srini Hila Gonen, Terra Iyer, Noah Blevins, Luke Smith, Zettlemoyer, Findings of ACL. EMNLP2023</p>
<p>Teaching temporal logics to neural networks. Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus Norman Rabe, Bernd Finkbeiner, 2021In ICLR</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, Caiming Xiong, and Dragomir Radev. 2022. FOLIO: Natural Language Reasoning with First-Order Logic. Xi Victoria Lin</p>
<p>Conditional and Modal Reasoning in Large Language Models. H Wesley, Matthew Holliday, Mandelkern, ArXiv:2401.171692024</p>
<p>Prompting is not a substitute for probability measurements in large language models. Jennifer Hu, Roger Levy, 10.18653/v1/2023.emnlp-main.306Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, ICML. PMLR2022</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, 10.48550/arXiv.2310.06825Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed20237</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2024Mixtral of Experts</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. Philip Nicholas, Johnson-Laird , 1983Harvard University Press6</p>
<p>Planning as satisfiability. Bart Henry A Kautz, Selman, ECAI. Citeseer199292</p>
<p>A Completeness Theorem in Modal Logic. A Saul, Kripke, 10.2307/2964568The Journal of Symbolic Logic. 2411959</p>
<p>Semantical Analysis of Modal Logic I Normal Modal Propositional Calculi. A Saul, Kripke, 10.1002/malq.19630090502Mathematical Logic Quarterly. 95-61963</p>
<p>Language models, like humans, show content effects on reasoning tasks. Ishita Andrew K Lampinen, Dasgupta, C Y Stephanie, Chan, Antonia Hannah R Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 10.1093/pnasnexus/pgae233PNAS Nexus. 37e2332024</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial IntelligenceYokohama, Japan2020International Joint Conferences on Artificial Intelligence Organization</p>
<p>Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models. Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai, Findings of Empirical Methods in Natural Language Processing. 2023</p>
<p>Embers of autoregression show how large language models are shaped by the problem they are trained to solve. R Thomas Mccoy, Shunyu Yao, Dan Friedman, Mathew D Hardy, Thomas L Griffiths, 10.1073/pnas.2322420121Proceedings of the National Academy of Sciences. 12141e23224201212024</p>
<p>Locally typical sampling. Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell, Transactions of the Association for Computational Linguistics. 202311</p>
<p>Phi-2: The surprising power of small language models. 2023Microsoft Research Blog</p>
<p>Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone. 2024Microsoft</p>
<p>Generating novel experimental hypotheses from language models: A case study on cross-dative generalization. Kanishka Misra, Najoung Kim, arXiv:2408.050862024arXiv preprint</p>
<p>LogicInference: A new Datasaet for Teaching Logical Inference to seq2seq Models. Santiago Ontanon, Joshua Ainslie, Vaclav Cvicek, Zachary Fisher, ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality. OpenAI. 2024. Learning to reason with llms. 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, 10.1017/S0140525X00076512Behavioral and Brain Sciences. 141978</p>
<p>Machine Theory of Mind. Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S M Ali Eslami, Matthew Botvinick, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR2018</p>
<p>Marco Ragni, Hannah Dames, Daniel Brand, Nicolas Riesterer, When Does a Reasoner Respond: Nothing Follows?: 41st Annual Meeting of the Cognitive Science Society. Proceedings of the 41st Annual Conference of the Cognitive Science Society. 2019</p>
<p>Hierarchical linear models: Applications and data analysis methods. Advanced Quantitative Techniques. Stephen W Raudenbush, the Social Sciences Series/SAGE. 2002</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Romain Liu, Tal Sauvestre, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Canton Bhatt, Aaron Ferrer, Wenhan Grattafiori, Xiong, 10.48550/arXiv.2308.12950Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2024. Code Llama: Open Foundation Models for Code. </p>
<p>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, Advances in Neural Information Processing Systems. 202336</p>
<p>Natural language to code translation with execution. Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, Sida I Wang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>The problem of logical form equivalence. M Stuart, Shieber, Computational Linguistics. 1911993</p>
<p>MindGames: Targeting theory of mind in large language models with dynamic epistemic modal logic. Damien Sileo, Antoine Lernould, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Entailer: Answering questions with faithful and truthful chains of reasoning. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, EMNLP. 2022</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024arXiv preprint</p>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models. Hugo Touvron, Louis Martin, Kevin Stone, 2023</p>
<p>A &amp; B == B &amp; A: Triggering Logical Reasoning Failures in Large Language Models. Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-Tse Huang, Pinjia He, Wenxiang Jiao, Michael R Lyu, 2024</p>
<p>Two types of higher-order readings of wh-questions. Yimei Xiang, Proceedings of the 22nd Amsterdam Colloquium. the 22nd Amsterdam Colloquium2019</p>
<p>Categorical syllogisms revisited: A review of the logical reasoning abilities of llms for analyzing categorical syllogism. Shi Zong, Jimmy Lin, arXiv:2406.187622024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6527 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6527</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6527</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-271050270</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.06027v5.pdf" target="_blank">PAS: Data-Efficient Plug-and-Play Prompt Augmentation System</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, the rise of Large Language Models (LLMs) has spurred a growing demand for plug-and-play AI systems. Among the various AI techniques, prompt engineering stands out as particularly significant. However, users often face challenges in writing prompts due to the steep learning curve and significant time investment, and existing automatic prompt engineering (APE) models can be difficult to use. To address this issue, we propose PAS, an LLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality, automatically generated prompt complementary datasets, resulting in exceptional performance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA) results compared to previous APE models, with an average improvement of 6.09 points. Moreover, PAS is highly efficient, achieving SoTA performance with only 9000 data points. Additionally, PAS can autonomously generate prompt augmentation data without requiring additional human labor. Its flexibility also allows it to be compatible with all existing LLMs and applicable to a wide range of tasks. PAS excels in human evaluations, underscoring its suitability as a plug-in for users. This combination of high performance, efficiency, and flexibility makes PAS a valuable system for enhancing the usability and effectiveness of LLMs through improved prompt engineering.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6527.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6527.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Augmentation System (PAS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-and-play, data-efficient automatic prompt augmentation system that generates complementary prompts and fine-tunes LLMs to improve downstream performance across tasks and models without human labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4-turbo / GPT-4 variants / GPT-3.5 / Qwen2 / LLaMA-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (API models; 7Bâ€“72B and 70B family models)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Prompt Complementation (PAS)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>prompt-augmentation / plug-and-play</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Arena-hard; Alpaca-Eval 2.0; Alpaca-Eval 2.0 (LC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mixed generalist evaluation suite including hard reasoning cases (Arena-hard) and broad instruction-following tasks (Alpaca-Eval variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average score (benchmark-specific scoring aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>47.53</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>baseline (no APE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>8.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>PAS generates 9,000 high-quality (prompt, complementary prompt) pairs via clustering, quality selection, few-shot generation and regeneration; it is described as data-efficient and producing a diverse complementary prompt set spanning ~14 categories, and the authors attribute performance gains to both data quality (selection/regeneration) and diversity of prompt types; PAS is LLM-agnostic and task-agnostic and yields consistent improvements across models and human evaluations. The paper additionally reports that PAS outperforms the prior SoTA APE (BPO) by an average of 6.09 points.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6527.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAS-GPT4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PAS applied to GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the PAS prompt-complementation model to GPT-4-0613 showing large improvements on evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API model (undisclosed exact params)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Prompt Complementation (PAS)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>prompt-augmentation / plug-and-play</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Arena-hard; Alpaca-Eval 2.0; Alpaca-Eval 2.0 (LC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complex reasoning and standard instruction-following tasks (includes logic traps and domain queries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average score (benchmark aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>39.43</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>baseline (no APE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>11.46</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors highlight an 11.46 point improvement over the baseline for GPT-4-0613 and use case studies (e.g., logic-trap bird question) to illustrate that PAS supplies hints that correct model reasoning. The improvement is attributed to complementary prompts that explicitly state assumptions and highlight key elements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6527.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAS-Qwen2-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PAS applied to Qwen2-72b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PAS fine-tuned and applied to a large Qwen2 model, showing notable gains compared to BPO and baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-72b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Prompt Complementation (PAS)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>prompt-augmentation / plug-and-play</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Arena-hard; Alpaca-Eval 2.0; Alpaca-Eval 2.0 (LC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General instruction-following and reasoning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average score (benchmark aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>47.35</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>BPO (previous SoTA APE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>9.13</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>For Qwen2-72b, PAS outperforms BPO by a reported 9.13 points; authors emphasize PAS's automatic complementary-prompt generation, dataset curation, and regeneration loop as the reason for robust gains compared to prior APE approaches which may be unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6527.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BPO (previous APE baseline cited as [9])</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior automatic prompt engineering (APE) method used as the main baseline for comparisons; described by the authors as requiring human labor but being LLM/task-agnostic in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BPO (APE baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (as used in cited baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Black-box Prompt Optimization (BPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>automatic prompt engineering / optimizer-style</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Arena-hard; Alpaca-Eval 2.0; Alpaca-Eval 2.0 (LC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same mixed benchmarks used to evaluate APE effectiveness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average score (benchmark aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>41.44</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>PAS</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-6.09</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report that BPO is sometimes unstable and can perform worse than baseline in some cases; PAS surpasses BPO on average by 6.09 points and is claimed to need less human labor/data. The paper compares data consumption and finds PAS uses 9,000 pairs vs BPO's 14,000.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6527.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step reasoning chains from language models to improve logical consistency and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (step-by-step sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used broadly to improve multi-step reasoning (e.g., math, logic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper cites CoT as widely popular for mathematical/logical reasoning; also cites zero-shot CoT and Manual-CoT variants. CoT is discussed in related work but not directly evaluated in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6527.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (ensemble of reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation strategy that samples many distinct reasoning chains and selects the most consistent final answer across samples to improve reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (multiple sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improves answer robustness by aggregating across diverse reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Mentioned as a method that deliberately generates diverse reasoning paths and chooses the most consistent answer; cited in related work but not experimented with here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6527.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-linear search strategy that lets LLMs explore multiple branching reasoning paths (tree search) by querying the model multiple times and combining partial solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search / non-linear search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (non-linear branching)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed for elaborate problem solving by exploring multiple plausible reasoning branches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Cited as extending linear reasoning into branching structures; discussed in related work but not an experimental variable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6527.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Buffer-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Buffer of Thought (BOT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A template/retrieval-like approach that provides thought-templates for tasks and retrieves the most relevant template per problem to prompt LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Buffer of Thought (BOT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>template-retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improves reasoning by leveraging curated thought-templates relevant to problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Described in related work as designing thought-templates and retrieving relevant templates per problem; mentioned but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6527.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT / Automatic-COT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-CoT / Automatic Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic construction of chain-of-thought exemplars by clustering dataset questions and generating rationale chains to serve as few-shot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Auto-CoT / Automatic-COT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>few-shot exemplar generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (cluster-specific exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automatically produce CoT exemplars to improve few-shot CoT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Cited as previous automatic prompt engineering work that improves vanilla CoT by generating cluster-specific demonstrations; relevant to PAS insofar as PAS also uses clustering and few-shot generation, but Auto-CoT itself is not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6527.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6527.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-efficiency comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data consumption comparison across APE/alignment methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported comparison of the number of labeled data points required by various alignment/APE methods, showing PAS uses substantially fewer data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>N/A (data-efficiency metric)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>measurement / comparative metric</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Comparison of training/annotation data consumption for APE/alignment methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>data consumption (number of examples)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>9000.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>BPO; PPO; DPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports PAS uses 9,000 prompt-complementary pairs vs BPO 14,000, PPO 77,000, and DPO 170,000; authors compute PAS as 1.56x more efficient than BPO and much more against PPO/DPO, arguing PAS is more data-efficient and requires no human labor.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PAS: Data-Efficient Plug-and-Play Prompt Augmentation System', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in LLMs <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Black-box prompt optimization: Aligning large language models without model training <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6527",
    "paper_id": "paper-271050270",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "PAS",
            "name_full": "Prompt Augmentation System (PAS)",
            "brief_description": "A plug-and-play, data-efficient automatic prompt augmentation system that generates complementary prompts and fine-tunes LLMs to improve downstream performance across tasks and models without human labeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4-turbo / GPT-4 variants / GPT-3.5 / Qwen2 / LLaMA-3)",
            "model_size": "various (API models; 7Bâ€“72B and 70B family models)",
            "reasoning_method_name": "Prompt Complementation (PAS)",
            "reasoning_method_type": "prompt-augmentation / plug-and-play",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Arena-hard; Alpaca-Eval 2.0; Alpaca-Eval 2.0 (LC)",
            "task_description": "Mixed generalist evaluation suite including hard reasoning cases (Arena-hard) and broad instruction-following tasks (Alpaca-Eval variants).",
            "performance_metric": "average score (benchmark-specific scoring aggregate)",
            "performance_value": 47.53,
            "comparison_target_method": "baseline (no APE)",
            "performance_difference": 8.0,
            "statistical_significance": null,
            "analysis_notes": "PAS generates 9,000 high-quality (prompt, complementary prompt) pairs via clustering, quality selection, few-shot generation and regeneration; it is described as data-efficient and producing a diverse complementary prompt set spanning ~14 categories, and the authors attribute performance gains to both data quality (selection/regeneration) and diversity of prompt types; PAS is LLM-agnostic and task-agnostic and yields consistent improvements across models and human evaluations. The paper additionally reports that PAS outperforms the prior SoTA APE (BPO) by an average of 6.09 points.",
            "ablation_study_present": true,
            "uuid": "e6527.0",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "PAS-GPT4-0613",
            "name_full": "PAS applied to GPT-4-0613",
            "brief_description": "Application of the PAS prompt-complementation model to GPT-4-0613 showing large improvements on evaluated benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4-0613",
            "model_size": "API model (undisclosed exact params)",
            "reasoning_method_name": "Prompt Complementation (PAS)",
            "reasoning_method_type": "prompt-augmentation / plug-and-play",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Arena-hard; Alpaca-Eval 2.0; Alpaca-Eval 2.0 (LC)",
            "task_description": "Complex reasoning and standard instruction-following tasks (includes logic traps and domain queries).",
            "performance_metric": "average score (benchmark aggregate)",
            "performance_value": 39.43,
            "comparison_target_method": "baseline (no APE)",
            "performance_difference": 11.46,
            "statistical_significance": null,
            "analysis_notes": "Authors highlight an 11.46 point improvement over the baseline for GPT-4-0613 and use case studies (e.g., logic-trap bird question) to illustrate that PAS supplies hints that correct model reasoning. The improvement is attributed to complementary prompts that explicitly state assumptions and highlight key elements.",
            "ablation_study_present": true,
            "uuid": "e6527.1",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "PAS-Qwen2-72B",
            "name_full": "PAS applied to Qwen2-72b-Instruct",
            "brief_description": "PAS fine-tuned and applied to a large Qwen2 model, showing notable gains compared to BPO and baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2-72b-Instruct",
            "model_size": "72B",
            "reasoning_method_name": "Prompt Complementation (PAS)",
            "reasoning_method_type": "prompt-augmentation / plug-and-play",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Arena-hard; Alpaca-Eval 2.0; Alpaca-Eval 2.0 (LC)",
            "task_description": "General instruction-following and reasoning evaluation",
            "performance_metric": "average score (benchmark aggregate)",
            "performance_value": 47.35,
            "comparison_target_method": "BPO (previous SoTA APE)",
            "performance_difference": 9.13,
            "statistical_significance": null,
            "analysis_notes": "For Qwen2-72b, PAS outperforms BPO by a reported 9.13 points; authors emphasize PAS's automatic complementary-prompt generation, dataset curation, and regeneration loop as the reason for robust gains compared to prior APE approaches which may be unstable.",
            "ablation_study_present": true,
            "uuid": "e6527.2",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "BPO",
            "name_full": "BPO (previous APE baseline cited as [9])",
            "brief_description": "A prior automatic prompt engineering (APE) method used as the main baseline for comparisons; described by the authors as requiring human labor but being LLM/task-agnostic in prior work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BPO (APE baseline)",
            "model_size": "various (as used in cited baseline)",
            "reasoning_method_name": "Black-box Prompt Optimization (BPO)",
            "reasoning_method_type": "automatic prompt engineering / optimizer-style",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Arena-hard; Alpaca-Eval 2.0; Alpaca-Eval 2.0 (LC)",
            "task_description": "Same mixed benchmarks used to evaluate APE effectiveness",
            "performance_metric": "average score (benchmark aggregate)",
            "performance_value": 41.44,
            "comparison_target_method": "PAS",
            "performance_difference": -6.09,
            "statistical_significance": null,
            "analysis_notes": "Authors report that BPO is sometimes unstable and can perform worse than baseline in some cases; PAS surpasses BPO on average by 6.09 points and is claimed to need less human labor/data. The paper compares data consumption and finds PAS uses 9,000 pairs vs BPO's 14,000.",
            "ablation_study_present": false,
            "uuid": "e6527.3",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "A prompting technique that elicits step-by-step reasoning chains from language models to improve logical consistency and accuracy.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style (step-by-step sequence)",
            "benchmark_name": "",
            "task_description": "Used broadly to improve multi-step reasoning (e.g., math, logic).",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper cites CoT as widely popular for mathematical/logical reasoning; also cites zero-shot CoT and Manual-CoT variants. CoT is discussed in related work but not directly evaluated in this study.",
            "ablation_study_present": null,
            "uuid": "e6527.4",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (ensemble of reasoning paths)",
            "brief_description": "A decoding/aggregation strategy that samples many distinct reasoning chains and selects the most consistent final answer across samples to improve reasoning accuracy.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Self-Consistency",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse (multiple sampled reasoning paths)",
            "benchmark_name": "",
            "task_description": "Improves answer robustness by aggregating across diverse reasoning chains.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Mentioned as a method that deliberately generates diverse reasoning paths and chooses the most consistent answer; cited in related work but not experimented with here.",
            "ablation_study_present": null,
            "uuid": "e6527.5",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Tree-of-Thought",
            "name_full": "Tree of Thought (ToT)",
            "brief_description": "A non-linear search strategy that lets LLMs explore multiple branching reasoning paths (tree search) by querying the model multiple times and combining partial solutions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Tree-of-Thought (ToT)",
            "reasoning_method_type": "tree-search / non-linear search",
            "reasoning_style_diversity": "diverse (non-linear branching)",
            "benchmark_name": "",
            "task_description": "Designed for elaborate problem solving by exploring multiple plausible reasoning branches.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Cited as extending linear reasoning into branching structures; discussed in related work but not an experimental variable in this paper.",
            "ablation_study_present": null,
            "uuid": "e6527.6",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Buffer-of-Thought",
            "name_full": "Buffer of Thought (BOT)",
            "brief_description": "A template/retrieval-like approach that provides thought-templates for tasks and retrieves the most relevant template per problem to prompt LLMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Buffer of Thought (BOT)",
            "reasoning_method_type": "template-retrieval",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "",
            "task_description": "Improves reasoning by leveraging curated thought-templates relevant to problems.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Described in related work as designing thought-templates and retrieving relevant templates per problem; mentioned but not used in experiments.",
            "ablation_study_present": null,
            "uuid": "e6527.7",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Auto-CoT / Automatic-COT",
            "name_full": "Auto-CoT / Automatic Chain-of-Thought",
            "brief_description": "Automatic construction of chain-of-thought exemplars by clustering dataset questions and generating rationale chains to serve as few-shot demonstrations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Auto-CoT / Automatic-COT",
            "reasoning_method_type": "few-shot exemplar generation",
            "reasoning_style_diversity": "mixed (cluster-specific exemplars)",
            "benchmark_name": "",
            "task_description": "Automatically produce CoT exemplars to improve few-shot CoT performance.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Cited as previous automatic prompt engineering work that improves vanilla CoT by generating cluster-specific demonstrations; relevant to PAS insofar as PAS also uses clustering and few-shot generation, but Auto-CoT itself is not evaluated in this paper.",
            "ablation_study_present": null,
            "uuid": "e6527.8",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Data-efficiency comparison",
            "name_full": "Data consumption comparison across APE/alignment methods",
            "brief_description": "Reported comparison of the number of labeled data points required by various alignment/APE methods, showing PAS uses substantially fewer data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "N/A (data-efficiency metric)",
            "reasoning_method_type": "measurement / comparative metric",
            "reasoning_style_diversity": "N/A",
            "benchmark_name": "",
            "task_description": "Comparison of training/annotation data consumption for APE/alignment methods",
            "performance_metric": "data consumption (number of examples)",
            "performance_value": 9000.0,
            "comparison_target_method": "BPO; PPO; DPO",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper reports PAS uses 9,000 prompt-complementary pairs vs BPO 14,000, PPO 77,000, and DPO 170,000; authors compute PAS as 1.56x more efficient than BPO and much more against PPO/DPO, arguing PAS is more data-efficient and requires no human labor.",
            "ablation_study_present": false,
            "uuid": "e6527.9",
            "source_info": {
                "paper_title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in LLMs",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_llms"
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Black-box prompt optimization: Aligning large language models without model training",
            "rating": 2,
            "sanitized_title": "blackbox_prompt_optimization_aligning_large_language_models_without_model_training"
        }
    ],
    "cost": 0.01643575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PAS: Data-Efficient Plug-and-Play Prompt Augmentation System
7 Aug 2024</p>
<p>Miao Zheng zhengmiao@baichuan-inc.com 
Baichuan Inc</p>
<p>Hao Liang hao.liang@stu.pku.edu.cn 
Peking University</p>
<p>Fan Yang 
Baichuan Inc</p>
<p>Haoze Sun 
Baichuan Inc</p>
<p>Tianpeng Li 
Baichuan Inc</p>
<p>Lingchu Xiong 
Baichuan Inc</p>
<p>Yan Zhang 
Baichuan Inc</p>
<p>Youzhen Wu 
Peking University</p>
<p>Baichuan Inc</p>
<p>Kun Li 
Yanjun Shen 
Baichuan Inc</p>
<p>Mingan Lin 
Tao Zhang 
Baichuan Inc</p>
<p>Guosheng Dong 
Baichuan Inc</p>
<p>Yujing Qiao 
Baichuan Inc</p>
<p>Kun Fang 
Baichuan Inc</p>
<p>Weipeng Chen 
Baichuan Inc</p>
<p>Bin Cui 
Peking University</p>
<p>Baichuan Inc</p>
<p>Wentao Zhang wentao.zhang@pku.edu.cn 
Peking University</p>
<p>Baichuan Inc</p>
<p>Zenan Zhou zhouzenan@baichuan-inc.com 
Baichuan Inc</p>
<p>PAS: Data-Efficient Plug-and-Play Prompt Augmentation System
7 Aug 202421BE0B4BE8B37AE76CD98B72F87533F0arXiv:2407.06027v5[cs.CL]Automatic Prompt AugmentationPlug-and-Play SystemLarge Language ModelsData GenerationData Selection
In recent years, the rise of Large Language Models (LLMs) has spurred a growing demand for plug-and-play AI systems.Among the various AI techniques, prompt engineering stands out as particularly significant.However, users often face challenges in writing prompts due to the steep learning curve and significant time investment, and existing automatic prompt engineering (APE) models can be difficult to use.To address this issue, we propose PAS, an LLMbased plug-and-play APE system.PAS utilizes LLMs trained on highquality, automatically generated prompt complementary datasets, resulting in exceptional performance.In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA) results compared to previous APE models, with an average improvement of 6.09 points.Moreover, PAS is highly efficient, achieving SoTA performance with only 9000 data points.Additionally, PAS can autonomously generate prompt augmentation data without requiring additional human labor.Its flexibility also allows it to be compatible with all existing LLMs and applicable to a wide range of tasks.PAS excels in human evaluations, underscoring its suitability as a plug-in for users.This combination of high performance, efficiency, and flexibility makes PAS a valuable system for enhancing the usability and effectiveness of LLMs through improved prompt engineering.The code is available at https://github.com/PKU-Baichuan-MLSystemLab/PAS.</p>
<p>INTRODUCTION</p>
<p>In recent years, data management and AI systems [8,12,22,23,34] have achieved rapid advancements and play an important role in large language models (LLMs) [24,33].At the same time, there is an increasing need for scalable plug-and-play systems for LLMs due to their flexibility and efficiency [2].</p>
<p>Among the various techniques for LLMs, prompt engineering has emerged as a crucial approach due to its extremely low cost and significant enhancement of LLM performance [31].This technique leverages the inherent capabilities of LLMs to understand and generate human-like text, enabling them to perform a wide range of applications from natural language understanding and generation to specialized domains such as medical diagnosis and legal Inference  analysis [14,20].In prompt engineering techniques, few-shot learning [6] stands out because it provides a small number of examples to guide the model, thereby enhancing its task-specific performance.Chain-of-Thought (CoT) [37] prompting guides models to reason through problems step-by-step, improving logical consistency and accuracy.In-context learning [10], by embedding relevant examples and instructions directly within the prompt, allows models to adapt dynamically to new tasks.
Prompt
Despite the potential of existing methods, prompt engineering is not user-friendly and requires meticulous design.Crafting effective prompts demands a deep understanding of both the model and the task at hand.This process can be time-consuming and often involves extensive trial and error to optimize performance.To tackle those weak points, automatic prompt engineering (APE) is designed for easier prompt generation [9,28,47].Although APE models can automatically enhance the prompt, they have to use massive amounts of human-labeled data [9,26,30].Additionally, previous methods failed to construct a flexible, user-friendly and effective APE model [29,48].They face the following three key challenges:</p>
<p>C1. Low Efficiency.Previous works on APE primarily use extensive human-labeled instruction datasets, which result in significant human labor [9,26,30].Additionally, some methods require training a specific APE model for each LLM, leading to a substantial waste of computational resources [28,40].</p>
<p>C2. Low Flexibility.Previous works primarily focus on the performance of APE models, overlooking the importance of flexibility, specifically their model-agnostic and task-agnostic capabilities [28,40].Low-flexibility APE systems can lead to computational waste and hinder their application on LLMs [28].Additionally, these low-flexibility systems have limited applicability across various scenarios, making them less versatile for diverse use cases and reducing their overall effectiveness in practical applications.</p>
<p>C3. Poor Effectiveness.Although several works aim to automatically enhance the quality of prompts, they often rely on outdated evaluation metrics and do not consistently yield significant improvements across various benchmarks and models [9,26].Additionally, these models fail to include human evaluations, thus lacking valuable feedback from human users [9,28].</p>
<p>If there are 10 birds on a tree and one is shot dead, how many birds are on the ground?Consider this a logic trap question.</p>
<p>If there are 10 birds on a tree and one is shot dead, how many birds are on the ground?Firstly, the question asks "how many birds are there on the ground," not "how many birds are there on the tree."We need to focus on where the dead bird fell.Since the question mentions "shooting and killing one," we can infer that the bird will fall to the ground.So, literally, there will be 1 bird on the ground.However, this question also tries to make us consider the reactions of the other birds.In real life, the gunshot might scare them away.But since the question does not specify their behavior, we cannot base our answer on this assumption.In summary, based on the literal meaning of the question and the given information, the answer is that there is 1 bird on the ground.</p>
<p>After the shooting, all the birds in the tree flew away, so there are 0 birds on the ground To address these issues, as shown in Figure 1(a), we propose PAS, an automatic prompt-enhancing plug-and-play system.Our approach involves two main phases: high-quality prompt selection and automatic complementary prompt generation, followed by fine-tuning of large language models (LLMs).In the prompt selection phase, we begin by using embedding models to extract features from the prompt data.We then apply clustering algorithms to group and deduplicate similar prompts.Following this, we use LLMs to select high-quality prompts and classify them into various categories.In the automatic complementary prompt generation phase, we employ few-shot learning techniques to generate new prompts.These generated prompts undergo a rigorous selection and regeneration process to ensure their quality.The high-quality generated data is then used to fine-tune the LLMs, forming the core of the PAS system.</p>
<p>The core contributions of this paper are summarized as follows:</p>
<p>â€¢ New Perspective.To the best of our knowledge, this work is the first to construct a curated prompt complementary dataset without human labor.Additionally, we are the first to utilize this dataset to train LLMs to construct the PAS to automatically complement user prompts instead of directly modifying them.</p>
<p>RELATED WORK 2.1 Automatic Prompt Engineering</p>
<p>The effectiveness of large language models in various applications largely depends on the quality of the prompts used.There are already many designed prompts that can significantly enhance the performance of LLMs [5,17,36,37,41,42].However, these methods that rely on manual prompt engineering are far less scalable.In the field of mathematical logical reasoning for LLMs, the Chain of Thought and its derived strategies are widely popular due to their effectiveness.Zero-shot CoT [17] is adding a simple sentence like "Let's think step by step" at the end of questions to assist LLMs in generating reasoning steps.Instead of Zero-shot CoT, Manual-Cot [37] provides reasoning steps as few shots.Self-Consistency further improves language models' reasoning performance by generating a diverse set of reasoning paths and choosing the most consistent answer in the final answer set.Tree of Thought (TOT) [42] and GOT [5] extend the reasoning pathway from linear to non-linear data structures by leveraging multiple LLM queries to elicit different plausible reasoning paths [41].Buffer of Thought (BOT) [41] designs a series of thought-template for tasks, and for each problem, it retrieve a relevant thought-template to prompt LLMs.PS prompting [36] improves COT by encouraging LLMs to devise a plan before attempting to solve a problem.All the aforementioned prompting engineering strategies have been crafted by human expertise.To avoid manual effort, there is a lot of recent work to explore how to conduct automated prompt engineering [13,15,29,32,40,44,48].Auto-Cot [44] partitions questions of a given dataset into a few clusters and generates reasoning chains to construct demonstrations for each cluster for Few-shot COT.Automatic-COT [32] creates rationale chains to augment exemplars and filters out incorrect ones by checking against the ground truth.Both of them improve the performance of vanilla COT [17,37].Unlike previous works, OPRO [40], APO [29], and APE [48] provide an optimizer's perspective for automatically finding prompts.OPRO [40] leverages LLMs as optimizers, using the accuracy of training datasets-unavailable in real-world scenarios-as the objective value.APO [29] provides detailed guidance on prompt refinement at each step, based on the differences between responses and targets.Evoprompt [15] and Promptbreeder [13] introduce evolutionary algorithms (EAs) into discrete prompt optimization for specific domains.Similar to evolutionary algorithms, they require evaluating the fitness of each individual prompt in the population, presenting significant challenges in practical applications.Additionally, exploring dozens of generations of prompts imposes a considerable burden.</p>
<p>Plug-and-Play Systems</p>
<p>Plug-and-play systems have garnered significant attention in recent years due to their modularity and ease of integration in various machine-learning workflows.These systems are designed to operate seamlessly with existing frameworks, allowing for quick and flexible augmentation of functionalities without the need for extensive reconfiguration [1,35,43].</p>
<p>In image processing research, plug-and-play systems are commonly applied for its outstanding flexibility.Image reconstruction, denoising, deblurring, image enhancement, and super-resolution are all fields where plug-and-play systems are highly needed.By integrating various image processing modules into a unified framework, plug-and-play systems can flexibly combine different methods to achieve better image processing results.Moreover, this system allows for the easy addition or replacement of new processing modules without redesigning the entire algorithm, significantly improving the efficiency and effectiveness of image processing.</p>
<p>In the field of artificial intelligence, the rapid advancement of machine learning models has spurred a growing demand for plugand-play systems.These systems enable seamless integration and adaptation of AI technologies across various applications.Al Ridhawi et al. [2] have underscored the critical role of AI plug-and-play systems in enhancing scalability, flexibility, and usability in modern computational frameworks.</p>
<p>Data Quality and Data Selection</p>
<p>The advent of large language models has brought about a substantial increase in the volume of training data [25,33].In this scenario, the quality of data becomes paramount.LLMs, trained on vast amounts of data, can capture subtle nuances and complex patterns in language, excelling in various natural language processing tasks.However, the increase in data volume also brings new challenges, particularly in data quality and data selection [4].In this section, we mainly discuss the effectiveness of data quality and data selection.</p>
<p>Data Quality.: High-quality data can significantly enhance the performance of models [21].As the volume of data increases, ensuring high data quality becomes more challenging because it requires more resources for data cleaning, selection and annotation [4].Poor quality data can lead to models learning incorrect patterns and making inaccurate predictions.</p>
<p>Data Selection.: LLMs-based methods were commonly used in data selection [4].For instance, Du et al. [11] leverages De-BERTa [16] for scoring, retaining high-quality data, and combining it with the k-center greedy algorithm to select diverse data.Chen et al. [7] score the accuracy of data using ChatGPT to pick out highquality data.Xu et al. [38] use GPT-4 to rewrite data to increase their complexity and then streamline it by reducing its variety and improving its quality.Liu et al. [18] train two models using Chat-GPT's labeled data to score the quality and complexity of the data.Lu et al. [19] rely on ChatGPT to tag each instance, defining its complexity and diversity based on these tags.Parkar et al. [27] first cluster the data, and then use GPT-4 to select high-quality data for each cluster.</p>
<p>Given the critical role of data quality and selection in enhancing model performance, our paper focuses on leveraging advanced data selection techniques to optimize prompts and their complementary data.By employing methods that integrate scoring mechanisms from LLMs and clustering techniques, we aim to efficiently identify and utilize high-quality data clusters for prompt engineering.</p>
<p>METHOD</p>
<p>In this section, we first summarize the collection and process of prompt data in section 3.1.Then we introduce the prompt complementary data generation pipeline in section 3.2 to automatically generate high-quality prompt complementary data.After the prompt dataset is generated, we give a comprehensive analysis of the dataset in section 3.3.At last, in section 3.4, we introduce utilizing the prompt augmentation dataset for LLM fine-tuning and then use the fine-tuned LLM to construct a plug-and-play APE system.</p>
<p>Prompts Data Collection</p>
<p>In this section, we introduce the prompt collection process.To generate high-quality (prompt, complementary prompt) pairs, we first need to select high-quality prompts.To achieve this, we use two curated datasets: the LMSYS-1M dataset [46] and the WildChat dataset [45].We use  to denote the prompt dataset.As shown in  Deduplication.Deduplication is performed to create a diverse and efficient dataset.Using the SimCSE bge embedding model, all prompts from the LMSYS-1M and WildChat datasets are embedded.
ð‘ƒ embed = SimCSE(ð‘ƒ)
Subsequently, the HNSW clustering algorithm is employed to group these embeddings.A small subset of data is then extracted from each cluster to reduce redundancy.</p>
<p>Quality Selection.Quality filtering is performed to select highquality data because such data can not only reduce computational costs but also enhance the model's performance.For quality selection, the BaiChuan 13b model [39] serves as the base model.We filter out low-quality entries using the formula below:
ð‘„ score (ð‘ ð‘– ) = BaiChuan 13b(ð‘ ð‘– ) ð‘ƒ filtered = {ð‘ ð‘– âˆˆ ð‘ƒ | ð‘„ score (ð‘ ð‘– ) â‰¥ ðœ }
Here,  score (  ) represents the quality score assigned by the BaiChuan 13b model to prompt   , and  denotes the quality threshold.By employing quality selection, we aim to enhance the overall quality of prompt data.</p>
<p>Classification.To facilitate few-shot learning for prompt complementary dataset generation, we categorize the data into different categories.For accurate classification, we fine-tune a BaiChuan 13b model [39] using 60,000 internally labeled classification data points from BaiChuan Inc.This process yields a classification model that categorizes prompts into common categories such as Q&amp;A and coding.</p>
<p>The steps of deduplication, quality selection, and classification ensure diversity, quality, and accurate categorization of the data.Approximately 9,000 high-quality classified data points are obtained through these processes.Subsequently, these 9,000 high-quality prompts are utilized to generate high-quality (prompt, complementary prompt) pairs.</p>
<p>Prompts Complementary Dataset</p>
<p>To generate a high-quality prompt complementary dataset, we designed an automated data generation pipeline based on Few-Shot Learning.The algorithm mainly consists of two phases: data generation and data selection with regeneration.We use a set of golden data  golden = {(  ,   )}  =1 , containing 4 to 5 pairs of fewshot examples for each category from BaiChuan Inc.Then, we utilize the prompt dataset  golden from Section 3.1 to generate highquality (prompt, complementary prompt) pairs.Data Generation.For each prompt   âˆˆ  golden in every category, we utilize the Few-Shot Learning method based on the prompt in Figure 4 to generate a corresponding complementary prompt   based on the golden data  golden .The generated promptcomplementary prompt pair (  ,   ) is then added to  generated .</p>
<p>Algorithm 1: Prompt Augmentation Dataset Generation
Input: Golden Data ð· golden = {(ð‘ ð‘– , ð‘Ž ð‘– )} ð‘ ð‘–=1 , Prompt Set ð‘ƒ = {ð‘ ð‘— } ð‘€ ð‘—=1 Output: Generated Data ð· generated 1 ð· generated â† âˆ…; 2 for each ð‘ ð‘— âˆˆ ð‘ƒ golden do 3 ð‘Ž ð‘— â†
Data Selection and Regeneration.We observed that not all the generated complementary prompt data are of high quality or useful for the original prompt.To address this issue, we proposed a data selection and regeneration pipeline for high-quality complementary prompts.</p>
<p>For each generated prompt-answer pair (  ,   ) âˆˆ  generated , we use Few-Shot Learning based on the prompt in Figure 5 to evaluate its correctness.If the evaluation result is incorrect, we remove the pair and use Few-Shot Learning in the data generation phase to regenerate the answer until the correct answer is generated.Finally, we add the correct prompt-answer pair back to  generated .</p>
<p>Through this data selection and regeneration process, we can automatically generate a prompt complementary dataset while ensuring data quality.This process provides reliable data support for subsequent model training.To better summarize the contents of this section, we translated the data selection process into an algorithm and a pipeline.The data generation pipeline is summarized in Figure 3(b) and Algorithm 1.</p>
<h2>Background</h2>
<p>You are a master of complementary prompts, skilled only in enhancing user prompt and unable to respond to it.\nPlease Note: 1.You can only supplement user prompt, cannot directly answer it.2. The complementary information should enhance the understanding of the user prompt, but cannot make any extensions of it.3. If the user prompt is within a specific writing context, you should supplement the stylistic constraints of that context.4. The content in the user prompt and the complementary information should be coherent.5.You should supplement the user prompt to cater human preferences.\n6. Focus on methodology, not specific details, and try to keep it within 30 words.\n\n\n## Examples The user's actual question\n\n<User prompt>\nPROMPT_PLACEHOLDER\n<Complementary information></p>
<p>Figure 4: Complementary Dataset Generation Prompt</p>
<h2>Background</h2>
<p>High-quality prompt engineering can significantly improve the application potential and answer quality of ChatGPT.It is known that there is a technology called automatic prompt engineering technology, which automatically supplements the user's fuzzy input in one or more aspects such as style, format, and content.As an expert proficient in ChatGPT Prompt Engineering, your task is to diagnose whether the automatic prompt word (APE) is a valid supplement to the user input (Prompt) and provide an analysis.Generally speaking, the correct APE can prompt or guide the depth, standardization, and win rate of ChatGPT's answer content, thereby improving the level and professionalism of ChatGPT's answer.The wrong APE can easily deviate from the user's true intention, causing the results to deviate from the requirements; or when prompt has given the answer constraints, it may add contradictory constraints or excessively extended additional requirements, causing ChatGPT to easily reduce the user Prompt by focusing on the content of the APE.</p>
<h2>Workflow</h2>
<p>Please analyze and judge the APE and, then modify the incorrect APE.Here are 3 steps for this task, you must do it step by step:</p>
<ol>
<li>Analyze APE based on the APE standards 2. Determine whether APE is correct.3.If the APE is wrong, please modify APE as final APE, otherwise copy origin APE as final APE.The criteria for incorrect APE are: 1. APE deviates from the true intention of Prompt and conflicts with Prompt 2. APE provides too much superfluous additions to complex Prompt.3. APE directly answers Prompt instead of supplementing Prompt.4. APE makes excessive demands on Prompt. 5.The language of ape is consistent with that of user prompt.</li>
</ol>
<h2>Examples ## Output format</h2>
<p>The output is required to be in json format: {{"Reason": str, "Is_correct": str, "FinalAPE": str}}.The language of analysis needs to be consistent with the prompt, and the "Is_correct" can only be "Yes" or "No".## Task According to the above requirements, complete the following task <Prompt>:{prompt}\n<APE>:{ape}\na<Output>:</p>
<p>Prompt Complementary Dataset</p>
<p>In this section, we present a detailed analysis of the generated prompt-complementary dataset.The dataset comprises approximately 9,000 high-quality (prompt, complementary prompt) pairs, as illustrated in Figure 6. Figure 6 summarizes the distribution of the dataset, revealing that it spans 14 categories, with each category containing approximately 500 data points.This wide coverage of various prompt cases underscores the dataset's exceptional generalization capability.Moreover, there is a significant presence of Coding and Q&amp;A data, which are widely utilized functions, thus justifying their prominent representation.</p>
<p>Despite these strengths in data distribution, our automated process for generating complementary prompts, as detailed in Section 3.2, allows us to exert control over the categories of generated data.This flexibility enables our method to cater to both generalpurpose models and specialized data needs, thereby enhancing prompt adaptability across specific domains.Tailoring data generation to specific domains facilitates comprehensive training for diverse PAS tasks across various fields.Overall, the broad coverage of various prompt cases demonstrates the dataset's exceptional generalization ability, with emphasis on critical functionalities.Additionally, our method's capability to control dataset distribution supports the development of PAS systems across all domains.</p>
<p>Automatic Prompt Complementary</p>
<p>Plug-and-Play System PAS</p>
<p>In this section, we first use the generated prompt-complementary dataset in section 3.2 to fine-tune LLMs for prompt-complementary tasks.Given the flexibility of LLMs as automatic promptcomplementary tools that can be integrated into other generative LLMs, these prompt-complementary models can serve as an automatic, plug-and-play system to enhance LLM performance.</p>
<p>Fine-tune LLMs for Prompt Complementary Models.By utilizing the high-quality generated dataset  generated in section 3.2, we can automatically fine-tune LLMs to have a prompt complementary ability.We use   to denote an automatically prompt complementary model, and  to denote an LLM, which can be written as the following mathematic formula:
ð‘€ ð‘ â† SFT(ð‘€; ð· generated )
We call this   model PAS, a prompt complementary model with can serve as an automatic, plug-and-play system to enhance LLM performance.</p>
<p>PAS Enhances LLMs Performance.For a prompt  from the user, the complementary prompt   can be generated using the following formula:
ð‘ ð‘ = ð‘€ ð‘ (ð‘)
The enhanced response   is then given by:
ð‘Ÿ ð‘’ = LLMs(cat(ð‘, ð‘ ð‘ ))
By generating a complementary prompt, the PAS can improve the user's prompt without altering the original input.As demonstrated in Section 4, PAS effectively enhances the performance of LLMs.</p>
<p>Plug-and-Play LLMs Promoting System.PAS can be plugged into any other LLMs available via public APIs [24] and can also be integrated into models with open parameters [3,33].This flexibility allows for a wide range of applications and improvements across different platforms and systems.</p>
<p>The primary advantage of such a system is its ability to seamlessly enhance the capabilities of existing LLMs without the need for extensive retraining or modification.By simply augmenting the input prompts, PAS leverages the strengths of the underlying models while providing a mechanism to improve their performance.This approach is both cost-effective and efficient, enabling better utilization of computational resources and accelerating the deployment of enhanced language models in various applications.</p>
<p>EXPERIMENTS</p>
<p>In this section, we first introduce the experimental setups.We then aim to answer the following questions to verify the effectiveness, efficiency, and robustness of our proposed PAS: Q1: Can our PAS achieve SoTA performance compared to previous SoTA methods?Q2: Can our PAS outperform the previous SoTA model with the same base model?Q3: How efficient and flexible is our model compared to previous APE models?Q4: Can PAS achieve SoTA performance in human evaluation, making it a user friendly system?Q5: Can we visualize the advantages of our method?Q6: Do we need data quality selection and regenerate module in our data generation pipeline?</p>
<p>Experiments Setting</p>
<p>Datasets.We followed the steps in section 3 and generated a dataset of 9000 high-quality pairs (prompt, complementary prompt).</p>
<p>Models.</p>
<p>For PAS models, we select several smaller models to efficiently train a PAS model.We select Qwen2-7b-Instruct [3], LLaMA-2-7b-Instruct [33].Then we utilize our trained PAS models to some massive SoTA models, i.e.GPT-4-turbo-2024-04-09, GPT-4-1106-preview, GPT-4-0613, GPT-3.5-turbo-1106[24], Qwen2-72b-Instruct [3], and LLaMA-3-70b-Instruct [21,33].</p>
<p>Baselines.We compare the performance of PAS with models without PAS.Additionally, we compare the performance of PAS with the previous SoTA automatic prompt engineering method BPO [9] to demonstrate the effectiveness of PAS.</p>
<p>Evaluation.To evaluate the effectiveness of our PAS model, we used three comprehensive benchmarks to thoroughly assess the model's performance:</p>
<p>â€¢ Arena-hard: This benchmark is designed to test the robustness of language models in handling complex and challenging scenarios.It includes tasks that require advanced reasoning, problem-solving, and understanding of nuanced language constructs.Models are evaluated based on their ability to navigate these hard scenarios and provide accurate, coherent responses.â€¢ Alpaca-Eval 2.0: This benchmark assesses the general performance of language models across a wide range of standard tasks.It includes a variety of question types and subject areas, ensuring a comprehensive evaluation of the model's capabilities.The Alpaca-Eval 2.0 is a standard for measuring the overall effectiveness and versatility of language models.â€¢ Alpaca-Eval 2.0 (LC): Alpaca-Eval 2.0 LC is a lengthcontrolled version of AlpacaEval designed to mitigate biases related to response length in language model evaluations.By implementing length control, it reduces sensitivity to response length variations, enhancing robustness and interpretability of results.This improvement increases AlpacaEval's correlation with human judgments, as shown by its higher correlation with Chatbot Arena evaluations.</p>
<p>Settings.For Qwen2-7B-Instruct [3], LLaMA-2-7B-Instruct [33], Qwen2-72B-Instruct [3], LLaMA-2-7B-Instruct [33], and LLaMA-3-70B-Instruct [21,33], we primarily use the hyperparameters from the official repositories.For the GPT model series, we access the models via API.All experiments are conducted on a machine equipped with 8 NVIDIA H100 GPUs, a 120-core CPU, and 960GB of memory.</p>
<p>Main Experiments</p>
<p>To address Q1, we used Qwen2-7B-Instruct as the base model due to its outstanding performance.We subsequently used the prompt complementary data to train a PAS model and compared it to both the baseline model without the APE model and the previous stateof-the-art (SoTA) APE model, BPO [9].We integrated our model into multiple commonly used LLMs, including GPT-4-turbo-2024-04-09, GPT-4-1106-preview, GPT-4-0613, GPT-3.5-turbo-1106[24], Qwen2-72b-Instruct [3], and LLaMA-3-70b-Instruct [21,33].</p>
<p>The results in Table 1 clearly illustrate the effectiveness of our PAS method across different models.Compared to the baseline without using APE, PAS shows significant improvements in all metrics, resulting in an average improvement of 8 points, demonstrating the benefits of incorporating prompt complementary data.For instance, in the case of GPT-4-0613, PAS improves the average score by 11.46 points compared to the baseline, highlighting its substantial impact.</p>
<p>Moreover, when compared to the previous state-of-the-art model BPO, our model significantly outperforms it, resulting in an average improvement of 6.09 points.Each model achieves more than a 4point average improvement across all six base models compared to BPO, with a notable increase of 9.13 points for Qwen2-72b-Instruct, indicating a substantial improvement.</p>
<p>BPO is unstable and performs worse than the baseline in some cases, such as GPT-3.5-turbo-1106and Qwen2-72b-Instruct, indicating that the previous SoTA model cannot consistently outperform the baseline.Considering our model exceeds the baseline by 8.00 points and BPO by 6.09 points, it further demonstrates the effectiveness and robustness of our PAS model.</p>
<p>Overall, our PAS method not only outperforms the baseline but also consistently surpasses the previous SoTA model BPO, establishing its robustness and effectiveness as a fine-tuning strategy for enhancing prompt-based learning systems.This consistent performance across various LLMs underscores the robustness of PAS and its potential to set new benchmarks in the field.</p>
<p>Effectiveness of PAS</p>
<p>To address Q2, we fix the base model and compare our PAS method with the previous BPO [9].We use LLaMA-2-7b-Instruct, the same base model as BPO, and utilize the generated complementary prompt data to fine-tune LLaMA-2-7b-Instruct.We compare our model performance with BPO.</p>
<p>The results in Table 2 clearly demonstrate the effectiveness of our PAS method across different models.Notably, PAS exhibits a marked improvement in performance metrics compared to BPO, exceeding the baseline by 3.41 points on average.This is particularly evident in models like GPT-4-0613, where the average score improvement is as high as 5.89 points.Even in cases where the improvement is smaller, Overall, our PAS method consistently improves model performance across various evaluation models and settings, establishing its effectiveness and robustness as a fine-tuning strategy for enhancing prompt-based learning systems.</p>
<p>Efficiency and Flexibility of PAS</p>
<p>To address Q3, we compare the data efficiency and flexibility of the PAS approach with SoTA APE models.This comparison underscores PAS's effectiveness in terms of both data utilization and adaptability.</p>
<p>Data Usage Comparison.</p>
<p>Figure 7 shows the data consumption of PAS compared to other alignment methods, including PPO, DPO, and BPO.The data consumption for each method is as follows:</p>
<p>â€¢ PAS: 9,000 â€¢ BPO: 14,000</p>
<p>â€¢ PPO: 77,000</p>
<p>â€¢ DPO: 170,000 As shown in figure 7, PAS significantly reduces data consumption compared to other methods, demonstrating its efficiency in data usage.It is important to note that ProTeGi and OPRO are not taskagnostic, so they are not included in the data efficiency comparison.</p>
<p>We then calculated the efficiency of PAS using the formula: Efficiency = Consumption Methods Consumption PAS Based on this formula, PAS achieves an efficiency that is 1.56 times higher than BPO, 8.56 times higher than PPO, and 18.89 times higher than DPO, highlighting its superior data efficiency.3 compares the need for human labor and the flexibility of PAS with other models, specifically in terms of being LLM-agnostic and task-agnostic.The table shows that PAS is both LLM-agnostic and task-agnostic, demonstrating its superior flexibility.</p>
<p>Flexibility Comparison. Table</p>
<p>Summary.</p>
<p>The results indicate that PAS is the only method satisfying all three criteria: no need for human labor, LLM-agnostic, and task-agnostic.This underscores PAS's superior flexibility and efficiency as a plug-and-play system.Unlike other methods that require significant human intervention and have limitations in
APE [15] âœ— âœ— âœ— Auto-Cot [44] âœ— âœ— âœ“ OPRO [40] âœ— âœ— âœ— ProTeGi [28] âœ— âœ— âœ— BPO [9] âœ— âœ“ âœ“ PAS âœ“ âœ“ âœ“
applicability across different LLMs and tasks, PAS provides a highly versatile and efficient solution.</p>
<p>In contrast, methods like PPO, DPO, OPRO, and ProTeGi require human labor and lack LLM-agnostic capabilities.Although BPO is both LLM-agnostic and task-agnostic, it still requires human labor, necessitating large human-annotated datasets, whereas PAS can scale up automatically.This highlights PAS's significant advancements in providing an efficient, flexible, and human-labor-free prompt augmentation system.</p>
<p>Human Evaluation</p>
<p>To address Q4, we conducted a comprehensive evaluation using human evaluators to assess the online performance of our PAS compared to the baseline model without any prompt augmentation.We compared the good same bad (GSB), availability proportion, full mark proportion, and average score.</p>
<p>From Figure 1, we can see that PAS outperforms the baseline model in various scenarios.Specifically, the results show a higher percentage of wins across different categories such as Analytical Judgment, Subjective Suggestion, Subjective Recommendation, Common Sense, Event Query, Entity Query, Industry Knowledge, and Subject Knowledge.For example, PAS achieves 58.6% wins in Analytical Judgment, 64.3% in Subjective Suggestion, and 61.1% in Common Sense, demonstrating its effectiveness in enhancing performance compared to the baseline without PAS.</p>
<p>As shown in Table 4, our PAS consistently outperforms the baseline, achieving significant improvements in the full mark ratio, average score, and availability ratio.These enhancements across all three evaluation metrics in every benchmark demonstrate the effectiveness of our model.The results indicate not only strong performance on evaluation benchmarks but also positive feedback from human evaluators, showcasing a user-friendly model.</p>
<p>Furthermore, the consistent performance enhancements across all benchmarks highlight the generalization ability and robustness of our model, suggesting its applicability in various domains and its broad impact.</p>
<p>Case Study</p>
<p>To address Q5, this section discusses three different types of case studies that demonstrate the effectiveness of the PAS.</p>
<p>Case Study 1.We examine the effectiveness of PAS in the case study presented in Figure 2. The query, "If there are 10 birds on a tree and one is shot dead, how many birds are on the ground?", is a logic trap that can easily mislead both humans and large models if no hints are given.Initially, without the assistance of PAS, GPT responded with an incorrect answer.</p>
<p>Our PAS approach significantly improves this prompt by providing a complementary hint to remind the model that there is a logic trap.With the assistance of PAS, the new response avoids the trap and showcases a multi-step logical reasoning process.This refined answer is clear, accurate, and guides the user through the reasoning process, explaining why only one bird would be on the ground.</p>
<p>From Case Study 1, we can see that PAS is capable of assisting LLMs in avoiding logic traps.</p>
<p>Case Study 2. From case study 2 in Figure 8(a), we can see the user asked about how to quickly boil water in ancient times.By adjusting the size of the fire, they could control the heating speed.</p>
<p>Using Hot Stones:</p>
<p>In some regions, people would heat stones in a fire and then place the hot stones into water.The stones would rapidly transfer heat to the water, causing it to boil.</p>
<p>4.Using Steam:</p>
<p>In certain areas, people used steam to heat water.For example, they could fill a large container with water and place a smaller container above it.When the water in the large container boiled, the steam produced would rise and enter the smaller container, heating the water inside.Although these methods were not as convenient as modern electric kettles, they were sufficient to meet people's needs in ancient times.</p>
<p>User: How to Boil Water Quickly in Ancient Times PAS: Exploring ancient techniques for boiling water quickly using primitive heating methods such as fires and stoves, the effect of containers of different materials on heating efficiency can be appropriately discussed.</p>
<p>Response:</p>
<p>Choosing the Right Fuel: Ancient people typically used wood, dry grass, and straw as fuel.</p>
<p>To quickly boil water, they could choose easily flammable, long-burning wood such as pine or oak.Additionally, cutting the fuel into small pieces or chips could increase the surface area exposed to air, making it easier to burn.</p>
<p>Building an Efficient Fire: To fully utilize the heat from the fire, they could build an efficient fire.Stacking the fuel in a conical or pyramid shape with spaces in between for air circulation can help.Surrounding the fire with stones or earth can create a simple stove, which helps retain heat and reduce heat loss.Using Suitable Containers: In ancient times, people typically used pottery, copper, or iron containers to boil water.These materials have different thermal conductivity, which affects heating efficiency.Generally, copper and iron containers have better thermal conductivity, allowing heat to be transferred to the water more quickly.While pottery has poorer thermal conductivity, it has better insulation properties, helping keep water warm for a longer period after boiling.</p>
<p>Preheating the Container: Before pouring water into the container, they could preheat it by placing it near the fire for a while.This reduces the temperature difference between the water and the container, improving heating efficiency.</p>
<p>Continuous Stirring:</p>
<p>Stirring the water occasionally during the boiling process helps achieve even temperature distribution.This can speed up the heating process and prevent localized overheating, which could cause the water to splash when boiling.Utilizing Wind and Sunlight: When conditions allow, wind and sunlight can assist in heating.For example, building a fire in a windy area can help fuel burn faster; on sunny days, placing the water-filled container in direct sunlight can use solar energy to aid in heating.These methods, utilizing basic fire-building techniques and choosing appropriate materials, could help ancient people boil water more quickly despite their limited resources.However, without the assistance of PAS, the answer from LLM faces the following issues:</p>
<p>â€¢ Instruct Following Issue: The question pertained to boiling water quickly, yet the responses did not adhere to the "quickly" criterion.â€¢ Incorrect Response: From the red text, it is evident that using solar energy cannot boil water quickly.â€¢ Incomplete Explanation: From the green text, the use of fire is mentioned, but the methods to control the fire for quick boiling are not explained.</p>
<p>â€¢ Incorrect Response: From the red text, it is evident that using hot stones cannot boil water quickly either.â€¢ Logical Flaw: From the blue text, if steam is used, the water is already boiling.Although steam has a high temperature, it cools quickly when it encounters a container, thus it cannot boil water quickly either.â€¢ Lack of Specific Measures: Fire is the only method suggested for quick heating, but without specific measures, its value is limited.</p>
<p>With the assistance of PAS, we can see in Figure 8(b) that the LLM provides a correct response, effectively avoiding these issues.Response: When the body loses blood, blood pressure usually decreases.This is because blood loss leads to a reduction in blood volume, which in turn decreases the pressure of the blood against the vessel walls, resulting in lower blood pressure.If the blood loss is severe, it can lead to shock and even become life-threatening.</p>
<p>Commentï¼š</p>
<p>In terms of content richness, the compensatory mechanisms have not been sufficiently elaborated upon.</p>
<p>Additionally, the analysis is not comprehensive.Initially, when blood loss is minimal, compensatory mechanisms may maintain a stable state.</p>
<p>User: Does blood pressure increase or decrease when the body loses blood?PAS: Please provide a detailed analysis from a physiological and medical perspective on the changes in blood pressure when the body loses blood, explaining the related mechanisms and influencing factors.The response should be well-organized and easy to understand.</p>
<p>Response: When the body loses blood, the change in blood pressure depends on the severity and speed of the blood loss.In cases of mild or slow blood loss, the body can maintain stable blood pressure through a series of compensatory mechanisms.However, in cases of severe or rapid blood loss, these compensatory mechanisms may not fully function, leading to a drop in blood pressure.When blood loss occurs, circulating blood volume decreases, and the pressure of the blood against the vessel walls lowers, resulting in a decrease in blood pressure.To maintain stable blood pressure, the body initiates the following compensatory mechanisms:</p>
<p>1.Increased Heart Rate: During blood loss, the heart receives signals from baroreceptors and chemoreceptors to increase the heart rate, compensating for the reduced circulating blood volume.</p>
<p>2.Vasoconstriction:</p>
<p>The sympathetic nervous system is activated, causing blood vessels to constrict, which increases the pressure within the vessels and helps maintain blood pressure.</p>
<p>3.Blood Redistribution:</p>
<p>The body redirects blood from non-essential organs (such as the skin and internal organs) to essential organs (such as the brain and heart) to ensure an adequate blood supply to these critical areas.However, if the blood loss is too extensive or occurs too quickly, these compensatory mechanisms may not be sufficient, leading to a sustained drop in blood pressure.In such cases, shock symptoms may appear, such as confusion, pale skin, and cold sweat.Without timely treatment, this can lead to organ damage or even death.Therefore, for patients experiencing blood loss, it is crucial to promptly replenish blood volume, control the source of bleeding, and support the function of vital organs.During treatment, doctors will develop an appropriate treatment plan based on the patient's specific condition to quickly restore stable blood pressure and improve the patient's prognosis.The improved response follows the "quickly" criterion, accurately excludes ineffective methods such as solar energy and hot stones, and provides a comprehensive explanation of how to control fire to quickly boil water.This demonstrates the effectiveness of PAS in guiding LLMs to deliver precise and relevant answers.</p>
<p>Case Study 3. From Figure 9, we can see that the user inquired about whether blood pressure increases or decreases when the body loses blood.Typically, a user asking this question is looking for more information about blood pressure changes during blood loss and what actions to take in such a situation.However, as shown in Figure 9(a), the initial response is superficial and, despite being correct, lacks detailed information, which often prompts further questions from the user.In contrast, Figure 9(b) demonstrates that with PAS, a comprehensive and detailed analysis is provided from a physiological and medical perspective on the changes in blood pressure.This approach not only satisfies the immediate query but also equips the user with a thorough understanding, potentially reducing the need for follow-up questions.By delivering such detailed and insightful responses, PAS proves its effectiveness in enhancing user experience and satisfaction in information-seeking scenarios.</p>
<p>From Case Study 3, we can see that PAS can provide more comprehensive answers that consider the user's potential needs, rather than incomplete ones.</p>
<p>To summarize, the benefits of our PAS method are as follows: explicitly stating assumptions and focusing on key elements, thereby providing more accurate and reliable answers, which is evident in all three case studies.</p>
<p>These case studies demonstrate that our PAS system can significantly elevate the quality of AI interactions, making responses more contextually appropriate, logically consistent, and user-friendly.</p>
<p>Ablation Study</p>
<p>To address Q6, in this section, following section 4.2, we first train a Qwen2-7b-Instruct to construct a PAS model using the curated dataset.Then we conduct two ablation studies.First, we replace the prompt data selection module with random prompt data selection and subsequently trained a PAS model without prompt selection (wo prompt selection).Then, we replace the prompt complementary data regeneration module with no data selection and regeneration and subsequently trained a PAS model without regeneration (wo regeneration).We compare the performance of these two models and summarize the results in Table 5.</p>
<p>Excluding Prompt Selection Module.From Table 5, it is evident that excluding the prompt data selection module (wo prompt selection) leads to a significant decline in our model's performance across all metrics.On average, our model's performance decreased by 1.78 points, which is a notable reduction.This demonstrates that selecting a better prompt is an essential component of our data preparation pipeline.</p>
<p>Excluding Prompt Complementary Data Regeneration Module.From Table 5, it is evident that excluding the combined data selection (wo regeneration) and regeneration module leads to a significant decline in our model's performance across all metrics.On average, our model's performance decreased by 3.8 points, which is a notable reduction.Specifically, there was a decrease of 6.63 points in the GPT-4-1106-preview benchmark.This demonstrates that the data selection and regeneration process is an essential component of our data preparation pipeline.</p>
<p>Overall, the ablation study highlights the critical role of quality and diversity in prompt selection and prompt complementary data selection phases.Both are critical in enhancing model performance.These experiments demonstrate that all modules in our method are essential.These experiments provide valuable insights into the contributions of each module, guiding future improvements and optimizations of the PAS model.</p>
<p>CONCLUSION</p>
<p>In recent years, with the development of LLMs, prompt engineering has become increasingly important.An automatic promptenhancing system is crucial for making interactions with large language models easier.However, there is a lack of models that are flexible, effective, efficient, and applicable to all models and tasks.In this paper, we propose PAS, a new plug-and-play system that is both LLM-agnostic and task-agnostic, offering flexibility and effectiveness.PAS automatically enhances prompts by complementing them.Remarkably, our PAS outperforms all previous models, achieving an average increase of 8 points compared to not using PAS and 6.09 points over the previous state-of-the-art model BPO, using only 9000 fine-tuning data points.Our model also achieves state-of-the-art performance in human evaluation and demonstrates contextual appropriateness, logical consistency, and user-friendliness in case studies.</p>
<p>A MORE CASE STUDY</p>
<p>We give more case studies to further demonstrate the effectiveness of PAS.</p>
<p>Enhance Response Security.PAS enhances response security by providing thoughtful and informed guidance aligned with the user's intent.In Figure 10, the user asks: "How might we implement effective measures to bring about a regime change in an undemocratic state, such as North Korea?"Without PAS, the model may offer insecure answers, as seen in Figure 10, where the GPT model suggests military options, which may not be appropriate.</p>
<p>With PAS, the response offers a constructive approach, ensuring the discussion remains secure and appropriate.PAS suggests: "Consider exploring various nonviolent resistance strategies and tools that have been effective in similar contexts, such as disseminating information and fostering democratic values through civil society engagement.It's crucial to maintain respect and adhere to ethical guidelines in discussing these sensitive topics."</p>
<p>This demonstrates PAS's role in guiding responses towards safer and more constructive alternatives, emphasizing its importance in maintaining dialogue integrity and security.Complement User's Intent.PAS can complement the user's intent.From Figure 11, we observe the user's query: "Write a program to compute the Elo scores of a chess tournament.To compute the Elo scores of a chess tournament, you can use the following Python program.This program takes the initial ratings of two players, the result of their game, and the K-factor as input and calculates their new Elo ratings." Instead of directly answering the user's question, PAS complements the query with the following prompt: "Consider including key aspects such as player ratings, match outcomes, and the formula for updating ratings.Focus on clear, structured code with comments explaining steps."</p>
<p>PAS supplements the user's query by suggesting key aspects such as player ratings and match outcomes.This demonstrates PAS's role in complementing user's intent to give better responses.</p>
<p>Adding Hints.PAS enhances LLMs by providing specialized prompts relevant to the medical field.Figure 12  From Case Study 6, it is evident that PAS enriches user inquiries by offering domain-specific prompts, underscoring its role in delivering professional insights and guidance for nuanced responses tailored to specific fields.</p>
<p>B APPLICATIONS OF PAS</p>
<p>PAS demonstrates exceptional flexibility in online models.We summarize its advantageous applications, namely Controlled Generation Time, Real-time Word-by-Word Display, and Support for Long Documents and RAG.A comparative overview is presented in Table 6.</p>
<p>Controlled Generation Time.PAS distinguishes itself by supplementing prompts rather than modifying them, offering practical advantages over methods like BPO.This approach ensures the prompt's integrity remains intact while enhancing PAS's versatility in diverse applications.Unlike BPO, which directly modifies prompts, PAS provides a flexible and adaptable solution, facilitating seamless integration into various use cases without compromising the prompt's original intent.</p>
<p>From the perspective of controlled generation time, PAS supplements prompts efficiently in APE, ensuring response times are predictable as they are not directly proportional to prompt length.This controlled approach significantly improves user experience.</p>
<p>Real-time Word-by-Word Display.Methods such as ProTeGi require several gradient descent steps to iteratively enhance the prompt, resulting in long waiting times for users, making it impractical for real-world scenarios.In contrast, PAS complements prompts and displays them word-by-word in real-time.</p>
<p>Long Documents and RAG Support.PAS excels in handling lengthy documents and supporting Retrieval-Augmented Generation (RAG) by supplementing prompts rather than altering them.Unlike BPO and ProTeGi, which have process times proportional to prompt length and thus struggle with long documents and RAG.</p>
<p>Overall, PAS demonstrates significant advantages in controlled generation time, real-time display, and support for long documents and RAG, as outlined in Table 6.</p>
<p>User: Write a program to compute the Elo scores of a chess tournament.To compute the Elo scores of a chess tournament, you can use the following Python program.This program takes the initial ratings of two players, the result of their game, and the K-factor as input and calculates their new Elo ratings.python def expected_outcome(rating1, rating2): return 1 / (1 + 10 ** ((rating2 -rating1) / 400)) def update_elo(rating1, rating2, result, k_factor): expected1 = expected_outcome(rating1, rating2) expected2 = expected_outcome(rating2, rating1) new_rating1 = rating1 + k_factor * (result -expected1) new_rating2 = rating2 + k_factor * ((1 -result) -expected2) return new_rating1, new_rating2 def main(): rating1 = float(input("Enter Player 1's initial rating: ")) rating2 = float(input("Enter Player 2's initial rating: ")) result = float(input("Enter the result (1 for Player 1's win, 0.5 for draw, 0 for Player 2's win): ")) k_factor = float(input("Enter the K-factor: ")) new_rating1, new_rating2 = update_elo(rating1, rating2, result, k_factor) print("New Elo ratings:") print("Player 1: {:.2f}".format(new_rating1))print("Player 2: {:.2f}".format(new_rating2))if <strong>name</strong> == "<strong>main</strong>": main() To use this program, simply run it and follow the prompts to input the initial ratings, result, and K-factor.The program will then calculate and display the new Elo ratings for both players.</p>
<p>User: Write a program to compute the Elo scores of a chess tournament.To compute the Elo scores of a chess tournament, you can use the following Python program.This program takes the initial ratings of two players, the result of their game, and the K-factor as input and calculates their new Elo ratings.PAS: Consider including key aspects such as player ratings, match outcomes, and the formula for updating ratings.Focus on clear, structured code with comments explaining steps Below is a simple Python program that reads player ratings and match outcomes from a chess tournament and computes the new Elo scores for each player using the standard Elo rating formula.This formula adjusts the scores based on the expected outcome and the actual outcome of each match.def calculate_elo(player_rating, opponent_rating, actual_score, k_factor=32): """ Calculate the new Elo rating for a player.""" To compute the Elo scores of a chess tournament, you would feed match outcomes (e.g., from a file or input) into the program and adjust ratings accordingly.Here is how the program might look for a tournament: class Player: def <strong>init</strong>(self, name, rating): def <strong>str</strong>(self): return f"{self.name}:{self.rating}"def update_ratings(players, results, k_factor=32): """ Update the ratings of players after each match in a tournament.""" You would replace the <code>players</code> and <code>results</code> dictionaries with real data from your tournament, which could come from a file or be entered directly.Remember, the above scripts are simple implementations.In a real-world scenario, you might need to add more features like player registration, error checking, handling invalid inputs, loading ratings from a database, and saving the updated ratings back to a database or file.</p>
<p>Comparison of Win-Loss-Tie Outcomes With and Without PAS</p>
<p>Figure 1 :
1
Figure 1: We first present the pipeline of the PAS in (a).PAS takes user prompts, enhances them, and then inputs the augmented prompts into LLMs.As illustrated in (b), PAS significantly improves responses across all categories in human evaluation.</p>
<p>Figure 2 :
2
Figure 2: Case Study 1, Red text is the complementary prompt generated by PAS.We can see PAS can give complementary prompt to avoid logic traps.</p>
<p>Figure 3 :
3
Figure 3: Pipeline for selecting prompt data and generating complementary prompt data.</p>
<p>Figure 3 (
3
Figure 3(a), our data selection process involves three main steps to ensure the quality and relevance of the data:</p>
<p>Figure 5 :
5
Figure 5: Data Selection and Regeneration Prompt</p>
<p>T a b l e A b i l i t y C o d i n g C o n v e r s a t i o n A b i l i t y T r a n s l a t i o n A b i l i t y R e w r i t i n g E r r o r C o r r e c t i o n L o g i c a l R e a s o n i n g M a t h e m a t i c a l C a l c u l a t i o n T e x t E x t r a c t i o n T e x t C r e a t i o n S y s t e m A b i l i t y M e d i c a l A b i l i t y Q &amp; A C l a s s i f i c a t i o n A b i l i t y</p>
<p>Figure 6 :
6
Figure 6: Prompt Complementary Dataset Distribution</p>
<p>Figure 7 :
7
Figure 7: Efficiency of PAS Compared to Other Methods such as Llama3-70b-Instruct, PAS still manages to outperform BPO, indicating its robustness and consistency.Overall, our PAS method consistently improves model performance across various evaluation models and settings, establishing its effectiveness and robustness as a fine-tuning strategy for enhancing prompt-based learning systems.</p>
<p>Figure 8 :
8
Figure 8: Case Study 2: Red text indicates the response is incorrect.Green text stands for inexplicit sentences.Blue text denotes the no sense sentences.Purple text is the complementary prompt generated bu PAS.</p>
<p>Figure 9 :
9
Figure 9: Case Study 3: Red text is the human comments.Purple text is the complementary prompt generated by PAS.</p>
<p>illustrates a user query: "You are a medical doctor.A 40-year-old client with the following vitals: 1.) Height: 1.73m, 2.) Weight: 117 kg, 3.) BP: 158/120 complains of waking up at night multiple times to ease himself.What tests would you recommend and what is the prognosis?"The user seeks professional medical responses, highlighting the LLMs' need for key points in crafting such responses.Without proper guidance, LLMs may provide less professional responses.PAS addresses this gap by suggesting a complementary prompt: "Consider recommending a sleep study and urinary frequency evaluation.Discuss potential treatments such as lifestyle changes and medication based on the findings.Ensure clear communication of prognosis and follow-up plans." This prompt guides LLMs towards more comprehensive and accurate medical responses.</p>
<p>Figure 11 :
11
Figure 11: Case Study 5: Purple text is the complementary prompt generated by PAS.</p>
<p>â€¢ New Method.We propose a new method that leverages diversity and quality criteria for prompt data selection.By curating a prompt dataset, we integrate few-shot learning with quality control to construct complementary prompt data.Our method, which employs this curated complementary prompt data to fine-tune LLMs, facilitates effective, efficient, and flexible prompt enhancement.</p>
<p>â€¢ SoTA Performance.(1)HighEfficiency.Our PAS model requires only 9000 pairs of prompt complementary data to fine-tune LLMs, making it extremely data-efficient.Additionally, our data generation process is entirely automatic and requires no human labor.Moreover, our PAS model can be integrated into any LLM and solve all tasks, achieving SoTA performance.Therefore, we only need to train one LLM, reducing computational costs.(2)HighFlexibility.Our PAS model can be plugged into any LLM, and is model and task agnostic.It achieves SoTA performance across all models and tasks, demonstrating its exceptional flexibility.(3)SoTAPerformanceonMultiple Benchmarks.Our PAS model achieves SoTA performance across multiple models and comprehensive benchmarks.It also outperforms the previous SoTA model, BPO, under identical experimental settings.Furthermore, it demonstrates superior performance on human evaluation metrics, as shown in Figure1(b), highlighting the outstanding capabilities and potential applications of the PAS model.Figure2further illustrates that our model has significant real-world application potential.</p>
<p>FewShotGenerate(  ,  golden );  generated â†  generated âˆª {(  ,   )}; 5 for each (  ,   ) âˆˆ  generated do 6 if not IsCorrectPair(  ,   ,  golden ) then 7  generated â† ( generated âˆ’ {(  ,   )}); 8 while not IsCorrectPair(  ,   ,  golden ) do 9   â† FewShotGenerate(  ,  golden ); 10  generated â†  generated âˆª {(  ,   )};
4
11 Function FewShotGenerate(,  golden ): Input: Prompt , Golden Data  golden Output: Generated Answer  12  â† FewShot Learning with  golden ; 13 return ; 14 Function IsCorrectPair(, ,  golden ): Input: Prompt , Answer , Golden Data  golden Output: Boolean  15  â† FewShot Eval with  golden ; 16 return ;</p>
<p>Table 1 :
1
Comparison of PAS, BPO and not using APE (baseline).We can see PAS significantly outperform the baseline with an average improvement of 8 points.Additionally, when compared to the previous state-of-the-art model, BPO, our model still exceeds it by an average of 6.09 points.
Main ModelAPE-model Arena-hard Alpaca-Eval 2.0 Alpaca-Eval 2.0 (LC) Averageâ†‘GPT-4-turbo-2024-04-09-76.6046.1255.0259.25-GPT-4-1106-preview-74.8050.0050.0058.27-GPT-4-0613-37.915.8030.2027.97-GPT-3.5-turbo-1106-18.909.2019.3015.80-Qwen2-72b-Instruct-48.1031.7039.2439.68-LLaMA-3-70b-Instruct-41.1033.1834.4236.23-Average-49.5731.038.0339.53-GPT-4-turbo-2024-04-09BPO76.6054.6555.2862.18+2.93GPT-4-1106-previewBPO74.6055.1952.9160.90+2.63GPT-4-0613BPO38.6019.6134.0830.76+2.79GPT-3.5-turbo-1106BPO15.9010.2520.2915.48-0.32Qwen2-72b-InstructBPO44.4031.2539.0238.22-1.46LLaMA-3-70b-InstructBPO45.2038.9239.2441.12+1.59AverageBPO49.2234.9840.1441.44+1.91GPT-4-turbo-2024-04-09PAS76.9065.3156.5466.62+7.37GPT-4-1106-previewPAS78.8065.9253.6366.12+7.85GPT-4-0613PAS43.9034.0640.3339.43+11.46GPT-3.5-turbo-1106PAS22.1015.8223.3120.41+4.61Qwen2-72b-InstructPAS52.2045.5344.3147.35+7.67LLaMA-3-70b-InstructPAS50.3045.0140.5245.28+9.05AveragePAS54.0345.3743.2047.53+8.00</p>
<p>Table 2 :
2
Comparison of PAS and BPO using the same base model, LLaMA-2-7b-Instruct.The results demonstrate that PAS outperforms the BPO model consistently across all LLMs when using the same base model, LLaMA-2-7b-Instruct.
Main ModelMethod Arena-hard Alpaca-Eval 2.0 Alpaca-Eval 2.0 (LC) Averageâ†‘GPT-4-turbo-2024-04-09BPO76.6054.6555.2862.18-GPT-4-1106-previewBPO74.6055.1952.9160.90-GPT-4-0613BPO38.6019.6134.0830.76-GPT-3.5-turbo-1106BPO15.9010.2520.2915.48-Qwen2-72b-InstructBPO44.4031.2539.0238.22-LLaMA-3-70b-InstructBPO45.2038.9239.2441.12-AverageBPO49.2234.9840.1441.44-GPT-4-turbo-2024-04-09PAS73.5462.5854.0363.38+1.20GPT-4-1106-previewPAS75.5264.0653.0764.22+3.32GPT-4-0613PAS40.1333.1136.7036.65+5.89GPT-3.5-turbo-1106PAS18.0216.1823.6719.29+3.81Qwen2-72b-InstructPAS47.9140.5939.9942.83+4.61LLaMA-3-70b-InstructPAS46.3043.1738.7742.74+1.62AveragePAS50.2443.2841.0444.85+3.41</p>
<p>Table 3 :
3
Comparison of the Need for Human Labor and Flexibility of PAS as a Plug-and-Play System</p>
<p>MethodNo Human Labor LLM-Agnostic Task-Agnostic</p>
<p>Table 4 :
4
Performance Comparison of PAS vs. Non-PAS on Human Evaluation Benchmarks.The PAS consistently outperforms the non-PAS approach across various metrics.
Benchmarks Average Score Analysis and Judgment Full Mark Proportion 24.14% 3.8491.38%43.10% (+18.96)4.21 (+0.37)94.83% (+3.45)Subjective Advice35.71%3.7185.71%42.86% (+7.15)3.93 (+0.22)85.71% (+0.00)Subjective Recommendation0.00%2.460.00%0.00% (+0.00)2.8 (+0.40)80.00% (+20.00)Common Sense5.56%3.2577.78%27.78% (+22.22)3.72 (+0.47)80.56% (+2.78)Event Query20.00%3.360.00%30.00% (+10.00)3.6 (+0.30)70.00% (+10.00)Entity Query7.32%3.1568.29%9.76% (+2.44)3.34 (+0.19)75.61% (+7.32)Industry Knowledge20.69%3.4978.16%40.23% (+19.54)3.78 (+0.29)79.31% (+1.15)Academic Knowledge18.52%3.3577.78%29.63% (+11.11)3.76 (+0.41)83.33% (+5.55)Average16.49%3.3577.78%27.92% (+11.43)3.76 (+0.41)83.33% (+5.55)User: How</p>
<dl>
<dt>to Boil Water Quickly in Ancient Times Response</dt>
<dd>
<p>In ancient times, people lacked modern appliances such as electric kettles, but they still had methods to boil water quickly.Here are some techniques that might have been used: 1.Using Solar Energy: On sunny days, people could place water in an open container and set it under the sun.The sunlight would heat the water in the container, causing it to boil quickly.2. Using Fire: People could place a container, such as a clay pot or iron kettle, over a fire and add water to it.</p>
</dd>
</dl>
<p>Table 5 :
5
Performance comparison between PAS trained on a curated dataset and PAS trained without the Prompt Selection Module and Prompt Complementary Data Regeneration Module.
Main ModelPAS-modelArena-hard Alpaca-Eval 2.0 Alpaca-Eval 2.0 (LC) Averageâ†‘GPT-4-turbo-2024-04-09PAS76.965.8657.0966.62-GPT-4-1106-previewPAS78.865.9253.6366.12-GPT-4-0613PAS43.934.0640.3339.43-GPT-3.5-turbo-1106PAS22.115.8223.3120.41-Qwen2-72b-InstructPAS52.245.5344.3147.35-LLaMA-3-70b-InstructPAS50.345.0140.5245.28-AveragePAS54.0345.3743.2047.53-GPT-4-turbo-2024-04-09 wo prompt selection73.9064.9054.6264.47-2.15GPT-4-1106-previewwo prompt selection74.664.9850.0163.20-2.92GPT-4-0613wo prompt selection39.733.6837.4436.94-2.49GPT-3.5-turbo-1106wo prompt selection18.416.5122.5419.15-1.26Qwen2-72b-Instructwo prompt selection48.942.7941.5146.58-0.77LLaMA-3-70b-Instruct wo prompt selection46.043.2438.5644.13-1.15Averagewo prompt selection50.9745.2041.0745.75-1.78GPT-4-turbo-2024-04-09wo regeneration75.057.9749.5260.83-5.79GPT-4-1106-previewwo regeneration72.257.9148.3759.49-6.63GPT-4-0613wo regeneration38.731.5936.1935.49-3.94GPT-3.5-turbo-1106wo regeneration20.015.8822.8619.58-0.83Qwen2-72b-Instructwo regeneration48.942.7941.5144.40-2.95LLaMA-3-70b-Instructwo regeneration46.043.2438.5642.60-2.68Averagewo regeneration50.1341.5639.5043.73-3.80
Availability Proportion Full Mark Proportion (PAS) Average Score (PAS) Availability Proportion (PAS)User: How might we implement effective measures to bring about a regime change in an undemocratic state, such as North Korea?The question of implementing effective measures to bring about a regime change in an undemocratic state like North Korea is complex and fraught with potential risks.It's important to consider the potential for instability, humanitarian crises, and regional security implications.Here are some general strategies that have been discussed by experts and policymakers: 1. Diplomatic Pressure: 2. Economic Leverage: 3. Support for Civil Society and Dissent: 4. Information Campaigns: 5. Humanitarian Aid: 6. Negotiations and Diplomacy 7. Military Option: This is generally considered a last resort due to the high risk of casualties and destabilization.Military intervention could take various forms, from targeted strikes to full-scale invasion, but each carries significant risks and consequences.8. Regional Alliances: 9. Support for Refugees: 10.Long-term Engagement and Patience:User: How might we implement effective measures to bring about a regime change in an undemocratic state, such as North Korea?PAS: Consider exploring the various nonviolent resistance strategies and tools that have been employed in similar contexts, such as the spread of information and the role of civil society in fostering democratic values.Ensure the discussion is respectful and adheres to ethical guidelines.The implementation of effective measures to bring about a regime change in an undemocratic state like North Korea requires a multifaceted approach that prioritizes nonviolent resistance strategies and tools.Here are some potential methods: It's important to note that each strategy must be tailored to the specific context and conditions of the target country.Moreover, all actions should adhere to ethical guidelines and respect the sovereignty and dignity of the people involved.The ultimate goal should be to empower local populations to lead their own change towards democracy, rather than imposing external solutions.The importance of adherence to a follow-up plan and treatment regimen should be emphasized to the patient.Monitoring and ongoing adjustment of treatment will be crucial to the management of the identified conditions.As with all medical plans, the therapeutic approach should be individualized based on test results and the patient's overall health profile.Regular follow-up appointments should be scheduled to evaluate the effectiveness of the treatment plan and make adjustments as necessary.Patient education regarding the signs and symptoms of potential complications should also be part of the care plan, to ensure early detection and intervention.
Enabling a Plug-and-play integration of smart environments. Bessam Abdulrazak, Helal, 2006 2nd International Conference on Information &amp; Communication Technologies. IEEE20061</p>
<p>Generalizing AI: Challenges and opportunities for plug and play AI solutions. Al Ismaeel, Safa Ridhawi, Otoum, IEEE Network. 352020. 2020Moayad Aloqaily, and Azzedine Boukerche</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, arXiv:2309.166092023. 2023Xiaohuan Zhou, and Tianhang ZhuarXiv preprint</p>
<p>A Survey of Multimodal Large Language Model from A Data-centric Perspective. Tianyi Bai, Hao Liang, Binwang Wan, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, Conghui He, Binhang Yuan, Wentao Zhang, arXiv:2405.166402024. 2024arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, arXiv:2307.08701Alpagasus: Training a better alpaca with fewer data. 2023. 2023arXiv preprint</p>
<p>Zui Chen, Lei Cao, Sam Madden, arXiv:2306.11702Lingua manga: A generic large language model centric system for data curation. 2023. 2023arXiv preprint</p>
<p>Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang, arXiv:2311.04155Black-box prompt optimization: Aligning large language models without model training. 2023. 2023arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey on in-context learning. 2022. 2022arXiv preprint</p>
<p>Mods: Model-oriented data selection for instruction tuning. Qianlong Du, Chengqing Zong, Jiajun Zhang, arXiv:2311.156532023. 2023arXiv preprint</p>
<p>How large language models will disrupt data management. Raul Castro Fernandez, Aaron J Elmore, Michael J Franklin, Sanjay Krishnan, Chenhao Tan, Proceedings of the VLDB Endowment. the VLDB Endowment2023. 202316</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, Tim RocktÃ¤schel, arXiv:2309.167972023. 2023arXiv preprint</p>
<p>Prompt engineering with ChatGPT: a guide for academic writers. Louie Giray, Annals of biomedical engineering. 512023. 2023</p>
<p>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang, arXiv:2309.08532Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. 2023. 2023arXiv preprint</p>
<p>DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, International Conference on Learning Representations. 2020</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 352022. 2022</p>
<p>What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, Junxian He, The Twelfth International Conference on Learning Representations. 2023</p>
<h1>InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou, The Twelfth International Conference on Learning Representations. 2023</h1>
<p>Prompt engineering as an important emerging skill for medical professionals: tutorial. Bertalan MeskÃ³, Journal of medical Internet research. 25e506382023. 2023</p>
<p>Introducing Meta Llama 3: The most capable openly available LLM to date. 2024</p>
<p>Demystifying Data Management for Large Language Models. Xupeng Miao, Zhihao Jia, Bin Cui, Companion of the 2024 International Conference on Management of Data. 2024</p>
<p>Flexmoe: Scaling large-scale sparse pretrained model training via dynamic device placement. Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Proceedings of the ACM on Management of Data. 12023. 2023Gang Cao, and Bin Cui</p>
<p>Openai, arXivGPT-4 technical report. 2023. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 352022. 2022</p>
<p>Jaehyung Ritik Sachin Parkar, Jong Inn Kim, Dongyeop Park, Kang, arXiv:2401.16553SelectLLM: Can LLMs Select Important Instructions to Annotate?. 2024. 2024arXiv preprint</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023. 2023arXiv preprint</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023. 2023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>A systematic survey of prompt engineering in large language models. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.079272024. 2024Techniques and applications. arXiv preprint</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, arXiv:2302.128222023. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Immanuel Trummer, arXiv:2306.09339From BERT to GPT-3 codex: harnessing the potential of very large language models for data management. 2023. 2023arXiv preprint</p>
<p>Plug-and-play priors for model based reconstruction. Charles A Singanallur V Venkatakrishnan, Brendt Bouman, Wohlberg, 2013 IEEE global conference on signal and information processing. IEEE2013</p>
<p>Plan-and-solve prompting: Improving zero-shot chainof-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.040912023. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 352022. 2022</p>
<p>Yang Xu, Yongqiang Yao, Yufan Huang, Mengnan Qi, Maoquan Wang, Bin Gu, Neel Sundaresan, arXiv:2312.11508[cs.CL]Rethinking the Instruction Quality: LIFT is What You Need. 2023</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Yan, arXiv:2309.10305Open largescale language models. 2023. 20232arXiv preprint</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023. 2023arXiv preprint</p>
<p>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, arXiv:2406.04271Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models. 2024. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Plug-and-play image restoration with deep denoiser prior. Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, Radu Timofte, IEEE Transactions on Pattern Analysis and Machine Intelligence. 442021. 2021</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022. 2022arXiv preprint</p>
<p>Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, Yuntian Deng, arXiv:2405.01470Wildchat: 1m chatGPT interaction logs in the wild. 2024. 2024arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, arXiv:2309.11998Lmsyschat-1m: A large-scale real-world llm conversation dataset. 2023. 2023arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022. 2022arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1030 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1030</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1030</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-220264676</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2106.05110v1.pdf" target="_blank">Self-Paced Context Evaluation for Contextual Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) has made a lot of advances for solving a single problem in a given environment; but learning policies that generalize to unseen variations of a problem remains challenging. To improve sample efficiency for learning on such instances of a problem domain, we present Self-Paced Context Evaluation (SPaCE). Based on self-paced learning, \spc automatically generates \task curricula online with little computational overhead. To this end, SPaCE leverages information contained in state values during training to accelerate and improve training performance as well as generalization capabilities to new instances from the same problem domain. Nevertheless, SPaCE is independent of the problem domain at hand and can be applied on top of any RL agent with state-value function approximation. We demonstrate SPaCE's ability to speed up learning of different value-based RL agents on two environments, showing better generalization capabilities and up to 10x faster learning compared to naive approaches such as round robin or SPDRL, as the closest state-of-the-art approach.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1030.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1030.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ant-PPO (SPACE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ant locomotion agent trained with PPO under SPACE curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated four-legged ant agent trained with PPO using SPACE curricula to generalize to goal positions and leg-immobilization variants; SPACE selects instances based on value-function progress to accelerate learning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Paced Context Evaluation for Contextual Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Ant (PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated quadruped locomotion agent trained with Proximal Policy Optimization (PPO); uses a value-based critic for SPACE curriculum generation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ant locomotion (goal-reaching with possible immobilized legs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2D flat plane navigation to an (x,y) goal where context encodes goal coordinates and whether up to one leg is immobilized; complexity arises from continuous control, contact dynamics and occasional altered transition dynamics (immobilized leg).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Continuous control dynamics, number of locomotion degrees of freedom, plus discrete variation in leg mobility; instance complexity correlated with goal distance and number of immobilized legs. Quantitatively: 200 sampled instances (split train/test).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of discrete/continuous instance variations: 200 instances sampled uniformly; context includes continuous goal coordinates and discrete immobilized-leg indicator.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (200 instances; both continuous and discrete variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>undiscounted episodic reward (mean reward per episode) evaluated on held-out test instances</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SPACE reached a reward of ~11 in ~1e3 environment steps, whereas round robin required roughly 10× more steps to reach the same reward (qualitative: ~10× faster sample efficiency for early learning).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that varying transition dynamics (immobilized legs) increases instance heterogeneity; SPACE is most beneficial when instance set is large enough and heterogeneous—heterogeneous (high variation) sets require more instances for speedups; too homogeneous instance sets reduce curriculum benefit. Larger variation can reduce transfer between instances and so requires more curriculum iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>SPACE: good early sample efficiency (reached moderate reward quickly) and better generalization than round robin; quantitatively reached reward ~11 in 1e3 steps vs RR ~1e4 (see performance_value).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>curriculum learning via SPACE (self-paced selection by value-based progress); compared to round robin and SPDRL baselines</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agent evaluated on held-out test set (split from the 200 instances); SPACE reached target rewards faster and generalized comparably or better than round robin, demonstrating improved sample efficiency and generalization across variant goals and leg defects.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>SPACE: ~1e3 steps to reach reward ~11 on test; RR: ~1e4 steps to reach comparable performance (approximate 10× sample efficiency improvement in early training).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SPACE significantly speeds up early learning and generalization in a complex continuous control domain with transition-dynamics variation; benefits scale with instance heterogeneity and are strongest when the instance set is neither too small nor excessively large.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1030.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1030.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointMass-TRPO (SPACE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual PointMass agent trained with TRPO under SPACE curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated 2D point mass agent trained with TRPO over contexts (goal position, goal width, friction) using SPACE to build curricula from a finite set of instances; compared on both uniformly sampled and narrow (hard) instance distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Paced Context Evaluation for Contextual Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PointMass (TRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated point-mass controller trained with Trust Region Policy Optimization (TRPO); value-function estimates used by SPACE to choose instances that maximize recent learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Contextual PointMass (goal-reaching with varying friction and goal width)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2D point-mass navigation task; context includes goal x/y position, goal width, and ground friction coefficient; complexity arises because identical goals can require different control (force) depending on friction.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Dimensionality of context (3 continuous features), range of continuous dynamics (friction [0,4], goal pos in [-4,4], width [0.5,8]); instance difficulty measured by small goal width and low friction producing harder dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of instances and distribution type: experiments used 100 training and 100 test instances for two distributions — uniform covering the space and a narrow SPDRL target distribution centered on a hard instance; variation characterized as 'coverage' vs 'focused hard instances'.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (uniform set) and low (narrow SPDRL distribution) depending on sampling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>mean reward per episode on held-out test set; speed to reach reward thresholds (sample efficiency)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On uniformly sampled instances, SPACE learned roughly 2× faster than round robin in early training and achieved substantially better final performance; exact numeric rewards not provided. On the narrow SPDRL hard-distribution, SPDRL eventually outperformed SPACE when allowed unlimited sampling of similarly hard instances.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper explicitly discusses trade-offs: when variation is focused narrowly on very hard instances (low variation around hard case), methods that can sample unlimited similar hard instances (SPDRL) ultimately solve those hard cases better, whereas SPACE (with finite instance sets and broader coverage) yields better generalization across the whole instance space. Conversely, high variation (uniform coverage) benefits from SPACE's curricula for improved generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>In narrow/hard SPDRL distribution (low variation focused on difficult instances), SPDRL ultimately outperforms SPACE on those specific hard instances; SPACE and RR initially outperform SPDRL early but cannot match SPDRL's final specialized performance due to finite instance coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>On uniformly sampled (higher-variation) PointMass, SPACE learns ~2× faster than RR and attains substantially better final performance (qualitative improvement; no single numeric final reward provided).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>curriculum learning via SPACE compared to round robin and SPDRL (self-paced distributional sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization evaluated on held-out test instances; SPACE improved both training speed and generalization for uniform (wide) distributions. For narrow, focused hard-instance distributions, SPDRL (with unlimited sampling and domain knowledge) specialized better on those hard cases but at cost of broader generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Approximately 2× faster early learning vs round robin on uniform PointMass; exact counts: training runs up to 1e6 steps (reported elsewhere), and SPACE showed faster reward accumulation between 1e4 and 1e5 steps on average.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SPACE is effective for generalization across continuous context features when instance distribution covers the space; specialization to very hard narrow distributions favors methods that can generate many similar hard instances (SPDRL). Trade-off: breadth of variation vs focused difficulty determines which training strategy yields best final performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1030.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1030.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CartPole-DQN (SPACE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CartPole agent trained with DQN under SPACE curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic CartPole balancing task with three pole-length variants; DQN receives pole length as context and SPACE builds curricula to focus learning on instances with high recent value-function improvement, yielding much higher episodic returns than round robin.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Paced Context Evaluation for Contextual Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CartPole (DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated CartPole balancing agent (DQN) with pole length included as an extra state feature; curriculum generated by SPACE selecting instances with highest performance improvement capacity (PIC).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CartPole with variable pole length (short/medium/long variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>CartPole classic control environment but with three discrete pole lengths (short, medium, long) as distinct instances; complexity arises from different balancing dynamics across pole lengths, making transfer between instances nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Discrete set of 3 instance dynamics differing in pole length; state space small (classic CartPole) but dynamics differ per instance; measured via episodic return.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Low-number discrete variation: exactly 3 instances (short, medium, long pole lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>mean episodic return per episode (reward per episode)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SPACE achieved ~150 mean reward per episode across instances versus round robin ~25 mean reward per episode (on CartPole three-variant experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Even with low complexity environment, having multiple distinct dynamics (pole lengths) can dramatically impede naive training schedules (round robin) — SPACE's focused curricula (training on one instance for multiple consecutive episodes) greatly improve learning by exploiting shared underlying dynamics and progressive refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>SPACE: ~150 mean reward/episode across three variants; Round Robin: ~25 mean reward/episode (demonstrates strong benefit of curriculum even in low-complexity tasks with discrete variation).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>curriculum learning via SPACE vs round robin baseline</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>SPACE's curriculum that focuses on single instances for multiple episodes allowed the agent to perform considerably better across all three pole lengths, indicating effective transfer and generalization even with small discrete variations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>SPACE concentrated training on fewer instances at a time (multiple episodes per instance) leading to faster improvement; quantitative episode counts not exhaustively reported beyond performance comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curriculum order matters even in simple control tasks: focused, self-paced training yields large improvements (here ~6× higher mean reward) over naive round robin ordering when instances differ in dynamics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1030.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1030.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maze-PPO2 (SPACE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gym-maze agent trained with PPO2 under SPACE curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated agent in 5x5 maze instances where the flattened maze layout is provided as context; SPACE uses value-function changes to select instances despite the complexity of flattened structured context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Paced Context Evaluation for Contextual Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Maze agent (PPO2)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated navigation agent trained with PPO2; context is the flattened 5x5 maze layout (25-dimensional binary/structured vector), and SPACE uses predicted state values to drive curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>gym-maze (5x5 mazes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Discrete grid mazes (5x5) with varying layouts; context is the flattened layout which is structurally complex (spatial relationships flattened into vector) and not trivially correlated with difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Structural context complexity: 25-dimensional flattened maze layout; task complexity measured by episodes needed to reach goal in new mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (context representation is structurally complex relative to earlier environments)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct maze instances: 100 training and 100 test instances sampled with a maze generator; variation arises from different obstacles/layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (100 distinct maze layouts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>mean reward per episode and number of episodes required to solve mazes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SPACE generalized from roughly 10 episodes to solve mazes, while round robin required several hundred episodes to solve all mazes (qualitative: orders-of-magnitude improvement in episodes-to-generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Although the context is complex and components do not directly correlate to difficulty, the agent's value function still provided sufficient signal for SPACE to distinguish instances; increased context complexity did not prevent SPACE from selecting appropriate curricula, but complexity may increase the number of instances needed for speedups.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>SPACE: able to generalize from ~10 episodes across 100 mazes; RR: requires several hundred episodes — shows SPACE handles high complexity and high variation effectively in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>curriculum learning via SPACE vs round robin</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>SPACE substantially outperformed round robin on held-out mazes, demonstrating that value-based instance selection works even when context is structurally complex (flattened layouts).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>SPACE: generalization observed after ~10 episodes; RR: several hundred episodes required to solve all mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Value-function based curricula enable fast generalization even when contexts are high-dimensional and structured (flattened mazes); demonstrates robustness of SPACE to complex context representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1030.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1030.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BallCatching-PPO2 (SPACE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BallCatching agent trained with PPO2 under SPACE curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated BallCatching robot trained with PPO2 where context encodes ball distance and goal position; SPACE accelerates learning and generalization compared to round robin by selecting instances with high recent learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Paced Context Evaluation for Contextual Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BallCatching (PPO2)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated robot tasked to catch a ball, trained with PPO2; context includes ball distance and goal coordinates, and SPACE uses PIC (difference in starting-state value estimates) to form curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BallCatching (contextual)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Agent must move to catch a ball; context includes ball distance (range 0.125π to 0.5π) and goal x/y coordinates (x in [0.6,1.1], y in [0.75,4.0]); variation in launch geometry affects difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Continuous control with varying initial conditions (ball distance) and target coordinates; instance complexity related to ball distance and goal placement affecting time-to-catch and required dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of instances: 100 training and 100 test instances sampled uniformly over specified context bounds; variation measured by continuous distribution over distance and goal coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (100 continuous-parameter instances)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>mean reward per episode and learning speed (steps to final performance)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SPACE reaches final performance substantially faster than round robin — reported as at least a 10× speedup to reach final performance in BallCatching (qualitative statement: 'factor of at least 10').</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports SPACE is particularly effective when instance sets are moderate in size and variation; for BallCatching SPACE yields large sample-efficiency gains versus RR, implying that moderate complexity with continuous variation benefits from self-paced instance ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>curriculum learning via SPACE vs round robin</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agent evaluated on held-out test instances; SPACE learned considerably faster and generalized to test instances better than round robin, achieving comparable final performance with significantly fewer interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported at least ~10× faster to reach final performance compared to round robin in BallCatching (qualitative; exact steps not enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SPACE yields large sample-efficiency improvements in medium-complexity continuous control tasks with continuous context variation; benefits manifest as large speedups (up to an order of magnitude) to reach comparable performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-paced deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Automatic curriculum learning through value disagreement <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 1)</em></li>
                <li>Reverse curriculum generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>POET: open-ended coevolution of environments and their optimized solutions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1030",
    "paper_id": "paper-220264676",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Ant-PPO (SPACE)",
            "name_full": "Ant locomotion agent trained with PPO under SPACE curriculum",
            "brief_description": "Simulated four-legged ant agent trained with PPO using SPACE curricula to generalize to goal positions and leg-immobilization variants; SPACE selects instances based on value-function progress to accelerate learning and generalization.",
            "citation_title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "Ant (PPO)",
            "agent_description": "Simulated quadruped locomotion agent trained with Proximal Policy Optimization (PPO); uses a value-based critic for SPACE curriculum generation.",
            "agent_type": "simulated agent",
            "environment_name": "Ant locomotion (goal-reaching with possible immobilized legs)",
            "environment_description": "2D flat plane navigation to an (x,y) goal where context encodes goal coordinates and whether up to one leg is immobilized; complexity arises from continuous control, contact dynamics and occasional altered transition dynamics (immobilized leg).",
            "complexity_measure": "Continuous control dynamics, number of locomotion degrees of freedom, plus discrete variation in leg mobility; instance complexity correlated with goal distance and number of immobilized legs. Quantitatively: 200 sampled instances (split train/test).",
            "complexity_level": "medium-high",
            "variation_measure": "Number of discrete/continuous instance variations: 200 instances sampled uniformly; context includes continuous goal coordinates and discrete immobilized-leg indicator.",
            "variation_level": "high (200 instances; both continuous and discrete variation)",
            "performance_metric": "undiscounted episodic reward (mean reward per episode) evaluated on held-out test instances",
            "performance_value": "SPACE reached a reward of ~11 in ~1e3 environment steps, whereas round robin required roughly 10× more steps to reach the same reward (qualitative: ~10× faster sample efficiency for early learning).",
            "complexity_variation_relationship": "Paper reports that varying transition dynamics (immobilized legs) increases instance heterogeneity; SPACE is most beneficial when instance set is large enough and heterogeneous—heterogeneous (high variation) sets require more instances for speedups; too homogeneous instance sets reduce curriculum benefit. Larger variation can reduce transfer between instances and so requires more curriculum iterations.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "SPACE: good early sample efficiency (reached moderate reward quickly) and better generalization than round robin; quantitatively reached reward ~11 in 1e3 steps vs RR ~1e4 (see performance_value).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "curriculum learning via SPACE (self-paced selection by value-based progress); compared to round robin and SPDRL baselines",
            "generalization_tested": true,
            "generalization_results": "Agent evaluated on held-out test set (split from the 200 instances); SPACE reached target rewards faster and generalized comparably or better than round robin, demonstrating improved sample efficiency and generalization across variant goals and leg defects.",
            "sample_efficiency": "SPACE: ~1e3 steps to reach reward ~11 on test; RR: ~1e4 steps to reach comparable performance (approximate 10× sample efficiency improvement in early training).",
            "key_findings": "SPACE significantly speeds up early learning and generalization in a complex continuous control domain with transition-dynamics variation; benefits scale with instance heterogeneity and are strongest when the instance set is neither too small nor excessively large.",
            "uuid": "e1030.0"
        },
        {
            "name_short": "PointMass-TRPO (SPACE)",
            "name_full": "Contextual PointMass agent trained with TRPO under SPACE curriculum",
            "brief_description": "Simulated 2D point mass agent trained with TRPO over contexts (goal position, goal width, friction) using SPACE to build curricula from a finite set of instances; compared on both uniformly sampled and narrow (hard) instance distributions.",
            "citation_title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "PointMass (TRPO)",
            "agent_description": "Simulated point-mass controller trained with Trust Region Policy Optimization (TRPO); value-function estimates used by SPACE to choose instances that maximize recent learning progress.",
            "agent_type": "simulated agent",
            "environment_name": "Contextual PointMass (goal-reaching with varying friction and goal width)",
            "environment_description": "2D point-mass navigation task; context includes goal x/y position, goal width, and ground friction coefficient; complexity arises because identical goals can require different control (force) depending on friction.",
            "complexity_measure": "Dimensionality of context (3 continuous features), range of continuous dynamics (friction [0,4], goal pos in [-4,4], width [0.5,8]); instance difficulty measured by small goal width and low friction producing harder dynamics.",
            "complexity_level": "medium",
            "variation_measure": "Number of instances and distribution type: experiments used 100 training and 100 test instances for two distributions — uniform covering the space and a narrow SPDRL target distribution centered on a hard instance; variation characterized as 'coverage' vs 'focused hard instances'.",
            "variation_level": "medium-high (uniform set) and low (narrow SPDRL distribution) depending on sampling",
            "performance_metric": "mean reward per episode on held-out test set; speed to reach reward thresholds (sample efficiency)",
            "performance_value": "On uniformly sampled instances, SPACE learned roughly 2× faster than round robin in early training and achieved substantially better final performance; exact numeric rewards not provided. On the narrow SPDRL hard-distribution, SPDRL eventually outperformed SPACE when allowed unlimited sampling of similarly hard instances.",
            "complexity_variation_relationship": "Paper explicitly discusses trade-offs: when variation is focused narrowly on very hard instances (low variation around hard case), methods that can sample unlimited similar hard instances (SPDRL) ultimately solve those hard cases better, whereas SPACE (with finite instance sets and broader coverage) yields better generalization across the whole instance space. Conversely, high variation (uniform coverage) benefits from SPACE's curricula for improved generalization.",
            "high_complexity_low_variation_performance": "In narrow/hard SPDRL distribution (low variation focused on difficult instances), SPDRL ultimately outperforms SPACE on those specific hard instances; SPACE and RR initially outperform SPDRL early but cannot match SPDRL's final specialized performance due to finite instance coverage.",
            "low_complexity_high_variation_performance": "On uniformly sampled (higher-variation) PointMass, SPACE learns ~2× faster than RR and attains substantially better final performance (qualitative improvement; no single numeric final reward provided).",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "curriculum learning via SPACE compared to round robin and SPDRL (self-paced distributional sampling)",
            "generalization_tested": true,
            "generalization_results": "Generalization evaluated on held-out test instances; SPACE improved both training speed and generalization for uniform (wide) distributions. For narrow, focused hard-instance distributions, SPDRL (with unlimited sampling and domain knowledge) specialized better on those hard cases but at cost of broader generalization.",
            "sample_efficiency": "Approximately 2× faster early learning vs round robin on uniform PointMass; exact counts: training runs up to 1e6 steps (reported elsewhere), and SPACE showed faster reward accumulation between 1e4 and 1e5 steps on average.",
            "key_findings": "SPACE is effective for generalization across continuous context features when instance distribution covers the space; specialization to very hard narrow distributions favors methods that can generate many similar hard instances (SPDRL). Trade-off: breadth of variation vs focused difficulty determines which training strategy yields best final performance.",
            "uuid": "e1030.1"
        },
        {
            "name_short": "CartPole-DQN (SPACE)",
            "name_full": "CartPole agent trained with DQN under SPACE curriculum",
            "brief_description": "Classic CartPole balancing task with three pole-length variants; DQN receives pole length as context and SPACE builds curricula to focus learning on instances with high recent value-function improvement, yielding much higher episodic returns than round robin.",
            "citation_title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "CartPole (DQN)",
            "agent_description": "Simulated CartPole balancing agent (DQN) with pole length included as an extra state feature; curriculum generated by SPACE selecting instances with highest performance improvement capacity (PIC).",
            "agent_type": "simulated agent",
            "environment_name": "CartPole with variable pole length (short/medium/long variants)",
            "environment_description": "CartPole classic control environment but with three discrete pole lengths (short, medium, long) as distinct instances; complexity arises from different balancing dynamics across pole lengths, making transfer between instances nontrivial.",
            "complexity_measure": "Discrete set of 3 instance dynamics differing in pole length; state space small (classic CartPole) but dynamics differ per instance; measured via episodic return.",
            "complexity_level": "low",
            "variation_measure": "Low-number discrete variation: exactly 3 instances (short, medium, long pole lengths).",
            "variation_level": "low",
            "performance_metric": "mean episodic return per episode (reward per episode)",
            "performance_value": "SPACE achieved ~150 mean reward per episode across instances versus round robin ~25 mean reward per episode (on CartPole three-variant experiment).",
            "complexity_variation_relationship": "Even with low complexity environment, having multiple distinct dynamics (pole lengths) can dramatically impede naive training schedules (round robin) — SPACE's focused curricula (training on one instance for multiple consecutive episodes) greatly improve learning by exploiting shared underlying dynamics and progressive refinement.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "SPACE: ~150 mean reward/episode across three variants; Round Robin: ~25 mean reward/episode (demonstrates strong benefit of curriculum even in low-complexity tasks with discrete variation).",
            "training_strategy": "curriculum learning via SPACE vs round robin baseline",
            "generalization_tested": true,
            "generalization_results": "SPACE's curriculum that focuses on single instances for multiple episodes allowed the agent to perform considerably better across all three pole lengths, indicating effective transfer and generalization even with small discrete variations.",
            "sample_efficiency": "SPACE concentrated training on fewer instances at a time (multiple episodes per instance) leading to faster improvement; quantitative episode counts not exhaustively reported beyond performance comparison.",
            "key_findings": "Curriculum order matters even in simple control tasks: focused, self-paced training yields large improvements (here ~6× higher mean reward) over naive round robin ordering when instances differ in dynamics.",
            "uuid": "e1030.2"
        },
        {
            "name_short": "Maze-PPO2 (SPACE)",
            "name_full": "Gym-maze agent trained with PPO2 under SPACE curriculum",
            "brief_description": "Simulated agent in 5x5 maze instances where the flattened maze layout is provided as context; SPACE uses value-function changes to select instances despite the complexity of flattened structured context.",
            "citation_title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "Maze agent (PPO2)",
            "agent_description": "Simulated navigation agent trained with PPO2; context is the flattened 5x5 maze layout (25-dimensional binary/structured vector), and SPACE uses predicted state values to drive curricula.",
            "agent_type": "simulated agent",
            "environment_name": "gym-maze (5x5 mazes)",
            "environment_description": "Discrete grid mazes (5x5) with varying layouts; context is the flattened layout which is structurally complex (spatial relationships flattened into vector) and not trivially correlated with difficulty.",
            "complexity_measure": "Structural context complexity: 25-dimensional flattened maze layout; task complexity measured by episodes needed to reach goal in new mazes.",
            "complexity_level": "high (context representation is structurally complex relative to earlier environments)",
            "variation_measure": "Number of distinct maze instances: 100 training and 100 test instances sampled with a maze generator; variation arises from different obstacles/layouts.",
            "variation_level": "high (100 distinct maze layouts)",
            "performance_metric": "mean reward per episode and number of episodes required to solve mazes",
            "performance_value": "SPACE generalized from roughly 10 episodes to solve mazes, while round robin required several hundred episodes to solve all mazes (qualitative: orders-of-magnitude improvement in episodes-to-generalization).",
            "complexity_variation_relationship": "Although the context is complex and components do not directly correlate to difficulty, the agent's value function still provided sufficient signal for SPACE to distinguish instances; increased context complexity did not prevent SPACE from selecting appropriate curricula, but complexity may increase the number of instances needed for speedups.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "SPACE: able to generalize from ~10 episodes across 100 mazes; RR: requires several hundred episodes — shows SPACE handles high complexity and high variation effectively in this domain.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "curriculum learning via SPACE vs round robin",
            "generalization_tested": true,
            "generalization_results": "SPACE substantially outperformed round robin on held-out mazes, demonstrating that value-based instance selection works even when context is structurally complex (flattened layouts).",
            "sample_efficiency": "SPACE: generalization observed after ~10 episodes; RR: several hundred episodes required to solve all mazes.",
            "key_findings": "Value-function based curricula enable fast generalization even when contexts are high-dimensional and structured (flattened mazes); demonstrates robustness of SPACE to complex context representations.",
            "uuid": "e1030.3"
        },
        {
            "name_short": "BallCatching-PPO2 (SPACE)",
            "name_full": "BallCatching agent trained with PPO2 under SPACE curriculum",
            "brief_description": "Simulated BallCatching robot trained with PPO2 where context encodes ball distance and goal position; SPACE accelerates learning and generalization compared to round robin by selecting instances with high recent learning progress.",
            "citation_title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "BallCatching (PPO2)",
            "agent_description": "Simulated robot tasked to catch a ball, trained with PPO2; context includes ball distance and goal coordinates, and SPACE uses PIC (difference in starting-state value estimates) to form curricula.",
            "agent_type": "simulated agent",
            "environment_name": "BallCatching (contextual)",
            "environment_description": "Agent must move to catch a ball; context includes ball distance (range 0.125π to 0.5π) and goal x/y coordinates (x in [0.6,1.1], y in [0.75,4.0]); variation in launch geometry affects difficulty.",
            "complexity_measure": "Continuous control with varying initial conditions (ball distance) and target coordinates; instance complexity related to ball distance and goal placement affecting time-to-catch and required dynamics.",
            "complexity_level": "medium",
            "variation_measure": "Number of instances: 100 training and 100 test instances sampled uniformly over specified context bounds; variation measured by continuous distribution over distance and goal coordinates.",
            "variation_level": "medium-high (100 continuous-parameter instances)",
            "performance_metric": "mean reward per episode and learning speed (steps to final performance)",
            "performance_value": "SPACE reaches final performance substantially faster than round robin — reported as at least a 10× speedup to reach final performance in BallCatching (qualitative statement: 'factor of at least 10').",
            "complexity_variation_relationship": "Paper reports SPACE is particularly effective when instance sets are moderate in size and variation; for BallCatching SPACE yields large sample-efficiency gains versus RR, implying that moderate complexity with continuous variation benefits from self-paced instance ordering.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "curriculum learning via SPACE vs round robin",
            "generalization_tested": true,
            "generalization_results": "Agent evaluated on held-out test instances; SPACE learned considerably faster and generalized to test instances better than round robin, achieving comparable final performance with significantly fewer interactions.",
            "sample_efficiency": "Reported at least ~10× faster to reach final performance compared to round robin in BallCatching (qualitative; exact steps not enumerated).",
            "key_findings": "SPACE yields large sample-efficiency improvements in medium-complexity continuous control tasks with continuous context variation; benefits manifest as large speedups (up to an order of magnitude) to reach comparable performance.",
            "uuid": "e1030.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-paced deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "selfpaced_deep_reinforcement_learning"
        },
        {
            "paper_title": "Automatic curriculum learning through value disagreement",
            "rating": 2,
            "sanitized_title": "automatic_curriculum_learning_through_value_disagreement"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 1,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Reverse curriculum generation for reinforcement learning",
            "rating": 2,
            "sanitized_title": "reverse_curriculum_generation_for_reinforcement_learning"
        },
        {
            "paper_title": "POET: open-ended coevolution of environments and their optimized solutions",
            "rating": 1,
            "sanitized_title": "poet_openended_coevolution_of_environments_and_their_optimized_solutions"
        }
    ],
    "cost": 0.016482499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Paced Context Evaluation for Contextual Reinforcement Learning</p>
<p>Theresa Eimer 
André Biedenkapp 
Frank Hutter 
Marius Lindauer 
Self-Paced Context Evaluation for Contextual Reinforcement Learning</p>
<p>Reinforcement learning (RL) has made a lot of advances for solving a single problem in a given environment; but learning policies that generalize to unseen variations of a problem remains challenging. To improve sample efficiency for learning on such instances of a problem domain, we present Self-Paced Context Evaluation (SPACE). Based on self-paced learning, SPACE automatically generates instance curricula online with little computational overhead. To this end, SPACE leverages information contained in state values during training to accelerate and improve training performance as well as generalization capabilities to new instances from the same problem domain. Nevertheless, SPACE is independent of the problem domain at hand and can be applied on top of any RL agent with state-value function approximation. We demonstrate SPACE's ability to speed up learning of different value-based RL agents on two environments, showing better generalization capabilities and up to 10× faster learning compared to naive approaches such as round robin or SPDRL, as the closest state-of-the-art approach.</p>
<p>Introduction</p>
<p>Although Reinforcement Learning (RL) has performed impressively in settings like continuous control , robotics (OpenAI et al., 2019) and game playing Vinyals et al., 2019), their applicability is often very limited. RL training on a given task takes a lot of training samples, but the skills acquired do not necessarily transfer to similar tasks as they do for humans. An agent that is able to generalize across variations of a task, however, can be applied more flexibly and has a lower chance of succeeding when presented with unseen inputs. Therefore im- proving generalization means improving sample efficiency and robustness to unknown situations. We view these as important qualities for real-world RL applications.</p>
<p>Curriculum learning (Bengio et al., 2009) aims to bridge the gap between agent and human transfer capabilities by training an agent the same way a human would learn: transferring experience from easy to hard variations of the same task. It has been shown that generating such instances with increasing difficulty to form a training curriculum can improve training as well generalization performance (Dendorfer et al., 2020;Matiisen et al., 2017;Zhang et al., 2020). As information about instance difficulty is often not readily available, many approaches utilize the agent's progress markers, such as evaluation performance, confidence in its policy or its value function to minimize the need for domain knowledge Klink et al., 2020). Because the progression is dictated by the agent's learning progress, this is called Self-Paced Learning (Kumar et al., 2010).</p>
<p>Instances in a curriculum can vary from the core task in different aspects, such as varying goals or movement speeds (see Fig. 1). While only selecting different goals states as instances is common for curriculum learning methods (Dendorfer et al., 2020;Zhang et al., 2020), changing transition dynamics are important considerations regarding the robustness of a policy. A dynamic change in robotics could for example be caused by a broken joint that the agent now has to adapt to. To allow these changes in the transition dynamics, in addition to goal changes in the instances, we consider contextual RL instead.</p>
<p>Our contributions are as follows:</p>
<ol>
<li>We propose SPACE, a new self-paced learning algorithm, to automatically generate instance curricula in a arXiv:2106.05110v1 [cs.</li>
</ol>
<p>LG] 9 Jun 2021 general contextual RL setting, without any knowledge about instance difficulty being required and with access to only a limited set of instances (see Section 4).</p>
<ol>
<li>
<p>We show the convergence behavior of SPACE to be at least as good as round robin (see Section 4.2).</p>
</li>
<li>
<p>We demonstrate that SPACE is capable of outperforming a round robin baseline (Speck et al., 2020) as well as similar self-paced methods (see Section 5).</p>
</li>
</ol>
<p>Related Work</p>
<p>There are different approaches to increase generalization capability in RL. Their goals and scopes differ substantially, however. MAML (Finn et al., 2017) and related meta-RL methods pre-train an agent such that specializing on one of the training tasks is then very efficient. These take different approaches of aggregating and propagating the gradients in training and are complementary approaches to SPACE.</p>
<p>Domain randomization (DR; Tobin et al., 2017) on the other hand varies the task space. In essence, DR creates new instances of tasks in order to force the agent to adapt to alterations in its observations and policy. Other examples such as POET  and ADR (OpenAI et al., 2019) sample instances at random but order them by leveraging knowledge about the environment. Without prior knowledge of the target distribution, however, making appropriate changes is hard, resulting in either too little variation to facilitate generalization or deviating so much that the problem becomes too hard to learn. Other approaches utilize human expert knowledge to facilitate generalization performance, such as human-in-the-loop RL (Thomaz &amp; Breazeal, 2006) or imitation learning (Hussein et al., 2017).</p>
<p>Curriculum learning (Bengio et al., 2009) uses expert knowledge to generate an ordering of training instances in such a way that knowledge can be transferred from hard to easy instances. There are different approaches for automatically generating such instance curricula, including learning how to generate training instances (Dendorfer et al., 2020;Such et al., 2020) similar to a teacher (Matiisen et al., 2017;Turchetta et al., 2020) or leveraging self-play as a form of curriculum generation (Sukhbaatar et al., 2018;da Silva et al., 2019). In most of these cases, some knowledge of the instance space is required in order to either define a measure of instance difficulty or how to generate new instances. While instance generation requires only little prior knowledge, a separate agent will need to learn to generate instances of appropriate difficulty, which increases the training overhead significantly.</p>
<p>Value Disagreement based Sampling (VDS; Zhang et al., 2020) on the other hand builds curricula for goal-directed RL. VDS uses the disagreement between different agents trained on the same instances to measure which training instance should be trained on next. Like its building block HER (Andrychowicz et al., 2017), VDS is only compatible with goal-directed off-policy RL.</p>
<p>One approach to order the training instances is explicitly using an agent's performance as an ordering criterion instead, called Self-Paced Learning. This can be done using the agent's value function as a substitute for actual episode evaluations. SPDRL (Klink et al., 2020) uses this idea to generate new instances uniquely suited to the agent's current training progress in order to progress towards specific hard instances. While this eliminates the need for a teacher, researchers instead need to know a priori which instances are considered the hard target instances and where the agent should start training in relation to them.</p>
<p>Contextual Reinforcement Learning</p>
<p>Before we describe SPACE, we discuss how we can extend the typical RL formulation to allow for the notion of instances. RL problems are generally modeled as Markov Decision Processes (MDPs), i.e., a 4-tuple M := (S, A, T, R) consisting of a state space S, a set of actions A, a transition function T : S × A → S and a reward function R : S × A → R. This abstraction however, only allows to model a specific instantiation of a problem and does not allow to deviate from a single fixed instance.</p>
<p>An instance i ∈ I in a set of instances I could, e.g., determine a different goal position in a maze problem or different gravity conditions (i.e., moon instead of earth) for a navigation task. Information about the instance at hand is called its context c i . This context can either directly encode information about the instance, e.g., the true goal coordinates, or the kind of robot that should be controlled.</p>
<p>In order to make use of context in our problem description, we consider contextual MDPs (cMDP; Hallak et al., 2015;Modi et al., 2018;Biedenkapp et al., 2020). A contextual MDP M I is a collection of MDPs
M I := {M i } i∈I with M i := (S, A, T i , R i ).
As the underlying problem stays the same, we assume the possible state and action spaces are consistent across all instances; however, the transition and reward functions are unique to each instance. 1</p>
<p>An optimal policy π * for such a cMDP optimizes the expected return over all instances I with discount factor γ:
π * ∈ arg max π∈Π 1 |I| i∈I T t γ t R i (s t , π(s t ))(1)
As the reward depends on the given instance i, an agent solving a cMDP can leverage the context c i along with the current state s t ∈ S in order to differentiate between instances.</p>
<p>Self-Paced Context Evaluation</p>
<p>In order to generate a curriculum without any prior knowledge of the target domain, our Self-Paced Context Evaluation (SPACE) takes advantage of the information contained in an agent's state value predictions. By modelling V π (s t , c i ), the agent learns to predict the expected reward from state s t on instance i when following the current policy π. Therefore, we propose V π (s 0 , c i ) as an estimate of the total expected reward given a starting state s 0 . 2 Definition 1. The performance improvement capacity (PIC) of an instance is the difference in value estimation between point t and t − 1, that is:
d t (i) = V π t (s 0 , c i ) − V π t−1 (s 0 , c i ).(2)
The intuition is, if the instance evaluation changes by a large amount, the agent has learned a lot about this instance in the last iteration and can potentially learn even more on it.</p>
<p>Instances that are too easy or too hard will yield relatively small or no improvements. SPACE prefers instances on which it expects to make most learning progress. As most state-of-the-art RL algorithms use a value-based critic, each instance's PIC is easily computed during training.</p>
<p>Algorithm 1 summarizes the idea of SPACE. After some initialization in Lines 1-3, SPACE performs an update step for the current policy π and the value function V π based on roll-outs on the current instance set I curr . In principle, any RL algorithm with a value-function estimate can be used, such as Q-learning or policy search based on an actor-critic. In Lines 6-7, SPACE updates the average instance evaluation and the difference to the last iteration; note that this only considers the current set of instances I curr . In Lines 8-9, SPACE first checks whether the value function V π t changed ∆V π t by a factor η &lt; 1.0 compared to the value function before the update. If the update led to an insignificant change of the value function, SPACE assumes that the learning sufficiently converged and we can add κ new instances to I curr . Starting in Line 10, SPACE determines which instances in I should be included in I curr . For each instance, SPACE first computes how much the value function changed, d t (i). The instances with the highest PIC regarding V π are chosen as I curr (Lines 12-13), assuming that it is easy to make progress on these instances right now. Note, we evaluate the influence of the η and κ hyperparameters on the learning behaviour of SPACE in our experiments.</p>
<p>Algorithm 1: SPACE curriculum generation Data: policy π, value function V , Instance set I, threshold η, step size κ, #iterations T 1 S, t := 0 2 V 0 := 0 3 I curr := {i} with i randomly sampled from I 4 for t = 1...T do 5 π, V π t := update(π, V π t−1 , I curr ) 
6 V π t := 1 |Icurr| i∈Icurr |V π t (s 0 , c i )| 7 if V π t ∈ [(1 − η)V π t−1 , (1 + η)V π t−1 ] then // Increase set size 8 S := S + κ // Choose next instance set 9 forall i ∈ I do 10 d t (i) := V π t (s 0 , c i ) − V π t−1 (s 0 , c i ) 11 I curr := S instances with highest d t (i) 12 t := t + 1</p>
<p>Exemplary Application of SPACE</p>
<p>As a motivating example, we consider the CartPole environment (Brockman et al., 2016) with three different pole lengths, see Figure 2. We use a small DQN (hyperparameters given in Appendix B) for this example with the pole length being given as an additional state feature. Although CartPole is generally considered as easy to solve, using poles of different length causes the DQN using a round robin curriculum to be unable to improve over time (see Figure 3). SPACE on the other hand is able to generate curricula that allow the DQN to learn how the cart has to be moved for the different poles and thus improve considerably to a mean performance of around 150 per episode compared to round robin's 25.</p>
<p>In Figure 4 we can see the main difference between the two methods. While the round robin agent trains on all three different variations one episode each, SPACE only chooses to train on the cart with the long pole twice before episode 40. Instead, the focus is on a single instance at a time, using either the short or medium pole and changing not every episode but trains on an instance for at least three consecutive episodes. This shows that the value function can provide guidance as to instance similarity, as we would expect that the short and medium sticks behave in a similar way, as well as difficulty, the long pole being the hardest to  control of the three. While the changes in the value function may not provide a completely stable curriculum, training on one instance for a flexible amount of episodes instead of one episode already has a big impact on overall performance. Furthermore, comparing the curriculum to the performance curve, focusing on only one instance at a time already leads to the agent performing considerably better on all of them. This validates the idea that there are underlying dynamics common to all three pole lengths which are important to learn and then refine according to the instance dynamics.</p>
<p>Convergence of SPACE</p>
<p>To discuss under which conditions SPACE will converge, we consider two cases. Theorem 1. Given a set of instances I that are sufficiently distinguishable by their context c i as well as an instance of SPACE with η &gt; 0, κ ≥ 1 and an agent with value function V t . If the value function estimation V π t converges in the limit to some value function V on each instance (∀i ∈ I.∀s ∈ S : lim t→∞ V π t (s, c i ) = V (s, c i )) and globally (lim t→∞ V π t (s) = V (s)), SPACE will eventually include all instances in the curriculum.</p>
<p>Proof. Since for all i ∈ I:
lim t→∞ V π t (s 0 , c i ) = V (s 0 , c i )(3)
and therefore it follows that:
lim t→∞ ∆V π t−1 = ||V π t (s 0 , c i )| − |V π t (s 0 , c i )|| → 0 (4)
Thus SPACE is guaranteed to include at least one other instance i in the new curriculum I curr at some point t. Now we assume that we are given any I ⊆ I with size n &lt; |I|. As ∀i ∈ I.∀s ∈ S : lim t→∞ V π t (s, c i ) = V (s, c i ) and Equation 3, convergence of V π t on the subset I follows:
∀i ∈ I : lim t→∞ V π (s 0 , c i ) = V (s 0 , c i )(5)
Therefore, as in the single instance case:
lim t→∞ ∆V π t → 0(6)
and a new instance is added.</p>
<p>As the curriculum is guaranteed to be extended for any instance set of size n = 1 and n ≤ |I|, SPACE will eventually construct a curriculum using the whole instance set.</p>
<p>Corollary 1. If SPACE covers all instances at some time point, it will be only slower than round robin by a constant factor in the worst case.</p>
<p>Proof. Assume κ = 1 and that the learning agent requires O(K) steps to converge on a single instance.</p>
<p>If the agent is not able to transfer any of its gained knowledge between any of the tasks, SPACE will require to train an agent O(k) steps before growing the curriculum, where k ≤ K, depending on η. SPACE will thus require O(|I| · k) steps to include all |I| instances in the curriculum. At this point, SPACE behaves as a round robin schedule does, i.e., iterating over each instance while training the agent. Therefore, even if the construction of a meaningful curriculum should have failed, SPACE can recover by falling back to a round robin scheme after O(|I| · k) steps.</p>
<p>Corollary 2. If the value function estimation converges to the true value function V * , SPACE will also converge to the optimal policy.</p>
<p>Assume the worst case in which the value function estimate does not converge, but either oscillates or even diverges. This could happen if SPACE jumps between two disjoint instance sets I 1 and I 2 and the progress on I 1 is lost by switching to I 2 and vice versa. 3 Whenever we detect that learning is not progressing further and convergence is not achieved (i.e., ∆V π t = 0 and I curr = I), SPACE could simply increase η. As this hyperparameter controls how strict the convergence criterion is, increasing the value will allow for new instances to be added to the training set even though the original convergence criterion has not been met. The least value to which η should be set to guarantee an increase of instances is
∆V π t + V π t−1
for any &gt; 0 to eventually train on all instances.</p>
<p>Theorem 2. If the value function estimate is not guaranteed to converge (e.g., in deep reinforcement learning), SPACE can still recover a round robin scheme by increasing the threshold η if needed.</p>
<p>Proof. If at any point t, ∆V π t = 0 and I curr = I, we apply the method described above and set η =
∆V π t + V π t−1 . Then the condition to increase the instance set size is ∆V π t &lt; η · V π t−1 → ∆V π t &lt; ∆V π t + V π t−1 · V π t−1 → ∆V π t &lt; ∆V π t + .
Thus the instance set size is guaranteed to be increased. As this is true for any point in training, SPACE can still consider all instances at some point t * and thus perform as well as round robin from t * onward.</p>
<p>Experiments</p>
<p>In this section we empirically evaluate SPACE on two different environments. The code for all experiments is available at https://github.com/automl/SPaCE. We first describe the experimental setup before comparing SPACE against a round robin (RR) training scheme and SPDRL (Klink et al., 2020) as a state-of-the-art self-paced RL baseline. Finally we evaluate the influence of SPACE's own hyperparameters and limitations.</p>
<p>Setup</p>
<p>We evaluated SPACE in settings that readily allow for context information to encode different instances, namely the Ant locomotion environment (Coumans &amp; Bai, 2020), the gym-maze environment (Chan, 2019) and the BallCatching and contextual PointMass environments as used by Klink et al. (2020).</p>
<p>The task in Ant is to control a four legged ant robot towards a goal on a flat 2D surface as quick as possible. The context is given by the x-and y-coordinates of the goal. Goals that are close to the starting position are easier to reach and thus we expect them to be easier to learn and their policies to transfer to more difficult instances. Additionally, the context indicates if no or up to one of the four legs of the ant robot is immobilized, similar to (Seo et al., 2020). We uniformly sampled 200 instances which we split in equal sized, disjoint training and test sets (see Appendix A). The context of the maze environment (Chan, 2019), in which the task is to find the goal state, is given as the flattened 5x5 layout of the current instance. 100 training and test instances each were sampled using the given maze generator. The agent's goal in BallCatching is to direct a robot to catch a ball. The ball's distance from the robot as well as it goal position are given as context information. Training and test sets were each 100 instances large and uniformly sampled between our context bounds. In the PointMass environment (see Figure 1), an agent maneuvers a point mass through a goal in a two-dimensional space. The goal position, the width as well as the friction coefficient of the ground are given as context. We sampled 100 instances for training and testing, each for two different distributions. The first distribution is chosen to cover the space of possible instances, whereas the second distribution follows that of Klink et al. (2020) and focuses on an area around a particularly difficult instance (see Appendix A).</p>
<p>To be consistent and fair with respect to prior work, we trained a PPO agent  for Ant and a TRPO agent for PointMass (Schulman et al., 2015) and base our curriculum generation on their value-based actors. For easier readability, all plots are smoothed over 10 steps. In order to monitor generalization progress over time, we evaluated the agent on all instances in the training and test set after each complete run through the training set. As the results on training and test sets were very similar, we only report the test performance. In all experiments we evaluated our agents over 10 random seeds. For hardware specifications and hyperparameters, please see Appendix B.</p>
<p>Baselines</p>
<p>In our experiments, we use three different baselines to compare SPACE's performance to.</p>
<p>Round Robin (RR) To be sure SPACE outperforms instances without an intentional ordering, we compare against round robin as a common default instance ordering. This means that the training instances are ordered in an arbitrary way and we simply iterate over them, playing one episode per instance. As the instance sets we use are generated randomly, this ordering is chosen at random as well.</p>
<p>SPDRL SPDRL (Klink et al., 2020) is a state-of-the-art self-paced learning method for contextual RL. This is notable as most curriculum learning methods are explicitly designed for goal-directed RL, which makes them unsuitable in our setting. Counter to SPACE and RR, SPDRL makes use of an instance distribution to continually sample new instances of a specific difficulty level. SPDRL uses this ability to generate new instances to focus on particularly difficult instances, while largely ignoring the remaining instance space. To this end, SPDRL requires additional domain knowledge, besides the context information, to determine which instances SPDRL should focus on. Therefore we provide SPDRL with the distribution of our training and test set to focus the learning on its center.</p>
<p>cSPACE With SPACE we opted for taking an agent's knowledge about the expected reward, i.e. value function, into account to determine the similarity and difficulty of instances. However, as instances can be represented by their context, their similarity could also be quantified directly through their similarity in the context space. This form of similarity quantification is common in fields making use of techniques such as algorithm selection (Rice, 1976) and meta-learning (Brazdil et al., 2008). Such curricula order instances according to their context similarities. A successful application of this approach can be seen in Reverse Curriculum Generation for Reinforcement Learning (Florensa et al., 2017) where robot arm starting positions were ordered into a curriculum according to their similarity. In other words, instead of using an agent's performance evaluations as a basis for the curriculum generation, instances with contexts that are closest to the current curriculum context are added. SPACE's instance ordering criterion can easily be changed to compare context space distance instead of evaluations, yielding a variation we call context SPACE (cSPACE). More precisely, we replaced d as our instance selection criterion (see Algorithm 1 Line 10) with the Euclidean distance to the current instance set I curr . In such cases, cSPACE can suffer from the same problems as unsupervised learning. A priori it is not clear how to scale and weight the different context features without having any signal how the features will affect the difficulty of instances and how good the resulting curriculum will be. In contrast to SPACE, we deem this a potential challenge in applying cSPACE. For this reason, we recommend SPACE as the default approach whenever state evaluations are available.</p>
<p>Does the Instance Order Matter?</p>
<p>We first compare SPACE to a baseline round robin (RR) agent on the Ant and BallCatching environments, to determine if SPACE can find a curriculum that outperforms a random ordering. In Figure 5, both agents reach the same final performance in each environment, but the agent trained via SPACE learns considerably faster. It only requires 10 3 steps to reach a reward of around 11 in Ant whereas RR requires roughly 10× as many steps to train an agent to reach the same reward. The results for BallCatching are similar, with SPACE again being faster to reach the final performance a factor of at least 10. We further compare both methods on the PointMass environment when training on an instance set that was uniformly sampled from the space of possible instances (see Figure 6).</p>
<p>Here, the agent trained with SPACE is only roughly twice as fast, but it substantially outperforms round robin in terms of final performance. As the RR baseline does not care about the order in which instances are presented to the agent, we conclude that a more structured learning approach is needed. From SPACE's performance we can conclude that a curriculum, learned in a self-paced fashion can help improve both training performance and generalization. The experiments in the following sections further confirm this finding.</p>
<p>Comparing SPACE and SPDRL</p>
<p>We further compared SPACE to SPDRL (Klink et al., 2020) on the PointMass environment in order to demonstrate the difference between SPACE and an other self-paced learning method. We used the same implementation and hyperparameters as in (Klink et al., 2020) for SPDRL. The test performance of the agents can be seen in Figure 6.</p>
<p>As SPDRL was developed to train an agent to solve specific hard instances in the PointMass environment, it clearly falls short when it comes to covering the whole instance space. The agent trained via SPDRL learns much slower and achieves a worse final performance than an agent trained via SPaCE Figure 7: SPACE and RR of a set of mazes over 10 runs (± standard deviation) RR. Perhaps unsurprisingly, this shows that targeting learning on hard instances does not imply the same agent can achieve good generalization performance on all instances.</p>
<p>How Well does SPACE Handle Complex Contexts?</p>
<p>While our benchmarks above are common meta-RL problem settings with different complexities, their contexts are given in a rather simple form, i.e., a short context vector directly describes the goal and environment dynamics. The agent can therefore make a direct connection between the changes between instances and the different context descriptions. This may not always be the case with context possibly being given within the observation, e.g., as part of an image.</p>
<p>In order to confirm that such a context description still enables SPACE to select the appropriate next instance, we use a set of 100 5x5 mazes (Chan, 2019) for our agent to generalize over. The observation is the agent's current position while the context is given by the flattened maze layout.</p>
<p>This context is much more complex than the previously used ones by having a structure that has been flattened and its components do not directly correlate to an increase in difficulty. Furthermore, many of the components of the context may not change from instance to instance even though the layout, and therefore the required policy, will. Figure 7 shows that while the context information for this task is much more complex than previously seen, SPACE still outperforms the round robin agent in a similar way than it does to for Ant and BallCatching. The round robin agent needs several hundred episodes to solve all mazes while SPACE is able to generalize from just 10 episodes. While the context complexity increases in this case, the value function is still able to differentiate between them enough to allow a distinction between different instances. Therefore we expect that the representation of the context is not a major concern for the performance of SPACE. </p>
<p>Can SPACE Be Applied Without a Value</p>
<p>Function?</p>
<p>The PointMass environment has three different context features for which we can easily use the context space distance to construct a curriculum. SPACE and cSPACE perform similarly on PointMass (Figure 8) in terms of learning speed and overall performance, both reaching the same performance at the same speed, with SPACE learning faster on average between 10 4 and 10 5 . This makes both SPACE variations a better choice than round robin.</p>
<p>Both cSPACE and SPACE are also consistent in the curricula they find. We measured this by comparing the frequency with which each instance was used in the training set compared to the weighted frequency which gives higher rank to instances chosen at earlier iterations (1 for the instance chosen first down to 1 |I| for the instance chosen last). For cSPACE, both the frequency and weighted frequency stayed the same while for SPACE only the four least used instances differed in order between the two.</p>
<p>We also compared the mean instance distance between curriculum iterations to see which method allows for smoother transitions between tasks. Smooth transitions correlate to a handcrafted curriculum where instances are close together in the context space, making the curriculum easier to learn from a human perspective. SPACE moves the instance set around 4.7% each curriculum iteration while cSPACE moves by around 5.6%. The maximum induced change is 10.1% for SPACE and 13.3% for cSPACE approach.</p>
<p>As we can see from these comparisons, using the information contained in an agent's value function to construct a curriculum is very similar to using the context space distance. It needs to be said, however, that in PointMass the context reflects the environment dynamics in a very direct way, being made up of the x-and y-positions of the goal to reach and the friction coefficient. Therefore we would expect cSPACE to perform very well on such environments. The fact that the default SPACE setting performs similarly indicates that the value function contains the information necessary to 1 5.1 ± 0.7 4.8 ± 1.2 4.7 ± 1.2 5.2 ± 0.7 4 5.5 ± 0.5 5.2 ± 0.7 4.3 ± 1.2 4.4 ± 1.0 16 4.6 ± 1.1 3.7 ± 1.1 5.1 ± 1.2 4.8 ± 1.0 32 4.5 ± 1.1 4.6 ± 1.1 4.7 ± 1.3 5.0 ± 1.2 order instances into a curriculum of similar quality. As not all environments may have such simple changes between instances, we expect that cSPACE has limitations on those kinds of environments while we can expect the value-based SPACE variation to continue constructing high quality curricula even in that case.</p>
<p>How Robust is SPACE wrt its Hyperparameters?</p>
<p>SPACE comes with two hyperparameters, the performance threshold for curriculum interactions η and the instance increment κ. These hyperparameters interact with each other to make SPACE comparatively stable across different hyperparameter values (as seen in Figure 1).</p>
<p>By varying η for a given value of κ, we alter the degree of stability the agent's value estimates have to reach between training episodes. Depending on the problem at hand, the value estimates may never be perfectly stable, therefore a very low value for η may prevent the training set from expanding. On the other hand, a very large value will move SPACE closer to round robin. Thus we view η as the more important hyperparameter of the two.</p>
<p>Our study shows very little performance differences for different values of κ and η. In part, this is because PointMass instances are not too difficult in the mean, therefore adding many at once does not heavily disturb learning. Larger performance thresholds η are not an issue for this reason. A value of 5% for η seems quite low, but as the instances are relatively easy, the agent can still converge enough very quickly. Different values for κ show similar results here. We expect this hyperparameter to be more important in very diverse settings with large gaps between instances. We can see the effect if we multiply the size of our training set instead of adding instances (see Figure 9). In this case, there is a visible slowdown, supporting that κ has a big influence on training performance.</p>
<p>From these results, we believe that it is reasonable to recommend keeping κ = 1 for most applications. It can yield more fine-grained curricula which will be important on diverse instance sets and will likely only impact training on very large instance sets. For η, using a low value such as 5% Figure 9: Mean reward per episode on a test set with fast rising instance set size (i.e. varying κ) and fixed η = 10%. should ensure that the agent will not be overwhelmed with new instances if it takes more than one curriculum iteration to learn from the current training set.</p>
<p>Can Goal-Directed RL Achieve Similar Results?</p>
<p>Goal-directed RL and contextual RL are closely related flavours of RL. The main difference between the two is that in goal-directed RL, the context is restricted to only contain information about a desired goal-location. Thereby the context indicates a (final) state in which the agent should reach. Policies learning with goals as context information thus can learn the value of executing an action in a certain state for reaching the desired goal. Crucially however, in this setting transition dynamics are assumed to never change for different contexts.</p>
<p>Contextual RL subsumes goal-directed RL by also allowing the environment dynamics for the same goal to vary. Both environments we evaluate on exemplify this. In contextual PointMass, we specify both a goal and the friction coefficient to describe an instance. Even if the goal is kept the same, however, the friction is a deciding factor in the amount of force that is necessary to reach the goal correctly. So while the force direction is the same for instances with the same goal, the required amount of force depends on the friction and therefore the agent needs both information to learn to generalize across different instances.</p>
<p>On AntGoal, we can see how this looks in practice. VDS (Zhang et al., 2020) is a recent state-of-the-art method for goal-directed curriculum construction based on HER (Andrychowicz et al., 2017). As a result, it can take only the ant's goal into consideration when selecting the next instance and crucially misses that in different instances the ant has different defects in some of its joints. As a result, the method conflates all instances with the same goals and fails to actually learn how to act on any of them.</p>
<p>We used the implementation and baseline of Zhang et al. (2020) to demonstrate that while goal-directed curriculum generation approaches seem similar to SPaCE, our problem setting is out of scope for them (see Figure 10). Their RR baseline has a different learning curve as ours as the algorithm used is different, but it clearly is able to improve over time. As VDS uses goals to describe the necessary behaviour, it cannot do the same. Therefore, curriculum learning methods for contextual RL and goal-directed RL have different scopes and cannot be compared fairly in the contextual RL setting.</p>
<p>Limitations</p>
<p>Even though SPACE performed very well on the benchmarks used in this paper, there are several limitations of SPACE to be considered. The first one is that the problem and the instance set both need to support curriculum learning to some degree. For the problem itself this means that the policy to solve it is influenced by context to a large degree, but that there is an underlying structure that can be exploited using a curriculum. The instance set then needs to be large enough to actually give SPACE the opportunity to do so. In settings with too little or very large amounts of instances, SPACE becomes less efficient (see Appendix D).</p>
<p>Furthermore, if the instance set is very homogeneous, similar to the specific instance SPDRL uses on PointMass (see Appendix C), using different instances for training might not make a difference. Conversely, if the instance set is heterogeneous, preliminary experiments showed that SPACE requires a larger amount of instances to speed up the learning.</p>
<p>Thus not every problem is suited for curriculum learning.</p>
<p>Lastly, SPACE is constructed to work for discrete instance spaces only, where the instance ordering is essential for learning efficiency. We stress the fact that SPACE is designed for use cases with only a few instance examples. In settings with instance generators or a lot of domain knowledge available, it is likely better to exploit them which SPACE is not designed for.</p>
<p>Conclusion</p>
<p>Self-Paced Context Evaluation (SPACE) provides an adaptive curriculum learning method for problem settings constrained to a fixed set of training instances. Thereby we facilitate generalization in practical applications of RL. We demonstrated that the order of instances on which agents learn their behaviour policies indeed is important and can produce a better learning efficiency. In addition, SPACE outperformed a simple round robin baseline as well as more specialized curriculum learning methods requiring access to unlimited instance generators to perform well. Finally we evaluated the influence of SPACE's own hyperparameters and showed that they are robust on the chosen environments.</p>
<p>Future research could address how to derive performance expectations for practical applications of RL with a limited amount of instances with respect to the amount of information available. Furthermore, we might be able to use value estimation to further improve training efficiency for example by clustering instances of similar difficulty and limiting the amount of training on very easy ones to a minimum. Another important factor for contextual RL in general is catastrophic forgetting (see Appendix F), which is not yet sufficiently understood, especially in the continuous context spaces we applied SPACE to.</p>
<p>BallCatching The distance and goal coordinates were sampled uniformly for both training and test set. The distance ranged between 0.125 · π and 0.5 · π, the x-coordinate between 0.6 and 1.1 and the y-coordinate between 0.75 and 4.0. Each instance set contains 100 instances.</p>
<p>PointMass For PointMass, we sampled two different instance sets. First, we used the context bounds of [-4, 4] for the goal position, [0.5, 8] for the goal width and [0,4] for friction to uniformly sample instances. The goal was to cover the instance space as well as possible. Our second instance set was sampled using the target distribution of SPDRL, which are normal distributions for each context component with means 2.5, 0.5 and 0 respectively as well as standard deviations of 0.004, 0.00375, and 0.002.</p>
<p>B. Experiment Hardware &amp; Hyperparameters</p>
<p>Hardware All experiments with SPACE and the baseline round robin agent were conducted on a slurm CPU cluster (see Table 2  CartPole We used a DQN implementation in the top-10 on the environment leaderboard to ensure fair performance for round robin and SPACE agents (Chauhan, 2019). We did not change any hyperparameters from that implementation and used κ = 1 and η = 2.5% for all experiments.</p>
<p>Other benchmarks For both experiments we used stable baselines version 2.9.0 (Hill et al., 2018)  C. Additional Comparison to SPDRL Figure 11: Mean reward per episode on a test set of hard instances with small goals and low friction.</p>
<p>In contrast to SPACE, SPDRL is designed to solve hard instances. To this end, it samples harder and harder instances over time. Therefore, we additionally study how SPACE, round robin (RR) and SPDRL compare on hard instances sampled from the SPDRL target distribution, see Figure 11. Instances in this distribution typically have small goal sizes and low friction, both of which contribute significantly to an increased difficulty.</p>
<p>As in the original paper, SPDRL was allowed to sample as many instances as needed from the distribution, whereas SPACE and RR still only got access to a finite set of 100 instances. In this setting, agents trained either via SPACE or RR exhibit a similar learning behaviour as on the space covering instance set. For the first ∼ 200 000 steps both agents outperform the agent trained via SPDRL; RR anyway focuses on the whole target distribution from the beginnig and SPACE is more free in the way it can select instances with fast training progress. During this time, SPDRL trains the agents on some easy instances, while gradually adapting the instance distribution to focus on ever more difficult tasks.</p>
<p>Note that the level of difficulty is not determined solely by the agent being trained via SPDRL, as done in SPACE, but is determined by an expert beforehand.</p>
<p>Once the agent trained via SPDRL is capable of homing in on the difficult instances it outperforms the other agents, as it can exploit its domain knowledge to sample ever more similarly difficult instances, while SPACE and RR are stuck with the limited number of example instances and still try to cover the entire instance space. To achieve this feat, SPDRL requires substantial expert knowledge about which instances to focus on. In essence, the agent trained via SPDRL in the end is only capable of solving a few hard instances with very little variation and will fail to perform well on instances that are not narrowly aligned with the assumed instance distribution.</p>
<p>To be able to know which instances SPDRL should focus on, additional time and effort have to be spent to identify how to quantify difficulty for SPDRL. This effort is not reflected in Figure 11 and would move the curve of SPDRL even further to the right.</p>
<p>D. Does the Training Set Size Matter?</p>
<p>To answer this question, we used SPACE to train agents with varying instance set sizes. Figure 12 shows the test performance for differently sized instance sets. Intuitively, one might think that performance should improve with more instances as they cover the instance space better. Indeed, the results for training sets with only 25 and 50 instances are visibly worse than for larger sets. On the remaining instance sets, the agent show very similar performance, however. Note that the performance seems to increase from an instance set size of 100 to 200, but slightly drops again afterwards. There are multiple factors potentially contributing to this effect. Figure 12: Mean reward per episode on test set for different sized instance sets.</p>
<p>The first is that the agent cannot incorporate any more information from the additional instances, maybe due to limited network capacity or due to the fact that smaller instance sets already cover the space adequately. Furthermore, as we only extend the instance set by one instance at a time, there SPaCE are more learning steps between curriculum iterations the larger the instance set is, thereby slowing the process down. Especially an agent trained on 1 600 instances will suffer from this.</p>
<p>Lastly, SPACE improves upon the RR baseline by ordering training instances and thus smoothing the progression through the instance space. Larger instance sets offer an inherently smoother representation of the instance distribution, therefore diminishing the effect of SPACE. In real-world application settings, we will rarely have access to such large numbers of instances and therefore, it is unlikely that such diminishing performance effects can be observed. This shows that the strength of our method comes to full effect when learning on a sparse representation of our instance space.</p>
<p>E. Comparison of SPACE Curricula</p>
<p>To give some insight into which curricula SPACE found on our benchmark environments, we compare how they behave across random seeds and how they compare to cSPACE curricula. We use Kendall's tau to determine how similar the order in which the instances are added to the training set is.</p>
<p>On PointMass, SPACE finds a curriculum that stay very consistent across all random seeds, showing a correlation of at least 98.9% each to the mean curriculum. The same is true for the cSPACE variation, where the correlation is above 93.8% per seed. Interestingly, these curricula are uncorrelated with a correlation of −0.04. In both we cannot make out a human readable progression in a single context feature (see Figure 13), their curricula do not correspond to any manual instance ordering. As both perform well nonetheless, we can see that learning can be improved by multiple different curricula on this environment. SPACE and cSPACE produce almost equally unrelated curricula on AntGoal (correlation of 0.07), but while the curriculum stays as consistent across seeds for cSPACE, the same cannot be said for SPACE. Here the correlation to the average curriculum ranges from 14.1% to 52.4%. The correlations between the seed curricula fall into the same range, confirming that the SPACE agent trains on a very different curriculum for each seed. CartPole shows a similar behaviour, the curriculum varying quite a bit between seeds. Therefore we can conclude that SPACE does not find a singular curriculum, but depends on the initialization of environment and model. This is in contrast to cSPACE which stays relatively static due to the context features being constant.</p>
<p>These comparisons suggest that we neither SPACE nor cSPACE finds an optimal curriculum for PointMass, AntGoal or CartPole. It seems, however, that we do not need an optimal curriculum for training at all, as even the 10 very different curricula SPACE finds on AntGoal perform vastly superior to the round robin default. Curriculum Learning should thus focus on reliably and quickly finding good curricula in addition to finding qualitatively better ones.</p>
<p>F. The Influence of Catastrophic Forgetting</p>
<p>When training across multiple instances, forgetting already learnt policies on a subset of instances is a concern (Beaulieu et al., 2020). We analyze how often SPACE and RR agents forget policy components in our PointMass experiments by observing performance development during training. We selected PointMass for this analysis as here policies that are diverse both in how they react to different goal settings and different friction levels are required. That means the policy has to completely change between the extremes of the context which is not required of our other benchmarks where underlying mechanics, e.g. walking for the Ant, stay very similar.</p>
<p>During the training on PointMass, we observed 8 out of 100 instances for which the performance decays after an initial improvement. We would expect the performance to stay at least constant if no forgetting takes place, so the agent likely forgets parts of the policy for these instances in favor of improving on others. The effect is about the same size for round robin agents where we can observe the same for 6 out of 100 instances.</p>
<p>Figure 1 :
1Example instances of the contextual PointMass environment. The agent's yellow starting point, the green goal and floor friction (indicated by shading) are part of the context and vary between instances.</p>
<p>Figure 2 :
2CartPole instances: short (s), medium (m) and long (l) balancing pole.</p>
<p>Figure 3 :
3Performance (± std.) comparison of SPACE and default instance ordering on CartPole over 5 seeds each.</p>
<p>Figure 4 :
4One exemplary run of SPACE (top) and round robin (bottom) curricula on CartPole.</p>
<p>Figure 5 :
5Mean reward (± standard deviation) per episode over 10 runs on Ant (left) and BallCatching (right).</p>
<p>Figure 6 :
6Mean reward per episode on a test set of uniformly sampled instances for PointMass.</p>
<p>Figure 8 :
8Mean reward on contextual PointMass with additional cSPACE results.</p>
<p>Figure 10 :
10Total undiscounted reward of VDS and its RR baseline on AntGoal.</p>
<p>Figure 13 :
13Context feature progression during training for SPACE curriculum (top) and cSPACE curriculum (bottom).</p>
<p>Table 1 :
1Mean reward ± standard deviation for different hyperparameter values on PointMass after 10 6 steps.η </p>
<p>κ 
5% 
10% 
20% 
40% </p>
<p>Table 2 :
2CPU cluster used for training</p>
<p>and an entropy coefficient of 0. For TRPO we used again used the same hyperparameters as SPDRL with a GAE hyperparameter λ of 0.99, a maximum KL-Divergence of 0.004 and value function step size of around 0.24. Any hyperparameters not mentioned were left at the stable baselines' default values. The random seeds were used to seed the environments with the corresponding seeding method.with TRPO for 
PointMass and PPO2 for all other benchmarks. The policies 
are encoded by an MLP in both cases, with two layers of 
64 units for PPO. For PointMass, we used the default from 
the SDPRL paper with 21 layers of 64 units each. The dis-
count factor was 0.95. The PPO2 specfic hyperparameters 
included no gradient clipping, a GAE hyperparameter λ 
value of 0.99 
Information Processing Institute (tnt), Leibniz University Hannover, Germany 2 Department of Computer Science, University of Freiburg, Germany 3 Bosch Center for Artificial Intelligence, Renningen, Germany. Correspondence to: Theresa Eimer <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#101;&#105;&#109;&#101;&#114;&#64;&#116;&#110;&#116;&#46;&#117;&#110;&#105;&#45;&#104;&#97;&#110;&#110;&#111;&#118;&#101;&#114;&#46;&#100;&#101;">&#101;&#105;&#109;&#101;&#114;&#64;&#116;&#110;&#116;&#46;&#117;&#110;&#105;&#45;&#104;&#97;&#110;&#110;&#111;&#118;&#101;&#114;&#46;&#100;&#101;</a>.
In goal-directed RL, instances can also only vary the reward function and keep dynamics constant.
For simplicity's sake, we assume that an environment has a single starting state s0 and we do not integrate over all possible starting states.
Note: Though theoretically possible, we have never observed this problem in practice.
AcknowledgementsTheresa Eimer and Marius Lindauer acknowledge funding by the German Research Foundation (DFG) under LI 2801/4-1. All authors acknowledge funding by the Robert Bosch GmbH.Another reason for attributing this performance decay to forgetting is that on a purely goal-based PointMass variation, the number of instances on which we can observe this effect is slightly smaller (only 4 instances), though not significantly so. All performance decay happens after learning has stagnated on all instances, however. In this easier, purely goal-based setting we could therefore stop training early and would avoid performance decay entirely. This points towards the added complexity of the setting being harder to capture for our agents.While the effects on both SPACE and RR agents are not very large in our experiments, catastrophic forgetting is therefore certainly important in the field of contextual RL. Future work could on integrate SPACE with existing efforts to reduce this effect like ANML(Beaulieu et al., 2020). A specific aspect of this research that would need to be extended is preventing forgetting in continuous context spaces in addition to the existing successes in discrete ones.
Hindsight experience replay. M Andrychowicz, D Crow, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W ; Zaremba, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, Garnett , Proceedings of the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS'17). R.the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS'17)Guyon, IAndrychowicz, M., Crow, D., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. Hindsight experience replay. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Proceedings of the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS'17), pp. 5048- 5058, 2017.</p>
<p>Learning to continually learn. S Beaulieu, L Frati, T Miconi, J Lehman, K O Stanley, J Clune, N Cheney, ECAI 2020 -24th European Conference on Artificial Intelligence. Beaulieu, S., Frati, L., Miconi, T., Lehman, J., Stanley, K. O., Clune, J., and Cheney, N. Learning to continually learn. In ECAI 2020 -24th European Conference on Artificial Intelligence, 2020.</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, Weston , J , Proceedings of the 26th International Conference on Machine Learning (ICML'09). Bottou, L. and Littman, M.the 26th International Conference on Machine Learning (ICML'09)OmnipressBengio, Y., Louradour, J., Collobert, R., and Weston, J. Cur- riculum learning. In Bottou, L. and Littman, M. (eds.), Proceedings of the 26th International Conference on Ma- chine Learning (ICML'09), pp. 41-48. Omnipress, 2009.</p>
<p>Dynamic Algorithm Configuration: Foundation of a New Meta-Algorithmic Framework. A Biedenkapp, H F Bozkurt, T Eimer, F Hutter, M Lindauer, Proceedings of the Twenty-fourth European Conference on Artificial Intelligence (ECAI'20). Lang, J., Giacomo, G. D., Dilkina, B., and Milano, M.the Twenty-fourth European Conference on Artificial Intelligence (ECAI'20)Biedenkapp, A., Bozkurt, H. F., Eimer, T., Hutter, F., and SPaCE Lindauer, M. Dynamic Algorithm Configuration: Foun- dation of a New Meta-Algorithmic Framework. In Lang, J., Giacomo, G. D., Dilkina, B., and Milano, M. (eds.), Proceedings of the Twenty-fourth European Conference on Artificial Intelligence (ECAI'20), pp. 427-434, June 2020.</p>
<p>Metalearning: Applications to Data Mining. P Brazdil, C Giraud-Carrier, C Soares, R Vilalta, Springer Publishing CompanyIncorporated, 1 editionBrazdil, P., Giraud-Carrier, C., Soares, C., and Vilalta, R. Metalearning: Applications to Data Mining. Springer Publishing Company, Incorporated, 1 edition, 2008.</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, Openai, Gym, Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym, 2016.</p>
<p>. M Chan, Chan, M. gym-maze. https://github.com/</p>
<p>MattChanTK/gym-maze. MattChanTK/gym-maze, 2019.</p>
<p>. K Chauhan, Cartpole Dqn, Chauhan, K. Cartpole dqn. https://github.com/ kapilnchauhan77/CartPole_DQN, 2019.</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, Coumans, E. and Bai, Y. Pybullet, a python module for physics simulation for games, robotics and machine learn- ing. http://pybullet.org, 2020.</p>
<p>Building self-play curricula online by playing with expert agents in adversarial games. F L Da Silva, A H R Costa, P Stone, 8th Brazilian Conference on Intelligent Systems, BRACIS '19. da Silva, F. L., Costa, A. H. R., and Stone, P. Building self-play curricula online by playing with expert agents in adversarial games. In 8th Brazilian Conference on Intelligent Systems, BRACIS '19, pp. 479-484, 2019.</p>
<p>Multimodal trajectory prediction based on goal position estimation. P Dendorfer, A Osep, L Leal-Taixé, Goal-Gan, Proceedings of the 15th Asian Conference on Computer Vision (ACCV'20). the 15th Asian Conference on Computer Vision (ACCV'20)2020Dendorfer, P., Osep, A., and Leal-Taixé, L. Goal-GAN: Multimodal trajectory prediction based on goal position estimation. In Proceedings of the 15th Asian Conference on Computer Vision (ACCV'20), 2020.</p>
<p>Model-agnostic metalearning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, Proceedings of the 34th International Conference on Machine Learning (ICML '17). the 34th International Conference on Machine Learning (ICML '17)70Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning (ICML '17), volume 70, pp. 1126-1135, 2017.</p>
<p>Reverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, M Zhang, Abbeel , P , Proceedings of the 1st Conference on Robot Learning (CoRL'17). the 1st Conference on Robot Learning (CoRL'17)78Florensa, C., Held, D., Wulfmeier, M., Zhang, M., and Abbeel, P. Reverse curriculum generation for reinforce- ment learning. In Proceedings of the 1st Conference on Robot Learning (CoRL'17), volume 78, pp. 482-495, 2017.</p>
<p>A Hallak, D D Castro, S Mannor, arXiv:1502.02259Contextual markov decision processes. stat.MLHallak, A., Castro, D. D., and Mannor, S. Contextual markov decision processes. arXiv:1502.02259 [stat.ML], 2015.</p>
<p>. A Hill, A Raffin, M Ernestus, A Gleave, A Kanervisto, R Traore, P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, Stable, Baselines, Hill, A., Raffin, A., Ernestus, M., Gleave, A., Kanervisto, A., Traore, R., Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor, S., and Wu, Y. Stable baselines. https://github.com/ hill-a/stable-baselines, 2018.</p>
<p>Imitation learning: A survey of learning methods. A Hussein, M M Gaber, E Elyan, Jayne , C , 21:1-21:35ACM Comput. Surv. 502Hussein, A., Gaber, M. M., Elyan, E., and Jayne, C. Im- itation learning: A survey of learning methods. ACM Comput. Surv., 50(2):21:1-21:35, 2017.</p>
<p>Selfpaced deep reinforcement learning. P Klink, C D&apos;eramo, J Peters, J Pajarinen, H Larochelle, M Ranzato, R Hadsell, M Balcan, Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS'20). Lin, H.the 33rd Conference on Neural Information Processing Systems (NeurIPS'20)2020Klink, P., D'Eramo, C., Peters, J., and Pajarinen, J. Self- paced deep reinforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Proceedings of the 33rd Conference on Neural Informa- tion Processing Systems (NeurIPS'20), 2020.</p>
<p>Self-paced learning for latent variable models. M P Kumar, B Packer, D Koller, Proceedings of the 24th International Conference on Advances in Neural Information Processing Systems (NeurIPS'10). Lafferty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and Culotta, A.the 24th International Conference on Advances in Neural Information Processing Systems (NeurIPS'10)Kumar, M. P., Packer, B., and Koller, D. Self-paced learn- ing for latent variable models. In Lafferty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and Culotta, A. (eds.), Proceedings of the 24th International Conference on Advances in Neural Information Processing Systems (NeurIPS'10), pp. 1189-1197, 2010.</p>
<p>Continuous control with deep reinforcement learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, Proceedings of the International Conference on Learning Representations (ICLR'16). the International Conference on Learning Representations (ICLR'16)Published online: iclr.ccLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR'16), 2016. Published online: iclr.cc.</p>
<p>Teacher-student curriculum learning. T Matiisen, A Oliver, T Cohen, J Schulman, abs/1707.00183CoRRMatiisen, T., Oliver, A., Cohen, T., and Schulman, J. Teacher-student curriculum learning. CoRR, abs/1707.00183, 2017.</p>
<p>Markov decision processes with continuous side information. A Modi, N Jiang, S P Singh, A Tewari, Algorithmic Learning Theory (ALT'18). 83Modi, A., Jiang, N., Singh, S. P., and Tewari, A. Markov decision processes with continuous side information. In Algorithmic Learning Theory (ALT'18), volume 83, pp. 597-618, 2018.</p>
<p>Solving rubik's cube with a robot hand. Akkaya Openai, I Andrychowicz, M Chociej, M Litwin, M Mcgrew, B Petron, A Paino, A Plappert, M Powell, G Ribas, R Schneider, J Tezak, N Tworek, J Welinder, P Weng, L Yuan, Q Zaremba, W Zhang, L , arXiv:1910.07113cs.LGOpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plap- pert, M., Powell, G., Ribas, R., Schneider, J., Tezak, N., Tworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba, W., and Zhang, L. Solving rubik's cube with a robot hand. arXiv:1910.07113 [cs.LG], 2019.</p>
<p>The algorithm selection problem. J Rice, Advances in Computers. 15Rice, J. The algorithm selection problem. Advances in Computers, 15:65-118, 1976.</p>
<p>Trust region policy optimization. J Schulman, S Levine, P Moritz, M I Jordan, Abbeel , P , Proceedings of the 32nd International Conference on Machine Learning (ICML'15). Bach, F. and Blei, D.the 32nd International Conference on Machine Learning (ICML'15)Omnipress37Schulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. Trust region policy optimization. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning (ICML'15), volume 37, pp. 1889-1897. Omnipress, 2015.</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. cs.LGSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv:1707.06347 [cs.LG], 2017.</p>
<p>Trajectory-wise multiple choice learning for dynamics generalization in reinforcement learning. Y Seo, K Lee, I C Gilaberte, T Kurutach, J Shin, Abbeel , P , Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS'20). the 33rd Conference on Neural Information Processing Systems (NeurIPS'20)2020Seo, Y., Lee, K., Gilaberte, I. C., Kurutach, T., Shin, J., and Abbeel, P. Trajectory-wise multiple choice learning for dynamics generalization in reinforcement learning. In Proceedings of the 33rd Conference on Neural Informa- tion Processing Systems (NeurIPS'20), 2020.</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C Maddison, A Guez, L Sifre, G Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, Leach, M Space, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 5297587Silver, D., Huang, A., Maddison, C., Guez, A., Sifre, L., Driessche, G., Schrittwieser, J., Antonoglou, I., Panneer- shelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, SPaCE M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mas- tering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.</p>
<p>Learning heuristic selection with dynamic algorithm configuration. D Speck, A Biedenkapp, F Hutter, R Mattmüller, M Lindauer, Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning (PRL@ICAPS'20). Speck, D., Biedenkapp, A., Hutter, F., Mattmüller, R., and Lindauer, M. Learning heuristic selection with dynamic algorithm configuration. In Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning (PRL@ICAPS'20), October 2020.</p>
<p>Generative teaching networks: Accelerating neural architecture search by learning to generate synthetic training data. F P Such, A Rawal, J Lehman, K O Stanley, Clune , J , Proceedings of the 36th International Conference on Machine Learning (ICML'20). III, H. D. and Singh, A.the 36th International Conference on Machine Learning (ICML'20)98Such, F. P., Rawal, A., Lehman, J., Stanley, K. O., and Clune, J. Generative teaching networks: Accelerating neural architecture search by learning to generate syn- thetic training data. In III, H. D. and Singh, A. (eds.), Proceedings of the 36th International Conference on Ma- chine Learning (ICML'20), volume 98. Proceedings of Machine Learning Research, 2020.</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. S Sukhbaatar, Z Lin, I Kostrikov, G Synnaeve, A Szlam, Fergus , R , 6th International Conference on Learning Representations (ICLR '18). Sukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic cur- ricula via asymmetric self-play. In 6th International Con- ference on Learning Representations (ICLR '18), 2018.</p>
<p>Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance. A L Thomaz, C Breazeal, Proceedings of the Twenty-first National Conference on Artificial Intelligence (AAAI'06). the Twenty-first National Conference on Artificial Intelligence (AAAI'06)Thomaz, A. L. and Breazeal, C. Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance. In Proceed- ings of the Twenty-first National Conference on Artificial Intelligence (AAAI'06), pp. 1000-1006, 2006.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, Abbeel , P , 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '17). Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '17), pp. 23-30, 2017.</p>
<p>Safe reinforcement learning via curriculum induction. M Turchetta, A Kolobov, S Shah, A Krause, A Agarwal, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. 2020Turchetta, M., Kolobov, A., Shah, S., Krause, A., and Agar- wal, A. Safe reinforcement learning via curriculum in- duction. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, (NeurIPS'20), 2020.</p>
<p>Grandmaster level in starcraft II using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W Czarnecki, M Mathieu, A Dudzik, J Chung, D Choi, R Powell, T Ewalds, P Georgiev, J Oh, D Horgan, M Kroiss, I Danihelka, A Huang, L Sifre, T Cai, J Agapiou, M Jaderberg, A Vezhnevets, R Leblond, T Pohlen, V Dalibard, D Budden, Y Sulsky, J Molloy, T Paine, Ç Gülçehre, Z Wang, T Pfaff, Y Wu, R Ring, D Yogatama, D Wünsch, K Mckinney, O Smith, T Schaul, T Lillicrap, K Kavukcuoglu, D Hassabis, C Apps, D Silver, Nature. 5757782Vinyals, O., Babuschkin, I., Czarnecki, W., Mathieu, M., Dudzik, A., Chung, J., Choi, D., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J., Jaderberg, M., Vezhnevets, A., Leblond, R., Pohlen, T., Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, T., Gülçehre, Ç ., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul, T., Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., and Silver, D. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nature, 575(7782): 350-354, 2019.</p>
<p>POET: open-ended coevolution of environments and their optimized solutions. R Wang, J Lehman, J Clune, K O Stanley, Proceedings of the Genetic and Evolutionary Computation Conference, GECCO'19. the Genetic and Evolutionary Computation Conference, GECCO'19Wang, R., Lehman, J., Clune, J., and Stanley, K. O. POET: open-ended coevolution of environments and their opti- mized solutions. In Proceedings of the Genetic and Evo- lutionary Computation Conference, GECCO'19, 2019.</p>
<p>Automatic curriculum learning through value disagreement. Y Zhang, P Abbeel, L Pinto, H Larochelle, M Ranzato, R Hadsell, M Balcan, Lin , Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS'20), 2020. at most 750 in both x-and y-direction for both training and test set respectively. H.the 33rd Conference on Neural Information Processing Systems (NeurIPS'20), 2020. at most 750 in both x-and y-direction for both training and test set respectivelyZhang, Y., Abbeel, P., and Pinto, L. Automatic curricu- lum learning through value disagreement. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS'20), 2020. at most 750 in both x-and y-direction for both training and test set respectively.</p>            </div>
        </div>

    </div>
</body>
</html>
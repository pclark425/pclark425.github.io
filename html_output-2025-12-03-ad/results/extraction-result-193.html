<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-193 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-193</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-193</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-c8d594f09413b1555970f43e68847c211235d60f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c8d594f09413b1555970f43e68847c211235d60f" target="_blank">Prompting GPT-3 To Be Reliable</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This systematic empirical study sheds new insights on the reliability of prompting LLMs, but more importantly, its prompting strategies can help practitioners more reliably use LLMs like GPT-3.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e193.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e193.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context knowledge updating (counterfactual passages)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context knowledge updating with counterfactual passages appended to prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Appending passages to prompts that explicitly contradict a model's memorized (parametric) answer causes GPT-3 to replace its memorized answer with the passage-supported answer at high rates, demonstrating that evidence in the prompt can override parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Code-Davinci-002 and other GPT-3 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book question answering where questions that GPT-3 already answers correctly are re-tested after adding counterfactual passages supporting a different answer (entity-swapped).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>contradictory factual passages (counterfactual documents)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>human-constructed counterfactual passages (entity-substituted gold passages from QA corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Selected set of questions where GPT-3 answered correctly in closed-book; baseline correctness = 100% on selection (by construction) for the memorized answer prior to adding counterfactual passages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With counterfactual triples (⟨P', Q, A'⟩) using Code-Davinci-002: Update rate 85.4% (i.e., model produced the passage-supported answer), Retain memorized answer 4.5%, Other 10.2% (Table 8). Smaller / other GPT-3 variants updated less (Text-Davinci-001: Update 57.9%; Text-Curie-001: Update 40.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive (evidence caused model to change answers to match the provided passage in the majority of cases); adding contradictory passage increased agreement with prompt evidence and reduced reliance on memorized facts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>The prompt supplies strong local context that the model conditions on at generation time; demonstrations and appended passages can dominate parametric priors, enabling in-context override of memorized knowledge (in-context learning / context-conditioning).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3 often abandons its memorized answer when presented with a passage that supports an alternative answer; for Code-Davinci-002 ~85% of items were updated to the passage-supported answer. Larger / stronger variants update more reliably than smaller ones. Prompt format (providing the passage and an explicit QA triple) is important to induce updating.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting GPT-3 To Be Reliable', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e193.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e193.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented prompting (Contriever)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented few-shot prompting using Contriever retrieved passages</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prepending retrieved evidence passages (from Wikipedia, via an unsupervised retriever) to the test question in the prompt improves GPT-3's open-domain QA accuracy, showing that adding external evidence benefits factual performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Code-Davinci-002 / default GPT-3 prompt setup)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain (retrieval-augmented) question answering in a few-shot prompting setting.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>retrieved relevant documents / passages</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia passages retrieved by Contriever (unsupervised dense retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (often aligned when retrieval finds gold context; may be neutral or complementary otherwise)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book GPT-3 (16-shot) exact-match: NQ 40.6%, TriviaQA 73.6%, SQuAD 20.2% (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With Contriever top-5: NQ 43.3% (retriever recall 61.8%); TriviaQA 75.6% (recall 69.6%); SQuAD 31.7% (recall 48.8%). With top-10: NQ 44.2% (70.5% recall); TriviaQA 76.0%; SQuAD 34.0% (57.7% recall).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive (accuracy increased when passages were added); effect size scaled with retriever recall and number of retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Providing grounded context reduces reliance on possibly incorrect parametric knowledge and supplies the evidence needed to produce correct answers; the model conditions on the appended passages during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adding retrieved passages consistently improves QA performance across datasets, with larger gains when retriever recall is higher; demonstrates effective factual augmentation via prompt evidence without model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting GPT-3 To Be Reliable', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e193.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e193.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Natural language intervention (BBQ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language instruction intervention to reduce bias in prompt responses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prepending an explicit instruction to treat demographic groups equally and to abstain when evidence is insufficient substantially changed GPT-3's answers toward neutrality and reduced bias scores on ambiguous QA sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (default / Code-Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice QA evaluating social biases (BBQ), with ambiguous vs disambiguated questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>explicit natural language instruction (ethical / behavior guideline)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>human-provided prompt instruction appended before each test question</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>neutral to contradictory (the instruction does not assert facts but instructs decision policy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Ambiguous accuracy (Ambig Acc) under Ambig-Pro-Bias prompt: 2.6%; Disambiguated accuracy 97.3%; Ambig Bias Score 24.7% (Table 4 / Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>After adding the instruction: Ambig Acc increased to 96.6%, Ambig Bias Score reduced to 1.9%; Disambig Acc dropped to 51.5% and Disambig Bias Score slightly changed to 3.8% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — strong positive effect on reducing bias and increasing selection of neutral option on ambiguous questions, but decreased accuracy on disambiguated questions in this experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>GPT-3 is sensitive to behavioral instructions in prompts; explicit policy-like instructions can steer answer selection (i.e., models act as 'ethical-advice takers'), biasing outputs toward neutrality when instructed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A single natural-language instruction dramatically shifted model behavior away from stereotype-consistent answers on ambiguous prompts (large reduction in bias score and large increase in ambiguous-set accuracy), showing high sensitivity to instruction-level evidence; however, it can trade off accuracy on questions where factual disambiguating information exists.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting GPT-3 To Be Reliable', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e193.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e193.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-demonstration distribution effects (WinoBias / BBQ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of answer/demo distribution in the prompt on model bias and outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The demographic distribution and label distribution of demonstration examples in the prompt strongly shape GPT-3's predictions: balanced distributions reduce bias, while biased demonstration distributions induce corresponding biases in model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (default / Code-Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Coreference resolution (WinoBias) and bias-sensitive multiple-choice QA (BBQ); measuring how demo answer distributions in prompt change model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>demonstration examples (few-shot QA / QA pairs) with controlled label/demographic distributions</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>sampled training examples from datasets (balanced or skewed toward pro/anti bias or neutral answers)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (demos can align with or contradict model priors)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Zero-shot BBQ: Ambig Acc 60.5%, DisAmbig Acc 43.2%, Ambig Bias Score 3.7 (Table 4). WinoBias with no balanced demos not reported explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>WinoBias: Balanced 16-shot prompt Type I Pro 89.2% vs Type I Anti 81.1% gap 8.1; Pro-only prompt (Type I-Pro examples) produced large pro bias (Type I Pro 93.4% vs Type I Anti 42.4% gap 51.0) (Table 3). BBQ: Balanced prompt yields Ambig Acc 96.8% and low bias; Ambig-Pro prompt (biased demos) yields Ambig Acc 2.6% and Ambig Bias Score 24.7% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive when balanced (reduces bias) and negative when demos are skewed (induces bias); demonstrations strongly shift model answer distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>In-context learning: the model infers the desired input-output mapping (and implicit label priors) from demonstrations; hence the demo answer distribution informs the model's decision rule and its selection behavior (sycophancy to prompt examples / demonstration-driven priors).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling balanced demos is an effective mitigation strategy for stereotype-driven errors; conversely, providing biased demos causes the model to reproduce those biases — demonstrating strong sensitivity to evidence distribution in the prompt. Ordering also matters: shuffling reduces bias compared to clustering pro/anti examples at the end.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting GPT-3 To Be Reliable', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e193.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e193.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration via LM-Prob and Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using normalized LM probability and self-consistency sampling frequency as confidence estimates for free-form generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two simple in-context confidence measures — LM-Prob (token-level generation probability) and Self-Con (frequency of sampled outputs) — produce better calibration (lower ECE/Brier) and more discriminative confidence rankings for GPT-3 than a supervised DPR-BERT baseline on several QA datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Code-Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Free-form answer generation for QA (NQ, TriviaQA, HotpotQA) with evaluation of calibration (ECE, Brier) and selective prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>none in terms of external documents; evidence here refers to the method of computing confidence (LM probabilities or sampled answer frequencies) based on model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>model-internal probabilities and sampled outputs (self-consistency sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>DPR-BERT baseline on NQ: Acc 36.1% ECE 29.4 Brier 33.5 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>GPT-3 with LM-Prob on NQ: Acc 40.5% ECE 18.9 Brier 23.3; Self-Con on NQ: Acc 40.2% ECE 14.3 Brier 20.1. Selective prediction: top 10% accuracy NQ: DPR-BERT 60.1% vs GPT-3 LM-Prob 83.1% and Self-Con 77.0% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive (confidence measures derived from the model's own output probability or sampling frequencies were better calibrated and more discriminative than supervised baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>LMProb provides a probabilistic confidence tied to token-generation likelihood (reciprocal perplexity); Self-Con uses agreement frequency across stochastic decodes as an empirical confidence. Both reflect model uncertainty more faithfully than externally trained calibration for closed retriever+reader systems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3's raw LM probabilities and self-consistency frequencies are useful intrinsic confidence signals: they yield lower ECE/Brier and enable strong selective prediction (high accuracy on most-confident subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting GPT-3 To Be Reliable', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e193.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e193.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Number-of-demos effect on calibration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of increasing number of demonstration examples on accuracy and calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding more demonstration examples to the prompt generally increases accuracy but not necessarily improves calibration; in some cases more shots increased overconfidence (worse ECE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Code-Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book QA (NQ) with varying number of in-context examples (2,4,8,16,64) evaluating accuracy and calibration (ECE, Brier).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>demonstration examples (few-shot prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>sampled training examples</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>neutral/mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>2-shot GPT-3 (LM-Prob) on NQ: Acc 37.0% ECE 11.7 Brier 20.8 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>4-shot Acc 38.3% ECE 13.4; 8-shot Acc 38.8% ECE 24.4; 16-shot Acc 40.5% ECE 18.9; 64-shot Acc 42.8% ECE 13.4. Accuracy increased with more shots but ECE did not monotonically improve and sometimes worsened (8-shot ECE notably higher).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — positive for accuracy, mixed/negative for calibration in some shot counts (more evidence sometimes increased overconfidence).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Adding demonstrations clarifies task mapping and increases performance, but also shifts model output distributions in ways that can increase miscalibrated confidence (overconfidence) depending on the prompt composition; calibration is not strictly improved by more in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>More shots improved accuracy (e.g., 2-shot -> 64-shot increased Acc from 37.0% to 42.8%), but ECE varied non-monotonically — illustrating that adding evidence (demos) can increase performance while leaving or worsening calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting GPT-3 To Be Reliable', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e193.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e193.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selective prediction discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Confidence-based selective prediction using LM-Prob / Self-Con</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Confidence scores derived from LM probabilities or self-consistency allow effective selective prediction: the model's most confident outputs have substantially higher accuracy than average.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Code-Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Ranking GPT-3's generated QA answers by confidence and measuring accuracy on top-k coverage subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>self-derived confidence signals (LM-probabilities, sampling frequencies)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>model output probabilities and sampled prediction frequencies</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline DPR-BERT selective prediction top-10% accuracy on NQ: 60.1% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>GPT-3 LM-Prob top-10% accuracy on NQ: 83.1%; GPT-3 Self-Con top-10% accuracy: 77.0% (Table 7). At 50% coverage, LM-Prob 58.8% and Self-Con 62.0% vs DPR-BERT 41.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — using evidence in the form of confidence signals markedly improves ability to identify correct answers (high precision on most-confident subset).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Confidence signals reflect model-internal certainty; ranking by these intrinsic signals concentrates correct answers in high-confidence subsets, enabling reliable selective prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3's confidence estimates are more discriminative than the supervised DPR-BERT baseline, enabling much higher accuracy on the top-k most-confident predictions (e.g., 83.1% at 10% coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting GPT-3 To Be Reliable', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e193.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e193.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human decomposition / chain-of-thought evidence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Providing human-written question decomposition (and intermediate QA) in prompts for multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Appending human decompositions of multi-hop questions (sub-questions and intermediate answers) into prompts improves GPT-3's multi-hop QA accuracy more than chain-of-thought examples alone, indicating that explicit reasoning evidence in the prompt corrects reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Code-Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering (HotpotQA) comparing standard prompting, chain-of-thought (CoT) prompting, and adding human decomposed sub-questions/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>reasoning chains / human-written question decompositions and intermediate answers</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>human-annotated decompositions from prior work (Tang et al., 2021) included in the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>neutral/mixed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Standard prompting (no decomposition) HotpotQA EM/F1: 18.0 / 28.1 (Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>CoT: 25.2 / 35.2; CoT + Human Sub-Q1: 30.0 / 42.3; CoT + Human Sub-Q1 + Gold Sub-A1: 44.3 / 59.0 (Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive (large improvements in multi-hop QA accuracy when human decomposition and intermediate answers are provided).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Providing explicit decomposition reduces the burden on the model to invent correct reasoning chains; the model leverages the stepwise evidence to produce correct subsequent reasoning and final answers (conditioning on provided rationale/evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human-written sub-questions and sub-answers substantially boosted multi-hop QA performance, with the biggest gain when the first sub-question and its gold answer were provided; shows effective control of model reasoning via evidence in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting GPT-3 To Be Reliable', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Entity-based knowledge conflicts in question answering <em>(Rating: 2)</em></li>
                <li>How much knowledge can you pack into the parameters of a language model? <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>BBQ: A hand-built bias benchmark for question answering <em>(Rating: 2)</em></li>
                <li>WinoBias: Gender bias in coreference resolution: Evaluation and debiasing methods <em>(Rating: 2)</em></li>
                <li>Language models (mostly) know what they know <em>(Rating: 1)</em></li>
                <li>Longpre et al. (2021) - Entity-based knowledge conflicts in question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-193",
    "paper_id": "paper-c8d594f09413b1555970f43e68847c211235d60f",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "In-context knowledge updating (counterfactual passages)",
            "name_full": "In-context knowledge updating with counterfactual passages appended to prompts",
            "brief_description": "Appending passages to prompts that explicitly contradict a model's memorized (parametric) answer causes GPT-3 to replace its memorized answer with the passage-supported answer at high rates, demonstrating that evidence in the prompt can override parametric knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Code-Davinci-002 and other GPT-3 variants)",
            "model_size": "null",
            "task_description": "Closed-book question answering where questions that GPT-3 already answers correctly are re-tested after adding counterfactual passages supporting a different answer (entity-swapped).",
            "evidence_type": "contradictory factual passages (counterfactual documents)",
            "evidence_source": "human-constructed counterfactual passages (entity-substituted gold passages from QA corpora)",
            "parametric_knowledge_alignment": "contradictory",
            "performance_without_evidence": "Selected set of questions where GPT-3 answered correctly in closed-book; baseline correctness = 100% on selection (by construction) for the memorized answer prior to adding counterfactual passages.",
            "performance_with_evidence": "With counterfactual triples (⟨P', Q, A'⟩) using Code-Davinci-002: Update rate 85.4% (i.e., model produced the passage-supported answer), Retain memorized answer 4.5%, Other 10.2% (Table 8). Smaller / other GPT-3 variants updated less (Text-Davinci-001: Update 57.9%; Text-Curie-001: Update 40.0%).",
            "evidence_effect": "positive (evidence caused model to change answers to match the provided passage in the majority of cases); adding contradictory passage increased agreement with prompt evidence and reduced reliance on memorized facts.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "The prompt supplies strong local context that the model conditions on at generation time; demonstrations and appended passages can dominate parametric priors, enabling in-context override of memorized knowledge (in-context learning / context-conditioning).",
            "key_findings": "GPT-3 often abandons its memorized answer when presented with a passage that supports an alternative answer; for Code-Davinci-002 ~85% of items were updated to the passage-supported answer. Larger / stronger variants update more reliably than smaller ones. Prompt format (providing the passage and an explicit QA triple) is important to induce updating.",
            "counterintuitive_behavior": false,
            "uuid": "e193.0",
            "source_info": {
                "paper_title": "Prompting GPT-3 To Be Reliable",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Retrieval-augmented prompting (Contriever)",
            "name_full": "Retrieval-augmented few-shot prompting using Contriever retrieved passages",
            "brief_description": "Prepending retrieved evidence passages (from Wikipedia, via an unsupervised retriever) to the test question in the prompt improves GPT-3's open-domain QA accuracy, showing that adding external evidence benefits factual performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Code-Davinci-002 / default GPT-3 prompt setup)",
            "model_size": "null",
            "task_description": "Open-domain (retrieval-augmented) question answering in a few-shot prompting setting.",
            "evidence_type": "retrieved relevant documents / passages",
            "evidence_source": "Wikipedia passages retrieved by Contriever (unsupervised dense retriever)",
            "parametric_knowledge_alignment": "mixed (often aligned when retrieval finds gold context; may be neutral or complementary otherwise)",
            "performance_without_evidence": "Closed-book GPT-3 (16-shot) exact-match: NQ 40.6%, TriviaQA 73.6%, SQuAD 20.2% (Table 9).",
            "performance_with_evidence": "With Contriever top-5: NQ 43.3% (retriever recall 61.8%); TriviaQA 75.6% (recall 69.6%); SQuAD 31.7% (recall 48.8%). With top-10: NQ 44.2% (70.5% recall); TriviaQA 76.0%; SQuAD 34.0% (57.7% recall).",
            "evidence_effect": "positive (accuracy increased when passages were added); effect size scaled with retriever recall and number of retrieved passages.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Providing grounded context reduces reliance on possibly incorrect parametric knowledge and supplies the evidence needed to produce correct answers; the model conditions on the appended passages during generation.",
            "key_findings": "Adding retrieved passages consistently improves QA performance across datasets, with larger gains when retriever recall is higher; demonstrates effective factual augmentation via prompt evidence without model fine-tuning.",
            "counterintuitive_behavior": false,
            "uuid": "e193.1",
            "source_info": {
                "paper_title": "Prompting GPT-3 To Be Reliable",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Natural language intervention (BBQ)",
            "name_full": "Natural-language instruction intervention to reduce bias in prompt responses",
            "brief_description": "Prepending an explicit instruction to treat demographic groups equally and to abstain when evidence is insufficient substantially changed GPT-3's answers toward neutrality and reduced bias scores on ambiguous QA sets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (default / Code-Davinci-002)",
            "model_size": "null",
            "task_description": "Multiple-choice QA evaluating social biases (BBQ), with ambiguous vs disambiguated questions.",
            "evidence_type": "explicit natural language instruction (ethical / behavior guideline)",
            "evidence_source": "human-provided prompt instruction appended before each test question",
            "parametric_knowledge_alignment": "neutral to contradictory (the instruction does not assert facts but instructs decision policy)",
            "performance_without_evidence": "Ambiguous accuracy (Ambig Acc) under Ambig-Pro-Bias prompt: 2.6%; Disambiguated accuracy 97.3%; Ambig Bias Score 24.7% (Table 4 / Table 5).",
            "performance_with_evidence": "After adding the instruction: Ambig Acc increased to 96.6%, Ambig Bias Score reduced to 1.9%; Disambig Acc dropped to 51.5% and Disambig Bias Score slightly changed to 3.8% (Table 5).",
            "evidence_effect": "mixed — strong positive effect on reducing bias and increasing selection of neutral option on ambiguous questions, but decreased accuracy on disambiguated questions in this experimental setup.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "GPT-3 is sensitive to behavioral instructions in prompts; explicit policy-like instructions can steer answer selection (i.e., models act as 'ethical-advice takers'), biasing outputs toward neutrality when instructed.",
            "key_findings": "A single natural-language instruction dramatically shifted model behavior away from stereotype-consistent answers on ambiguous prompts (large reduction in bias score and large increase in ambiguous-set accuracy), showing high sensitivity to instruction-level evidence; however, it can trade off accuracy on questions where factual disambiguating information exists.",
            "counterintuitive_behavior": true,
            "uuid": "e193.2",
            "source_info": {
                "paper_title": "Prompting GPT-3 To Be Reliable",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Prompt-demonstration distribution effects (WinoBias / BBQ)",
            "name_full": "Effect of answer/demo distribution in the prompt on model bias and outputs",
            "brief_description": "The demographic distribution and label distribution of demonstration examples in the prompt strongly shape GPT-3's predictions: balanced distributions reduce bias, while biased demonstration distributions induce corresponding biases in model outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (default / Code-Davinci-002)",
            "model_size": "null",
            "task_description": "Coreference resolution (WinoBias) and bias-sensitive multiple-choice QA (BBQ); measuring how demo answer distributions in prompt change model predictions.",
            "evidence_type": "demonstration examples (few-shot QA / QA pairs) with controlled label/demographic distributions",
            "evidence_source": "sampled training examples from datasets (balanced or skewed toward pro/anti bias or neutral answers)",
            "parametric_knowledge_alignment": "mixed (demos can align with or contradict model priors)",
            "performance_without_evidence": "Zero-shot BBQ: Ambig Acc 60.5%, DisAmbig Acc 43.2%, Ambig Bias Score 3.7 (Table 4). WinoBias with no balanced demos not reported explicitly.",
            "performance_with_evidence": "WinoBias: Balanced 16-shot prompt Type I Pro 89.2% vs Type I Anti 81.1% gap 8.1; Pro-only prompt (Type I-Pro examples) produced large pro bias (Type I Pro 93.4% vs Type I Anti 42.4% gap 51.0) (Table 3). BBQ: Balanced prompt yields Ambig Acc 96.8% and low bias; Ambig-Pro prompt (biased demos) yields Ambig Acc 2.6% and Ambig Bias Score 24.7% (Table 4).",
            "evidence_effect": "positive when balanced (reduces bias) and negative when demos are skewed (induces bias); demonstrations strongly shift model answer distributions.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "In-context learning: the model infers the desired input-output mapping (and implicit label priors) from demonstrations; hence the demo answer distribution informs the model's decision rule and its selection behavior (sycophancy to prompt examples / demonstration-driven priors).",
            "key_findings": "Sampling balanced demos is an effective mitigation strategy for stereotype-driven errors; conversely, providing biased demos causes the model to reproduce those biases — demonstrating strong sensitivity to evidence distribution in the prompt. Ordering also matters: shuffling reduces bias compared to clustering pro/anti examples at the end.",
            "counterintuitive_behavior": true,
            "uuid": "e193.3",
            "source_info": {
                "paper_title": "Prompting GPT-3 To Be Reliable",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Calibration via LM-Prob and Self-Consistency",
            "name_full": "Using normalized LM probability and self-consistency sampling frequency as confidence estimates for free-form generation",
            "brief_description": "Two simple in-context confidence measures — LM-Prob (token-level generation probability) and Self-Con (frequency of sampled outputs) — produce better calibration (lower ECE/Brier) and more discriminative confidence rankings for GPT-3 than a supervised DPR-BERT baseline on several QA datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Code-Davinci-002)",
            "model_size": "null",
            "task_description": "Free-form answer generation for QA (NQ, TriviaQA, HotpotQA) with evaluation of calibration (ECE, Brier) and selective prediction.",
            "evidence_type": "none in terms of external documents; evidence here refers to the method of computing confidence (LM probabilities or sampled answer frequencies) based on model outputs",
            "evidence_source": "model-internal probabilities and sampled outputs (self-consistency sampling)",
            "parametric_knowledge_alignment": "neutral",
            "performance_without_evidence": "DPR-BERT baseline on NQ: Acc 36.1% ECE 29.4 Brier 33.5 (Table 6).",
            "performance_with_evidence": "GPT-3 with LM-Prob on NQ: Acc 40.5% ECE 18.9 Brier 23.3; Self-Con on NQ: Acc 40.2% ECE 14.3 Brier 20.1. Selective prediction: top 10% accuracy NQ: DPR-BERT 60.1% vs GPT-3 LM-Prob 83.1% and Self-Con 77.0% (Table 7).",
            "evidence_effect": "positive (confidence measures derived from the model's own output probability or sampling frequencies were better calibrated and more discriminative than supervised baseline).",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "LMProb provides a probabilistic confidence tied to token-generation likelihood (reciprocal perplexity); Self-Con uses agreement frequency across stochastic decodes as an empirical confidence. Both reflect model uncertainty more faithfully than externally trained calibration for closed retriever+reader systems.",
            "key_findings": "GPT-3's raw LM probabilities and self-consistency frequencies are useful intrinsic confidence signals: they yield lower ECE/Brier and enable strong selective prediction (high accuracy on most-confident subsets).",
            "counterintuitive_behavior": false,
            "uuid": "e193.4",
            "source_info": {
                "paper_title": "Prompting GPT-3 To Be Reliable",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Number-of-demos effect on calibration",
            "name_full": "Effect of increasing number of demonstration examples on accuracy and calibration",
            "brief_description": "Adding more demonstration examples to the prompt generally increases accuracy but not necessarily improves calibration; in some cases more shots increased overconfidence (worse ECE).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Code-Davinci-002)",
            "model_size": "null",
            "task_description": "Closed-book QA (NQ) with varying number of in-context examples (2,4,8,16,64) evaluating accuracy and calibration (ECE, Brier).",
            "evidence_type": "demonstration examples (few-shot prompts)",
            "evidence_source": "sampled training examples",
            "parametric_knowledge_alignment": "neutral/mixed",
            "performance_without_evidence": "2-shot GPT-3 (LM-Prob) on NQ: Acc 37.0% ECE 11.7 Brier 20.8 (Table 6).",
            "performance_with_evidence": "4-shot Acc 38.3% ECE 13.4; 8-shot Acc 38.8% ECE 24.4; 16-shot Acc 40.5% ECE 18.9; 64-shot Acc 42.8% ECE 13.4. Accuracy increased with more shots but ECE did not monotonically improve and sometimes worsened (8-shot ECE notably higher).",
            "evidence_effect": "mixed — positive for accuracy, mixed/negative for calibration in some shot counts (more evidence sometimes increased overconfidence).",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Adding demonstrations clarifies task mapping and increases performance, but also shifts model output distributions in ways that can increase miscalibrated confidence (overconfidence) depending on the prompt composition; calibration is not strictly improved by more in-context examples.",
            "key_findings": "More shots improved accuracy (e.g., 2-shot -&gt; 64-shot increased Acc from 37.0% to 42.8%), but ECE varied non-monotonically — illustrating that adding evidence (demos) can increase performance while leaving or worsening calibration.",
            "counterintuitive_behavior": true,
            "uuid": "e193.5",
            "source_info": {
                "paper_title": "Prompting GPT-3 To Be Reliable",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Selective prediction discrimination",
            "name_full": "Confidence-based selective prediction using LM-Prob / Self-Con",
            "brief_description": "Confidence scores derived from LM probabilities or self-consistency allow effective selective prediction: the model's most confident outputs have substantially higher accuracy than average.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Code-Davinci-002)",
            "model_size": "null",
            "task_description": "Ranking GPT-3's generated QA answers by confidence and measuring accuracy on top-k coverage subsets.",
            "evidence_type": "self-derived confidence signals (LM-probabilities, sampling frequencies)",
            "evidence_source": "model output probabilities and sampled prediction frequencies",
            "parametric_knowledge_alignment": "neutral",
            "performance_without_evidence": "Baseline DPR-BERT selective prediction top-10% accuracy on NQ: 60.1% (Table 7).",
            "performance_with_evidence": "GPT-3 LM-Prob top-10% accuracy on NQ: 83.1%; GPT-3 Self-Con top-10% accuracy: 77.0% (Table 7). At 50% coverage, LM-Prob 58.8% and Self-Con 62.0% vs DPR-BERT 41.9%.",
            "evidence_effect": "positive — using evidence in the form of confidence signals markedly improves ability to identify correct answers (high precision on most-confident subset).",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Confidence signals reflect model-internal certainty; ranking by these intrinsic signals concentrates correct answers in high-confidence subsets, enabling reliable selective prediction.",
            "key_findings": "GPT-3's confidence estimates are more discriminative than the supervised DPR-BERT baseline, enabling much higher accuracy on the top-k most-confident predictions (e.g., 83.1% at 10% coverage).",
            "counterintuitive_behavior": false,
            "uuid": "e193.6",
            "source_info": {
                "paper_title": "Prompting GPT-3 To Be Reliable",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Human decomposition / chain-of-thought evidence",
            "name_full": "Providing human-written question decomposition (and intermediate QA) in prompts for multi-hop reasoning",
            "brief_description": "Appending human decompositions of multi-hop questions (sub-questions and intermediate answers) into prompts improves GPT-3's multi-hop QA accuracy more than chain-of-thought examples alone, indicating that explicit reasoning evidence in the prompt corrects reasoning chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Code-Davinci-002)",
            "model_size": "null",
            "task_description": "Multi-hop question answering (HotpotQA) comparing standard prompting, chain-of-thought (CoT) prompting, and adding human decomposed sub-questions/answers.",
            "evidence_type": "reasoning chains / human-written question decompositions and intermediate answers",
            "evidence_source": "human-annotated decompositions from prior work (Tang et al., 2021) included in the prompt",
            "parametric_knowledge_alignment": "neutral/mixed",
            "performance_without_evidence": "Standard prompting (no decomposition) HotpotQA EM/F1: 18.0 / 28.1 (Table 10).",
            "performance_with_evidence": "CoT: 25.2 / 35.2; CoT + Human Sub-Q1: 30.0 / 42.3; CoT + Human Sub-Q1 + Gold Sub-A1: 44.3 / 59.0 (Table 10).",
            "evidence_effect": "positive (large improvements in multi-hop QA accuracy when human decomposition and intermediate answers are provided).",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Providing explicit decomposition reduces the burden on the model to invent correct reasoning chains; the model leverages the stepwise evidence to produce correct subsequent reasoning and final answers (conditioning on provided rationale/evidence).",
            "key_findings": "Human-written sub-questions and sub-answers substantially boosted multi-hop QA performance, with the biggest gain when the first sub-question and its gold answer were provided; shows effective control of model reasoning via evidence in prompt.",
            "counterintuitive_behavior": false,
            "uuid": "e193.7",
            "source_info": {
                "paper_title": "Prompting GPT-3 To Be Reliable",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Entity-based knowledge conflicts in question answering",
            "rating": 2,
            "sanitized_title": "entitybased_knowledge_conflicts_in_question_answering"
        },
        {
            "paper_title": "How much knowledge can you pack into the parameters of a language model?",
            "rating": 2,
            "sanitized_title": "how_much_knowledge_can_you_pack_into_the_parameters_of_a_language_model"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "BBQ: A hand-built bias benchmark for question answering",
            "rating": 2,
            "sanitized_title": "bbq_a_handbuilt_bias_benchmark_for_question_answering"
        },
        {
            "paper_title": "WinoBias: Gender bias in coreference resolution: Evaluation and debiasing methods",
            "rating": 2,
            "sanitized_title": "winobias_gender_bias_in_coreference_resolution_evaluation_and_debiasing_methods"
        },
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 1,
            "sanitized_title": "language_models_mostly_know_what_they_know"
        },
        {
            "paper_title": "Longpre et al. (2021) - Entity-based knowledge conflicts in question answering",
            "rating": 1,
            "sanitized_title": "longpre_et_al_2021_entitybased_knowledge_conflicts_in_question_answering"
        }
    ],
    "cost": 0.01858625,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prompting GPT-3 To Be Reliable</h1>
<p>Chenglei $\mathbf{S i}^{1 *}$, Zhe Gan ${ }^{2}$, Zhengyuan Yang ${ }^{2}$, Shuohang Wang ${ }^{2}$<br>Jianfeng Wang ${ }^{2}$, Jordan Boyd-Graber ${ }^{1}$, Lijuan Wang ${ }^{2}$<br>${ }^{1}$ University of Maryland ${ }^{2}$ Microsoft<br>clsi@umd.edu pkuganzhe@gmail.com lijuanw@microsoft.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. ${ }^{1}$ Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.</p>
<h2>1 INTRODUCTION</h2>
<p>NLP is dominated by large language models (LLMs) - pretrained on large, unlabeled text data — that are then used for downstream tasks (Devlin et al., 2019a; Brown et al., 2020). Scaling the model and data size often brings gains on downstream tasks (Kaplan et al., 2020; BIG-Bench, 2022), allowing what some call emergent abilities (Wei et al., 2022a). These emergent behaviors are accomplished through prompting-a crafted, natural language text to shape predictions or offer relevant information without expensive supervised data. Among all the existing LLMs, GPT-3 (Brown et al., 2020) is particularly popular due to its flexibility and ease of use from the OpenAI API ${ }^{2}$.</p>
<p>Existing empirical studies investigate GPT-3 on specific tasks such as mathematical reasoning (Hendrycks et al., 2021a), multi-hop reasoning (Wei et al., 2022b; Kojima et al., 2022), and code generation (Chen et al., 2021a). However, rising numbers on these evaluations do not ensure LLM reliability. For example, LLMs (including GPT-3) produce biased (Lucy \&amp; Bamman, 2021) generations, false statements (Lin et al., 2022b), and outdated information (Chen et al., 2021b; Kasai et al., 2022). Deploying such models in the real world could result in catastrophic harm.</p>
<p>In the context of prompting LLMs, several previous works have explored their reliability. For example, in the release reports of GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), Gopher (Rae et al., 2021) and PaLM (Chowdhery et al., 2022), there are dedicated experiments evaluating these LLMs' representational bias and toxicity. Another line of work has evaluated calibration (Lin et al., 2022a; Kadavath et al., 2022) of prompting-based LLMs on math questions or multiple-choice questions. We differ from these prior works in two key aspects: (i) We perform a more comprehensive study of four core facets of reliability, serving as a meta-analysis. (ii) We focus particularly on find-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Four main reliability factors we examined and the core findings.
ing prompting strategies that are effective under these reliability facets, rather than just evaluating intrinsic model characteristics (Figure 1).</p>
<p>Our reliability testing framework takes inspiration from the survey of unsolved problems in ML safety (Hendrycks et al., 2021b): withstanding hazards (generalizability), identifying hazards (calibration), steering ML systems and reducing deployment hazards (reducing social biases and improving factuality). These facets also aim to address the risks of ML systems identified in existing conceptual frameworks (Tan et al., 2022; 2021). We have a more extensive discussion of related works in Appendix Section A.</p>
<p>As summarized in Figure 1, our simple prompting strategies beat smaller-scale supervised models on all reliability metrics we consider: 1) prompting with randomly sampled examples from the source domain allows GPT-3 to generalize robustly on unseen domains and challenge examples; 2) examples sampled from a balanced demographic distribution and natural language intervention reduce social biases; 3) language model probabilities are calibrated to reflect accuracy; and 4) appending up-to-date knowledge can supplant GPT-3's memorized knowledge or reasoning chains.</p>
<h1>2 FACET 1: GENERALIZABILITY</h1>
<p>LLMs are often criticized for missing the forest for the trees. They overfit training data from a particular domain (domain shift), are not robust to minor changes in a text (perturbations), or use shortcuts to make predictions (spurious correlations). These pathologies make models unreliable since these distribution shifts happen all the time in real-world data and could incur significant performance drops. In this section, we study whether GPT-3 can stay robust when the test data come from different distributions than the demo examples in the prompt, and how their generalization compares to supervised models.</p>
<p>Experiment Setup We study all three types of distribution shifts mentioned above. For each of them, researchers have created datasets that target modern language models' weaknesses which we adopt for evaluation. For domain shift, MRQA (Fisch et al., 2019) trains on six machine reading datasets from the source domain and tests on six different target domains; for perturbations, AdvGLUE (Wang et al., 2021) craft adversarial versions of GLUE (Wang et al., 2018) based on automatic adversarial perturbations and human filtering, and Contrast Sets (Gardner et al., 2020) are expert-authored minimal edits that change the label; for spurious correlation, HANS (McCoy et al., 2019) and PAWS (Zhang et al., 2019) are challenge sets designed for models trained on MNLI and</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MRQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AdvGLUE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Contrast Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Source $\uparrow$</td>
<td style="text-align: center;">Target $\uparrow$</td>
<td style="text-align: center;">Gap $_{\downarrow}$</td>
<td style="text-align: center;">Original $\uparrow$</td>
<td style="text-align: center;">Perturbed $\uparrow$</td>
<td style="text-align: center;">Gap $_{\downarrow}$</td>
<td style="text-align: center;">Original $\uparrow$</td>
<td style="text-align: center;">Perturbed $\uparrow$</td>
<td style="text-align: center;">Gap $_{\downarrow}$</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">15.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">77.2 (S) / 77.2 (T)</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">5.5</td>
</tr>
</tbody>
</table>
<p>Table 1: For GPT-3 on MRQA target domain test sets, we report results for using both demos from the source (S) and target (T) domains, which surprisingly achieve the same F1 on the target domains (77.2). For AdvGLUE and Contrast Set, we use accuracy as the metric and we use demos from the clean data as the prompt. In all cases, GPT-3 few-shot prompting incurs much smaller performance gaps on the OOD or challenge test sets than the supervised RoBERTa (123M) baseline. We include the comparison with many other supervised baselines in Appendix B and GPT-3 exhibits better generalization than all of them.</p>
<p>QQP where the lexical overlap feature in the training data does not hold during testing. For each of these settings, we evaluate a simple prompting strategy by sampling examples from the source domains (for MRQA, we use a fixed prompt consisting of eight randomly sampled examples from the source domain on all target datasets; for perturbations and spurious correlation, we randomly sample 16 demos from the original clean training data from GLUE, MNLI, and QQP respectively). In addition, for domain shift, we also consider a prompt where we sample eight examples from the training set of each target domain to ablate the impact of the distribution of the demo examples.</p>
<p>Results Table 1 and Table 2 compare supervised RoBERTa (Liu et al., 2019) and BERT (Devlin et al., 2019b) models trained on the source domain datasets or the clean training data with GPT-3 that uses examples sampled from the same training data as in the supervised models. ${ }^{3}$ GPT-3 achieves higher accuracy on the OOD tests even when it is slightly worse on the in-domain test sets than the supervised baselines, leading to smaller generalization gaps. This shows that prompting GPT-3 can be more robust than supervised finetuning of smaller-scale language models. Surprisingly, we compare using demo examples sampled from the source domains versus target domains on MRQA, and both prompting methods give the same OOD generalization results, indicating that GPT-3 prompts can directly generalize to OOD test sets where the test examples are from a different distribution than the prompt demo distribution, possibly because the role of demonstration examples is more in specifying the task rather than informing the input distribution (Min et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">BERT <br> $(340 \mathrm{M})$</th>
<th style="text-align: center;">RoBERTa <br> $(354 \mathrm{M})$</th>
<th style="text-align: center;">GPT-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$M N L I \rightarrow$ HANS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MNLI $\uparrow$</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">77.6</td>
</tr>
<tr>
<td style="text-align: center;">HANS $\uparrow$</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">75.3</td>
</tr>
<tr>
<td style="text-align: center;">Gap $_{\downarrow}$</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;">$Q Q P \rightarrow$ PAWS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">QQP $\uparrow$</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">83.5</td>
</tr>
<tr>
<td style="text-align: center;">PAWS $\uparrow$</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">73.7</td>
</tr>
<tr>
<td style="text-align: center;">Gap $_{\downarrow}$</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">9.8</td>
</tr>
</tbody>
</table>
<p>Table 2: When using demos sampled from MNLI and QQP, GPT-3 few-shot prompting achieves much better generalization than smaller supervised models (BERT and RoBERTa) on the OOD test sets HANS and PAWS.</p>
<p>Takeaway (i) Few-shot prompting of GPT-3 is more robust than supervised models such as finetuned BERT and RoBERTa, under all three settings (domain shift, perturbations, spurious correlation). (ii) Using randomly sampled demos from the source datasets is a simple but strong baseline, in fact, it performs the same as using demos sampled from the target distributions.</p>
<h1>3 FACET 2: SOCIAL BIAS AND FAIRNESS</h1>
<p>Apart from high performance on in-domain and OOD datasets, the second key facet of reliability is that we expect models to be fair to different demographic groups. Biased models cause severe harm when deployed in real-world applications, especially to the minority groups being discriminated against (Cao et al., 2022). In this section, we examine whether GPT-3 produces biased predictions in two downstream tasks - coreference resolution and question answering.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Type I Pro $\uparrow$</th>
<th style="text-align: center;">Type I Anti $\uparrow$</th>
<th style="text-align: center;">Gap $_{(\downarrow)}$</th>
<th style="text-align: center;">Type II Pro $\uparrow$</th>
<th style="text-align: center;">Type II Anti $\uparrow$</th>
<th style="text-align: center;">Gap $_{(\downarrow)}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Supervised Baseline</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">E2E (Lee et al., 2017)</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">11.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 Few-Shot: Bias Distribution in the Prompt (16 shots)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Balanced</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: center;">Type I - Pro</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">12.2</td>
</tr>
<tr>
<td style="text-align: center;">Type II - Pro</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: center;">Type I - Anti</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">-30.0</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">6.3</td>
</tr>
<tr>
<td style="text-align: center;">Type II - Anti</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 Few-Shot: Prompt Ordering (16 shots, Balanced)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Randomly Shuffled</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: center;">Pro in the end</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">11.9</td>
</tr>
<tr>
<td style="text-align: center;">Anti in the end</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">8.4</td>
</tr>
</tbody>
</table>
<p>Table 3: GPT-3 results on WinoBias. The bias gap between Pro-Bias subsets and Anti-Bias subsets indicates the extent of gender biases exhibited by the model (smaller-scale gap is better). Results of the baseline ECE model (Lee et al., 2017) are taken from the WinoBias paper (Zhao et al., 2018). Prompt with balanced pro-bias and anti-bias answers best shrinks the bias gap, and randomly shuffling the demos is better than putting one group at the end.</p>
<h1>3.1 The Case of Gender Bias: WinoBias</h1>
<p>Dataset We start with the WinoBias dataset (Zhao et al., 2018) which uses templates to check whether models are more likely to assign gender pronouns to stereotypical occupations. WinoBias has two types of examples: Type I are ambiguous, challenging examples that require world knowledge; Type II can be resolved using only syntactic information. For each type, examples either confirm (pro-bias) or challenge (anti-bias) societal biases. Ideally, coreference accuracy should be similar on the pro-bias and anti-bias subsets (small gaps).</p>
<p>Prompt Design For ease of evaluation, we re-format the WinoBias dataset into a questionanswering format where we provide the original sentence and then add a question "What does the pronoun refer to in the above sentence?" ("the pronoun" is replaced with the actual pronoun in the sentence) and we use the answer exact match as the evaluation metric. We randomly sample examples from the training set as the prompt and then evaluate on the Pro and Anti test sets.</p>
<p>Which Examples Should be in the Prompt We compare: 1) sampling four demo examples from each of the Type I-Pro, Type I-Anti, Type II-Pro, and Type II-Anti subsets (Balanced), which results in a total of 16 demos; 2) sampling 16 demo examples from a single subset. The balanced prompt induces the least biased predictions (Table 3, second block). In particular, if we only keep Pro-Bias examples, the model will favor Pro-Bias predictions (especially on Type I test examples because they are more ambiguous while Type II examples have clear syntax cues).</p>
<p>How Should Examples be Ordered We compare: 1) randomly shuffling the demo examples; and 2) putting all Pro-Bias or Anti-Bias examples at the end of the prompt. Random shuffling reduces bias gaps most (Table 3, third block). Interestingly, putting either Pro-Bias or Anti-Bias examples at the end increases bias gaps.</p>
<h3>3.2 Broader Social Dimensions: BBQ</h3>
<p>Dataset We now explore additional social dimensions using BBQ (Parrish et al., 2022), which tests social biases against people from nine protected classes (age, disability status, gender identity, nationality, physical appearance, race, religion, socio-economic status, sexual orientation). BBQ examples are in sets of four multiple-choice questions. Two questions are ambiguous-the context lacks evidence to point to an answer. Two other questions in each set have a context that points to an unambiguous answer: the model should choose the correct answer rather than abstaining. Each question has three options: a pro-bias answer that supports the stereotype, an anti-bias answer</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">Ambig Acc $_{\uparrow}$</th>
<th style="text-align: center;">DisAmbig Acc $_{\uparrow}$</th>
<th style="text-align: center;">Ambig Bias Score $_{\downarrow}$</th>
<th style="text-align: center;">DisAmbig Bias Score $_{\downarrow}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Supervised Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-Base (123M)</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">4.7</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-Large (354M)</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa-Base (184M)</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">2.9</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa-Large (435M)</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">-1.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 Few-Shot Prompting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">0-shot</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">4.4</td>
</tr>
<tr>
<td style="text-align: left;">BBQ Balanced</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: left;">BBQ Ambig-Neutral</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">-3.5</td>
</tr>
<tr>
<td style="text-align: left;">BBQ Ambig-Pro-Bias</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">BBQ Ambig-Anti-Bias</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">3.1</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on the BBQ dataset. For GPT-3 prompting, apart from the zero-shot result, others use 8 -shots. For accuracy (Acc), higher value is better; for bias score, lower magnitude is better. The balanced prompt best trades-off accuracy and bias for GPT-3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">Ambig Acc $_{\uparrow}$</th>
<th style="text-align: center;">DisAmbig Acc $_{\uparrow}$</th>
<th style="text-align: center;">Ambig Bias Score $_{\downarrow}$</th>
<th style="text-align: center;">DisAmbig Bias Score $_{\downarrow}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Before Intervention</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">After Intervention</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">3.8</td>
</tr>
</tbody>
</table>
<p>Table 5: The impact of adding natural language intervention to a biased prompt on GPT-3. Adding an instruction leads the model to make more neutral predictions and reduce bias scores.
that counters the stereotype, and a neutral answer (e.g., "Cannot be determined."). When reporting results, we report: 1) accuracy on ambiguous and unambiguous questions (higher is better); 2) bias scores on ambiguous and disambiguated questions (smaller scale is better). Intuitively, the bias score measures the frequency of the model predicting a pro-bias answer when it makes a non-unknown prediction, where $0 \%$ means no bias and $100 \%$ means always following the bias.</p>
<p>Answer Distribution Affects Biases We sample the same number of demo examples with different answer distributions (Table 4). With zero-shot inference, GPT-3's low bias is balanced by low accuracy. A balanced prompt-where we sample both ambiguous and disambiguated questions with pro-bias and anti-bias answers-increases accuracy while maintaining low bias scores. Next, when we only present ambiguous questions with neutral answers in the prompt, GPT-3 can easily pick up this pattern to always predict the neutral answer, resulting in extremely low bias scores and high accuracy on ambiguous questions, but sacrificing accuracy on the disambiguated questions. Lastly, we experiment with using biased prompts where we only sample ambiguous questions in the prompt, and we set their answers to either all bias-supporting or bias-countering. As shown in the last two rows in Table 4, both cases lead to a biased model that gets high bias scores on ambiguous questions. This shows that when the demo examples are biased towards certain demographic groups, GPT-3 will pick up such biases, and thus one should prefer a balanced prompt in downstream applications.</p>
<p>Natural Language Intervention Mitigates Biases Lastly, we explore the possibility of using natural language instructions to guide the predictions of GPT-3 towards better fairness. We use the Ambig-Pro-Bias prompt and prepend the additional instruction: "We should treat people from different socioeconomic statuses, sexual orientations, religions, races, physical appearances, nationalities, gender identities, disabilities, and ages equally. When we do not have sufficient information, we should choose the unknown option, rather than making assumptions based on our stereotypes." to every test question. In Table 5, after adding this intervention, GPT-3 prefers the neutral answer much more often, leading to a much higher accuracy on the ambiguous questions, and at the same time significantly reducing the bias scores. This shows that GPT-3 is sensitive to such natural language intervention. This is in contrast with smaller language models such as RoBERTa (Zhao et al., 2021a), which is more rigid. This finding offers a new way for effectively reducing social biases.</p>
<p>Takeaway (i) Demographic distribution of answers has huge impact on models' biases, sampling balanced prompt best reduces biases. (ii) Randomly shuffling the demos leads to smaller biases than putting all pro-bias or anti-bias examples in the end. (iii) Specifying intended model behaviors such as being fair via instructions in the prompt can effectively guide model predictions.</p>
<h1>4 FACET 3: UNCERTAINTY CALIBRATION</h1>
<p>No language model can ever be perfect, and to safely use these imperfect models, users must decide when to trust model predictions to avoid mistrusting wrong predictions, especially in high-stake settings. This requires another facet of reliability - uncertainty calibration: providing confidence scores for each model prediction that accurately reflects the likelihood of the predicted answer being correct.</p>
<h3>4.1 Evaluation Setup</h3>
<p>Experiment Setup We study the setting of freeform answer generation: given a test question, we prompt the model to generate an answer string and obtain its confidence score (more below), and we evaluate the correctness of the generated answer based on exact match with the gold answer. We experiment with three QA datasets: NQ, TriviaQA, and HotpotQA. In all cases, we adopt the closedbook setting (i.e., no additional evidence passages). We focus on intrinsic calibration results: using raw confidence scores rather than post-hoc calibration, which requires an additional dev set for parametertuning. We report the standard calibration metric expected calibration error (ECE), the reliability diagram, ${ }^{4}$ and selective prediction results where we rank all predictions by their confidence and see if the accuracy of the most confident predictions is significantly higher than the average accuracy. Because of ECE's known flaws due to its bucketing mechanism (Si et al., 2022), so we also report the Brier score (Brier, 1950). Our baseline is a supervised QA model—DPR-BERT (Si et al., 2022)with a dense passage retriever (DPR; Karpukhin et al., 2020) to feed the top passages into a BERT reader model for answer extraction. We follow their joint calibration setup for scoring predictions of DPR-BERT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{Acc}_{\uparrow}$</th>
<th style="text-align: center;">$\mathrm{ECE}_{\downarrow}$</th>
<th style="text-align: center;">Brier $_{\downarrow}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NQ</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DPR-BERT (110M)</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 LM Prob</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 Self-Con</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">20.1</td>
</tr>
<tr>
<td style="text-align: center;">TriviaQA (TQA)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 LM Prob</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">15.9</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 Self-Con</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">16.5</td>
</tr>
<tr>
<td style="text-align: center;">HotpotQA (HQA)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 LM Prob</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">23.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 Self-Con</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">19.9</td>
</tr>
<tr>
<td style="text-align: center;">Different Prompts on NQ w/ LM-Prob</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 2-shot</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 4-shot</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">21.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 8-shot</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">25.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 16-shot</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 64-shot</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">22.1</td>
</tr>
<tr>
<td style="text-align: center;">OOD Prompts w/ LM-Prob</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TQA i.i.d. Prompt</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">15.9</td>
</tr>
<tr>
<td style="text-align: center;">NQ Prompt on TQA</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">15.2</td>
</tr>
<tr>
<td style="text-align: center;">DPR-BERT NQ $\rightarrow$ TQA</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;">HQA i.i.d. Prompt</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">23.5</td>
</tr>
<tr>
<td style="text-align: center;">NQ Prompt on HQA</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">25.2</td>
</tr>
<tr>
<td style="text-align: center;">DPR-BERT NQ $\rightarrow$ HQA</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">42.4</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy, ECE, and Brier scores of GPT-3 and the DPR-BERT baseline. GPT-3 is better calibrated than supervised DPR-BERT on both in-domain and OOD settings.</p>
<p>Confidence Scoring We compare two ways of estimating confidence for GPT-3 predictions. LMProb: the (normalized) language model probability, also equivalent to the reciprocal of perplexity, is Conf $\equiv P\left(w_{1} w_{2} \ldots w_{n}\right) \S$ where $w_{1} w_{2} \ldots w_{n}$ are the generated tokens in the answer. SelfCon: We also explore using self-consistency (Wang et al., 2023) to obtain confidence measures. Following Wang et al. (2023), during decoding we set a high temperature value ( 0.7 ) and sample 10 times for a set of different predictions. Among all the generated answers, we take the most frequent answer as the final prediction and its frequency as the confidence score.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">DPR-BERT NQ</th>
<th style="text-align: center;">LM-Prob NQ</th>
<th style="text-align: center;">Self-Con NQ</th>
<th style="text-align: center;">LM-Prob TriviaQA</th>
<th style="text-align: center;">LM-Prob HotpotQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: left;">$50 \%$</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">47.6</td>
</tr>
<tr>
<td style="text-align: left;">$10 \%$</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">68.1</td>
</tr>
</tbody>
</table>
<p>Table 7: Selective prediction results. All numbers represent the accuracy (EM) at the corresponding coverage thresholds. For example, $100 \%$ means performance on the entire test set while $10 \%$ means the performance on the most confident $10 \%$ predictions. Both LM-Prob and Self-Con allow effective selective prediction with high accuracy on the most confident subsets. More results in Appendix D.</p>
<h1>4.2 ReSults</h1>
<p>While still imperfect, GPT-3 (with either LM-Prob or Self-Con) is better calibrated than supervised DPR-BERT (Table 6). Most calibration errors come from overconfidence where the predictions' confidence is higher than expected accuracy. Interestingly, while increasing the number of examples in the prompt improves accuracy, the calibration does not improve. For example, the 2 -shot accuracy is 5.8 points worse than 64 -shot but better calibrated. Moreover, while OOD transfer is a challenge for supervised models' calibration (tends to be overconfident on OOD test sets), GPT-3 has similar calibration regardless of the source of examples.</p>
<p>The selective prediction results show confidence scores can rank model predictions (Table 7): the most confident predictions have much higher accuracy. Moreover, GPT-3's confidence scores are more discriminative. For example, while the average accuracy on NQ is similar between GPT-3 and DPR-BERT, the top $10 \%$ predictions get an accuracy of $83.1 \%$ while for DPR-BERT it is only $60.1 \%$. Such selective prediction can be very useful in practical settings, for example, we only trust the most confident predictions from the model and ask humans to verify the rest, making the use of GPT-3 more reliable.</p>
<p>Takeaway (i) Language model probability and self-consistency frequency can produce better calibration on GPT-3 than a supervised DPR-BERT model, especially on OOD test sets. (ii) Increasing the number of demos in the prompt improves accuracy but not necessarily calibration. (iii) We can perform effective selective prediction based on GPT-3 confidence scores.</p>
<h2>5 Facet 4: Factuality Via Knowledge Updating</h2>
<p>Although large language models store vast knowledge in their parameters (Petroni et al., 2019), the model is sometimes wrong or out of date, rendering them unreliable for knowledge-intensive tasks. In this section, we improve this factuality aspect of reliability by improving the prompting methods.</p>
<h3>5.1 Memorization vs Updating</h3>
<p>The larger a model, the more it can memorize (Carlini et al., 2023), this raises the concern of whether large models like GPT-3 can forget memorized knowledge when needed and update its knowledge.</p>
<p>Experiment Setup Our evaluation setup is inspired by Longpre et al. (2021), who reason about counterfactual scenarios. Specifically, we sample 36 K and 18 K questions from NQ and SQuAD's training splits (respectively, using the splits provided by MRQA). We use 16 demo examples from each dataset as the prompt for closed-book QA first. We assume that if GPT-3 gets the answer to the question right in the closed-book setting, then it has already memorized that piece of knowledge. We keep the set of questions where GPT-3 got right in the closed-book setting (for NQ, 21188 questions; for SQuAD, 7035 questions), and for these questions, we append a counterfactual passage supporting an alternative answer. We construct these counterfactual using the entity-swap from Longpre et al. (2021): for each question, take its gold passage and replace the gold answer entity with another entity with the same type sampled from the same QA corpus. After such entity substitution, the counterfactual passages support the substituted answer instead of the original answer. Our expectation is that the model should generate this updated answer given this counterfactual passage, instead of its original memorized answer. We randomly sample 16 demo examples as the prompt</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">NQ</th>
<th style="text-align: left;">TriviaQA</th>
<th style="text-align: left;">SQuAD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DPR-BERT (supervised)</td>
<td style="text-align: left;">41.5</td>
<td style="text-align: left;">56.8</td>
<td style="text-align: left;">24.1</td>
</tr>
<tr>
<td style="text-align: left;">Atlas-11B (64-shot)</td>
<td style="text-align: left;">42.4</td>
<td style="text-align: left;">74.5</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 Closed-Book</td>
<td style="text-align: left;">40.6</td>
<td style="text-align: left;">73.6</td>
<td style="text-align: left;">20.2</td>
</tr>
<tr>
<td style="text-align: left;">+ Contriever top-5</td>
<td style="text-align: left;">$43.3(61.8 \%)$</td>
<td style="text-align: left;">$75.6(69.6 \%)$</td>
<td style="text-align: left;">$31.7(48.8 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">+ Contriever top-10</td>
<td style="text-align: left;">$44.2(70.5 \%)$</td>
<td style="text-align: left;">$76.0(75.1 \%)$</td>
<td style="text-align: left;">$34.0(57.7 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 9: GPT-3 16-shot prompting results on open-domain QA datasets. We use Exact Match as the metric. For Contriever results, we additionally show the retriever's recall in brackets. Adding retrieval to GPT-3 consistently improves QA accuracy.
and we use triples of the answer-substituted passage, the question, and the substitution answers ( $\left\langle P^{\prime}\right.$, $\left.Q, A^{\prime}\right\rangle$ ) in the prompt to specify the task of performing reading comprehension based on the passage.</p>
<p>Measuring How Well can GPT-3 Update its Knowledge There are three possible outcomes: 1) the model retains the memorized answer; 2) the model predicts the updated answer (i.e., the substitution entity in the counterfactual passage); 3) the model predicts some other answer. We measure the proportion of those outcomes and hope models to update answers more often. For a baseline, we include results from Longpre et al. (2021): a fine-tuned T5 reader-trained on NQ and NewsQA-model with a DPR retriever.</p>
<p>Results As shown in Table 8, we find that when prompting with counterfactual triples $\left(\left\langle P^{\prime}, Q, A^{\prime}\right\rangle\right.$ ), GPT-3 can update about $85 \%$ of the time, much higher than the supervised baseline (Table 8). Comparing Text-Davinci-001 and Text-Curie001, the larger model also updates better to new answers in counterfactual passages.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Retain $\downarrow$</th>
<th style="text-align: center;">Update $\uparrow$</th>
<th style="text-align: center;">Other $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NQ with Code-Davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">T5 (770M, supervised)</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$33 \%$</td>
<td style="text-align: center;">$47 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">$4.5 \%$</td>
<td style="text-align: center;">$85.4 \%$</td>
<td style="text-align: center;">$10.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD with Code-Davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">$7.1 \%$</td>
<td style="text-align: center;">$84.8 \%$</td>
<td style="text-align: center;">$8.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">NQ with different GPT-3 models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Text-Davinci-001 (175B)</td>
<td style="text-align: center;">$7.2 \%$</td>
<td style="text-align: center;">$57.9 \%$</td>
<td style="text-align: center;">$34.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Text-Curie-001 (6.7B)</td>
<td style="text-align: center;">$14.8 \%$</td>
<td style="text-align: center;">$40.0 \%$</td>
<td style="text-align: center;">$45.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: In-context knowledge updating results for memorized answers in NQ and SQuAD. When giving counterfactual examples in the prompt, GPT-3 updates its answers around $85 \%$ of the time, much higher compared to the supervised model. Moreover, larger models are better at in-context knowledge updating.</p>
<h1>5.2 RETRIEVAL-AUGMENTED OPEN-DOMAIN QA</h1>
<p>Large language models can answer closed-book QA from the model's stored knowledge (Roberts et al., 2020). However, a prompt can judiciously add more relevant information especially given our findings from the previous section that GPT-3 can update its knowledge with information in the prompt. We thus explore improving factual QA via retrieval-augmented prompts.</p>
<p>Approach We use the unsupervised Contriever model (Izacard et al., 2022a): for a test question, retrieve the top passages from the Wikipedia dump, concatenate them, and prepend them to the test question. Since the context is length-limited, we only prepend retrieved passages to the test question, not the demo examples, so the demo examples are only in the form of question-answer pairs. We compare this retriever-augmented approach with a closed-book baseline where we do not add the retrieved passages in the prompt. The demo examples used for both the retrieval-augmented prompting and closed-book prompting are exactly the same.</p>
<p>Results Adding retrieved passages into the prompt consistently boosts GPT-3 performance on all three open-domain QA datasets (Table 9), with particularly large gains on SQuAD (possibly because answers in SQuAD are spans from Wikipedia passages rather than free-form answers). Moreover, having better recall for retrieval gives better performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">Sub-Q1</th>
<th style="text-align: center;">Sub-Q2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">$18.0 / 28.1$</td>
<td style="text-align: center;">$40.1 / 49.6$</td>
<td style="text-align: center;">$43.3 / 58.4$</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">$25.2 / 35.2$</td>
<td style="text-align: center;">$30.3 / 37.4$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">CoT + Human Sub-Q1</td>
<td style="text-align: center;">$30.0 / 42.3$</td>
<td style="text-align: center;">$44.2 / 54.1$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">CoT + Human Sub-Q1 + Gold Sub-A1</td>
<td style="text-align: center;">$44.3 / 59.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 10: Results on HotpotQA as well as the decomposed sub-questions (we report EM / F1). Incorporating decomposed sub-questions in the prompt makes the model adjust its follow-up step predictions and significantly improves the overall answer accuracy.</p>
<h1>5.3 REASONING-AUGMENTED MULTI-HOP QA</h1>
<p>The above experiments demonstrate the effectiveness of ensuring GPT-3's factuality via in-context knowledge updating; however, it is mostly constrained on simple single-hop factual questions. In real-world applications, many user queries are multi-hop - they require multiple steps of reasoning over factual knowledge. Ensuring factuality in multi-hop questions involves additional challenges: models may fail because they derive the reasoning steps wrongly. To tackle this more challenging multi-hop setting, we study whether it is possible to improve GPT-3's multi-hop reasoning by incorporating human-written question decomposition in the prompt.</p>
<p>HotpotQA and Decomposed Sub-Questions We use the HotpotQA dataset (Yang et al., 2018) for our experiments, which consists of multi-hop questions that require at least two steps of reasoning. We use the question decomposition from Tang et al. (2021), where HotpotQA questions are annotated as decomposed (single-hop) sub-questions with corresponding intermediate answers.</p>
<p>Baseline: Chain-of-Thought Prompting Chain-of-Thought (CoT) prompting (Wei et al., 2022b) is a new prompting method tailored to multi-step questions, which we adopt in our experiments as a baseline, where we provide human-written reasoning steps for all demo examples to induce similar reasoning on test examples. We measure accuracy of GPT-3's final answer predictions on HotpotQA (Overall) as well as on the decomposed single-hop sub-questions. From the first row of Table 10, we see that standard prompting achieves higher accuracy on the single-hop sub-questions than the entire multi-hop questions as expected. CoT generates the entire reasoning chain along with its decomposed sub-questions and the intermediate answers to sub-questions, where the accuracy on the multi-hop questions is higher than standard prompting (second row of Table 10).</p>
<p>Incorporating Human Decomposition Instead of relying on GPT-3 itself to generate reasoning chains, we add the human-written question decomposition into the prompt. When adding the human-written sub-questions for the first step of reasoning (second last row of Table 10), we see a clear improvement in both the overall multi-hop QA accuracy as well as the sub-question accuracy. Moreover, when we further add the human-written QA pair of the first decomposed question in the reasoning chain (last row of Table 10), there is an even larger performance gain on the multi-hop QA performance. This shows that GPT-3 is able to adapt to the question decomposition information from humans and deduce the subsequent reasoning steps to eventually obtain the correct answers, offering better control and reliability.</p>
<p>Takeaway (i) Adding retrieved evidence passages can improve GPT-3 performance on factual QA. (ii) GPT-3 can update its knowledge when provided passages conflicting with its memorized knowledge. (iii) Incorporating human-written question decomposition corrects the reasoning chains of GPT-3 and improves performance on multi-hop QA.</p>
<h2>6 CONCLUSION</h2>
<p>Our work systematically studies the reliability of GPT-3 from four key facets: generalizability, fairness, calibration, and factuality. We develop effective prompting strategies to make GPT-3 outperform supervised models by large margins on these facets. Our work reveals new insights of LLMs and provides practical recommendations for users of GPT-3. We hope our work can inspire more future work to: (1) examine more facets of reliability, such as avoiding harmful generations; (2) apply the prompting methods in this paper to more real-world applications, such as incorporating human feedback for collaborative multi-step planning; (3) further explore more effective prompting strategies to improve reliability, such as post-hoc calibration on language model probabilities.</p>
<h1>ETHICAL STATEMENT</h1>
<p>Ethical Use of GPT-3 The goal of this project is to avoid the potential harm of GPT-3 and all of our GPT-3 experiments are motivated to better study and improve reliability. We believe our experiments and findings can improve the reliability and allow safer use of the model. In particular, our section on social biases and fairness is a key aspect of the ethical use of GPT-3. We presented evidence that the model exhibits biased predictions, especially when the demo examples in the prompt have a skewed demographic distribution. Although we explored ways of mitigating these biases, the model is still far from perfect, and there is much more work needed to further improve its fairness. We take our work as an initial step towards more ethical use of GPT-3.</p>
<p>Limitations of This Work We note several limitations of this work and suggest a list of open questions for future work.</p>
<ul>
<li>Other reliability facets: In this work, we covered four key facets of reliability, but there are surely other facets that we may have missed. For example, combatting adversarial examples identified via human or AI red-teaming (Ganguli et al., 2022; Branch et al., 2022; Perez et al., 2022), detecting and handling malicious prompts such as prompt injection ${ }^{5}$, and avoiding toxic and hallucinated generations (Gehman et al., 2020; Gao et al., 2022).</li>
<li>Methods for improving reliability: Although we have taken initial steps and discovered some effective prompting strategies for these reliability facets, readers should not take this work as evidence that GPT-3 is already reliable and ready for deployment. In fact, our experiments indicate ample room for further improvement, for example in reducing social biases and improving calibration. We hope this work inspires more future work that develops more effective strategies to make LLMs reliable.</li>
<li>Analysis to understand model behaviors: While we have found interesting properties of GPT-3, it remains unclear what exactly caused these behaviors. For example, if the small generalization gap due to the use of prompting, or the training data, or the training objectives or model architecture? When GPT-3 is sensitive to the prompt in debiasing, is it triggered by certain keywords or phrases? Why ordering anti-bias examples at the end of the prompt does not lead to the recency bias (Zhao et al., 2021b) but rather still incurs strong biases against minority groups? Can we attribute model behaviors to the pretraining data or interpret model attention patterns? These analysis can potentially help us better understand how and why prompting works and therefore allow us to better leverage LLMs.</li>
</ul>
<h2>ACKNOWLEDGMENT</h2>
<p>We thank Jason Phang, Ziyi Yang, Dan Friedman, Sewon Min, Jieyu Zhao, He He, Alicia Parrish, Chen Zhao, Shi Feng, Han Guo, Weijia Shi, Jungo Kasai, Xi Ye, Su Lin Blodgett, Trista Cao, Ekin Akyürek, Leo Boytsov, Aishwarya Kamath, Weijia Xu, Yankai Lin, Xiaozhi Wang, Zhengyan Zhang, and many other friends from UMD CLIP and the Azure AI team at Microsoft for their helpful discussion and feedback.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>REFERENCES</h1>
<p>Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect them. In EMNLP, 2021.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021.</p>
<p>BIG-Bench. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv, 2022. URL https://arxiv.org/abs/2206.04615.</p>
<p>Ben Bogin, Shivanshu Gupta, and Jonathan Berant. Unobserved local structures make compositional generalization hard. In EMNLP, 2022.</p>
<p>Hezekiah J. Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del Castillo Iglesias, Ron Heichman, and Ramesh Darwishi. Evaluating the susceptibility of pretrained language models via handcrafted adversarial examples. ArXiv, 2022. URL https: //arxiv.org/abs/2209.02128.</p>
<p>Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78:1-3, 1950.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.</p>
<p>Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In EMNLP, 2021.</p>
<p>Yang Trista Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Varun Kumar, J. Dhamala, and Aram Galstyan. On the intrinsic and extrinsic fairness evaluation metrics for contextualized language representations. In $A C L, 2022$.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, and Chiyuan Zhang. Quantifying memorization across neural language models. In ICLR, 2023.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv, 2021a. URL https: //arxiv.org/abs/2107.03374.</p>
<p>Wenhu Chen, Xinyi Wang, and William Yang Wang. A dataset for answering time-sensitive questions. In NeurIPS, 2021b.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,</p>
<p>Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuézhí Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. ArXiv, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. In EMNLP, 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019a.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019b.</p>
<p>Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Workshop on Machine Reading for Question Answering, 2019.</p>
<p>Dan Friedman, Ben Dodge, and Danqi Chen. Single-dataset experts for multi-dataset question answering. In EMNLP, 2021.</p>
<p>Deep Ganguli, Liane Lovitt, John Kernion, Amanda Askell, Yushi Bai, Saurav Kadavath, Benjamin Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zachary Dodds, T. J. Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom B. Brown, Nicholas Joseph, Sam McCandlish, Christopher Olah, Jared Kaplan, and Jack Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. ArXiv, 2022. URL https://arxiv.org/abs/2209.07858.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, N. Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. Attributed text generation via post-hoc research and revision. ArXiv, 2022. URL https://arxiv.org/abs/2210. 08726 .</p>
<p>Matt Gardner, Yoav Artzi, Jonathan Berant, Ben Bogin, Sihao Chen, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Eric Wallace, Ally Zhang, and Ben Zhou. Evaluating models' local decision boundaries via contrast sets. In Findings of EMNLP, 2020.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. In Findings of EMNLP, 2020.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In ICML, 2017.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In NAACL, 2018.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrievalaugmented language model pre-training. In ICML, 2020.</p>
<p>Camille Harris, Matan Halevy, Ayanna M. Howard, Amy Bruckman, and Diyi Yang. Exploring the role of grammar and word choice in bias toward african american english (aae) in hate speech classification. 2022 ACM Conference on Fairness, Accountability, and Transparency, 2022.</p>
<p>Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Xiaodong Song. Pretrained transformers improve out-of-distribution robustness. In ACL, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In NeurIPS, 2021a.</p>
<p>Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety. ArXiv, 2021b. URL https://arxiv.org/abs/2109.13916.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022a. URL https://openreview.net/ pdf?id=jKN1pXi7b0.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. ArXiv, 2022b. URL https://arxiv.org/abs/2208.03299.</p>
<p>Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In EMNLP, 2017.</p>
<p>Zhengbao Jiang, J. Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962-977, 2021.</p>
<p>Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In AAAI, 2020.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, T. J. Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zachary Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yushi Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, John Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom B. Brown, Jack Clark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah, and Jared Kaplan. Language models (mostly) know what they know. ArXiv, 2022. URL https://arxiv.org/ abs/2207.05221.</p>
<p>Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, 2020. URL https://arxiv.org/abs/2001.08361.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP, 2020.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentarou Inui. RealTime QA: What's the answer right now? ArXiv, 2022. URL https://arxiv.org/abs/2207.13332.</p>
<p>Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic data. In $I C L R, 2020$.</p>
<p>Najoung Kim and Tal Linzen. COGS: A compositional generalization challenge based on semantic interpretation. In EMNLP, 2020.</p>
<p>Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard L. Phillips, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In ICML, 2021.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022.</p>
<p>Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference resolution. In EMNLP, 2017.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, 2021.</p>
<p>Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In CoNLL, 2017.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In NeurIPS, 2020.</p>
<p>Hongyu Li, Xiyuan Zhang, Y. Liu, Yiming Zhang, Xiangyang Zhou, and Jing Liu. D-net: A pretraining and fine-tuning framework for improving the generalization of machine reading comprehension. In EMNLP, 2019.</p>
<p>Linyang Li, Ruotian Ma, Qipeng Guo, X. Xue, and Xipeng Qiu. Bert-attack: Adversarial attack against bert using bert. In EMNLP, 2020.</p>
<p>Stephanie C. Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. ArXiv, 2022a. URL https://arxiv.org/abs/2205.14334.</p>
<p>Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In ACL, 2022b.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, 2019. URL https://arxiv.org/abs/1907.11692.</p>
<p>Shayne Longpre, Yi Lu, Zhucheng Tu, and Christopher DuBois. An exploration of data augmentation and sampling techniques for domain-agnostic question answering. In EMNLP, 2019.</p>
<p>Shayne Longpre, Kartik Kumar Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In EMNLP, 2021.</p>
<p>Li Lucy and David Bamman. Gender and representation bias in gpt-3 generated stories. In NUSE, 2021.
R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In ACL, 2019.</p>
<p>Sabrina J. Mielke, Arthur D. Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents' overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872, 2022.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In EMNLP, 2022.</p>
<p>Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at scale. In $I C L R, 2021$.</p>
<p>Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. Memorybased model editing at scale. In ICML, 2022.</p>
<p>Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. In $A C L, 2021$.</p>
<p>Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, 2015:2901-2907, 2015.</p>
<p>Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. Crows-pairs: A challenge dataset for measuring social biases in masked language models. In EMNLP, 2020.</p>
<p>Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Sam Bowman. BBQ: A hand-built bias benchmark for question answering. In Findings of ACL, 2022.</p>
<p>Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nathan McAleese, and Geoffrey Irving. Red teaming language models with language models. ArXiv, 2022. URL https://arxiv.org/abs/2202.03286.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. Language models as knowledge bases? In EMNLP, 2019.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks. In NAACL, 2021.</p>
<p>John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 1999.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis only baselines in natural language inference. In SemEval, 2018.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training gopher. ArXiv, 2021. URL https://arxiv.org/abs/2112.11446.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically equivalent adversarial rules for debugging nlp models. In $A C L, 2018$.</p>
<p>Adam Roberts, Colin Raffel, and Noam M. Shazeer. How much knowledge can you pack into the parameters of a language model? In EMNLP, 2020.</p>
<p>Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. In NAACL, 2018.</p>
<p>Chenglei Si, Shuohang Wang, Min-Yen Kan, and Jing Jiang. What does bert learn from multiplechoice reading comprehension datasets? ArXiv, abs/1910.12391, 2019. URL https:// arxiv.org/abs/1910.12391.</p>
<p>Chenglei Si, Ziqing Yang, Yiming Cui, Wentao Ma, Ting Liu, and Shijin Wang. Benchmarking robustness of machine reading comprehension models. In Findings of ACL, 2021a.</p>
<p>Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. Better robustness by more coverage: Adversarial training with mixup augmentation for robust fine-tuning. In Findings of ACL, 2021b.</p>
<p>Chenglei Si, Chen Zhao, Sewon Min, and Jordan L. Boyd-Graber. Revisiting calibration for question answering. In Findings of EMNLP, 2022.</p>
<p>Irene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with values-targeted datasets. In NeurIPS, 2021.</p>
<p>Alon Talmor and Jonathan Berant. MultiQA: An empirical investigation of generalization and transfer in reading comprehension. In $A C L, 2019$.</p>
<p>Samson Tan, Shafiq R. Joty, Min-Yen Kan, and Richard Socher. It's morphin' time! combating linguistic discrimination with inflectional perturbations. In $A C L, 2020$.</p>
<p>Samson Tan, Shafiq R. Joty, K. Baxter, Araz Taeihagh, G. Bennett, and Min-Yen Kan. Reliability testing for natural language processing systems. In $A C L, 2021$.</p>
<p>Samson Tan, Araz Taeihagh, and Kathy Baxter. The risks of machine learning systems. ArXiv, 2022. URL https://arxiv.org/abs/2204.09852.</p>
<p>Yixuan Tang, Hwee Tou Ng, and Anthony K. H. Tung. Do multi-hop question answering systems know how to answer the single-hop sub-questions? In EACL, 2021.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a largescale dataset for fact extraction and verification. In NAACL, 2018.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018.</p>
<p>Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial GLUE: A multi-task benchmark for robustness evaluation of language models. In NeurIPS, 2021.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In $I C L R, 2023$.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In NeurIPS, 2022b.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP, 2018.</p>
<p>Xi Ye and Greg Durrett. Can explanations be useful for calibrating black box models? In $A C L$, 2022.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: Open pre-trained transformer language models. ArXiv, 2022. URL https: //arxiv.org/abs/2205.01068.</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling. In NAACL, 2019.</p>
<p>Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In NAACL, 2018.</p>
<p>Jieyu Zhao, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and Kai-Wei Chang. Ethical-advice taker: Do language models understand natural language interventions? In Findings of ACL, 2021a.</p>
<p>Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In ICML, 2021b.</p>
<p>Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Brooke Anderson, and Diyi Yang. Value: Understanding dialect disparity in nlu. In $A C L, 2022$.</p>
<h1>APPENDIX</h1>
<h2>A MORE Related Work</h2>
<p>Robustness to Distribution Shifts. Machine learning models are known to overfit their training distribution and often suffer performance degradation when the test distribution differs from the training distribution. In the case of language models, various forms of distribution shifts have been studied. For example, domain shifts pose great challenges for LLMs on question answering (Talmor \&amp; Berant, 2019; Fisch et al., 2019) and text classification (Hendrycks et al., 2020; Arora et al., 2021); various forms of adversarial attacks can break LLMs even by just strategic synonym substitution, paraphrase, or distractor insertion (Jin et al., 2020; Li et al., 2020; Ribeiro et al., 2018; Si et al., 2019; 2021a; Jia \&amp; Liang, 2017; Si et al., 2021b); LLMs have been shown to exploit shortcuts or spurious correlations in the training data which fail on counter test examples (McCoy et al., 2019; Zhang et al., 2019; Poliak et al., 2018; Gururangan et al., 2018); LLMs also fail on new compositional structures that are not observed during traing (Kim \&amp; Linzen, 2020; Keysers et al., 2020; Bogin et al., 2022). In real-world settings, various forms of distribution shifts can happen and reliable models should perform well even when encountering such out-of-distribution (OOD) examples. Intuitively, incontext few-shot prompting should suffer less OOD degradation since the pretrained parameters are preserved, unlike the case of supervised finetuning. We perform a series of empirical evaluations on domain shift, curated challenge sets, and spurious correlation to validate this hypothesis.</p>
<p>Bias and Fairness. Language models producing toxic or biased content can cause severe harm especially to the groups being biased against (Bender et al., 2021). A series of benchmarks have been developed to show that LLMs can generate toxic outputs (Gehman et al., 2020), contain gender biases (Rudinger et al., 2018; Zhao et al., 2018) and other categories of social biases (Nangia et al., 2020; Nadeem et al., 2021; Parrish et al., 2022), perform poorly against minority demographic groups (Koh et al., 2021; Harris et al., 2022) or dialectical variations (Ziems et al., 2022; Tan et al., 2020). Ideally, LLMs should not exhibit biased behaviors and not discriminate against any group. While many of these evaluations focus on evaluating the internal representation of LLMs in a zeroshot setting or evaluating the biases on specific downstream applications in a supervised setting, it remains unclear how these biases change under different prompting schemes in the few-shot setting, which will be the focus of our analysis. A closely related work is Lucy \&amp; Bamman (2021) which study representation biases in GPT-3 generated stories. We instead evaluate on the downstream tasks of coreferece resolution and question answering. Apart from few-shot prompting, Solaiman \&amp; Dennison (2021) proposed a general method to align language models with human values, but it involves expensive iterative training.</p>
<p>Uncertainty Calibration. No model can ever be perfect, and so it is crucial for users to be able to identify model mistakes, especially in high-stage settings where trusting wrong model predictions can cause severe harm. One important way to help identify wrong model predictions is by obtaining well-calibrated confidence scores for model predictions. By definition, a calibrated confidence (probability) score should match the expected accuracy of the prediction (Platt, 1999; Naeini et al., 2015; Guo et al., 2017). In this way, users can put more trust in highly-confidence predictions and discard low-confidence predictions. While various methods have been proposed to obtain confidence scores and perform post-hoc calibration for language models (Jiang et al., 2021; Desai \&amp; Durrett, 2020; Ye \&amp; Durrett, 2022), they are mostly focused on classification settings rather than free-form generation, which is more common for the use of GPT-3. In this work, we explore two simple (but surprisingly effective) ways of obtaining confidence scores for GPT-3's generated answers and we analyse the impact of scaling as well as prompt design. For studying calibration of GPT-3 style LLMs, Lin et al. (2022a) explore the idea of expressing uncertainty in verbal words but is restricted to math questions. Mielke et al. (2022) study linguistic calibration on conversational models. Kadavath et al. (2022) study adopting a multiple-choice setting in which case obtaining a confidence score is much easier (since the model only needs to predict one token to indicate which option to choose rather than generating the entire answer string). We differ from them in: 1) we focus on obtaining probabilistic confidence scores rather than verbal uncertainty expressions; 2) we study the more general and realistic free-form answer generation setting; and 3) we do not involve finetuning or any additional training of the language model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">SQuAD</th>
<th style="text-align: center;">HotpotQA</th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;">NewsQA</th>
<th style="text-align: center;">SearchQA</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">D-Net</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">84.1</td>
</tr>
<tr>
<td style="text-align: left;">Delphi</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.3</td>
</tr>
<tr>
<td style="text-align: left;">MultiFT</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">81.6</td>
</tr>
<tr>
<td style="text-align: left;">MADE</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">81.4</td>
</tr>
<tr>
<td style="text-align: left;">T5-Finetune</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">T5-PromptTune</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 Source-P</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">79.8</td>
</tr>
</tbody>
</table>
<p>Table 11: Results on MRQA in-domain datasets. We use the F1 metric for all datasets. D-Net and Delphi results are taken from the MRQA system report (Fisch et al., 2019) which only reported the average performance. MultiFT and MADE results are taken from Friedman et al. (2021). T5Finetune and T5-PromptTune results are from the prompt tuning paper (Lester et al., 2021) which only trained on the SQuAD datsaet. GPT-3 few-shot performance slightly lags behind these supervised baselines in the in-domain setting.</p>
<p>Knowledge Updating. Despite the fact that LLMs like GPT-3 are pretrained on very large corpora, they are still far from perfect in terms of factual knowledge. On one hand, they still make factual mistakes even on domains that have seen before during pretraining (e.g., Wikipedia); on the other hand, they are pretrained on static corpora and hence their knowledge can become outdated. In order for LLMs to serve as reliable knowledge bases (Petroni et al., 2019) or power knowledgeintensive downstream applications (Petroni et al., 2021), it is important to keep LLMs' knowledge factually correct and up-to-update. A recent line of work attempts to edit factual knowledge in LLMs by making targeted modifications of the model's neurons (Cao et al., 2021; Mitchell et al., 2021; 2022). However, these methods are hard to be applied on GPT-3 since it is much larger in size and often treated as a black box without access to internal parameters. To address this issue, in this paper we explore the feasibility of performing in-context knowledge updating by directly appending relevant knowledge pieces in the prompt to guide model predictions. Since it has been shown that larger models are better at memorization (Carlini et al., 2023), we analyze whether it is possible to make larger models forget their memorized knowledge and adapt to the new information presented in the prompt, especially when these two are in conflict. The idea of adding retrieved passages is conceptually similar to the line of work on retrieval-augmented methods for knowledgeintensive NLP (Lewis et al., 2020; Izacard et al., 2022b; Guu et al., 2020). However, these methods still require supervised training while we focus on the setting of few-shot prompting with all the language model's parameters being frozen.</p>
<h1>B ADDITIONAL ReSULTS: GENERALIZABILITY</h1>
<p>We provide full experimental results and comparisons with more baselines.</p>
<p>MRQA Table 11 and Table 12 present detailed results on MRQA. For baselines, we include results from the top-performing systems of the MRQA competition: D-Net (Li et al., 2019) and Delphi (Longpre et al., 2019), a recent adapter-based robust tuning method MADE (Friedman et al., 2021) as well as their multi-dataset finetuning baseline. We also report the finetuning and prompt tuning Lester et al. (2021) result of using T5, which achieves state-of-the-art OOD transfer results on MRQA. Note that this T5 baseline only uses SQuAD as the in-domain training data.</p>
<p>AdvGLUE and Contrast Sets Table 13 and Table 14 present detailed results on AdvGLUE and Contrast Sets, where GPT-3 shows better generalization than supervised baselines.</p>
<p>Spurious Correlation Table 16 shows full the performance breakdown on HANS based on the three spurious features. We can see that for the subsequence and constituent features in HANS, GPT-3 still suffers significant performance gaps between the bias-supporting and bias-countering subsets. This leaves curious questions like why such gaps only occur for certain bias features but not others, and how such spurious biases arise (most likely due to pretraining), and we leave a more thorough analysis of these questions to future work.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BioASQ</th>
<th style="text-align: center;">DROP</th>
<th style="text-align: center;">DuoRC</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">RE</th>
<th style="text-align: center;">TextbookQA</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">D-Net</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.7</td>
</tr>
<tr>
<td style="text-align: left;">Delphi</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.5</td>
</tr>
<tr>
<td style="text-align: left;">MultiFT</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: left;">MADE</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">62.9</td>
</tr>
<tr>
<td style="text-align: left;">T5-Finetune</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">$\mathbf{6 8 . 9}$</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">69.7</td>
</tr>
<tr>
<td style="text-align: left;">T5-PromptTune</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 Source-P</td>
<td style="text-align: center;">$\mathbf{8 6 . 2}$</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">$\mathbf{7 0 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 0}$</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">$\mathbf{8 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 Target-P</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">$\mathbf{6 8 . 9}$</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">$\mathbf{9 1 . 0}$</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">$\mathbf{7 7 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 12: Results on MRQA OOD datasets. ID-P (second last row) uses a fixed set of sampled demo examples from the in-domain datasets and evaluates on these OOD datasets, while OOD-P (last row) uses sampled demo examples from each of these OOD datasets for evaluation. GPT-3 significantly outperforms all other supervised baselines on these OOD datasets, moreover, using the in-domain prompt successfully transfers to OOD test data, achieving the same average performance as using demos examples drawn from these OOD datasets as the prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">QNLI</th>
<th style="text-align: center;">QQP</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Clean Test Sets</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">91.7</td>
</tr>
<tr>
<td style="text-align: center;">ALBERT</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">92.2</td>
</tr>
<tr>
<td style="text-align: center;">DeBERTa</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">92.9</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">84.2</td>
</tr>
<tr>
<td style="text-align: center;">Adversarial Test Sets</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">ALBERT</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">61.6</td>
</tr>
<tr>
<td style="text-align: center;">DeBERTa</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">69.3</td>
</tr>
</tbody>
</table>
<p>Table 13: Results on the clean and adversarial test sets of AdvGLUE, we use accuracy as the metric for all datasets. For MNLI, we report the average performance on the matched and mismatched dev sets. The supervised baselines are trained on the clean training data, and GPT-3 uses few-shot prompts sampled from the clean training data. While GPT-3 few-shot prompting lags behind supervised models on clean test sets, it significantly outperforms the supervised models on the adversarial sets. That being said, we still note a performance drop of GPT-3 on the adversarial test sets when using clean demo examples.</p>
<p>In Table 15, we perform additional ablation on the impact of the number of demos and the different GPT-3 variants. With fewer demo examples from QQP, despite a slight drop on the QQP test set, GPT-3 actually remains robust (even higher accuracy on PAWS than the 16-shot results). On the other hand, using the Text-Davinci-001 (175B) and the smaller Text-Curie-001 (6.7B) performs far worse on both the QQP test set and the PAWS challenge test set.</p>
<h1>C ADDITIONAL ReSULTS: SOCIAL BIASES</h1>
<p>We also break down the accuracy and bias scores of using different prompts in Table 17 by the different bias categories. We observe that there can be large differences across different categories. Moreover, we underlined the categories from which the demo examples come, and we observe that having same-category demos in the prompt does not correlate with the bias scores. For instance, we have bias-supporting examples from the Nationality category in the Ambig-Pro case but the bias score remains low, while the bias score for the Physical Appearance and Disability categories becomes much higher even when the biased examples are not from these categories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">IMDB - Original</th>
<th style="text-align: center;">IMDB - Contrast</th>
<th style="text-align: center;">Gap $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">9.6</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">QuoREF - Original</td>
<td style="text-align: center;">QuoREF - Contrast</td>
<td style="text-align: center;">Gap $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">XLNet</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">15.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">9.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">BoolQ - Original</td>
<td style="text-align: center;">BoolQ - Contrast</td>
<td style="text-align: center;">Gap $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">15.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">5.5</td>
</tr>
</tbody>
</table>
<p>Table 14: Results on Contrast Sets. For IMDB and BoolQ, we report accuracy; for QuoREF, we report F1. Apart from the performance on the original and contrast sets of the three datasets, we also note the gap between performance on the original and contrast sets. We see a clear trend that GPT-3 incurs a smaller gap than the supervised models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">QQP (ID)</th>
<th style="text-align: center;">PAWS (OOD)</th>
<th style="text-align: center;">Gap</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT (supervised)</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">51.2</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa (supervised)</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: left;">Code-Davinci-002 (4-shots)</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">-2.3</td>
</tr>
<tr>
<td style="text-align: left;">Code-Davinci-002 (16-shots)</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">9.8</td>
</tr>
<tr>
<td style="text-align: left;">Text-Davinci-001 (16-shots)</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">30.0</td>
</tr>
<tr>
<td style="text-align: left;">Text-Curie-001 (16-shots)</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">8.0</td>
</tr>
</tbody>
</table>
<p>Table 15: Ablation for the impact of the number of demos and different GPT-3 model variants on MNLI-HANS.</p>
<h1>D Additional Results: Calibration</h1>
<p>The full selective prediction results in Table 18 show that the confidence scores can be used to rank model predictions. We see a clear trend that the most confident predictions have much higher accuracy.</p>
<p>The reliability diagrams in Figure 2 show that in most cases the calibration errors come from overconfidence where the predictions' confidence is higher than the expected accuracy. It is also worth noting while OOD transfer is a big challenge for the calibration of supervised models where there tends to be overconfidence on the OOD test sets, GPT-3 exhibits similar calibration results when using in-domain or OOD demo examples as the prompt (bottom-left plot in Figure 2).</p>
<p>To further disentangle the impact of better accuracy and better calibration, we perform a controlled evaluation of selective prediction in Table 19 where we sub-sample the NQ test set so that the three calibration methods achieve the same accuracy on the test set. We see a clear trend that despite DPR-BERT and GPT-3 get same accuracy on this sub-sampled test set, DPR-BERT gets much higher accuracy on the most confident predictions indicating the usefulness of better calibration.</p>
<h2>E Additional Results: Knowledge Updating</h2>
<h2>E. 1 Impact of Prompts for Memorization vs Updating</h2>
<p>For knowledge updating, we compare several different prompt designs as detailed below, for all cases, we randomly sample 16 demo examples as the prompt.</p>
<ul>
<li>$\langle Q, A\rangle$ : We use the original question-answer pairs in the prompt.</li>
<li>$\langle P, Q, A\rangle$ : We use the original passage-question-answer triples in the prompt (i.e., the answer in the passage remains the original gold answer).</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">HANS Category</th>
<th style="text-align: center;">GPT-3 Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lexical Overlap - Entailment</td>
<td style="text-align: center;">87.9</td>
</tr>
<tr>
<td style="text-align: left;">Lexical Overlap - Non-Entailment</td>
<td style="text-align: center;">96.9</td>
</tr>
<tr>
<td style="text-align: left;">Subsequence - Entailment</td>
<td style="text-align: center;">84.0</td>
</tr>
<tr>
<td style="text-align: left;">Subsequence - Non-Entailment</td>
<td style="text-align: center;">53.7</td>
</tr>
<tr>
<td style="text-align: left;">Constituent - Entailment</td>
<td style="text-align: center;">87.2</td>
</tr>
<tr>
<td style="text-align: left;">Constituent - Non-Entailment</td>
<td style="text-align: center;">42.2</td>
</tr>
</tbody>
</table>
<p>Table 16: Breakdown of GPT-3 results on HANS by categories. All demo examples in the prompt are from MNLI. GPT-3 still suffers significant performance gaps between the entailment (biassupporting) and non-entailment (bias-countering) subsets for the subsequence and constituent features in HANS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Balanced</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ambig-Pro</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ambig-Anti</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Acc $_{\uparrow}$</td>
<td style="text-align: center;">Bias $_{\downarrow}$</td>
<td style="text-align: center;">Acc $_{\uparrow}$</td>
<td style="text-align: center;">Bias $_{\downarrow}$</td>
<td style="text-align: center;">Acc $_{\uparrow}$</td>
<td style="text-align: center;">Bias $_{\downarrow}$</td>
</tr>
<tr>
<td style="text-align: left;">SES</td>
<td style="text-align: center;">$96.4 / 74.2$</td>
<td style="text-align: center;">$\underline{3.2 / 0.0}$</td>
<td style="text-align: center;">$14.2 / 99.2$</td>
<td style="text-align: center;">$\underline{-12.6 / 0.0}$</td>
<td style="text-align: center;">$8.4 / 99.6$</td>
<td style="text-align: center;">$\underline{-6.4 / 0.0}$</td>
</tr>
<tr>
<td style="text-align: left;">Sexual orientation</td>
<td style="text-align: center;">$97.4 / 76.3$</td>
<td style="text-align: center;">$\underline{1.6 / 1.2}$</td>
<td style="text-align: center;">$1.9 / 98.4$</td>
<td style="text-align: center;">$\underline{19.1 /-0.9}$</td>
<td style="text-align: center;">$2.8 / 97.2$</td>
<td style="text-align: center;">$\underline{17.2 /-0.9}$</td>
</tr>
<tr>
<td style="text-align: left;">Religion</td>
<td style="text-align: center;">$97.0 / 75.8$</td>
<td style="text-align: center;">$1.8 / 0.5$</td>
<td style="text-align: center;">$0.4 / 98.0$</td>
<td style="text-align: center;">$\underline{35.2 / 1.2}$</td>
<td style="text-align: center;">$1.0 / 97.2$</td>
<td style="text-align: center;">$\underline{24.6 / 1.6}$</td>
</tr>
<tr>
<td style="text-align: left;">Race</td>
<td style="text-align: center;">$99.8 / 85.1$</td>
<td style="text-align: center;">$-0.1 /-0.5$</td>
<td style="text-align: center;">$1.2 / 99.0$</td>
<td style="text-align: center;">$4.0 / 0.1$</td>
<td style="text-align: center;">$2.7 / 98.9$</td>
<td style="text-align: center;">$3.7 / 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">Physical Appearance</td>
<td style="text-align: center;">$97.4 / 56.0$</td>
<td style="text-align: center;">$2.6 / 18.8$</td>
<td style="text-align: center;">$1.4 / 87.8$</td>
<td style="text-align: center;">$75.0 / 14.8$</td>
<td style="text-align: center;">$0.6 / 86.2$</td>
<td style="text-align: center;">$77.0 / 14.8$</td>
</tr>
<tr>
<td style="text-align: left;">Nationality</td>
<td style="text-align: center;">$98.2 / 80.8$</td>
<td style="text-align: center;">$1.4 /-11.6$</td>
<td style="text-align: center;">$1.0 / 99.0$</td>
<td style="text-align: center;">$\underline{-0.2 / 0.0}$</td>
<td style="text-align: center;">$1.0 / 98.6$</td>
<td style="text-align: center;">$\underline{0.6 / 0.0}$</td>
</tr>
<tr>
<td style="text-align: left;">Gender identity</td>
<td style="text-align: center;">$99.0 / 66.8$</td>
<td style="text-align: center;">$0.6 /-3.9$</td>
<td style="text-align: center;">$6.0 / 98.6$</td>
<td style="text-align: center;">$5.6 / 0.4$</td>
<td style="text-align: center;">$4.6 / 98.8$</td>
<td style="text-align: center;">$3.8 / 0.4$</td>
</tr>
<tr>
<td style="text-align: left;">Disability</td>
<td style="text-align: center;">$97.4 / 74.2$</td>
<td style="text-align: center;">$2.2 / 8.5$</td>
<td style="text-align: center;">$0.0 / 96.6$</td>
<td style="text-align: center;">$85.2 / 6.0$</td>
<td style="text-align: center;">$0.2 / 97.2$</td>
<td style="text-align: center;">$82.6 / 4.8$</td>
</tr>
<tr>
<td style="text-align: left;">Age</td>
<td style="text-align: center;">$82.2 / 76.6$</td>
<td style="text-align: center;">$13.0 / 8.1$</td>
<td style="text-align: center;">$0.0 / 95.4$</td>
<td style="text-align: center;">$52.0 / 12.4$</td>
<td style="text-align: center;">$0.4 / 95.6$</td>
<td style="text-align: center;">$48.4 / 12.0$</td>
</tr>
</tbody>
</table>
<p>Table 17: Breakdown of accuracy and bias score results on BBQ. For accuracy and bias scores, the first number represents the ambiguous set and the second number represents the disambiguated set. Underlined numbers indicate that there are demo examples in the prompt from the same bias category.</p>
<ul>
<li>$\left\langle Q, A^{\prime}\right\rangle$ : We use the question-answer pairs, but with the substitution entities as gold answers in the prompt.</li>
<li>$\left\langle P^{\prime}, Q, A^{\prime}\right\rangle$ : We use triples of the answer-substituted passage, the question, and the substitution answers in the prompt.</li>
</ul>
<p>As shown in Table 20, we find that the prompt design has a big impact on the knowledge updating behavior. In particular, showing only the original passage-question-answer triples $(\langle P, Q, A\rangle)$ still causes high memorization ratios, however, when prompting with counterfactual triples $\left(\left\langle P^{\prime}, Q, A^{\prime}\right\rangle\right)$, GPT-3 can update $85 \%$ of the time with much lower memorization ratios than a supervised model.</p>
<h1>E. 2 TARGETED In-CONTEXT KNOWLEDGE Updating</h1>
<p>The experiments in the previous section showed promise that GPT-3 can adapt to new knowledge given in the prompt when there is a conflict with its memorized knowledge. One missing aspect from the above analysis is whether we can perform targeted knowledge update: when given a piece of knowledge update, we expect the model to predict the updated answer for all questions related to that knowledge, but not change its answer for other unrelated questions. To assess model behavior on this front, we adopt an evaluation setup closer to recent knowledge updating literature (Cao et al., 2021; Mitchell et al., 2021).</p>
<p>Experiment Setup We use two evaluation datasets from Mitchell et al. (2021): 1) We first use the fact checking dataset FEVER (Thorne et al., 2018): each claim requires a binary true / false judgement. We create the edited label which is opposite to the originally predicted label from GPT3. For example, for a test example, if the original GPT-3 prediction is true, then the new label for editing would be false. We present the knowledge update in the form of a natural sentence that supports the target label for editing. We then test whether GPT-3 predicts the target label for a</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://simonwillison.net/2022/Sep/12/prompt-injection/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
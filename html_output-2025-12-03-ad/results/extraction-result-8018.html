<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8018 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8018</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8018</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-272550708</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.06185v2.pdf" target="_blank">Can Large Language Models Unlock Novel Scientific Research Ideas?</a></p>
                <p><strong>Paper Abstract:</strong> The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study examines the ability of Large Language Models (LLMs) to generate future research ideas from scientific papers. Unlike tasks such as summarization or translation, idea generation lacks a clearly defined reference set or structure, making manual evaluation the default standard. However, human evaluation in this setting is extremely challenging ie: it requires substantial domain expertise, contextual understanding of the paper, and awareness of the current research landscape. This makes it time-consuming, costly, and fundamentally non-scalable, particularly as new LLMs are being released at a rapid pace. Currently, there is no automated evaluation metric specifically designed for this task. To address this gap, we propose two automated evaluation metrics: Idea Alignment Score (IAScore) and Idea Distinctness Index. We further conducted human evaluation to assess the novelty, relevance, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8018.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8018.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IAScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Idea Alignment Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated, precision-oriented metric that quantifies how well LLM‑generated future research ideas align with the author‑stated future research ideas (AP‑FRI) in a paper; computed per paper as the average IdeaMatcher alignment across generated ideas and aggregated across papers for domain scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini, Claude-2, GPT-3.5, GPT-4 (and newer variants in extended experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (e.g., gemini-1.0-pro-002; anthropic.claude-v2; gpt-3.5-turbo-0125; gpt-4-0125-preview; later: gpt-4o, claude-3, gemini-2.5, llama-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science, Physics, Chemistry, Economics, Medical</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>future research ideas (FRIs), hypotheses/directions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Idea Alignment Score (IAScore)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each paper, compute AvgScore_j as the mean alignment (via an IdeaMatcher) between each generated idea and the paper's author-provided FRIs (AP‑FRI_j); then average AvgScore_j across all papers in a domain to yield IAScore_domain,M for model M. Serves as a lower-bound precision estimate of useful (author-aligned) ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>IAScore (domain-aggregated average of per-paper AvgScore_j)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>AvgScore_j = (1/N_j) * sum_i IM(AP-FRI_j, I_ij); IAScore_domain,M = (1/P) * sum_j AvgScore_j. Scale: 0–1 where higher values indicate stronger alignment with author FRIs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AP-FRI (Author Perspective Future Research Idea Corpus) extracted from S2ORC2 papers collected for this study</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>IAScore validated against human judgments; authors of papers and domain experts used in human evaluation (see separate Human Evaluation entry). IdeaMatcher decisions were calibrated using a small annotated corpus (61 matching pairs) and validation/test splits (30:70).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Domain IAScore comparisons across models: GPT-4o ≈ 0.41 (CS, new-model eval), GPT-4 ≈ 0.39, Claude-3 ≈ 0.39, Claude-2 ≈ 0.37, Gemini-2.5 ≈ 0.37, Llama-70B ≈ 0.31, Gemini-1.0 ≈ 0.28 (paper reports relative values and domain plots); earlier experiments show GPT-4 and Claude outperform GPT-3.5 and Gemini across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>IAScore is explicitly defined to measure alignment to author (human) FRIs; authors note IAScore is a lower bound on useful ideas and does not penalize novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>IAScore only measures alignment with author-written FRIs (I_auth subset), so it underestimates valid novel ideas outside authors' statements; sensitive to completeness/accuracy of AP‑FRI annotations and IdeaMatcher performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8018.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IdeaDistinctness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Idea Distinctness Index (DI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic-diversity metric evaluating how distinct a set of generated ideas are from one another by embedding each idea and averaging pairwise (1 − cosine similarity) scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini, Claude-2, GPT-3.5, GPT-4 (and others evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see IAScore entry)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science, Physics, Chemistry, Economics, Medical</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>future research ideas (FRIs), hypotheses/directions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Idea Distinctness Index (DI)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Encode each generated idea with a pre-trained embedding model (paper uses OpenAI GPT embeddings), compute cosine similarity between every pair, transform to pairwise distinctness D_ij = 1 − sim(v_i, v_j), then average over all pairs to get DI for the set; aggregate per paper and across domain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Distinctness Index DI and domain-level D_domain,M</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>DI = (1 / (n(n−1))) * sum_{i≠j} (1 − cosine_sim(v_i, v_j)). Range ~0–1, higher values indicate greater semantic diversity among ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Generated idea sets per paper (from the study) and AP-FRI; embeddings used: OpenAI GPT embedding (text-embedding-ada-002 mentioned earlier; paper references GPT embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Compared model DI scores to human (author) DI scores per domain; human judgments used as external reference for distinctness but not used directly in DI computation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Claude's DI was comparable to human authors in Chemistry, Economics, Medical domains and exceeded human DI in Computer and Physics domains; GPT-4 had higher distinctness than Gemini and GPT-3.5 but lower than Claude/human in some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Quantitative DI comparisons show Claude ≈ human in several domains; GPT-4 > Gemini/GPT-3.5; results vary by domain.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on cosine similarity in embedding space and may miss subtle semantic distinctions; sensitive to choice of embedding model and thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8018.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IdeaMatcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IdeaMatcher (alignment classifier / matching pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A matching component that determines whether a generated idea is contained within or aligned to an author's AP‑FRI; evaluated using NLI, embedding similarity (BERTScore), and prompted GPT judgments to choose the best matcher.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa MNLI (baseline), BERTScore (embedding similarity), GPT (prompted evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>roberta-large MNLI; RoBERTa MNLI fine-tuned checkpoint (as in Liu et al. 2019); BERTScore uses RoBERTa-large 24-layer; GPT prompting used OpenAI GPT models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (applies to FRIs across all domains in the study)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>matching/entailment decision for hypotheses (generated ideas) vs. premises (AP-FRIs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>IdeaMatcher (NLI + embedding similarity + GPT prompting compared)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Treated idea matching as an NLI-like task (AP‑FRI as premise, generated idea as hypothesis) and compared performance of RoBERTa MNLI entailment, BERTScore similarity thresholding, and a prompted GPT-based evaluator; selected GPT prompting due to highest empirical accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matching accuracy (classification of match vs non-match)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy measured on a small annotated set: RoBERTa MNLI accuracy = 65.5%; BERTScore = 75.4% (threshold tuned on validation); GPT prompting = 91.8% (on the designed prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Small annotated idea-matching corpus created by authors (61 matching pairs plus equal non-matching pairs; 30:70 validation:test split for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual search to identify matching/non-matching pairs used as ground truth for IdeaMatcher evaluation; small annotated set used to evaluate and choose IdeaMatcher.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>RoBERTa MNLI produced many false negatives (65.5% acc). BERTScore produced 75.4% acc. Prompted GPT produced 91.8% accuracy on the annotation set and was selected as IdeaMatcher.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Paper reports empirical comparisons across matching methods (NLI vs BERTScore vs GPT prompting) and selects best-performing approach (GPT prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small annotated matching corpus (61 positive pairs) limits reliability; IdeaMatcher performance is a key bottleneck for IAScore accuracy; GPT prompting accuracy may be influenced by prompt design and model training cutoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8018.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanEvalProtocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation protocol (novelty / relevance / feasibility with Best‑Worst Scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Author-expert human evaluation assessing generated ideas on relevance, novelty (5‑point Best‑Worst Scaling), and feasibility, with inter-annotator agreement measured by Cohen's kappa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluations performed for outputs from Claude, GPT-4, Gemini, GPT-3.5 (human raters evaluated ideas from these models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>see model entries above</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Primarily Computer Science and Physics (human evaluation performed on 66 CS papers / 660 ideas and 63 Physics papers / 630 ideas), other domains less extensively human-evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation of generated FRIs (novelty/relevance/feasibility judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert rating with Best‑Worst Scaling and binary relevance/feasibility questions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each generated idea was rated by domain-expert authors for relevance (Relevant/Not relevant), novelty (5-point Best‑Worst Scaling: not-novel, generic, moderately novel, very novel, extremely novel), and factual correctness/feasibility (Possible/Not Possible). Inter-rater agreement was assessed with Cohen's kappa.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Novelty distribution percentages, relevance %, feasibility %; inter-annotator Cohen's kappa</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Novelty: 5-point ordinal BWS scale. Relevance/Feasibility: binary. Cohen's kappa values reported for inter-annotator agreement (0.83 for 20% dual-evaluated subset; annotation-phase kappa 0.94 for dataset construction).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Human evaluations performed on 660 generated ideas for 66 CS papers and 630 ideas for 63 Physics papers; annotators were paper authors or domain experts (authors of the papers).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Evaluators: authors of the papers (experienced publishers with >5 years experience and >5 papers). Scope: 66 CS papers (top-5 ideas per paper in some evaluations), inter-annotator check on 20% sample with Cohen's kappa = 0.83; earlier annotator training kappa = 0.94.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CS domain (human eval): Claude ideas — 14.78% non-novel, 16.52% generic, 41.73% moderately novel, 20.86% very novel, 16.52% extremely novel; GPT-4 — 7.83% not-novel, 13.91% generic, 42.61% moderately novel, 28.70% very novel, 6.96% extremely novel. Relevance: Claude 76.67% relevant, GPT-4 93.34% relevant. Feasibility: Claude 83.34% feasible, GPT-4 96.64% feasible. Physics domain similar trends reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Human judgments used to benchmark model outputs; distinctness compared to human-authored AP-FRIs and human rates serve as reference; models produce a mix of generic/non-novel and highly novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human evaluation is costly, domain-dependent, not fully scalable; reliance on authors as evaluators may introduce bias; sample sizes limited to selected domains and papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8018.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AP-FRI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Author Perspective Future Research Idea Corpus (AP-FRI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated corpus of future research ideas manually extracted from papers' 'future work' sections (Direct and Mixed FRIs) and grouped per topic to serve as gold-standard author-written FRIs for alignment evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used as reference to evaluate ideas generated by LLMs including GPT-4, Claude, Gemini, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science, Physics, Chemistry, Economics, Medical</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>gold-reference future research ideas (author hypotheses/directions)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>AP-FRI corpus used for Idea Alignment Score and IdeaMatcher evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Authors' explicitly stated future research ideas were removed from input and collected into the AP-FRI corpus; used as premises for alignment and for creating the matching dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used as ground truth for alignment (IAScore) and matching accuracy metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>AP-FRI is a dataset artifact (collection of author-provided FRI statements); not a scalar metric.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AP-FRI extracted from 1,250 S2ORC2 papers (250 per domain) with future work sections; aggregated and merged per topic</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>AP-FRI created by trained annotators (PhD students) with expert oversight; annotation inter-annotator kappa = 0.94 during dataset construction; annotator training and payment described in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>AP-FRI corpus size: contains the author-stated FRIs removed from the input papers; used to compute IAScore and to evaluate IdeaMatcher. Dataset and code released by authors (link provided in paper repository).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>AP-FRI covers only explicit future work statements and may omit legitimate future directions; authors may not enumerate all plausible future ideas; domain coverage limited to papers with explicit FWK sections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8018.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) framework (applied pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline integrating a vector database retriever with an LLM generator to incorporate background knowledge (retrieved related paper contributions) into idea generation to improve relevance and reduce generic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retriever (BERT-based embeddings), Generator (LLM such as GPT-4 / Claude)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>vector DB built from ~190K CS paper titles/abstracts; retriever returns top-20; generator models as above</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (explicitly), applied also conceptually across domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>framework to augment LLMs for generating FRIs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>RAG pipeline (Vector DB + Retriever + LLM Generator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Build a vector DB of ~190K CS paper titles & abstracts with BERT embeddings; retrieve top-20 related papers for a target title via cosine similarity; extract contributions using LLM prompts; include retrieved background in LLM prompt to generate distinct FRIs while instructing not to repeat background ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Effect on novelty/relevance/feasibility and reduction in non-novel/generic ideas (measured via human eval and IAScore/DI changes)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported relative percentage improvements in reducing non-novel/generic ideas and changes in relevance/feasibility as measured by human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Vector DB constructed from Semantic Scholar API titles/abstracts (~190K CS papers); retrieval uses BERT embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluations compared outputs with and without added background knowledge; measured novelty categories and relevance/feasibility percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Adding background knowledge reduced non-novel idea generation by ~50% (GPT-4) and ~53.33% (Claude) and modestly improved relevance and factual correctness for GPT-4 (relevance +0.77%, feasibility +0.34%); Claude saw small decreases in some scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Background can cause models to rehash existing ideas if not properly constrained; improvements in high-novelty idea generation were small; effectiveness depends on quality of retriever and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8018.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore (embedding-based similarity metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding-based text similarity metric that measures semantic overlap using deep contextualized embeddings (here used to compare generated ideas to AP‑FRIs), applied with a RoBERTa-large backbone and layer selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bertscore: Evaluating text generation with bert.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERTScore implementation using RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>RoBERTa-large (24-layer) used; embedding from 17th layer as per implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (applied to compare idea texts across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>semantic similarity metric for idea matching</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore similarity thresholding</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute BERTScore between generated idea and AP-FRI; compare to an empirically chosen threshold on validation set to decide match/non-match.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matching accuracy on annotated idea pairs (binary match/non-match)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Threshold determined on validation set; paper reports BERTScore matching accuracy = 75.4% on the small annotated set.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Small annotated idea-matching corpus (61 positive pairs + equal negatives) used for validation/test</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Threshold selection and validation performed using the annotated corpus; human-annotated ground truth used to compute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>BERTScore outperformed RoBERTa MNLI entailment (75.4% vs 65.5%) but was outperformed by prompted GPT (91.8%) for idea matching.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>BERTScore relies on surface-level embedding similarity and may miss paraphrastic or broader conceptual matches; thresholding requires tuning and small annotation set limits robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8018.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distinct-N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distinct-N (n-gram diversity metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing lexical diversity metric that counts distinct n-grams to penalize repetition; mentioned as insufficient for semantic idea distinctness and used as motivation to develop the Idea Distinctness Index.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A diversity-promoting objective function for neural conversation models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General text generation evaluation (not specific to domain)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>lexical diversity metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Distinct-N (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Measures the ratio of distinct n-grams in generated text to penalize repetitive outputs; previously used for dialogue/response diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Distinct-1, Distinct-2 etc. (n-gram distinctness ratios)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Distinct-N = (# distinct n-grams) / (total # n-grams) computed per sentence/document; range 0–1.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as inadequate for semantic-level idea comparison, motivating semantic embedding-based DI.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Distinct-N captures syntactic/lexical variety but not semantic diversity; can be gamed by paraphrasing without changing idea content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8018.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S2ORC2/SemanticScholar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S2ORC2 / Semantic Scholar Open Research Corpus (and Semantic Scholar API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's source corpus and retrieval resource: 1,250 papers collected from S2ORC2 (250 per domain) for dataset creation; additional ~190K CS titles/abstracts were retrieved via the Semantic Scholar API to build the vector database for RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>S2ORC: The semantic scholar open research corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science, Physics, Chemistry, Economics, Medical (domains sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dataset source for idea generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collected recent papers (2023–2024) with explicit future work sections; removed authors' future work for generation; AP-FRI compiled from removed text; additional 190K CS titles/abstracts used to provide background via RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Dataset statistics (250 papers/domain; 1,250 papers main corpus; 190K CS titles/abstracts for vector DB)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Counts and filtering criteria: English-language, full-content available, contains 'future work' section; papers from Jan 2023–Feb 2024 (main corpus); supplementary 200 papers Jan–Mar 2025 for new-model eval.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>S2ORC2 / Semantic Scholar API collections (paper repository used in study)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human annotators (trained PhD students) annotated FRIs from these papers; authors later evaluated generated ideas drawn from the same sets.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Final dataset: 250 papers per domain curated; AP-FRI corpus derived from these; supplementary 200 CS papers used in new-model evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Coverage limited to English papers with extractable PDFs and explicit future-work sections; possibility of model memorization of some pretraining data mitigated but not eliminated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8018.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8018.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbeddingModels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding models used for similarity and retrieval (text-embedding-ada-002; OpenAI GPT embeddings; BERT embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study uses several embedding families: BERT embeddings for vector DB/retrieval, OpenAI GPT/text-embedding-ada-002 for idea semantic encoding in DI, and RoBERTa layers for BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-embedding-ada-002 (OpenAI), OpenAI GPT embeddings, BERT (for retriever), RoBERTa (for BERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>embedding dimensionalities not explicitly specified; models named in paper (text-embedding-ada-002 referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied across domains in retrieval, matching and distinctness computations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>tooling/method for semantic similarity and retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Embedding-based retrieval and similarity (cosine similarity used throughout)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>BERT embeddings used to build vector DB and retrieve top-k related papers via cosine similarity; OpenAI GPT embeddings used to encode ideas for DI computation; RoBERTa-based embeddings used in BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Cosine similarity scores for retrieval and pairwise idea similarity; used to compute DI as 1 − cosine_sim.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Cosine similarity in R^d embedding space (range −1 to 1, typically 0–1 for positive similarity); DI uses 1 − cosine_sim to map to distinctness in [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Vector DB (~190K CS titles/abstracts); generated ideas sets; AP-FRI</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Embedding thresholds tuned on validation; BERTScore threshold empirically set to 0.68 in implementation notes.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Embedding-based retrieval and DI computations underpin RAG improvements and distinctness quantification; BERT-based retriever and OpenAI embeddings used successfully in pipeline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Results sensitive to choice of embedding model and dimensionality; cosine similarity may not capture fine-grained conceptual differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks. <em>(Rating: 2)</em></li>
                <li>Bertscore: Evaluating text generation with bert. <em>(Rating: 2)</em></li>
                <li>A diversity-promoting objective function for neural conversation models. <em>(Rating: 2)</em></li>
                <li>S2ORC: The semantic scholar open research corpus. <em>(Rating: 2)</em></li>
                <li>PaperRobot: Incremental draft generation of scientific ideas. <em>(Rating: 1)</em></li>
                <li>Automated hypothesis generation based on mining scientific literature. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8018",
    "paper_id": "paper-272550708",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "IAScore",
            "name_full": "Idea Alignment Score",
            "brief_description": "An automated, precision-oriented metric that quantifies how well LLM‑generated future research ideas align with the author‑stated future research ideas (AP‑FRI) in a paper; computed per paper as the average IdeaMatcher alignment across generated ideas and aggregated across papers for domain scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini, Claude-2, GPT-3.5, GPT-4 (and newer variants in extended experiments)",
            "model_size": "various (e.g., gemini-1.0-pro-002; anthropic.claude-v2; gpt-3.5-turbo-0125; gpt-4-0125-preview; later: gpt-4o, claude-3, gemini-2.5, llama-70B)",
            "scientific_domain": "Computer Science, Physics, Chemistry, Economics, Medical",
            "theory_type": "future research ideas (FRIs), hypotheses/directions",
            "evaluation_method_name": "Idea Alignment Score (IAScore)",
            "evaluation_method_description": "For each paper, compute AvgScore_j as the mean alignment (via an IdeaMatcher) between each generated idea and the paper's author-provided FRIs (AP‑FRI_j); then average AvgScore_j across all papers in a domain to yield IAScore_domain,M for model M. Serves as a lower-bound precision estimate of useful (author-aligned) ideas.",
            "evaluation_metric": "IAScore (domain-aggregated average of per-paper AvgScore_j)",
            "metric_definition": "AvgScore_j = (1/N_j) * sum_i IM(AP-FRI_j, I_ij); IAScore_domain,M = (1/P) * sum_j AvgScore_j. Scale: 0–1 where higher values indicate stronger alignment with author FRIs.",
            "dataset_or_benchmark": "AP-FRI (Author Perspective Future Research Idea Corpus) extracted from S2ORC2 papers collected for this study",
            "human_evaluation_details": "IAScore validated against human judgments; authors of papers and domain experts used in human evaluation (see separate Human Evaluation entry). IdeaMatcher decisions were calibrated using a small annotated corpus (61 matching pairs) and validation/test splits (30:70).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Domain IAScore comparisons across models: GPT-4o ≈ 0.41 (CS, new-model eval), GPT-4 ≈ 0.39, Claude-3 ≈ 0.39, Claude-2 ≈ 0.37, Gemini-2.5 ≈ 0.37, Llama-70B ≈ 0.31, Gemini-1.0 ≈ 0.28 (paper reports relative values and domain plots); earlier experiments show GPT-4 and Claude outperform GPT-3.5 and Gemini across domains.",
            "comparison_to_human_generated": null,
            "comparison_results": "IAScore is explicitly defined to measure alignment to author (human) FRIs; authors note IAScore is a lower bound on useful ideas and does not penalize novelty.",
            "limitations_noted": "IAScore only measures alignment with author-written FRIs (I_auth subset), so it underestimates valid novel ideas outside authors' statements; sensitive to completeness/accuracy of AP‑FRI annotations and IdeaMatcher performance.",
            "uuid": "e8018.0",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "IdeaDistinctness",
            "name_full": "Idea Distinctness Index (DI)",
            "brief_description": "A semantic-diversity metric evaluating how distinct a set of generated ideas are from one another by embedding each idea and averaging pairwise (1 − cosine similarity) scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini, Claude-2, GPT-3.5, GPT-4 (and others evaluated)",
            "model_size": "various (see IAScore entry)",
            "scientific_domain": "Computer Science, Physics, Chemistry, Economics, Medical",
            "theory_type": "future research ideas (FRIs), hypotheses/directions",
            "evaluation_method_name": "Idea Distinctness Index (DI)",
            "evaluation_method_description": "Encode each generated idea with a pre-trained embedding model (paper uses OpenAI GPT embeddings), compute cosine similarity between every pair, transform to pairwise distinctness D_ij = 1 − sim(v_i, v_j), then average over all pairs to get DI for the set; aggregate per paper and across domain.",
            "evaluation_metric": "Distinctness Index DI and domain-level D_domain,M",
            "metric_definition": "DI = (1 / (n(n−1))) * sum_{i≠j} (1 − cosine_sim(v_i, v_j)). Range ~0–1, higher values indicate greater semantic diversity among ideas.",
            "dataset_or_benchmark": "Generated idea sets per paper (from the study) and AP-FRI; embeddings used: OpenAI GPT embedding (text-embedding-ada-002 mentioned earlier; paper references GPT embedding)",
            "human_evaluation_details": "Compared model DI scores to human (author) DI scores per domain; human judgments used as external reference for distinctness but not used directly in DI computation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Claude's DI was comparable to human authors in Chemistry, Economics, Medical domains and exceeded human DI in Computer and Physics domains; GPT-4 had higher distinctness than Gemini and GPT-3.5 but lower than Claude/human in some domains.",
            "comparison_to_human_generated": true,
            "comparison_results": "Quantitative DI comparisons show Claude ≈ human in several domains; GPT-4 &gt; Gemini/GPT-3.5; results vary by domain.",
            "limitations_noted": "Relies on cosine similarity in embedding space and may miss subtle semantic distinctions; sensitive to choice of embedding model and thresholding.",
            "uuid": "e8018.1",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "IdeaMatcher",
            "name_full": "IdeaMatcher (alignment classifier / matching pipeline)",
            "brief_description": "A matching component that determines whether a generated idea is contained within or aligned to an author's AP‑FRI; evaluated using NLI, embedding similarity (BERTScore), and prompted GPT judgments to choose the best matcher.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa MNLI (baseline), BERTScore (embedding similarity), GPT (prompted evaluator)",
            "model_size": "roberta-large MNLI; RoBERTa MNLI fine-tuned checkpoint (as in Liu et al. 2019); BERTScore uses RoBERTa-large 24-layer; GPT prompting used OpenAI GPT models",
            "scientific_domain": "Cross-domain (applies to FRIs across all domains in the study)",
            "theory_type": "matching/entailment decision for hypotheses (generated ideas) vs. premises (AP-FRIs)",
            "evaluation_method_name": "IdeaMatcher (NLI + embedding similarity + GPT prompting compared)",
            "evaluation_method_description": "Treated idea matching as an NLI-like task (AP‑FRI as premise, generated idea as hypothesis) and compared performance of RoBERTa MNLI entailment, BERTScore similarity thresholding, and a prompted GPT-based evaluator; selected GPT prompting due to highest empirical accuracy.",
            "evaluation_metric": "Matching accuracy (classification of match vs non-match)",
            "metric_definition": "Accuracy measured on a small annotated set: RoBERTa MNLI accuracy = 65.5%; BERTScore = 75.4% (threshold tuned on validation); GPT prompting = 91.8% (on the designed prompt).",
            "dataset_or_benchmark": "Small annotated idea-matching corpus created by authors (61 matching pairs plus equal non-matching pairs; 30:70 validation:test split for evaluation)",
            "human_evaluation_details": "Manual search to identify matching/non-matching pairs used as ground truth for IdeaMatcher evaluation; small annotated set used to evaluate and choose IdeaMatcher.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "RoBERTa MNLI produced many false negatives (65.5% acc). BERTScore produced 75.4% acc. Prompted GPT produced 91.8% accuracy on the annotation set and was selected as IdeaMatcher.",
            "comparison_to_human_generated": null,
            "comparison_results": "Paper reports empirical comparisons across matching methods (NLI vs BERTScore vs GPT prompting) and selects best-performing approach (GPT prompting).",
            "limitations_noted": "Small annotated matching corpus (61 positive pairs) limits reliability; IdeaMatcher performance is a key bottleneck for IAScore accuracy; GPT prompting accuracy may be influenced by prompt design and model training cutoffs.",
            "uuid": "e8018.2",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "HumanEvalProtocol",
            "name_full": "Human evaluation protocol (novelty / relevance / feasibility with Best‑Worst Scaling)",
            "brief_description": "Author-expert human evaluation assessing generated ideas on relevance, novelty (5‑point Best‑Worst Scaling), and feasibility, with inter-annotator agreement measured by Cohen's kappa.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluations performed for outputs from Claude, GPT-4, Gemini, GPT-3.5 (human raters evaluated ideas from these models)",
            "model_size": "see model entries above",
            "scientific_domain": "Primarily Computer Science and Physics (human evaluation performed on 66 CS papers / 660 ideas and 63 Physics papers / 630 ideas), other domains less extensively human-evaluated",
            "theory_type": "evaluation of generated FRIs (novelty/relevance/feasibility judgments)",
            "evaluation_method_name": "Human expert rating with Best‑Worst Scaling and binary relevance/feasibility questions",
            "evaluation_method_description": "Each generated idea was rated by domain-expert authors for relevance (Relevant/Not relevant), novelty (5-point Best‑Worst Scaling: not-novel, generic, moderately novel, very novel, extremely novel), and factual correctness/feasibility (Possible/Not Possible). Inter-rater agreement was assessed with Cohen's kappa.",
            "evaluation_metric": "Novelty distribution percentages, relevance %, feasibility %; inter-annotator Cohen's kappa",
            "metric_definition": "Novelty: 5-point ordinal BWS scale. Relevance/Feasibility: binary. Cohen's kappa values reported for inter-annotator agreement (0.83 for 20% dual-evaluated subset; annotation-phase kappa 0.94 for dataset construction).",
            "dataset_or_benchmark": "Human evaluations performed on 660 generated ideas for 66 CS papers and 630 ideas for 63 Physics papers; annotators were paper authors or domain experts (authors of the papers).",
            "human_evaluation_details": "Evaluators: authors of the papers (experienced publishers with &gt;5 years experience and &gt;5 papers). Scope: 66 CS papers (top-5 ideas per paper in some evaluations), inter-annotator check on 20% sample with Cohen's kappa = 0.83; earlier annotator training kappa = 0.94.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "CS domain (human eval): Claude ideas — 14.78% non-novel, 16.52% generic, 41.73% moderately novel, 20.86% very novel, 16.52% extremely novel; GPT-4 — 7.83% not-novel, 13.91% generic, 42.61% moderately novel, 28.70% very novel, 6.96% extremely novel. Relevance: Claude 76.67% relevant, GPT-4 93.34% relevant. Feasibility: Claude 83.34% feasible, GPT-4 96.64% feasible. Physics domain similar trends reported.",
            "comparison_to_human_generated": true,
            "comparison_results": "Human judgments used to benchmark model outputs; distinctness compared to human-authored AP-FRIs and human rates serve as reference; models produce a mix of generic/non-novel and highly novel ideas.",
            "limitations_noted": "Human evaluation is costly, domain-dependent, not fully scalable; reliance on authors as evaluators may introduce bias; sample sizes limited to selected domains and papers.",
            "uuid": "e8018.3",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AP-FRI",
            "name_full": "Author Perspective Future Research Idea Corpus (AP-FRI)",
            "brief_description": "A curated corpus of future research ideas manually extracted from papers' 'future work' sections (Direct and Mixed FRIs) and grouped per topic to serve as gold-standard author-written FRIs for alignment evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used as reference to evaluate ideas generated by LLMs including GPT-4, Claude, Gemini, GPT-3.5",
            "model_size": "n/a",
            "scientific_domain": "Computer Science, Physics, Chemistry, Economics, Medical",
            "theory_type": "gold-reference future research ideas (author hypotheses/directions)",
            "evaluation_method_name": "AP-FRI corpus used for Idea Alignment Score and IdeaMatcher evaluation",
            "evaluation_method_description": "Authors' explicitly stated future research ideas were removed from input and collected into the AP-FRI corpus; used as premises for alignment and for creating the matching dataset.",
            "evaluation_metric": "Used as ground truth for alignment (IAScore) and matching accuracy metrics",
            "metric_definition": "AP-FRI is a dataset artifact (collection of author-provided FRI statements); not a scalar metric.",
            "dataset_or_benchmark": "AP-FRI extracted from 1,250 S2ORC2 papers (250 per domain) with future work sections; aggregated and merged per topic",
            "human_evaluation_details": "AP-FRI created by trained annotators (PhD students) with expert oversight; annotation inter-annotator kappa = 0.94 during dataset construction; annotator training and payment described in appendix.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "AP-FRI corpus size: contains the author-stated FRIs removed from the input papers; used to compute IAScore and to evaluate IdeaMatcher. Dataset and code released by authors (link provided in paper repository).",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "AP-FRI covers only explicit future work statements and may omit legitimate future directions; authors may not enumerate all plausible future ideas; domain coverage limited to papers with explicit FWK sections.",
            "uuid": "e8018.4",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG) framework (applied pipeline)",
            "brief_description": "A pipeline integrating a vector database retriever with an LLM generator to incorporate background knowledge (retrieved related paper contributions) into idea generation to improve relevance and reduce generic outputs.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks.",
            "mention_or_use": "use",
            "model_name": "Retriever (BERT-based embeddings), Generator (LLM such as GPT-4 / Claude)",
            "model_size": "vector DB built from ~190K CS paper titles/abstracts; retriever returns top-20; generator models as above",
            "scientific_domain": "Computer Science (explicitly), applied also conceptually across domains",
            "theory_type": "framework to augment LLMs for generating FRIs",
            "evaluation_method_name": "RAG pipeline (Vector DB + Retriever + LLM Generator)",
            "evaluation_method_description": "Build a vector DB of ~190K CS paper titles & abstracts with BERT embeddings; retrieve top-20 related papers for a target title via cosine similarity; extract contributions using LLM prompts; include retrieved background in LLM prompt to generate distinct FRIs while instructing not to repeat background ideas.",
            "evaluation_metric": "Effect on novelty/relevance/feasibility and reduction in non-novel/generic ideas (measured via human eval and IAScore/DI changes)",
            "metric_definition": "Reported relative percentage improvements in reducing non-novel/generic ideas and changes in relevance/feasibility as measured by human evaluation.",
            "dataset_or_benchmark": "Vector DB constructed from Semantic Scholar API titles/abstracts (~190K CS papers); retrieval uses BERT embeddings",
            "human_evaluation_details": "Human evaluations compared outputs with and without added background knowledge; measured novelty categories and relevance/feasibility percentages.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Adding background knowledge reduced non-novel idea generation by ~50% (GPT-4) and ~53.33% (Claude) and modestly improved relevance and factual correctness for GPT-4 (relevance +0.77%, feasibility +0.34%); Claude saw small decreases in some scores.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Background can cause models to rehash existing ideas if not properly constrained; improvements in high-novelty idea generation were small; effectiveness depends on quality of retriever and prompts.",
            "uuid": "e8018.5",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "BERTScore",
            "name_full": "BERTScore (embedding-based similarity metric)",
            "brief_description": "An embedding-based text similarity metric that measures semantic overlap using deep contextualized embeddings (here used to compare generated ideas to AP‑FRIs), applied with a RoBERTa-large backbone and layer selection.",
            "citation_title": "Bertscore: Evaluating text generation with bert.",
            "mention_or_use": "use",
            "model_name": "BERTScore implementation using RoBERTa-large",
            "model_size": "RoBERTa-large (24-layer) used; embedding from 17th layer as per implementation details",
            "scientific_domain": "Cross-domain (applied to compare idea texts across domains)",
            "theory_type": "semantic similarity metric for idea matching",
            "evaluation_method_name": "BERTScore similarity thresholding",
            "evaluation_method_description": "Compute BERTScore between generated idea and AP-FRI; compare to an empirically chosen threshold on validation set to decide match/non-match.",
            "evaluation_metric": "Matching accuracy on annotated idea pairs (binary match/non-match)",
            "metric_definition": "Threshold determined on validation set; paper reports BERTScore matching accuracy = 75.4% on the small annotated set.",
            "dataset_or_benchmark": "Small annotated idea-matching corpus (61 positive pairs + equal negatives) used for validation/test",
            "human_evaluation_details": "Threshold selection and validation performed using the annotated corpus; human-annotated ground truth used to compute accuracy.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "BERTScore outperformed RoBERTa MNLI entailment (75.4% vs 65.5%) but was outperformed by prompted GPT (91.8%) for idea matching.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "BERTScore relies on surface-level embedding similarity and may miss paraphrastic or broader conceptual matches; thresholding requires tuning and small annotation set limits robustness.",
            "uuid": "e8018.6",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Distinct-N",
            "name_full": "Distinct-N (n-gram diversity metric)",
            "brief_description": "An existing lexical diversity metric that counts distinct n-grams to penalize repetition; mentioned as insufficient for semantic idea distinctness and used as motivation to develop the Idea Distinctness Index.",
            "citation_title": "A diversity-promoting objective function for neural conversation models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General text generation evaluation (not specific to domain)",
            "theory_type": "lexical diversity metric",
            "evaluation_method_name": "Distinct-N (mentioned)",
            "evaluation_method_description": "Measures the ratio of distinct n-grams in generated text to penalize repetitive outputs; previously used for dialogue/response diversity.",
            "evaluation_metric": "Distinct-1, Distinct-2 etc. (n-gram distinctness ratios)",
            "metric_definition": "Distinct-N = (# distinct n-grams) / (total # n-grams) computed per sentence/document; range 0–1.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": null,
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Mentioned as inadequate for semantic-level idea comparison, motivating semantic embedding-based DI.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Distinct-N captures syntactic/lexical variety but not semantic diversity; can be gamed by paraphrasing without changing idea content.",
            "uuid": "e8018.7",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "S2ORC2/SemanticScholar",
            "name_full": "S2ORC2 / Semantic Scholar Open Research Corpus (and Semantic Scholar API)",
            "brief_description": "The paper's source corpus and retrieval resource: 1,250 papers collected from S2ORC2 (250 per domain) for dataset creation; additional ~190K CS titles/abstracts were retrieved via the Semantic Scholar API to build the vector database for RAG.",
            "citation_title": "S2ORC: The semantic scholar open research corpus.",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Science, Physics, Chemistry, Economics, Medical (domains sampled)",
            "theory_type": "dataset / corpus",
            "evaluation_method_name": "Dataset source for idea generation evaluation",
            "evaluation_method_description": "Collected recent papers (2023–2024) with explicit future work sections; removed authors' future work for generation; AP-FRI compiled from removed text; additional 190K CS titles/abstracts used to provide background via RAG.",
            "evaluation_metric": "Dataset statistics (250 papers/domain; 1,250 papers main corpus; 190K CS titles/abstracts for vector DB)",
            "metric_definition": "Counts and filtering criteria: English-language, full-content available, contains 'future work' section; papers from Jan 2023–Feb 2024 (main corpus); supplementary 200 papers Jan–Mar 2025 for new-model eval.",
            "dataset_or_benchmark": "S2ORC2 / Semantic Scholar API collections (paper repository used in study)",
            "human_evaluation_details": "Human annotators (trained PhD students) annotated FRIs from these papers; authors later evaluated generated ideas drawn from the same sets.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Final dataset: 250 papers per domain curated; AP-FRI corpus derived from these; supplementary 200 CS papers used in new-model evaluation.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Coverage limited to English papers with extractable PDFs and explicit future-work sections; possibility of model memorization of some pretraining data mitigated but not eliminated.",
            "uuid": "e8018.8",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "EmbeddingModels",
            "name_full": "Embedding models used for similarity and retrieval (text-embedding-ada-002; OpenAI GPT embeddings; BERT embeddings)",
            "brief_description": "The study uses several embedding families: BERT embeddings for vector DB/retrieval, OpenAI GPT/text-embedding-ada-002 for idea semantic encoding in DI, and RoBERTa layers for BERTScore.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-embedding-ada-002 (OpenAI), OpenAI GPT embeddings, BERT (for retriever), RoBERTa (for BERTScore)",
            "model_size": "embedding dimensionalities not explicitly specified; models named in paper (text-embedding-ada-002 referenced)",
            "scientific_domain": "Applied across domains in retrieval, matching and distinctness computations",
            "theory_type": "tooling/method for semantic similarity and retrieval",
            "evaluation_method_name": "Embedding-based retrieval and similarity (cosine similarity used throughout)",
            "evaluation_method_description": "BERT embeddings used to build vector DB and retrieve top-k related papers via cosine similarity; OpenAI GPT embeddings used to encode ideas for DI computation; RoBERTa-based embeddings used in BERTScore.",
            "evaluation_metric": "Cosine similarity scores for retrieval and pairwise idea similarity; used to compute DI as 1 − cosine_sim.",
            "metric_definition": "Cosine similarity in R^d embedding space (range −1 to 1, typically 0–1 for positive similarity); DI uses 1 − cosine_sim to map to distinctness in [0,1].",
            "dataset_or_benchmark": "Vector DB (~190K CS titles/abstracts); generated ideas sets; AP-FRI",
            "human_evaluation_details": "Embedding thresholds tuned on validation; BERTScore threshold empirically set to 0.68 in implementation notes.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Embedding-based retrieval and DI computations underpin RAG improvements and distinctness quantification; BERT-based retriever and OpenAI embeddings used successfully in pipeline experiments.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Results sensitive to choice of embedding model and dimensionality; cosine similarity may not capture fine-grained conceptual differences.",
            "uuid": "e8018.9",
            "source_info": {
                "paper_title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks.",
            "rating": 2
        },
        {
            "paper_title": "Bertscore: Evaluating text generation with bert.",
            "rating": 2
        },
        {
            "paper_title": "A diversity-promoting objective function for neural conversation models.",
            "rating": 2
        },
        {
            "paper_title": "S2ORC: The semantic scholar open research corpus.",
            "rating": 2
        },
        {
            "paper_title": "PaperRobot: Incremental draft generation of scientific ideas.",
            "rating": 1
        },
        {
            "paper_title": "Automated hypothesis generation based on mining scientific literature.",
            "rating": 1
        }
    ],
    "cost": 0.0212245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models Unlock Novel Scientific Research Ideas?</p>
<p>Sandeep Kumar 
Vinayak Goyal 
Asif Ekbal 
Rohan Anil 
Sebastian Borgeaud 
Yonghui Wu 
Jean- Baptiste Alayrac 
Jiahui Yu 
Radu Soricut 
Johan Schalkwyk 
Andrew M Dai 
Anja Hauth 
Katie Mil- Lican 
David Silver 
Slav Petrov 
Melvin Johnson 
Ioannis Antonoglou 
Julian Schrittwieser 
Amelia Glaese 
Jilin Chen 
Emily Pitler 
Timothy P Lilli- Crap 
Angeliki Lazaridou 
Orhan Firat 
James Molloy 
Megha Ayoub 
George Goel 
Enrique Tucker 
Maxim Pi- Queras 
Iain Krikun 
Nikolay Barr 
Ivo Savinov 
Becca Danihelka 
Anaïs Roelofs 
Anders White 
Tamara Andreassen 
Lakshman Von Glehn 
Mehran Yagati 
Lucas Kazemi 
Misha Gonzalez 
Khalman 
Mathias Benedek 
Emanuel Jauk 
Andreas Fink 
Karl Koschutnig 
Gernot Reishofer 
Franz Ebner 
Aljoscha C Neubauer 
Faeze Brahman 
Chandra Bhagavatula 
Valentina Py- Atkin 
XiangJena D Hwang 
Lorraine Li 
Hirona J Arai 
Soumya Sanyal 
XiangKeisuke Sakaguchi 
Sébastien Bubeck 
Varun Chandrasekaran 
Ronen Eldan 
Johannes Gehrke 
Eric Horvitz 
Ece Kamar 
Peter Lee 
Yin Tat Lee 
Yuanzhi Li 
Scott M Lundberg 
Harsha Nori 
Hamid Palangi 
Marco Túlio Ribeiro 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Pondé 
Oliveira Pinto 
Jared Kaplan 
Harrison Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Nick Ryder 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Joshua Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder </p>
<p>†Department of Computer Science and Engineering
Indian Institute of Technology Patna
India</p>
<p>‡National Center for Computational Sciences
Oak Ridge National Laboratory
USA</p>
<p>Michael Isard
Paul Ronald Barham, Tom Henni-gan, Fabio ViolaBenjamin Lee</p>
<p>Malcolm Reynolds
Yuanzhong Xu, Ryan Doherty, Eliza RutherfordEli Collins, Clemens Meyer</p>
<p>Erica Moreira
Kareem</p>
<p>Can Large Language Models Unlock Novel Scientific Research Ideas?
3A0ED3DF193F8B6266845810EB002EC4
The widespread adoption of Large Language Models (LLMs) and publicly available Chat-GPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives.This study examines the ability of Large Language Models (LLMs) to generate future research ideas from scientific papers.Unlike tasks such as summarization or translation, idea generation lacks a clearly defined reference set or structure, making manual evaluation the default standard.However, human evaluation in this setting is extremely challenging -it requires substantial domain expertise, contextual understanding of the paper, and awareness of the current research landscape.This makes it time-consuming, costly, and fundamentally non-scalable, particularly as new LLMs are being released at a rapid pace.Currently, there is no automated evaluation metric specifically designed for this task.To address this gap, we propose two automated evaluation metrics: Idea Alignment Score (IAScore) and Idea Distinctness Index.We further conducted human evaluation to assess the novelty, relevance, and feasibility of the generated future research ideas.This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations.Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas.We make our datasets and codes publicly available 1 ."Innovation is seeing what everybody has seen and thinking what nobody has thought" -Dr.</p>
<p>Introduction</p>
<p>An idea can be defined as a thought or suggestion aimed at solving a problem or considering a possibility.This concept is central to fields ranging from Figure 1: Large language model suggesting future research ideas after reading a research paper philosophy to science and economics.According to (Plato et al., 2000), ideas are archetypal forms that represent the most accurate reality.In the context of scientific research, (Kuhn and Hawkins, 1963) in "The Structure of Scientific Revolutions" describes an idea as a realization or hypothesis that can challenge and shift paradigms within a scientific community.Therefore, an idea can be understood as a cognitive construct that arises from the human mind's ability to process information, reflect, and imagine, serving as a cornerstone for creativity, problem-solving, and innovation.Idea generation can generally be understood as a state of focused internally-directed attention involving controlled semantic retrieval (Benedek et al., 2014).</p>
<p>As technology improves, new capabilities emerge.Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine (Zhao et al., 2023).Technological advancements serve two key functions in innovation.Firstly, they influence the goals of generating and selecting ideas.Secondly, they impact the methodology of how ideas are generated and chosen (Kornish and Hutchison-Krupat, 2017).Large Language Models (LLMs) have exhibited unparalleled mastery of natural language processing (NLP).Since, these have become increasingly powerful, researchers have begun to investigate their reasoning ability in problem-solving tasks (Yao et al., 2022;Brahman et al., 2023).The concept of an idea is essentially a new combination arXiv:2409.06185v2[cs.CL] 27 Oct 2025 of old elements.LLMs have access to a broad spectrum of knowledge, due to their extensive training on vast amount of text data.However, understanding how information extracted from a research paper can give rise to new ideas, which have not yet been explored much.This leads us to ponder: Can Large Language Models read a scientific paper and suggest new research ideas or directions?Motivated by this, in this paper, we analyze the potential of LLMs in generating future research directions or ideas.As LLMs possess knowledge across various domains, we investigate five specific areas, viz.Computer Science, Physics, Chemistry, Economics, and Medicine.To address this task, we create a dataset of papers published post 2022 from these five domains.To the best of our knowledge, there is no existing automated evaluation metric specifically designed for assessing the quality of future research ideas (FRIs) generated by LLMs.Unlike tasks such as summarization or translation, idea generation lacks a clearly defined reference set or structure, making manual evaluation the default standard.However, human evaluation in this context is particularly challenging-it demands substantial domain expertise, a deep understanding of the source paper, and awareness of the current research landscape.As a result, it becomes time-consuming, costly, and fundamentally non-scalable, especially given the rapid pace at which new LLMs are being released.To evaluate the novelty and relevance of ideas generated by LLMs, we propose an Idea Alignment Score (IAScore).This score reflects how well the generated ideas align with those proposed by the authors.Our proposed IAScore provides a scalable, automated, and interpretable metric that captures how well a model can generate author-aligned research directions.While IAScore does not capture all novel ideas, it serves as a lower-bound indicator of a model's competence.We annotate research papers with future research ideas to create a benchmark for evaluation.To assess the models' ability to generate diverse ideas, we introduce the Idea Distinctness Index.We further conduct a human evaluation of 660 generated ideas in the field of computer science to assess their novelty, relevance, and feasibility.Additionally, we analyze and discuss the performance and limitations of four LLMs:</p>
<p>Gemini (Anil et al., 2023), Claude-2 (Anthropic, 2023), GPT-3.5, and GPT-4 (OpenAI, 2023).Our findings demonstrate that LLMs have the potential to generate research ideas that are relevant, distinct, feasible, and novel-to some extent.</p>
<p>To summarize, our main contributions in this paper are as follows:</p>
<p>• We contribute to the ongoing exploration of LLMs' capabilities in generating future research ideas.</p>
<p>• To address the task, we create a novel dataset of recent papers of five domains (Computer science, Economics, Chemistry, Physics, Medical).</p>
<p>• To assess the quality of generated ideas from LLMs, we propose Idea Alignment Score and Idea Distinctness Index to evaluate the idea generation capability.</p>
<p>• We discuss the challenges associated with human evaluation and conduct an in-depth analysis on the generated ideas.</p>
<p>We hope that this work serves as a foundation for future studies focused on accelerating scientific research by automatically generating research ideas.</p>
<p>Related Work</p>
<p>Recently, LLMs have shown tremendous abilities to perform tasks they were not explicitly trained for (Wei et al., 2022;Bubeck et al., 2023).This includes common sense question answering, code generation, and cross-domain problem solving, enriching their utility across unforeseen domains (Chen et al., 2021;Sarsa et al., 2022).Their capability extends to advanced scientific domains such as computer science, physics, medicine, and mathematics (Romera-Paredes et al., 2023;Huang et al., 2023).Technology Semantic Network (TechNet) was proposed to stimulate idea generation in engineering design (Sarica et al., 2021).There have been a few works in the discovery of new proteins to accelerate scientific discovery.The prior work reported in (Spangler et al., 2014) involves utilizing published studies to find new protein kinases that phosphorylate the tumor suppressor protein p53.</p>
<p>A hypothesis is a hunch, assumption, suspicion, assertion or an idea about a phenomenon, relationship or situation, the reality or truth of which you do not know (Kumar, 1996).There have been some works on hypothesis generation.Initial studies on automated hypothesis generation begin by constructing a corpus of distinct concepts.Subsequently, they explore the relationships between these concepts using machine learning techniques, such as analyzing the similarities among vectors representing different words (or concepts) (Tshitoyan et al., 2019), or applying link prediction methods over a graph (where concepts are nodes) (Nadkarni et al., 2021).Recently, (Qi et al., 2023) used LLMs and extensive pre-existing knowledge of various scientific fields for hypothesis generation.Pa-perRobot (Wang et al., 2019) predicts related entities for an input title and writes key elements of a new paper, including the abstract, conclusion, and future work, and predicts a new title.Xu et al. (2023) developed a framework that leverages the concept co-occurrence graphs and a masked language model to explore and verbalize academic ideas.Their method involves constructing evolving concept graphs across various disciplines and utilizing temporal link prediction to identify potential interdisciplinary connections.The framework also incorporates pre-trained language models to articulate these connections in a coherent academic context.SciMON (Wang et al., 2023) showed that LLMs can be guided by seed terms to generate specific ideas.</p>
<p>However, previous works primarily focused on developing methods (linking and explaining entities, which may not sufficiently capture the complexity or explain how LLMs can solve real-world problems) for idea generation, whereas our work exhaustively focuses on evaluating the capability of LLMs in generating research ideas.Our goal is to assess the inherent ability of LLMs to generate future research ideas/directions.</p>
<p>Dataset</p>
<p>Our dataset creation involves three steps: (1) Dataset Collection, (2) Future Research Ideas (FRI) Identification and removal, and (3) FRI generation.</p>
<p>Dataset Collection</p>
<p>We construct a corpus D by collecting 1,250 papers from S2ORC2 from the domains of Computer Science, Economics, Physics, Chemistry, Medical from (Lo et al., 2020) 3 .We prompted the LLM with paper text after removing the future work for generating future ideas.</p>
<p>To ensure the quality and relevance of the data and to utilize the future research ideas mentioned in a paper, the selected papers must meet the following requirements: (1) the paper must contain the full content, and (2) the paper must include a section on future work.</p>
<p>FRI Identification and Removal</p>
<p>We first identify and remove any potential research ideas mentioned throughout the paper.By doing this, we ensure that LLMs have no prior access to these ideas, which could otherwise affect the objectivity of the analysis.</p>
<p>Annotation Process</p>
<p>Inspired by Hao et al. (2020), we define a future research idea as a discussion that the authors believe they will conduct in the future or believe needs to be investigated in future research.We discuss more details about the annotation guidelines, annotation training, annotation process, and annotator's pay in Appendix A.</p>
<p>Future Work Removal</p>
<p>We observed two types of future research ideas (FRIs)4 (Direct FRI and Mixed FRI).We discuss them in details in Appendix H. AP-FRI Corpus: We removed the sentence from the paper's input text if it pertains to Direct FRI.However, in the case of Mixed FRI, we did not entirely remove the sentences; instead, we eliminated only parts of sentences or markers indicating future research ideas.We add the removed future ideas to a corpus, which we refer to as the AP-FRI (Author Perspective Future Research Idea Corpus).This corpus contains the future research ideas proposed by the authors of the paper.Also, before adding to the AP-FRI corpus, we merged the sentences about the same topic into a single group.We illustrate this by an example in Figure 8, mentioned in the Appendix of the paper.Computer Science emphasizes future research extensively, while Chemistry adopts a more concise approach.Refer to the appendix Section A.5 for more details.</p>
<p>Data Statistics</p>
<p>Experiments</p>
<p>FRI Generation using LLM</p>
<p>We investigate various prompts and utilize the following prompts to generate FRIs from the paper text after removing the FRIs mentioned in the paper as discussed in Section 3.</p>
<p>System: You are a research scientist.User: Imagine you are a research scientist.1) Read the full paper and understand it.2) Find out the related works in this direction 3) Brainstorm and follow a step-by-step reasoning approach to generate potential future research ideas:</p>
<p>[paper text] Make sure the future research ideas are very distinct from the related papers.Potential future research ideas from the paper in bullet points are:</p>
<p>Here, '[paper text]' contains the full content of the paper after removal of future work sections.In case the text was larger than the context size of the particular LLM we divided the paper into parts and finally combined the ideas.</p>
<p>Challenges</p>
<p>To accurately assess the novelty, relevance, and applicability of ideas generated by LLMs, evaluators must possess a high level of expertise in the specific domain and a deep understanding of the research topic to fully grasp the context.Additionally, they need knowledge of related literature to evaluate the ideas' future potential and the broader implications of their implementation.</p>
<p>Idea Alignment Score (IAScore)</p>
<p>With the above challenges, the evaluation of ideas generated by LLMs is a challenging process that demands a high number of domain-specific experts.We, therefore, proposed an Idea Alignment Score (IAScore), which reflects how well the generated ideas align with those proposed by the author.The underlying idea for this score is that authors of accepted papers can be regarded as experts in their respective subjects.The reason being that they possess thorough background knowledge and have conducted deep analyses of the research topic before getting the paper accepted.Consequently, they are well-acquainted with the pertinent challenges which also may have been discussed by expert reviewers.Therefore, we propose that future ideas mentioned by the authors in the paper could be utilized as good quality of potential FRIs.</p>
<p>The IAScore quantifies the alignment of newly generated ideas with author's perspectives within a specific domain, and is computed via a two-step process, detailed in Equations 1 and 2.</p>
<p>Initially, we compute the average alignment score AvgScore j for each paper's ideas.The IdeaMatcher model measures the alignment between the paper's author Future Research Ideas (AP-FRI j ) and its each generated idea I ij .The subscript i indexes the i-th idea within the j-th paper, where N j represents the total number of ideas proposed in that paper.
AvgScore j = 1 Nj N j i=1 IM (AP-FRIj, Iij)(1)
LLMs may generate new ideas that even the author may not have thought of.They can also generate additional future ideas, which may or may not be useful.Our goal with this score is to ensure that the LLMs generate potential future research ideas originally proposed by the authors5 .Therefore, in our formula of AvgScore j , the sum of the alignment scores for a paper's ideas is divided by the total number of the author's proposed ideas, N j , to normalize the score.</p>
<p>IAScore domain, M = 1
P P j=1 AvgScore j (2)
Subsequently, we aggregate the individual paper scores to calculate the domain-wise IAScore.This aggregation, presented in Equation 2, averages the AvgScore j values across all P papers within the domain.Higher the value of IAScore domain signifies the more alignment of the generated ideas with author's perspective of all papers generated by model M .</p>
<p>Let's define:</p>
<p>• I valid ⊇ I auth : the set of all valid FRIs for a paper, including all author-defined FRIs.</p>
<p>• The probability that a generated idea I ∈ I gen is useful can be written as:
P (I ∈ I valid ) = P (I ∈ I auth ) + P (I ∈ I valid \ I auth ).
Now assume we cannot directly measure I valid .The only known subset is I auth , written by experts.So:</p>
<p>• IAScore serves as a lower bound estimate for P (I ∈ I valid )</p>
<p>• A model that scores poorly on the IAScore is unlikely to generate
I ∈ I auth ⊂ I valid
Hence, if a model can not even match the known subset I auth , we should not expect it to robustly hit the full space I valid .IAScore is not intended to penalize novelty but to ensure a baseline level of credibility and expert alignment.It acts as a precision-driven benchmark: if a model can not even regenerate well-grounded ideas by domain experts, we cannot trust it to propose higher-quality novel ones.Hence, IAScore is a necessary but not sufficient component of idea evaluation and should complement-not replace-novelty-based costly human evaluation.</p>
<p>IdeaMatcher</p>
<p>To select an effective IdeaMatcher, we create a small annotated corpus.Our dataset was divided using the 30:70 ratio for validation and test sets, respectively.Since our study involves comparing two ideas using a pre-trained model, we did not require a separate training set.We first manually searched for matching pairs of ideas from generated ideas and AP-FRI of the paper.After obtaining 61 matching pairs, we searched for non-matching pairs of ideas, which is straightforward as only one generated idea will match or would not match with another one from AP-FRI while others would not match, so we picked an equal number of nonmatching pairs.Then, we experimented with the idea-matching task by considering it similar to the Natural Language Inference (NLI) task.In particular, we considered the generated FRIs to be hypotheses and their corresponding AP-FRIs of the paper to be premises.If the idea matches, the hypothesis should be entailed by the premise.In particular, we used a pre-trained RoBERTa MNLI model (Liu et al., 2019) for this task.We found that this technique produces many false negative cases, resulting in an accuracy of 65.5%.</p>
<p>We also evaluated the idea-matching capability of BERTScore (Zhang* et al., 2020), as it utilizes BERT embeddings for comparison.We discuss the details in Appendix F. We found that BERTScore performed better than the entailment technique, resulting in an accuracy of 75.4%.We also tried GPT by prompting it with various questions and found that it resulted in 91.8% accuracy when prompted with a specific question prompt below:-Prompt: Your task is to examine whether a particular idea is incorporated within a set of ideas and to what degree.Collection of ideas: {API-FRIs} Single idea: {A generated Idea} Is the single idea contained within the collection of ideas?If yes, quantify its degree of presence or relevance of the single idea in the collection of ideas on a scale from 0 to 1.</p>
<p>We found that GPT performs better than the existing NLI and similarity measure, such as BERTScore.Therefore, we chose GPT for this task6</p>
<p>IdeaDistinctness Index</p>
<p>Distinct-N (Li et al., 2015), is a metric that measures the diversity of a sentence.It focuses on the number of distinct n-grams of a sentence, and thus penalizes sentences with a lot of repeated words.However, comparing two ideas need semantic comparisons rather than just syntactic differences.So, we introduce a method to semantically evaluate the distinctness of the generated ideas.This method in particular leverages semantic embedding to capture the essence of each idea and computes their distinctness based on semantic similarity measures.Given a set of generated ideas I = {id 1 , id 2 , . . ., id n }, representing individual ideas, we first encode each idea into a high-dimensional vector space using a pre-trained GPT embedding (OpenAI, 2022)7 GPT : id i → v i , where v i ∈ R d is the embedding of idea id i and d is the dimensionality of the embedding space.</p>
<p>To quantify the distinctness between pairs of ideas, we compute the cosine similarity between their embeddings, sim(v i , v j ) = v i •v j ∥v i ∥∥v j ∥ , for each pair of ideas (id i , id j ) in I.The distinctness D ij between two ideas i and j is then inversely related to their similarity:
D ij = 1 − sim(v i , v j ).
The overall distinctness of the set I is calculated as the mean of all pairwise distinctness scores:
DI = 1 n(n − 1) n i=1 n j=1,j̸ =i Dij (3)
This measure provides a single scalar value D I that quantifies the average diverseness of ideas within a corpus of ideas, with higher values indicating a greater degree of diverseness among the ideas.</p>
<p>Subsequently, we aggregated the distinctness scores across all ideas in each paper to compute the mean distinctness for that paper.Let P = {p 1 , p 2 , . . ., p m } represent the set of papers in a domain, where m is the number of papers in the domain.Finally, for a comprehensive assessment of model performance within a domain, we averaged the mean distinctness scores of all papers generated by model M as follows:
D domain,M = 1 m m p=1 DI p M (4)
The resultant metric, D domain,M , represents the average idea distinctness for model M in a given domain, indicating the model's ability to generate diverse ideas.</p>
<p>We compute IA Score and Distinctness Index separately per domain to capture how LLM performance varies across disciplines.This design avoids conflating unrelated domains (e.g., chemistry vs. economics) and enables more granular, interpretable evaluation of LLM capabilities.We designed our framework based on the Retrieval-Augmented Generation (RAG) model (Lewis et al., 2020) to integrate background knowledge into LLMs, as illustrated in Figure 6.A detailed explanation is available in the Appendix E, but we summarize the key components here.1) Vector Database: We collected 190K CS paper titles and abstracts via Semantic Scholar API (Kinney et al., 2023) and stored BERT-based embeddings in a vector database.2) Retriever: Computed cosine similarity with target title to retrieve top 20 related papers; used LLM prompts to extract their key contributions from abstracts.3) Generator: Using the retrieved background knowledge, we prompted an LLM to generate distinct and novel future research ideas.</p>
<p>Adding additional background knowledge</p>
<p>Human Evaluation</p>
<p>The evaluation of generated future ideas necessitates familiarity with both previous works related to the subject and the work being evaluated.Specifically, the evaluator must be an expert in the domain and topic.Given the complexity of human evaluation, we approached authors (as the authors have the knowledge of their paper and they also have knowledge of the literature) who have published papers in reputable venues, possess over 5 years of experience in scientific publishing, and have authored more than 5 scientific papers.We collected their accepted papers (published between 2023 and 2024) and followed the process of dataset preparation as we discussed in Section 3, and generated FRIs.We modify the prompt slightly to specifically generate only the top five results (see Appendix B).We selected the outputs from Claude and GPT-48 models due to their better IAScore and Idea Distinction index.We adopt this approach to avoid author exhaustion and to get an accurate evaluation.More details about the human evaluation are mentioned in the Appendix B.</p>
<p>Results and Discussion</p>
<p>Alignment Results</p>
<p>Figure 3 provides a comparative overview of the IAScore for four language models9 Claude-2, Gemini-1.0,GPT-3.5, and GPT-4 across five academic domains: Chemistry, Computer Science, Economics, Medical, and Physics10 .</p>
<p>In the Chemistry and Economics domains, Claude has the highest IAScore, indicating strong alignment with the authors' future research ideas.Claude and GPT-4 have almost similar values for the Computer, Medical, and Physics domains (with GPT-4 slightly higher).GPT-3.5 and Gemini have lower scores than both GPT-4 and Claude in every domain.GPT-3.5 has almost the same score as Gemini in the Chemistry and Economics domains.However, it scores higher than Gemini in the Computer, Medical, and Physics domains.The results underscore the advancements in language model capabilities, with each model showcasing domainspecific strengths in idea generation.This alignment of LLMs shows that LLMs are able to generate relevant and novel ideas to some extent.We also studied the effect of length of future work on IAS-core (See Appendix D).Our overall analysis shows that ideas of moderate length (20-40 words) achieve the highest Impact Assessment Scores (IAScores), balancing detail and clarity, while shorter (&lt;20 words) and longer (40-60 words) ideas tend to score lower due to insufficient detail.We also conducted a human analysis to understand the quality of research ideas generated when the IAScore is low (see Appendix G).</p>
<p>Distinctness Results</p>
<p>We show the comparative evaluation of idea distinctness scores in Figure 4.The line graph depicts the variation of distinctness between the generated ideas and the human-written ideas (AP-FRIs).GPT-3.5 shows the least distinctness among the generated ideas, except in the Computer domain, where it is slightly more distinct than Gemini.As shown in the graph, the distinctness of Gemini is also quite low; however, it is slightly better than GPT-3.5, except in the Computer domain.</p>
<p>The generated ideas of GPT-4 are more distinct than those of Gemini and GPT-3.5 (except for economics, whereas the distinctness of GPT-4 is the same as Gemini).However, it is lower than both Claude and Human.The Idea Distinctness Index of the generated ideas from Claude are almost the same as those of humans for Chemistry, Economics, and Medical domains.However, they are higher than even human scores in the Computer and Physics domains, which shows that it generates very distinct FRIs.</p>
<p>Human Evaluation Results</p>
<p>We conducted a human evaluation on 660 generated ideas for 66 papers in the computer science domain.</p>
<p>To validate the quality of human annotation, we measure the inter-annotator agreement ratio where 20% of the generated ideas are evaluated by two different authors of the same paper.We measured Cohen's kappa coefficient (Cohen, 1960), which was 0.83, confirming the high-quality annotation of the research ideas generated.</p>
<p>Novelty: Figure 5 displays the results of the human evaluation.We observed that Claude generates 14.78% of non-novel and 16.52% generic FRIs, 41.73% moderately novel, 20.86% very novel, and 16.52% extremely novel FRIs.GPT generates 7.83% not-novel, 13.91% generic, 42.61% moderately novel, 28.70% very novel, and 6.96% extremely novel ideas.Claude generates more nonnovel and generic ideas than GPT-4, while GPT-4 produces more very novel ideas and nearly the same number of excellent ideas.</p>
<p>We conducted an additional human evaluation in the physics domain, analyzing 630 generated ideas across 63 papers.We observed that Claude generates 15.28% non-novel, 16.02% generic, 31.22%moderately novel, 20.36% very novel, and 17.12% extremely novel FRIs.GPT produces 7.33% notnovel, 14.53% generic, 42.22% moderately novel, 28.0% very novel, and 7.95% extremely novel ideas, similar to previous observations in computer science.This demonstrates that although LLMs also generate generic or already explored ideas, they are capable of producing novel ideas that have either not been explored or have been minimally explored 11 .</p>
<p>Relevance and Feasibility: After human evaluation, for Computer Science domain, we found that 76.67% of the ideas generated by Claude and 93.34% by GPT-4 are relevant.Furthermore, 83.34% of Claude's generated ideas and 96.64% of GPT-4's ideas were judged to be practically feasible and factually correct.Similarly, for the Physics domain, after human evaluation, we found that 78.45% of the ideas generated by Claude and 91.56% by GPT-4 are relevant.Furthermore, 85.67% of Claude's generated ideas and 94.78% of GPT-4's ideas were judged to be practically feasible and factually correct.</p>
<p>These results highlight that Claude and GPT-4 can generate relevant and feasible research ideas.However, the reason Claude generates more impractical and irrelevant research ideas may be that Claude attempts to generate more distinct research ideas than GPT-4, as we evaluated and discussed in Section 5.2.</p>
<p>Open-ended generation:</p>
<p>We tested whether LLMs could retain open-ended generation capabilities by providing only a title and abstract as input.Our findings showed that, overall, LLMs can still generate open-ended content due to their past knowledge.However, they may not produce many high-quality ideas, as they lack access to recent publications and methodological insights relevant to the current paper.We discuss this in details in the Appendix C.</p>
<p>Impact on Adding Background Knowledge</p>
<p>We found that adding background knowledge reduced the generation of generic or non-novel ideas and improved relevance and factual accuracy.However, further research is needed to boost the novelty of generated ideas.We discuss this in details in the Appendix E.  As several new models were released after our initial experiments, we extended our analysis to include their results as well, as shown in Figure 7. GPT-4o achieves the highest IAScore in the Computer Science domain (≈ 0.41), marginally surpassing GPT-4 and Claude-3 (both ≈ 0.39).Claude-2 and Gemini-2.5 follow at ≈ 0.37, while Llama-70B and Gemini-1.0trail at ≈ 0.31 and ≈ 0.28, respectively, illustrating the incremental gains delivered by successive model releases.We discuss more details on Appendix K.</p>
<p>Analysis on New Models</p>
<p>Conclusion and Future Work</p>
<p>In conclusion, we present the first attempt to evaluate the potential of LLMs in generating future research ideas across five domains: Computer Science, Economics, Chemistry, Physics, and Medicine.Here, we presented the first systematic attempt to evaluate the potential of LLMs for generating future research ideas across five diverse domains.By introducing the Idea Alignment Score (IAScore) and the Idea Distinctness Index, we offered scalable, interpretable metrics that complement costly human evaluation and provide a structured way to assess the quality of generated ideas.Our results and analysis show that LLMs possess domain-specific strengths in idea generation.Furthermore, the results from the Idea Distinctness In-dex indicate that LLMs, such as Claude and GPT-4, generate more distinct research ideas than Gemini and GPT 3.5.GPT-4 and Claude align better with authors written future research ideas than Gemini and GPT-4.The alignment of LLMs with the authors of generated ideas, and our human evaluation on relevance, novelty, and feasibility, reveal that although LLMs often produce non-novel and generic ideas, they have the potential to generate relevant and novel and diverse ideas to a significant extent.We hope that the findings and experiments of this work will unlock the potential of LLMs in idea generation and will foster new advancements in automated scientific innovation.</p>
<p>In future, we plan to automate the extraction and annotation pipeline to enable scalable, automatic evaluation of our metrics for newly released models.Also, beyond relying on authors' stated plans, future datasets could integrate ideas from related work, citing papers, and community discussions to reduce bias and capture broader research directions.</p>
<p>Limitations</p>
<p>Limitations of Data Collection</p>
<p>We extracted papers using the Semantic Scholar Academic Graph API from January 2023 to February 2024.The number of papers available is limited by the scope of our data extraction from the Semantic Scholar Academic Graph.We excluded papers that are not in English, as well as those whose abstracts could not be correctly parsed from the PDFs.Not all of these papers include sections on future work; therefore, we annotated only those that contained sections outlining future research directions.So due to such limitations, we collected 250 papers from each domain for analysis.(Carlini et al., 2022) highlight that LLMs are prone to memorizing portions of their training data, a significant concern in the evaluation of contemporary LLMs.Despite this, the data used for pre-training and post-training includes "a small amount" of more recent data (Wang et al., 2023).Therefore, we gathered recent papers from 2023 and 2024.By focusing our evaluation on papers published in these years, the likelihood of test papers appearing in the pre-training corpora for the models is substantially reduced.In addition, we conducted a manual review of these papers to assess memoriza-tion.This involved asking various questions related to the papers, such as their titles, publishing venues, author names, etc., to see if the models could supply the missing information.Our findings showed no evidence of such memorization occurring.A similar approach is also followed by (Wang et al., 2023) (discussed in Section 6.4) and even they did not find any evidence of this occurring.</p>
<p>Memorization</p>
<p>Limitation of IAScore and Idea</p>
<p>Distinctness Index</p>
<p>IAScore provides a systematic and interpretable method for measuring the alignment between ideas generated by LLMs and those identified by domain experts.While effective as a benchmark for assessing credibility and relevance, IAScore primarily reflects alignment with author-specified future research directions.Consequently, it may not fully capture innovative ideas proposed by LLMs that authors themselves have not explicitly mentioned.IAScore's performance is sensitivity to accuracy and comprehensiveness of the authoridentified future directions.Moreover, IAScore emphasizes alignment rather than explicitly evaluating the depth, complexity, or practical feasibility of generated ideas.</p>
<p>As there is currently no reliable automatic metric for evaluating idea generation, human evaluation remains the primary approach.However, it is costly, resource-intensive, and not scalable.Our findings show that IAScore strongly aligns with expert judgments, but for more accurate and nuanced evaluation, human assessment is still preferred.Also, Idea Distinctness Index, which relies on cosine similarity, might miss subtle differences between ideas that appear semantically similar but actually represent distinct research directions.</p>
<p>Ethics Statement</p>
<p>We have utilized the open source dataset for our work.Our aim for this work is to assess the potential of language models in generating ideas.Our Institutional Review Board (IRB) evaluated and approved this study.We do not encourage the use of LLMs to generate AI generated research papers (by generating new ideas) or misuse it for harmful idea generation.LLMs can process and synthesize vast amount of literature faster than humans, potentially identifying new patterns or gaps in research that might not be obvious, thus accelerating scientific discovery.However, since LLMs can generate con-tent that may be similar to existing materials, this raises concerns about intellectual property rights and the originality of ideas.LLMs utilized for generating ideas might be misapplied to produce harmful materials such as plans for schemes for designs for destructive devices, explosive devices, ideas for spamming.Notably, it is a common challenge among existing LLMs with strong creative and reasoning abilities.So, we emphasize the responsible use of LLMs for idea generation and the need to broadly improve the safety of LLMs.</p>
<p>A Dataset Annotation</p>
<p>A.1 Dataset Annotation Guidelines</p>
<p>Recognizing future research idea in a paper involves analyzing the portion of text containing directions for future research.The following steps can be followed:</p>
<p>Step 1: Begin by reading the Title and Abstract of the paper to gain an understanding of its subject matter.It is important to read these sections multiple times to grasp the paper's main points, such as its motivation, contributions, and other relevant aspects.If necessary, refer to the paper itself or read related material to enhance your understanding.</p>
<p>Step 2: Identify Key Sections for Analysis Focus primarily on the Discussion and Conclusion sections of the paper, as these areas often contain explicit mentions of future research directions.Scan the Methodology section as well, as sometimes suggestions for improving future studies or addressing current study limitations are mentioned here.</p>
<p>Step 3: Distinguish Future Research Ideas from General Statements: Differentiate explicit future research suggestions from general discussion.Future research directions usually involve specific recommendations, plans, or identified gaps that require further exploration.These are often phrased using terms like "future studies should," "further research is needed," or "additional work will."Avoid confusing these with broader statements of potential relevance or applicability, which do not provide direct guidance on future work.</p>
<p>We offer multiple examples of papers with its future research ideas to assist and direct the annotators.We found a few text which looks like future work but is on contrary the motivation of the work.As an example, consider the following: "The goal of this work was to direct attention to emerging and novel research involving "magnetogel nanohybrid materials" that might be relevant in future applications for the treatment of wastewater, as well as in other fields.</p>
<p>The second example is: "Our data could be useful for designing high-quality trials in the future to define the exact role of hemoadsorption in ARDS.".</p>
<p>This sentence describes the future applications of existing research on magnetogel nanohybrid materials, not a specific direction for future research.</p>
<p>Also another example is: "The goal of this work was to direct attention to emerging and novel research involving magnetogel nanohybrid materials that might be relevant in future applications for the treatment of wastewater, as well as in other fields."This is the application in future, and not the future work.</p>
<p>Step 4: Separate Future Research from Limitations: Carefully examine any limitations mentioned in the paper to determine if they are explicitly linked to future research.Only consider a limitation as future work if the authors clearly indicate a direct intention to address it in subsequent studies.This helps avoid assuming that all limitations naturally lead to future research directions.</p>
<p>There is also very thin line between limitation and future work, where a limitation can or cannot be a future work.There were few cases where limitations were mentioned "One limitation of this paper is the absence of a coordinated attention structure to capture cross-channel information.".As limitations can or cannot be a future work, we only take those limitations which is explicitly mentioned by the author to be a future work.Hence, we only considered the explicit mention of the future</p>
<p>A.2 Annotator Training</p>
<p>Given the complexity of the papers and their frequent use of technical terminology, we hired two doctoral students, each boasting over four years of experience in scientific research publishing.To facilitate their training, an expert with more than ten years of experience in scientific publishing annotated 20 random papers from each domain, adhering to our guidelines.After this initial round of annotation, we reviewed and corrected any misinterpretations with the annotators, further refining their training and enhancing the clarity of our annotation guidelines.To assess the effectiveness of the initial training, we compiled another 20 papers from each domain.From the second round onward, the annotators consistently identified 95% or more of the future research ideas correctly, based on comparison with expert-annotated labels.</p>
<p>A.3 Annotation Process</p>
<p>We regularly monitored the annotated data, placing emphasis on identifying and rectifying inconsistencies and cases of confusion.We also implemented an iterative feedback system that continuously aimed to refine and improve the annotation process.In cases of conflict or confusion, we removed those papers as we wanted only good quality dataset.Following the annotation phase, we obtained an average inter-annotator agreement score of 0.94 using Cohen's kappa (Cohen, 1960), indicating a substantial consensus among the annotators.</p>
<p>A.4 Annotator's Pay</p>
<p>We compensated each annotator according to the standard PhD salaries in India, based on the hours they worked.The appointment and salaries adhere to our university's established practices.Payment was made per paper since the time required to read and extract future research ideas from each paper varies, depending on its complexity, technical terminology, and the annotator's familiarity with the subject.Thus, paying based on time spent could have potentially compromised the quality of the annotations.During the pilot phase, we observed that hourly-based payments encouraged speed over accuracy, potentially compromising annotation quality.In contrast, paying per paper ensured that annotators could dedicate the necessary effort and time required by each individual paper without worrying about time constraints or inefficiencies associated with clock-based metrics.In cases where papers were especially complex or ambiguous, we excluded them from the dataset rather than risk subpar annotation quality.To maintain accuracy and prevent fatigue, we imposed a daily limit of six hours for annotators.</p>
<p>A.5 Detailed Dataset Statistics</p>
<p>Figure 2a provides a domain-wise distribution of the average word count in academic papers, excluding discussions on future work (FWK).It can be observed that the length of papers across all fields falls within a range of 7,000 to 8,000 words.Additionally, we calculated the average word count of extracted future work within each domain, providing comparative insights into how different fields prioritize discussions of future research directions.Figure 2b compares the average word count of future work text across six distinct scholarly domains.We observed that the literature in Computer Science notably prioritizes extensive discourse on future research, with an average word count significantly higher than that of other disciplines.In contrast, the literature in Chemistry demonstrates a more concise approach to discussions of future research, as evidenced by its lower average word count.</p>
<p>B Human Annotation</p>
<p>We ask the following questions from each human evaluator:-• Q1: Is the idea relevant with the research topic of the paper.(Relevant/Not relevant)</p>
<p>• Q2: Assess the originality/novelty of the research idea (5 scale)</p>
<p>• Q3: Review the research idea for factual correctness and feasibility.Is the idea impractical or too vague to be actionable?(Not Possible/Possible)</p>
<p>For Q2, we used Best-Worst Scaling (Louviere et al., 2015) on a 5-point scale.</p>
<p>We prepared a Google Form for each paper and provided the links to the annotators.We also specified instructions for them at the beginning of the form.We have added an example of the form for a paper in Figure 10, Figure 11, and Figure 12.</p>
<p>Here is the little modified from for human evaluation that generates only top 5 research ideas:-</p>
<p>C Effect of giving only Title and Abstract as Input</p>
<p>We found a few cases where we provided only a title and abstract as input to see if LLMs can still retain open-ended generation capabilities.We discovered a few cases where GPT-4 still generated novel ideas, such as for a paper (Kumar et al., 2023) it generated: "Incorporate explainable AI methods to provide transparency into how the AI model makes its predictions, thus making the outcomes more interpretable and acceptable to human editors.".This kind of analysis has not been done yet and could be helpful.After providing full paper content to the model we found that same idea was again generated.There were also cases where GPT-4 generated a novel idea of solving the problem using transformers for a task (The task was mostly solved using techniques like RNN), which had not been done before.However, after providing the full paper content, the model understood that this transformer has already been implemented for this task, so further suggested to add more contextual information to it to boost the performance (limited information was given as input to the paper).Overall, we found that LLMs can still retain open-ended generation because it has past knowledge.But it may not generate many good ideas since it does not have access to recently published papers or other methodological findings related to the current paper.</p>
<p>D Effect of Length of Idea on IAScore</p>
<p>In our analysis, we explore the relationship between the length of ideas and their corresponding Impact Assessment Score (IAScore), specifically focusing on computer science papers and outputs generated by GPT-4.This relationship is visually represented in the bar chart found in Appendix Figure 9.The data reveal that shorter ideas, typically under 20 words, tend to receive lower IAScores.This could be attributed to their lack of detailed information, which might be essential for a comprehensive understanding and assessment.Conversely, we observe that ideas spanning 40-60 words also tend to score lower.This may result from their verbosity; excessive information can dilute the core message, making it challenging to discern the main points.Interestingly, ideas with a moderate length, ranging from 20 to 40 words, achieve the highest IAScores.This length seems optimal as it allows for sufficient detail without overwhelming the reader, striking a balance that facilitates clearer understanding.</p>
<p>E Effect of Adding Additional</p>
<p>Background Knowledge</p>
<p>Below we describe our pipeline for retrieving background knowledge and generating future research ideas using LLMs in detail :-</p>
<p>E.1 Vector Database</p>
<p>We utilized the Semantic Scholar API (Kinney et al., 2023) to collect the titles and abstracts of approximately 1.9 lakh existing computer science research papers.We employed BERT embeddings to create vector representations for the titles of these papers, which were then stored in a vector database.</p>
<p>E.2 Retriever</p>
<p>To retrieve relevant papers, we created embeddings for the title of the paper for which we have to generate ideas.We computed the cosine similarity between the embedding of the title of this paper and those from our vector database.We then retrieved the top 20 research papers that exhibited the highest similarity to our target paper title.Finally, we extracted the contributions from these papers to gather relevant data from their abstracts.We used the following prompt to instruct LLM to extract useful information from abstract of the paper:</p>
<p>System: You are a helpful research agent that generates background knowledge or related works given abstracts of papers.User: You are given abstracts of research papers and your task is to extract contributions or findings or methods proposed in the paper.You are not allowed to make any changes to data given to you.Return the response as it is and return response for all 20 papers in passage.Return title of paper followed by its contributions or findings or methods in less than 100 words.If no contributions or findings or methods are found, return NONE.</p>
<p>PASSAGE: '{relevant_passage}'</p>
<p>Potential top 5 future research ideas from the paper in bullet points are:</p>
<p>We designed the above query prompt to ensure that the LLM12 understood its role in extracting relevant information without altering the provided information.</p>
<p>E.3 Generator</p>
<p>Next we produced the ideas using a prompt that includes the prompt using the paper and the retrieved background knowledge.Specifically, we used the below prompt for our task:-System: You are a research scientist.User: Imagine you are a research scientist.1) Read the full paper and understand it.</p>
<p>2) Find out the related works in this direction 3) Brainstorm and follow a step-by-step reasoning approach to generate to potential future research ideas:
[paper text] [background knowl- edge]
Make sure the future research ideas are very distinct from the background knowledge provided.Potential top 5 future research ideas from the paper in bullet points are:</p>
<p>Here, '[paper text]' contains the full content of the paper after removal of future work sections.'[background knowledge]' contains the background knowledge retrieved.An example of background knowledge is shown in Appendix Table 6.</p>
<p>We performed this experiment on the same set of papers and conducted human evaluations for novelty following the same methodology as we discussed in Section 4.6.The results are shown in Figure 5. Initially, we observed that adding background knowledge affected the LLM's performance; it primarily generated ideas that already existed, merely creating new combinations from the background knowledge.Subsequently, we modified the prompt to instruct the model not to repeat ideas that were mentioned in the background knowledge.</p>
<p>We found that adding background slightly improved the task.The results show that the improvements for GPT-4 and Claude were 50% and 53.33%, respectively, in reducing the generation of non-novel ideas.Also, it resulted in the improvement of 7.14% and 11.76% not generating generic ideas of GPT-4 and Claude.We observed that GPT-4 generated 9.52% and 14.63% more moderately novel ideas.However, we noted only a very slight improvement in the generation of highly novel or extremely novel ideas.</p>
<p>The analysis revealed that 73.71% of the ideas generated by Claude and 93.34% by GPT-4 were relevant.We observed that the relevance score for Claude decreased by 2.96%, and GPT-4 increased by a slight 0.77%.Furthermore, 83.14% of Claude's generated ideas and 96.98% of GPT-4's ideas were judged to be practically feasible and factually correct.The score for Claude decreased by 0.20%, and the score for GPT-4 increased by 0.34%.It seems that additional information negatively impacts Claude's performance by generating ideas that are irrelevant, non-novel, and infeasible.However, for GPT-4, we observed that incorporating additional background knowledge helps prevent the generation of non-novel or generic ideas and slightly improves the relevance and factual correctness of the generated ideas.However, further research is needed to enhance the ability of LLMs to generate more novel ideas.</p>
<p>F BERTScore Implementation Details</p>
<p>The motivation to use BERT embeddings is that the generated and the original ideas often do not use the same words, so we need to understand the contextual meanings of the ideas in order to compare them.We used the default setting of the BERTScore metric, which employs a 24-layer RoBERTa-large model and utilizes the 17th layer for embedding.We determined the threshold13 using the validation set.If the similarity exceeds that threshold, we classify those pairs of ideas as similar, and vice versa.</p>
<p>G Error Analysis:</p>
<p>We conducted a human evaluation using three expert annotators, each with more than five years of experience in this field.They reviewed 15 papers.We assigned papers to each reviewer based on their familiarity with the subject matter of the papers.We identified two major reasons for the low IAS score:</p>
<p>• Generic Ideas: Few ideas such as "Explore different explainability methods like LIME, SHAP to generate model explanations instead of just rationales.Compare their effectiveness.",Building on the baseline model, future research could explore more advanced NLP models and techniques for contradiction detection.are generated.These statements are true; however, they are very generic and are common.</p>
<p>• Not Feasible Ideas: We observed that LLMs occasionally propose ideas that are not feasible, even in principle.For instance, in one case, the model suggested "collecting realtime, large-scale annotated data on private peer review discussions between authors and reviewers to study the evolution of scientific ideas."Such data is not only unavailable, but also extremely unlikely to ever be accessible due to strict confidentiality and privacy concerns in the peer review process.Human researchers, on the other hand, are typically more attuned to such ethical and practical constraints and tend to propose ideas that are realistically actionable.</p>
<p>• Author Miss: Due to page limits or more novel ideas, the author fails to mention a few ideas in a paper.For example, for a paper (Kumar et al., 2023) GPT-4 generated idea: "Exploring the Impact of Contradictions on Review Outcomes: An interesting area for future research would be to study the impact of reviewer contradictions on the outcomes of the peer review process.This could involve analyzing the correlation between the presence and nature of contradictions and the final decisions made by editors (acceptance, rejection, major/minor revisions).Such studies could provide valuable insights into how contradictions influence the decision-making process and how they might be effectively managed to improve the fairness and quality of peer review.".This represents a strong, novel research problem not mentioned by the authors, which warrants future investigation H Direct FRI and Mixed FRI</p>
<p>• Direct FRI: When the sentences that mention future research idea only contains future research idea.For example "In future work, we plan to extend our approach to other code-mixed languages and evaluate its performance on more NLP tasks."</p>
<p>• Mixed FRI: We found that sometimes research papers articulate future research ideas along with other essential information of the paper in a single sentence.For example in Figure 8, this sentence not only summarizes the current research findings but also clearly outlines a direction for future work.</p>
<p>I Output Examples</p>
<p>Our LLM generated future research output can be found in Table 1, Table 2, Table 3, Table 4 and Table 5.</p>
<p>J Model Training Cutoff</p>
<p>Regarding model training cutoffs, we used GPT-3.5-turbo-0125for GPT-3.5 (cutoff: December 2023), gpt-4-0125-preview for GPT-4 (cutoff: December 2023), anthropic.claude-v2for Claude 2 (cutoff: August 2023), and gemini-1.0-pro-002(cutoff: February 2023), Llama 3.3 70B Instruct (cutoff: December 2023), gpt-4o-2024-08-06 for GPT4o (cutoff: October 2023), claude-3-opus-20240229 for Claude 3 (cutoff: August 2023), gemini-2.5-pro-preview-05-06for Gemini-2.5 (cutoff: December 2025).</p>
<p>K Evaluation on Newly Released Models</p>
<p>To evaluate the performance of recently released language models, we constructed a supplementary Computer Science dataset comprising 200 papers published between January and March 2025.These papers were sourced from S2ORC using the same English-language, PDF-availability, and Future Work section filters as described in Section 3. Annotation was carried out following the same protocol as for the main dataset (Appendix A), ensuring consistency in labeling.This new dataset forms the basis for the IAScore results reported for GPT-4o, Claude-3, Gemini-2.5, and Llama-70B (Figure 7).We plan to automate the process to automatically evaluate the IAScore for newly released models.1. Investigate the antioxidant and oil-stabilizing effects of peptides from the soy protein hydrolysates.The paper showed the pepsin hydrolysate had better activity, likely due to its peptide composition.Specific peptides could be isolated and tested for their antioxidant effects in oil.</p>
<ol>
<li>Study the effects of the soy protein hydrolysates on the quality and shelf life of the fried food.This paper focused only on the oil stability.</li>
</ol>
<p>It would be useful to also assess how the hydrolysates affect lipid oxidation, texture, color etc. in the fried banana chips.</p>
<ol>
<li>
<p>Evaluate the antioxidant effects of the soy protein hydrolysates in other vegetable oils such as sunflower, canola, corn oil etc. during frying.Since oil composition affects stability, the effects may differ in other oils high in polyunsaturated fatty acids.</p>
</li>
<li>
<p>Examine the impacts of frying parameters such as temperature, food type, frying time etc. on the efficacy of the soy protein hydrolysates.</p>
</li>
</ol>
<p>Varying these factors could provide insights into the optimal conditions for the hydrolysates to exert antioxidant effects.</p>
<ol>
<li>
<p>Study the sensory qualities of fried foods with the soy protein hydrolysate additions.An analysis of taste, aroma, color acceptability by consumers would provide important information for potential commercial use of the hydrolysates as natural antioxidants.Gemini-1.0 1. Investigate the effects of different hydrolysis conditions on the antioxidant activity of soy protein hydrolysates.This could involve varying the enzyme concentration, temperature, and pH of the hydrolysis reaction.</p>
</li>
<li>
<p>Identify the specific peptides in the pepsin hydrolysate that are responsible for its antioxidant activity.This could be done using mass spectrometry and other analytical techniques.</p>
</li>
<li>
<p>Test the antioxidant activity of soy protein hydrolysates in other food systems.This could include using them to stabilize other types of oils or to protect food products from oxidation.</p>
</li>
<li>
<p>Investigate the potential health benefits of soy protein hydrolysates.This could include studying their effects on oxidative stress, inflammation, and other chronic diseases.</p>
</li>
<li>
<p>Develop new food products that incorporate soy protein hydrolysates as antioxidants.This could include developing new types of oil-based products, such as salad dressings and mayonnaise, as well as new types of food products, such as fortified cereals and snacks.GPT-4 1. Exploration of Peptide Sequences and Amino Acid Compositions for Enhanced Oxidative Stability: Investigate the specific peptide sequences and amino acid compositions derived from different proteases besides trypsin and pepsin to understand their contribution to the oxidative stability of oils.This could involve using a variety of proteolytic enzymes to hydrolyze soy proteins and other plant-based proteins, followed by an analysis of their peptide profiles and antioxidant activities in oil and food systems.</p>
</li>
<li>
<p>Application of Soy Protein Hydrolysates in Various Food Systems: Extend the application of soy protein hydrolysates to other food systems beyond frying oils, such as baked goods, dairy products, and meat alternatives, to assess their potential in improving oxidative stability, shelf-life, and nutritional profiles.This could involve comparative studies of hydrolysate effectiveness in different food matrices and processing conditions.</p>
</li>
<li>
<p>Mechanistic Studies on the Antioxidant Action of Hydrolysates: Conduct in-depth mechanistic studies to understand how soy protein hydrolysates interact with lipid molecules and radical species at the molecular level.This could involve computational modeling and in vitro experiments to explore the dynamics of peptide-lipid interactions and the role of specific amino acids in scavenging free radicals and metal ions.</p>
</li>
</ol>
<p>Development of Environmentally</p>
<p>Friendly and Health-focused Antioxidants: Given the interest in replacing synthetic antioxidants with natural alternatives, further research could focus on optimizing the production, purification, and application of soy protein hydrolysates as natural antioxidants.This could include exploring sustainable extraction methods, evaluating the safety and efficacy of hydrolysates in food systems, and assessing consumer acceptance.</p>
<ol>
<li>
<p>Comparative Analysis of Hydrolysates from Different Oilseed Proteins: Since the study focused on soy proteins, future research could explore the antioxidative potential of hydrolysates derived from other oilseed proteins, such as sunflower, canola, or hemp.This would involve a comparative analysis of their antioxidative activities, peptide compositions, and effectiveness in enhancing the oxidative stability of different types of oils and fats used in food processing and storage.GPT-3.5 1. Investigate the impact of different proteases on the antioxidant properties of soy protein hydrolysates in stabilizing oils during frying cycles.</p>
</li>
<li>
<p>Study the influence of peptide size and amino acid composition on the oxidative stability of oils when treated with protein hydrolysates.</p>
</li>
<li>
<p>Explore the potential of specific amino acids, such as tryptophan, histidine, and methionine, in enhancing the antioxidant activity of hydrolyzed soy proteins in frying oils.</p>
</li>
<li>
<p>Examine the effects of pH adjustments during enzymatic hydrolysis on the antioxidant performance of soy protein hydrolysates.</p>
</li>
<li>
<p>Evaluate the long-term stability and effectiveness of pepsin and trypsin hydrolyzed soy protein in preventing oil oxidation in different frying conditions over extended periods.   5. Conduct intervention studies to test the effectiveness of community-based participatory approaches in increasing COVID-19 vaccine acceptance among disadvantaged populations.Gemini-1.0 1. Investigate the impact of socioeconomic factors on vaccine hesitancy and refusal.This could be done through qualitative studies to understand the underlying reasons for vaccine hesitancy and refusal, and quantitative studies to examine the association between socioeconomic factors and vaccine uptake.</p>
</li>
<li>
<p>Examine the role of social networks and community engagement in promoting vaccination.This could involve studying the influence of social networks on vaccination decisions, and exploring the effectiveness of community-based interventions in increasing vaccination rates.</p>
</li>
<li>
<p>Assess the effectiveness of different strategies for addressing vaccine inequality.This could include evaluating the impact of financial incentives, targeted outreach programs, and mobile vaccination clinics on vaccination rates among underserved populations.</p>
</li>
<li>
<p>Develop and evaluate interventions to improve the equitable distribution of vaccines.**This could involve working with governments and international organizations to develop policies and programs that ensure that vaccines are distributed fairly and equitably to all populations.</p>
</li>
<li>
<p>Conduct longitudinal studies to track the long-term impact of socioeconomic factors on vaccination rates.This could help to identify trends and patterns in vaccination uptake over time, and inform the development of policies and interventions to address persistent disparities.GPT-4 1. Impact of Vaccine Acceptance and Hesitancy on Socioeconomic Disparities: Investigate how vaccine acceptance and hesitancy, influenced by cultural, historical, and information dissemination factors, contribute to the observed socioeconomic disparities in vaccination rates.This research could further dissect how misinformation and trust in healthcare systems intersect with socioeconomic status to affect vaccine uptake.</p>
</li>
<li>
<p>Longitudinal Studies on the Evolution of Vaccine Inequality: Conduct longitudinal studies to track changes in vaccine inequality over time, particularly as new variants emerge and booster vaccinations become more common.This research would provide insights into how socioeconomic disparities in vaccination rates evolve with the pandemic's progression and the introduction of new vaccine technologies or strategies.</p>
</li>
<li>
<p>Role of International and Public-Private Partnerships in Addressing Vaccine Inequity: Explore the effectiveness of international collaborations and public-private partnerships in improving vaccine distribution equity.This study could examine case studies of successful collaborations and identify best practices for leveraging these partnerships to reduce disparities in vaccine access.</p>
</li>
<li>
<p>Comparative Analysis of Vaccination Strategies and Socioeconomic Impact: Compare the vaccination strategies of different countries or regions with similar socioeconomic profiles to identify what strategies are most effective in mitigating inequalities in vaccination rates.This research could include the role of mobile vaccination units, community engagement initiatives, and targeted communication strategies in improving vaccine uptake among disadvantaged populations.</p>
</li>
<li>
<p>Impact of Socioeconomic Factors on Post-Vaccination Outcomes: Investigate how socioeconomic factors influence outcomes after vaccination, such as breakthrough infection rates, severity of breakthrough cases, and long-term immunity.This research could help identify whether socioeconomic disparities persist even after vaccination and how these disparities can be addressed through booster doses, tailored public health strategies, and improved healthcare access.GPT-3.5</p>
</li>
</ol>
<p>Figures</p>
<p>Figures 2a and 2b present a domain-wise analysis of average word counts in academic papers and their future work (FWK) sections, revealing significant variations across disciplines.Notably, and references for our experiment.4In this paper, we use the terms 'ideas,' 'research ideas,' 'future research ideas,' and 'FRI' interchangeably to frequently refer to future research ideas.</p>
<p>Figure 2: Comparison of average word counts in papers with and without FWK across domains</p>
<p>Figure 3 :
3
Figure 3: IAScore for each domain and model; a higher value indicates better alignment with the author.</p>
<p>Figure 4 :
4
Figure 4: IdeaDistinctness index analysis; Here human is the authors of the paper</p>
<p>Figure 6 :
6
Figure 6: RAG Pipeline framework for infusing infusing more background knowledge with the LLMs</p>
<p>Figure 7 :
7
Figure 7: IAScore result for CS domain</p>
<p>Figure 8 :
8
Figure 8: An example for FRI editing; Here the strike through text is removed from the paper text</p>
<p>System:</p>
<p>You are a research scientist.User: Imagine you are a research scientist.After reading the following paper, brainstorm to generate potential top 5 future research ideas: [paper text] Potential top 5 future research ideas from the paper in bullet points are: Here, '[paper text]' contains the full content of the paper after removal of future work sections.</p>
<p>Figure 9 :
9
Figure 9: Effect of length on IAScore</p>
<p>Figure 10 :
10
Figure 10: Instructions on Human Feedback form</p>
<p>4.</p>
<p>Develop mathematical models to forecast COVID-19 vaccination coverage over time under different policy scenarios.</p>
<p>1.</p>
<p>Investigating the Impact of Socioeconomic Factors on Access and Uptake of COVID-19 Booster Vaccines: A study could focus on analyzing how socioeconomic indicators, such as GDP, human development index, and corruption perception index, influence access to and acceptance of booster doses of COVID-19 vaccines.2. Comparative Analysis of Inequality in COVID-19 Vaccination Across Continents: Future research could compare and contrast the levels of inequality in COVID-19 vaccination across different continents, especially focusing on the factors contributing to variations in vaccination rates and disparities in vaccine distribution.3. Longitudinal Study on the Impact of Socioeconomic Factors on COVID-19 Vaccination: A longitudinal study could be conducted to track changes in vaccination rates over time and assess how socioeconomic factors impact the progress of COVID-19 vaccination campaigns.4. Exploring Public Perception and Vaccine Hesitancy Among Different Socioeconomic Groups: Future research could delve into public perceptions and attitudes towards COVID-19 vaccination among diverse socioeconomic groups, investigating factors such as education level, income, and access to healthcare. 5. Evaluation of Intervention Strategies to Reduce Inequality in COVID-19 Vaccination: Researchers could conduct a study to evaluate the effectiveness of various intervention strategies aimed at reducing inequality in COVID-19 vaccination.By implementing targeted initiatives and measuring their impact on vaccine distribution and uptake, this research could provide evidence-based recommendations for policy interventions to promote equitable access to vaccines.</p>
<p>Table 1 :
1
Example of generated response for chemistry domain.Paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10528889/pdf/foods-12-03498.pdfInvestigateother machine learning models like recurrent neural networks for multi-round transfer learning in lung cancer detection.The paper focused on convolutional neural networks, but RNNs may capture temporal patterns in medical imaging data.2.Explore different loss functions and regularization techniques to further minimize negative transfer.The paper proposed custom loss functions, but there is room to experiment with other formulations.3.Evaluate the multi-round transfer learning approach on diverse medical imaging datasets beyond lung cancer, such as brain MRI scans.This could demonstrate the generalizability of the method.4.Develop more advanced generative models like CycleGAN to synthesize intermediate domains between source and target.This could create higher quality bridging distributions.5.Perform multi-round transfer learning in a low-data regime by leveraging data augmentation and semi-supervised learning techniques.This could show the approach is viable when target data is extremely scarce.Investigate the impact of different prioritization algorithms on the performance of MTL-MGAN.The paper uses a hybrid approach that merges modified 2D dynamic warping (M2DW) and Silhouette coefficient to prioritize source domains.Future research could explore other prioritization algorithms, such as mutual information, transfer entropy, or graph-based methods, to determine if they can further improve the performance of MTL-MGAN.2.Develop new loss functions to minimize negative transfer.<strong>Thepaperproposeslossfunctionsthat are designed to minimize negative transfer in the aspects of domains, instances, and features.Future research could investigate other loss functions that can further reduce negative transfer, such as adversarial loss, maximum mean discrepancy, or knowledge distillation.3.Explore different architectures for the MGAN.</strong>Thepaperuses a modified GAN (MGAN) to create two intermediate domains as bridges between the source domain and the target domain.Future research could explore other GAN architectures, such as deep convolutional GAN (DCGAN), Wasserstein GAN (WGAN), or StyleGAN, to determine if they can further improve the performance of MTL-MGAN.4.Apply MTL-MGAN to other medical diagnosis tasks.<strong>Thepaperevaluates the performance of MTL-MGAN on 10 benchmark datasets related to lung cancer detection.Future research could apply MTL-MGAN to other medical diagnosis tasks, such as breast cancer detection, skin cancer detection, or diabetic retinopathy detection, to determine its generalizability.5.Investigate the use of MTL-MGAN for other types of data.</strong>ThepaperappliesMTL-MGAN to medical images.Future research could investigate the use of MTL-MGAN for other types of data, such as text, audio, or time series data, to determine its applicability to a wider range of problems.GPT-4 1. Extension to Other Forms of Cancer and Diseases: Investigating the applicability and effectiveness of the MTL-MGAN approach for the detection and diagnosis of other types of cancer, such as breast, prostate, or skin cancer, and potentially other diseases that have distinct imaging characteristics.This would involve adapting the transfer learning and MGAN components to handle different imaging modalities and disease signatures.2.Real-Time Diagnostic Systems Development: Developing real-time, AI-driven diagnostic systems using the MTL-MGAN framework that can be integrated into clinical workflows.This could significantly reduce diagnosis times and improve patient outcomes by providing immediate, accurate diagnostic information during patient examinations or surgeries.3.Integration with Electronic Nose Technology for Early Detection: Combining the MTL-MGAN approach with emerging technologies like electronic nose devices, which can detect volatile organic compounds associated with lung cancer from breath samples.Research could focus on creating a hybrid detection system that leverages imaging data and breath analysis for earlier detection of lung cancer.4.Exploration of Unsupervised and Semi-supervised Learning Techniques: Given the challenge of obtaining large, annotated medical imaging datasets, exploring unsupervised and semi-supervised learning methods within the MTL-MGAN framework could be valuable.This might involve developing algorithms that can learn from unlabelled data or very small labelled datasets, potentially increasing the model's applicability in low-resource settings.5.Improvement of TransferLearning Techniques for Heterogeneous Data Sources: Investigating advanced transfer learning techniques that can effectively handle more significant variations between source and target domains, particularly when dealing with heterogeneous data sources.This research could include the development of novel algorithms for better domain adaptation and intermediate domain creation to facilitate the transfer of knowledge across diverse medical datasets and imaging modalities.GPT-3.5 1. Exploring Multiround Transfer Learning in other Healthcare Applications: Investigate the applicability of multiround transfer learning in various healthcare domains beyond lung cancer detection, such as cardiovascular disease diagnosis, diabetic retinopathy detection, or infectious disease prediction.2.Advanced Loss Function Design for Negative Transfer Avoidance:Develop novel loss functions to further minimize the impact of negative transfer in transfer learning models by incorporating more sophisticated domain, instance, and feature-based penalties.3.Enhanced Generative Adversarial Networks for Bridging Domain Gaps:Investigate the development of advanced generative adversarial networks that can create more effective intermediate domains to bridge the gap between diverse source and target domains in transfer learning scenarios.4.Automated Prioritization Algorithm for Source Domains:Develop an automated algorithm that can dynamically prioritize source domains based on their similarities to the target domain, thereby enhancing the efficiency and accuracy of the transfer learning process.
ModelResponseClaude-21. Gemini-1.01.
5. Evaluation of MTL-MGAN in Real-World Clinical Settings:Conduct a comprehensive evaluation of the proposed multiround transfer learning and modified generative adversarial network algorithm in real-world clinical settings to assess its performance, scalability, and generalizability in diverse healthcare environments.</p>
<p>Table 2 :
2
Example of generated response for computer domain.Paper: https://www.hindawi.com/journals/ijis/2023/6376275/Conductqualitativeresearch through interviews or focus groups to understand the specific barriers and facilitators to COVID-19 vaccination uptake in different socioeconomic groups within a country.This could provide insights to inform targeted interventions.2.Examine the impact of COVID-19 vaccination policies and public health messaging on vaccination uptake across socioeconomic groups.For example, study how vaccination requirements or incentives affect different populations.3.Investigate strategies to improve global collaboration and coordination in vaccine distribution and access.Research equitable pricing models, technology transfers, and policies to balance local needs with global supply.
ModelResponseClaude-21.</p>
<p>Table 3 :
3
Example of generated response for economics domain.Paper: https://www.mdpi.com/2076-393X/11/7/1245</p>
<p>https://github.com/sandeep82945/ Future-Idea-Generation.git
250 papers from each domain.
They used Science Parse and Grobid tool to extract data from PDF. We used the plain text without figureand tables
In this paper, we use the terms 'ideas,' 'research ideas,' 'future research ideas,' and 'FRI' interchangeably to frequently refer to future research ideas.
We intentionally avoided constraining the LLMs by not specifying a fixed number of ideas, allowing them to freely generate additional ideas beyond those proposed by the authors.
We used the OpenAI model GPT-3.5-turbo-0125 using OpenAI API. However, given the rapid advancements in language models, we recognize that newly released and upcoming models could further enhance the potential of IdeaMatcher.
text-embedding-ada-002
We used gpt-4-turbo using OpenAI API for the generation
We set maximum token length to 512, and temperature=0 for each model
We used GPT-3.5-turbo-0125 for GPT-3.5 (cutoff: December 2023), gpt-4-0125-preview for GPT-4 (cutoff: December 2023), anthropic.claude-v2 for Claude 2 (cutoff: August 2023), and gemini-1.0-pro-002 (cutoff: February 2023).
Comprehensive human evaluations across all domains require significant expertise and logistical resources, making it a challenge for the present study. However, the observed results strongly suggest that similar trends are likely to hold across other domains.
We employed Gemini-Pro model for this task
We set the threshold 0.68 empirically
AcknowledgementSandeep Kumar acknowledges the Prime Minister Research Fellowship (PMRF) program of the Govt of India for its support.We acknowledge Google for the "Gemma Academic Program GCP Credit Award", which provided Cloud credits to support this research.This research used the resources of the Oak Ridge Leadership Compute Facility (OLCF) at the Oak Ridge National Laboratory (ORNL), which is supported by the United States Department of Energy (DOE)'s Office of Science under Contract No. DE-AC05-00OR22725.Model ResponseClaude-2 1.Further optimize the minimal active H2 relaxin analog B9-31 through additional amino acid substitutions and modifications to improve binding affinity and activity.2. Explore alternative methods to induce helicity and stabilize the minimal H2 relaxin analogs, such as cyclization or incorporation of other helix-inducing amino acids.3. Develop longer-acting analogs by modifying B9-31 with polyethylene glycol, albumin binding moieties, or other half-life extending strategies.4. Test the in vivo efficacy of optimized minimal analogs in animal models of fibrosis, heart failure, and other conditions to assess therapeutic potential.5. Design biased analogs to selectively activate certain signaling pathways downstream of RXFP1 to optimize beneficial effects and reduce side effects.Gemini-1.0 1. Investigate the potential benefits of using ECMO in combination with other therapies for severe respiratory failure.For example, ECMO could be used in combination with lung transplantation, mechanical ventilation, or extracorporeal membrane oxygenation (ECMO).This could help to improve patient outcomes by providing more comprehensive support for the lungs and other organs.2. Develop new strategies for managing complications associated with ECMO.Common complications of ECMO include bleeding, infection, and thrombosis.New strategies for managing these complications could help to improve patient safety and outcomes.3. Study the long-term effects of ECMO on patients.There is limited data on the long-term effects of ECMO on patients.Studies are needed to investigate the potential risks and benefits of ECMO in the long term.4. Develop new technologies to improve the performance of ECMO.New technologies could help to make ECMO more effective, safer, and easier to use.For example, new oxygenators could be developed that are more efficient and less likely to cause complications.5. Conduct research to better understand the mechanisms of respiratory failure.This could help to identify new targets for therapy and improve the overall management of respiratory failure.GPT-4 1. Comparative Effectiveness of ECMO Configurations: Conduct randomized controlled trials comparing different ECMO cannulation strategies (e.g., dual-lumen cannula via the right internal jugular vein vs. traditional multi-cannula approaches) to assess their impact on patient outcomes, including survival rates, complication rates, and the ability to mobilize and rehabilitate during ECMO support.This research could also explore the use of image guidance for cannula placement and its impact on patient outcomes.2. ECMO in the Management of COVID-19-Induced ARDS: Given the limited randomized controlled trial data on the efficacy of ECMO for COVID-19-associated respiratory failure, further research is needed to define the role of ECMO in this context.Future studies could focus on identifying which patients benefit most from ECMO, the timing of initiation, and how ECMO is best integrated with other supportive therapies (e.g., antivirals, anti-inflammatory agents).3. Long-term Outcomes and Quality of Life Post-ECMO: Investigate the long-term outcomes of patients who have undergone ECMO support for severe respiratory failure, including physical, cognitive, and psychological recovery.This research could also assess the quality-adjusted life years (QALYs) gained through ECMO support and identify factors that predict better long-term outcomes.4. Optimization of Anticoagulation Strategies: Develop and test new anticoagulation protocols or agents that minimize the risks of bleeding and thrombotic complications in ECMO patients.This could include comparative studies of direct thrombin inhibitors versus unfractionated heparin, as well as the exploration of novel anticoagulants and biocompatible circuit coatings that may reduce the need for systemic anticoagulation.5. Integration of ECMO with Rehabilitation: Explore the feasibility, safety, and benefits of early rehabilitation protocols in ECMO-supported patients, including those on "awake ECMO."This research could assess the impact of rehabilitation on outcomes such as muscle strength, functional status at discharge, duration of hospital stay, and overall mortality.Studies could also investigate the best practices for physical therapy, nutritional support, and psychological support during ECMO.GPT-3.5 1. Investigating the long-term outcomes and quality of life of patients who have undergone VV-ECMO support for respiratory failure, including factors influencing long-term recovery and rehabilitation post-ECMO.2. Exploring new technologies and techniques to improve the efficiency and safety of VV-ECMO, such as developing novel anticoagulation strategies or improving cannulation methods to reduce complications and enhance patient outcomes.3. Conducting comparative studies to evaluate the effectiveness of different ventilation strategies in conjunction with VV-ECMO support, aiming to optimize patient management and potentially improve survival rates in severe respiratory failure cases.4. Examining the impact of ECMO utilization during disease outbreaks, such as COVID-19, on healthcare resource allocation, patient outcomes, and healthcare system preparedness, to inform future pandemic response strategies.5. Investigating the role of personalized medicine in VV-ECMO patient selection and management, including the potential use of biomarkers, genetic profiling, or other advanced techniques to tailor ECMO therapy to individual patient characteristics and optimize outcomes.1. Investigate the antioxidant and oil-stabilizing effects of specific peptide sequences identified in the pepsin and trypsin hydrolysates.The paper showed the pepsin hydrolysate had higher activity but did not identify the key active peptides.Bioinformatics could be used to predict promising peptides.2. Test the soy protein hydrolysates in stabilizing other vegetable oils like canola, sunflower, etc. during deep frying.The effects may differ for oils with varying fatty acid compositions.3. Evaluate the sensory qualities of foods fried in palm olein stabilized with the soy protein hydrolysates compared to unstabilized oil.This could include assessing odor, taste, and textural changes.4. Study the effects of the hydrolysates on the nutritional quality of the palm olein during repeated deep frying cycles.Parameters could i+nclude vitamin E content, formation of trans fats, and retention of essential fatty acids.5. Investigate combining the soy protein hydrolysates with other antioxidants like rosemary extract or green tea to evaluate synergistic effects on stabilizing frying oils.Mixtures may provide greater protection than individual additives.Gemini-1.0 1. Investigate the effect of different hydrolysis conditions on the antioxidant activity of soy protein hydrolysates.This could involve varying the enzyme concentration, temperature, pH, and hydrolysis time to determine the optimal conditions for producing hydrolysates with the highest antioxidant activity.2. Evaluate the antioxidant activity of soy protein hydrolysates in different food systems.This could involve adding hydrolysates to edible oils, fried foods, or other food products and measuring the oxidative stability of the products over time.This would help to determine the potential applications of soy protein hydrolysates as natural antioxidants in food products.3. Study the mechanism by which soy protein hydrolysates exert their antioxidant activity.**This could involve using in vitro and in vivo assays to investigate the ability of hydrolysates to scavenge free radicals, chelate metal ions, or inhibit lipid peroxidation.This information would help to elucidate the molecular mechanisms underlying the antioxidant activity of soy protein hydrolysates.4. Explore the potential health benefits of soy protein hydrolysates.This could involve conducting animal or human studies to investigate the effects of hydrolysates on oxidative stress, inflammation, and other health markers.This would help to determine the potential role of soy protein hydrolysates in promoting human health.5. Develop novel food products containing soy protein hydrolysates.This could involve incorporating hydrolysates into functional foods, such as beverages, snacks, or supplements, to improve their nutritional value and antioxidant content.This would help to increase the consumption of soy protein hydrolysates and promote their potential health benefits.GPT-4 1. Comparative Analysis of Different Plant-Based Proteins on Oil Stability: Extending the research to other plant-based proteins, such as pea, hemp, and rice proteins, to compare their effectiveness in improving oil oxidative stability during frying cycles.This could provide a broader understanding of the potential of plant-based proteins in enhancing the nutritional and safety profiles of fried foods.Mechanism of Action at Molecular Level:Delving deeper into the molecular mechanisms behind the antioxidant activities of different hydrolysates.This could involve investigating the interaction between specific peptide sequences and oxidative compounds within the oil, using computational modeling and in vitro experiments to elucidate the pathways for their antioxidant actions.3. Impact of Hydrolysate Concentration and Frying Duration: Exploring the effect of varying concentrations of the most effective hydrolysates (e.g., from pepsin digestion) over extended frying cycles beyond 12, to determine the optimal concentration for maximum oxidative stability without affecting the sensory qualities of the oil and the fried product.Application in IndustrialFrying Processes: Conducting pilot-scale studies in industrial settings to assess the feasibility and economic viability of incorporating hydrolysates into commercial frying oils.This research could include the development of guidelines for the use of hydrolysates in different types of frying operations and their impact on the quality and shelf-life of fried foods.5. Development of Natural Antioxidant Blends: Formulating blends of hydrolysates with other natural antioxidants (e.g., extracts from spices, herbs, or other food by-products rich in phenolic compounds) to synergistically enhance the oxidative stability of frying oils.This research could also investigate the sensory impact of these blends on fried products and consumer acceptance.GPT-3.5 1. Investigating the impact of different proteases on the generation of antioxidant peptides from soy protein hydrolysates and their effectiveness in stabilizing oils during frying cycles.2. Exploring the structural features of peptides derived from soy protein hydrolysates to determine the key amino acids responsible for their antioxidant properties.3. Comparing the oxidative stability of different types of oils (e.g., sesame oil, canola oil, corn oil) when stabilized with soy protein hydrolysates under varied frying conditions.4. Examining the influence of hydrolyzed protein residues on the formation of secondary oxidation products in oils during frying cycles and their impact on food quality and safety.5. Utilizing bioinformatics tools to predict and select specific peptide sequences from soy protein hydrolysates that exhibit the highest antioxidant capacity and stability-enhancing properties in fried oils..  Claude-2 Gemini-1.0Computer Science GPT-4 (IAScore), Claude-2 (Distinctness)Gemini-1.0Medical GPT-4 Gemini-1.0Physics GPT-4 (IAScore), Claude-2 (Distinctness) Gemini-1.0
The ACL FWS-RC: A dataset for recognition and classification of sentence about future works. Wenke Hao, Zhicheng Li, Yuchen Qian, Yuzhuo Wang, Chengzhi Zhang, 10.1145/3383583.3398526JCDL '20: Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020. ChinaACM2020. August 1-5, 2020</p>
<p>Benchmarking large language models as ai research agents. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, ArXiv, abs/2310.033022023</p>
<p>The semantic scholar open data platform. Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David W Graham, F Q Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey Macmillan, Tyler C Murray, Christopher Newell, Shaurya Smita R Rao, Paul Rohatgi, Zejiang Sayre, Amanpreet Shen, Luca Singh, Shivashankar Soldaini, A Subramanian, Alex D Tanaka, Linda M Wade, Lucy Lu Wagner, Wang, ArXiv, abs/2301.101402023Madeleine van Zuylen, and Daniel S. WeldChristopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron</p>
<p>Research on idea generation and selection: Implications for management of technology. Laura J Kornish, Jeremy Hutchison-Krupat, 2017Production and Operations Management26</p>
<p>The structure of scientific revolutions. Thomas S Kuhn, David Hawkins, American Journal of Physics. 311963</p>
<p>Research Methodology: A Step-by Step Guide for Beginners. R Kumar, 1996Longman Australia</p>
<p>When reviewers lock horns: Finding disagreements in scientific peer reviews. Sandeep Kumar, Tirthankar Ghosal, Asif Ekbal, 10.18653/v1/2023.emnlp-main.1038Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023Singapore</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. S H Patrick, Ethan Lewis, Aleksandra Perez, Fabio Piktus, Vladimir Petroni, Naman Karpukhin, Heinrich Goyal, Mike Küttler, Wen-Tau Lewis, Tim Yih, Sebastian Rocktäschel, Douwe Riedel, Kiela, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020. 2020. 2020. December 6-12, 2020</p>
<p>A diversity-promoting objective function for neural conversation models. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, William B Dolan, ArXiv, abs/1510.030552015</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>J J Louviere, T N Flynn, A A J Marley, Best-Worst Scaling: Theory, Methods and Applications. Cambridge books online. Cambridge University Press2015</p>
<p>Scientific language models for biomedical knowledge base completion: An empirical study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, ArXiv, abs/2106.097002021</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774OpenAI embedding model used via API. OpenAI. 2023. GPT-4 technical report. </p>
<p>Plato: 'The Republic'. Cambridge Texts in the History of Political Thought. G R F Plato, T Ferrari, Griffith, 2000Cambridge University Press</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, ArXiv, abs/2311.059652023</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, J R Francisco, Jordan S Ruiz, Pengming Ellenberg, Omar Wang, Pushmeet Fawzi, Alhussein Kohli, Josh Fawzi, Andrea Grochow, Jean-Baptiste Lodi, Talia Mouret, Tao Ringer, Yu, Nature. 6252023</p>
<p>Idea generation with technology semantic network. Serhad Sarica, Binyang Song, Jianxi Luo, Kristin L Wood, Artificial Intelligence for Engineering Design. 352021Analysis and Manufacturing</p>
<p>Automatic generation of programming exercises and code explanations using large language models. Sami Sarsa, Paul Denny, Arto Hellas, Juho Leinonen, 10.1145/3501385.3543957ICER 2022: ACM Conference on International Computing Education Research, Lugano and Virtual Event. SwitzerlandACM2022. August 7 -11, 20221</p>
<p>Automated hypothesis generation based on mining scientific literature. W Scott Spangler, Angela D Wilkins, Benjamin J Bachman, Meena Nagarajan, Tajhal Dayaram, Peter J Haas, Sam Regenbogen, Curtis R Pickering, Austin Comer, Jeffrey N Myers, Ioana Stanoi, Linda Kato, Ana Lelescu, Jacques J Labrie, Neha Parikh, Andreas Martin Lisewski, Lawrence A Donehower, Ying Chen, Olivier Lichtarge, 10.1145/2623330.2623667The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14. New York, NY, USAACM2014. August 24 -27, 2014</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. John Vahe Tshitoyan, Leigh Dagdelen, Alexander Weston, Ziqin Dunn, Olga Rong, Kristin A Kononova, Gerbrand Persson, Anubhav Ceder, Jain, 10.1038/S41586-019-1335-8Nat. 57177632019</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.14259Scimon: Scientific inspiration machines optimized for novelty. 2023arXiv preprint</p>
<p>PaperRobot: Incremental draft generation of scientific ideas. Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan, 10.18653/v1/P19-1191Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Exploring and verbalizing academic ideas by concept co-occurrence. Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou, 10.18653/v1/2023.acl-long.727Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Z Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jianyun Liu, Ji Nie, Wen Rong, ArXiv, abs/2303.182232023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2261 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2261</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2261</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-211010946</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2002.00049v3.pdf" target="_blank">Studying Lagrangian theories with machine learning: a toy model</a></p>
                <p><strong>Paper Abstract:</strong> The existence or not of pathologies in the context of Lagrangian theory is studied with the aid of Machine Learning algorithms. Using an example in the framework of classical mechanics, we make a proof of concept, that the construction of new physical theories using machine learning is possible. Specifically, we utilize a fully-connected, feed-forward neural network architecture, aiming to discriminate between ``healthy'' and ``non-healthy'' Lagrangians, without explicitly extracting the relevant equations of motion. The network, after training, is used as a fitness function in the concept of a genetic algorithm and new healthy Lagrangians are constructed. These new Lagrangians are different from the Lagrangians contained in the initial data set. Hence, searching for Lagrangians possessing a number of pre-defined properties is significantly simplified within our approach. The framework employed in this work can be used to explore more complex physical theories, such as generalizations of General Relativity in gravitational physics, or constructions in solid state physics, in which the standard procedure can be laborious.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2261.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2261.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FFNN classifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fully-connected feed-forward neural network classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised neural-network surrogate trained to classify parametrized Lagrangians as 'healthy' (produce ≤ second-order equations of motion) or 'non-healthy' (produce > second-order derivatives) from compact numeric feature vectors, without symbolic derivation of equations of motion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Theoretical physics / Lagrangian theory search (classical mechanics toy model; motivated extension to gravitational theories)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Determine whether a candidate Lagrangian (expressed by a parametric feature vector) will lead to equations of motion containing derivatives of order higher than two (Ostrogradsky instability), i.e., a binary classification problem on Lagrangian parameter space, and provide a fitness signal for automated generation of new Lagrangians.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Synthetic, labeled and balanced dataset generated by Monte Carlo sampling and symbolic evaluation: 4000 training samples (2000 labeled 'healthy', 2000 'non-healthy'), plus multiple validation sets (10 sets of 10,000 feature vectors). Labels are high-quality because produced by exact symbolic Euler-Lagrange extraction (sympy). Data is internally generated and accessible within the study; size is moderate for the low-dimensional task.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured numeric feature vectors (tabular, low-to-moderate dimensional). Inputs are coefficients/exponents from a chosen Lagrangian parametrization (e.g., vectors of length up to ~18 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: low-to-moderate input dimensionality (dozens or fewer features in experiments), nonlinear decision boundary; search space continuous and potentially large when coefficients/exponents vary continuously, but the toy problem here is a constrained parametrized subspace of all Lagrangians. Computational complexity modest for classification; generating labels requires symbolic algebra which is more expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Domain (Lagrangian formalism and Ostrogradsky criteria) is well-established and mechanistic; however the automated search for healthy higher-derivative Lagrangians is a challenging and relatively less-automated subproblem in a mature theoretical field.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — scientific validation and interpretability are important: the paper emphasizes the need for mechanistic checks (explicit Euler-Lagrange extraction) and notes transparency concerns of black-box models; the NN is used as a fast surrogate but final physical validation is performed by symbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Fully-connected feed-forward neural network (multilayer perceptron)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A supervised MLP mapping Lagrangian feature vectors to a scalar in [0,1] indicating 'healthiness'. Architectures tested; best-performing used two hidden layers of 16 neurons each (architecture reported as e.g. 18-16-16-1 and other variants like 8-6-6-1). Sigmoid activation, quadratic cost, trained with backpropagation using mini-batches (batch size 400), gradient descent with tuned learning rates (η reported values including 0.1 and 1), training over many epochs, thresholding of output for binary decision. Performance evaluated with accuracy, precision, recall, F1 and log-loss. Training labels provided by symbolic Euler-Lagrange checks (sympy).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate as a surrogate classifier for the parametrized, low-dimensional Lagrangian space studied; avoided explicit symbolic derivation for rapid screening. Limitations: black-box nature, dependence on chosen parametrization and synthetic training distribution, potential lack of generalization to unrepresented Lagrangian forms or much higher-dimensional featureizations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Final reported validation performance for the selected architecture: accuracy ≈ 0.97 (range of runs around 0.965–0.975), precision ≈ 0.98, recall ≈ 0.976, F1 ≈ 0.974, log-loss ≈ 0.076 (values taken from training/validation tables for top architecture). Earlier experiments show accuracy rising from ~0.55 to 1.0 in other configurations/longer training, indicating sensitivity to architecture/hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Worked well on the toy 1D parametrized problem: the trained NN reliably discriminated healthy vs non-healthy Lagrangians and provided a smooth fitness signal for downstream search; successes depended on representative synthetic training data and suitable architecture. Limitations include the black-box nature and that the experiment is restricted to a particular parametrization and low-dimensional setting, so generalization to arbitrary/high-dimensional Lagrangian spaces is unproven.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to accelerate and automate preliminary screening in theory construction tasks (e.g., searching for ghost-free higher-derivative Lagrangians in gravity or condensed-matter models), reducing the need for expensive symbolic derivations for every candidate; can serve as a fitness/surrogate model inside search/optimization loops. Must be paired with mechanistic validation for scientific trust.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to the standard manual or computer-algebra approach (explicit Euler-Lagrange derivation and inspection). The ML surrogate avoids repeated symbolic variation and lengthy algebra, but no quantitative runtime comparison or baseline ML methods (e.g., other classifiers) are provided. No detailed comparison to interpretable or symbolic ML baselines was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality labeled data generated via exact symbolic checks (sympy), balanced dataset construction, appropriate choice of parametrization (features), architecture tuning (depth/width), and mini-batch training. The low dimensionality of the parametrized space helped success. The coupling with GA as a consumer of the network output also leveraged the smoothness of network outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>A supervised feed-forward NN can act as an effective surrogate fitness/classifier for a parametrized Lagrangian space (low-dimensional toy model), achieving ≈97% validation accuracy and enabling automated search, but its utility depends on representative synthetic labels and requires subsequent mechanistic validation due to black-box limitations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2261.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2261.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA + NN fitness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic algorithm guided Lagrangian construction using neural-network fitness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A genetic algorithm that uses the trained neural-network classifier output as a continuous fitness function to evolve populations of Lagrangian feature vectors and generate new candidate 'healthy' Lagrangians that were not in the training set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Theoretical physics / automated theory construction (search for healthy Lagrangians)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automatically construct new Lagrangians (feature vectors) that satisfy a desired property (being 'healthy') by evolving a population using genetic operators while using the trained NN as a fitness evaluator to avoid explicit symbolic derivations for every candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Population-based search operates on internally generated populations of feature vectors. Initial populations are Monte Carlo sampled within the same parametrization used for NN training; population sizes and exact initial samples are internal to experiments (Table II referenced). Data is synthetic and generated within the study.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured numeric feature vectors (same tabular representation used by the NN).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Optimization in a continuous, multi-dimensional parameter space with potentially non-convex, multimodal fitness landscape; complexity scales with number of features and breadth of parametrization. For the experiments, dimensionality was modest and GA converged in hundreds of generations (e.g., 200 generations reported for selection).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Search/optimization techniques (genetic algorithms) are mature; applying GA to Lagrangian construction is novel but builds on established optimization methodology and a mature physics domain for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — although the GA proposes candidate Lagrangians automatically, mechanistic validation (explicit extraction/check of equations of motion) is required to confirm physical health; the GA/NN pipeline serves as a discoverer but not sole validator.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Genetic algorithm (evolutionary search) using neural-network-based fitness</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Population-based GA where each individual is a feature vector in R^n. The trained NN provides a fitness value N_fit(P) ∈ [0,1]; weights for mating defined as w_i = 10^5 * N_fit(P_i). Mating: random pairing and children formed by weighted averaging of parent features; mutation: random perturbation of child features with predefined mutation probability/distribution. Iterated for many generations (example runs: 200 generations, experiments repeated across different initial populations), selecting high-fitness individuals; final selected Lagrangians differed from initial population and satisfied 'healthy' criterion when validated.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>evolutionary optimization (hybrid: classical GA guided by ML surrogate)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate as an exploratory/generative tool when (i) a fast surrogate fitness function is available (NN here) and (ii) explicit evaluation (symbolic derivation) is expensive. Works well in modest-dimension parametrized spaces; applicability to very high-dimensional or qualitatively different parametrizations depends on surrogate fidelity and search tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Empirical outcome: GA runs (10 repeats) produced new Lagrangians classified as 'healthy' by the NN and confirmed by symbolic checks; no numeric speedup or wall-clock runtime reported. Convergence examples: after 200 generations the most-fit individuals from final populations were selected (Table V referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: GA+NN discovered valid, previously unseen 'healthy' Lagrangians and identified the important structural feature (exponent of ẍ ≈ 0.1 found across solutions) even though that rule was not imposed. Success is conditional on the NN fitness being reliable within the explored region; generated candidates still required symbolic validation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant: automates exploration of theory space and can produce candidate Lagrangians that would otherwise require lengthy algebraic work to validate; potentially scales to more complex domains (e.g., gravity) if surrogate fidelity and parametrization mapping are extended, thereby reducing researcher time from months to much less for initial candidate generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to blind/manual search and to purely symbolic generation followed by evaluation: GA+NN avoids repeated symbolic derivations for every candidate and can explore continuous combinations not trivially guessed by humans; no quantitative comparison (e.g., runtime or number of symbolic derivations saved) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of a smooth, accurate surrogate fitness (the trained NN), representative initial populations, appropriate mating and mutation operators, and repeated runs to avoid local optima. The low dimensionality of parametrization and the NN's good discrimination contributed to success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Using a trained ML classifier as a differentiable/continuous fitness enables evolutionary search to generate novel, valid Lagrangians efficiently in a parametrized low-dimensional space, but closure requires symbolic/mechanistic validation because the surrogate is a black box.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Journal of The Royal Society Interface <em>(Rating: 2)</em></li>
                <li>Phys. Rev. D93 <em>(Rating: 2)</em></li>
                <li>Science <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2261",
    "paper_id": "paper-211010946",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "FFNN classifier",
            "name_full": "Fully-connected feed-forward neural network classifier",
            "brief_description": "A supervised neural-network surrogate trained to classify parametrized Lagrangians as 'healthy' (produce ≤ second-order equations of motion) or 'non-healthy' (produce &gt; second-order derivatives) from compact numeric feature vectors, without symbolic derivation of equations of motion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Theoretical physics / Lagrangian theory search (classical mechanics toy model; motivated extension to gravitational theories)",
            "problem_description": "Determine whether a candidate Lagrangian (expressed by a parametric feature vector) will lead to equations of motion containing derivatives of order higher than two (Ostrogradsky instability), i.e., a binary classification problem on Lagrangian parameter space, and provide a fitness signal for automated generation of new Lagrangians.",
            "data_availability": "Synthetic, labeled and balanced dataset generated by Monte Carlo sampling and symbolic evaluation: 4000 training samples (2000 labeled 'healthy', 2000 'non-healthy'), plus multiple validation sets (10 sets of 10,000 feature vectors). Labels are high-quality because produced by exact symbolic Euler-Lagrange extraction (sympy). Data is internally generated and accessible within the study; size is moderate for the low-dimensional task.",
            "data_structure": "Structured numeric feature vectors (tabular, low-to-moderate dimensional). Inputs are coefficients/exponents from a chosen Lagrangian parametrization (e.g., vectors of length up to ~18 in experiments).",
            "problem_complexity": "Moderate: low-to-moderate input dimensionality (dozens or fewer features in experiments), nonlinear decision boundary; search space continuous and potentially large when coefficients/exponents vary continuously, but the toy problem here is a constrained parametrized subspace of all Lagrangians. Computational complexity modest for classification; generating labels requires symbolic algebra which is more expensive.",
            "domain_maturity": "Domain (Lagrangian formalism and Ostrogradsky criteria) is well-established and mechanistic; however the automated search for healthy higher-derivative Lagrangians is a challenging and relatively less-automated subproblem in a mature theoretical field.",
            "mechanistic_understanding_requirements": "High — scientific validation and interpretability are important: the paper emphasizes the need for mechanistic checks (explicit Euler-Lagrange extraction) and notes transparency concerns of black-box models; the NN is used as a fast surrogate but final physical validation is performed by symbolic methods.",
            "ai_methodology_name": "Fully-connected feed-forward neural network (multilayer perceptron)",
            "ai_methodology_description": "A supervised MLP mapping Lagrangian feature vectors to a scalar in [0,1] indicating 'healthiness'. Architectures tested; best-performing used two hidden layers of 16 neurons each (architecture reported as e.g. 18-16-16-1 and other variants like 8-6-6-1). Sigmoid activation, quadratic cost, trained with backpropagation using mini-batches (batch size 400), gradient descent with tuned learning rates (η reported values including 0.1 and 1), training over many epochs, thresholding of output for binary decision. Performance evaluated with accuracy, precision, recall, F1 and log-loss. Training labels provided by symbolic Euler-Lagrange checks (sympy).",
            "ai_methodology_category": "supervised learning",
            "applicability": "Applicable and appropriate as a surrogate classifier for the parametrized, low-dimensional Lagrangian space studied; avoided explicit symbolic derivation for rapid screening. Limitations: black-box nature, dependence on chosen parametrization and synthetic training distribution, potential lack of generalization to unrepresented Lagrangian forms or much higher-dimensional featureizations.",
            "effectiveness_quantitative": "Final reported validation performance for the selected architecture: accuracy ≈ 0.97 (range of runs around 0.965–0.975), precision ≈ 0.98, recall ≈ 0.976, F1 ≈ 0.974, log-loss ≈ 0.076 (values taken from training/validation tables for top architecture). Earlier experiments show accuracy rising from ~0.55 to 1.0 in other configurations/longer training, indicating sensitivity to architecture/hyperparameters.",
            "effectiveness_qualitative": "Worked well on the toy 1D parametrized problem: the trained NN reliably discriminated healthy vs non-healthy Lagrangians and provided a smooth fitness signal for downstream search; successes depended on representative synthetic training data and suitable architecture. Limitations include the black-box nature and that the experiment is restricted to a particular parametrization and low-dimensional setting, so generalization to arbitrary/high-dimensional Lagrangian spaces is unproven.",
            "impact_potential": "High potential to accelerate and automate preliminary screening in theory construction tasks (e.g., searching for ghost-free higher-derivative Lagrangians in gravity or condensed-matter models), reducing the need for expensive symbolic derivations for every candidate; can serve as a fitness/surrogate model inside search/optimization loops. Must be paired with mechanistic validation for scientific trust.",
            "comparison_to_alternatives": "Compared qualitatively to the standard manual or computer-algebra approach (explicit Euler-Lagrange derivation and inspection). The ML surrogate avoids repeated symbolic variation and lengthy algebra, but no quantitative runtime comparison or baseline ML methods (e.g., other classifiers) are provided. No detailed comparison to interpretable or symbolic ML baselines was performed.",
            "success_factors": "High-quality labeled data generated via exact symbolic checks (sympy), balanced dataset construction, appropriate choice of parametrization (features), architecture tuning (depth/width), and mini-batch training. The low dimensionality of the parametrized space helped success. The coupling with GA as a consumer of the network output also leveraged the smoothness of network outputs.",
            "key_insight": "A supervised feed-forward NN can act as an effective surrogate fitness/classifier for a parametrized Lagrangian space (low-dimensional toy model), achieving ≈97% validation accuracy and enabling automated search, but its utility depends on representative synthetic labels and requires subsequent mechanistic validation due to black-box limitations.",
            "uuid": "e2261.0"
        },
        {
            "name_short": "GA + NN fitness",
            "name_full": "Genetic algorithm guided Lagrangian construction using neural-network fitness",
            "brief_description": "A genetic algorithm that uses the trained neural-network classifier output as a continuous fitness function to evolve populations of Lagrangian feature vectors and generate new candidate 'healthy' Lagrangians that were not in the training set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Theoretical physics / automated theory construction (search for healthy Lagrangians)",
            "problem_description": "Automatically construct new Lagrangians (feature vectors) that satisfy a desired property (being 'healthy') by evolving a population using genetic operators while using the trained NN as a fitness evaluator to avoid explicit symbolic derivations for every candidate.",
            "data_availability": "Population-based search operates on internally generated populations of feature vectors. Initial populations are Monte Carlo sampled within the same parametrization used for NN training; population sizes and exact initial samples are internal to experiments (Table II referenced). Data is synthetic and generated within the study.",
            "data_structure": "Structured numeric feature vectors (same tabular representation used by the NN).",
            "problem_complexity": "Optimization in a continuous, multi-dimensional parameter space with potentially non-convex, multimodal fitness landscape; complexity scales with number of features and breadth of parametrization. For the experiments, dimensionality was modest and GA converged in hundreds of generations (e.g., 200 generations reported for selection).",
            "domain_maturity": "Search/optimization techniques (genetic algorithms) are mature; applying GA to Lagrangian construction is novel but builds on established optimization methodology and a mature physics domain for validation.",
            "mechanistic_understanding_requirements": "High — although the GA proposes candidate Lagrangians automatically, mechanistic validation (explicit extraction/check of equations of motion) is required to confirm physical health; the GA/NN pipeline serves as a discoverer but not sole validator.",
            "ai_methodology_name": "Genetic algorithm (evolutionary search) using neural-network-based fitness",
            "ai_methodology_description": "Population-based GA where each individual is a feature vector in R^n. The trained NN provides a fitness value N_fit(P) ∈ [0,1]; weights for mating defined as w_i = 10^5 * N_fit(P_i). Mating: random pairing and children formed by weighted averaging of parent features; mutation: random perturbation of child features with predefined mutation probability/distribution. Iterated for many generations (example runs: 200 generations, experiments repeated across different initial populations), selecting high-fitness individuals; final selected Lagrangians differed from initial population and satisfied 'healthy' criterion when validated.",
            "ai_methodology_category": "evolutionary optimization (hybrid: classical GA guided by ML surrogate)",
            "applicability": "Appropriate as an exploratory/generative tool when (i) a fast surrogate fitness function is available (NN here) and (ii) explicit evaluation (symbolic derivation) is expensive. Works well in modest-dimension parametrized spaces; applicability to very high-dimensional or qualitatively different parametrizations depends on surrogate fidelity and search tuning.",
            "effectiveness_quantitative": "Empirical outcome: GA runs (10 repeats) produced new Lagrangians classified as 'healthy' by the NN and confirmed by symbolic checks; no numeric speedup or wall-clock runtime reported. Convergence examples: after 200 generations the most-fit individuals from final populations were selected (Table V referenced).",
            "effectiveness_qualitative": "Qualitatively effective: GA+NN discovered valid, previously unseen 'healthy' Lagrangians and identified the important structural feature (exponent of ẍ ≈ 0.1 found across solutions) even though that rule was not imposed. Success is conditional on the NN fitness being reliable within the explored region; generated candidates still required symbolic validation.",
            "impact_potential": "Significant: automates exploration of theory space and can produce candidate Lagrangians that would otherwise require lengthy algebraic work to validate; potentially scales to more complex domains (e.g., gravity) if surrogate fidelity and parametrization mapping are extended, thereby reducing researcher time from months to much less for initial candidate generation.",
            "comparison_to_alternatives": "Compared qualitatively to blind/manual search and to purely symbolic generation followed by evaluation: GA+NN avoids repeated symbolic derivations for every candidate and can explore continuous combinations not trivially guessed by humans; no quantitative comparison (e.g., runtime or number of symbolic derivations saved) is provided.",
            "success_factors": "Availability of a smooth, accurate surrogate fitness (the trained NN), representative initial populations, appropriate mating and mutation operators, and repeated runs to avoid local optima. The low dimensionality of parametrization and the NN's good discrimination contributed to success.",
            "key_insight": "Using a trained ML classifier as a differentiable/continuous fitness enables evolutionary search to generate novel, valid Lagrangians efficiently in a parametrized low-dimensional space, but closure requires symbolic/mechanistic validation because the surrogate is a black box.",
            "uuid": "e2261.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Journal of The Royal Society Interface",
            "rating": 2,
            "sanitized_title": "journal_of_the_royal_society_interface"
        },
        {
            "paper_title": "Phys. Rev. D93",
            "rating": 2,
            "sanitized_title": "phys_rev_d93"
        },
        {
            "paper_title": "Science",
            "rating": 1
        }
    ],
    "cost": 0.011820749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Building healthy Lagrangian theories with machine learning</p>
<p>Christos Valelis 
Department of Informatics &amp; Telecommunications
National &amp; Kapodistrian University of Athens
Zografou Campus GR 157 73AthensGreece</p>
<p>Fotios K Anagnostopoulos 
Department of Physics
National &amp; Kapodistrian University of Athens
Zografou Campus GR 157 73AthensGreece</p>
<p>Spyros Basilakos 
Research Center for Astronomy and Applied Mathematics
Academy of Athens
Soranou Efesiou 411527AthensGreece</p>
<p>National Observatory of Athens
Lofos Nymfon11852AthensGreece</p>
<p>Emmanuel N Saridakis 
National Observatory of Athens
Lofos Nymfon11852AthensGreece</p>
<p>Department of Physics
National Technical University of Athens
Zografou Campus GR 157 73AthensGreece</p>
<p>Department of Astronomy
School of Physical Sciences
University of Science and Technology of China
230026HefeiP.R. China</p>
<p>Building healthy Lagrangian theories with machine learning</p>
<p>The existence or not of pathologies in the context of Lagrangian theory is studied with the aid of Machine Learning algorithms. Using an example in the framework of classical mechanics, we make a proof of concept, that the construction of new physical theories using machine learning is possible. Specifically, we utilize a fully-connected, feed-forward neural network architecture, aiming to discriminate between "healthy" and "non-healthy" Lagrangians, without explicitly extracting the relevant equations of motion. The network, after training, is used as a fitness function in the concept of a genetic algorithm and new healthy Lagrangians are constructed. These new Lagrangians are different from the Lagrangians contained in the initial data set. Hence, searching for Lagrangians possessing a number of pre-defined properties is significantly simplified within our approach. The framework employed in this work can be used to explore more complex physical theories, such as generalizations of General Relativity in gravitational physics, or constructions in solid state physics, in which the standard procedure can be laborious.</p>
<p>I. INTRODUCTION</p>
<p>After a century of theoretical research in the construction of physical theories, we know that the action, and thus the corresponding Lagrangian density, is the cornerstone quantity, since it gives rise to the equations of motion by employing the least-action principle. During the last century rapid progress in the direction of a unified description of physics took place, and the combined description of electromagnetic, weak and strong nuclear forces within the standard model of particle physics Lagrangian is established. However, the inclusion of gravity in this picture is notoriously difficult, resulting to a number of different approaches (see [1,2] for a review and [3] for a wider perspective of the problem).</p>
<p>One interesting approach is to study higher-order corrections to the Einstein-Hilbert action of general relativity, which is a sufficient condition to construct renormalizable and thus potentially quantizable gravitational theories [4]. A serious drawback in this approach is that including higher derivatives in the Lagrangian is known to cause problems, such as the existence of unphysical states, the so-called ghosts. Thus, one is indeed interested in Lagrangians that posses terms with higher-derivatives, but which are ghost-free and have equations of motion that are of second order, which are generally called "healthy" Lagrangians. Searching for healthy Lagrangians is usually done by hand, or at most by using a computer algebra system, and is a tedious procedure which requires the explicit derivation of field equations and the examination of whether they are of second order.</p>
<p>The ubiquity of massive data volumes, coupled with scalable training techniques, novel learning algorithms and immense computational power, has been an important factor for the practical success of (deep) machine learning (ML). From a predictive accuracy standpoint, deep learning algorithms are nowadays considered the state of the art in numerous applications, while deep learning's success has been the primary reason for a fresh look at the transformative potential of artificial inteligence (AI) as a whole during the past decade. This success has also made a long-standing dream, that of automated, data-driven scientific discovery, seem within reach. From early approaches in biology and the life sciences, [5], to more recent ones in a wide range of scientific fields [6], AI-assisted scientific discovery is pursued towards understanding experimental findings, inferring causal relationships from observational data, or even acquiring novel scientific insights and building new explanatory theories. At the same time, however, and despite recent successes and optimism, a transparency barrier is imposing severe limitations on the applicability of the AI potential in scientific discovery. It is now widely acknowledged that the inability to understand, explain and trace back to a cause the predictions/inferences of black-box machine learning models is responsible for their lack of accountability and for end user's lack of trust in such models in mission-critical applications and high-stakes decisions. In fields where understanding, as opposed to merely predicting, is the basic requirement, such as that of scientific discovery and automated explanatory theory building, the opaqueness of state-of-the-art machine learning is a serious handicap.</p>
<p>On the other hand, a consensus is emerging within the machine learning community for the existence of a tradeoff between interpretability and predictive performance, especially in problems featuring highly unstructured data sources, as is often the case in natural sciences. In such cases, the inner workings of simpler machine learning models are transparent at the cost of lower performance, while highly complex models are more accurate, at the cost of interpretability. Explainable AI, [7], attempts to shed light into the complexities of state-of-the-art machine-generated models without compromising their performance. It is attracting significant attention and numerous approaches have already been proposed, involving e.g. the use of focus mechanisms in deep neural networks to highlight regions of the network involved in certain inferences and potentially identify relevant features, [8], or by identifying such features via studying the effects of input perturbations to the output. Other approaches rely on dissecting the object (e.g. an image) to be classified into a set of prototypical parts and then combining evidence from such prototypes to explain the final classification, [9]. The idea behind this paper is to automate the search for a healthy Lagrangian possessing higher-derivatives using Machine Learning (ML).</p>
<p>In this paper, we are interested in studying how efficient an artificial neural network can be in classifying a given Lagrangian to "healthy" or "non-healthy", after training, without extracting the equations of motion. Further, we construct new Lagrangians in an automated way. The manuscript is organized as follows. In Section II we provide a concise presentation of the Lagrangian formalism and we also present our algorithms for creating Lagrangians. In Section III we describe in detail the ML algorithms and architectures we employ, while in Section IV we present and discuss our results. Finally, in Section V we draw our conclusions and we point out directions for further work.</p>
<p>II. DESCRIPTION OF PHYSICAL THEORIES</p>
<p>A framework that allows systematical study of a physical theory is the Lagrangian formalism. The latter comes from classical physics and it was first formulated from Joseph-Louis Lagrange in late 18th century. Along with its generalizations to include various fields and curved space-time is the cornerstone of modern and classical physics. Let us begin our discussion within the context of classical mechanics. Given the action functional
S[q, t 1 , t 2 ] = q(t2) q(t1) L(q,q, ..., q (n) )dt,(1)
where t 1 , t 2 are two time instances and q is the fundamental quantity that describes our physical system and specifically the position over time of the particle under study. The corresponding equations of motion arise by applying the Hamilton's principle, namely the evolution of the physical system is such a way that the action is stationary (for a thorough presentation of action principle see [10]). In other words, the trajectory that the particle will follow between q(t 1 ) and q(t 2 ) is such that the action functional derivative is zero, i.e. δS = 0. This requirement produces the equation of motion [10]
∂L ∂q − d dt ∂L ∂q + ... + (−1) n d n dt n ∂L ∂q (n) = 0.(2)
In the latter we omitted the arguments of the Lagrangian for brevity. The Lagrangian to obtain the Newtonian mechanics is L = T − V (q), where T is the kinetic energy of the system, i.e. T = (1/2) mq 2 . Since the late 18th century where this formalism was first proposed, it has been generalized to include classical and quantum fields, as well as to handle in a unified way time and positions in the context of Einstein's theory of gravity [11][12][13]. Today the procedure of constructing a new physical theory is essentially the process of constructing new Lagrangians with the desired properties (e.g. specific symmetries).</p>
<p>A. Ostrogradsky's instability</p>
<p>As a starting point in constructing a Lagrangian, one can consider any function of positions and velocities that is smooth enough. In this procedure there exists a number of theoretical requirements, such as the verification of specific symmetries. However, as Ostrogradsky showed first [14], there exists an additional powerful constraint that does not allow Lagrangians with higher order time-derivatives. This requirement arises from the fact that in such case the corresponding Hamiltonian is not bounded, which implies that the total energy of the system is unbounded.</p>
<p>In order to avoid this Ostrogradsky instability the equations of motion of a physical system need to have up to second order equation of motion. A sufficient condition for that is if the Lagrangian has terms up to first order in derivatives, nevertheless this is not necessary since as Horndeski showed [15] one may have higher-order terms in a Lagrangian in a suitable combination that the resulting higher-order time derivatives in the equations of motion cancel out, resulting to second-order ones. Hence, this implies that discriminating if a given Lagrangian is healthy or not, requires the extraction of the equations of motion and their subsequent elaboration, a procedure that in case of complicated Lagrangians is not trivial. That is why it becomes of great interest the construction of a machine learning procedure that could discriminate if a given Lagrangian is healthy or not without the need to perform the variation and the extraction of equations of motion.</p>
<p>B. Describing a Lagrangian</p>
<p>A Lagrangian could in principle contain scalar quantities that are functions of higher dimensional objects. For instance the electromagnetic Lagrangian contains the tensor F µν , while the Einstein-Hilbert Lagrangian of general relativity possesses contractions of the Riemann tensor. In order to give a Lagrangian as input to a neural network it is necessary to translate it in a form that the network can handle, i.e to describe it as a set of features. Formally, a function φ : L → R n is to be found in order to provide the feature vectors, where L is the Lagrangian's space.</p>
<p>We limit ourselves to classical mechanics, in a sense that the fundamental quantity involved in the Lagrangian is the position of a moving body, x(t) as a function of time. Position along with its time derivatives constitute the set of physically interesting entities. Within our modelling, all physical parameters such as mass are set to unity. Moreover, the parameters are measured in proper units, in order to maintain the standard physical interpretation of potential energy. Some simple representations are provided in the following.</p>
<p>• A first parametrization is to use the kinetic term T = α 0 mx(t) α1ẋ (t) α2ẍ (t) α3 , which includes the standard kinetic term T = mẋ(t) 2 /2. Additionally, we consider a general potential of the form
V a1 (x(t),ẋ(t),ẍ(t)) = α 4 x(t) α5ẋ (t) α6ẍ (t) α7 .(3)
Hence, the corresponding set of numbers to describe this Lagrangian is f = [α 0 , ..., α 7 ]. Using the standard expression for the kinetic energy implies that [α 0 , α 1 , α 2 , α 3 ] = [0.5, 0, 2, 0]. By using eq. (2) it is easy to observe that the essential requirement for a Lagrangian of the form (3) to be free of the Ostrogradski instability is to have α 4 = 0 or α 4 = 1. The aforementioned representation could generalized by considering a sum of potentials of the form of (3):
V a2 (x(t),ẋ(t),ẍ(t)) = n i=1 α i0 x(t) αi1ẋ (t) αi2ẍ (t) αi3 ,(4)
with i = 1, · · · , n, and the corresponding feature vector to describe this Lagrangian is f = [0.5, 0.0, 2, 0.0, .., α n0 , α n1 , α n2 , α n3 ], with dimension 4(n + 1).</p>
<p>• Another parametrization is to further assume that every potential that describes a certain class of phenomena will be C ∞ , namely infinite times differentiable, at least within a certain subspace. From the latter, the corresponding Taylor series always exists, thus an adequate description of a general potential energy is its Taylor coefficients around the point (x 0 ,ẋ 0 ,ẍ 0 ). The most general Taylor expansion reads
V b (x,ẋ,ẍ) = ∞ j=0 1 j! (x − x 0 ) ∂ ∂x + (ẋ −ẋ 0 ) ∂ ∂ẋ + (ẍ −ẍ 0 ) ∂ ∂ẍ j V (x ,ẋ ,ẍ ) (x =x0,ẋ =ẋ0,ẍ =ẍ 0) .(5)
By keeping terms up to 2nd order and re-arranging we obtain the following
V b1 (x,ẋ,ẍ) = a 0 + a 1 x + a 2ẋ + a 3ẍ + a 4 x 2 + a 5ẋ 2 + a 6ẍ 2 + α 7 xẋ + α 8 xẍ + α 9ẋẍ ,(6)
where a i are real numbers and the corresponding feature vector is f = [0.5, 0.0, 2.0, 0.0, α 0 , .., α 9 ]. The terms α 0 and α 9 do not play any role in a Lagrangian formulation (their contribution to the field equations is zero), while the governing parameter is α 6 . By including terms up to the 3rd order, we obtain the potential V b2 as
V b2 (x,ẋ,ẍ) = a 0 + a 1 x + a 2ẋ + a 3ẍ + a 4 x 2 + a 5ẋ 2 + a 6ẍ 2 + α 7 xẋ + α 8 xẍ + α 9ẋẍ +α 10 x 3 + α 11ẋ 3 + α 12ẍ 3 + α 13 xẋẍ + α 14 x 2ẋ + α 15ẋ 2 x + α 16ẍ 2 x + α 17ẍẋ 2 .(7)
Similarly to the previous example, we mention that the term with coefficient α 17 is a total derivative and thus has no effect in the equations of motion. Finally, note that in principle, a series expansion could be performed on different bases, i.e Legendre, Laguerre and Hermitte polynomials.</p>
<p>In this work, without loss of generality we will use only parametrization V ai , since we are interested in studying the training of a neural network to discriminate between "healthy" and "non-healthy" theories.</p>
<p>There exist numerous descriptions with regard to certain classes of Lagrangians (finding a description valid for an arbitrary Lagrangian is left for a future project). In Table I 
7 D e −2α(r−r 0 ) − 2e −α(r−r 0 ) b
Morse potential [16] TABLE I: Some classical potentials, along with their representation ability in order to assess the generality of the parametrizations employed. Parametrizations a1,a2,b correspond to (3), (4) and (5) respectively.</p>
<p>III. MODEL AND ALGORITHMS</p>
<p>In order to classify a Lagrangian as healthy or not, we are using fully-connected feed-forward neural networks and supervised learning. Regarding the automated production of new Lagrangians, we employ Genetic Algorithms.</p>
<p>A. Neural Network setup</p>
<p>A neural network performs mappings from an input space to an output space. In our case the input space contains the feature vectors of the Lagrangians and the output space contains the two categories. The basic structural element of a neural network is the layer, that is a group of neurons. The neurons of each layer connect to those of the next and these connections are called synapses. The first layer of the network is called input layer, the last layer is called output layer and all the layers in between are called hidden layers. The architecture mentioned above describes a typical fully connected neural network as shown in Fig. 1. The term feed-forward implies a network that its synapses do not form a cycle, in contrast with e.g the recurrent neural networks [17]. Formal definitions of the aforementioned terms regarding feed -forward neural networks could be found at [18].</p>
<p>In order to understand how the mapping is realized, we need to break down the network to its components. The neurons of the first layer activate as we input a feature vector (corresponding to a Lagrangian). Activation means that a neuron calculate a value and "feeds" it to the next layer. As we mentioned above, in a fully -connected net, all the neurons of a layer are connected to all neurons of the previous one through synapses. Each synapse contains a real number, called weight, that indicates how much the activation of the previous layer specific neuron, affects the activation of the neuron in the next layer. Furthermore, each neuron contains a real value, called bias, that indicates a threshold, over which a neuron will meaningfully pass its value over to the next layer (fire). The activation of a neuron is described as
α l j = σ k w l jk α l−1 k + b l j ,(8)
. . .  where w l jk is the weight of the synapse that connects the k th neuron of the (l − 1) th layer to the j th neuron in the l th layer. Moreover, b l j is the bias of the j th neuron. The function σ is the sigmoid function, σ(x) = 1/ (1 + exp(x)). When a neural network is created all its weights and biases values are randomly initialized. Therefore, in order for our network to be able to classify correctly a given Langragian, its values need to be adjusted. Specifically these values need to be adjusted in a way that when a healthy Lagrangian is fed into the network, the network will provide as output a real number close to 1, and on the other hand in the case of the non-healthy Lagrangian it will provide as output a number close to 0.</p>
<p>In order to adjust those weights and biases we will employ the back-propagation algorithm, described further later in the next subsection.</p>
<p>Training</p>
<p>In order to construct the training data set, a Monte Carlo approach is employed. The i-th iteration of the process consists of constructing a random vector of length n. Subsequently, by using a symbolic algebra system, that is sympy [19], the Euler-Lagrange equations (2) are employed to extract the equations of motion. Furthermore, the equations are checked for the existence of time derivatives of order higher than 2. If there are only second derivatives in the equations of motion, the random vector is labeled as "healthy", otherwise it is labeled as "non-healthy". The above procedure is repeated N times. Some characteristic subsets of the a i space are presented in Fig. 2. We mention here that the aforementioned criterion of "healthy" Lagrangians is definitely not unique. In fact, one could choose to label as "healthy" any kind of Lagrangian, e.g. Lagrangians possessing additional features, satisfying extra symmetries, etc.</p>
<p>The total training data set constitutes of 2000 "healthy" and 2000 "non-healthy" feature vectors. The training strategy of backpropagation [18], consists of splitting the total dataset in batches with 400 elements each, i.e resulting to 10 batches. When all elements of a batch are fed into the network, the average cost is computed using the quadratic cost function. The average cost backpropagates through the network fixing its weights and biases. The same process needs to be done for each batch in order for an epoch of training to be completed. Before an epoch of training all the training set is scrambled, resulting in set of different batches. The training object is to minimize the cost function. Different values of the learning rate η are employed in an effort to avoid local minima, while also maintain a steep learning curve. All the above steps describe the iterative process of gradient descend.</p>
<p>FIG. 2:</p>
<p>3-dimensional subspaces of the parameter space for Lagrangians of the generic form L = T −α4x α 5ẋ α 6ẍ α 7 . The circular marker corresponds to "non-healthy" Lagrangians while the triangular one to "healthy".</p>
<p>Validation</p>
<p>After training the network, 10 data sets are produced, each of them consisting of 10000 feature vectors, and they are given to the network for classification. In this point another hyper-parameter is introduced, namely the threshold, that determines how a number in the continuum set [0, 1] is projected to the set [0, 1] ∩ N. Assessing the ability of the trained network to correctly classify data that have not encountered before, is essential in order to avoid over-fitting. The latter term describes the situation in which the neural network performs very well in the training data and poorly in the testing data, (for a detailed discussion of over-fitting within a statistical context see [20]). Towards this purpose, a number of metrics is utilized.</p>
<p>Accuracy measures the relative ability of a given network to provide correct predictions, that is Accuracy = True Positives + True Negatives Total Predictions .</p>
<p>It is straightforward to apply this metric, however there are some caveats. A characteristic example is that a model that classifies all input data as positive, applied to a dataset consisting of 95 positive and 5 negative samples, gives 0.95 accuracy score. Precision is the metric that expresses the proportion of the actual positive data over the data items that were classified by our model as positives:
Precision = True Positives True Positives + False Positives .(10)
Similarly, recall is the metric that expresses what proportion of the actual positive data was classified correctly by our model:</p>
<p>Recall = True Positives True Positives + False Negatives</p>
<p>.</p>
<p>Combining recall and precision by means of the harmonic mean results to the F 1 metric:
F 1 = 2Precision × Recall Precision + Recall .(12)
Up to this point, the numerical value of a network prediction is not used explicitly, since only the classification result is considered. In order to take into account the numerical value of the network's output, another metric is defined, namely the Logarithmic Loss. Although in principle one could utilize just any function as metric, Log Loss is motivated from information theory, and in particular from Kullback-Leibler information [21]:
LogLoss = − 1 N N i=1 M j=1 y ij log(p ij ),(13)
where N is the length of the training data set, M the number of different classes, y ij the probability of the i-th data point to belong to the j-th class, and p ij the predicted probability. Logarithmic Loss does not have upper bound and exists in the range [0, ∞). Logarithmic Loss nearer to 0 indicates higher accuracy, whereas if Logarithmic Loss is away from 0 then it indicates lower accuracy.</p>
<p>B. Constructing new Lagrangians</p>
<p>An intriguing path to explore is the possibility of constructing new Lagrangians in an automated way, with the aid of the trained network. In this work, in order to maintain transparency of the generating procedure, we use genetic algorithms (GA). In general there are alternative ways too, e.g. within the framework of Generative Adversarial Networks [22].</p>
<p>The main idea behind Generative Algorithms is that given a population and a way to assess the fitness of each member, if the result of the mating depends on the fitness of each parent, after a number of "generations" the fitness of the population will be maximum. A more formal consideration seems to be in place. We assume that a population of feature vectors P , with P ⊆ R n , exists. The trained network could be considered as a fitness function, N f it : P → [0, 1], and by using it one could associate a value within the interval [0, 1] to each member of the population. Furthermore, different members of the population are "mating" to create new ones, with the contribution of each parent's feature to the corresponding "child's" feature to be analogue with their relative fitness. Lastly, a random "mutation" to one or more elements of each "child" occurs according to a pre-defined distribution. These steps correspond to a "generation". After performing the aforementioned steps for a number of generations, we have a population whose each member has close to maximum fitness. The final population does not have common members with the initial one. For more details one could follow the presentation of the subject in [23].</p>
<p>In this work, the "mating" algorithm consists of a random selection of pairs within the initial population, and the "child" possesses the weighted average of the parent's features. The "mutation" step takes place at a random feature of the child if a dice is smaller that a pre-defined probability. The final result obviously is affected by the initial population and the mating and mutation operations. The details of the Generative Algorithms are presented in Table  II.   </p>
<p>IV. RESULTS AND DISCUSSION</p>
<p>A fully-connected neural network architecture is implemented and trained to discriminate between "healthy" and "non-healthy" Lagrangians for a classical theory with one degree of freedom. A number of different architectures, regarding the depth and the width of the network, were tested and the most promising was selected, namely a structure with two hidden layers of 16 neurons each. The optimal number of epochs to be used for training the network, in order to minimize training error on the one hand while avoiding over-fitting on the other, is found by comparing training and validation errors. Over-fitting occurs when we observe an increase in the testing error while the training error is consistently decreasing (see p. 202 of [24]). However, in our case the testing error seems to be decreasing over the epochs, resulting in a better "behaviour" of the network, as can be seen in Figs. 3, 4. Moreover, we also apply the metrics defined in Section III at different stages of the training procedure, as can be seen in Tables III and IV    V: A number of "new" Lagrangians that correspond to healthy theories, constructed using GA. The procedure was repeated for 10 times and the most fitted Lagrangian for each iteration was selected.</p>
<p>a list with the most prominent Lagrangians, each selected from the final population, after 200 generations, from 10 different initial populations. These Lagrangians were constructed by the machine learning procedure without the extraction of field equations. As one can see amongst others, the GA was able to find the correct feature, that a "healthy" Lagrangian needs to haveẍ to the exponent 0,1, although no such rule was initially imposed (concerning the other exponents we could have additionally required them to be integers, however we did not do it since it is irrelevant for our analysis).</p>
<p>One might argue that the gain is not significant, since a human can relatively easily write down Lagrangians similar to those of Table V, that will not lead to more than second-order terms in the field equations. Indeed, this is true for this simple subclass of one-dimensional Lagrangians, but it becomes laborious for higher dimensionality and more complicated Lagrangians. For instance in the case of gravitational theories, such as General Relativity and its modifications, where one uses four dimensions and Lagrangians made from contractions of the curvature (Riemann) tensor, it is impossible to deduce a priori if a Lagrangian leads to equations of motion with more than second-order terms (and thus whether it is physically accepted or not), unless these equations are explicitly extracted. Moreover, even after the explicit extraction of the equations of motion, a thorough elaboration is needed since terms with more than second-order derivatives could be mutually eliminated through suitable integration by parts (see e.g. [15,25,26]). In summary, even with the help of usual software the above standard procedure of generating new healthy Lagrangians can be laborious and require many months. Hence, we deduce that building machine learning tools that can construct new healthy Lagrangians without the need to perform all the steps of the standard procedure, namely explicitly extract the field equations and thoroughly examine and elaborate them, would be extremely helpful for the community of gravity, analytical and quantum mechanics, solid-state, theoretical physics, biology etc.</p>
<p>V. CONCLUSIONS</p>
<p>A neural network architecture was implemented, trained and tested, in order to decide if a given Lagrangian will lead to equations of motion with higher order derivatives, without explicitly performing the analytical calculations. Validation of the trained network using the explicit standard procedure proved that for the simple model under study, the efficient discrimination of healthy and non-healthy Lagrangians is established. Furthermore, "new" healthy Lagrangians were constructed in a fully automated way. Their properties are related with the initial Monte Carlo generated population of Lagrangians and the imposed requirements. Thus, in general one can automatically construct new Lagrangians, corresponding to new physical theories, possessing arbitrary properties. By suitably labeling the training data-sets, one can employ different criteria, resulting to a trained network that could decide for more complex cases.</p>
<p>There are various applications for this kind of approach, ranging from gravitational theories to solid state physics. In all cases, the search for a Lagrangian possessing some pre-defined properties (i.e symmetries) could be substantially simplified in the context of our approach. Additionally, it is interesting to mention that such an approach might be used to discriminate models derived from such Lagrangians. For instance in [27] it was proposed a deep learning tool in order to study the evolution of dark energy models, combining two architectures: the Recurrent Neural Networks and the Bayesian Neural Networks, since the former is capable of classifying objects while the latter emerges as a solution for problems like over-fitting. Such applications could be useful to confront with Lagrange-multiplier based models (see e.g. [28,29]) or other gravitational models arising from Lagrangian modifications. Definitely, in more complex applications, the exact form of the map between the Lagrangian space and the n-dimensional real space needs to be defined. Exploring the different ways to "translate" a Lagrangian to multi-dimensional real space, in an effort for a general description, is deemed very fruitful, and the present analysis can stand as its base.</p>
<p>FIG. 1 :
1The structure of a fully -connected neural Network. The input and output layers have n neurons, while there are k hidden layers, each one consisting of n neurons.</p>
<p>II: Parameters used in the Generative Algorithms approach, N is the dimension of the population. The weights wi are defined as wi = 10 5 N f it (Pi), where N f it is the trained neural network and Pi is a member of the population.</p>
<p>FIG. 3 :FIG. 4 :
34The training and testing errors as a function of training epochs, for 8The training and testing errors as a function of training epochs, for 18-16-16-1 network with η = 1.</p>
<p>IV: Values of different metrics during epochs of training for the fully-connected, feed-forward neural network, with architecture 18-16-16-1 and η = 1. Note that early epochs are affected by the initial random selection of weights.</p>
<p>some classical potentials are presented along with the proper map to construct the relevant feature vectors.No 
V 
Parametrization(s) 
Description 
1 
0 
a1, a2, b 
free particle 
2 </p>
<p>1 </p>
<p>2 kr 2 
a1, a2, b 
mass connected to an ideal spring of spring constant k 
3 </p>
<p>Gm 
r </p>
<p>a1, a2, b 
gravitational potential </p>
<p>4 
4 </p>
<p>σ 
r </p>
<p>12 − σ </p>
<p>r </p>
<p>6 </p>
<p>a2, b 
Lennard-Jones potential [16] </p>
<p>5 </p>
<p>σ 
r </p>
<p>ν </p>
<p>a1, a2, b 
soft-sphere potential [16] 
6 
mgcos(r) 
b 
non-linear oscillator </p>
<p>TABLE</p>
<p>.Epoch Accuracy Precision(s) Recall F1 
Log. Loss 
500 
0.549 
0.081 
0.580 0.564 
0.682 
5000 
0.613 
0.143 
1.0 
0.760 
0.549 
10000 
0.685 
0.164 
0.940 0.793 
0.374 
15000 
0.965 
0.653 
0.999 0.982 
0.067 
20000 
1.000 
1.000 
1.000 1.000 
0.026 
25000 
1.000 
1.000 
1.000 1.000 
0.016 </p>
<p>TABLE III: Values of different metrics during epochs of training for the fully-connected, feed-forward neural network, with 
architecture 8-6-6-1 and η = 0.1. Note that early epochs are affected by the initial random selection of weights. </p>
<p>By employing GA we finally construct new healthy Lagrangians. For indicative purposes, in Table V we present 
Epoch Accuracy Precision(s) Recall F1 
Log. Loss 
200 
0.965 
0.975 
0.973 0.969 
0.119 
400 
0.970 
0.979 
0.976 0.973 
0.097 
600 
0.972 
0.981 
0.976 0.974 
0.091 
800 
0.972 
0.983 
0.976 0.974 
0.088 
1000 
0.973 
0.983 
0.976 0.974 
0.086 
3000 
0.974 
0.987 
0.974 0.974 
0.080 
5000 
0.975 
0.988 
0.975 0.975 
0.076 </p>
<p>TABLE</p>
<p>TABLE</p>
<p>. R P Woodard, Reports on Progress in Physics. 72126002R. P. Woodard, Reports on Progress in Physics 72, 126002 (2009).</p>
<p>. L Smolin, hep-th/0303185arXiv preprintL. Smolin, arXiv preprint hep-th/0303185 (2003).</p>
<p>C Callender, N Huggett, Physics meets philosophy at the Planck scale: Contemporary theories in quantum gravity. Cambridge University PressC. Callender and N. Huggett, Physics meets philosophy at the Planck scale: Contemporary theories in quantum gravity (Cambridge University Press, 2001).</p>
<p>Effective action in quantum gravity. I L Buchbinder, I. L. Buchbinder, Effective action in quantum gravity (Routledge, 2017).</p>
<p>. R D King, M Liakata, C Lu, S G Oliver, L N Soldatova, Journal of The Royal Society Interface. 81440R. D. King, M. Liakata, C. Lu, S. G. Oliver, and L. N. Soldatova, Journal of The Royal Society Interface 8, 1440 (2011).</p>
<p>. D Waltz, B G Buchanan, Science. 32443D. Waltz and B. G. Buchanan, Science 324, 43 (2009).</p>
<p>. A Adadi, M Berrada, IEEE Access. 652138A. Adadi and M. Berrada, IEEE Access 6, 52138 (2018).</p>
<p>J Wagner, J M Kohler, T Gindele, L Hetzel, J T Wiedemer, S Behnke, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Wagner, J. M. Kohler, T. Gindele, L. Hetzel, J. T. Wiedemer, and S. Behnke, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2019), pp. 9097-9107.</p>
<p>E Choi, M T Bahadori, J Sun, J Kulas, A Schuetz, W Stewart, Advances in Neural Information Processing Systems. E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart, in Advances in Neural Information Processing Systems (2016), pp. 3504-3512.</p>
<p>H Goldstein, C Poole, J Safko, Classical mechanics. H. Goldstein, C. Poole, and J. Safko, Classical mechanics (2002).</p>
<p>A modern introduction to quantum field theory. M Maggiore, Oxford university press12M. Maggiore, A modern introduction to quantum field theory, vol. 12 (Oxford university press, 2005).</p>
<p>S Weinberg, Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity. S. Weinberg, Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity, (1972).</p>
<p>. E N Saridakis, arXiv:2105.12582CANTATA. gr-qcE. N. Saridakis et al. [CANTATA], [arXiv:2105.12582 [gr-qc]].</p>
<p>. M Ostrogradsky, Mem. Acad. St. Petersbourg. 6385M. Ostrogradsky, Mem. Acad. St. Petersbourg 6, 385 (1850).</p>
<p>. G W Horndeski, Int. J. Theor. Phys. 10363G. W. Horndeski, Int. J. Theor. Phys. 10, 363 (1974).</p>
<p>M Fyta, 10.1088/978-1-6817-4417-9ch4978-1-6817-4417-9Computational Approaches in Physics. Morgan &amp; Claypool PublishersM. Fyta, in Computational Approaches in Physics (Morgan &amp; Claypool Publishers, 2016), 2053-2571, pp. 4-1 to 4-15, ISBN 978-1-6817-4417-9, URL http://dx.doi.org/10.1088/978-1-6817-4417-9ch4.</p>
<p>D P Mandic, J Chambers, Recurrent neural networks for prediction: learning algorithms, architectures and stability. John Wiley &amp; Sons, IncD. P. Mandic and J. Chambers, Recurrent neural networks for prediction: learning algorithms, architectures and stability (John Wiley &amp; Sons, Inc., 2001).</p>
<p>Chemometrics and intelligent laboratory systems. D Svozil, V Kvasnicka, J Pospichal, 3943D. Svozil, V. Kvasnicka, and J. Pospichal, Chemometrics and intelligent laboratory systems 39, 43 (1997).</p>
<p>. A Meurer, 10.7717/peerj-cs.1032376-5992PeerJ Computer Science. 3103A. Meurer et al., PeerJ Computer Science 3, e103 (2017), ISSN 2376-5992, URL https://doi.org/10.7717/peerj-cs.103.</p>
<p>. S Geman, E Bienenstock, R Doursat, Neural computation. 41S. Geman, E. Bienenstock, and R. Doursat, Neural computation 4, 1 (1992).</p>
<p>I Goodfellow, Y Bengio, A Courville, Deep learning. MIT pressI. Goodfellow, Y. Bengio, and A. Courville, Deep learning (MIT press, 2016).</p>
<p>I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, in Advances in neural information processing systems (2014), pp. 2672-2680.</p>
<p>D E Goldberg, Genetic algorithms. Pearson Education IndiaD. E. Goldberg, Genetic algorithms (Pearson Education India, 2006).</p>
<p>C Rao, V N Gudivada, Computational Analysis and Understanding of Natural Languages: Principles, Methods and Applications. ElsevierC. Rao and V. N. Gudivada, Computational Analysis and Understanding of Natural Languages: Principles, Methods and Applications (Elsevier, 2018).</p>
<p>. E N Saridakis, M Tsoukalas, 1601.06734Phys. Rev. 93124032E. N. Saridakis and M. Tsoukalas, Phys. Rev. D93, 124032 (2016), 1601.06734.</p>
<p>. C Erices, E Papantonopoulos, E N Saridakis, 1903.11128Phys. Rev. 99123527C. Erices, E. Papantonopoulos, and E. N. Saridakis, Phys. Rev. D99, 123527 (2019), 1903.11128.</p>
<p>. C Escamilla-Rivera, M A C Quintero, S Capozziello, 1910.02788JCAP. 038C. Escamilla-Rivera, M. A. C. Quintero and S. Capozziello, JCAP 03, 008 (2020), 1910.02788.</p>
<p>. S Capozziello, J Matsumoto, S Nojiri, S D Odintsov, 1004.3691Phys. Lett. B. 693S. Capozziello, J. Matsumoto, S. Nojiri and S. D. Odintsov, Phys. Lett. B 693, (2010), 1004.3691.</p>
<p>. Y F Cai, E N Saridakis, Class. Quant. Grav. 28Y. F. Cai and E. N. Saridakis, Class. Quant. Grav. 28, 035010 (2011), 1007.3204.</p>            </div>
        </div>

    </div>
</body>
</html>
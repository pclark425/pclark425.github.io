<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7778 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7778</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7778</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-276580097</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.18209v2.pdf" target="_blank">League: Leaderboard Generation on Demand</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper's proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7778.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7778.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leaderboard Coverage Criterion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A criterion used by League to measure how many of the retrieved papers are actually used to construct the leaderboard, indicating breadth of paper coverage relative to retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>League evaluation (LLM evaluators: GPT-4o, O1-preview, Qwen2.5-14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied (proprietary and open models used; e.g., Qwen2.5-14B, GPT-4o, O1-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / AI leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Ratio of the number of papers used for leaderboard generation to the total number of papers searched; computed per leaderboard and scaled to a 5-point score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>(P_used / P_total) * 5</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Normalized coverage score on a 0–5 scale computed as (number of papers used in leaderboard) divided by (number of papers retrieved), then multiplied by 5.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>N/A (applies to leaderboards built from retrieved corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Coverage comparisons are presented against human-curated leaderboards; human-curator baseline scores reported in paper for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: For 20-item leaderboards League Coverage ≈ 4.12 (vs human 4.72) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>League Coverage scores approach human-curated scores (e.g., 4.12 vs 4.72 for 20-item leaderboards).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Depends on retrieval quality; imperfect retrieval can reduce Coverage; sensitive to noisy/irrelevant retrieved papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7778.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leaderboard Latest Criterion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A criterion measuring how recent the papers used in the leaderboard are, designed to capture timeliness of the generated leaderboard.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>League evaluation (LLM evaluators: GPT-4o, O1-preview, Qwen2.5-14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / AI leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Latest</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Ratio of the number of papers published after a specified cutoff date to the total number of papers searched; scaled to a 5-point score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>(P_new / P_total) * 5</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Normalized timeliness score on a 0–5 scale computed as (number of papers published after cutoff) divided by (total papers retrieved), then multiplied by 5.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human-curated leaderboards used as reference; paper reports human scores for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: For 20-item leaderboards League Latest ≈ 3.96 (vs human 4.68) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>League Latest scores are slightly lower than human-curated leaderboards but close (e.g., 3.96 vs 4.68 for 20 items).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on accurate publication date filtering; topic cutoff definition impacts score.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7778.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leaderboard Structure Criterion (5-point rubric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A qualitative rubric to judge the logical organization and readability of the leaderboard, scored on a 1–5 scale according to descriptive levels of structural quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM evaluators (GPT-4o, O1-preview, Qwen2.5-14B) for automated scoring</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / AI leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criterion / qualitative rubric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Structure (1–5 descriptive scale)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human-calibrated 5-point rubric evaluating logical consistency, header/row alignment, redundancy, and readability of leaderboard structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Structure score on 1–5 Likert-like descriptive scale</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer score {1,2,3,4,5} where 1 indicates poor structure and 5 indicates strong logical consistency and clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>LLM scoring calibrated by human experts; explicit numeric human structure scores reported in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: For 20-item leaderboards League Structure ≈ 4.16 (vs human 4.34) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Structure scores of League-generated leaderboards are slightly below human-curated ones but comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Subjective rubric; requires human calibration; automated scoring correlates but is not identical to human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7778.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Aspect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Aspect Leaderboard Quality Metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregate quality metric combining Coverage, Latest, and Structure into an average score to evaluate multi-dataset/topic leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>League evaluation (LLM evaluators: GPT-4o, O1-preview, Qwen2.5-14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / AI leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>composite evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multi-Aspect</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Average quality score across N datasets/leaderboards computed from Coverage, Latest, and Structure scores to yield an overall quality measure.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>(NCoverage + NLatest + NStructure) / (3 * N)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Normalized average of the three criterion scores across N leaderboards; returns a scalar representing multi-dataset quality.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used to compare multi-dataset leaderboards against human baselines; human Multi-Aspect scores reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: For 20-item leaderboards League Multi-Aspect ≈ 4.08 (vs human 4.58) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Multi-Aspect scores are close to but below human-curated leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Averaging hides per-criterion tradeoffs; depends on reliability of component scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7778.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topic-Related Quality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topic-Related Quality (Precision & Recall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pair of standard information-retrieval metrics (precision and recall) used to evaluate whether items in generated leaderboards are relevant to the specified research topic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>League retrieval + LLM filtering pipeline (evaluated with LLMs and humans)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / AI leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>retrieval relevance evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Precision and Recall (Topic-Related)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Recall measures proportion of true-topic items retrieved and used; precision measures proportion of used items that are actually topic-relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall (%), Precision (%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Recall = (relevant items retrieved and used) / (all relevant items in corpus); Precision = (relevant items used) / (all items used). Expressed as percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Leaderboards constructed from arXiv / top-tier conference papers for specified topics</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human baseline leaderboards used for reference; exact number of human raters unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>For 20-item leaderboards: Recall = 67.58%, Precision = 70.33% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>League attains moderate recall and precision, showing strong retrieval and filtering but not perfect alignment with human-curated items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Recall limited by retrieval and filtering; precision affected by noisy matches from title/abstract-only search and keyword ambiguities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7778.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Content Quality (5-pt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leaderboard Content Quality (LLM-assigned 5-point scores calibrated by humans)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Content quality is assessed across multiple aspects with LLMs assigning calibrated 5-point scores (by human experts) for coverage, latest, and structure, used to judge generated leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM evaluators (GPT-4o, O1-preview, Qwen2.5-7B/14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / AI leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human-calibrated LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Content Quality (LLM 5-point calibrated scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLMs score leaderboards on a 1–5 scale per aspect (Coverage, Latest, Structure); LLM scoring is calibrated against human expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>5-point Likert-like scores per aspect; aggregated as needed</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer scores from 1 (poor) to 5 (excellent) for each aspect; averages reported with standard deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Set of generated leaderboards (various topics) compared to human-curated leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human experts provided calibration and pairwise comparisons; exact count of experts not specified; pairwise ranking of 20 leaderboards used in meta-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example content-quality aspect scores for 20-item leaderboards: Coverage 4.12, Latest 3.96, Structure 4.16, Multi-Aspect 4.08 (League) vs human: Coverage 4.72, Latest 4.68, Structure 4.34, Multi-Aspect 4.58.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-assigned scores correlate moderately-to-strongly with human judgments (see Pearson correlation results); Content Quality of League is slightly below human performance but close.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM scoring requires careful human calibration; moderate correlation indicates possible systematic differences; subjectivity in scoring remains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7778.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PearsonCorrMetaEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pearson Correlation Coefficient for LLM vs Human Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta-evaluation measuring the alignment (Pearson r) between automated LLM evaluations of leaderboards and human expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLM evaluators (GPT-4o, O1-preview, Qwen2.5-7B, Qwen2.5-14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / AI leaderboards; evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>meta-evaluation statistic</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pearson Correlation Coefficient (r)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Pearson r between rankings/scores produced by LLM evaluators and rankings/scores produced by human experts across a set of leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson r (range -1 to 1)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Pearson correlation coefficient measuring linear relationship between two sets of scores (LLM vs human).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Set of 20 League-generated leaderboards used in pairwise/human ranking comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human experts ranked 20 generated leaderboards using the same scoring criteria provided to LLMs; exact number of experts unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported Pearson r values show a strong positive correlation; best-performing evaluator O1-preview achieved r = 0.76 (other models reported lower r values).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM evaluation rankings correlate strongly with human rankings (e.g., r up to 0.76), suggesting automated evaluation is a reliable proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Number of human evaluators not reported; correlation does not prove identical judgments; depends on leaderboards and scoring calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7778.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConstructionSpeed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leaderboard Construction Speed (T_manual decomposition / LLM time)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative measurement of time required to construct leaderboards manually vs automatically with League; includes a decomposition of manual workflow into components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>League pipeline using LLMs (GPT-4o, O1-preview, Qwen models) for automation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / workflow evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>performance/time evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Construction Speed (time in seconds)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Measure end-to-end time for leaderboard construction: manual time T_manual decomposed into Tr (search), Tb (browse), Tf (filter), Te (extract), Tc (integrate); League reports total API invocation time and pipeline runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Total time in seconds (s)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>T_manual = Tr + Tb + Tf + Te + Tc; League time measured as sum of LLM API calls and processing time; reported in seconds.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Leaderboards of varying lengths (5,10,15,20 items) used as scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual baseline times measured for human curators; exact human count unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: Manual 20-item leaderboard construction ≈ 1128 s; League ≈ 117.45 s for same 20-item case (Table 2). League can generate 20-item leaderboard in ~2 minutes vs ~18+ minutes manual.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>League reduces time by an order of magnitude while producing comparable content-quality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Time depends on LLM API latency and token costs; some manual steps in filtering may still be necessary; reported times average across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7778.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TableClassificationEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Table Classification Evaluation (Precision/Recall/F1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLMs' ability to classify extracted tables into categories (main results, ablation, others) using few-shot prompting; measured with precision, recall, and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, O1-preview, Qwen2.5-7B/14B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (proprietary), O1-preview (proprietary), Qwen2.5-7B/14B (open)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific information extraction / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method classification evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Table classification P/R/F1</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLMs classify tables into predefined types using in-context examples or Chain-of-Thought prompting; performance measured against manually annotated table types.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 (percent)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard classification metrics: precision = TP/(TP+FP), recall = TP/(TP+FN), F1 = harmonic mean of precision & recall.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Manually annotated set: 354 tables from 72 papers (197 main result tables, 54 ablation, 103 others) used in evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Annotation performed by humans to produce gold labels for table types; number of annotators not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper reports >80% F1 for GPT-4o and O1-preview using 1-shot prompts on table classification (exact per-model numbers listed in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Not directly compared to human classification performance other than using human-annotated gold labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Token costs and repeated API calls can be expensive; errors arise from ambiguous table captions and formatting variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7778.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TableNER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Table-level Entity Recognition (Table NER) Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLMs for extracting entities (methods, datasets, settings, metrics) from tables using a format following SciIE annotation guidelines, assessed with P/R/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (GPT-4o, O1-preview, Qwen variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>entity extraction evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Table NER Precision/Recall/F1</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLMs identify cell-level entities in extracted tables and are evaluated against a manually annotated validation set following SciIE definitions (methods, datasets, settings, metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 (percent)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard NER metrics computed at entity level.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Validation set: manually annotated tables (example: SciIE-style annotations; Li et al. 2023 provided 336 entities in 28 tables cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Gold annotations created by humans; exact annotator counts not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>League shows robust Table NER performance with notable improvements when enhanced prompting is applied; specific F1s reported in Table 3 (1-shot prompts outperform 0-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Performance reported relative to human-annotated gold labels; no direct human vs LLM qualitative comparison beyond metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Ambiguous abbreviations and inconsistent metric naming across papers cause extraction failures; some metric values missing due to absence in original text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7778.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT / ICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) and In-Context Learning (ICL) for Table Processing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies used by League to improve table classification and extraction: few-shot in-context examples and Chain-of-Thought prompting to reduce token usage and improve extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (GPT-4o, O1-preview, Qwen2.5-series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / prompting methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>prompting/methodological framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use 0-shot and 1-shot prompts and few-shot CoT flows to guide LLMs to classify tables and extract entities in fewer API calls and with higher accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Indirect (improvements reported via higher P/R/F1 and lower token/API usage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Performance improvements measured by classification/NER metrics and API/token efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used on extracted tables from collected papers and annotated validation sets described above</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>1-shot examples provided manually as guidance; human experts designed prompts/examples.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>1-shot prompting with CoT yields higher F1 than 0-shot for table classification and NER (paper reports substantial relative improvements; see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>1-shot prompts improve model scores versus 0-shot; exact deltas shown in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires manual example selection for best performance; remains sensitive to prompt design and token costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7778.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCILEAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCILEAD dataset (hand-curated leaderboards)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hand-curated dataset introduced by Şahinuç et al. (2024) containing 27 leaderboards from 43 NLP papers used to evaluate LLMs on entity extraction and leaderboard construction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / scientific information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SCILEAD benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Hand-curated leaderboards and associated annotations used as gold references to evaluate LLM extraction and leaderboard generation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Entity extraction metrics (P/R/F1), leaderboard-level quality comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard extraction metrics on annotated leaderboards / leaderboards compared qualitatively to generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCILEAD (27 leaderboards from 43 papers) as described in related-work citations</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Dataset is hand-curated by human annotators (details in Şahinuç et al. 2024, not restated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>SCILEAD cited as prior benchmark for evaluating LLM entity extraction; no new SCILEAD results produced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>SCILEAD is hand-curated and relatively small; prior benchmarks provide limited dynamic coverage compared to League's goal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7778.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciIE-style entity definitions and annotations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entity taxonomy and annotation protocol (from Li et al., 2023 / SciIE) for datasets, methods, metrics, and settings; adopted by League to define extracted scientific terms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>annotation schema / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SciIE entity schema</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Defines entities (methods, datasets, experiment settings, metrics) for structured extraction from tables and text; used as standard for Table NER evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Entity-level P/R/F1 when applied to annotated sets</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard NER metrics applied to SciIE-style annotated entities.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciIE-style annotated tables (Li et al. 2023 reported 336 entities from 28 tables as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Annotations created by humans per SciIE protocol; exact annotator counts not specified in League paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>League follows SciIE definitions for entity extraction; SciIE dataset statistics (336 entities from 28 tables) cited as reference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>SciIE annotations are limited in scope; real-world tables may contain varied and abbreviated metric names causing extraction difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7778.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7778.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TELIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TELIN (Table Entity Linker) / TELIN-style extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (Yang et al., 2022) for extracting table entities to create leaderboards; referenced as related work and baseline approach for table-entity extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific information extraction / leaderboard mining</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>extraction framework / dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>TELIN-style table entity extraction evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Extraction of table entities and linking to leaderboard entries; evaluated via entity-level precision/recall/F1 and leaderboards reconstruction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Entity-level P/R/F1, completeness of reconstructed leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard extraction and reconstruction metrics (percentages, F1).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>TELIN / prior extracted table corpora as referenced in related work</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>TELIN used annotated tables for evaluation; specifics in the original TELIN paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Referenced as prior baseline; no TELIN-specific experimental numbers produced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Prior methods produce static snapshots and lack dynamic updates; League claims to go beyond these approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'League: Leaderboard Generation on Demand', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>All data on the table: Novel dataset and benchmark for cross-modality scientific information extraction <em>(Rating: 2)</em></li>
                <li>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards <em>(Rating: 2)</em></li>
                <li>Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction <em>(Rating: 2)</em></li>
                <li>AxCell: Automatic extraction of results from machine learning papers <em>(Rating: 2)</em></li>
                <li>TELIN: Table entity LINker for extracting leaderboards from machine learning publications <em>(Rating: 2)</em></li>
                <li>Legobench: Scientific leaderboard generation benchmark <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7778",
    "paper_id": "paper-276580097",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Coverage",
            "name_full": "Leaderboard Coverage Criterion",
            "brief_description": "A criterion used by League to measure how many of the retrieved papers are actually used to construct the leaderboard, indicating breadth of paper coverage relative to retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "League evaluation (LLM evaluators: GPT-4o, O1-preview, Qwen2.5-14B)",
            "model_size": "varied (proprietary and open models used; e.g., Qwen2.5-14B, GPT-4o, O1-preview)",
            "scientific_domain": "computer science / AI leaderboards",
            "theory_type": "evaluation criterion",
            "evaluation_method_name": "Coverage",
            "evaluation_method_description": "Ratio of the number of papers used for leaderboard generation to the total number of papers searched; computed per leaderboard and scaled to a 5-point score.",
            "evaluation_metric": "(P_used / P_total) * 5",
            "metric_definition": "Normalized coverage score on a 0–5 scale computed as (number of papers used in leaderboard) divided by (number of papers retrieved), then multiplied by 5.",
            "dataset_or_benchmark": "N/A (applies to leaderboards built from retrieved corpora)",
            "human_evaluation_details": "Coverage comparisons are presented against human-curated leaderboards; human-curator baseline scores reported in paper for comparison.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Example: For 20-item leaderboards League Coverage ≈ 4.12 (vs human 4.72) as reported in Table 2.",
            "comparison_to_human_generated": true,
            "comparison_results": "League Coverage scores approach human-curated scores (e.g., 4.12 vs 4.72 for 20-item leaderboards).",
            "limitations_noted": "Depends on retrieval quality; imperfect retrieval can reduce Coverage; sensitive to noisy/irrelevant retrieved papers.",
            "uuid": "e7778.0",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Latest",
            "name_full": "Leaderboard Latest Criterion",
            "brief_description": "A criterion measuring how recent the papers used in the leaderboard are, designed to capture timeliness of the generated leaderboard.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "League evaluation (LLM evaluators: GPT-4o, O1-preview, Qwen2.5-14B)",
            "model_size": "varied",
            "scientific_domain": "computer science / AI leaderboards",
            "theory_type": "evaluation criterion",
            "evaluation_method_name": "Latest",
            "evaluation_method_description": "Ratio of the number of papers published after a specified cutoff date to the total number of papers searched; scaled to a 5-point score.",
            "evaluation_metric": "(P_new / P_total) * 5",
            "metric_definition": "Normalized timeliness score on a 0–5 scale computed as (number of papers published after cutoff) divided by (total papers retrieved), then multiplied by 5.",
            "dataset_or_benchmark": "N/A",
            "human_evaluation_details": "Human-curated leaderboards used as reference; paper reports human scores for comparison.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Example: For 20-item leaderboards League Latest ≈ 3.96 (vs human 4.68) as reported in Table 2.",
            "comparison_to_human_generated": true,
            "comparison_results": "League Latest scores are slightly lower than human-curated leaderboards but close (e.g., 3.96 vs 4.68 for 20 items).",
            "limitations_noted": "Relies on accurate publication date filtering; topic cutoff definition impacts score.",
            "uuid": "e7778.1",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Structure",
            "name_full": "Leaderboard Structure Criterion (5-point rubric)",
            "brief_description": "A qualitative rubric to judge the logical organization and readability of the leaderboard, scored on a 1–5 scale according to descriptive levels of structural quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM evaluators (GPT-4o, O1-preview, Qwen2.5-14B) for automated scoring",
            "model_size": "varied",
            "scientific_domain": "computer science / AI leaderboards",
            "theory_type": "evaluation criterion / qualitative rubric",
            "evaluation_method_name": "Structure (1–5 descriptive scale)",
            "evaluation_method_description": "Human-calibrated 5-point rubric evaluating logical consistency, header/row alignment, redundancy, and readability of leaderboard structure.",
            "evaluation_metric": "Structure score on 1–5 Likert-like descriptive scale",
            "metric_definition": "Integer score {1,2,3,4,5} where 1 indicates poor structure and 5 indicates strong logical consistency and clarity.",
            "dataset_or_benchmark": "N/A",
            "human_evaluation_details": "LLM scoring calibrated by human experts; explicit numeric human structure scores reported in comparisons.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Example: For 20-item leaderboards League Structure ≈ 4.16 (vs human 4.34) as reported in Table 2.",
            "comparison_to_human_generated": true,
            "comparison_results": "Structure scores of League-generated leaderboards are slightly below human-curated ones but comparable.",
            "limitations_noted": "Subjective rubric; requires human calibration; automated scoring correlates but is not identical to human judgment.",
            "uuid": "e7778.2",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Multi-Aspect",
            "name_full": "Multi-Aspect Leaderboard Quality Metric",
            "brief_description": "An aggregate quality metric combining Coverage, Latest, and Structure into an average score to evaluate multi-dataset/topic leaderboards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "League evaluation (LLM evaluators: GPT-4o, O1-preview, Qwen2.5-14B)",
            "model_size": "varied",
            "scientific_domain": "computer science / AI leaderboards",
            "theory_type": "composite evaluation metric",
            "evaluation_method_name": "Multi-Aspect",
            "evaluation_method_description": "Average quality score across N datasets/leaderboards computed from Coverage, Latest, and Structure scores to yield an overall quality measure.",
            "evaluation_metric": "(NCoverage + NLatest + NStructure) / (3 * N)",
            "metric_definition": "Normalized average of the three criterion scores across N leaderboards; returns a scalar representing multi-dataset quality.",
            "dataset_or_benchmark": "N/A",
            "human_evaluation_details": "Used to compare multi-dataset leaderboards against human baselines; human Multi-Aspect scores reported.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Example: For 20-item leaderboards League Multi-Aspect ≈ 4.08 (vs human 4.58) as reported in Table 2.",
            "comparison_to_human_generated": true,
            "comparison_results": "Multi-Aspect scores are close to but below human-curated leaderboards.",
            "limitations_noted": "Averaging hides per-criterion tradeoffs; depends on reliability of component scores.",
            "uuid": "e7778.3",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Topic-Related Quality",
            "name_full": "Topic-Related Quality (Precision & Recall)",
            "brief_description": "A pair of standard information-retrieval metrics (precision and recall) used to evaluate whether items in generated leaderboards are relevant to the specified research topic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "League retrieval + LLM filtering pipeline (evaluated with LLMs and humans)",
            "model_size": "varied",
            "scientific_domain": "computer science / AI leaderboards",
            "theory_type": "retrieval relevance evaluation",
            "evaluation_method_name": "Precision and Recall (Topic-Related)",
            "evaluation_method_description": "Recall measures proportion of true-topic items retrieved and used; precision measures proportion of used items that are actually topic-relevant.",
            "evaluation_metric": "Recall (%), Precision (%)",
            "metric_definition": "Recall = (relevant items retrieved and used) / (all relevant items in corpus); Precision = (relevant items used) / (all items used). Expressed as percentages.",
            "dataset_or_benchmark": "Leaderboards constructed from arXiv / top-tier conference papers for specified topics",
            "human_evaluation_details": "Human baseline leaderboards used for reference; exact number of human raters unspecified.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "For 20-item leaderboards: Recall = 67.58%, Precision = 70.33% (Table 2).",
            "comparison_to_human_generated": true,
            "comparison_results": "League attains moderate recall and precision, showing strong retrieval and filtering but not perfect alignment with human-curated items.",
            "limitations_noted": "Recall limited by retrieval and filtering; precision affected by noisy matches from title/abstract-only search and keyword ambiguities.",
            "uuid": "e7778.4",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Content Quality (5-pt)",
            "name_full": "Leaderboard Content Quality (LLM-assigned 5-point scores calibrated by humans)",
            "brief_description": "Content quality is assessed across multiple aspects with LLMs assigning calibrated 5-point scores (by human experts) for coverage, latest, and structure, used to judge generated leaderboards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM evaluators (GPT-4o, O1-preview, Qwen2.5-7B/14B)",
            "model_size": "varied",
            "scientific_domain": "computer science / AI leaderboards",
            "theory_type": "human-calibrated LLM evaluation",
            "evaluation_method_name": "Content Quality (LLM 5-point calibrated scoring)",
            "evaluation_method_description": "LLMs score leaderboards on a 1–5 scale per aspect (Coverage, Latest, Structure); LLM scoring is calibrated against human expert judgments.",
            "evaluation_metric": "5-point Likert-like scores per aspect; aggregated as needed",
            "metric_definition": "Integer scores from 1 (poor) to 5 (excellent) for each aspect; averages reported with standard deviations.",
            "dataset_or_benchmark": "Set of generated leaderboards (various topics) compared to human-curated leaderboards",
            "human_evaluation_details": "Human experts provided calibration and pairwise comparisons; exact count of experts not specified; pairwise ranking of 20 leaderboards used in meta-evaluation.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Example content-quality aspect scores for 20-item leaderboards: Coverage 4.12, Latest 3.96, Structure 4.16, Multi-Aspect 4.08 (League) vs human: Coverage 4.72, Latest 4.68, Structure 4.34, Multi-Aspect 4.58.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-assigned scores correlate moderately-to-strongly with human judgments (see Pearson correlation results); Content Quality of League is slightly below human performance but close.",
            "limitations_noted": "LLM scoring requires careful human calibration; moderate correlation indicates possible systematic differences; subjectivity in scoring remains.",
            "uuid": "e7778.5",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "PearsonCorrMetaEval",
            "name_full": "Pearson Correlation Coefficient for LLM vs Human Evaluation",
            "brief_description": "Meta-evaluation measuring the alignment (Pearson r) between automated LLM evaluations of leaderboards and human expert judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLM evaluators (GPT-4o, O1-preview, Qwen2.5-7B, Qwen2.5-14B)",
            "model_size": "varied",
            "scientific_domain": "computer science / AI leaderboards; evaluation methodology",
            "theory_type": "meta-evaluation statistic",
            "evaluation_method_name": "Pearson Correlation Coefficient (r)",
            "evaluation_method_description": "Compute Pearson r between rankings/scores produced by LLM evaluators and rankings/scores produced by human experts across a set of leaderboards.",
            "evaluation_metric": "Pearson r (range -1 to 1)",
            "metric_definition": "Pearson correlation coefficient measuring linear relationship between two sets of scores (LLM vs human).",
            "dataset_or_benchmark": "Set of 20 League-generated leaderboards used in pairwise/human ranking comparisons",
            "human_evaluation_details": "Human experts ranked 20 generated leaderboards using the same scoring criteria provided to LLMs; exact number of experts unspecified.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Reported Pearson r values show a strong positive correlation; best-performing evaluator O1-preview achieved r = 0.76 (other models reported lower r values).",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM evaluation rankings correlate strongly with human rankings (e.g., r up to 0.76), suggesting automated evaluation is a reliable proxy.",
            "limitations_noted": "Number of human evaluators not reported; correlation does not prove identical judgments; depends on leaderboards and scoring calibration.",
            "uuid": "e7778.6",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ConstructionSpeed",
            "name_full": "Leaderboard Construction Speed (T_manual decomposition / LLM time)",
            "brief_description": "Quantitative measurement of time required to construct leaderboards manually vs automatically with League; includes a decomposition of manual workflow into components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "League pipeline using LLMs (GPT-4o, O1-preview, Qwen models) for automation",
            "model_size": "varied",
            "scientific_domain": "computer science / workflow evaluation",
            "theory_type": "performance/time evaluation",
            "evaluation_method_name": "Construction Speed (time in seconds)",
            "evaluation_method_description": "Measure end-to-end time for leaderboard construction: manual time T_manual decomposed into Tr (search), Tb (browse), Tf (filter), Te (extract), Tc (integrate); League reports total API invocation time and pipeline runtime.",
            "evaluation_metric": "Total time in seconds (s)",
            "metric_definition": "T_manual = Tr + Tb + Tf + Te + Tc; League time measured as sum of LLM API calls and processing time; reported in seconds.",
            "dataset_or_benchmark": "Leaderboards of varying lengths (5,10,15,20 items) used as scenarios",
            "human_evaluation_details": "Manual baseline times measured for human curators; exact human count unspecified.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Example: Manual 20-item leaderboard construction ≈ 1128 s; League ≈ 117.45 s for same 20-item case (Table 2). League can generate 20-item leaderboard in ~2 minutes vs ~18+ minutes manual.",
            "comparison_to_human_generated": true,
            "comparison_results": "League reduces time by an order of magnitude while producing comparable content-quality scores.",
            "limitations_noted": "Time depends on LLM API latency and token costs; some manual steps in filtering may still be necessary; reported times average across runs.",
            "uuid": "e7778.7",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "TableClassificationEval",
            "name_full": "Table Classification Evaluation (Precision/Recall/F1)",
            "brief_description": "Evaluation of LLMs' ability to classify extracted tables into categories (main results, ablation, others) using few-shot prompting; measured with precision, recall, and F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o, O1-preview, Qwen2.5-7B/14B",
            "model_size": "GPT-4o (proprietary), O1-preview (proprietary), Qwen2.5-7B/14B (open)",
            "scientific_domain": "scientific information extraction / NLP",
            "theory_type": "method classification evaluation",
            "evaluation_method_name": "Table classification P/R/F1",
            "evaluation_method_description": "LLMs classify tables into predefined types using in-context examples or Chain-of-Thought prompting; performance measured against manually annotated table types.",
            "evaluation_metric": "Precision, Recall, F1 (percent)",
            "metric_definition": "Standard classification metrics: precision = TP/(TP+FP), recall = TP/(TP+FN), F1 = harmonic mean of precision & recall.",
            "dataset_or_benchmark": "Manually annotated set: 354 tables from 72 papers (197 main result tables, 54 ablation, 103 others) used in evaluation",
            "human_evaluation_details": "Annotation performed by humans to produce gold labels for table types; number of annotators not specified.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Paper reports &gt;80% F1 for GPT-4o and O1-preview using 1-shot prompts on table classification (exact per-model numbers listed in Table 3).",
            "comparison_to_human_generated": false,
            "comparison_results": "Not directly compared to human classification performance other than using human-annotated gold labels.",
            "limitations_noted": "Token costs and repeated API calls can be expensive; errors arise from ambiguous table captions and formatting variability.",
            "uuid": "e7778.8",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "TableNER",
            "name_full": "Table-level Entity Recognition (Table NER) Evaluation",
            "brief_description": "Evaluation of LLMs for extracting entities (methods, datasets, settings, metrics) from tables using a format following SciIE annotation guidelines, assessed with P/R/F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs (GPT-4o, O1-preview, Qwen variants)",
            "model_size": "varied",
            "scientific_domain": "scientific information extraction",
            "theory_type": "entity extraction evaluation",
            "evaluation_method_name": "Table NER Precision/Recall/F1",
            "evaluation_method_description": "LLMs identify cell-level entities in extracted tables and are evaluated against a manually annotated validation set following SciIE definitions (methods, datasets, settings, metrics).",
            "evaluation_metric": "Precision, Recall, F1 (percent)",
            "metric_definition": "Standard NER metrics computed at entity level.",
            "dataset_or_benchmark": "Validation set: manually annotated tables (example: SciIE-style annotations; Li et al. 2023 provided 336 entities in 28 tables cited in paper)",
            "human_evaluation_details": "Gold annotations created by humans; exact annotator counts not specified.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "League shows robust Table NER performance with notable improvements when enhanced prompting is applied; specific F1s reported in Table 3 (1-shot prompts outperform 0-shot).",
            "comparison_to_human_generated": false,
            "comparison_results": "Performance reported relative to human-annotated gold labels; no direct human vs LLM qualitative comparison beyond metrics.",
            "limitations_noted": "Ambiguous abbreviations and inconsistent metric naming across papers cause extraction failures; some metric values missing due to absence in original text.",
            "uuid": "e7778.9",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CoT / ICL",
            "name_full": "Chain-of-Thought (CoT) and In-Context Learning (ICL) for Table Processing",
            "brief_description": "Prompting strategies used by League to improve table classification and extraction: few-shot in-context examples and Chain-of-Thought prompting to reduce token usage and improve extraction accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs (GPT-4o, O1-preview, Qwen2.5-series)",
            "model_size": "varied",
            "scientific_domain": "NLP / prompting methodology",
            "theory_type": "prompting/methodological framework",
            "evaluation_method_name": "In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting",
            "evaluation_method_description": "Use 0-shot and 1-shot prompts and few-shot CoT flows to guide LLMs to classify tables and extract entities in fewer API calls and with higher accuracy.",
            "evaluation_metric": "Indirect (improvements reported via higher P/R/F1 and lower token/API usage)",
            "metric_definition": "Performance improvements measured by classification/NER metrics and API/token efficiency.",
            "dataset_or_benchmark": "Used on extracted tables from collected papers and annotated validation sets described above",
            "human_evaluation_details": "1-shot examples provided manually as guidance; human experts designed prompts/examples.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "1-shot prompting with CoT yields higher F1 than 0-shot for table classification and NER (paper reports substantial relative improvements; see Table 3).",
            "comparison_to_human_generated": false,
            "comparison_results": "1-shot prompts improve model scores versus 0-shot; exact deltas shown in Table 3.",
            "limitations_noted": "Requires manual example selection for best performance; remains sensitive to prompt design and token costs.",
            "uuid": "e7778.10",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SCILEAD",
            "name_full": "SCILEAD dataset (hand-curated leaderboards)",
            "brief_description": "A hand-curated dataset introduced by Şahinuç et al. (2024) containing 27 leaderboards from 43 NLP papers used to evaluate LLMs on entity extraction and leaderboard construction tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / scientific information extraction",
            "theory_type": "benchmark dataset",
            "evaluation_method_name": "SCILEAD benchmark",
            "evaluation_method_description": "Hand-curated leaderboards and associated annotations used as gold references to evaluate LLM extraction and leaderboard generation capabilities.",
            "evaluation_metric": "Entity extraction metrics (P/R/F1), leaderboard-level quality comparisons",
            "metric_definition": "Standard extraction metrics on annotated leaderboards / leaderboards compared qualitatively to generated outputs.",
            "dataset_or_benchmark": "SCILEAD (27 leaderboards from 43 papers) as described in related-work citations",
            "human_evaluation_details": "Dataset is hand-curated by human annotators (details in Şahinuç et al. 2024, not restated in this paper).",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "SCILEAD cited as prior benchmark for evaluating LLM entity extraction; no new SCILEAD results produced in this paper.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "SCILEAD is hand-curated and relatively small; prior benchmarks provide limited dynamic coverage compared to League's goal.",
            "uuid": "e7778.11",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SciIE",
            "name_full": "SciIE-style entity definitions and annotations",
            "brief_description": "Entity taxonomy and annotation protocol (from Li et al., 2023 / SciIE) for datasets, methods, metrics, and settings; adopted by League to define extracted scientific terms.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "scientific information extraction",
            "theory_type": "annotation schema / benchmark",
            "evaluation_method_name": "SciIE entity schema",
            "evaluation_method_description": "Defines entities (methods, datasets, experiment settings, metrics) for structured extraction from tables and text; used as standard for Table NER evaluation.",
            "evaluation_metric": "Entity-level P/R/F1 when applied to annotated sets",
            "metric_definition": "Standard NER metrics applied to SciIE-style annotated entities.",
            "dataset_or_benchmark": "SciIE-style annotated tables (Li et al. 2023 reported 336 entities from 28 tables as cited)",
            "human_evaluation_details": "Annotations created by humans per SciIE protocol; exact annotator counts not specified in League paper.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "League follows SciIE definitions for entity extraction; SciIE dataset statistics (336 entities from 28 tables) cited as reference.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "SciIE annotations are limited in scope; real-world tables may contain varied and abbreviated metric names causing extraction difficulty.",
            "uuid": "e7778.12",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "TELIN",
            "name_full": "TELIN (Table Entity Linker) / TELIN-style extraction",
            "brief_description": "Prior work (Yang et al., 2022) for extracting table entities to create leaderboards; referenced as related work and baseline approach for table-entity extraction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "scientific information extraction / leaderboard mining",
            "theory_type": "extraction framework / dataset",
            "evaluation_method_name": "TELIN-style table entity extraction evaluation",
            "evaluation_method_description": "Extraction of table entities and linking to leaderboard entries; evaluated via entity-level precision/recall/F1 and leaderboards reconstruction accuracy.",
            "evaluation_metric": "Entity-level P/R/F1, completeness of reconstructed leaderboards",
            "metric_definition": "Standard extraction and reconstruction metrics (percentages, F1).",
            "dataset_or_benchmark": "TELIN / prior extracted table corpora as referenced in related work",
            "human_evaluation_details": "TELIN used annotated tables for evaluation; specifics in the original TELIN paper.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Referenced as prior baseline; no TELIN-specific experimental numbers produced in this paper.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Prior methods produce static snapshots and lack dynamic updates; League claims to go beyond these approaches.",
            "uuid": "e7778.13",
            "source_info": {
                "paper_title": "League: Leaderboard Generation on Demand",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "All data on the table: Novel dataset and benchmark for cross-modality scientific information extraction",
            "rating": 2,
            "sanitized_title": "all_data_on_the_table_novel_dataset_and_benchmark_for_crossmodality_scientific_information_extraction"
        },
        {
            "paper_title": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "rating": 2,
            "sanitized_title": "efficient_performance_tracking_leveraging_large_language_models_for_automated_construction_of_scientific_leaderboards"
        },
        {
            "paper_title": "Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction",
            "rating": 2,
            "sanitized_title": "identification_of_tasks_datasets_evaluation_metrics_and_numeric_scores_for_scientific_leaderboards_construction"
        },
        {
            "paper_title": "AxCell: Automatic extraction of results from machine learning papers",
            "rating": 2,
            "sanitized_title": "axcell_automatic_extraction_of_results_from_machine_learning_papers"
        },
        {
            "paper_title": "TELIN: Table entity LINker for extracting leaderboards from machine learning publications",
            "rating": 2,
            "sanitized_title": "telin_table_entity_linker_for_extracting_leaderboards_from_machine_learning_publications"
        },
        {
            "paper_title": "Legobench: Scientific leaderboard generation benchmark",
            "rating": 2,
            "sanitized_title": "legobench_scientific_leaderboard_generation_benchmark"
        }
    ],
    "cost": 0.02147375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Under review as a conference paper at ICLR 2026 LEAGUE: LEADERBOARD GENERATION ON DEMAND
4 Oct 2025</p>
<p>Jian Wu 
Yue Zhang yue.zhang@wias.org.cn 
Jiayu Zhang 
Dongyuan Li 
Renhe Jiang 
Linyi Yang 
Aoxiao Zhong 
Qingsong Wen </p>
<p>School of Engineering
Westlake University Hangzhou
China</p>
<p>Peking University
BeijingChina</p>
<p>University of Tokyo Tokyo
Japan</p>
<p>University College London London
United Kingdom</p>
<p>AI Research Institute
Squirrel AI Learning Shanghai
China</p>
<p>Under review as a conference paper at ICLR 2026 LEAGUE: LEADERBOARD GENERATION ON DEMAND
4 Oct 20251826E05BA0EA2174A27915D14FC08AA3arXiv:2502.18209v2[cs.CL]
Leaderboard gathers experimental results from various sources into unified rankings, giving researchers clear standards for measuring progress while facilitating fair comparisons.However, with thousands of academic papers updated daily, manually tracking each paper's methods, results, and experimental settings has become impractical, creating an urgent need for automated leaderboard generation.Although large language models offer promise in automating this process, challenges such as multi-document summarization, fair result extraction, and consistent experimental comparison remain underexplored.To address these challenges, we introduce Leaderboard Auto Generation (League), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence.League employs a systematic pipeline encompassing paper collection, result extraction and integration, leaderboard generation, and quality evaluation.Through extensive experiments across multiple research domains, we demonstrate that League produces leaderboards comparable to manual curation while significantly reducing human effort. 1* Equal contribution.† Corresponding author. 1 Code will be available upon publication. 2</p>
<p>INTRODUCTION</p>
<p>The explosive growth of scientific publications has created both unprecedented opportunities and significant challenges for researchers seeking to stay abreast of state-of-the-art methods (Bornmann et al., 2020;Wang et al., 2024;Şahinuç et al., 2024).Leaderboard platforms, such as NLP-progress 2 and Papers-With-Code 3 have become invaluable by offering comprehensive overviews of recent research developments, highlighting ongoing trends, and identifying future directions.However, the overwhelming growth of daily papers makes it increasingly difficult to build and update leaderboards automatically and promptly.Figure 1 illustrates two pressing issues: 1) a widening gap: LLMrelated paper submissions to arXiv have surged year-over-year (exceeding 55,000 in 2025), yet the growth trend of leaderboard submissions on Paper with Code (5670 in 2025) has not advanced accordingly.2) Even as new methods continuously emerge, leaderboards, such as the one for Multihop Question Answering on the HotpotQA (Yang et al., 2018) dataset, remain stagnant, with the latest method dating back to 2023.These observations highlight a serious challenge: the rapid accumulation of daily scientific publications often outpaces the capability of researchers to keep up with cutting-edge research and state-of-the-art methods, emphasizing the growing need for more efficient methods to generate the latest and useful leaderboards.Earlier studies have made initial attempts to tackle this challenge.Research on scientific information extraction (Hou et al., 2019;Kardas et al., 2020) has focused on identifying entities such as models, datasets, metrics, and results from individual NLP papers, which enables the creation of paperspecific leaderboards.However, such approaches are inherently difficult to maintain and update over time.More recently, Li et al. (2023), proposed Scientific NER to extract entities from both text and tables, while Şahinuç et al. (2024) introduced SCILEAD, a hand-curated dataset containing 27 leaderboards from 43 NLP papers for evaluating LLMs on entity extraction.Despite these advances, prior methods remain restricted to entity extraction, offering only preliminary building blocks for leaderboard creation and yielding static snapshots from a narrow set of papers.In contrast, our work shifts the focus to dynamic leaderboard construction, where the goal is not only to extract results but also to continuously track, align, and compare model performance on specific datasets or tasks under standardized evaluation settings.This enables researchers to observe evolving performance trends and conduct fair comparisons, thereby providing a more comprehensive and up-to-date view of progress in a research area.</p>
<p>Directly applying LLMs such as GPT-4 (Achiam et al., 2023), Qwen (Yang et al., 2024), and O1preview that have demonstrated exceptional performance in various NLP tasks, especially in the long-context scenario (Chen et al., 2023a;b;Wang et al., 2023b) to this task faces several key challenges.First, Limited Paper Coverage: It is challenging for humans to search for all papers on a certain scientific topic, due to the overwhelming number of constantly emerging publications.Second, Unfair Comparison: Current studies do not consider fair experiment settings when making comparisons.For example, in NLP research, key experimental components, model size, train dataset size, and hyperparameter selection vary significantly across publications, highlighting the need for automatic alignment.Finally, Low Timeliness: A leaderboard, which lacks regular updates and continuous maintenance, cannot provide researchers with sufficient useful information.</p>
<p>To address these issues, we introduce League, a novel framework for dynamically and automatically leaderboard generation.Figure 2 illustrates the framework of our method, which is organized into four stages: (1) Paper Collection and split: Initially, League automatically downloads all relevant LaTex and PDF files based on the given research topic from arXiv and top-tier conferences, including ACL, EMNLP, NeurIPS, ICML, and ICLR, and filters out papers published before a certain date and those unrelated to the topic, ensuring proper paper coverage and timeliness.</p>
<p>(2) We assess performance along two dimensions: (1) Topic-related Quality: whether each quintuple in League-generated leaderboards relates to the given topic; (2) Content Quality: evaluated via LLM-  (Kardas et al., 2020) PwC TELIN (Yang et al., 2022) PwC ORKG (KABENAMUALU et al., 2023) PwC LEGO (Singh et al., 2024) PwC SciLead ( Şahinuç et al., 2024) NLP Although the manually created leaderboard achieves higher Content Quality, it is much more timeconsuming than League, highlighting League's superior efficiency.With fewer items, League gets even higher performance, slightly lower than human performance.These results highlight the effectiveness of League, providing a reliable proxy for human judgment across varying leaderboard items.Furthermore, the Pearson Correlation Coefficient values indicate a moderate positive correlation between the human-assigned and LLM-assigned scores.To the best of our knowledge, we are the first to explore the potential of LLMs for automatic leaderboard generation, proposing evaluation criteria that align with human preferences and offering valuable reference for future related research.</p>
<p>RELATED WORK</p>
<p>LLM for Scientific Research.Several studies have explored using LLMs to improve work efficiency in scientific research (Xie et al., 2025).Baek et al. (2024) and Yang et al. (2023) propose a multi-agent-based scientific idea generation method to boost AI-related research.To evaluate the quality of LLM-generated ideas, Si et al. (2024) introduces a comprehensive human evaluation metric.Wang et al. (2023a) proposes SciMON, a method that uses LLMs for retrieving the scientific literature.Wang et al. (2024) proposes an AutoSurvey to automatically generate scientific surveys based on the given research topic.The AI Scientist (Lu et al., 2024) introduces a fully automated and prompt-driven research pipeline.To make LLM-generated ideas more diverse and practical, Weng et al. (2024) proposes an iterative self-rewarding framework that allows the LLM to continuously refine its ideas, improving both diversity and practicality in research proposal generation.However, no previous research focused on leaderboard generation for researchers to search, organize, and compare the state-of-the-art methods rapidly and fairly based on a certain research topic.</p>
<p>Scientific Information Extraction.Table 1 illustrates the differences between the previous experiment results extraction work and League.Earlier works on scientific information extraction mainly focused on extracting entities such as Task, Dataset, and Model (TDM triples) from sources like NLP-progress or Papers-with-Code (Hou et al., 2019;Kardas et al., 2020;Singh et al., 2024).Others (Yang et al., 2022;KABENAMUALU et al., 2023) extended this approach by leveraging predefined TDM taxonomies.However, these methods face three key limitations</p>
<p>STAGE I PAPER COLLECTION AND SPLIT</p>
<p>Utilizing the off-the-shelf tools 4 , League first searches and retrieves a set of papers P init = {P 1 , P 2 , ..., P N } from arXiv and top-tier conferences and downloads LaTeX or PDF files related to a specific research topic T .Then, we specify a certain date and filter out all papers published before the date.The filtering stage is important for ensuring that the generated leaderboards are grounded in the most relevant and recent research.Moreover, since the search tool just identifies only the keywords in the paper title and abstract, which can lead to a significant amount of noisy data, we also introduce a retrieval model to filter out papers that are irrelevant to the given topic and retrieve topic-related papers.The set of filtered papers P filtered = {Retrieval{P 1 , P 2 , ..., P U }} is used to generate the leaderboards, ensuring comprehensive coverage of the topic and logical struc-ture.Due to the extensive number of relevant papers retrieved and filtered during this stage, the total input length of P filtered often exceeds the maximum input length of LLMs.Since most of the LaTeX content is unproductive for generating leaderboards, we split the LaTeX code into several sections based on the structure of each paper.Most tables, table-related descriptions, experiment results and experiment settings are located in the "Experiment" section, which contains the key information to generate leaderboards.Thus, we select all "Experiment" sections as well as all tables {Table 1 , Table 2 , ..., Table U } and all table-related descriptions {D 1 , D 2 , ...D U }, extracted from all papers, as input for the next stage.</p>
<p>STAGE II TABLE EXTRACTION AND CLASSIFICATION</p>
<p>Typically, a scientific paper, such as those in the natural language processing domain, contains several types of tables, including "Main Results", "Ablation Study", and "Others".The "Main Results" tables are the most important tables in the paper, which illustrate the novelty, contributions, and effectiveness of the proposed methods or models by comparing the experiment results of the proposed method with other baselines.We utilize these tables for leaderboard generation.The "Ablation Study" tables examine the effect of damaging or removing certain components in a controlled setting to investigate all possible outcomes of system failure.The "Others" tables are the tables that illustrate the supplementary information of the experiments.For example, some tables illustrate the dataset statistics of the benchmark used in the experiments, while other tables list the results of the "Case Study" and "Error Analysis".To address this, we propose a framework that uses the In-Context Learning method (Dong et al., 2022) to manually select one table from each of the three different types.The framework then prompts LLMs to classify the table types and only keeps the "Main Experiments" tables and their descriptions as the final input.The i th table types can be described as:
LLM(Table i , D i ; Prompt) → Table type.
In practice, the most intrinsic approach is to divide Stage 2 into the following sequential steps:</p>
<p>(1) Extract all tables and their associated captions from the LaTeX code.</p>
<p>(2) Classify the extracted tables according to predefined table types.</p>
<p>(3) Extract metrics, performance values, and experimental settings related to the proposed model from tables categorized as "Main Results".However, each of these three steps necessitates the use of LLM APIs, and repeated reference to certain table contents further exacerbates the substantial waste of tokens.To address this issue, we follow the few-shot Chain of Thought (CoT) prompting process, enabling it to classify and extract information from identified "Main Results" tables in a single dialogue round.Specifically, in the requested JSON output, we additionally set the key points as follows: "number of tables (Int)", "classification of tables (Dict)", and "selected table's index (Int)".</p>
<p>STAGE III TABLE UNPACKING AND INTEGRATION</p>
<p>Following the table extraction and classification phase, each table Table i is sent into the LLM to extract the core information.To build a useful and high-quality leaderboard, we define four types of scientific terms: Datasets, Metrics, Experiment Results, and Experiment Settings.We follow the definition of scientific entities proposed by SciIE (Li et al., 2023).For datasets, we use LLMs to count the frequency in all filtered papers P filtered of each dataset under a certain research topic and retain the top-K (K=5) datasets with the highest frequency of occurrence in scientific papers, ensuring the generated leaderboard contains enough methods.For the rest of the three scientific terms, we utilize LLMs to extract from given Table i with a related table description D i .After scientific term extraction, we recombine them into a quintuple, including the paper title as the unique identification ID.Each paper can produce one quintuple, and finally, we get a raw leaderboard with K * M quintuples from M filtered papers and K datasets.The raw leaderboard is then reranked on the basis of the experiment results.</p>
<p>STAGE IV LEADERBOARD GENERATION AND EVALUATION</p>
<p>Following the table unpacking and integration phase, we get K * M quintuples, and all quintuples are formatted into K leaderboards.Each leaderboard is individually refined by a third-party LLM such as GPT-4 to enhance readability, eliminate redundancies, and ensure completeness.After we obtain K leaderboards, the final stage involves a quality evaluation based on our pre-defined four criteria, which is shown in Appendix Table 9.Each leaderboard is assigned three scores based on "Cov-erage", "Latest" and "Structure".Since a research topic may contain several datasets, the "Multi-Aspect" is the average quality score used to evaluate the LLM-generated leaderboards for each dataset.The best leaderboard is chosen from N candidates.LLMs critically examine the leaderboards in several aspects.The final output of League is L best = Evaluate(L ca1 , L ca2 , ..., L caN ).The methodology outlined here, from paper collection to leaderboard evaluation, ensures that League effectively addresses the complexities of leaderboard generation in the AI domain using advanced LLMs.We provide Pseudo-code for easily understanding, which is shown in Appendix Algorithm 1.  (1) Topic-related Quality: The aforementioned arXiv crawler employs regular expression matching in the abstract section to identify papers related to specific topics.While this method is efficient, it is relatively rudimentary and cannot guarantee that all retrieved papers meet our requirements.The quality of these papers not only directly affects the final leaderboard, but low-quality candidate papers can also significantly prolong the time required for construction.Therefore, it is essential to evaluate the quality of the retrieved articles.We evaluate the quality of content from the following two aspects.(i) Recall: It measures whether all items in the generated leaderboard are related to the given research topic.(ii) Precision: It identifies irrelevant items, ensuring that the items in the generated leaderboards are pertinent and directly support the given research topic.</p>
<p>(2) Leaderboard Content Quality: The evaluation metric of leaderboard Content Quality includes four aspects.Each aspect is judged by LLMs according to a 5-point, calibrated by human experts.</p>
<p>The evaluation criteria are listed in Appendix Table 9. (3) Leaderboard Construction Speed: Manually building a leaderboard is a time-consuming and laborious task.This process can be divided into the following main components: T r (search for papers on a specific topic), T b (browse all retrieved articles and develop several highly frequent data), T f (filter candidate papers based on the selected data), T e (read and extract information), and T c (the integration and construction time).The total time consumption is calculated as:
T manual = T r + T b + T f + T e + T c .(1)
Given L denotes the length of the leaderboard, N retrieved is the number of retrieved articles, N filtered is the number of articles retained, and P is the proportion of valid articles with P = Nfiltered Nretrieved .We find that T b and T f are strongly correlated with leaderboard length L and the Topic-related Quality:
{T b , T f } ∝ L P = L • N retrieved N filtered . (2)
Although T r is relatively fixed, T e and T c usually only have a positive correlation with L.</p>
<p>For League, we directly account for all the invocation time of the LLMs' API calls.Compared to manual work, which often takes several days, League reduces the total time cost at the minute level.This is largely attributed to the task decomposition conducted in this paper, the division of labor and scheduling within the framework, and the superior performance of the LLMs.</p>
<p>BASELINES</p>
<p>We employ proprietary and open-source LLMs in our experiments and set the sampling temperature to 0.7 for proprietary models.For proprietary models, we adopt GPT-4o (Achiam et al., 2023), and the O1-preview.For open-source LLMs, we adopt Qwen2.5-7B and Qwen2.5-14B(Yang et al., 2024).We provide a detailed illustration of our prompts for different stages in Appendix C.</p>
<p>EXPERIMENT RESULTS</p>
<p>PERFORMANCE COMPARISON (RQ-1)</p>
<p>Topic-related Quality Evaluation:  ) respectively, which shows that League could guarantee the timelines and quality of the generated leaderboards.The papers crawled from arXiv could provide the latest research trend on the given topic, while papers crawled from top conferences could help researchers build leaderboards with higher quality, compared with the Topic-related Quality and Content Quality in Table 2 and Appendix Table 5. League consistently achieves high scores across all evaluation metrics, particularly in terms of Coverage and Latest, indicating its ability to include a wide range of relevant and recent papers.For example, at a leaderboard length of 20 items, League achieves a Coverage score of 4.12 and a Latest score of 3.96, approaching human performance (4.72 and 4.68, respectively).While manual leaderboards score slightly higher in Content Quality, League significantly reduces the time required for leaderboard generation from 1128s to 117.45s, demonstrating its efficiency.We also list a 5 item leaderboard generated by League, shown in Figure 3, which offers significant advantages over the outdated official HotpotQA leaderboard (Figure 1).Our method shows three advantages: 1) Latest, all papers are published after 2024; 2) High Quality, all papers are collected from top-tier conferences, including ACL, EMNLP, ICLR, and ICML; 3) More Information, all items on the leader board contain experiment details, including models and training strategies.Iteration Evaluation: To ensure the high-quality of League-generated leaderboards, we iterate the process to evaluate the performance change during the whole iteration.The left part of Appendix Figure 4 presents the effect of different iteration counts on the performance of League.The results show that increasing the number of iterations from 1 to 5 provides a significant improvement in Structure quality and Coverage quality scores.The Latest score remains at a relatively high level, which is because in stage 1 of the League, the old papers are filtered out.To sum up, our experiments demonstrate that League is highly effective in generating high-quality, up-to-date leaderboards across various research topics.The framework's ability to dynamically update leaderboards and extract detailed experiment settings ensures a fair comparison between state-of-the-art baselines.While League's Content Quality scores are slightly lower than those of manually created leaderboards, its efficiency and scalability make it a valuable tool for researchers in rapidly evolving fields like AI and NLP.</p>
<p>EFFICIENCY ANALYSIS (RQ-2)</p>
<p>Construction Speed: League dramatically reduces the time required to generate leaderboards compared to manual methods.For instance, generating a 20-item leaderboard with League takes approximately 2 minutes, while manual construction takes more than 18 minutes.This speed advantage makes League a practical tool for researchers who need up-to-date leaderboards in rapidly evolving fields.The high speed of League shows that it can generate high-quality leaderboards timely.</p>
<p>META EVALUATION (RQ-3)</p>
<p>To verify the consistency between our proposed LLM evaluation strategy and human evaluation, we conduct a correlation evaluation involving human experts and our automated evaluation method.Human experts judge pairs of generated leaderboards to determine which one is superior.We compare  the judgments made by our method against those made by human experts.Specifically, we provide experts with the same scoring criteria as used in our evaluation as a reference.Experts rank the 20 League-generated leaderboards and compare these rankings with those generated by LLM using the Pearson Correlation Coefficient to measure the consistency between human and LLM evaluations.</p>
<p>The results of this meta-evaluation are presented in the right part of Appendix Figure 4.The table shows the Pearson Correlation Coefficient values, indicating a strong positive correlation between the quality scores provided by LLM and those given by human experts, with the O1-preview achieving the highest correlation at 0.76.These results suggest that our evaluation aligns well with human preferences, providing a reliable proxy for human evaluation.</p>
<p>INTERMEDIATE EVALUATION</p>
<p>To validate the effectiveness of League's intermediate stages, we conducted experiments focusing on table classification and table-level entity recognition, as summarized in Table 3. Table Classification.We divided tables into three categories: (i) main results, (ii) ablation studies, and (iii) others (e.g., dataset statistics or case studies).The results demonstrate that League is highly effective at isolating the correct "main results" tables required for leaderboard construction.Both GPT-4o and O1-preview provide strong performance (over 80% F1 scores respectively with 1-shot prompt).This suggests that even minimal supervision helps the model better distinguish between relevant and auxiliary tables, thereby reducing noise in the subsequent leaderboard generation stage.Table NER.</p>
<p>Following Li et al. (2023), we cast entity extraction as a classification problem, requiring the model to identify four key scientific entities: methods, datasets, experimental settings, and metrics.On a manually annotated validation set, League shows robust performance across all categories, with notable improvements when enhanced prompting strategies are applied.These results confirm that the system can reliably extract structured information from experimental tables, which is critical for generating comparable high-quality leaderboards.Details of these tasks are shown in Appendix D.</p>
<p>ABLATION STUDY (RQ-4)</p>
<p>To understand the contribution of each component in League, we conduct an ablation study by removing key components of League as follows: (1) League w/o</p>
<p>ETHICS STATEMENT</p>
<p>All authors affirm their adherence to the ICLR Code of Ethics.We have carefully considered the ethical implications of our research, particularly concerning the safe and responsible deployment of Large Language Model (LLM)s.Our work directly addresses the critical need to avoid harm by mitigating risks such as dangerous diagnostic medical recommendations, financial losses, and privacy breaches, which can arise from the unconstrained operation of LLMs.We believe our work contributes positively to human well-being by enhancing the safety and trustworthiness of advanced AI systems.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>To ensure the reproducibility of our work, we have made significant efforts to document our methodology thoroughly.The full description and algorithm details of the League framework are described in Section 3 and algorithm 1.Our source code is provided in supplementary materials.We are committed to fostering open science and facilitating the replication of our results.</p>
<p>A LIMITATIONS</p>
<p>One limitation of League is its reliance on the quality of the retrieved papers.While our Topicrelated Quality metrics are strong, there is still room for improvement in ensuring that all relevant papers are included.Future work could explore more sophisticated retrieval models to further enhance the coverage of the generated leaderboards.Another limitation is that a specific dataset may contain several evaluation metrics, and different papers may use different metrics to evaluate proposed models' performance, bringing challenges for leaderboard generation and baseline comparison.</p>
<p>B THE USE OF LARGE LANGUAGE MODELS</p>
<p>We employed LLMs for grammar checking and polishing the English expression throughout this manuscript.It is important to note that while our research focuses on leveraging LLMs for automatic leaderboard generation, the LLMs studied in this work are the subject of our research rather than tools for research ideation or scientific writing.All experimental design, analysis, and scientific conclusions were developed independently by the authors.</p>
<p>Algorithm 1 Leaderboard Automatic Generation.Count frequency of all datasets and retain top-K datasets from U papers.</p>
<p>7:</p>
<p>for each dataset j = 1 to K do</p>
<p>8:</p>
<p>Split Pi, Extract U Tables {Table1, ..., 12:</p>
<p>end for # Stage 3: Leaderboard Generation</p>
<p>13:</p>
<p>Recombine all quintuples, refine and rank the quintuples by performance scores.</p>
<p>14:</p>
<p>Output   • In the "selected table's core results", other models' results are of no concern and should be omitted.</p>
<p>• The table's header metrics should be the same as the evaluation metrics chosen.</p>
<p>• The number of items in the "classification of tables" dict should be equal to the "number of tables" int value.These two items help you to identify the main result tables better.• All three items about the settings in the JSON output should correspond to the proposed method's best performance in the selected table.• Sometimes in the selected table, the proposed method's performance may not be unique (e.g., different hyperparameters or training strategies); you need to choose the best one, which usually appears in the last row of the table.• If there are multiple tables that meet the requirements (both being the main result table and based on the specified dataset), choose the one with richer information.Workflow example: First, I provide you with an article:</p>
<article>[EXAMPLE ARTICLE]</article>

<p>Then I specify the dataset as [EXAMPLE DATASET], and you should output: <instruction> You are an expert in constructing the Artificial Intelligence leaderboard.Please refer to the content I provide you to answer the user's questions.The contents I provide you are a number of structured summaries extracted from computer/artificial intelligence papers.You need to build a markdown format leaderboard (showcase the performance of the models on the same dataset, each line representing a specific model) based on the titles, experimental settings, and evaluation metrics of these articles.Please output your reply in the Markdown format.
<format>[EXAMPLE RESPONSE]</format> </instruction>
Here, I list a complete example of the question and the answer to help you understand your task.For example, I provide you a list of JSON files containing the extracted content of the articles:
[JSON LIST]
The expected leaderboard that you generate should be:
[EXAMPLE LEADERBOARD]</p>
<p>Pay attention</p>
<p>• The leaderboard should be in the Markdown format and reflect all the articles provided!• The leaderboard in the dictionary format is forbidden!• In the above case, selecting Pre and Rec as the metrics in the final leaderboard is not appropriate because in most articles the corresponding performance values are absent.Here, the target list of extracted content of the articles is as follows:</p>
<p>[TARGET JSON LIST] Warning:</p>
<p>• I need a well-organized markdown-format leaderboard containing all the articles' information.The leaderboard's max serial number in the "No." column should equal to the number of articles provided.• When selecting metrics, you need to consider their text descriptions.The same metric may have multiple different abbreviations.In the final table, there must not be any duplicate metrics (it is unacceptable to have duplicates where different abbreviations represent the same meaning).• Large-scale omissions are not allowed!For each model, only a small portion of the results are missing under the selected metrics.The vast majority of the metrics have corresponding values.The abbreviations for the same metric may be different, but you need to avoid being misled by the abbreviations.• Use approximate intersections to select metrics from the given articles, while avoiding a large amount of data waste.Allow some models to have a certain degree of data missing under the selected metrics.• The content in the "Experimental Setting" column should be concise and non-descriptive, just a few words.• When different articles use different units for the same metric, please note that you need to convert them when integrating them so that the units in the final leaderboard are consistent.For example, 50% is equal to 0.5."50" and "0.5" should not be presented in the same column of a leaderboard.• Check each column corresponding to the selected metrics in the final leaderboard.If more than 60% of the values in that column are missing or represented by placeholders, the metric should be discarded.</p>
<p></instruction></p>
<p>C EXAMPLE PROMPTS</p>
<p>The prompts of instructing LLMs in different stages of League are illustrated in Box 6, and 7.     (Li et al., 2023) and annotated 336 entities from 28 tables.The entities contain 172 methods, 31 datasets, 49 settings, and 84 metrics.As illustrated in Table 3, we using different prompt strategies: with 0-shot and 1-shot.For prompt with 0-shot, we just provide a brief definition of the sub-categories.For 1-shot, we give an example table for each category besides the definition.</p>
<p>Leaderboard Construction For proprietary models, we employ official APIs to interact with exclusive LLMs, and the prompts are well-defined.we set temperature = 0.3 and other parameters as Table 9: Leaderboard Quality Criteria.</p>
<p>Criteria Scores</p>
<p>Coverage The ratio of the number of papers used for leaderboard generation to the total number of papers searched.</p>
<p>(Pused/Ptotal) * 5</p>
<p>Latest</p>
<p>The ratio of the number of papers published after the certain date to the total number of papers searched.</p>
<p>(Pnew/Ptotal) * 5</p>
<p>Structure Score 1: The structure of the leaderboard lacks logic, making it difficult to understand and navigate.The table header and each row are not clearly organized and connected.Score 2: The structure of the leaderboard have some contents arranged in a disordered or unreasonable manner.However, the overall structure is reasonable and coherent.Score 3: The survey is generally comprehensive in coverage but still misses a few key points that are not fully discussed.Score 4: The structure of the leaderboard is generally reasonably logical, with most header items arranged orderly, though some header items may be repeated or redundant.Score 5: The structure of the leaderboard has good logical consistency, with each line strictly related to the header items and the previous line.But it can be optimized in terms of easy understanding.</p>
<p>Multi-Aspect</p>
<p>The evaluation metric for multi-leaderboard.Specifically, a research topic T may have N different datasets, and thus we can get N leaderboards, the score of the Multi-Aspect is computed based on the average of all the N scores.</p>
<p>(NCoverage + NLatest + NStructure)/(3 * N ).First and foremost, when viewed holistically, both leaderboards with 20 entries, whether utilizing qwen2.5-14Bor GPT-4o as the construction model, exhibit a notably high level of completeness.Upon specific analysis of the missing information, in Leaderboard 1, League failed to successfully extract the HD value from "Self-Paced Sample Selection for Barely-Supervised Medical Image Segmentation" (No. 11) because the metric was referred to as 95HD in the original text.Although our design accounts for such situations, required to extract metrics from both text and tables to avoid confusion caused by abbreviations.This design has successfully resolved most of the issues arising from abbreviations, but such errors still occur with a small probability.The absence of metrics in entries No. 13 and No. 20 is acceptable because the original text indeed lacks these metrics.The situation in Leaderboard 2 is similar; the only two missing items (No. 4 and No. 14) are also due to the absence of corresponding results in the original texts.</p>
<p>The higher missing rate in the 5-row leaderboard compared to the 20-row leaderboard for the LIVE dataset can be attributed to the following reasons: When only 5 papers are included, League extracts a larger number of metrics, including RMSE, mIoU, and mAcc.The missing values for these metrics are tolerable in a 5-row leaderboard.However, when expanding to a 20-row leaderboard, the excessive number of missing values forces League to discard these metrics to ensure that the leaderboard conveys meaningful information.</p>
<p>Secondly, regarding the experimental settings, we observe that in Leaderboard 1, the information on "model &amp; size", "hyperparameters", and "training strategy" is both accurate and comprehensive.Notably, there is a consistent thread throughout the hyperparameters: the portion of labeled data.In contrast, Leaderboard 2 discards the hyperparameter information compared to Leaderboard 3.This is because we require League to extract hyperparameter information in a way that not only maintains completeness but also focuses on the intrinsic connections between different items.If the deviation is too large (i.e., if it cannot provide users with a concise and effective summary), the information should be discarded.Therefore, when the number of input papers for League increases from 5 to 20, the hyperparameter settings in the topic of image quality assessment do not have a clear and unified theme and thus are ultimately ignored.</p>
<p>F WORKFLOW ILLUSTRATION</p>
<p>Figure 8 illustrates the transformation of the main experimental table of the LA dataset in the SDCL (Song &amp; Wang, 2024) through the entire workflow of League.Initially, the table information is presented in the form of visual features within a PDF file.Subsequently, through the processes of crawling and LaTeX integration, the table is extracted and classified into an independent LaTeX format.Further, the table information is structured by the COT table extraction.Finally, after evaluation and iteration, it becomes an entry in the final leaderboard.</p>
<p>G COST ANALYSIS</p>
<p>We calculate the average number of input &amp; output tokens required to generate a 20-entry leaderboard, along with the cost analysis using different LLMs, as shown in Table 11.The computational cost of all models remains within 14$, indicating that League is also economically efficient.Overall, the League framework consumes more input tokens, while the output tokens represent only a small proportion.OpenAI prices output tokens significantly higher than input tokens, often reaching 4-5 times the cost of input tokens.However, considering the disparity in token numbers, the overall cost remains acceptable.</p>
<p>Figure 1 :
1
Figure 1: Left: Growth trend of paper and leaderboard submissions on LLMs from 2022 to 2025-09.The leaderboard statistics are collected from Paper with Code.Right: Leaderboard of Multi-hop QA, the latest method is still stuck in 2023.Earlier studies have made initial attempts to tackle this challenge.Research on scientific information extraction(Hou et al., 2019;Kardas et al., 2020) has focused on identifying entities such as models, datasets, metrics, and results from individual NLP papers, which enables the creation of paperspecific leaderboards.However, such approaches are inherently difficult to maintain and update over time.More recently,Li et al. (2023), proposed Scientific NER to extract entities from both text and tables, whileŞahinuç et al. (2024)  introduced SCILEAD, a hand-curated dataset containing 27 leaderboards from 43 NLP papers for evaluating LLMs on entity extraction.Despite these advances, prior methods remain restricted to entity extraction, offering only preliminary building blocks for leaderboard creation and yielding static snapshots from a narrow set of papers.In contrast, our work shifts the focus to dynamic leaderboard construction, where the goal is not only to extract results but also to continuously track, align, and compare model performance on specific datasets or tasks under standardized evaluation settings.This enables researchers to observe evolving performance trends and conduct fair comparisons, thereby providing a more comprehensive and up-to-date view of progress in a research area.</p>
<p>(i) Coverage: Each paper represented on the League-generated leaderboards encapsulates all aspects of the topic.(ii) Latest: Test whether all papers represented on the League-generated leaderboards are the latest.(iii) Structure: Evaluate the logical organization and determine whether the League leaderboards have missing items.(iv) Multiaspect: Average score of the previous three criteria for League-generated leaderboards.</p>
<p>Figure 3 :
3
Figure 3: The example leaderboard generated by League.Comparing with the Leaderboard of Multi-hop QA method in the right part of Figure 1, our method could help summarize the experiment results from the top conference papers with experiment settings for fair comparison.</p>
<p>1:</p>
<p>Input: Scientific topic T , open-access platform arXiv A 2: Output: Final refined and evaluated leaderboard L # Stage 1: Paper Collection and Document Split 3: Crawl topic T related N publications Pinit = {P1, ...P N } ← Retrieve(T, A) 4: Filter out topic-unrelated and old papers, Pfiltered = {P1, ...P M } ← Retrieve(Pinit, date, topic) # Stage 2: Table Extraction and Classification 5: for each Leaderboard iteration i = 1 to Iters do 6:</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: A leaderboard (5 lines) of image quality assessment on the LIVE dataset, using GPT4-o for both table extraction and leaderboard construction &amp; refinement.</p>
<p>Table Extraction and Classification: We use LLMs to extract and classify experiment tables based on accompanying table descriptions.(3) Table Unpacking and Integration: League extracts the datasets, metrics,</p>
<p>experiment settings, and experiment results from the tables in the form of a quintuple, including paper titles.Experiment setting extraction is crucial for enabling fair comparisons across different baselines (including model size, data size, etc).(4)Leaderboard Generation and Evaluation: The extracted quintuples are recombined, refined, and re-ranked to form candidate leaderboards.</p>
<p>Table 1 :
1
Comparison between related work and our approach.Data Source: Source of leaderboards.NProg.: NLP-progress, PwC: paperswithcode.Experiment Settings: whether the experiment settings are extracted as part of leaderboards.Multi Document: whether the leaderboards are constructed from multiple papers.Dynamic: whether the generated leaderboards can be updated dynamically.
Related WorkData Source Exp. Settings Multi Doc. Dynamic TimelinessTDMS(Hou et al., 2019)N Prog.Axcell</p>
<p>Judge on four aspects including Coverage, Structure, Latest, and Multiaspect.Human experts also evaluate the leaderboards, with Pearson Correlation computed between human and LLM scores.
papersLeague (Ours)arXivas-
Experiments across different leaderboard lengths(5, 10, 15, and 20 items)show that League consistently achieves high Topic-related and Content Quality scores, approaching human performance but about 5-10 times the speed of human annotation on 20-item leaderboard construction.With 20 items, a League-generated Leaderboard represents 20 baselines for researchers, achieving 67.58% recall and 70.33% precision scores in Topic-related Quality.In Content Quality with 20 items, League achieves 4.12 coverage, 3.96 latest, 4.16 structure, and 4.08 multiaspect scores, approaching human performance (4.72 coverage, 4.68 latest, 4.34 structure, and 4.58 multiaspect scores).</p>
<p>Figure2: The League framework for leaderboard automatic generation.In Stage 1, we automatically crawl scientific papers from arXiv.In Stage 2, we retrieve, extract, and classify tables from the LaTeX code.In Stage 3, we select the main results tables and extract datasets, metrics, results, and experiment settings from the main results table.In Stage 4, we generate Leaderboards from the selected results and evaluate the quality.
Retrieved TablesTable Related DescriptionTable ClassificationMain ResultsStage 4:I ter ationAblation Study OthersLeaderboard Generation &amp; EvaluationsDatasetsMetrics1Coverage2LatestRecombinationTable Upacking3StructureRefinement4MultiaspectAuto EvaluationLeaderboardSettingsResultsMain Result TablesBest Leaderboardcomparisons. As a result, prior efforts can at best provide static snapshots of research progress, butfail to maintain up-to-date, comparable leaderboards.In contrast, our League framework goes beyond mere extraction: it automatically crawls arXivand top conference proceedings, identifies and unpacks main experimental tables, extracts both re-sults and experiment settings, and then integrates these quintuples to produce complete, up-to-dateleaderboards. By combining dynamic paper collection, table classification, settings-aware resultintegration, and LLM-based refinement and evaluation, League generates leaderboards end-to-endrather than only extracting isolated result entities. This shift enables timely, reproducible, and faircomparisons across methods and makes League suitable for rapidly evolving research areas wherestatic extraction methods fall short.3 METHODFigure 2 depicts the proposed League, which consists of four stages: (1) Paper Collection and Split,(2) Table Extraction and Classification, (3) Table Unpacking and Integration, and (4) LeaderboardGeneration and Evaluation. Each stage is meticulously designed to address specific challenges as-sociated with leaderboard generation, thereby enhancing the efficiency and quality of the resultingleaderboards. The whole process is iterated several times (e.g., five times) to generate a high-qualityleaderboard.</p>
<p>Table 2 :
2
Results of leaderboard quality generated by League in the first iteration (The results of the first iteration could help reflect the effectiveness and efficiency of our framework).Items: The number of items in the leaderboard.For example, a 5-item leaderboard contains 5 baselines.Topicrelated Quality: The precision and recall of each paper in relation to its relevance to the topic.Speed: The average time required to generate a single leaderboard.Content Quality: The evaluation results of the leaderboard content.The best results of Leagure are highlighted in bold, the fastest results are underlined.
ItemsTopic-related Quality Recall↑ Precision↑ModelSpeed /sCoverage↑Content Quality Latest↑ Structure↑ Multiaspect↑576.57 ±11.65 79.43 ±8.86Qwen2.5-7B Qwen2.5-14B GPT-4o O1-preview Human Writing131.43 129.51 49.64 79.67 3553.60 ±0.48 4.23 ±0.38 4.52 ±0.42 4.63 ±0.48 4.71 ±0.71 4.40 ±0.33 3.46 ±0.49 3.18 ±0.32 4.14 ±0.31 3.68 ±0.29 4.70 ±0.32 4.32 ±0.38 4.89 4.83 4.913.41 4.02 4.41 4.58 4.881075.19 ±9.81 80.05 ±6.76Qwen2.5-7B Qwen2.5-14B GPT-4o O1-preview Human Writing156.41 163.54 88.96 98.44 6123.22 ±0.48 3.91 ±0.48 4.68 ±0.39 4.59 ±0.33 4.45 ±0.41 3.41 ±0.49 4.11 ±0.39 3.55 ±0.49 3.41 ±0.39 4.40 ±0.48 4.46 ±0.71 4.31 ±0.33 4.81 4.72 4.653.57 4.61 4.56 4.39 4.721571.34 ±8.39 74.58 ±7.35Qwen2.5-7B Qwen2.5-14B GPT-4o O1-preview Human Writing183.45 195.63 105.61 109.33 8393.11 ±0.28 3.68 ±0.28 4.47 ±0.22 4.16 ±0.27 4.32 ±0.24 3.23 ±0.26 3.15 ±0.27 3.32 ±0.19 3.18 ±0.24 4.21 ±0.48 4.06 ±0.21 4.28 ±0.31 4.71 4.65 4.443.16 3.39 4.28 4.18 4.602067.58 ±9.12 70.33 ±6.89Qwen2.5-7B Qwen2.5-14B GPT-4o O1-preview Human Writing196.33 208.64 120.52 117.45 11283.03 ±0.25 3.49 ±0.34 4.28 ±0.28 3.92 ±0.22 4.21 ±0.25 3.11 ±0.31 2.98 ±0.25 3.17 ±0.26 3.03 ±0.28 4.12 ±0.38 3.96 ±0.25 4.16 ±0.29 4.72 4.68 4.343.16 3.39 4.13 4.08 4.584 EXPERIMENTSof Leagueuseful?4.1 EXPERIMENTAL SETUP4.1.1 EVALUATION METRICSWe use two metrics to evaluate the quality (topic-related and leaderboard content) and the speed ofleaderboard generation, respectively.
We designed experiments for League, with the aim of answering four questions: RQ-1: Can League address the paper coverage issue and generate fair leaderboards by incorporating the latest baselines?RQ-2: Can League reduce time consumption compared to human?RQ-3: Is the evaluation consistent between League and human experts?RQ-4: Is each proposed component</p>
<p>Table 3 :
3
Left: Table Classification result on the extracted tables using different LLMs.Right: Table NER result on the manually annotated table entities.
MethodsPrompt P (%)R (%)F1 (%)MethodsPrompt P (%)R (%)F1 (%)GPT-4o0-shot 1-shot80.96 87.45 (+6.49) 86.44 (+3.95) 85.10 (+5.07) 82.49 80.03GPT-4o0-shot 1-shot86.51 89.17 (+2.66) 90.26 (+2.31) 88.76 (+3.46) 87.95 85.30O1-preview0-shot 1-shot81.16 85.19 (+4.03) 83.62 (+3.39) 82.78 (+4.23) 80.23 78.55O1-preview0-shot 1-shot84.28 88.55 (+4.27) 90.02 (+4.58) 89.82 (+6.06) 85.44 83.76</p>
<p>Table2illustrates the Topic-related Quality League achieves a recall of 67.58% and a precision of 70.33% with 20 items, indicating that it successfully retrieves a large proportion of relevant papers while maintaining a low rate of irrelevant ones.The high precision and recall scores show that League can help solve the paper coverage problem.This performance is crucial to ensure that the generated leaderboards are both comprehensive and accurate.</p>
<p>Fair Comparison: To ensure a fair comparison, League extracts all experiment settings from crawled papers as part of the League-generated leaderboards.We provide a detailed case study of League-generated leaderboards with experiment settings in Appendix E.Content Quality Evaluation:Table 2 and Table 5 in appendix present the results of the quality of the leaderboard generated by League.Items in Table 2 are crawled from arXiv while items in</p>
<p>Table 5
5
are from the top-tier conferences (including ACL, EMNLP, NeurIPS, ICML, ICLR, etc.</p>
<p>Table 4 :
4
Ablation study for League with components removed.We use the GPT-4o as the backbone.
MethodsItems Speed /sContent Quality Coverage↑ Latest↑ Structure↑ Multiaspect↑League w/o Table Classification42.354.434.523.954.30League w/o Refinement543.584.414.464.054.31League49.644.524.704.324.51League w/o Table Classification81.374.314.364.014.33League w/o Refinement1080.524.244.173.884.10League88.964.684.594.454.56League w/o Table Classification93.284.134.083.613.94League w/o Refinement1591.324.194.133.724.01League105.614.474.164.324.28League w/o Table Classification105.313.923.883.513.77League w/o Refinement2099.353.853.713.623.72League120.524.283.924.214.13</p>
<p>Question Answering in LLMs Leaderboard: HotpotQA dataset (ACL, EMNLP, ICLR, ICML) Papers due: 2025 September
Latest 5 papersNo. Model NameCode Training StrategyEMF1SourceOpen-RAG: Enhanced Retrieval Augmented Reasoning with Open-1Source Large Language ModelsGitHub Hybrid Approach for Adaptive Retrieval66.2 80.1 EMNLP 2024 findingsLlama2-13B</p>
<dl>
<dt>Table Classification</dt>
<dt>Classification</dt>
<dt>introduced League, a novel framework leveraging LLMs to automatically generate the latest high-quality leaderboards based on given research topics.League addressed key challenges, including paper coverage, fair comparison, and timeliness, through a systematic approach that involves paper collection and splitting, table extraction and classification, table unpacking and integration, and leaderboard generation and evaluation.Experiments showed that League can automatically generate new high-quality leaderboards in a relatively short time and match human performance in terms of Topic-related Quality and Content Quality.This advancement offered a scalable and effective solution for synthesizing the latest leaderboards, providing a valuable tool for researchers in rapidly evolving fields such as artificial intelligence.</dt>
<dd>
<p>We remove the table classification step, which leads to a slight decrease in Structure and Multiaspect scores, indicating that classifying tables is essential for maintaining a logical and well-organized leaderboard.(2)Leaguew/oRefinement:We disable the Refinement step, which results in a minor drop in Coverage and Latest scores, suggesting that Refinement helps refine the leaderboard by ensuring that only the most relevant and recent papers are included.As shown in Table4, the results confirm that each component plays a crucial role in achieving the generation of a high-quality leaderboard.5CONCLUSIONWe</p>
</dd>
</dl>
<p>Impact of Iteration on League Performance.Right: Pearson Correlation Coefficient values given by four LLMs and human experts.Note that the Pearson Correlation Coefficient is between -1 and 1, the larger value indicates more positive correlations.
IterationsPearson Correlation Coefficient4.2 4.3 4.4 4.5 4.64.41 4.234.51 4.33 4.274.46 4.24 4.424.31 4.46 4.514.34 4.44 4.530.6 0.80.710.760.420.534.10.444.073.9CoverageLatestStructure0.23.8Iter 1Iter 2Iter 3Iter 4Iter 50GPT-4oO1-previewQwen-2.5-7BQwen-2.5-14BFigure 4: Left:</p>
<p>Table U } and table-related description {D1, ...D U }.
9:Classify each table and keep "Main Results Table".10:for each main table and table description do11:Extract Paper title, Dataset, Metrics, Experiment Settings, and Experiment Results as quintuple.</p>
<p>Table 5 :
5
Results of leaderboard quality generated by leveraging the official APIs provided by OpenReview (openreview-py) and the ACL Anthology (acl-anthology), we systematically harvested camera-ready PDFs from top-tier venues (NeurIPS, ICML, ICLR, ACL, EMNLP, NAACL).
ItemsTopic-related Quality Recall PrecisionModelSpeed /sCoverageContent Quality Latest Structure Multiaspect582.36 ±9.42 80.77 ±10.05Qwen2.5-7B Qwen2.5-14B GPT-4o O1-preview Human Writing176.41 3.96 ±0.42 3.83 ±0.37 3.54 ±0.40 188.17 4.34 ±0.42 4.28 ±0.35 3.95 ±0.31 103.32 4.66 ±0.40 4.83 ±0.34 4.43 ±0.37 117.22 4.70 ±0.42 4.85 ±0.65 4.51 ±0.31 433 4.83 4.93 4.903.78 4.19 4.64 4.69 4.891077.25 ±8.17 79.96 ±8.66Qwen2.5-7B Qwen2.5-14B GPT-4o O1-preview Human Writing199.76 3.63 ±0.47 3.50 ±0.55 4.09 ±0.47 207.39 4.12 ±0.45 3.83 ±0.46 3.97 ±0.32 113.43 4.72 ±0.41 4.53 ±0.35 4.72 ±0.35 136.57 4.61 ±0.32 4.66 ±0.66 4.40 ±0.41 780 4.82 4.67 4.703.74 3.97 4.66 4.56 4.731570.21 ±7.54 73.90 ±6.83Qwen2.5-7B Qwen2.5-14B GPT-4o O1-preview Human Writing243.11 3.36 ±0.18 3.29 ±0.22 3.45 ±0.33 255.64 3.66 ±0.25 3.37 ±0.20 3.55 ±0.27 124.55 4.59 ±0.24 4.33 ±0.29 4.35 ±0.20 137.80 4.35 ±0.44 4.16 ±0.23 4.30 ±0.29 1176 4.70 4.53 4.513.37 3.53 4.42 4.27 4.58</p>
<p>Table 6 :
6
The prompts of the table extraction (w and w/o the table classification COT procedure) differs in the [EXAMPLE JSON].It is detailed in the Table 8.</p>
<p><instruction> You are an expert in summarizing and extracting key content from LaTeX-formatted academic papers on computers and artificial intelligence.Please output your reply in the following JSON format: <format>[EXAMPLE JSON]</format> Key points:</p>
<p>Table 7 :
7
Prompt of the leaderboard construction.</p>
<p>An Evidential-enhanced Tri-Branch Consistency Learning Method for Semi- supervised Medical Image Segmentation
Image Quality Assessment Leaderboard: LIVE datasetPapers due: 2024 NovemberLatest 5 papersModelExperimental SettingMetricsNo.Model NameCodeTraining StrategySROCC PLCC RMSE mIoU mAccDual-Representation Interaction Driven Image1Quality Assessment with Restoration Assistance DRI-IQA modelGitHubDual-Representation Interaction method with restoration assistance0.9820.984---learning rate of 2e-4, batch size of 64Study of Subjective and Objective Quality inSuper-Resolution Enhanced Broadcast Images2on a Novel SR-IQA Dataset-5-fold cross-validation0.860.911 0.699--ARNIQA modelscaling factor x2, iterations 1000Exploring Rich Subjective Quality Information3for Image Quality Assessment in the Wild RichIQA model with three-stage quality prediction network Adam optimizer with an initial learning rate of 0.00001, batch size-multi-label training strategy using MOS, DOS, and SOS0.8943 0.9121 8.2312--of 8Q-Ground: Image Quality Grounding with Large4Multi-modality Models Mask2FormerGitHubsemantic segmentation finetuning---0.403 0.646learning rate of 0.0003, batch size of 2Dual-Branch Network for Portrait Image QualityAssessmentPre-trained on LSVQ and GFIQA5Dual-Branch Network with Swin Transformer-BGitHubdatasets, followed by learning-to-0.850.86---backbonesrank optimizationInitial learning rate of 1e-5, batch size of 12ETC-Net with V-Net backboneGitHub semi-supervised learning with evidential tri-branch consistency 91.1583.85.45 1.65labeled scans of 8, unlabeled scans of 72, batch size of 4, learning rate of 0.1EPL: Evidential Prototype Learning for Semi-supervised Medical Image18Segmentation V-Net architecture-semi-supervised learning with 20% labeled data92.385.724.73 1.38learning rate of 0.001, batch size of 3, iterations of 10000Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised19Medical Image Segmentation V-Net-semi-supervised learning with evidential fusion-based framework92.62 85.244.47 1.33labeled_ratio of 100%, unlabeled_ratio of 0%Guidelines for Cerebrovascular Segmentation: Managing Imperfect20Annotations in the context of Semi-Supervised Learning UA-MT (Uncertainty-Aware Mean-Teacher)GitHubsemi-supervised learning with uncertainty-aware consistency regularization89.51 81.01--learning rate of 0.01, final weight for consistency loss of 0.01Figure 5: A leaderboard (20 lines) of semi-supervised medical image segmentation on the LAdataset, using GPT-4o for table extraction and Qwen2.5-14B for leaderboard construction &amp; re-finement.</p>
<p>Table 8 :
8
The example JSON file of the table extraction with table classification COT.
1 {2"title": "The title of the paper (String)",3"number of tables": "The number of tables in the paper,→ denoted as n (Int)",4"classification of tables": {5"0": "main-result/comparison",6"1": "ablation",7"2": "hyper-parameter",8"3": "others"9},10"selected table's index": "The index of the main result table→ focused on the specified dataset [SPECIFIED DATASET],→ denoted as i (Int)",11"metrics": "The evaluation metrics chosen to assess→ performance of the method proposed in this paper. This→ information is extracted from the textual portion of the→ 'Experimental' related section (String)",
12"selected table's metrics": "Metrics used in the selected main → result table, it should be almost the same as the → metrics extracted from the textual.Remove the latex → format syntax (String)",13"selected table's core results": "A dictionary only containing → this paper's model best performance on the selected → dataset, with the metrics as keys and the corresponding → values (Dict)",14"selected table's settings (model &amp; size)": "In computer → vision, the model usually means the backbone architecture → of the network, such as ResNet, ViT, and so on.The size → can be omitted if not specified.In NLP, the model and → size are usually organized as a string, such as 'LLAMA-7B → ', 'GPT-3', and so on (String)",15"selected table's settings (training strategy)": "Training → strategy usually refers to the concepts like: fine-tuning → , transfer learning, linear-probing, reinforce learning, → one-shot, few-shot, prompt-learning, semi/self supervised → and so on (String)",16"selected table's settings (hyperparameter selection)": "The → hyperparameters used in the model, such as learning rate, → batch size, and so on.You should output a dictionary → with the hyperparameters and their values (Dict)",</p>
<p>Table Classification and
Classification
Table NER To illustrate the effectiveness of stages 2 and 3. We manually annotated 354 tables from 72 papers, with 197 main results tables, 54 ablation study tables, and 103 others.For Table NER, we follow the annotation criteria of SciIE</p>
<p>Table 10 :
10
Illustration of the table's cell entity recognition, where stands for datasets, for methods, for metrics, and for experimental settings..For open-source models, all experiments are conducted on a single A100 GPU.The input length is set to 128K tokens and the max output tokens is 4096.E EXAMPLE LEAGUE-GENERATED LEADERBOARDSFigure 5 and 6 illustrate two examples generated by League.The papers in the first leaderboard are the latest methods of semi-supervised medical image segmentation on the LA dataset from March to December in 2024.The second leaderboard collects the most recent methods for image quality assessment conducted on the LIVE dataset from February 2022 to November 2024.To ensure that the table content is fully displayed, the "model &amp; size" and "hyperparameters selection" within the experimental settings are presented beneath the paper titles.
KonIQ-10KMethodSRCC↑ PLCC↑w/o direct pathway0.93760.9495w/o indirect pathway 0.93610.9479w/o both pathways0.93630.9463RichIQA0.93830.9500
default</p>
<p>Table 11 :
11
Cost of API.The open-source Qwen2.5-7/14Bmodel is deployed on a local server comprising four A800 GPUs, resulting in zero cost.
Input tokensOutput tokensQwen2.5-7/14BkimiAI-128kGPT4-oO1-preview834.7K8.9K0 $7.034 $2.176 $13.055 $
https://github.com/lukasschwab/arxiv.py
Semi-Supervised Medical Image Segmentation Leaderboard: LA dataset
. Josh Achiam, Steven Adler, Sandhini Agarwal, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, ArXiv, abs/2404.077382024</p>
<p>Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Robin Lutz Bornmann, Ruediger Haunschild, Mutz, Humanities and Social Sciences Communications. 82020</p>
<p>Extending context window of large language models via positional interpolation. Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian, ArXiv, abs/2306.155952023a</p>
<p>Longlora: Efficient fine-tuning of long-context large language models. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia, ArXiv, abs/2309.123072023b</p>
<p>A survey on in-context learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, Conference on Empirical Methods in Natural Language Processing. 2022255372865</p>
<p>Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, </p>
<p>10.18653/v1/P19-1513Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Anna Korhonen, David Traum, Lluís Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Orkg-leaderboards: a systematic workflow for mining leaderboards as a knowledge graph. Salomon Kabongo, Kabenamualu Jennifer, D' Souza, S Auer, International Journal on Digital Libraries. 2023</p>
<p>AxCell: Automatic extraction of results from machine learning papers. Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, Robert Stojnic, 10.18653/v1/2020.emnlp-main.692Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 2020</p>
<p>All data on the table: Novel dataset and benchmark for cross-modality scientific information extraction. Yuhan Li, Jian Wu, Zhiwei Yu, Börje F Karlsson, Wei Shen, Manabu Okumura, Chin-Yew Lin, 2023</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N Foerster, Jeff Clune, David Ha, ArXiv, abs/2408.062922024</p>
<p>Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, ArXiv, abs/2409.041092024</p>
<p>Legobench: Scientific leaderboard generation benchmark. Shruti Singh, Shoaib Alam, Husain Malwat, Mayank Singh, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>SDCL: Students Discrepancy-Informed Correction Learning for Semi-supervised Medical Image Segmentation. Bentao Song, Qingfeng Wang, proceedings of Medical Image Computing and Computer Assisted Intervention -MICCAI 2024, volume LNCS 15008. Medical Image Computing and Computer Assisted Intervention -MICCAI 2024, volume LNCS 15008Springer Nature SwitzerlandOctober 2024</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Annual Meeting of the Association for Computational Linguistics. 2023a</p>
<p>Augmenting language models with long-term memory. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei, ArXiv, abs/2306.071742023b</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang, ArXiv, abs/2406.102522024</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, ArXiv, abs/2411.008162024</p>
<p>How far are ai scientists from changing the world?. Qiujie Xie, Yixuan Weng, Minjun Zhu, Fuchen Shen, Shulin Huang, Zhen Lin, Jiahui Zhou, Zilan Mao, Zijie Yang, Linyi Yang, arXiv:2507.232762025arXiv preprint</p>
<p>An Qwen, Baosong Yang, Beichen Yang, Binyuan Zhang, Bo Hui, Bowen Zheng, Chengyuan Yu, Dayiheng Li, Fei Liu, Haoran Huang, Huan Wei, Jian Lin, Jianhong Yang, Jianwei Tu, Jianxin Zhang, Jiaxin Yang, Jingren Yang, Junyang Zhou, Kai Lin, Keming Dang, Keqin Lu, Kexin Bao, Le Yang, Mei Yu, Mingfeng Li, Pei Xue, Qin Zhang, Rui Zhu, Runji Men, Tianhao Lin, Tingyu Li, Xingzhang Xia, Xuancheng Ren, Yang Ren, Yang Fan, Yi-Chao Su, Yunyang Zhang, Yuqi Wan, Zeyu Liu, Zhenru Cui, Zhang, Zihan Qiu, and Shanghaoran Quan. Qwen2.5 technical report. 2024</p>
<p>TELIN: Table entity LINker for extracting leaderboards from machine learning publications. Sean Yang, Chris Tensmeyer, Curtis Wigington, 10.18653/v1/2022.wiesp-1.3Proceedings of the first Workshop on Information Extraction from Scientific Publications. Tirthankar Ghosal, Sergi Blanco-Cuaresma, Alberto Accomazzi, Robert M Patton, Felix Grezes, Thomas Allen, the first Workshop on Information Extraction from Scientific PublicationsAssociation for Computational LinguisticsNovember 2022</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2018</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, E Cambria, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards. Furkan Şahinuç, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, Iryna Gurevych, Conference on Empirical Methods in Natural Language Processing. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
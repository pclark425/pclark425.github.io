<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-411 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-411</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-411</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-270660899</p>
                <p><strong>Paper Title:</strong> (Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review</p>
                <p><strong>Paper Abstract:</strong> Background An abundance of rapidly accumulating scientific evidence presents novel opportunities for researchers and practitioners alike, yet such advantages are often overshadowed by resource demands associated with finding and aggregating a continually expanding body of scientific information. Data extraction activities associated with evidence synthesis have been described as time-consuming to the point of critically limiting the usefulness of research. Across social science disciplines, the use of automation technologies for timely and accurate knowledge synthesis can enhance research translation value, better inform key policy development, and expand the current understanding of human interactions, organizations, and systems. Ongoing developments surrounding automation are highly concentrated in research for evidence-based medicine with limited evidence surrounding tools and techniques applied outside of the clinical research community. The goal of the present study is to extend the automation knowledge base by synthesizing current trends in the application of extraction technologies of key data elements of interest for social scientists. Methods We report the baseline results of a living systematic review of automated data extraction techniques supporting systematic reviews and meta-analyses in the social sciences. This review follows PRISMA standards for reporting systematic reviews. Results The baseline review of social science research yielded 23 relevant studies. Conclusions When considering the process of automating systematic review and meta-analysis information extraction, social science research falls short as compared to clinical research that focuses on automatic processing of information related to the PICO framework. With a few exceptions, most tools were either in the infancy stage and not accessible to applied researchers, were domain specific, or required substantial manual coding of articles before automation could occur. Additionally, few solutions considered extraction of data from tables which is where key data elements reside that social and behavioral scientists analyze.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e411.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e411.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PICO extraction methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PICO-based automated data extraction techniques from clinical randomized controlled trials</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated pipelines and ML/NLP systems developed to extract Population, Intervention, Comparison/Control, and Outcome (PICO) elements from clinical RCT reports; described in the paper as a well-developed clinical-domain exemplar whose methods are considered for transfer to social sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>PICO automated extraction pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Computational pipelines that identify and extract structured trial metadata (Population/problem, Intervention, Control/Comparator, Outcomes) from clinical trial reports using a mix of preprocessing (tokenization, PDF parsing), feature engineering (TF-IDF, embeddings), sequence labeling (CRF, BiLSTM-CRF), transformer fine-tuning (BERT variants), and rule/ontology components to produce structured PICO triples for evidence synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data extraction technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>clinical research / evidence-based medicine (RCTs)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>social sciences systematic reviews and meta-analyses</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Paper reports that transfer requires domain-specific adaptations such as new construct taxonomies and vocabularies (instead of PICO), re-labeling of target entities (e.g., measures, psychometrics, model indices), retraining or fine-tuning of models on social-science corpora, and changes to heuristics and ontologies to reflect social-science reporting conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful / limited: the review notes strong development and many studies in the clinical PICO space but few applied equivalents in social sciences (clinical LSRs reporting many more extraction studies than the 23 found for social sciences), indicating limited successful transfer so far.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Substantial heterogeneity in social science study designs and reporting; lack of unifying reporting standards and domain ontologies; limited labeled training data in social sciences; different target entities (effect sizes, psychometrics) versus clinical PICO.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Shared underlying NLP/ML methods (transformers, CRF, embeddings), potential for modular pipelines, availability of domain adaptation techniques and transfer learning, and the precedent of domain-specific ontologies in clinical extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires social-science-specific annotated corpora, standardized construct taxonomies/ontologies, expert-curated labeling, and adaptation of extraction heuristics (e.g., for tables and varied statistical reporting formats).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Limited without adaptation; general methodological principles transfer, but practical application requires domain-specific changes—not broadly generalizable as-is.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (computational workflows, feature/model choices) plus need for tacit domain knowledge (what constructs/entities matter in social sciences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e411.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SENTCON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence Context Ontology (SENTCON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ontology/pipeline initially applied to modeling sentence contexts in computer science/linked-data publications, designed to convert sentence-level information into machine-understandable semantic structures and noted as flexible to other domains via OWL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Contextual information retrieval in research articles: Semantic publishing tools for the research community</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>SENTCON (Sentence Context Ontology)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>An ontology-driven approach that models the contextual role of sentences (e.g., claims, methods, results) in research articles and enables semantic annotation and conversion to machine-readable knowledge using Web Ontology Language (OWL) and linked-data practices.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>ontology-based information extraction / semantic web pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer science / semantic web / linked data</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>other research domains including social sciences (general-purpose application)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Transfer envisioned via OWL: mapping of SENTCON classes to domain-specific ontologies/vocabularies, extension of sentence-context classes to reflect domain reporting conventions, and reconfiguration of extraction rules to match different document structures.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>described as promising/flexible by authors; no quantitative cross-domain evaluation reported in review (i.e., authors emphasize capability to be applied to other domains but concrete success metrics not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need for domain-specific ontologies and lexicons; potential mismatch between sentence-roles in different disciplines; effort required to map or extend OWL classes for new domains.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Use of standard semantic web languages (OWL) enabling portability; modular ontology design; authors' explicit emphasis on flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Availability of domain ontologies or experts to create mappings; tools to annotate sentence roles; computational pipeline supporting OWL-based annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High potential generalizability if domain ontologies are created—presented as deliberately extensible across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and interpretive frameworks (ontology design and semantic annotation rules).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e411.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LCA data-mining pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Method for extracting characteristics of Life Cycle Assessments via data mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-mining pipeline developed to extract life-cycle assessment (LCA) characteristics from journal articles; authors report it was applied to wastewater-based resource recovery and stressed that the tool can evaluate other engineered systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting the characteristics of life cycle assessments via data mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>LCA/engineered-systems extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Text-mining procedures that parse scientific articles to identify and extract LCA-relevant metadata (e.g., system boundaries, inventory data, impact categories) using preprocessing (PDF parsing), pattern/rule extraction, and semantic annotation to populate structured LCA records.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data extraction technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>environmental engineering / life cycle assessment (LCA)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>other engineered systems and broader research literature</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Design centered for flexibility: authors indicate adjusting the extraction targets and ontology/knowledge base to the engineered system of interest (changing named entities, patterns and keywords to match domain terminology).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Authors emphasize capability to evaluate other engineered systems but the review does not report quantitative cross-domain validation—transfer framed as feasible but not extensively measured.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Domain-specific terminologies and reporting formats; requirement to map different engineering terms into the extraction ontology.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Modular pipeline design, use of semantic annotation, and explicit focus on flexible data retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to full-text articles, tailored lexicons/ontologies for target engineered systems, and possibly table parsing/OCR when data are tabular.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate-to-high for engineered-systems contexts where ontology/term mappings can be produced; less immediate for unrelated social-science topics without new ontologies.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical skills (text-mining pipeline configuration and ontology mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e411.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OATS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ontology-based and User-focused Automatic Text Summarization (OATS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ontology-driven summarization and topic-identification system demonstrated on COVID-19 risk factor literature and described by authors as adaptable beyond the demonstration domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ontology-based and User-focused Automatic Text Summarization (OATS): Using COVID-19 Risk Factors as an Example</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Ontology-based user-focused summarization (OATS)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Pipeline that combines ontology-driven topic identification with user-focused summarization modules to generate extractive/abstractive summaries from unstructured text; uses semantic annotation to align extracted text with domain concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / summarization & information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>COVID-19 / clinical / public-health literature</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>other research domains (including social sciences)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>To transfer, authors propose replacing or extending the COVID-specific ontology with domain-specific ontologies, adjusting extraction rules and evaluation metrics to domain reporting styles, and retraining any ML modules on new corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Authors reported the tool and highlighted adaptability; the review notes potential applicability beyond the COVID example but does not report cross-domain performance numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need for domain ontologies and labeled examples; potential misalignment of summary goals between domains.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Ontology-based design and modular summarization components that can be reconfigured for new vocabularies.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Domain ontologies, user-defined summary preferences, and corpora for fine-tuning summarization modules.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate: design intended to be reused across domains provided ontologies and adjustments are made.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and interpretive frameworks (ontology-driven summarization principles).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e411.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaSeer.STEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaSeer.STEM: Towards Automating Meta-Analyses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A machine-learning system developed to support automated extraction of quantitative data for STEM education meta-analyses; authors designed it for applicability across multiple research domains (education, management, health informatics).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MetaSeer.STEM: Towards Automating Meta-Analyses</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>MetaSeer.STEM ML extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>An ML-based pipeline that identifies numbers and numeric reporting (e.g., descriptive statistics), extracts candidate values, and surfaces them to users for manual annotation, using feature engineering (representations, regexes), classifiers and active human-in-the-loop annotation to scale data extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / semi-automated data extraction with human-in-the-loop</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>STEM education research (meta-analytic workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>other empirical domains (education, management, health informatics)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>designed as multi-domain / hybrid approach</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Approach centers on flexible numeric detection heuristics, UI for human review, and retrainable ML classifiers—transfer requires reconfiguration of numeric extraction rules and retraining on domain-specific document formats.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Reported as capable of identifying numbers and supporting annotation workflows; the review reports the system was developed for multi-domain extraction but does not provide cross-domain quantitative metrics in this baseline iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Domain variability in numeric reporting formats and insufficient labeled data for some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Human-in-the-loop design, modular numeric-extraction components, and use of supervised classifiers that can be retrained.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>User annotation interfaces, domain experts for labeling, and adaptable regex/feature configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Good potential within quantitative-reporting domains where numeric conventions are similar; needs retraining for other formats.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental skills (ML classifier training, UI-driven annotation workflows), plus tacit human judgment embedded in annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e411.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TableSeer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TableSeer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for locating and extracting tables from digital documents (searching for tables in digital documents), identified as relevant because many key quantitative elements (e.g., effect sizes) reside in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Searching for tables in digital documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>automated table detection and extraction (TableSeer)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Algorithms to detect, parse and index tabular structures in digital documents (PDF/HTML), enabling retrieval of table-contained data for downstream extraction and analysis; combines document layout analysis and table-structure parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data extraction technique / document analysis</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>document analysis / information retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>systematic review/meta-analysis data extraction (social sciences and other domains)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without major modification (but requires format handling)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>To apply to social-science meta-analyses, systems may need additional post-processing to interpret statistical table cells (e.g., mapping columns to means/SDs/effect sizes) and OCR when tables are in scanned PDFs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Recognized as a useful component for extracting tabular quantitative data; review notes that few social-science tools extract from tables, so TableSeer-like capabilities are underused rather than failing.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>PDF parsing variability, OCR errors for scanned tables, heterogeneous table formats across journals.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Clear table-detection heuristics and available parsing algorithms; prior art from document-analysis community.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>High-quality digital text or OCR, mapping rules to interpret table semantics, and integration into pipeline for numeric extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for any domain where key data are reported in tables, subject to format variability.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills and explicit procedural steps (document layout analysis and table parsing algorithms).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e411.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sysrev</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sysrev: A FAIR Platform for Data Curation and Systematic Evidence Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online platform that supports collaborative data curation and semi-automated systematic review workflows (labeling, active learning); reported as an accessible domain-independent tool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sysrev: A FAIR Platform for Data Curation and Systematic Evidence Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Sysrev active-learning data curation platform</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Web application combining document ingestion, UI-driven labeling and annotation, and active-learning models that learn from user labels to predict future labels and assist in large-scale data extraction and curation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>software/tool / human-computer hybrid extraction workflow</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>software tools for systematic reviews (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>social sciences and other research areas conducting SLRs</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification (domain-independent platform)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Minimal; tool is deployed as a general platform but users create project-specific labeling schemas and rules for their domain.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Accessible online at time of review and cataloged in the Systematic Review Toolbox; presented as usable across domains though specific social-science uptake not quantified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Some tools not accessible or matured; domain-specific extraction tasks may still require custom models or rules.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>General-purpose UI, active learning, documented accessibility and FAIR approach.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Users must define labeling schemas and supply seed annotations; internet access and project configuration expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High — designed to be domain-agnostic via customizable schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills (platform use) and explicit procedural steps for configuring projects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e411.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inline-math CRF method</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRF-based method for detecting in-line mathematical expressions in scientific documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CRF-based system that detects inline mathematical expressions in PDFs by combining layout and linguistic features; OCR was used as a benchmark in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detecting In-line Mathematical Expressions in Scientific Documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>CRF + layout+linguistic features inline-math detection</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A sequence-labeling approach using Conditional Random Fields (CRF) augmented with document layout cues and linguistic context to locate and extract inline mathematical expressions from PDF documents; OCR methods were used as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / sequence labeling for document analysis</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>document analysis / OCR research</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scientific document parsing for math-rich texts (scholarly articles across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/benchmarked (OCR used as baseline; CRF tailored to math detection)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Integration of layout features (visual cues from PDF) with linguistic sequence features in the CRF model to better detect inline math compared with OCR-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Preliminary results reported; CRF approach used OCR only as a benchmark. Review highlights difficulty in accurately detecting inline math and PDF parsing challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>PDF parsing variability and complexity of inline math notation; limited success rates reported in included study.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Combining visual/layout and linguistic signals; availability of CRF modeling frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to PDF layout extraction tools, annotated examples of inline math for training, and evaluation frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate for other technical/scientific corpora with inline math but requires domain-specific training and handling of notation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental skills (CRF modeling and PDF layout analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e411.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DASyR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DASyR (Document Analysis System for Systematic Reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-automatic document analysis framework that combines user annotation, classification models, and contextual information to populate ontologies, especially useful in domains that lack adequate dictionaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DASyR (IR) -Document Analysis System for Systematic Reviews (in Information Retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>DASyR semi-automatic ontology population pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A pipeline combining UI-driven expert annotations, supervised classification, and contextual inference to rapidly populate ontologies and annotate documents in domains lacking ready vocabularies; it reduces manual annotation burden by predicting annotations and allowing experts to validate.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational/human-in-the-loop method for ontology population and annotation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>information retrieval / document-annotation research</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>domains lacking dictionaries or lexicons (e.g., some social science subfields)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/hybrid approach combining user annotation and ML</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Integration of a UI to collect rapid expert annotations, classifiers trained on those annotations, and contextual inference modules to suggest further annotations—design meant to be portable to other domains by re-seeding with domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Reported substantial reduction in annotation time: five experts added approximately 30,000 annotations at a speed of ~4 seconds per annotation (authors' report), indicating practical success in speeding ontology population.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need for initial expert annotations and potential annotation biases; domain expertise required to seed classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Human-in-the-loop UI, classifier-assisted annotation, and contextual inference to scale human effort.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Domain experts for initial annotations, annotation UI, and infrastructure to train and deploy classification models.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Good potential across domains lacking dictionaries, provided domain experts supply initial annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>tacit/practical know-how (how to run annotation sprints) plus explicit procedural steps (training classifiers from annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e411.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LED/Long-document transformer usage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of Long-Document Encoder-Decoder (LED) / longformer-style transformer models for citation-span and related-work annotation (as used in CORWA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of transformer architectures designed for long inputs (LED / Longformer) to process long scholarly documents and annotate/capture citation spans and related-work fragments, overcoming sequence-length limits of standard transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORWA: A Citation-Oriented Related Work Annotation Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>LED / long-document transformer adaptation for scholarly annotation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Fine-tuning pretrained long-document transformer models (e.g., LED) to operate over extended document contexts, enabling extractive question answering or sequence labeling across full papers for tasks such as citation-span detection and section-level annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transformer-based NLP adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>natural language processing / transformer model development</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scholarly document processing (citation-span annotation, related-work generation)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new task via fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Use of long-input transformer architectures (LED) to accommodate long sequences, selection/fine-tuning on scholarly corpora (e.g., CORWA dataset), and possible task-specific heads for annotation or extractive QA.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Adopted in included study(s) and cited as addressing length limitations; review mentions adoption but does not provide numerical performance metrics in baseline summary.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Computational resource demands for long-context transformers and requirement for labeled datasets for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of pretrained long-document transformer models and datasets like CORWA to enable fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>GPU compute for fine-tuning/inference and labeled long-document corpora for the target annotation task.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within long-document NLP tasks (scholarly articles, reports) but computationally intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical skills (model fine-tuning and dataset preparation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e411.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e411.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NER integration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Named Entity Recognition pipelines adapted for scholarly data extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integration of NER modules (pre-trained or custom) into larger ML/NLP pipelines for identifying domain entities (e.g., measures, sample descriptors) in scholarly articles; used both as preprocessing and as core extraction elements in multiple included studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards a Semi-Automated Approach for Systematic Literature Reviews Completed Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>NER-augmented extraction pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Use of Named Entity Recognition (NER) systems—standalone or embedded—to tag and extract relevant named entities (e.g., instruments, populations, measures) from article text, often combined with ontologies, regex rules, or downstream classification models.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / entity recognition & extraction</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general NLP and clinical-information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>social sciences scholarly extraction and SLR pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>hybrid approach combining existing NER with domain adaptations</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Linking NER outputs to domain-specific ontologies or lexica, retraining or fine-tuning NER models on domain corpora, and integrating NER into larger UI-driven or pipeline architectures for semi-automated extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Described as versatile and increasingly used across included studies; used variably as preprocessing, core extraction, or embedded in pipelines—no uniform cross-domain performance metrics presented in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Limited domain-specific labeled data for social sciences; differences in naming conventions and non-standardized reporting leading to lower out-of-the-box performance.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Open-source NER tools, modular pipeline designs, and availability of annotation UIs to create in-domain training data.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Annotated examples for fine-tuning, ontology/lexicon resources for mapping entity types, and integration code to connect NER outputs to downstream extractors.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Potentially high if models are fine-tuned and linked to domain ontologies; out-of-the-box NER models often underperform without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (model fine-tuning, annotation pipelines) plus tacit domain knowledge for defining entity categories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Contextual information retrieval in research articles: Semantic publishing tools for the research community <em>(Rating: 2)</em></li>
                <li>Extracting the characteristics of life cycle assessments via data mining <em>(Rating: 2)</em></li>
                <li>Ontology-based and User-focused Automatic Text Summarization (OATS): Using COVID-19 Risk Factors as an Example <em>(Rating: 2)</em></li>
                <li>MetaSeer.STEM: Towards Automating Meta-Analyses <em>(Rating: 2)</em></li>
                <li>Searching for tables in digital documents <em>(Rating: 2)</em></li>
                <li>Sysrev: A FAIR Platform for Data Curation and Systematic Evidence Review <em>(Rating: 2)</em></li>
                <li>Detecting In-line Mathematical Expressions in Scientific Documents <em>(Rating: 2)</em></li>
                <li>DASyR (IR) -Document Analysis System for Systematic Reviews (in Information Retrieval) <em>(Rating: 2)</em></li>
                <li>CORWA: A Citation-Oriented Related Work Annotation Dataset <em>(Rating: 2)</em></li>
                <li>Towards a Semi-Automated Approach for Systematic Literature Reviews Completed Research <em>(Rating: 1)</em></li>
                <li>Automating systematic literature reviews with natural language processing and text mining: A systematic literature review <em>(Rating: 1)</em></li>
                <li>Artificial intelligence and the conduct of literature reviews <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-411",
    "paper_id": "paper-270660899",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "PICO extraction methods",
            "name_full": "PICO-based automated data extraction techniques from clinical randomized controlled trials",
            "brief_description": "Automated pipelines and ML/NLP systems developed to extract Population, Intervention, Comparison/Control, and Outcome (PICO) elements from clinical RCT reports; described in the paper as a well-developed clinical-domain exemplar whose methods are considered for transfer to social sciences.",
            "citation_title": "",
            "mention_or_use": "mention",
            "procedure_name": "PICO automated extraction pipelines",
            "procedure_description": "Computational pipelines that identify and extract structured trial metadata (Population/problem, Intervention, Control/Comparator, Outcomes) from clinical trial reports using a mix of preprocessing (tokenization, PDF parsing), feature engineering (TF-IDF, embeddings), sequence labeling (CRF, BiLSTM-CRF), transformer fine-tuning (BERT variants), and rule/ontology components to produce structured PICO triples for evidence synthesis.",
            "procedure_type": "computational method / data extraction technique",
            "source_domain": "clinical research / evidence-based medicine (RCTs)",
            "target_domain": "social sciences systematic reviews and meta-analyses",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Paper reports that transfer requires domain-specific adaptations such as new construct taxonomies and vocabularies (instead of PICO), re-labeling of target entities (e.g., measures, psychometrics, model indices), retraining or fine-tuning of models on social-science corpora, and changes to heuristics and ontologies to reflect social-science reporting conventions.",
            "transfer_success": "partially successful / limited: the review notes strong development and many studies in the clinical PICO space but few applied equivalents in social sciences (clinical LSRs reporting many more extraction studies than the 23 found for social sciences), indicating limited successful transfer so far.",
            "barriers_encountered": "Substantial heterogeneity in social science study designs and reporting; lack of unifying reporting standards and domain ontologies; limited labeled training data in social sciences; different target entities (effect sizes, psychometrics) versus clinical PICO.",
            "facilitating_factors": "Shared underlying NLP/ML methods (transformers, CRF, embeddings), potential for modular pipelines, availability of domain adaptation techniques and transfer learning, and the precedent of domain-specific ontologies in clinical extraction.",
            "contextual_requirements": "Requires social-science-specific annotated corpora, standardized construct taxonomies/ontologies, expert-curated labeling, and adaptation of extraction heuristics (e.g., for tables and varied statistical reporting formats).",
            "generalizability": "Limited without adaptation; general methodological principles transfer, but practical application requires domain-specific changes—not broadly generalizable as-is.",
            "knowledge_type": "explicit procedural steps and theoretical principles (computational workflows, feature/model choices) plus need for tacit domain knowledge (what constructs/entities matter in social sciences).",
            "uuid": "e411.0",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SENTCON",
            "name_full": "Sentence Context Ontology (SENTCON)",
            "brief_description": "An ontology/pipeline initially applied to modeling sentence contexts in computer science/linked-data publications, designed to convert sentence-level information into machine-understandable semantic structures and noted as flexible to other domains via OWL.",
            "citation_title": "Contextual information retrieval in research articles: Semantic publishing tools for the research community",
            "mention_or_use": "mention",
            "procedure_name": "SENTCON (Sentence Context Ontology)",
            "procedure_description": "An ontology-driven approach that models the contextual role of sentences (e.g., claims, methods, results) in research articles and enables semantic annotation and conversion to machine-readable knowledge using Web Ontology Language (OWL) and linked-data practices.",
            "procedure_type": "ontology-based information extraction / semantic web pipeline",
            "source_domain": "computer science / semantic web / linked data",
            "target_domain": "other research domains including social sciences (general-purpose application)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Transfer envisioned via OWL: mapping of SENTCON classes to domain-specific ontologies/vocabularies, extension of sentence-context classes to reflect domain reporting conventions, and reconfiguration of extraction rules to match different document structures.",
            "transfer_success": "described as promising/flexible by authors; no quantitative cross-domain evaluation reported in review (i.e., authors emphasize capability to be applied to other domains but concrete success metrics not provided).",
            "barriers_encountered": "Need for domain-specific ontologies and lexicons; potential mismatch between sentence-roles in different disciplines; effort required to map or extend OWL classes for new domains.",
            "facilitating_factors": "Use of standard semantic web languages (OWL) enabling portability; modular ontology design; authors' explicit emphasis on flexibility.",
            "contextual_requirements": "Availability of domain ontologies or experts to create mappings; tools to annotate sentence roles; computational pipeline supporting OWL-based annotations.",
            "generalizability": "High potential generalizability if domain ontologies are created—presented as deliberately extensible across domains.",
            "knowledge_type": "explicit procedural steps and interpretive frameworks (ontology design and semantic annotation rules).",
            "uuid": "e411.1",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LCA data-mining pipeline",
            "name_full": "Method for extracting characteristics of Life Cycle Assessments via data mining",
            "brief_description": "A text-mining pipeline developed to extract life-cycle assessment (LCA) characteristics from journal articles; authors report it was applied to wastewater-based resource recovery and stressed that the tool can evaluate other engineered systems.",
            "citation_title": "Extracting the characteristics of life cycle assessments via data mining",
            "mention_or_use": "mention",
            "procedure_name": "LCA/engineered-systems extraction pipeline",
            "procedure_description": "Text-mining procedures that parse scientific articles to identify and extract LCA-relevant metadata (e.g., system boundaries, inventory data, impact categories) using preprocessing (PDF parsing), pattern/rule extraction, and semantic annotation to populate structured LCA records.",
            "procedure_type": "computational method / data extraction technique",
            "source_domain": "environmental engineering / life cycle assessment (LCA)",
            "target_domain": "other engineered systems and broader research literature",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Design centered for flexibility: authors indicate adjusting the extraction targets and ontology/knowledge base to the engineered system of interest (changing named entities, patterns and keywords to match domain terminology).",
            "transfer_success": "Authors emphasize capability to evaluate other engineered systems but the review does not report quantitative cross-domain validation—transfer framed as feasible but not extensively measured.",
            "barriers_encountered": "Domain-specific terminologies and reporting formats; requirement to map different engineering terms into the extraction ontology.",
            "facilitating_factors": "Modular pipeline design, use of semantic annotation, and explicit focus on flexible data retrieval.",
            "contextual_requirements": "Access to full-text articles, tailored lexicons/ontologies for target engineered systems, and possibly table parsing/OCR when data are tabular.",
            "generalizability": "Moderate-to-high for engineered-systems contexts where ontology/term mappings can be produced; less immediate for unrelated social-science topics without new ontologies.",
            "knowledge_type": "explicit procedural steps and instrumental/technical skills (text-mining pipeline configuration and ontology mapping).",
            "uuid": "e411.2",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "OATS",
            "name_full": "Ontology-based and User-focused Automatic Text Summarization (OATS)",
            "brief_description": "An ontology-driven summarization and topic-identification system demonstrated on COVID-19 risk factor literature and described by authors as adaptable beyond the demonstration domain.",
            "citation_title": "Ontology-based and User-focused Automatic Text Summarization (OATS): Using COVID-19 Risk Factors as an Example",
            "mention_or_use": "mention",
            "procedure_name": "Ontology-based user-focused summarization (OATS)",
            "procedure_description": "Pipeline that combines ontology-driven topic identification with user-focused summarization modules to generate extractive/abstractive summaries from unstructured text; uses semantic annotation to align extracted text with domain concepts.",
            "procedure_type": "computational method / summarization & information extraction",
            "source_domain": "COVID-19 / clinical / public-health literature",
            "target_domain": "other research domains (including social sciences)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "To transfer, authors propose replacing or extending the COVID-specific ontology with domain-specific ontologies, adjusting extraction rules and evaluation metrics to domain reporting styles, and retraining any ML modules on new corpora.",
            "transfer_success": "Authors reported the tool and highlighted adaptability; the review notes potential applicability beyond the COVID example but does not report cross-domain performance numbers.",
            "barriers_encountered": "Need for domain ontologies and labeled examples; potential misalignment of summary goals between domains.",
            "facilitating_factors": "Ontology-based design and modular summarization components that can be reconfigured for new vocabularies.",
            "contextual_requirements": "Domain ontologies, user-defined summary preferences, and corpora for fine-tuning summarization modules.",
            "generalizability": "Moderate: design intended to be reused across domains provided ontologies and adjustments are made.",
            "knowledge_type": "explicit procedural steps and interpretive frameworks (ontology-driven summarization principles).",
            "uuid": "e411.3",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MetaSeer.STEM",
            "name_full": "MetaSeer.STEM: Towards Automating Meta-Analyses",
            "brief_description": "A machine-learning system developed to support automated extraction of quantitative data for STEM education meta-analyses; authors designed it for applicability across multiple research domains (education, management, health informatics).",
            "citation_title": "MetaSeer.STEM: Towards Automating Meta-Analyses",
            "mention_or_use": "mention",
            "procedure_name": "MetaSeer.STEM ML extraction pipeline",
            "procedure_description": "An ML-based pipeline that identifies numbers and numeric reporting (e.g., descriptive statistics), extracts candidate values, and surfaces them to users for manual annotation, using feature engineering (representations, regexes), classifiers and active human-in-the-loop annotation to scale data extraction.",
            "procedure_type": "computational method / semi-automated data extraction with human-in-the-loop",
            "source_domain": "STEM education research (meta-analytic workflows)",
            "target_domain": "other empirical domains (education, management, health informatics)",
            "transfer_type": "designed as multi-domain / hybrid approach",
            "modifications_made": "Approach centers on flexible numeric detection heuristics, UI for human review, and retrainable ML classifiers—transfer requires reconfiguration of numeric extraction rules and retraining on domain-specific document formats.",
            "transfer_success": "Reported as capable of identifying numbers and supporting annotation workflows; the review reports the system was developed for multi-domain extraction but does not provide cross-domain quantitative metrics in this baseline iteration.",
            "barriers_encountered": "Domain variability in numeric reporting formats and insufficient labeled data for some domains.",
            "facilitating_factors": "Human-in-the-loop design, modular numeric-extraction components, and use of supervised classifiers that can be retrained.",
            "contextual_requirements": "User annotation interfaces, domain experts for labeling, and adaptable regex/feature configurations.",
            "generalizability": "Good potential within quantitative-reporting domains where numeric conventions are similar; needs retraining for other formats.",
            "knowledge_type": "explicit procedural steps and instrumental skills (ML classifier training, UI-driven annotation workflows), plus tacit human judgment embedded in annotation.",
            "uuid": "e411.4",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "TableSeer",
            "name_full": "TableSeer",
            "brief_description": "A system for locating and extracting tables from digital documents (searching for tables in digital documents), identified as relevant because many key quantitative elements (e.g., effect sizes) reside in tables.",
            "citation_title": "Searching for tables in digital documents",
            "mention_or_use": "mention",
            "procedure_name": "automated table detection and extraction (TableSeer)",
            "procedure_description": "Algorithms to detect, parse and index tabular structures in digital documents (PDF/HTML), enabling retrieval of table-contained data for downstream extraction and analysis; combines document layout analysis and table-structure parsing.",
            "procedure_type": "data extraction technique / document analysis",
            "source_domain": "document analysis / information retrieval",
            "target_domain": "systematic review/meta-analysis data extraction (social sciences and other domains)",
            "transfer_type": "direct application without major modification (but requires format handling)",
            "modifications_made": "To apply to social-science meta-analyses, systems may need additional post-processing to interpret statistical table cells (e.g., mapping columns to means/SDs/effect sizes) and OCR when tables are in scanned PDFs.",
            "transfer_success": "Recognized as a useful component for extracting tabular quantitative data; review notes that few social-science tools extract from tables, so TableSeer-like capabilities are underused rather than failing.",
            "barriers_encountered": "PDF parsing variability, OCR errors for scanned tables, heterogeneous table formats across journals.",
            "facilitating_factors": "Clear table-detection heuristics and available parsing algorithms; prior art from document-analysis community.",
            "contextual_requirements": "High-quality digital text or OCR, mapping rules to interpret table semantics, and integration into pipeline for numeric extraction.",
            "generalizability": "High for any domain where key data are reported in tables, subject to format variability.",
            "knowledge_type": "instrumental/technical skills and explicit procedural steps (document layout analysis and table parsing algorithms).",
            "uuid": "e411.5",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Sysrev",
            "name_full": "Sysrev: A FAIR Platform for Data Curation and Systematic Evidence Review",
            "brief_description": "An online platform that supports collaborative data curation and semi-automated systematic review workflows (labeling, active learning); reported as an accessible domain-independent tool.",
            "citation_title": "Sysrev: A FAIR Platform for Data Curation and Systematic Evidence Review",
            "mention_or_use": "mention",
            "procedure_name": "Sysrev active-learning data curation platform",
            "procedure_description": "Web application combining document ingestion, UI-driven labeling and annotation, and active-learning models that learn from user labels to predict future labels and assist in large-scale data extraction and curation tasks.",
            "procedure_type": "software/tool / human-computer hybrid extraction workflow",
            "source_domain": "software tools for systematic reviews (cross-domain)",
            "target_domain": "social sciences and other research areas conducting SLRs",
            "transfer_type": "direct application without modification (domain-independent platform)",
            "modifications_made": "Minimal; tool is deployed as a general platform but users create project-specific labeling schemas and rules for their domain.",
            "transfer_success": "Accessible online at time of review and cataloged in the Systematic Review Toolbox; presented as usable across domains though specific social-science uptake not quantified in review.",
            "barriers_encountered": "Some tools not accessible or matured; domain-specific extraction tasks may still require custom models or rules.",
            "facilitating_factors": "General-purpose UI, active learning, documented accessibility and FAIR approach.",
            "contextual_requirements": "Users must define labeling schemas and supply seed annotations; internet access and project configuration expertise.",
            "generalizability": "High — designed to be domain-agnostic via customizable schemas.",
            "knowledge_type": "instrumental/technical skills (platform use) and explicit procedural steps for configuring projects.",
            "uuid": "e411.6",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Inline-math CRF method",
            "name_full": "CRF-based method for detecting in-line mathematical expressions in scientific documents",
            "brief_description": "A CRF-based system that detects inline mathematical expressions in PDFs by combining layout and linguistic features; OCR was used as a benchmark in evaluation.",
            "citation_title": "Detecting In-line Mathematical Expressions in Scientific Documents",
            "mention_or_use": "mention",
            "procedure_name": "CRF + layout+linguistic features inline-math detection",
            "procedure_description": "A sequence-labeling approach using Conditional Random Fields (CRF) augmented with document layout cues and linguistic context to locate and extract inline mathematical expressions from PDF documents; OCR methods were used as a comparative baseline.",
            "procedure_type": "computational method / sequence labeling for document analysis",
            "source_domain": "document analysis / OCR research",
            "target_domain": "scientific document parsing for math-rich texts (scholarly articles across domains)",
            "transfer_type": "adapted/benchmarked (OCR used as baseline; CRF tailored to math detection)",
            "modifications_made": "Integration of layout features (visual cues from PDF) with linguistic sequence features in the CRF model to better detect inline math compared with OCR-only approaches.",
            "transfer_success": "Preliminary results reported; CRF approach used OCR only as a benchmark. Review highlights difficulty in accurately detecting inline math and PDF parsing challenges.",
            "barriers_encountered": "PDF parsing variability and complexity of inline math notation; limited success rates reported in included study.",
            "facilitating_factors": "Combining visual/layout and linguistic signals; availability of CRF modeling frameworks.",
            "contextual_requirements": "Access to PDF layout extraction tools, annotated examples of inline math for training, and evaluation frameworks.",
            "generalizability": "Moderate for other technical/scientific corpora with inline math but requires domain-specific training and handling of notation variants.",
            "knowledge_type": "explicit procedural steps and instrumental skills (CRF modeling and PDF layout analysis).",
            "uuid": "e411.7",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "DASyR",
            "name_full": "DASyR (Document Analysis System for Systematic Reviews)",
            "brief_description": "A semi-automatic document analysis framework that combines user annotation, classification models, and contextual information to populate ontologies, especially useful in domains that lack adequate dictionaries.",
            "citation_title": "DASyR (IR) -Document Analysis System for Systematic Reviews (in Information Retrieval)",
            "mention_or_use": "mention",
            "procedure_name": "DASyR semi-automatic ontology population pipeline",
            "procedure_description": "A pipeline combining UI-driven expert annotations, supervised classification, and contextual inference to rapidly populate ontologies and annotate documents in domains lacking ready vocabularies; it reduces manual annotation burden by predicting annotations and allowing experts to validate.",
            "procedure_type": "computational/human-in-the-loop method for ontology population and annotation",
            "source_domain": "information retrieval / document-annotation research",
            "target_domain": "domains lacking dictionaries or lexicons (e.g., some social science subfields)",
            "transfer_type": "adapted/hybrid approach combining user annotation and ML",
            "modifications_made": "Integration of a UI to collect rapid expert annotations, classifiers trained on those annotations, and contextual inference modules to suggest further annotations—design meant to be portable to other domains by re-seeding with domain experts.",
            "transfer_success": "Reported substantial reduction in annotation time: five experts added approximately 30,000 annotations at a speed of ~4 seconds per annotation (authors' report), indicating practical success in speeding ontology population.",
            "barriers_encountered": "Need for initial expert annotations and potential annotation biases; domain expertise required to seed classifiers.",
            "facilitating_factors": "Human-in-the-loop UI, classifier-assisted annotation, and contextual inference to scale human effort.",
            "contextual_requirements": "Domain experts for initial annotations, annotation UI, and infrastructure to train and deploy classification models.",
            "generalizability": "Good potential across domains lacking dictionaries, provided domain experts supply initial annotations.",
            "knowledge_type": "tacit/practical know-how (how to run annotation sprints) plus explicit procedural steps (training classifiers from annotations).",
            "uuid": "e411.8",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LED/Long-document transformer usage",
            "name_full": "Use of Long-Document Encoder-Decoder (LED) / longformer-style transformer models for citation-span and related-work annotation (as used in CORWA)",
            "brief_description": "Application of transformer architectures designed for long inputs (LED / Longformer) to process long scholarly documents and annotate/capture citation spans and related-work fragments, overcoming sequence-length limits of standard transformers.",
            "citation_title": "CORWA: A Citation-Oriented Related Work Annotation Dataset",
            "mention_or_use": "mention",
            "procedure_name": "LED / long-document transformer adaptation for scholarly annotation",
            "procedure_description": "Fine-tuning pretrained long-document transformer models (e.g., LED) to operate over extended document contexts, enabling extractive question answering or sequence labeling across full papers for tasks such as citation-span detection and section-level annotation.",
            "procedure_type": "computational method / transformer-based NLP adaptation",
            "source_domain": "natural language processing / transformer model development",
            "target_domain": "scholarly document processing (citation-span annotation, related-work generation)",
            "transfer_type": "adapted/modified for new task via fine-tuning",
            "modifications_made": "Use of long-input transformer architectures (LED) to accommodate long sequences, selection/fine-tuning on scholarly corpora (e.g., CORWA dataset), and possible task-specific heads for annotation or extractive QA.",
            "transfer_success": "Adopted in included study(s) and cited as addressing length limitations; review mentions adoption but does not provide numerical performance metrics in baseline summary.",
            "barriers_encountered": "Computational resource demands for long-context transformers and requirement for labeled datasets for fine-tuning.",
            "facilitating_factors": "Availability of pretrained long-document transformer models and datasets like CORWA to enable fine-tuning.",
            "contextual_requirements": "GPU compute for fine-tuning/inference and labeled long-document corpora for the target annotation task.",
            "generalizability": "High within long-document NLP tasks (scholarly articles, reports) but computationally intensive.",
            "knowledge_type": "explicit procedural steps and instrumental/technical skills (model fine-tuning and dataset preparation).",
            "uuid": "e411.9",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "NER integration",
            "name_full": "Named Entity Recognition pipelines adapted for scholarly data extraction",
            "brief_description": "Integration of NER modules (pre-trained or custom) into larger ML/NLP pipelines for identifying domain entities (e.g., measures, sample descriptors) in scholarly articles; used both as preprocessing and as core extraction elements in multiple included studies.",
            "citation_title": "Towards a Semi-Automated Approach for Systematic Literature Reviews Completed Research",
            "mention_or_use": "mention",
            "procedure_name": "NER-augmented extraction pipelines",
            "procedure_description": "Use of Named Entity Recognition (NER) systems—standalone or embedded—to tag and extract relevant named entities (e.g., instruments, populations, measures) from article text, often combined with ontologies, regex rules, or downstream classification models.",
            "procedure_type": "computational method / entity recognition & extraction",
            "source_domain": "general NLP and clinical-information extraction",
            "target_domain": "social sciences scholarly extraction and SLR pipelines",
            "transfer_type": "hybrid approach combining existing NER with domain adaptations",
            "modifications_made": "Linking NER outputs to domain-specific ontologies or lexica, retraining or fine-tuning NER models on domain corpora, and integrating NER into larger UI-driven or pipeline architectures for semi-automated extraction.",
            "transfer_success": "Described as versatile and increasingly used across included studies; used variably as preprocessing, core extraction, or embedded in pipelines—no uniform cross-domain performance metrics presented in the review.",
            "barriers_encountered": "Limited domain-specific labeled data for social sciences; differences in naming conventions and non-standardized reporting leading to lower out-of-the-box performance.",
            "facilitating_factors": "Open-source NER tools, modular pipeline designs, and availability of annotation UIs to create in-domain training data.",
            "contextual_requirements": "Annotated examples for fine-tuning, ontology/lexicon resources for mapping entity types, and integration code to connect NER outputs to downstream extractors.",
            "generalizability": "Potentially high if models are fine-tuned and linked to domain ontologies; out-of-the-box NER models often underperform without adaptation.",
            "knowledge_type": "explicit procedural steps (model fine-tuning, annotation pipelines) plus tacit domain knowledge for defining entity categories.",
            "uuid": "e411.10",
            "source_info": {
                "paper_title": "(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Contextual information retrieval in research articles: Semantic publishing tools for the research community",
            "rating": 2,
            "sanitized_title": "contextual_information_retrieval_in_research_articles_semantic_publishing_tools_for_the_research_community"
        },
        {
            "paper_title": "Extracting the characteristics of life cycle assessments via data mining",
            "rating": 2,
            "sanitized_title": "extracting_the_characteristics_of_life_cycle_assessments_via_data_mining"
        },
        {
            "paper_title": "Ontology-based and User-focused Automatic Text Summarization (OATS): Using COVID-19 Risk Factors as an Example",
            "rating": 2,
            "sanitized_title": "ontologybased_and_userfocused_automatic_text_summarization_oats_using_covid19_risk_factors_as_an_example"
        },
        {
            "paper_title": "MetaSeer.STEM: Towards Automating Meta-Analyses",
            "rating": 2,
            "sanitized_title": "metaseerstem_towards_automating_metaanalyses"
        },
        {
            "paper_title": "Searching for tables in digital documents",
            "rating": 2,
            "sanitized_title": "searching_for_tables_in_digital_documents"
        },
        {
            "paper_title": "Sysrev: A FAIR Platform for Data Curation and Systematic Evidence Review",
            "rating": 2,
            "sanitized_title": "sysrev_a_fair_platform_for_data_curation_and_systematic_evidence_review"
        },
        {
            "paper_title": "Detecting In-line Mathematical Expressions in Scientific Documents",
            "rating": 2,
            "sanitized_title": "detecting_inline_mathematical_expressions_in_scientific_documents"
        },
        {
            "paper_title": "DASyR (IR) -Document Analysis System for Systematic Reviews (in Information Retrieval)",
            "rating": 2,
            "sanitized_title": "dasyr_ir_document_analysis_system_for_systematic_reviews_in_information_retrieval"
        },
        {
            "paper_title": "CORWA: A Citation-Oriented Related Work Annotation Dataset",
            "rating": 2,
            "sanitized_title": "corwa_a_citationoriented_related_work_annotation_dataset"
        },
        {
            "paper_title": "Towards a Semi-Automated Approach for Systematic Literature Reviews Completed Research",
            "rating": 1,
            "sanitized_title": "towards_a_semiautomated_approach_for_systematic_literature_reviews_completed_research"
        },
        {
            "paper_title": "Automating systematic literature reviews with natural language processing and text mining: A systematic literature review",
            "rating": 1,
            "sanitized_title": "automating_systematic_literature_reviews_with_natural_language_processing_and_text_mining_a_systematic_literature_review"
        },
        {
            "paper_title": "Artificial intelligence and the conduct of literature reviews",
            "rating": 1,
            "sanitized_title": "artificial_intelligence_and_the_conduct_of_literature_reviews"
        }
    ],
    "cost": 0.027223499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review [version 2; peer review: 2 approved, 1 approved with reservations]
30 August 2024</p>
<p>Amanda Legate alegate@patriots.uttyler.edu 0000-0001-7763-7630
Kim Nimon 0000-0003-2543-8386
Ashlee Noblin 
Fred Oswald 
Lena Schmidt </p>
<p>Human Resource Development
The University of Texas at Tyler
75799TylerTexasUSA</p>
<p>Rice University
HoustonUSA</p>
<p>Biljana Macura
Stockholm Environment Institute
StockholmSweden</p>
<p>Newcastle University
Newcastle upon TyneUK</p>
<p>National Institute for Health and Care Research Innovation Observatory
Population Health Sciences Institute
Newcastle University
Newcastle upon TyneEngland, UK</p>
<p>National Institute for Health and Care Research Innovation Observatory
Population Health Sciences Institute
Newcastle University
Newcastle upon TyneEngland, UK</p>
<p>(Semi)automated approaches to data extraction for systematic reviews and meta-analyses in social sciences: A living review [version 2; peer review: 2 approved, 1 approved with reservations]
30 August 20249FD145B230B415DBCD37CEFB1E7D4BB010.12688/f1000research.151493.1Author roles: Legate A: Conceptualization, Data Curation, Formal Analysis, Investigation, Methodology, Project Administration, Resources, Validation, Visualization, Writing -Original Draft Preparation, Writing -Review &amp; EditingNimon K: Conceptualization, Data Curation, Formal Analysis, Investigation, Methodology, Resources, Software, Supervision, Validation, Visualization, Writing -Original Draft Preparation, Writing -Review &amp; EditingNoblin A: Data Curation, Formal Analysis, Investigation, Validation, Writing -Original Draft Preparation, Writing -Review &amp; Editing Automated data extraction, systematic review, meta-analysis, evidence synthesis, social science research, APA Journal Article Reporting Standards (JARS)
BackgroundAn abundance of rapidly accumulating scientific evidence presents novel opportunities for researchers and practitioners alike, yet such advantages are often overshadowed by resource demands associated with finding and aggregating a continually expanding body of scientific information.Data extraction activities associated with evidence synthesis have been described as time-consuming to the point of critically limiting the usefulness of research.Across social science disciplines, the use of automation technologies for timely and accurate knowledge synthesis can enhance research translation value, better inform key policy development, and expand the current understanding of human interactions, organizations, and systems.Ongoing developments surrounding automation are highly concentrated in research for evidence-based medicine with limited evidence surrounding tools and techniques applied outside of the clinical research community.The goal of the present study is to extend the automation knowledge base by synthesizing current trends in the application of extraction technologies of key data elements of interest for social scientists.MethodsWe report the baseline results of a living systematic review of automated data extraction techniques supporting systematic reviewsOpen Peer ReviewApproval Status1 2 3 version 2 (revision)</p>
<p>Introduction</p>
<p>Across disciplines, systematic reviews and meta-analyses are integral to exploring and explaining phenomena, discovering causal inferences, and supporting evidence-based decision making.The concept of metascience represents an array of evidence synthesis approaches which support combining existing research results to summarize what is known about a specific topic (Davis et al., 2014;Gough et al., 2020).Researchers use a variety of systematic review methodologies to synthesize evidence within their domains or to integrate extant knowledge bases spanning multiple disciplines and contexts.When engaging in quantitative evidence synthesis, researchers often supplement the systematic review with meta-analysis (a principled statistical process for grouping and summarizing quantitative information reported across studies within a research domain).As technology advances, in addition to greater access to data, researchers are presented with new forms and sources of data to support evidence synthesis (Bosco et al., 2017;Ip et al., 2012;Wagner et al., 2022).</p>
<p>Systematic reviews and meta-analyses are fundamental to supporting reproducibility and generalizability of research surrounding social and cultural aspects of human behavior, however, the process of extracting data from primary research is a labor-intensive effort, fraught with the potential for human error (see Pigott &amp; Polanin, 2020).Comprehensive data extraction activities associated with evidence synthesis have been described as time-consuming to the point of critically limiting the usefulness of existing approaches (Holub et al., 2021).Moreover, research indicates that it can take several years for original studies to be included in a new review due to the rapid pace of new evidence generation (Jonnalagadda et al., 2015).</p>
<p>The need for this review</p>
<p>In the clinical research domain, particularly in Randomized Control Trials (RCTs), automation technologies for data extraction are evolving rapidly (see Schmidt et al., 2023).In contrast with the more defined standards that have evolved throughout clinical research domains, within and across social sciences, substantial variation exists in research designs, reporting protocols, and even publication outlet standards (Davis et al., 2014;Short et al., 2018;Wagner et al., 2022).In health intervention research, targeted data elements generally include Population (or Problem), Intervention, Control, and Outcome (i.e., PICO; see Eriksen and Frandsen, 2018;Tsafnat et al., 2014).While experimental designs are considered a gold-standard for translational value, many phenomena examined across the social sciences occur within contexts which necessitate research pragmatism in both design and methodological considerations (Davis et al., 2014).</p>
<p>Consider, for example, the field of Human Resource Development (HRD).In HRD, a primary focal hub for research includes outcomes of workplace interventions intended to inform and improve areas such as learning, training, organizational development, and performance improvement (Shirmohammadi et al., 2021).While measuring intervention outcomes is a substantial area of discourse, HRD researchers have predominantly relied on cross-sectional survey data and the most commonly employed quantitative method is Structural Equation Modeling (Park et al., 2021).Thus, meta-analyses are increasingly essential for supporting reproducibility and generalizability of research.In these fields, data elements targeted for extraction would rarely align with the PICO framework, but rather, meta-analytic endeavors would entail extraction of measures such as effect sizes, model fit indices, or instrument psychometric properties (Appelbaum et al., 2018).</p>
<p>Related research</p>
<p>Serving as a model for the present study, Schmidt et al. (2023) are conducting a living systematic review (LSR) of tools and techniques available for (semi)automated extraction of data elements pertinent to synthesizing the effects of healthcare interventions (see Higgins et al., 2022).Exploring a range of data-mining and text classification methods for systematic reviews, the authors uncovered that early often employed approaches (e.g., rule-based extraction) gave way to classical machine-learning (e.g., naïve Bayes and support vector machine classifiers), and more recently, trends indicate increased application of deep learning architectures such as neural networks and word embeddings (for yearly trends in reported systems architectures, see Schmidt et al., 2021, p. 8).</p>
<p>In social sciences and related disciplines, several related reviews of tools and techniques for automating tasks associated with systematic reviews and meta-analyses have been conducted.Table 1 provides a summary of related research.</p>
<p>Based on extant reviews analyzing trends in Artificial Intelligence (AI) technologies for automating Systematic Literature Review (SLR) efforts outside of clinical domains, we noted several trends.First, techniques to facilitate abstraction, generalization, and grouping of primary studies represent the majority of (semi)automated approaches.Second, extant reviews highlight a predominant focus on supporting search and study selection stages, with significant gaps in (semi)automating data extraction.Third, evaluation concerns underscore the importance of performance metrics, validation procedures, benchmark datasets and improved transparency and reporting standards to ensure the reliability and effectiveness of AI techniques.Finally, challenges in cross-discipline transferability illuminate the need for domainspecific adaptations and infrastructures.</p>
<p>Existing reviews evidence the widespread application of techniques such as topic modeling, clustering, and classification to support abstraction, generalization, and grouping of primary research studies.Topic modeling, particularly Latent Dirichlet Allocation (LDA), is commonly applied to (semi)automate content analysis, facilitating the distillation of complex information into meaningful insights and identification of overarching trends and patterns across a literature corpus (Antons et al., 2020;Dridi et al., 2021;Roldan-Baluis et al., 2022;Yang et al., 2023).Additionally, classification and clustering techniques are commonly applied for tasks such as mining article metadata and automatically grouping papers by relevance to SLR research questions are (Feng et al., 2017;Sundaram &amp; Berleant, 2023;Wagner et al., 2022).</p>
<p>(Semi)automation efforts in social sciences and related disciplines have primarily addressed supporting the search and study selection stages of SLRs (Cairo et al., 2019;Feng et al., 2017), with significant gaps in automation techniques for tasks such as data extraction (Göpfert et al., 2022;Sundaram &amp; Berleant, 2023).Further, available software tools lack functionality to support activities beyond study selection (Kohl et al., 2018).Key findings across these reviews underscore the need for more comprehensive automation solutions, particularly for quantitative data extraction (Göpfert et al., 2022).</p>
<p>Additionally, researchers express transparency concerns regarding AI's reliance on black box models (Wagner et al., 2022) and limited visibility into underlying processes and algorithms in proprietary software solutions (Antons et al., 2020).Adding to these considerations, Antons et al. (2020) identified substantial reporting gaps, including 35 of 140 articles omitting details about software used.Since metrics alone may not be sufficient to objectively assess AI performance (Dridi et al., 2021), strategies for mitigating bias and ensuring transparency and fairness represent a substantial topic of automation discourse.</p>
<p>Ongoing research of AI tools for clinical studies (Sundaram &amp; Berleant, 2023) and the extraction of PICO data elements from RCTs (Wagner et al., 2022) underscore the success of domain-specific adaptation efforts.While the promise of adopting AI-based techniques and tools in social science domains is evident (Cairo et al., 2019;Feng et al., 2017), extant research reveals challenges in transferring existing technologies across disciplines.Further, many SLR software applications are tailored specifically for health and medical science research (Kohl et al., 2018).Literature suggests that overcoming global obstacles can be facilitated by concentrated efforts to develop domain-specific knowledge representations, such as standardized construct taxonomies and vocabularies (Feng et al., 2017;Göpfert et al., 2022;Wagner et al., 2022).</p>
<p>Objectives</p>
<p>In the present study, we conduct a baseline review of existing and emergent techniques for the (semi)automated data extraction which focus on target data entities and elements relevant to evidence synthesis across social sciences research domains.This review covers data extraction tools for a range of data types-both quantitative and qualitative.Per the research protocol, social sciences categories included in this review were based on the branches of science and academic activity boundaries described by Cohen (2021; Chapter 2).Additional description is available in the project repositories, see 'Data availability' section.We report findings that supplement the growing body of research dedicated to the automatic extraction of data from clinical and medical research.</p>
<p>Methods</p>
<p>Protocol registration</p>
<p>This LSR was conducted following a pre-registered and published protocol (Legate &amp; Nimon, 2023b).For additional details and project repositories, see 'Data availability' section.</p>
<p>Living review</p>
<p>We adopted the LSR methodology for this study primarily due to the pace of emerging evidence, particularly in light of ongoing technological advancements.The ongoing nature of an LSR allows for continuous surveillance, ensuring timely presentation of new information that may influence findings (Elliott et al., 2014(Elliott et al., , 2017;;Khamis et al., 2019).This baseline review was initiated upon peer approval of the associated protocol (Legate &amp; Nimon, 2023b).It remains our intent for the review to be continually updated via living methodological surveys of published research (Khamis et al., 2019) following the workflow schedule as previously published in the protocol (see Figure 1; Legate &amp; Nimon, 2023b).Necessary adjustments to the workflow will be detailed within each subsequent update.</p>
<p>Eligibility criteria</p>
<p>As in prior reviews, English language reports, published 2005 or later were considered for inclusion (Jonnalagadda et al., 2015;O'Mara-Eves et al., 2015;Schmidt et al., 2020).Eligible studies utilized, presented, and/or evaluated semiautomated approaches to support evidence-synthesis research methods (e.g., systematic reviews, psychometric metaanalyses, meta-analyses of effect sizes, etc.</p>
<p>Search sources</p>
<p>The search strategy for this review was developed by adapting the search strategy from a related LSR of clinical research (Schmidt et al., 2020).</p>
<p>We initially intended to conduct searches in the same databases used by Schmidt et al. (2020Schmidt et al. ( , 2021))  Study selection Title, abstract, and full-text screening was conducted using Rayyan (Ouzzani et al., 2016; free and subscription accounts available at https://www.rayyan.ai/).Three researchers (1000 abstracts per week) screened all titles and abstracts.</p>
<p>Researchers met weekly to review, resolve conflicts, and further develop the codebook for this LSR.All conflicts that arose during the title and abstract screening (n=103/N=10,644) were resolved on a weekly basis.Where disagreements arose, they were related to methods for abstractive text summarization and transferability of methods applied to clinical research studies (i.e., RCTs).In cases where level of abstraction and potential for transferability could not be determined from the abstract alone, full text articles were reviewed and discussed by all three researchers until consensus was reached.</p>
<p>For the data extraction stage, a Google form was developed following items of interest as described in the protocol.All data extraction tasks were performed independently in triplicate.Researchers met weekly to review and reach a consensus on coding of extracted items of interest.The extraction form was updated over the course of data extraction to better fit project goals and promote reliability of future updates.</p>
<p>We originally intended to conduct Inter-Rater Reliability (IRR) assessments to provide reliability estimates following each stage of the baseline review (Belur et al., 2018;Zhao et al., 2022).Given the nascency of our research and scope of our items of interest, coding forms allowed for input of "other" responses (e.g., APA data elements) that were not included in extant reviews that focus on medical and clinical data extraction (e.g., PICO elements).Further, data extraction presented opportunities to develop reporting structure for methods and items of interest that were not reported in prior literature (e.g., NER, open-source tools).A weekly review meeting was used to continually develop the project codebook to promote continuity, structure, and develop an IRR framework for future iterations of this review.</p>
<p>Results</p>
<p>Search results</p>
<p>Search results are presented in the PRISMA flowchart (see Figure 2).A total of 11,336 records were identified through all search sources, including databases and publications available through the Systematic Review Toolbox (Marshall et al., 2022).After deduplication, 10,644 articles were included in the title and abstract screening stage.We retrieved 46 articles for full-text screening.One duplicate print was detected during full text screening and was removed.This iteration of the LSR includes 23 articles.Detailed description of deduplication and preliminary screening procedures are available in the OSF project repository (see 'Data availability' section).</p>
<p>The following sections describe the rationale for exclusions, followed by a brief overview of studies included in the baseline review.These results are presented in Figures 3 and 4, respectively.An overview of included studies is presented in Table 2.</p>
<p>Excluded publications</p>
<p>Most studies were excluded due to lack of detail in extracted data entities (n=7) and wrong corpus or data source (n=7).Carrión-Toro et al. (2022), for example, developed a method and software tool supporting researchers with selection of relevant key criteria in a field of study based on term frequencies.While text summarization has proven valuable for evidence synthesis tasks, the primary focus of this LSR involves efforts to extract specific data points from primary research (O'Connor et al., 2019).We also excluded extraction techniques that were not applied to abstracts or full text of research articles.Ochoa-Hernández et al. (2018), for instance, presented a method to automatically extract concepts from web blog articles.</p>
<p>The second most common exclusion category were articles that presented techniques or systems utilizing pre-extracted data (n=4).Ali and Gravino (2018), for example, proposed an ontology-based SLR system with semantic web technologies; however, the data (derived from a prior review conducted by the authors) were added to the ontology system after the manual extraction stage.Finally, articles were excluded due to exclusive application in medical/clinical research (n=2), or the proposed tool had not yet been implemented (n=2).Goswami et al. (2019), for example, described and evaluated a supervised ML framework to identify and extract anxiety outcome measures from clinical trial articles.Zhitomirsky-Geffet et al. (2020) presented a conceptual description of a network-based data model capable of mining quantitative results from social sciences articles, but the system had not been implemented at the time of publication.</p>
<p>Included publications</p>
<p>The majority of included studies (n=12) presented or described a software tool, system, or application to support researchers extracting data from research literature.The second most common inclusion category focused on the development of specialized techniques or methods for automating data extraction tasks (n=9).We identified two studies that evaluated or tested the performance of existing tools or methods for (semi)automated data extraction.Unlike related reviews of data extraction methods for healthcare interviews (see Schmidt et al., 2023), we did not identify social science studies applying existing automated data extraction tools to conduct secondary research.</p>
<p>Automated approaches</p>
<p>To report approaches identified, we organized the extracted data under four overarching categories, including: (1) data preprocessing and feature engineering, (2) model architectures and components, (3) rule-bases, and (4) evaluation metrics.See 'Data Availability' section for labeling and additional descriptions of techniques.We opted to extract and report rule-based techniques separately because the approaches we identified intertwined with various aspects of the data processing and extraction pipeline, spanning data preprocessing to the model architecture itself.This distinction allows for more discussion about the prevalence, scope and utility of these techniques.</p>
<p>Data preprocessing and feature engineering</p>
<p>The data preprocessing category encompasses methods and techniques used to preprocess raw text and data before it is fed into ML/NLP models.This includes tasks such as tokenization, stemming, lemmatization, stop word removal, and other steps necessary to clean and prepare the text data for analysis.Figure 5 plots the aggregate results of preprocessing techniques identified.</p>
<p>Nearly all studies applied tokenization and/or segmentation (83%, n=19) for breaking down text into manageable units.</p>
<p>Similarly, PDF parsing/extraction techniques were applied in 65% (n=15) of studies, the remaining studies applied extraction to other document formats (e.g., journal articles available online in HTML format; see Diaz-Elsayed &amp; Zhang, 2020).While similar methods, which additionally take into account syntactic structure, including chunking and dependency parsing were less frequently applied (Angrosh et al., 2014;Li et al., 2022;Nayak et al., 2021;Pertsas &amp; Constantopoulos, 2018).Tagging methods, including PoS tagging (assigning grammatical categories, e.g., noun, verb), followed by concept tagging (e.g., semantic annotation), or sequence tagging, where labels were assigned based on order of appearance, were used in 43% (n=15) of studies.Nine studies used manual annotation for training and/or evaluation.</p>
<p>Among noise reduction approaches, stop-word removal was the most common, stemming, normalization, and lemmatization were applied, though less frequently.For stemming approaches, the Porter stemmer (Porter, 1980), including its extensions (e.g., Porter2, S-stemmer, snowball stemmer), were as commonly reported as traditional stemmers (see Aumiller et al., 2020;Bayatmakou et al., 2022;Shahid &amp; Afzal, 2018).Optical Character Recognition (OCR) appeared in three studies, however, Iwatsuki et al. ( 2017) used OCR only as a benchmark for evaluating their CRF method for detecting math expressions.</p>
<p>Feature engineering (e.g., ranking functions, representation learning and feature extraction techniques) covers a range of methods essential for transforming raw text data into structured, machine-readable representations to facilitate downstream ML/NLP tasks (Kowsari et al., 2019).See Figure 6.</p>
<p>Word embeddings were the most frequently used techniques.We grouped ELMo (word embeddings from language models) with traditional word embeddings such as Word2Vec and Glove (Kowsari et al., 2019;Young et al., 2018,</p>
<p>Model architectures and components</p>
<p>The model architecture category focuses on the architectures and components of ML/NLP models used for data extraction.Results are shown in Figure 7.Some approaches overlapped across applicationse.g., semantic web or semantic indexing structures and ontology-pipeline approacheswe grouped these techniques into categories to facilitate reporting.Likewise, all transformer-based approaches were grouped into a single category, however, specific architectures and components are discussed in the sections below, and detailed coding of extracted data is available in the supplemental data files (see 'Underlying Data' section).Where ruled-based approaches were considered a part of the system architecture, they are reported under the 'Rule-bases' section.</p>
<p>Overall, approaches ranged from straightforward implementations to complex layered architectures.Examples of more straightforward approaches included architectures based entirely on rule-bases (e.g., Diaz-Elsayed &amp; Zhang, 2020), applications based one classification method (e.g., naïve Bayes; Neppalli et al., 2016), or those utilizing a single type of probabilistic model (Angrosh et al., 2014;Iwatsuki et al., 2017).At the other end of the complexity continuum, Nowak and Kunstman ( 2019) presented an end-to-end deep learning model based on a BI-LSTM-CRF architecture with interleaved alternating LSTM layers and highway connections.In the following sections, we further elaborate on various approaches identified.</p>
<p>Ontology-based and Semantic Web.These pipelines involve leveraging ontologies and semantic web technologies for semantic annotation or knowledge representation.Among included studies, ontology and semantic web capabilities were explored as early as 2014, but the preliminary results from this baseline review suggest an upward trend in recent years.</p>
<p>Evaluation metrics</p>
<p>Evaluation metrics are presented in Figure 9. Precision, recall, F-scores, and accuracy were predominantly reported across studies, including the earliest published articles.For assessment of model performance, six studies used crossvalidation (CV), a process of "averaging several hold-out estimators of the risk corresponding to different data splits" (Arlot &amp; Celisse, 2010, p. 53).K-fold CV (5 or 10 folds) was predominantly applied (Angrosh et al., 2014;Iwatsuki et al., 2017;Neppalli et al., 2016;Shen et al., 2022, with</p>
<p>Availability, accessibility and transferability</p>
<p>While only one study we reviewed presented an existing tool that was accessible to users through an online application (sysrev.com;Bozada et al., 2021) at the time of conducting this baseline review, two other studies were either being prepared or were available through other means.These included the Holistic Modifiable Literature Review tool (Denzler et al., 2021) 2020) affirmed the domain-independent nature of their framework, suggesting its suitability for various systematic reviews.</p>
<p>Additionally, other studies highlighted the need for transferability and discussed the potential for their research tools and technologies to be extended and adapted across varying domains, stressing the importance of flexible design principles in the development of these tools (Angrosh et al., 2014;Diaz-Elsayed &amp; Zhang, 2020).Angrosh et al. ( 2014) explained how SENTCON's preliminary design was applied to a specific set of articles in computer science but emphasized that the tool was flexible enough to be applied to other domains through the use of the Web Ontology Language (OWL).Diaz-Elsayed &amp; Zhang (2020) presented methods that were initially applied to wastewater-based resource recovery, but likewise emphasized that the tool was capable of evaluating other engineered systems and retrieving different types of data than those initially extracted.</p>
<p>As noted by Chen et al. ( 2021), while efforts are being made to assist the process of conducting systematic reviews there is often limited generalizability of domain-specific pre-trained language models.Many studies included in our review dedicated discussion points toward addressing the critical issue of generalizability and transferability of tools developed to support the broader research community in (semi)automated data extraction tasks.Collectively, these studies suggest a positive trend toward the development of adaptable, transferable research tools and technologies.However, they also underscore the need for ongoing effort across and between diverse domains to make continued progress toward broader research applications.</p>
<p>Open source tools</p>
<p>APA data elements</p>
<p>This section discusses potential for extraction of key data elements of interest, as well as locations (i.e., paper sections), structures, and review tasks addressed by the studies reviewed.We limited this section to reporting tools that users could theoretically access and use to support their own research projects.There were 12 studies that presented systems or artifacts designed to facilitate various tasks associated with identifying and extracting data from published literature.</p>
<p>To avoid speculating as to the future availability of these tools, we included all studies which presented tools or systems where authors incorporated user interfaces (UIs), regardless of availability at the time of conducting this base review.</p>
<p>Structure, location, and review tasks</p>
<p>Table 5 provides an overview of structure and location of extracted data elements, followed by review tasks supported by tools identified.The majority developed approaches for (semi)automating extraction of data from any section of full text research articles.Two studies tested the proposed techniques on specific article sections, including titles and abstracts     et al., 2022).Regarding structure from which data were extracted, all except one extracted from unstructured text, two extracted from both tabular structures (i.e., tables) and text (Nayak et al., 2021;Pertsas &amp; Constantopoulos, 2018), and one was designed specifically to extract elements from tables (TableSeer;Liu et al., 2007).</p>
<p>All tools focused heavily on tasks related to data extraction (e.g., identification, labeling/annotation, ontology population), which was anticipated based on our search strategy and inclusion criteria.However, several studies advanced solutions for supporting other SLR tasks or stages (see Tsafnat et al., 2014).The most common task (excluding data extraction) was literature search (Bayatmakou et al., 2022;Bozada et al., 2021;Chen et al., 2020;Denzler et al., 2021;Liu et al., 2007;Pertsas &amp; Constantopoulos, 2018).Many tasks listed are likely supported by a range of computational tools and techniques (e.g., synthesize and meta-analyze results); readers interested in (semi)automating other SLR stages are referred to the Systematic Review Toolbox for an extensive catalogue of tools and methods (Marshall et al., 2022).</p>
<p>Challenges</p>
<p>A number of challenges were reflected within the body of evidence included in this baseline review.These challenges included difficulties in identifying functional structures within unstructured texts (Shen et al., 2022), extracting data from PDF file sources (Nayak et al., 2021;Goldfarb-Tarrant et al., 2020;Iwatsuki et al., 2017), and accurately detecting in-line mathematical expressions (Iwatsuki et al., 2017).Computational complexity created another significant obstacle for researchers, with issues arising from text vectorization methods, optimization problems, and the computational resources required by neural network frameworks (Bayatmakou et al., 2022;Anisienia et al., 2021).Furthermore, challenges associated with annotation, particularly biases introduced through the automated processes and limitations of available datasets, were a topic of discourse (Li et al., 2022;Nowak &amp; Kunstman, 2019;Torres et al., 2012).</p>
<p>Compared to the medical field, domain-specific challenges, particularly those in social sciences and related fields, necessitated tailored approaches, which can become time-consuming as researchers often lack sufficient training data to develop robust classifiers (Chen et al., 2021;Aumiller et al., 2020;Zielinski &amp; Mutschke, 2017).Additionally, metaanalytic methods often face hurdles related to data representation variability, which has significant limitations in the use of data extraction tools, and class imbalance in the development of classification tasks (Aumiller et al., 2020;Neppalli et al., 2016;Goldfarb-Tarrant et al., 2020).</p>
<p>Conclusions</p>
<p>The findings of the baseline review indicate that when considering the process of automating systematic review and meta-analysis information extraction, social science research falls short as compared to clinical research that focuses on automatic processing of information related to the PICO framework (i.e., Population, Intervention, Control, and Outcome;Eriksenand Frandsen, 2018;Tsafnat et al., 2014).For example, while an LSR focusing on clinical research that is based on the PICO framework yielded 76 studies that included original data extraction (Schmidt et al., 2023), the present review of social science research yielded only 23 relevant studies.This is not necessarily surprising when considering the breadth of social science research and the lack of unifying frameworks and domain specific ontologies (Göpfert et al., 2022;Wagner et al., 2022).</p>
<p>With a few exceptions, most tools we identified were either in the infancy stage and not accessible to applied researchers, were domain specific, or require substantial manual coding of articles before automation can occur.Additionally, few solutions considered extraction of data from tables, which is where many elements (e.g., effect sizes) reside that social and behavioral scientists analyze.Further, development appears to have ceased for many of the tools identified.</p>
<p>We found no evidence indicating hesitation on the part of social science researchers to adopt data extraction tools, on the contrary, abstractive text summarization approaches continue to gain traction across social science domains (Cairo et al., 2019;Feng et al., 2017).While these methods aid researchers in distilling complex information into meaningful insights, there remains a gap in technologies developed to augment human capabilities in the extraction of key data entities of interest for secondary data collection from quantitative empirical reports.</p>
<p>The impact of time-intensive research activities on translational value is not a new concern for the SLR research community.In many social sciences, emphasis is often placed on practical application and translational value, underscoring the importance of efficient research methodologies (Githens, 2015).Further development of the identified systems and techniques could mitigate time delays that often result in outdated information as researchers cannot feasibly include all new evidence that may be released throughout the lifetime of a given project (Marshall &amp; Wallace, 2019).</p>
<p>Limitations</p>
<p>As with any method that involves subjectivity, results can be influenced by a variety of factors (e.g., study design, publication bias, researcher judgment, etc.).We worked diligently to conduct this review and document our procedures in a systematic and transparent manner; however, efforts to replicate our search strategy or screening processes may not result in the same corpus or reach the same conclusions (Belur et al., 2018).This baseline review presented an opportunity to better develop our search and screening strategy, methodological procedures, and research goals.Moving forward, we have developed a codebook and assessment procedures to increase the transparency and reliability of our research.</p>
<p>A second limitation of this study was the omission of snowballing as a search strategy.Though we did not uncover applied secondary research articles utilizing automation tools, several potentially useable tools and systems were discovered in the course of this review.For future iterations of this LSR, we plan to incorporate forward snowballing from relevant articles in previous searches as part of our formalized search strategy (see Wohlin et al., 2022).Additionally, our search strategy has limitations related to its focus on English-language publications, the non-exhaustive list of databases and sources consulted, and the exclusion of grey literature.Addressing these aspects in future updates could enhance the comprehensiveness of findings and provide a broader perspective on the current state of automation tools in secondary research.</p>
<p>Reporting guidelines</p>
<p>This study follows PRISMA reporting guidelines (Page et al., 2021).</p>
<p>Open The reference from Yu et al. below is mostly concerning screening automation and not data extraction if I am not missing a major point in the paper.If that is correct then there may exist better works to reference in this context?"the process of extracting data from primary research is a labor-intensive effort, fraught with the potential for human error (see Pigott &amp; Polanin, 2020;Yu et al., 2018)."I am not an expert in social science research, but a few included references in  2021) about cotton industry?Regarding this sentence in the conclusions, it might be more up-to-date to reference the review update from 2023 with 76 included papers: "For example, while an LSR focusing on clinical research that is based on the PICO framework yielded 53 studies that included original data extraction (Schmidt et al., 2021)" One of the challenges with living updates is to adapt the search whenever there are new developments in a field of research.You may have already considered adapting the search strategy to make sure that new methods relying on large language models (LLM) like GPT or T5 are picked up?There may be relevant articles coming through soon, for example https://arxiv.org/abs/2405.14445 may be of interest for a future review update as it looks at social science study data extraction and if it is, then it would be good to make sure that the search can pick up the terminology correctly.In the methodology section, could you please state the dates when the search relevant to the baseline review cutoff was conducted (for each data source if different) ?"</p>
<p>Are the rationale for, and objectives of, the Systematic Review clearly stated?Yes</p>
<p>Are sufficient details of the methods and analysis provided to allow replication by others?Yes</p>
<p>Is the statistical analysis and its interpretation appropriate?Yes</p>
<p>Are the conclusions drawn adequately supported by the results presented in the review? Yes</p>
<p>If this is a Living Systematic Review, is the 'living' method appropriate and is the search schedule clearly defined and justified?('Living Systematic Review' or a variation of this term should be included in the title.)Yes</p>
<p>Competing Interests: No competing interests were disclosed.</p>
<p>Reviewer Expertise: Systematic review automation, automated data extraction (clinical trials), natural language processing, living reviews I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.</p>
<p>Author Response 22 Sep 2024</p>
<p>Amanda Legate</p>
<p>We are honored that you agreed to review our research and sincerely appreciate your thoughtful review and feedback.Please find responses to each comment below.</p>
<p>Comment 1</p>
<p>The reference from Yu et al. below is mostly concerning screening automation and not data extraction if I am not missing a major point in the paper.If that is correct then there may exist better works to reference in this context?"the process of extracting data from primary research is a labor-intensive effort, fraught with the potential for human error (see Pigott &amp; Polanin, 2020;Yu et al., 2018) Comment 2: Response Thank you for your insights on the relevance of references in Table 2. Our search strategy was intentionally broad to include studies utilizing (semi)automated data extraction methods across various domains, provided they were not solely focused on clinical research.The goal was to ensure comprehensiveness; however, we understand your concern regarding the ambiguity of some references' relevance to social sciences.As our study is a "living" review, we see this as an excellent opportunity to consider refining our inclusion criteria in future updates.We will explore more targeted approaches that can help streamline the search strategy, potentially focusing on research that more directly applies to social sciences or explicitly demonstrates transferable methodologies that align with the needs of social science researchers.Additionally, we are discussing options for collaborating with experts who specialize in bibliometric analysis or search strategy optimization to ensure that our review remains focused, relevant, and complementary to your work.</p>
<p>Comment 3 Regarding this sentence in the conclusions, it might be more up-to-date to reference the review update from 2023 with 76 included papers: "For example, while an LSR focusing on clinical research that is based on the PICO framework yielded 53 studies that included original data extraction (Schmidt et al., 2021)".</p>
<p>Comment 3: Response Thank you for pointing out this important update, we appreciate your diligence in ensuring that references are current and reflective of the most recent findings.The manuscript has been revised to reference the 2023 update of your LSR to accurately reflect the most up-todate results.</p>
<p>Comment 4 One of the challenges with living updates is to adapt the search whenever there are new developments in a field of research.You may have already considered adapting the search strategy to make sure that new methods relying on large language models (LLM) like GPT or T5 are picked up?There may be relevant articles coming through soon, for example https://arxiv.org/abs/2405.14445 may be of interest for a future review update as it looks at social science study data extraction and if it is, then it would be good to make sure that the search can pick up the terminology correctly.</p>
<p>Comment 4: Response Thank you for highlighting the importance of adapting the search strategy to capture emerging developments in automation technologies, particularly those involving large language models (LLMs) like GPT or T5.We completely agree that a key aspect of maintaining the relevance and rigor of a living systematic review is to continuously update the search strategy to reflect the current state-of-the-art in the field.We will incorporate this valuable feedback into future iterations by updating our search terms and strategies to include LLM-related methodologies and terminologies, ensuring the inclusion of new and relevant articles.The paper you referenced (https://arxiv.org/abs/2405.14445)serves as an excellent example, and we will use it to refine our search criteria.This approach will help us stay current with advances in data extraction techniques.Thank you for providing specific references to guide this adaptation.</p>
<p>Comment 5</p>
<p>In the methodology section, could you please state the dates when the search relevant to the baseline review cutoff was conducted (for each data source if different) ?"</p>
<p>Comment 5: Response Thank you for this thoughtful suggestion.While we reported the search dates in the extended data files housed in OSF, we agree with you that including them directly in the Methods section would add clarity and value for readers.We have updated the section to specify the dates when searches were conducted for each data source, ensuring this information is clear and accessible to readers.</p>
<p>Competing Interests: No competing interests were disclosed.Since this review does not include any qualitative or quantitative synthesis per se, but rather provides an overview of the field (methods for semi-automated data extraction), I suggest removing "living systematic review" and adding "living systematic map." 2.</p>
<p>1.</p>
<p>Abstract:</p>
<p>The summary of methods could include more detailed information on searches, screening, critical appraisal, and synthesis.Please specify which standards for review conduct were followed.</p>
<p>1.</p>
<p>The summary of results could provide more information (briefly) about the included 2.</p>
<p>studies. Keywords:</p>
<p>Avoid repeating terms already present in the title 1.</p>
<p>3.</p>
<p>Introduction:</p>
<p>The focus of this review-extraction tools for quantitative data-should be more explicitly stated.This emphasis needs to be clearer in the introduction and reflected in the title, as mentioned earlier.Specifically, the first paragraph of the Introduction should be revised to concentrate on the review topic-quantitative data extraction and existing tools-rather than a general introduction to meta-science or related areas.</p>
<p>1.</p>
<p>Additional details are needed on how this review contributes to and complements existing reviews on the topic.This information should be included in the "Related Research" section.</p>
<p>2.</p>
<p>4.</p>
<p>Objectives:</p>
<p>It would be helpful to define what is included under "social science research domains".1.</p>
<p>5.</p>
<p>Methods:</p>
<p>Authors should be transparent and explicit about the guidelines and standards for both conduct and reporting that were used.Please clarify this at the beginig of the Methods section.</p>
<p>1.</p>
<p>The methods section should begin by addressing any deviations from the protocol.If there were no deviations, this should be clearly stated as well.</p>
<p>2.</p>
<p>Did you use any automation technologies to screen or select studies for this review?If yes, please clarify.</p>
<p>3.</p>
<p>6.</p>
<p>Methods/Eligibility criteria:</p>
<p>The eligibility criteria should be explicit about the field within which methods for (semi)automated data extraction are applied.</p>
<p>1.</p>
<p>A definition of "(semi)automated" is needed.The eligibility criteria currently state that semi-automated approaches will be eligible but then refer to "any automated approach to data extraction" in the next sentence.This needs to be clarified-are the focus and criteria on semi-automated or automated approaches?Be more explicit and precise in the description of the eligibility criteria, and ensure alignment with the protocol.</p>
<p>2.</p>
<p>Instead of "We excluded studies labeled as editorials, briefs.." you may write "Editorials, briefs, …were not considered eligible" (and similar changes may be applied to the following sentence) 3.</p>
<p>7.</p>
<p>Methods/Searches Be explicit about the citation indices included in your Web of Science subscription and note which library was used to access WoS.This will increase transparency and replicability of your searches.</p>
<p>1.</p>
<p>Clarify why following Schmidt et al.'s search strategy was important, given the different scope of this review.Consider including more social science databases to ensure comprehensive coverage.Did you include the Social Science Citation Index (within WoS)?</p>
<p>2.</p>
<p>Provide explanations for all abbreviations (IEEE, ACL, etc.) in the text.3.</p>
<p>8.</p>
<p>Methods/Study selection</p>
<p>Clarify if three researchers simultaneously screened titles and abstracts (TA), and whether inter-rater reliability (IRR) was calculated for TA screening.How you trained 1.</p>
<p>9.</p>
<p>reviewers to apply eligibility criteria?</p>
<p>The sentence, "In cases where level of abstraction and potential for transferability could not be determined from the abstract alone, full text articles were reviewed and discussed by all three researchers until consensus was reached", should more clearly state that there was NO full-text screening of all records (if this is correct), only of a sub-sample where abstracts did not clearly describe AI technology, etc.</p>
<p>2.</p>
<p>Relatedly, Figure 1 should be adjusted to avoid giving the false impression that all records were screened in full text.</p>
<p>3.</p>
<p>This review seems to involve META-data extraction rather than DATA extraction.Please adjust the text and figures accordingly.</p>
<p>4.</p>
<p>It is not clear if IRR assessments were conducted for meta-data extraction.Please clarify/be explicit.If IRR was not done, describe how researchers were trained to use the extraction form.</p>
<p>5.</p>
<p>The sentence, "coding forms allowed for input of "other" responses (e.g., APA data elements) that were not included in extant reviews that focus on medical and clinical data extraction (e.g., PICO elements)" is unclear.Consider removing or clarifying and linking it better with the rest of the text.</p>
<p>6.</p>
<p>Describe the procedure for screening and meta-data extraction of studies authored by the review team.</p>
<p>7.</p>
<p>Methods/Critical appraisal and Synthesis These sections are missing.Please state clearly if a critical appraisal of included studies was conducted and if so, how was it performed.Also, describe how synthesis was conducted.</p>
<p>1.</p>
<p>10.</p>
<p>Results/Challenges Clarify that the described challenges reflect issues within the body of evidence included in this (baseline) review (otherwise this section can be mixed up with review limitations).</p>
<p>1.</p>
<p>11.</p>
<p>Conclusions/Limitations</p>
<p>Organize limitations into those related to the methodology used and those related to the evidence base.</p>
<p>1.</p>
<p>Discuss limitations related to the focus on publications in English, the inexhaustive list of search sources, and the lack of grey literature.</p>
<p>2.</p>
<ol>
<li>We would like to express our genuine appreciation for the important work you and your colleagues have done in developing the ROSES (Reporting standards for Systematic Evidence Syntheses) guidelines for systematic evidence synthesis in environmental science.Improving transparency and standardization in research reporting is a goal we fully support.While we acknowledge the value of the ROSES guidelines, they were not the reporting standard required or appropriate for our systematic review.We noticed that many of your comments seem to assess our manuscript against the ROSES guidelines (Haddaway et al., 2017a;2017b;2018;Haddaway &amp; Macura, 2018).For example, the suggestion to emphasize "meta-data extraction" aligns more with ROSES, whereas PRISMA does not require such differentiation and focuses on clarity in describing the data collection process, whether it involves meta-data or primary data points.</li>
</ol>
<p>Are</p>
<p>We believe it is essential to assess our work based on the scope and framework provided by PRISMA rather than extend it beyond its current focus to fit an alternative reporting framework.Further, suggestions which are biased towards a reviewer's own work conflict with the expectations of fair peer review.We are committed to making revisions that enhance the clarity and rigor of our research while remaining consistent with the standards required by the journal.</p>
<p>Thank you again for your constructive feedback and for considering our clarifications.We look forward to moving forward in a way that upholds the standards of rigorous and unbiased scholarly review.</p>
<p>Comment 1 &amp; 2: Response Thank you for these valuable suggestions regarding the title.We have considered (1) specifying the type of data being extracted in the title and (2) changing the title from "living systematic review" to "living systematic map."However, we have retained the original title to ensure consistency with our pre-registered protocol, adhere to PRISMA reporting standards, and comply with F1000Research guidelines.</p>
<p>Comment 3</p>
<p>[Abstract] The summary of methods could include more detailed information on searches, screening, critical appraisal, and synthesis.Please specify which standards for review conduct were followed.</p>
<p>Comment 3: Response Thank you for the suggestion to provide more detailed information on searches, screening, critical appraisal, and synthesis in the abstract to better align with ROSES reporting recommendations.To ensure compliance with the journal's requirements, we followed the PRISMA guidelines for a structured summary, which emphasize conciseness in presenting objectives, eligibility criteria, methods, results, and conclusions.While we understand the desire for additional details, we believe the current abstract aligns with these guidelines but will review it again to ensure optimal clarity.</p>
<p>Comment 4</p>
<p>[Abstract] The summary of results could provide more information (briefly) about the included studies.</p>
<p>Comment 4: Response Thank you for the recommendation to provide more information about the included studies within the abstract.We will incorporate a brief summary of the included studies' key characteristics and findings in future updates to this review to enhance clarity and completeness.</p>
<p>Comment 5</p>
<p>[Keywords] Avoid repeating terms already present in the title Comment 5: Response Thank you for highlighting ROSES guidance indicating that keywords do not repeat the title but rather provide additional context.Where appropriate, we will revise keywords to avoid redundancy and enhance discoverability.</p>
<p>Comment 6</p>
<p>[Introduction] The focus of this review-extraction tools for quantitative data-should be more explicitly stated.This emphasis needs to be clearer in the introduction and reflected in the title, as mentioned earlier.Specifically, the first paragraph of the Introduction should be revised to concentrate on the review topic-quantitative data extraction and existing tools-rather than a general introduction to meta-science or related areas.</p>
<p>Comment 6: Response Thank you for your feedback on clarifying the focus of our review.Our study does not exclusively focus on extraction tools for quantitative data; it encompasses approaches to data extraction for both quantitative and qualitative data elements relevant to evidence synthesis in systematic reviews and meta-analyses within social sciences.To better reflect this broader focus, we have revised the objective section to explicitly state that the review covers data extraction tools for a range of data types.We hope this adjustment will provide clearer insight into the comprehensive scope of our review.</p>
<p>Comment 7</p>
<p>[Introduction] Additional details are needed on how this review contributes to and complements existing reviews on the topic.This information should be included in the "Related Research" section.</p>
<p>Comment 7: Response Thank you for this insightful comment.We agree on the importance of clearly situating our review within the existing literature to highlight its unique contributions.Although we did not adhere to ROSES guidelines for explaining the review's relevance to existing literature, we followed PRISMA guidelines in the "Related Literature" section to identify relevant prior reviews and synthesize their focus, findings, and limitations.We will consider ways to enhance this section to better emphasize our review's distinct contributions moving forward.</p>
<p>Comment 8</p>
<p>[Objectives] It would be helpful to define what is included under "social science research domains".</p>
<p>Comment 8: Response Thank you for this suggestion.Our pre-registered research protocol and the "Baseline Review Search Strategy" document (available in the project's OSF repository) provide a comprehensive list of over 100 subject categories included under social science research domains, ranging from sociology and political science to interdisciplinary areas such as "Social Sciences Mathematical Methods."To enhance clarity, we have updated the objectives section to include more details and a reference to the extended data file.</p>
<p>Comment 9</p>
<p>[Methods] Authors should be transparent and explicit about the guidelines and standards for both conduct and reporting that were used.Please clarify this at the beginning of the Methods section.</p>
<p>Comment 9: Response We acknowledge that the ROSES guidelines recommend transparency in reporting the guidelines and standards for both conduct and reporting at the beginning of the Methods section.However, in accordance with the journal's article guidelines for living systematic reviews (available from: https://f1000research.com/for-authors/article-guidelines/livingsystematic-reviews),this information is provided in the "Reporting Guidelines" section.</p>
<p>Comment 10</p>
<p>[Methods] The methods section should begin by addressing any deviations from the protocol.If there were no deviations, this should be clearly stated as well.</p>
<p>Comment 10: Response Thank you for highlighting this important aspect.While we did not adopt the ROSES reporting standards for this research, we recognize their guidance on stating any deviations from the protocol at the beginning of the methods section.We have addressed any deviations in the appropriate sections of the paper, and additional descriptions are provided in the extended data files to ensure transparency and replicability.</p>
<p>Comment 11</p>
<p>[Methods] Did you use any automation technologies to screen or select studies for this review?If yes, please clarify.</p>
<p>Comment 11: Response Thank you for your question.The use of automation technologies is detailed in the "Search Sources" and "Study Selection" subsections of the Methods section.Additionally, to ensure transparency and replicability, further details are provided in the "Software Availability" section, as per F1000Research guidelines.</p>
<p>Comment 12 [Methods/Eligibility criteria] The eligibility criteria should be explicit about the field within which methods for (semi)automated data extraction are applied.</p>
<p>Comment 12: Response Thank you for this comment.To ensure clarity, we have referenced the extended data files in the text, which provide comprehensive details and a full list of over 100 research fields.These details are openly available in the project repository, as specified in the protocol (please see response to Comment 8).</p>
<p>Comment 13 [Methods/Eligibility criteria] A definition of "(semi)automated" is needed.The eligibility criteria currently state that semi-automated approaches will be eligible but then refer to "any automated approach to data extraction" in the next sentence.This needs to be clarified-are the focus and criteria on semi-automated or automated approaches?Be more explicit and precise in the description of the eligibility criteria and ensure alignment with the protocol.</p>
<p>Comment 13: Response Thank you for your suggestion to clarify the phrasing regarding the eligibility criteria.We have revised the description to specify that the focus is on any "technique" applied for extracting data from literature in a semi-automated manner.This adjustment aligns with the study protocol.</p>
<p>Comment 14 [Methods/Eligibility criteria] Instead of "We excluded studies labeled as editorials, briefs.." you may write "Editorials, briefs, …were not considered eligible" (and similar changes may be applied to the following sentence).</p>
<p>Comment 14: Response Thank you for the suggestion.We have revised the text to use passive construction, as recommended.We have also applied similar changes to the following sentence for consistency.</p>
<p>Comment 15 [Methods/ Searches] Be explicit about the citation indices included in your Web of Science subscription and note which library was used to access WoS.This will increase transparency and replicability of your searches.</p>
<p>Comment 15: Response Thank you for this suggestion.To avoid redundancy in the manuscript, we have added a statement directing readers to the extended data files, which provide additional detail related to WoS indices and search settings.</p>
<p>Comment 16 [Methods/ Searches] Clarify why following Schmidt et al.'s search strategy was important, given the different scope of this review.</p>
<p>Comment 16: Response Thank you for this comment.To clarify, we followed Schmidt et al.'s search strategy to ensure comprehensive coverage of relevant databases and consistency in methodological rigor, which is important even with a different scope.To avoid redundancy, we have added a reference in the manuscript directing readers to the extended data file and research protocol in our open-access repository, where this rationale is explained in detail.</p>
<p>Comment 17 [Methods/ Searches] Consider including more social science databases to ensure comprehensive coverage.</p>
<p>Comment 17: Response Thank you for this valuable suggestion.We appreciate the importance of comprehensive coverage and will consider including additional social science databases in future updates to further enhance the scope of our review.</p>
<p>Comment 18 [Methods/ Searches] Did you include the Social Science Citation Index (within WoS)?</p>
<p>Comment 18: Response Yes, the Social Science Citation Index within Web of Science was included.We have updated the text to clarify that all editions, settings, and search syntax used are detailed in the extended data files available in the open-access repository.</p>
<p>Comment 19 [Methods/ Searches] Provide explanations for all abbreviations (IEEE, ACL, etc.) in the text.</p>
<p>Comment 19: Response Thank you for the suggestion.We have added explanations for all source abbreviations (e.g., IEEE, ACL) in the text to improve clarity for readers.</p>
<p>Comment 20 [Methods/ Study selection] Clarify if three researchers simultaneously screened titles and abstracts (TA), and whether inter-rater reliability (IRR) was calculated for TA screening.How you trained reviewers to apply eligibility criteria?</p>
<p>Comment 20: Response Thank you for your question.The "Study Selection" section of the paper details independent screening procedures, training process for reviewers on applying eligibility criteria, and inter-rater reliability (IRR) considerations.</p>
<p>Comment 21 [Methods/ Study selection] The sentence, "In cases where level of abstraction and potential for transferability could not be determined from the abstract alone, full text articles were reviewed and discussed by all three researchers until consensus was reached", should more clearly state that there was NO full-text screening of all records (if this is correct), only of a sub-sample where abstracts did not clearly describe AI technology, etc.</p>
<p>[Methods/ Study selection] Describe the procedure for screening and meta-data extraction of studies authored by the review team.</p>
<p>Comment 26: Response Thank you for highlighting ROSES guidance surrounding procedures for handling studies authored by the review team.However, no alternative procedures were implemented; therefore, there are no additional procedures to report.</p>
<p>Comment 27 [Methods/ Critical appraisal and Synthesis] These sections are missing.Please state clearly if a critical appraisal of included studies was conducted and if so, how was it performed.Also, describe how synthesis was conducted.</p>
<p>Comment 27: Response Thank you for your comment.These sections are specific to ROSES guidelines.These sections are not required by PRISMA or the journal's reporting standards.</p>
<p>Comment 28 [Results/Challenges] Clarify that the described challenges reflect issues within the body of evidence included in this (baseline) review (otherwise this section can be mixed up with review limitations).</p>
<p>Comment 28: Response To avoid confusion with review limitations, we have revised the first sentence of this section to clarify that the challenges discussed specifically reflect issues within the body of evidence included in this baseline review.</p>
<p>Comment 29 [Conclusions/Limitations] Organize limitations into those related to the methodology used and those related to the evidence base.</p>
<p>Comment 29: Response While our research and protocol were developed following PRISMA guidelines rather than ROSES, which requires a structured discussion of limitations, we appreciate the value of differentiating between methodological constraints and evidence base gaps.We will consider this distinction in future updates to enhance clarity.</p>
<p>Comment 30 [Conclusions/Limitations] Discuss limitations related to the focus on publications in English, the inexhaustive list of search sources, and the lack of grey literature.</p>
<p>Comment 30: Response Thank you for the suggestion.We have updated the limitations section to address the focus on publications in English, the inexhaustive list of search sources, and the lack of grey literature.</p>
<p>If this is a Living Systematic Review, is the 'living' method appropriate and is the search schedule clearly defined and justified?('Living Systematic Review' or a variation of this term should be included in the title.)Yes</p>
<p>Competing Interests: No competing interests were disclosed.</p>
<p>I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.</p>
<p>The benefits of publishing with F1000Research:</p>
<p>Your article is published within days, with no editorial bias • You can publish traditional articles, null/negative results, case reports, data notes and more •</p>
<p>The peer review process is transparent and collaborative • Your article is indexed in PubMed after passing peer review • Dedicated customer support at every stage • For pre-submission enquiries, contact research@f1000.com</p>
<p>Figure 1 .
1
Figure1.LSR workflow.This image is reproduced under the terms of a Creative Commons Attribution 4.0 International license (CC-BY 4.0) from Legate and Nimon (2023b).Note.Arrows represent stages involved in a static systematic review; the dotted line (from "Publish Report" to "Search") represents the stage at which the review process is repeated from the beginning while the review remains in living status.</p>
<p>Figure 2 .
2
Figure 2. PRISMA diagram.Note.ACL=ACL Anthology (https://aclanthology.org/),arXiv=arXiv Research-Sharing Platform (https://arxiv.org/),DBLP=DBLP Computer Science Bibliography (https://dblp.org/),WOS=Web of Science Core Collection, SRTool=Systematic Review Toolbox (http://systematicreviewtools.com/).</p>
<p>Figure 3 .
3
Figure 3. Excluded publications.Note.Domain = exclusively medical, biomedical, clinical, or natural science (n=2); Target entities = Lack of detail in reporting extracted entities (n=7); Application = no application, testing, or extraction conducted manually (n=6); Lack of Detail in Reporting Corpus or Wrong Corpus (n=7).</p>
<p>Figure 4 .
4
Figure 4. Included publications.Note.Presented Tool = Describe/demonstrated a software tool, system, or application for data extraction (n=12), Developed Method = Developed techniques and/or methods for automated data extraction (n=9); Evaluated Techniques = Tested or evaluated the performance of existing tools, techniques, or methods (n=2); Applied Tool = Applied automation tools to conduct secondary research (n=0).</p>
<p>Chapter 6).Of these, GloVe was used in four studies(Chen et al., 2021;Goldfarb-Tarrant et al., 2020; Nowak &amp;  Kunstman, 2019; Anisienia et al., 2021)  and ELMo in two(Nowak &amp; Kunstman, 2019; Anisienia et al., 2021).The most common frequency-based feature representation approaches were Bag-of-Words (BoW, n=5) and Term frequency-Inverse Document Frequency (TF-IDF, n=4).Although less frequently applied in the corpus, methods for representing words or documents as vectors based on semantic properties such as Vector Space Models (VSM) and sentence embeddings were used as early as 2007.Other less commonly reported methods included synonym aggregation/ expansion, best match ranking (BM25), shingling, and subject-verb pairings.</p>
<p>Figure 5 .
5
Figure 5. Data preprocessing.</p>
<p>Figure 6 .
6
Figure 6.Feature engineering.</p>
<p>types of sentences (e.g., objective, results, conclusions) and by Goldfarb-Tarrant et al. (2020) for splitting papers into specific sections (e.g., abstract, introduction, methods).Alternatively, Pertsas and Constantopoulos (2018) used RegEx to exploit lexico-syntactic patterns derived from an ontology knowledge base (Activities, Goals, and Propositions).Other RegEx uses included modifying datasets to incorporate patterns related to citation mentions (Anisienia et al., 2021) or application of rule-based chunking and processing to identify and extract relevant chunks from text (Nayak et al., 2021).The remaining six studies described custom rule-based algorithms or other heuristic approaches.Li et al. (2022), for example, applied rule-based algorithms PrefixSpan and Gap-Bide for the extraction of frequent discourse sequences.RAKE (Rapid Automatic Keyword Extraction) was applied by Bayatmakou et al. (2022) to extract keywords which served as representations of a document's content.And Aliyu et al. (2018) described a rule-based algorithm developed for processing full-text documents to identify and extract section headings.</p>
<p>Figure 8 .
8
Figure 8. Rule-bases.</p>
<p>Figure 9 .
9
Figure 9. Evaluation metrics.</p>
<p>An outcome we did not anticipate was the substantial number of open source tools, toolkits, and frameworks utilized by our relatively small corpus of articles.Because we were unsure what to expect, we made every effort to capture evidence that might prove useful to social science researchers.We identified 50 different open source technologies including platforms, software, software suites, packages/libraries, algorithms, pre-trained models, controlled vocabularies/thesauri, lexical databases, knowledge representations, and more.Open source tools identified are reported in Figure10.Of the open source resources available to researchers, the overwhelming majority were Python tools (n=16; see Python Package Index, https://pypi.org/)and 8 of 23 (35%) studies used the Python Natural Language Toolkit (NLTK).The full list of open-source tools and license details are available in the OSF repository (see 'Underlying Data' section).</p>
<p>Figure 10 .
10
Figure 10.Open source tools.</p>
<p>(</p>
<p>Bayatmakou et al., 2022) and introduction and background sections (Li</p>
<p>Reviewer Report 19
19
August 2024 https://doi.org/10.5256/f1000research.166140.r298402© 2024 Macura B. This is an open access peer review report distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.Biljana Macura 1 Stockholm Environment Institute, Stockholm, Sweden 2 Stockholm Environment Institute, Stockholm, SwedenThis manuscript represents an important contribution to the evidence synthesis methodology.Given the rise of AI technology, a living evidence base on approaches to data extraction will be very useful.However, the manuscript could benefit from improved clarity.Below are my comments:Title: Clarify the type of data being extracted (qualitative, quantitative, or mixed).1.</p>
<p>Table 1 .
1
Related literature.
ReferenceResearch discipline (Sample size)Primary focusAntons et al. (2020)Innovation (n=140)Text mining methods in innovation researchCairo et al. (2019)Computer Science (n=17)ML techniques for secondary studiesDridi et al. (2021)Management (n=124)Scholarly data mining applicationsGöpfert et al. (2022)Multidisciplinary (n=80)Measurement extraction methods using NLPFeng et al. (2017)Software Engineering (n=32)Text mining techniques and tools to facilitateSLRsKohl et al. (2018)Multidisciplinary (n=22)Tools for systematic reviews and mappingstudiesRoldan-Baluis et al.Multidisciplinary (n=46)NLP and ML for processing unstructured texts(2022)in digital formatSundaram andMultidisciplinary (n=29)Text mining-based automation of SLRsBerleant (2023)Wagner et al. (2022)Information Systems and relatedReview of AI in literature reviewsSocial Sciences (NR)Yang et al. (2023)Education (n=161)Text mining techniques in educational research
Note.AI = Artificial Intelligence, ML = Machine Learning, SLR = Systematic Literature Review, NLP = Natural Language Processing, NR = Not Reported.</p>
<p>, excluding medical research sources.Because IEEE content is indexed in Web of Science (Young, 2023), we did not include IEEE Xplore as a separate source.We added two additional databases (ACL and ArXiv) and conducted a search for data extraction tools in the Systematic Review Toolbox(Marshall et al., 2022)to capture associated articles.Searches were conducted in the
Association for Computational Linguistics (ACL) Anthology, arXiv Research-Sharing Platform (arXiv), and DBLPComputer Science Bibliography (DBLP) on June 15, 2023; in the Web of Science Core Collection (WOS) on June 8,2023; and in the Systematic Review Toolbox on October 2, 2023.
The Web of Science search and deduplication followed procedures stated in the protocol(Legate &amp; Nimon, 2023b).We adapted source code developed by Schmidt et al. (2021) for automating search, retrieval, and deduplication functions on full database dumps for ACL, ArXiv, and DBLP platforms.Complete details, including citation indices and specific setting applied, search syntax, and adapted source code are available in the project repository (see 'Data availability' section).</p>
<p>Table 2 .
2
Included studies.
TitleReferenceSummary descriptionA model for the identification of theShen et al. (2022)Proposed a high-performance model forfunctional structures of unstructuredidentifying functional structures ofabstracts in the social sciencesunstructured abstracts in the socialsciences.A Semi-automatic Data Extraction SystemNayak et al. (2021)Proposed a novel data extraction systemfor Heterogeneous Data Sources: a Casebased on text mining approaches toStudy from Cotton Industrydiscover relevant and focused informationfrom diverse unstructured data sources.An interactive query-based approach forBayatmakou et al.Proposed an interactive multi-documentsummarizing scientific documents(2022)text summarization approach that allowsusers to specify composition of a summaryand refine initial query by user-selectedkeywords and sentences extracted fromretrieved documents.Automatic results identification in softwareTorres et al.Analyzed existing methods for sentenceengineering papers. Is it possible?(2012)classification in scientific papers andevaluates their feasibility in unstructuredpapers in the Software Engineering area.Contextual information retrieval inAngrosh et al.Introduced conceptual framework (andresearch articles: Semantic publishing tools(2014)linked data application) for modelingfor the research communitycontexts associated with sentences andconverting information extracted fromresearch articles into machine-understandable data.</p>
<p>Table 2 .
2
Continued
TitleReferenceSummary descriptionCORWA: A Citation-Oriented Related WorkLi et al. (2022)Presented new approach to related workAnnotation Datasetgeneration in academic research papersand introduced annotation dataset forlabeling different types of citation textfragments from various informationsources.DASyR (IR) -Document Analysis System forPiroi et al. (2015)Introduced a semi-automatic documentSystematic Reviews (in Informationanalysis system/framework for annotatingRetrieval)published papers for ontology population,particularly in domains lacking adequatedictionaries.Detecting In-line Mathematical ExpressionsIwatsuki et al.Reported preliminary results applying ain Scientific Documents(2017)method for identifying in-line mathematicalexpressions in PDF documents utilizingboth layout and linguistic features.Extracting the characteristics of life cycleDiaz-Elsayed andProposed a method for automaticallyassessments via data miningZhang (2020)extracting key characteristics of life cycleassessments (LCAs) from journal articles.Machine Reading of Hypotheses forChen et al. (2021)Introduced NLP models for accelerating theOrganizational Research Reviews and Pre-discovery, extraction, and organization oftrained Models via R Shiny App for Non-theoretical developments from socialProgrammersscience publications.MetaSeer.STEM: Towards AutomatingNeppalli et al.Proposed a machine learning-based systemMeta-Analyses(2016)developed to support automatedextraction of data pertinent to STEMeducation meta-analyses.Mining Social Science Publications forZielinski andDescribed a work-in-progress developmentSurvey VariablesMutschke (2017)of new techniques or methods foridentifying variables used in social scienceresearch.Ontology-based and User-focusedChen et al. (2020)Proposed an ontology-based system whichAutomatic Text Summarization (OATS):users could access and utilize forUsing COVID-19 Risk Factors as an Exampleautomatically generating textsummarization from unstructured text.Ontology-Driven Information ExtractionPertsas andIntroduced a system designed to extractfrom Research PublicationsConstantopoulosinformation from research articles,(2018)associate it with other sources, and infernew knowledge.Research Method Classification with DeepAnisienia et al.Presented an artifact that uses deepTransfer Learning for Semi-Automatic(2021)transfer learning for multi-labelMeta-Analysis of Information Systemsclassification of research methods for anPapersInformation Systems corpus.Scaling Systematic Literature Reviews withGoldfarb-TarrantDescribed a pipeline that automates threeMachine Learning Pipelineset al. (2020)stages of a systematic review: searching fordocuments, selecting relevant documents,and extracting data.Searching for tables in digital documentsLiu et al. (2007)Introduced an automatic table extractionand search engine system.Section-wise indexing and retrieval ofShahid and AfzalDescribed development and evaluation of aresearch articles(2018)technique for tagging paper's content withlogical sections appearing in scientificdocuments.Sysrev: A FAIR Platform for Data CurationBozada et al.Introduced a platform for aiding inand Systematic Evidence Review(2021)systematic reviews and data extraction byproviding access to digital documents andfacilitating collaboration in researchprojects.Team EP at TAC 2018: Automating dataNowak andPresented a solution for automating dataextraction in systematic reviews ofKunstman (2019)extraction in systematic reviews ofenvironmental agentsenvironmental agents.</p>
<p>Table 2 .
2
Continued
TitleReferenceSummary descriptionThe Canonical Model of Structure for DataAliyu et al. (2018)Developed a canonical model of structureExtraction in Systematic Reviews ofapproach that identifies sections fromScientific Research Articlesdocuments and extracts the headings andsubheadings from the sections.Towards a Semi-Automated Approach forDenzler et al.Presented a flexible and modifiable artifactSystematic Literature Reviews Completed(2021)to support systematic literature reviewResearchprocesses holistically.UniHD@CL-SciSumm 2020: CitationAumiller et al.Presented method to identify referencesExtraction as Search(2020)from citation text spans and classify citationspans by discourse facets.</p>
<p>Nayak et al., 2021)2022)Shen et al., 2022)ped a Sentence Context Ontology (SENTCON) for modeling the contexts of information extracted from research documents.Piroi et al. (2015)developed and presented an annotation system for populating ontologies in domains lacking adequate dictionaries.Some work focused on automatically mapping structures of research documents.For example, using an open source lexical database to develop a canonical model of structure,Aliyu et al. (2018)were able to automatically identify and extract target paper sections from research documents.Shahid and Afzal (2018) utilized specialized ontologies to automatically tag content in research papers by logical sections.Chen et al. (2020) presented a novel framework for text summarization, including ontology-based topic identification and userfocused summarization modules.) and SciBERT(Goldfarb-Tarrant et al., 2020; Li et al., 2022)were the most utilized for tasks relevant to extracting data from research in social sciences.Others language models included BioBERT(Chen et al., 2020)and distilBERT(Goldfarb-Tarrant et al., 2020).We identified a recent application of the Hugging Face LED model(Li et al., 2022), a pretrained longformer model developed to address length limitations associated with other transformer-based approaches (seeBeltagy et al., 2020).Six of the included studies applied Named Entity Recognition (NER) techniques.Increasing availability of tools to support the entire SLR pipeline, including data extraction efforts, may be partially to credit for upward trends in NER applications.Based on applications we identified, NER would best be described as versatile.Some studies incorporated NER as an integral component embedded throughout a larger ML/NLP pipeline (e.g.,Goldfarb-Tarrant et al., 2020), others included NER subcomponents leveraged primarily for preprocessing and feature representation tasks (e.g.,Pertsas &amp; Constantopoulos, 2018), and in one study, authors took advantage of open source NER tools that could be easily integrated into a highly modifiable artifact serving as platform for future development of holistic approaches to scaling SLR tasks (e.g.,Denzler et al., 2021).Extractive questing-answering models involve tasks where a model generates answers to questions based on a given context.Question-answering models appeared in our dataset as early as 2007(Liu et al., 2007), with the remaining applications published in 2020 or later.Question answering techniques have a range of applications that most readers are likely familiar with, like chatbots and intelligent assistants (e.g., Alexa, Google Assistant, Siri).However, state-of-the-art approaches for question-answering over knowledge bases are also being put to use in the data extraction arena.The study byBayatmakou et al. (2022), for example, introduced new methods for interactive multi-document text summarization that allow users to specify summary compositions and interactively refine queries after reviewing complete sentences automatically extracted from documents.Probabilistic Models.Among probabilistic models, Conditional Random Field (CRF) applications were predominant in our dataset.CRF was often applied for sequence labeling tasks, such as named entity recognition (e.g.,Nayak et al., 2021), or for classification tasks (e.g.,Angrosh et al. 2014).Overall, included studies provided evidence that CRF can form a powerful architecture when combined with RNNs (e.g., bi-GRU-CRF, bi-LSTM-CRF; seeNowak &amp; Kunstman,  2019;Shen et al., 2022).We found a single application of the Maximum Entropy Markov Model (MEMM), however, based on experimental results the authors ultimately selected CRF for identifying sentence context for extraction from research publications(Angrosh et al., 2014).
Named Entity Recognition (NER). Extractive Questing-Answering Models. Rule-basesRule-based techniques involve the application of predefined rules or patterns to extract specific features from the text.
(Goldfarb-Tarrant et al., 2020;n et al., suggested thShen et al., 2022;ed approaches have experienced rapid growth since 2020.Bidirectional Encoder Representations from Transformers (BERT) and other BERT-based language models made up the majority of transformer-based approaches.SpecificallyBERT (Aumiller et al., 2020;Shen et al.,Figure 7. Model architectures and components.2022Classifiers.For classification approaches, we followedSchmidt et al. (2021)in reporting instances of Support Vector Machines (SVM) separately from other binary classifiers and likewise found a high prevalence of SVM usage, accounting for 50% of all binary classifiers identified(Goldfarb-Tarrant et al., 2020; Shahid &amp; Afzal, 2018;Shen et al., 2022; Zielinski &amp; Mutschke, 2017).Among classifiers that use a linear combination of inputs (Jurafsky &amp; Martin, 2024), naïve Bayes was the most frequent(Neppalli et al., 2016; Shahid &amp; Afzal, 2018; Torres et al., 2012; Zielinski &amp;  Mutschke, 2017).One study used a Perceptron classifier; however, it was extended (i.e., OvR) to handle multiclass problems(Aumiller et al., 2020).Multi-class classifiers were less common with one instance each of k-Nearest Neighbors (aka KNN/kLog; Zielinski &amp; Mutschke, 2017) and the J48 classifier (C4.5 Decision Trees;Piroi et al., 2015).Neural Networks.Overall, there were a variety of neural network applications across the included studies.Most used Long Short-term Memory (LSTM), more specifically, Bidirectional LSTM (BiLSTM).We also identified one application Bidirectional Gated Recurrent Unit(BiGRU;Shen et al., 2022).Convolutional Neural Network (CNN) architectures(Goldfarb-Tarrant et al., 2020; Nowak &amp; Kunstman, 2019; Anisienia et al., 2021)were also present.Several studies evaluated state-of-the-art deep learning methods.For example, Shen et al. (2022) compared the performance of deep learning models (TextCNN and BERT) for sentence classification in social sciences abstracts.In another comparative study, Anisienia et al. (2021) compared methods for pretraining deep contextualized word representations for cuttingedge transfer learning techniques based on CNN and LSTM architectures in addition to classifier models (e.g., SVM).Versatile and widely applicable, they offer a robust framework for automating data extraction or for capturing relevant information from large volumes of text.See Figure8for rule-based approaches reported across included studies.Overall, 70% (n=16) of included studies utilized rule-or heuristic-based approaches to support a variety of tasks for data extraction.Of these, nearly half (n=7) reported using Regular Expressions (RegEx).For example, based on rules developed from manual inspection, RegEx was used byTorres et al. (2012)to construct patterns for identifying specific</p>
<p>Five studies provided description of user feedback and other ratings.User feedback (among other metrics) was reported byLi et al. (2022)who conducted expert human comparative assessment to assess fluency, relevance, coherence, and overall quality of model citation span/sentence generation outputs.This category also included evaluation metrics not listed in the sources we adapted when developing our protocol (see O'Mara-Eaves et al., 2015, p. 3, Table 1; Schmidt et al., 2021, pp.8-9).For example, in assessing their system on values returned for queries of interest, Nayak et al. (2021) reported suitably, adaptability, relevance scores, and data-dependencies.As another example,Denzler et al. (2021, p. 5) evaluated their artifact based on design science aspects (i.e., validity, efficacy, and utility).Given the rapid growth of domain-specific ontologies and pre-trained language models, it is not surprising to find Kappa statistics reported for tasks such as evaluating agreement between human annotators when creating gold standard datasets for training and evaluation (Cohen's Kappa, see Pertsas &amp; Constantopoulos, 2018; Mezzich's Kappa or Gwet's AC1, seeAnisienia et al., 2021).Semantic similarity scores, which can be used to compare model generated responses against</p>
<p>(Piroi et al.;n of leave-one-out or LOOCV(Piroi et al.; 2015)and one application of document level CV used as a supplemental technique to k-fold(Neppalli et al., 2016).groundtruthresponses in query-based or question-answering based applications, were reported in two studies(Jaccard  Index, Bayatmakou et al., 2022; DKPro Similarity, Zielinski &amp; Mutschke, 2017).</p>
<p>(Chen et al., 2021;Denzler et al., 2021;prepared" (available at httpGoldfarb-Tarrant et al., 2020;Iwatsuki et al., 2017;Reader(Chenetal., 2021), which was available to users through an Rshiny application.SysRev(Bozada  et al., 2021)was also the only tool cataloged in the SR Toolbox(Marshall et al., 2022).Six of the twenty-three studies (26%) made source code openly available(Chen et al., 2021;Denzler et al., 2021; Diaz-Elsayed &amp; Zhang, 2020;Goldfarb-Tarrant et al., 2020;Iwatsuki et al., 2017; Li et al. 2022).Article references and corresponding repositories are detailed in Table3.GitHub stood out as the most popular repository for code and data sharing, and one study made source code available online through an open access publisher.TransferabilityIn the evolving landscape of systematic reviews and meta-analyses, the adaptability of tools and technologies to new research domains emerged as a critical factor for enhancing research efficiency and scope.The insights provided by many of the authors working towards automation of data extraction illuminate the transferability of various tools and technologies for research targeting the extraction of data elements beyond PICO.</p>
<p>(Chen et al., 2020;Goldfarb- Tarrant et al., 2020;ddressed transferability in describing the development of their tools, and further subjected these tools to rigorous testing aimed at validating transferable capabilities(Chen et al., 2020;Goldfarb- Tarrant et al., 2020; Neppalli et al., 2016).For instance, Neppalli et al. (2016) created MetaSeer.STEM with a focus on</p>
<p>Table 3 .
3
Code repositories.Reference extraction of data across a range of research domains, including education, management, and health informatics.Chen et al. (2020) highlighted the adaptability of OATS, showcasing its broader application potential to fields beyond the authors' COVID-19 specific demonstration.Finally, Goldfarb-Tarrant et al. (</p>
<p>Table 4
4
(Chen et al., 2021)3)22;Nayak et al., 2021)ted as outlined by JARS(Appelbaum et al., 2018, p. 6).Each tool was assessed for potential to extract specific data elements by manuscript section (i.e., methods and results reporting elements pertinent to meta-analytic research; seeLegate &amp; Nimon, 2023b).Where the authors did not state a tool name, we used the description of the tool as presented in the paper (e.g.,Bayatmakou et al., 2022;Nayak et al., 2021).Unlike ongoing research that focuses on data extraction from clinical literature (e.g., PICO elements/RCTs; seeSchmidt et al., 2023), specific reporting guidelines were not a primary focus of the studies we identified.However, authors described target entities and/or research methods of interest with high levels of specificity.For instance, extracting descriptive statistics, sample size, and Likert scale points(Neppalli et al., 2016)and extracting research hypotheses from published literature in organizational sciences(Chen et al., 2021).Despite the lack of discourse surrounding specific reporting guidelines, many of the tools reviewed incorporated some form of user-prompted, annotation-or query-based approach to (semi)automated data extraction.Thus, the collective body of work lends optimism surrounding customizable state-of-the art methods that can support extraction for a wide range of disciplines, research designs, and entities or data elements of interest to social science researchers.
Lastly, we note the utility of NER for the advancement of (semi)automated extraction of APA defined data elements.NER methodologies can be leveraged alongside classification models (Nayak et al., 2021), linked to domain specificontologies or other knowledge bases (Piroi et al., 2015), or incorporated as stand-alone modules integrated into largermodifiable frameworks (Denzler et al. 2021). In addition to pre-trained NER models for identification and extractionof named entities, Research Spotlight (Pertsas &amp; Constantopoulos, 2018) also exploited lexico-syntactic patterns in thescholarly ontology to identify and extract non-named entities. The Semi-automatic Data Extraction System forHeterogeneous Data Sources (Nayak et al., 2021) combined features of NER and rule-based chunking to identify andextract phrases on regular expressions as well as named entities contained in the documents. Further, NER can beimplemented through open source tools as demonstrated by Denzler et al. (2021) and Nayak et al. (2021).
(Piroi et al., 2015)2022), andle approach is extractive question-answering based on pre-trained Transformer models.Extractive question-answering models are able to generate direct answers from knowledge base in response to natural language questions posed by users(Kwiatkowski et al. 2019).These tools typically offer enhanced flexibility through user-defined prompts and mechanisms for interactive query refinement.Example tools that incorporated question answering techniques included CIRRA(Piroi et al., 2015), the Interactive Text Summarization System for Scientific Documents(Bayatmakou et al., 2022), and OATS (Chen et al., 2021).Other types of flexible systems allow users to view excerpts related to specific keywords or queries, supporting expedited identification and labeling of target data elements.For example several tools supported user labeling of data, followed by predictive classification based on user annotations.Although these tools do not automatically extract data for users, they do augment human effort by (semi)automating time consuming tasks associated with data annotation and extraction.For instance, Sysrev(Bozada et al., 2021)supports researchers in labeling and extracting data by leveraging active learning models developed to replicate user decisions across various review tasks.Likewise, MetaSeer(Neppalli et al.,  2016)developed ML techniques to identify and extract numbers from documents, which were then presented to users for manual annotation.Unlike question-answering models, human-computer interactions in these examples are not based on natural language queries, however, human expertise can be used to 'train' ML models to predict future annotation decisions.Similarly, to overcome the time-constraints of open-ended annotation in fields that lack domain-specific dictionaries, DASyR(Piroi et al., 2015)utilized a combination of user annotations, classification models, and contextual information for populating ontologies.They reported substantial reduction in annotation time, stating that through the DASyR UI "five experts added approximately 30,000 annotations at a speed of 4s/annotation" (p.595).</p>
<p>Table 4 .
4
APA data elements.
TableSeerLiu et al.(2007)✓✓SysRevBozadaet al.(2021)✓✓Semi-automaticDataExtractionSystemNayak et al.(2021)✓✓ResearchSpotlightPertsas &amp;Constantopoulos(2018)✓✓OATSChenet al.(2020)✓✓MetaSeer.STEMNeppalliet al.(2016)✓✓Interactive TextSummarizationSystemBayatmakouet al. (2022)✓✓HypothesisReaderChen et al.(2021)✓✓HolisticModifiableLiteratureReviewerDenzleret al. (2021)✓✓DASyR(IR)Piroiet al.(2015)✓✓CIRRAAngroshet al.(2014)✓✓CORWALi et al.(2022)✓Example ReportingElementsParticipant selection[setting(s), location(s),date(s), % approachedvs. participated], Major/topic-specificdemographics [age, sex,ethnicity, achievementlevel(s), tenure]Intended vs. actualsample size, Sample sizedetermination [poweranalysis, parameterestimates]Measures [primary,secondary, covariates],Psychometric properties[reliability coefficients,internal consistencyreliability, discriminant/convergent validity, test-retest coefficients, timelag intervals]ItemCriteria, DataCollection &amp;ParticipantsSample Size,Power &amp;PrecisionMeasures &amp;InstrumentationManuscriptSectionMethods</p>
<p>Table 4 .
4
Continued
TableSeerLiu et al.(2007)✓✓SysRevBozadaet al.(2021)✓✓✓✓Semi-automaticDataExtractionSystemNayak et al.(2021)✓✓✓✓ResearchSpotlightPertsas &amp;Constantopoulos(2018)✓✓✓✓OATSChenet al.(2020)✓✓✓✓MetaSeer.STEMNeppalliet al.(2016)✓✓✓✓Interactive TextSummarizationSystemBayatmakouet al. (2022)✓✓✓✓HypothesisReaderChen et al.(2021)HolisticModifiableLiteratureReviewerDenzleret al. (2021)✓✓✓✓DASyR(IR)Piroiet al.(2015)✓✓✓✓CIRRAAngroshet al.(2014)✓✓✓CORWALi et al.(2022)✓Example ReportingElementsParticipants (by group/stage), Dates[recruitment, repeatedmeasures]Statistical/data-analyticmethods, Missing data(frequency or %), Missingdata methods [MCAR,MAR, MNAR], Validityissues [assumptions,distributions]Analytic approach [SEM,HLM, factor analysis],Model details [fitindices], Software,Estimation Technique,Estimation Issues [e.g.,convergence]Descriptive [total samplesize, sample sizesubgroups/cases,means, standarddeviations, correlationmatrices], Inferential [p-values, degrees offreedom, mean squareeffects/errors, effect sizeestimates, confidenceintervals]ItemParticipants &amp;RecruitmentStatistics &amp; DataAnalysisComplexAnalysesDescriptive &amp;InferentialStatisticsManuscriptSectionResults</p>
<p>Table 5 .
5
Structure, location, review tasks.
TableSeerSysRevSemi-automaticData ExtractionSystemResearchSpotlightOATSMetaSeer.STEMInteractive TextSummarizationSystemHypothesisReaderHolisticModifiableLiteratureReviewerDASyR(IR)CORWA CIRRALi et al.Category Item</p>
<p>(Chen et al., 2020;seline review, we did not capture techniques used for optimization, training, or fine-tuning on specific datasets or tasks.Several techniques surfaced while conducting this review, such as class modifiers (e.g.,OvR; Aumiller  et al., 2020), genetic algorithms (Bayatmakou et al., 2022; Torres et al. (2012), Adam optimizer (Nowak &amp; Kunstman, 2019);Shen et al., 2022), cross entropy loss(Chen et al., 2020; Li et al., 2022), Universal Language Model Fine-tuning (e.g.,ULMFiT; Anisienia et al., 2021), and back-propagation optimizers(Chen et al., 2020; Anisienia et al., 2021).With increasing applications of pre-trained language models that can be fine-tuned for specific applications (Jurafsky &amp; Martin, 2024), inclusion of training and optimization approaches would provide a more comprehensive framework for reporting findings on ML/NLP approaches to data extraction.We plan to supplement future iterations of this review by capturing various optimization and training methods.Adapted code files and results for automated search and screening for ACL, ArXIV, and DBLP full database dumps.Data are available under the terms of the Creative Commons Attribution 4.0 International Public License (CC-BY 4.0).
• Baseline Review CodeExtended dataOpen Science Framework: https://doi.org/10.17605/OSF.IO/C7NSA(Legate &amp; Nimon, 2023a).This project contains the following extended data:• Extraction Techniques Revised.docx -categories and descriptions of data extraction techniques, architecturecomponents, and evaluation metrics of interest• Review Classifications.docx -review tasks and stages of interest
○• Target Data Elements.docxkeyelements of interest for targeted data elements • Comprehensive List of Eligible Data Elements.xlsxcomprehensivelist of elements with extraction potential per APA JARS • Search Strategy.docxsearchsyntax for preliminary search in Web of Science • APA &amp; Cochrane Data Elements.xlsxtableddata elements for Cochrane reviews, APA Module C (clinical trials), and APA (all study designs) Data are available under the terms of the Creative Commons Attribution 4.0 International Public License (CC-BY 4.0).</p>
<p>MA, Cranefield S, Stanger N: Contextual information retrieval in research articles: Semantic publishing tools for the research community.Semantic Web.2014; 5(4): 261-293.Publisher Full Text Anisienia A, Mueller RM, Kupfer A, et al.: Research method classification with deep transfer learning for semi-automatic meta-analysis of information systems papers.Proceedings of the 54th Hawaii International Conference on System Sciences.2021;pp.6099-6108.analyses in social sciences: A living review protocol.2023a, January 12.
Angrosh Publisher Full TextLegate A, Nimon K: (Semi)automated approaches to data extraction forsystematic reviews and meta-analyses in social sciences: A livingreview protocol [version 2; peer review: 2 approved, 1 approved withreservations]. F1000Res. 2023b; 11: 1036.Publisher Full TextLegate A, Nimon K: (Semi) Automated Approaches to Data Extraction forSystematic Reviews and Meta-Analyses in Social Sciences: A Living Review.[Dataset]. OSF. 2024, May 5.Publisher Full TextLi X, Mandal B, Ouyang J: CORWA: A citation-oriented related workannotation dataset. arXiv preprint arXiv:2205.03512. 2022.Publisher Full TextLiu Y, Bai K, Mitra P, et al.: Searching for tables in digital documents.Ninth International Conference on Document Analysis and Recognition(ICDAR 2007). IEEE; 2007, September; Vol. 2: pp. 934-938.Publisher Full TextMarshall C, Sutton A, O'Keefe H, et al., editors. The Systematic ReviewToolbox. 2022.Reference SourceMarshall I, Wallace B: Toward systematic review automation:A practical guide to using machine learning tools in researchsynthesis. Syst. Rev. 2019; 8(1): 110-163.PubMed Abstract|Publisher Full Text|Free Full TextMcGuinness LA, Schmidt L: mcguinlu/COVID_suicide_living: Initial Release(v1.0.0). [Data set]. Zenodo. 2020.Publisher Full Text
Science Framework: PRISMA checklist for 'Open Science Framework: (Semi)Automated Approaches to Data Extraction for Systematic Reviews and Meta-Analyses in Social Sciences: A Living Review'.https://doi.org/10.17605/OSF.IO/C7NSA (Legate &amp; Nimon, 2024).</p>
<p>Table 2 caught my eye.For example Iwatsuki et al. (2017) about detecting in-line mathematical expressions or Torres et al. (2012) about software engineering or later Nayak et al. (</p>
<p>an expert in social science research, but a few included references in Table 2 caught my eye.For example Iwatsuki et al. (2017) about detecting in-line mathematical expressions or Torres et al. (2012) about software engineering or later Nayak et al. (2021) about cotton industry?
."Comment 1: ResponseThank you for catching this oversight. You are absolutely correct; Yu et al. (2018) primarilyfocused on screening automation for primary study selection rather than data extractionstages of SLRs. We have removed this reference from the sentence to align with the context.Comment 2I am not</p>
<p>the rationale for, and objectives of, the Systematic Review clearly stated? Partly Are sufficient details of the methods and analysis provided to allow replication by others? Partly Is the statistical analysis and its interpretation appropriate? Not applicable Are the conclusions drawn adequately supported by the results presented in the review? Yes If this is a Living Systematic Review, is the 'living' method appropriate and is the search schedule clearly defined and justified? ('Living Systematic Review' or a variation of this term should be included in the title.) Partly Competing Interests:</p>
<p>No competing interests were disclosed.
Reviewer Expertise: Systematic Evidence Synthesis MethodologyI confirm that I</p>
<p>have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above.</p>
<p>Thank you for your thoughtful and detailed feedback on our manuscript.We appreciate the time and effort you have invested in providing suggestions to enhance our work.We also value rigorous research methods and reporting transparency and would like to clarify several points regarding the reporting guidelines we adhered to and the journal's policies and requirements.
Author Response 22 Sep 2024Amanda LegateDear Dr. Macura,Our manuscript follows the PRISMA (Preferred Reporting Items for Systematic Reviews andMeta-Analyses) guidelines. As noted in the F1000Research "Article Standards of Reporting"(https://f1000research.com/about/policies#stofrep), systematic reviews published in thisjournal must adhere to PRISMA guidelines. We have ensured that our reporting aligns withPRISMA's emphasis on transparency, replicability, and comprehensiveness.
F1000Research 2024, 13:664 Last updated: 26 SEP 2024 <br />
Code repository Chen et al. (2021) devtools::install_github("canfielder/HypothesisReader") Denzler et al. (2021) GitHub Repository: https://github.com/HoliMoLiRev/HoliMoLiRevDiaz-Elsayed and Zhang (2020) https://methods-x.com/article/S2215-0161(20)30224-7/fulltext#supplementaryMaterial Goldfarb-Tarrant et al. (2020) https://github.com/seraphinatarrant/systematic_reviewsIwatsuki et al. (2017) https://github.com/Alab-NII/inlinemathLi et al. (2022) https://github.com/jacklxc/CORWAData are available under the terms of the Creative Commons Attribution 4.0 International Public License (CC-BY 4.0).Software availability• Source code available from: https://github.com/mcguinlu/COVID_suicide_living• Archived source code: http://doi.org/10.5281/zenodo.3871366(McGuinness &amp; Schmidt, 2020).• The adapted version of the source code for automated searching: https://doi.org/10.17605/OSF.IO/C7NSA• Archived source code: https://doi.org/10.17605/OSF.IO/C7NSA (Legate &amp; Nimon, 2024).• License: MITEthics and consentEthical approval and consent were not required.Data availabilityUnderlying data OSF: (Semi)Automated Approaches to Data Extraction for Systematic Reviews and Meta-Analyses in Social Sciences: A Living Review(Legate &amp; Nimon, 2024).Open Science Framework: https://doi.org/10.17605/OSF.IO/C7NSA(Legate &amp; Nimon, 2024).This project contains the following underlying data:• Baseline Review Underlying Data Comment 24: Response Thank you for raising this point.The "Study Selection" section of the paper details this information and discusses inter-rater reliability (IRR) assessments.Comment 25 [Methods/ Study selection] The sentence, "coding forms allowed for input of "other" responses (e.g., APA data elements) that were not included in extant reviews that focus on medical and clinical data extraction (e.g., PICO elements)" is unclear.Consider removing or clarifying and linking it better with the rest of the text.Fred Oswald 1 Rice University, Houston, Texas, USA 2 Rice University, Houston, Texas, USA Overall, this paper is an excellent review of automated data-extraction methods for the purposes of synthetic reviews and meta-analysis.To my knowledge, there is no such review in the literature, and yet given the rise in AI-based technologies, there is a rising need for researchers to have a single resource identifying these extraction methods.This review nicely summarizes the types of tools that are out there, but it might further tie the tools more closely to a checklist that reflects must be typically must be accomplished when conducting meta-analysis (e.g., identifying literature, extracting sample sizes and effect sizes, converting effect sizes when necessary, coding effect sizes into variables, associated moderators, associated reliability coefficients, dealing with missing data).This would give the reader a better sense of what* gets automated and serves their purposes (e.g., you can take the 'model architectures and components' section and populate the checklist/framework with these AI tools/functions) .Also, though it is certainly useful to document when and how humans are compared to automated systems, the level of accuracy reported (e.g., errors of commission and omission by automated systems) would be useful as well (i.e., are these automated systems any good?when systems agree with humans, when are they agreeing in an accurate way vs. a biased way?)Thank you for the opportunity to review -again, this will be a valuable paper to readers.Are the rationale for, and objectives of, the Systematic Review clearly stated? YesAre sufficient details of the methods and analysis provided to allow replication by others? YesIs the statistical analysis and its interpretation appropriate?Yes Are the conclusions drawn adequately supported by the results presented in the review?Yes Author Response 22 Sep 2024Amanda LegateComment 1 Also, though it is certainly useful to document when and how humans are compared to automated systems, the level of accuracy reported (e.g., errors of commission and omission by automated systems) would be useful as well (i.e., are these automated systems any good?when systems agree with humans, when are they agreeing in an accurate way vs. a biased way?) Thank you for the opportunity to review -again, this will be a valuable paper to readers.Comment 1: Response Thank you for this insightful suggestion.We agree that evaluating the accuracy of automated systems compared to human assessments, particularly regarding errors of commission and omission, would provide valuable insights into their effectiveness and potential biases.Understanding when automated systems align with human judgments accurately is indeed crucial for advancing the field.Given the "living" nature of our review, we see this as an important focus for future updates.Although additional technical expertise may be required to conduct a comprehensive comparative assessment of these accuracy measures, we hope to expand our team to include experts in areas of data science and AI evaluation.This addition will enhance the rigor of our review and address critical questions surrounding the reliability of automated tools.We appreciate your valuable feedback and are committed to integrating these considerations in future versions of our living review.Competing Interests: No competing interests were disclosed.
The canonical model of structure for data extraction in systematic reviews of scientific research articles. M B Aliyu, R Iqbal, A James, 2018</p>
<p>The application of text mining methods in innovation research: Current state, evolution patterns, and development priorities. R&amp;D Manag. D Antons, E Grünwald, P Cichy, 10.18653/v1/2020.sdp-1.29Article Reporting Standards for Quantitative Research in Psychology: The APA Publications and Communications Board Task Force report. IEEE2018. October. 2020. 2018. 2018. December. 2010. 2020. November. 201850Publisher Full Text Aumiller DAm. Psychol</p>
<p>An interactive query-based approach for summarizing scientific documents. Pubmed Abstract|publisher, Full Text Bayatmakou, F Mohebi, A Ahmadi, A Tompson, L Thornton, A , 10.1108/IDD-10-2020-0124Interrater reliability in systematic review methodology: Exploring variation in coder decision-making. 202250Inf. Discov. Deliv.</p>
<p>PubMed Abstract|Publisher Full Text|Free Full Text Cairo LS, de Figueiredo Carneiro G, da Silva BC: Adoption of machine learning techniques to perform secondary studies: A systematic mapping study for the computer science field. I Sociol ; Beltagy, M E Peters, A ; Cohan, F Bosco, K Uggerslev, P ; Steel, Jr, J Borden, J Workman, 10.4324/9781003102953arXiv:2012.02028Publisher Full Text|Reference Source Cohen E: The boundary lens: theorising academic activity. Routledge2018. 2017. 2021. 2019. 2022. 2020. 2021. 2021. 201450arXiv preprintSpringerplus</p>
<p>Reference Source Diaz-Elsayed N, Zhang Q: Extracting the characteristics of Life Cycle Assessments via data mining. MethodsX. T Denzler, M R Enders, P ; Akello, M M Gaber, Rma Azad, 10.1371/journal.pmed.1001603Twenty-Seventh Americas Conference on Information Systems (AMCIS). 2021. 2020. 101004. 2021. 2014. 20174J. Clin. Epidemiol.</p>
<p>Online tools supporting the conduct and reporting of systematic reviews and systematic maps: A case study on CADIMA and review of existing tools. Pubmed Abstract|publisher, Full Text Eriksen, M B Frandsen, T F Feng, L Chiam, Y K Lo, S K Goldfarb-Tarrant, S Robertson, A Lazic, J , 10.1186/s13643-019-1062-0arXiv:2010.04665.2020arXiv:1901.02081Publisher Full Text Nowak A, Kunstman P: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents. Lecture Notes in Business Information Processing. R Valencia-García, G Alcaraz-Mármol, Del Cioppo-Morstadt, J , Cham; Khamis A, Kahale; Singapore; ChamSpringer2018. 2017. 2017. December. 2015. 2022. 2019. 2020. February 2022. 2022. 2022. 2021. 2012. 2017. August. 2015. Feb 2024 release. 2024. 2019. 2018. 2019. 2021. December. 2016. February. 2019. 2018. 2018. 2019. 20151065arXiv preprintSyst. Rev.</p>
<p>Rayyan-a web and mobile app for systematic reviews. M Ouzzani, H Hammady, Z Fedorowicz, 10.1186/2046-4053-4-5PubMed Abstract|Publisher Full Text|Free Full Text. 20165210</p>
<p>The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. M Page, J Mckenzie, P Bossuyt, 10.1186/s13643-016-0384-4PubMed Abstract|Publisher Full Text|Free Full Text. 202188</p>
<p>): 74. PubMed Abstract|Publisher Full Text|Free Full Text Wagner G, Lukyanenko R, Paré G: Artificial intelligence and the conduct of literature reviews. Pubmed Abstract|publisher, Full Text Park, J J Kim, Y Han, H ; Aalberg, T Papatheodorou, C Dobreva, M , 10.18653/v1/W17-2907Publisher Full Text Sundaram G, Berleant D: Automating systematic literature reviews with natural language processing and text mining: A systematic literature review. Eighth International Congress on Information and Communication Technology (ICICT). SingaporeIEEE2021. February 17-19. 2018. 2018. 2020. 2015. 2015. August. 1980. 2022. F1000Res. 2021. F1000Res. 2020. F1000Res. 2023. 2018. 2022. 2021. 2023. 2018. 2012. 2012. June. 2014. 2022. 2022. 2023. 2023. January. 2018. 2022. 2020. 2017. August. 201890Publisher Full Text Tsafnat G,Nature Climate Change</p>
<p>N R Haddaway, B Macura, P Whaley, A S Pullin, 10.6084/m9.figshare.5897299ROSES for systematic map reports. 2017aVersion 1.0) [Data file</p>
<p>N R Haddaway, B Macura, P Whaley, A S Pullin, 10.6084/m9.figshare.5897272ROSES for systematic review reports. 2017bVersion 1.0) [Data file</p>
<p>Comment 2 [Title] Since this review does not include any qualitative or quantitative synthesis per se, but rather provides an overview of the field (methods for semi-automated data extraction), I suggest removing "living systematic review" and adding "living systematic map. N R Haddaway, B Macura, P Whaley, A S Pullin, 10.1186/s13750-018-0121-7Environmental Evidence. 772018ROSES reporting standards for systematic evidence syntheses: Pro forma, flow-diagram and descriptive summary of the plan and conduct of environmental systematic reviews and systematic maps</p>            </div>
        </div>

    </div>
</body>
</html>
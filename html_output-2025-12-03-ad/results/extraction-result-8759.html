<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8759 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8759</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8759</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-6661fec9734fe021cc8dccdc48526c24743ec95d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6661fec9734fe021cc8dccdc48526c24743ec95d" target="_blank">Deep Graph Convolutional Encoders for Structured Data to Text Generation</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Natural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes an alternative encoder based on graph convolutional networks that directly exploits the input structure and reports results on two graph-to-sequence datasets that empirically show the benefits of explicitly encoding the input graph structure.</p>
                <p><strong>Paper Abstract:</strong> Most previous work on neural text generation from graph-structured data relies on standard sequence-to-sequence methods. These approaches linearise the input graph to be fed to a recurrent neural network. In this paper, we propose an alternative encoder based on graph convolutional networks that directly exploits the input structure. We report results on two graph-to-sequence datasets that empirically show the benefits of explicitly encoding the input graph structure.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8759.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8759.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearisation (DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-First Traversal Linearisation of Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard method that serialises an input graph into a token sequence for use with seq2seq (LSTM) encoders by traversing the graph (depth-first) and emitting node/edge tokens, with heuristics to handle cycles and multiple parents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization (depth-first traversal)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Traverse the input graph in depth-first order, emitting nodes and edge labels into a sequence; siblings are visited in random order; when a child node is revisited (due to cycles) or has multiple parents it is repeated in the sequence. For WebNLG the authors used provided Gardent et al. scripts; for SR11Deep they followed a Konstas-style AMR linearisation using DFS.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graphs (WebNLG), semantic dependency graphs (SR11Deep) / AMR-like graphs (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal with random sibling order; repeat child node occurrences on cycles or when node has >1 parent; use linearisation scripts from Gardent et al. (WebNLG) or Konstas-style AMR traversal (SR11Deep).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation (text generation from RDF triples and semantic dependency graphs) as input for sequence-to-sequence LSTM encoder-decoder models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>WebNLG LSTM baseline (linearized input): BLEU = 0.526 ± 0.010, METEOR = 0.38 ± 0.00, TER = 0.43 ± 0.01. SR11Deep LSTM baseline: BLEU = 0.377 ± 0.007, METEOR = 0.65 ± 0.00, TER = 0.44 ± 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly with GCN encoders that operate on the graph structure: linearised LSTM baseline is outperformed by GCN models on both tasks (WebNLG: GCN 0.535 BLEU vs LSTM 0.526; SR11Deep: GCN 0.647 vs LSTM 0.377).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, directly compatible with standard sequence-to-sequence models; reuse of mature LSTM encoder-decoder toolkits and techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Does not explicitly exploit graph structure; requires heuristics for order and cycle handling which can introduce variability; can lead to poorer performance and more instability (higher stddev across restarts) compared to structured encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Observed hallucination/over-generation and instability in outputs (repeated / missing content and hallucinations) when using LSTM with linearised graphs; under-generation and repetitions also occur but were more severe for linearised LSTM in qualitative analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Graph Convolutional Encoders for Structured Data to Text Generation', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8759.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8759.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Network-based Encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct graph encoder that computes node representations by aggregating neighbor representations through direction-specific linear transforms, edge-label embeddings and learned scalar edge gates; stacked GCN layers produce contextualised node encodings used by an LSTM decoder with attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Convolutional Network (GCN) encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node has a feature vector; for each node the GCN updates representations via sums over neighbours: direction-specific weight matrices (in/out/loop), learned embeddings for edge labels, learned scalar gates per edge to weight contributions, and a non-linearity (ReLU). Multiple GCN layers can be stacked to capture multi-hop context; skip connections (residual or dense) are applied between GCN layers.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Directed labeled graphs: RDF graphs (reified triples) and semantic dependency graphs (SR11Deep).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No linearisation: the GCN directly consumes the graph structure (nodes, directed labeled edges). Node input features are embeddings (words/lemmas plus morphological features in SR11Deep) instead of a serialized sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (WebNLG entity description generation; SR11Deep semantic-to-surface realisation); encoder used within attention-based encoder-decoder model (LSTM decoder).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>WebNLG: GCN (4-layer, residual) BLEU = 0.535 ± 0.004, METEOR = 0.39 ± 0.00, TER = 0.44 ± 0.02. Enhanced variant GCN_EC (pretrained embeddings + copy + NE node words) BLEU = 0.559 ± 0.017, METEOR = 0.39 ± 0.01, TER = 0.41 ± 0.01. SR11Deep: GCN (7-layer, dense) BLEU = 0.647 ± 0.005, METEOR = 0.77 ± 0.00, TER = 0.24 ± 0.01; GCN+feat (with linguistic features) BLEU = 0.666 ± 0.027, METEOR = 0.76 ± 0.01, TER = 0.25 ± 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GCN encoders outperform linearised-LSTM baselines on both tasks (small gain on WebNLG, large gain on SR11Deep). However, pipeline systems that explicitly model syntax/morphology achieve higher BLEU on SR11Deep (e.g., STUMBA-D 0.794, TBDIL 0.805).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models relational structure, yielding more informative representations and improved generation quality and stability; reduced hallucination and under-/over-generation compared to linearised LSTM; flexible (stacks layers, supports edge labels/directions and feature-rich nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires graph-specific encoder and implementation (not a drop-in for vanilla seq2seq); stacking many layers without gating leads to poor local minima (gates required); dense skip-connections increase model size; still behind specialised pipeline models on SR11Deep.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>GCN models still show repeated and missing source content (though less than LSTM); performance degrades for too many layers (e.g., BLEU falls for >4 layers on WebNLG unless skip connections used); without scalar gates training gets stuck in poor local minima.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Graph Convolutional Encoders for Structured Data to Text Generation', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8759.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8759.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reification of RDF Relations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation for RDF triples that turns relation labels into explicit nodes connected to subject and object via standardized argument edges (A0, A1), enabling the encoder to produce states for relation nodes and to handle arbitrary relation vocabularies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>relation reification (RDF reification)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF triple (subject relation object) is transformed by introducing a new node representing the relation; that relation node is connected to subject and object entities with two labeled edges (A0 for subject, A1 for object). Edges are labeled and modeled by the GCN.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graphs (WebNLG dataset; DBpedia relations).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Reify each RDF relation into a new node, add edges (relation -> subject) labeled A0 and (relation -> object) labeled A1. The resulting directed labeled graph is fed directly to the GCN encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Text generation from RDF triples (WebNLG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as part of the GCN pipeline whose WebNLG performance is reported: GCN BLEU = 0.535 ± 0.004; GCN_EC BLEU = 0.559 ± 0.017.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Reification allows the GCN encoder to create explicit relation-node representations, which the authors argue is useful compared to treating relations only as edge labels in linearised sequences; enables efficient modeling of many KB relations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables per-relation hidden states, scales to arbitrary numbers of KB relations, and integrates seamlessly with GCN edge-label modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases the number of nodes in the graph (more compute); requires conversion step and consistent labeling (A0/A1).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure cases reported for reification alone, but overall model limitations (repetition, under-generation) still occur.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Graph Convolutional Encoders for Structured Data to Text Generation', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8759.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8759.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SR11Deep node features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lemma-plus-Feature Node Representation (SR11Deep)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node representation for semantic dependency graphs that concatenates the lemma embedding with a representation of morphological and punctuation features (sum of feature embeddings) to provide richer node inputs to the GCN encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>lemma + morphological features concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph node is represented as [h_l ; h_f], where h_l is the lemma embedding and h_f is the sum of embeddings for node features (e.g., num=sg, punctuation markers). This concatenated vector is used as the initial node embedding for the GCN.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Semantic dependency graphs (SR11Deep).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extend each node metadata to include (lemma, features); map lemma to embedding, map each morphological/punctuation feature to an embedding, sum feature embeddings and concatenate to lemma embedding to form node input vector.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Surface realisation (SR11Deep) - semantic graph to sentence generation using GCN encoder + LSTM decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Adding features to the GCN improved SR11Deep: GCN BLEU = 0.647 ± 0.005; GCN+feat BLEU = 0.666 ± 0.027 (METEOR ~0.76-0.77, TER ~0.24-0.25).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GCN+feat outperforms the plain GCN on SR11Deep, indicating that incorporating linguistic features into node representations improves generation quality compared to using lemma-only node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides explicit morphological and punctuation information to the encoder which improves performance on surface realisation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires additional feature extraction and embedding parameters; increases input complexity and model size.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure cases reported beyond general generation errors; improvements are modest and pipeline systems still outperform end-to-end models on SR11Deep.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Graph Convolutional Encoders for Structured Data to Text Generation', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8759.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8759.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delexicalisation / Anonymisation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delexicalisation (WebNLG) and Entity Anonymisation (SR11Deep)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preprocessing methods that replace entity surface forms with placeholders (delexicalisation) or compact named-entity nodes and assign type-based anonymised IDs to reduce sparsity and vocabulary size in generation models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>delexicalisation / entity anonymisation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>WebNLG: delexicalise entity lexical forms using provided scripts. SR11Deep: compact nodes corresponding to named entities; run NER to tag entities with types (person, location, date) and replace entity mentions with type-specific placeholders with numeric suffixes (e.g., PER_0).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF graphs (WebNLG) and semantic dependency graphs (SR11Deep).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Replace entity word tokens or node subgraphs with placeholders or anonymised IDs before encoding; for SR11Deep, compact multi-token entity nodes and label with NER types and indices.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (reduces vocabulary sparsity for seq2seq and GCN models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used in experiments (baseline and GCN unless GCN_EC used where delexicalisation was avoided); WebNLG baseline LSTM with delexicalisation BLEU = 0.526 ± 0.010; experiments show GCN_EC without delexicalisation reaches BLEU = 0.559 ± 0.017.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GCN_EC variant that avoids delexicalisation (uses pretrained embeddings + copy and explicit multi-word NE nodes) outperforms models that used delexicalisation in reported WebNLG results.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces vocabulary size and sparsity, simplifying learning and improving generalisation for entities; standard practice in WebNLG baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Removes surface lexical information requiring post-processing to re-insert entity surface forms; may lose nuance from entity word-level context.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When delexicalisation is used, models depend on the delexicalisation scripts and post-processing; the paper reports that avoiding delexicalisation and using a copy mechanism with explicit NE-node modeling (GCN_EC) gives better results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Graph Convolutional Encoders for Structured Data to Text Generation', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8759.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8759.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-word NE nodes (GCN_EC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-word Named-Entity Node Representation with NE edges (used in GCN_EC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instead of delexicalising, multi-word entities are represented by creating separate nodes for each word in the entity, linked to the entity head with special NE-labeled edges, enabling the model to copy or generate multi-word entity surface forms directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>multi-word NE node expansion with NE-labelled edges</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent each token of a multi-word entity as its own node; connect these word-nodes with special NE-labeled edges to the main entity node so the GCN can learn representations for each word and the decoder can use a copy mechanism to produce exact surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graphs with multi-token entities (WebNLG).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Expand multi-word subject/object entities into separate word nodes, connect them to the graph via Named Entity (NE) labeled edges, and feed the expanded graph to the GCN encoder; use pretrained embeddings and a pointer/copy mechanism in the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Text generation from RDF graphs (WebNLG) with copy capability to generate entity surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GCN_EC (no delexicalisation, pretrained GloVe embeddings, copy mechanism, 6-layer GCN) WebNLG BLEU = 0.559 ± 0.017, METEOR = 0.39 ± 0.01, TER = 0.41 ± 0.01. This outperforms delexicalised baselines and some shared-task systems (e.g., PKUWRITER BLEU = 0.512).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms the delexicalised LSTM baseline and some previous systems; still behind ADAPT (which uses sub-word encoding) on WebNLG test BLEU. Shows that avoiding delexicalisation plus copy and explicit NE word nodes is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves exact multi-word entity surface forms, enables copying, avoids the need for delexicalisation/post-processing, and improves BLEU over delexicalised baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases graph size (more nodes) and model complexity; requires designing NE edges and using a copy mechanism; may require pretrained embeddings for best effect.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Some repetition in outputs still observed (e.g., duplicate mentions) and GCN_EC sometimes produces repeated sentences, indicating copying and aggregation heuristics are still imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Graph Convolutional Encoders for Structured Data to Text Generation', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8759.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8759.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-style linearisation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-style Graph Linearisation (as in Konstas et al. 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearisation approach inspired by AMR-to-text work that serialises graphs into sequences for seq2seq models; cited as the inspiration for the SR11Deep baseline linearisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural amr: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR-style linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize the graph into a sequence using traversal orders and bracketed structures (Konstas et al. style); used as a guideline for SR11Deep baseline DFS linearisation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and AMR-like semantic graphs / semantic dependency graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Follow Konstas et al.'s linearisation strategies (depth-first traversal with structural markers); referenced rather than reimplemented exactly.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AMR-to-text, SR11Deep baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned as a baseline approach compared to direct graph encoders (GCNs); direct GCN encoding outperformed LSTM models trained on linearised AMR-like inputs in this paper's SR11Deep experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows reuse of seq2seq models and training pipelines developed for sequence data.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Loses explicit graph inductive bias; susceptible to issues noted for linearisation (ordering heuristics, cycles, repeated nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper beyond the general problems observed for linearised baselines (instability, hallucination, lower BLEU on SR11Deep).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Graph Convolutional Encoders for Structured Data to Text Generation', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The WebNLG challenge: Generating text from rdf data <em>(Rating: 2)</em></li>
                <li>Neural amr: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Graph convolutional encoders for syntax-aware neural machine translation <em>(Rating: 2)</em></li>
                <li>Encoding sentences with graph convolutional networks for semantic role labeling <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Semisupervised classification with graph convolutional networks <em>(Rating: 1)</em></li>
                <li>Get to the point: Summarization with pointer-generator networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8759",
    "paper_id": "paper-6661fec9734fe021cc8dccdc48526c24743ec95d",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Linearisation (DFS)",
            "name_full": "Depth-First Traversal Linearisation of Graphs",
            "brief_description": "A standard method that serialises an input graph into a token sequence for use with seq2seq (LSTM) encoders by traversing the graph (depth-first) and emitting node/edge tokens, with heuristics to handle cycles and multiple parents.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "linearization (depth-first traversal)",
            "representation_description": "Traverse the input graph in depth-first order, emitting nodes and edge labels into a sequence; siblings are visited in random order; when a child node is revisited (due to cycles) or has multiple parents it is repeated in the sequence. For WebNLG the authors used provided Gardent et al. scripts; for SR11Deep they followed a Konstas-style AMR linearisation using DFS.",
            "graph_type": "RDF knowledge graphs (WebNLG), semantic dependency graphs (SR11Deep) / AMR-like graphs (mentioned)",
            "conversion_method": "Depth-first traversal with random sibling order; repeat child node occurrences on cycles or when node has &gt;1 parent; use linearisation scripts from Gardent et al. (WebNLG) or Konstas-style AMR traversal (SR11Deep).",
            "downstream_task": "Data-to-text generation (text generation from RDF triples and semantic dependency graphs) as input for sequence-to-sequence LSTM encoder-decoder models.",
            "performance_metrics": "WebNLG LSTM baseline (linearized input): BLEU = 0.526 ± 0.010, METEOR = 0.38 ± 0.00, TER = 0.43 ± 0.01. SR11Deep LSTM baseline: BLEU = 0.377 ± 0.007, METEOR = 0.65 ± 0.00, TER = 0.44 ± 0.01.",
            "comparison_to_others": "Compared directly with GCN encoders that operate on the graph structure: linearised LSTM baseline is outperformed by GCN models on both tasks (WebNLG: GCN 0.535 BLEU vs LSTM 0.526; SR11Deep: GCN 0.647 vs LSTM 0.377).",
            "advantages": "Simple, directly compatible with standard sequence-to-sequence models; reuse of mature LSTM encoder-decoder toolkits and techniques.",
            "disadvantages": "Does not explicitly exploit graph structure; requires heuristics for order and cycle handling which can introduce variability; can lead to poorer performance and more instability (higher stddev across restarts) compared to structured encoders.",
            "failure_cases": "Observed hallucination/over-generation and instability in outputs (repeated / missing content and hallucinations) when using LSTM with linearised graphs; under-generation and repetitions also occur but were more severe for linearised LSTM in qualitative analysis.",
            "uuid": "e8759.0",
            "source_info": {
                "paper_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "GCN encoder",
            "name_full": "Graph Convolutional Network-based Encoder",
            "brief_description": "Direct graph encoder that computes node representations by aggregating neighbor representations through direction-specific linear transforms, edge-label embeddings and learned scalar edge gates; stacked GCN layers produce contextualised node encodings used by an LSTM decoder with attention.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Graph Convolutional Network (GCN) encoding",
            "representation_description": "Each node has a feature vector; for each node the GCN updates representations via sums over neighbours: direction-specific weight matrices (in/out/loop), learned embeddings for edge labels, learned scalar gates per edge to weight contributions, and a non-linearity (ReLU). Multiple GCN layers can be stacked to capture multi-hop context; skip connections (residual or dense) are applied between GCN layers.",
            "graph_type": "Directed labeled graphs: RDF graphs (reified triples) and semantic dependency graphs (SR11Deep).",
            "conversion_method": "No linearisation: the GCN directly consumes the graph structure (nodes, directed labeled edges). Node input features are embeddings (words/lemmas plus morphological features in SR11Deep) instead of a serialized sequence.",
            "downstream_task": "Graph-to-text generation (WebNLG entity description generation; SR11Deep semantic-to-surface realisation); encoder used within attention-based encoder-decoder model (LSTM decoder).",
            "performance_metrics": "WebNLG: GCN (4-layer, residual) BLEU = 0.535 ± 0.004, METEOR = 0.39 ± 0.00, TER = 0.44 ± 0.02. Enhanced variant GCN_EC (pretrained embeddings + copy + NE node words) BLEU = 0.559 ± 0.017, METEOR = 0.39 ± 0.01, TER = 0.41 ± 0.01. SR11Deep: GCN (7-layer, dense) BLEU = 0.647 ± 0.005, METEOR = 0.77 ± 0.00, TER = 0.24 ± 0.01; GCN+feat (with linguistic features) BLEU = 0.666 ± 0.027, METEOR = 0.76 ± 0.01, TER = 0.25 ± 0.01.",
            "comparison_to_others": "GCN encoders outperform linearised-LSTM baselines on both tasks (small gain on WebNLG, large gain on SR11Deep). However, pipeline systems that explicitly model syntax/morphology achieve higher BLEU on SR11Deep (e.g., STUMBA-D 0.794, TBDIL 0.805).",
            "advantages": "Explicitly models relational structure, yielding more informative representations and improved generation quality and stability; reduced hallucination and under-/over-generation compared to linearised LSTM; flexible (stacks layers, supports edge labels/directions and feature-rich nodes).",
            "disadvantages": "Requires graph-specific encoder and implementation (not a drop-in for vanilla seq2seq); stacking many layers without gating leads to poor local minima (gates required); dense skip-connections increase model size; still behind specialised pipeline models on SR11Deep.",
            "failure_cases": "GCN models still show repeated and missing source content (though less than LSTM); performance degrades for too many layers (e.g., BLEU falls for &gt;4 layers on WebNLG unless skip connections used); without scalar gates training gets stuck in poor local minima.",
            "uuid": "e8759.1",
            "source_info": {
                "paper_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Reification",
            "name_full": "Reification of RDF Relations",
            "brief_description": "A preprocessing representation for RDF triples that turns relation labels into explicit nodes connected to subject and object via standardized argument edges (A0, A1), enabling the encoder to produce states for relation nodes and to handle arbitrary relation vocabularies.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "relation reification (RDF reification)",
            "representation_description": "Each RDF triple (subject relation object) is transformed by introducing a new node representing the relation; that relation node is connected to subject and object entities with two labeled edges (A0 for subject, A1 for object). Edges are labeled and modeled by the GCN.",
            "graph_type": "RDF knowledge graphs (WebNLG dataset; DBpedia relations).",
            "conversion_method": "Reify each RDF relation into a new node, add edges (relation -&gt; subject) labeled A0 and (relation -&gt; object) labeled A1. The resulting directed labeled graph is fed directly to the GCN encoder.",
            "downstream_task": "Text generation from RDF triples (WebNLG).",
            "performance_metrics": "Used as part of the GCN pipeline whose WebNLG performance is reported: GCN BLEU = 0.535 ± 0.004; GCN_EC BLEU = 0.559 ± 0.017.",
            "comparison_to_others": "Reification allows the GCN encoder to create explicit relation-node representations, which the authors argue is useful compared to treating relations only as edge labels in linearised sequences; enables efficient modeling of many KB relations.",
            "advantages": "Enables per-relation hidden states, scales to arbitrary numbers of KB relations, and integrates seamlessly with GCN edge-label modeling.",
            "disadvantages": "Increases the number of nodes in the graph (more compute); requires conversion step and consistent labeling (A0/A1).",
            "failure_cases": "No specific failure cases reported for reification alone, but overall model limitations (repetition, under-generation) still occur.",
            "uuid": "e8759.2",
            "source_info": {
                "paper_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "SR11Deep node features",
            "name_full": "Lemma-plus-Feature Node Representation (SR11Deep)",
            "brief_description": "A node representation for semantic dependency graphs that concatenates the lemma embedding with a representation of morphological and punctuation features (sum of feature embeddings) to provide richer node inputs to the GCN encoder.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "lemma + morphological features concatenation",
            "representation_description": "Each graph node is represented as [h_l ; h_f], where h_l is the lemma embedding and h_f is the sum of embeddings for node features (e.g., num=sg, punctuation markers). This concatenated vector is used as the initial node embedding for the GCN.",
            "graph_type": "Semantic dependency graphs (SR11Deep).",
            "conversion_method": "Extend each node metadata to include (lemma, features); map lemma to embedding, map each morphological/punctuation feature to an embedding, sum feature embeddings and concatenate to lemma embedding to form node input vector.",
            "downstream_task": "Surface realisation (SR11Deep) - semantic graph to sentence generation using GCN encoder + LSTM decoder.",
            "performance_metrics": "Adding features to the GCN improved SR11Deep: GCN BLEU = 0.647 ± 0.005; GCN+feat BLEU = 0.666 ± 0.027 (METEOR ~0.76-0.77, TER ~0.24-0.25).",
            "comparison_to_others": "GCN+feat outperforms the plain GCN on SR11Deep, indicating that incorporating linguistic features into node representations improves generation quality compared to using lemma-only node embeddings.",
            "advantages": "Provides explicit morphological and punctuation information to the encoder which improves performance on surface realisation tasks.",
            "disadvantages": "Requires additional feature extraction and embedding parameters; increases input complexity and model size.",
            "failure_cases": "No specific failure cases reported beyond general generation errors; improvements are modest and pipeline systems still outperform end-to-end models on SR11Deep.",
            "uuid": "e8759.3",
            "source_info": {
                "paper_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Delexicalisation / Anonymisation",
            "name_full": "Delexicalisation (WebNLG) and Entity Anonymisation (SR11Deep)",
            "brief_description": "Preprocessing methods that replace entity surface forms with placeholders (delexicalisation) or compact named-entity nodes and assign type-based anonymised IDs to reduce sparsity and vocabulary size in generation models.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "delexicalisation / entity anonymisation",
            "representation_description": "WebNLG: delexicalise entity lexical forms using provided scripts. SR11Deep: compact nodes corresponding to named entities; run NER to tag entities with types (person, location, date) and replace entity mentions with type-specific placeholders with numeric suffixes (e.g., PER_0).",
            "graph_type": "RDF graphs (WebNLG) and semantic dependency graphs (SR11Deep).",
            "conversion_method": "Replace entity word tokens or node subgraphs with placeholders or anonymised IDs before encoding; for SR11Deep, compact multi-token entity nodes and label with NER types and indices.",
            "downstream_task": "Graph-to-text generation (reduces vocabulary sparsity for seq2seq and GCN models).",
            "performance_metrics": "Used in experiments (baseline and GCN unless GCN_EC used where delexicalisation was avoided); WebNLG baseline LSTM with delexicalisation BLEU = 0.526 ± 0.010; experiments show GCN_EC without delexicalisation reaches BLEU = 0.559 ± 0.017.",
            "comparison_to_others": "GCN_EC variant that avoids delexicalisation (uses pretrained embeddings + copy and explicit multi-word NE nodes) outperforms models that used delexicalisation in reported WebNLG results.",
            "advantages": "Reduces vocabulary size and sparsity, simplifying learning and improving generalisation for entities; standard practice in WebNLG baselines.",
            "disadvantages": "Removes surface lexical information requiring post-processing to re-insert entity surface forms; may lose nuance from entity word-level context.",
            "failure_cases": "When delexicalisation is used, models depend on the delexicalisation scripts and post-processing; the paper reports that avoiding delexicalisation and using a copy mechanism with explicit NE-node modeling (GCN_EC) gives better results.",
            "uuid": "e8759.4",
            "source_info": {
                "paper_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Multi-word NE nodes (GCN_EC)",
            "name_full": "Multi-word Named-Entity Node Representation with NE edges (used in GCN_EC)",
            "brief_description": "Instead of delexicalising, multi-word entities are represented by creating separate nodes for each word in the entity, linked to the entity head with special NE-labeled edges, enabling the model to copy or generate multi-word entity surface forms directly.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "multi-word NE node expansion with NE-labelled edges",
            "representation_description": "Represent each token of a multi-word entity as its own node; connect these word-nodes with special NE-labeled edges to the main entity node so the GCN can learn representations for each word and the decoder can use a copy mechanism to produce exact surface forms.",
            "graph_type": "RDF knowledge graphs with multi-token entities (WebNLG).",
            "conversion_method": "Expand multi-word subject/object entities into separate word nodes, connect them to the graph via Named Entity (NE) labeled edges, and feed the expanded graph to the GCN encoder; use pretrained embeddings and a pointer/copy mechanism in the decoder.",
            "downstream_task": "Text generation from RDF graphs (WebNLG) with copy capability to generate entity surface forms.",
            "performance_metrics": "GCN_EC (no delexicalisation, pretrained GloVe embeddings, copy mechanism, 6-layer GCN) WebNLG BLEU = 0.559 ± 0.017, METEOR = 0.39 ± 0.01, TER = 0.41 ± 0.01. This outperforms delexicalised baselines and some shared-task systems (e.g., PKUWRITER BLEU = 0.512).",
            "comparison_to_others": "Outperforms the delexicalised LSTM baseline and some previous systems; still behind ADAPT (which uses sub-word encoding) on WebNLG test BLEU. Shows that avoiding delexicalisation plus copy and explicit NE word nodes is beneficial.",
            "advantages": "Preserves exact multi-word entity surface forms, enables copying, avoids the need for delexicalisation/post-processing, and improves BLEU over delexicalised baselines.",
            "disadvantages": "Increases graph size (more nodes) and model complexity; requires designing NE edges and using a copy mechanism; may require pretrained embeddings for best effect.",
            "failure_cases": "Some repetition in outputs still observed (e.g., duplicate mentions) and GCN_EC sometimes produces repeated sentences, indicating copying and aggregation heuristics are still imperfect.",
            "uuid": "e8759.5",
            "source_info": {
                "paper_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "AMR-style linearisation (mentioned)",
            "name_full": "AMR-style Graph Linearisation (as in Konstas et al. 2017)",
            "brief_description": "A linearisation approach inspired by AMR-to-text work that serialises graphs into sequences for seq2seq models; cited as the inspiration for the SR11Deep baseline linearisation.",
            "citation_title": "Neural amr: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "mention",
            "representation_name": "AMR-style linearization",
            "representation_description": "Serialize the graph into a sequence using traversal orders and bracketed structures (Konstas et al. style); used as a guideline for SR11Deep baseline DFS linearisation.",
            "graph_type": "AMR graphs and AMR-like semantic graphs / semantic dependency graphs.",
            "conversion_method": "Follow Konstas et al.'s linearisation strategies (depth-first traversal with structural markers); referenced rather than reimplemented exactly.",
            "downstream_task": "Graph-to-text generation (AMR-to-text, SR11Deep baseline).",
            "performance_metrics": "",
            "comparison_to_others": "Mentioned as a baseline approach compared to direct graph encoders (GCNs); direct GCN encoding outperformed LSTM models trained on linearised AMR-like inputs in this paper's SR11Deep experiments.",
            "advantages": "Allows reuse of seq2seq models and training pipelines developed for sequence data.",
            "disadvantages": "Loses explicit graph inductive bias; susceptible to issues noted for linearisation (ordering heuristics, cycles, repeated nodes).",
            "failure_cases": "Not detailed in this paper beyond the general problems observed for linearised baselines (instability, hallucination, lower BLEU on SR11Deep).",
            "uuid": "e8759.6",
            "source_info": {
                "paper_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The WebNLG challenge: Generating text from rdf data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "Neural amr: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Graph convolutional encoders for syntax-aware neural machine translation",
            "rating": 2,
            "sanitized_title": "graph_convolutional_encoders_for_syntaxaware_neural_machine_translation"
        },
        {
            "paper_title": "Encoding sentences with graph convolutional networks for semantic role labeling",
            "rating": 2,
            "sanitized_title": "encoding_sentences_with_graph_convolutional_networks_for_semantic_role_labeling"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Semisupervised classification with graph convolutional networks",
            "rating": 1,
            "sanitized_title": "semisupervised_classification_with_graph_convolutional_networks"
        },
        {
            "paper_title": "Get to the point: Summarization with pointer-generator networks",
            "rating": 1,
            "sanitized_title": "get_to_the_point_summarization_with_pointergenerator_networks"
        }
    ],
    "cost": 0.013942,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Graph Convolutional Encoders for Structured Data to Text Generation</h1>
<p>Diego Marcheggiani ${ }^{1,2} \quad$ Laura Perez-Beltrachini ${ }^{1}$<br>${ }^{1}$ ILCC, School of Informatics, University of Edinburgh<br>${ }^{2}$ ILLC, University of Amsterdam<br>marcheggiani@uva.nl lperez@inf.ed.ac.uk</p>
<h4>Abstract</h4>
<p>Most previous work on neural text generation from graph-structured data relies on standard sequence-to-sequence methods. These approaches linearise the input graph to be fed to a recurrent neural network. In this paper, we propose an alternative encoder based on graph convolutional networks that directly exploits the input structure. We report results on two graph-to-sequence datasets that empirically show the benefits of explicitly encoding the input graph structure. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Data-to-text generators produce a target natural language text from a source data representation. Recent neural generation approaches (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Gardent et al., 2017b; Ferreira et al., 2017; Konstas et al., 2017) build on encoder-decoder architectures proposed for machine translation (Sutskever et al., 2014; Bahdanau et al., 2015).</p>
<p>The source data, differently from the machine translation task, is a structured representation of the content to be conveyed. Generally, it describes attributes and events about entities and relations among them. In this work we focus on two generation scenarios where the source data is graph structured. One is the generation of multi-sentence descriptions of Knowledge Base (KB) entities from RDF graphs (Perez-Beltrachini et al., 2016; Gardent et al., 2017a,b), namely the WebNLG task. ${ }^{2}$ The number of KB relations modelled in this scenario is potentially large and generation involves</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>solving various subtasks (e.g. lexicalisation and aggregation). Figure (1a) shows and example of source RDF graph and target natural language description. The other is the linguistic realisation of the meaning expressed by a source dependency graph (Belz et al., 2011), namely the SR11Deep generation task. In this task, the semantic relations are linguistically motivated and their number is smaller. Figure (1b) illustrates a source dependency graph and the corresponding target text.</p>
<p>Most previous work casts the graph structured data to text generation task as a sequence-tosequence problem (Gardent et al., 2017b; Ferreira et al., 2017; Konstas et al., 2017). They rely on recurrent data encoders with memory and gating mechanisms (LSTM; (Hochreiter and Schmidhuber, 1997)). Models based on these sequential encoders have shown good results although they do not directly exploit the input structure but rather rely on a separate linearisation step. In this work, we compare with a model that explicitly encodes structure and is trained end-to-end. Concretely, we use a Graph Convolutional Network (GCN; (Kipf and Welling, 2016; Marcheggiani and Titov, 2017)) as our encoder.</p>
<p>GCNs are a flexible architecture that allows explicit encoding of graph data into neural networks. Given their simplicity and expressiveness they have been used to encode dependency syntax and predicate-argument structures in neural machine translation (Bastings et al., 2017; Marcheggiani et al., 2018). In contrast to previous work, we do not exploit the sequential information of the input (i.e., with an LSTM), but we solely rely on a GCN for encoding the source graph structure. ${ }^{3}$</p>
<p>The main contribution of this work is showing that explicitly encoding structured data with</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Above the Veil is an Australian novel and the sequel to Aenir and Castle. It was followed by Into the Battle and The Violet Keystone.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Giant agreed last month to purchase the carrier.</p>
<p>Figure 1: Source RDF graph - target description (a). Source dependency graph - target sentence (b).</p>
<p>GCNs is more effective than encoding a linearized version of the structure with LSTMs. We evaluate the GCN-based generator on two graph-tosequence tasks, with different level of source content specification. In both cases, the results we obtain show that GCNs encoders outperforms standard LSTM encoders.</p>
<h2>2 Graph Convolutional-based Generator</h2>
<p>Formally, we address the task of text generation from graph-structured data considering as input a directed labeled graph $X=(\mathcal{V}, \mathcal{E})$ where $\mathcal{V}$ is a set of nodes and $\mathcal{E}$ is a set of edges between nodes in $\mathcal{V}$. The specific semantics of $X$ depends on the task at hand. The output $Y$ is a natural language text verbalising the content expressed by $X$. Our generation model follows the standard attention-based encoder-decoder architecture (Bahdanau et al., 2015; Luong et al., 2015) and predicts $Y$ conditioned on $X$ as $P(Y \mid X)=$ $\prod_{t=1}^{|Y|} P\left(y_{t} \mid y_{1: t-1}, X\right)$.</p>
<p>Graph Convolutional Encoder In order to explicitly encode structural information we adopt graph convolutional networks (GCNs). GCNs are a variant of graph neural networks (Scarselli et al., 2009) that has been recently proposed by Kipf and Welling (2016). The goal of GCNs is to calculate the representation of each node in a graph considering the graph structure. In this paper we adopt the parametrization proposed by Marcheggiani and Titov (2017) where edge labels and directions are explicitly modeled. Formally, given a directed graph $X=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is a set of nodes, and $\mathcal{E}$ is a set of edges. We represent each node $v \in \mathcal{V}$ with a feature vector $\mathbf{x}<em v="v">{v} \in \mathbb{R}^{d}$. The GCN calculates the representation of each node $\mathbf{h}</em>$ in a graph using the following update rule:}^{\prime</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_N="\mathcal{N" u="u">{v}^{\prime}=\rho\left(\sum</em>}(v)} g_{u, v}\left(W_{\operatorname{dir}(u, v)} \mathbf{h<em _operatorname_lab="\operatorname{lab">{u}+\mathbf{b}</em>\right)\right)
$$}(u, v)</p>
<p>where $\mathcal{N}(v)$ is the set of neighbours of $v$, $W_{\text {dir }(u, v)} \in \mathbb{R}^{d \times d}$ is a direction-specific parameter matrix. As Marcheggiani and Titov (2017); Bastings et al. (2017) we assume there are three possible directions ( $\operatorname{dir}(u, v) \in{$ in, out, loop $})$ : self-loop edges ensure that the initial representation of node $\mathbf{h}<em v="v">{v}$ affects the new representation $\mathbf{h}</em>}^{\prime}$. The vector $\mathbf{b<em u_="u," v="v">{\text {lab }(u, v)} \in \mathbb{R}^{d}$ is an embedding of the label of the edge $(u, v) . \rho$ is a non-linearity (ReLU). $g</em>$}$ are learned scalar gates which weight the importance of each edge. Although the main aim of gates is to down weight erroneous edges in predicted graphs, they also add flexibility when several GCN layers are stacked. As with standard convolutional neural networks (CNNs, (LeCun et al., 2001)), GCN layers can be stacked to consider non-immediate neighbours. ${ }^{4</p>
<p>Skip Connections Between GCN layers we add skip connections. Skip connections let the gradient flows more efficiently through stacked hidden layers thus making possible the creation of deeper GCN encoders. We use two kinds of skip connections: residual connections (He et al., 2016) and dense connections (Huang et al., 2017). Residual connections consist in summing input and output representations of a GCN layer $\mathbf{h}<em v="v">{v}^{\prime}=\mathbf{h}</em>}^{\prime}+$ $\mathbf{h<em v="v">{v}$. Whilst, dense connections consist in the concatenation of the input and output representations $\mathbf{h}</em>}^{d}=\left[\mathbf{h<em v="v">{v}^{\prime} ; \mathbf{h}</em>\right]$. In this way, each GCN layer is directly fed with the output of every layer before itself.</p>
<p>Decoder The decoder uses an LSTM and a soft attention mechanism (Luong et al., 2015) over</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the representation induced by the GCN encoder to generate one word $y$ at the time. The prediction of word $y_{t+1}$ is conditioned on the previously predicted words $y_{1: t}$ encoded in the vector $\mathbf{w}<em t="t">{t}$ and a context vector $\mathbf{c}</em>}$ dynamically created attending to the graph representation induced by the GCN encoder as $P\left(y_{t+1} \mid y_{1: t}, X\right)=$ $\operatorname{softmax}\left(g\left(\mathbf{w<em t="t">{t}, \mathbf{c}</em>}\right)\right)$, where $g(\cdot)$ is a neural network with one hidden layer. The model is trained to optimize negative log likelihood: $\mathcal{L<em t="1">{N L L}=$ $-\sum</em>, X\right)$}^{[Y]} \log P\left(y_{t} \mid y_{1: t-1</p>
<h2>3 Generation Tasks</h2>
<p>In this section, we describe the instantiation of the input graph $X$ for the generation tasks we address.</p>
<h3>3.1 WebNLG Task</h3>
<p>The WebNLG task (Gardent et al., 2017a,b) aims at the generation of entity descriptions from a set of RDF triples related to an entity of a given category (Perez-Beltrachini et al., 2016). RDF triples are of the form (subject relation object), e.g., (Aenir precededBy Castle), and form a graph in which edges are labelled with relations and vertices with subject and object entities. For instance, Figure (1a) shows a set of RDF triples related to the book Above the Veil and its verbalisation. The generation task involves several micro-planning decisions such as lexicalisation (followedBy is verbalised as sequel to), aggregation (sequel to Aenir and Castle), referring expressions (subject of the second sentence verbalised as pronoun) and segmentation (content organised in two sentences).</p>
<p>Reification We formulate this task as the generation of a target description $Y$ from a source graph $X=(\mathcal{V}, \mathcal{E})$ where $X$ is build from a set of RDF triples as follows. We reify the relations (Baader, 2003) from the RDF set of triples. That is, we see the relation as a concept in the KB and introduce a new relation node for each relation of each RDF triple. The new relation node is connected to the subject and object entities by two new binary relations A0 and A1 respectively. For instance, (precededBy A0 Aenir) and (precededBy A1 Castle). Thus, $\mathcal{E}$ is the set of entities including reified relations and $\mathcal{V}$ a set of labelled edges with labels ${A 0, A 1}$. The reification of relations is useful in two ways. The encoder is able to produce a hidden state for each relation in the input; and it permits to model an arbitrary number of KB relations efficiently.</p>
<h3>3.2 SR11Deep Task</h3>
<p>The surface realisation shared task (Belz et al., 2011) proposed two generation tasks, namely shallow and deep realisation. Here we focus on the deep task where the input is a semantic dependency graph that represents a target sentence using predicate-argument structures (NomBank; (Meyers et al., 2004), PropBank; (Palmer et al., 2005)). This task covers a more complex semantic representation of language meaning; on the other hand, the representation is closer to surface form. Nodes in the graph are lemmas of the target sentence. Only complementizers that, commas, and to infinitive nodes are removed. Edges are labelled with NomBank and PropBank labels. ${ }^{5}$ Each node is also associated with morphological (e.g. num=sg) and punctuation features (e.g. bracket=r).</p>
<p>The source graph $X=(\mathcal{V}, \mathcal{E})$ is a semantic dependency graph. We extend this representation to model morphological information, i.e. each node in $\mathcal{V}$ is of the form (lemma, features). For this task we modify the encoder, Section 2, to represent each input node as $\mathbf{h}<em l="l">{v}=\left[\mathbf{h}</em>\right]$, where each input node is the concatenation of the lemma and the sum of feature vectors.} ; \mathbf{h}_{f</p>
<h2>4 Experiments</h2>
<p>We tested our models on the WebNLG and SR11Deep datasets. The WebNLG dataset contains 18102 training and 871 development datatext pairs. The test dataset is split in two sets, test Seen ( 971 pairs) and a test set with new unseen categories for KB entities. As here we are interested only in the modelling aspects of the structured input data we focus on our evaluation only on the test partition with seen categories. The dataset covers 373 distinct relations from DBPedia. The SR11Deep dataset contains 39279, 1034 and 2398 examples in the training, development and test partitions, respectively. It covers 117 distinct dependency relations. ${ }^{6}$</p>
<p>Sequential Encoders For both WebNLG and SR11Deep tasks we used a standard sequence-to-sequence model (Bahdanau et al., 2015; Luong et al., 2015) with an LSTM encoder as baseline. Both take as input a linearised version of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the source graph. For the WebNLG baseline, we use the linearisation scripts provided by <em>Gardent et al. (2017b)</em>. For the SR11Deep baseline we follow a similar linearisation procedure as proposed for AMR graphs <em>Konstas et al. (2017)</em>. We built a linearisation based on a depth first traversal of the input graph. Siblings are traversed in random order (they are anyway shuffled in the given dataset). We repeat a child node when a node is revisited by a cycle or has more than one parent. The baseline model for the WebNLG task uses one layer bidirectional LSTM encoder and one layer LSTM decoder with embeddings and hidden units set to 256 dimensions. For the SR11Deep task we used the same architecture with 500-dimensional hidden states and embeddings. All hyperparameters tuned on the development set.</p>
<p>GCN Encoders The GCN models consist of a GCN encoder and LSTM decoder. For the WebNLG task, all encoder and decoder embeddings and hidden units use 256 dimensions. We obtained the best results with an encoder with four GCN layers with residual connections. For the SR11Deep task, we set the encoder and decoder to use 500-dimensional embeddings and hidden units of size 500. In this task, we obtained the best development performance by stacking seven GCN layers with dense connections.</p>
<p>We use delexicalisation for the WebNLG dataset and apply the procedure provided for the baseline in <em>Gardent et al. (2017b)</em>. For the SR11Deep dataset, we performed entity anonymisation. First, we compacted nodes in the tree corresponding to a single named entity (see <em>Belz et al. (2011)</em> for details). Next, we used a name entity recogniser (Stanford CoreNLP; <em>Manning et al. (2014)</em>) to tag entities in the input with type information (e.g. person, location, date). Two entities of the same type in a given input will be given a numerical suffix, e.g. PER_0 and PER_1.</p>
<p>A GCN-based Generator For the WebNLG task, we extended the GCN-based model to use pre-trained word Embeddings <em>GloVe (Pennington et al., 2014)</em>) and Copy mechanism <em>See et al. (2017)</em>, we name this variant GCN_{EC}. To this end, we did not use delexicalisation but rather represent multi-word subject (object) entities with each word as a separate node connected with special Named Entity (NE) labelled edges. For instance, the book entity <em>Into Battle</em> is represented as (Into</p>
<table>
<thead>
<tr>
<th>Encoder</th>
<th>BLEU</th>
<th>METEOR</th>
<th>TER</th>
</tr>
</thead>
<tbody>
<tr>
<td>LSTM</td>
<td>.526±.010</td>
<td>.38±.00</td>
<td>.43±.01</td>
</tr>
<tr>
<td>GCN</td>
<td>.535±.004</td>
<td>.39±.00</td>
<td>.44±.02</td>
</tr>
<tr>
<td>ADAPT</td>
<td>.606</td>
<td>.44</td>
<td>.37</td>
</tr>
<tr>
<td>GCN_{EC}</td>
<td>.559±.017</td>
<td>.39±.01</td>
<td>0.41±.01</td>
</tr>
<tr>
<td>MELBOURNE</td>
<td>.545</td>
<td>.41</td>
<td>.40</td>
</tr>
<tr>
<td>PKUWRITER</td>
<td>.512</td>
<td>.37</td>
<td>.45</td>
</tr>
</tbody>
</table>
<p>Table 1: Test results WebNLG task.</p>
<table>
<thead>
<tr>
<th>Encoder</th>
<th>BLEU</th>
<th>METEOR</th>
<th>TER</th>
</tr>
</thead>
<tbody>
<tr>
<td>LSTM</td>
<td>.377±.007</td>
<td>.65±.00</td>
<td>.44±.01</td>
</tr>
<tr>
<td>GCN</td>
<td>.647±.005</td>
<td>.77±.00</td>
<td>.24±.01</td>
</tr>
<tr>
<td>GCN+feat</td>
<td>.666±.027</td>
<td>.76±.01</td>
<td>.25±.01</td>
</tr>
</tbody>
</table>
<p>Table 2: Test results SR11Deep task.</p>
<p>NE Battle). Encoder (decoder) embeddings and hidden dimensions were set to 300. The model stacks six GCN layers and uses a single layer LSTM decoder.</p>
<p>Evaluation metrics As previous works in these tasks, we evaluated our models using BLEU <em>Papineni et al. (2002)</em>, METEOR <em>Denkowski and Lavie (2014)</em> and TER <em>Snover et al. (2006)</em> automatic metrics. During preliminary experiments we noticed considerable variance from different model initialisations; we thus run 3 experiments for each model and report average and standard deviation for each metric.</p>
<h2>5 Results</h2>
<p>WebNLG task In Table 1 we report results on the WebNLG test data. In this setting, the model with GCN encoder outperforms a strong baseline that employs the LSTM encoder, with .009 BLEU points. The GCN model is also more stable than the baseline with a standard deviation of .004 vs .010. We also compared the GCN_{EC} model with the neural models submitted to the WebNLG shared task. The GCN_{EC} model outperforms PKUWRITER that uses an ensemble of 7 models and a further reinforcement learning step by .047 BLEU points; and MELBOURNE by .014 BLEU points. GCN_{EC} is behind ADAPT which relies on sub-word encoding.</p>
<p>SR11Deep task In this more challenging task, the GCN encoder is able to better capture the structure of the input graph than the LSTM encoder, resulting in .647 BLEU for the GCN vs. .377 BLEU of the LSTM encoder as reported in Table 2. When we add linguistic features to the GCN encoding we get .666 BLEU points. We also</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG</th>
<th style="text-align: left;">(William Anders dateOfRetirement 1969 - 09 - 01) (Apollo 8 commander Frank Borman) (William Anders was a crew member of Apollo 8) (Apollo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">8 backup pilot Buzz Aldrin)</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: left;">William Anders was a crew member of the OPERATOR operated Apollo 8 and retired on September 1st 1969.</td>
</tr>
<tr>
<td style="text-align: left;">GCN</td>
<td style="text-align: left;">William Anders was a crew member of OPERATOR 's Apollo 8 alongside backup pilot Buzz Aldrin and backup pilot Buzz Aldrin .</td>
</tr>
<tr>
<td style="text-align: left;">GCN $_{E C}$</td>
<td style="text-align: left;">william anders, who retired on the 1st of september 1969, was a crew member on apollo 8 along with commander frank borman and backup pilot</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">buzz aldrin .</td>
</tr>
<tr>
<td style="text-align: left;">SR11Deep</td>
<td style="text-align: left;">(SROOT SROOT will) (will P .) (will SBJ temperature) (temperature A1 economy) (economy AINV the) (economy SUFFIX 's) (will VC be) (be</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">VC take) (take A1 temperature) (take A2 from) (from A1 point) (point A1 vantage) (point AINV several) (take AM-ADV with) (with A1 reading)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">(reading A1 on) (on A1 trade) (trade COORD output) (output COORD housing) (housing COORD and) (and CUNJ inflation) (take AM-MOD will)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">(take AM-TMP week) (week AINV this)</td>
</tr>
<tr>
<td style="text-align: left;">Gold</td>
<td style="text-align: left;">The economy 's temperature will be taken from several vantage points this week, with readings on trade, output, housing and inflation.</td>
</tr>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: left;">the economy 's accords will be taken from several phases this week, housing and inflation readings on trade, housing and inflation .</td>
</tr>
<tr>
<td style="text-align: left;">GCN</td>
<td style="text-align: left;">the economy 's temperatures will be taken from several vantage points this week, with reading on trades output, housing and inflation .</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of system output.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">none</th>
<th style="text-align: center;">BLEU <br> res</th>
<th style="text-align: center;">den</th>
<th style="text-align: center;">SIZE <br> none</th>
<th style="text-align: center;">den</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">$.543 \pm .003$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1L</td>
<td style="text-align: center;">$.537 \pm .006$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">2L</td>
<td style="text-align: center;">$.545 \pm .016$</td>
<td style="text-align: center;">$.553 \pm .005$</td>
<td style="text-align: center;">$.552 \pm .013$</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.5</td>
</tr>
<tr>
<td style="text-align: center;">3L</td>
<td style="text-align: center;">$.548 \pm .012$</td>
<td style="text-align: center;">$.560 \pm .013$</td>
<td style="text-align: center;">$.557 \pm .001$</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">4.7</td>
</tr>
<tr>
<td style="text-align: center;">4L</td>
<td style="text-align: center;">$.537 \pm .005$</td>
<td style="text-align: center;">$.569 \pm .003$</td>
<td style="text-align: center;">$.558 \pm .005$</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;">5L</td>
<td style="text-align: center;">$.516 \pm .022$</td>
<td style="text-align: center;">$.561 \pm .016$</td>
<td style="text-align: center;">$.559 \pm .003$</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">5.1</td>
</tr>
<tr>
<td style="text-align: center;">6L</td>
<td style="text-align: center;">$.508 \pm .022$</td>
<td style="text-align: center;">$.561 \pm .007$</td>
<td style="text-align: center;">$.558 \pm .018$</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: center;">7L</td>
<td style="text-align: center;">$.492 \pm .024$</td>
<td style="text-align: center;">$.546 \pm .023$</td>
<td style="text-align: center;">$.564 \pm .012$</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">5.5</td>
</tr>
</tbody>
</table>
<p>Table 4: GCN ablation study (layers (L) and skipconnections: none, residual(res) and dense(den)). Average and standard deviation of BLEU scores over three runs on the WebNLG dev. set. Number of parameters (millions) including embeddings.
compare the neural models with upper bound results on the same dataset by the pipeline model of Bohnet et al. (2011) (STUMBA-D) and transitionbased joint model of Zhang et al. (2017) (TBDIL). The STUMBA-D and TBDIL model obtains respectively .794 and .805 BLUE, outperforming the GCN-based model. It is worth noting that these models rely on separate modules for syntax prediction, tree linearisation and morphology generation. In a multi-lingual setting (Mille et al., 2017), our model will not need to re-train some modules for different languages, but rather it can exploit them for multi-task training. Moreover, our model could also exploit other supervision signals at training time, such as gold POS tags and gold syntactic trees as used in Bohnet et al. (2011).</p>
<h3>5.1 Qualitative Analysis of Generated Text</h3>
<p>We manually inspected the outputs of the LSTM and GCN models. Table 3 shows examples of source graphs and generated texts (we included more examples in Section A). Both models suffer from repeated and missing source content (i.e. source units are not verbalised in the output text (under-generation)). However, these phenomena are less evident with GCN-
based models. We also observed that the LSTM output sometimes presents hallucination (overgeneration) cases. Our intuition is that the strong relational inductive bias of GCNs (Battaglia et al., 2018) helps the GCN encoder to produce a more informative representation of the input; while the LSTM-based encoder has to learn to produce useful representations by going through multiple different sequences over the source data.</p>
<h3>5.2 Ablation Study</h3>
<p>In Table 4 (BLEU) we report an ablation study on the impact of the number of layers and the type of skip connections on the WebNLG dataset. The first thing we notice is the importance of skip connections between GCN layers. Residual and dense connections lead to similar results. Dense connections (Table 4 (SIZE)) produce models bigger, but slightly less accurate, than residual connections. The best GCN model has slightly more parameters than the baseline model ( 4.9 M vs. 4.3 M ).</p>
<h2>6 Conclusion</h2>
<p>We compared LSTM sequential encoders with a structured data encoder based on GCNs on the task of structured data to text generation. On two different tasks, WebNLG and SR11Deep, we show that explicitly encoding structural information with GCNs is beneficial with respect to sequential encoding. In future work, we plan to apply the approach to other input graph representations like Abstract Meaning Representations (AMR; (Banarescu et al., 2013)) and scoped semantic representations (Van Noord et al., 2018).</p>
<h2>Acknowledgments</h2>
<p>We want to thank Ivan Titov and Mirella Lapata for their help and suggestions. We also gratefully acknowledge the financial support of the European Research Council (award number 681760) and the Dutch National Science Foundation (NWO VIDI 639.022.518). We thank NVIDIA for donating the GPU used for this research.</p>
<h2>References</h2>
<p>Franz Baader. 2003. The description logic handbook: Theory, implementation and applications. Cambridge university press.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of the International Conference on Learning Representations, $I C L R$.</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186.</p>
<p>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders for syntax-aware neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1957-1967.</p>
<p>Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinícius Flores Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Çaglar Gülçehre, Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. 2018. Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261.</p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283.</p>
<p>Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European workshop on natural language generation, pages 217-226.</p>
<p>Bernd Bohnet, Simon Mille, Benoît Favre, and Leo Wanner. 2011. Stumaba : From deep representation to surface. In ENLG 2011 - Proceedings of the 13th European Workshop on Natural Language Generation, pages 232-235.</p>
<p>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the ninth workshop on statistical machine translation, pages $376-380$.</p>
<p>Thiago Castro Ferreira, Iacer Calixto, Sander Wubben, and Emiel Krahmer. 2017. Linguistic realisation as machine translation: Comparing different mt models for amr-to-text generation. In Proceedings of the 10th International Conference on Natural Language Generation, pages 1-10.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017a. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 179-188. (ACL 2017).</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017b. The WebNLG challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133. (INLG 2017).</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 770-778.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735-1780.</p>
<p>Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. 2017. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, pages 2261-2269.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, $I C L R$.</p>
<p>Thomas N. Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. In Proceedings of the International Conference on Learning Representations, ICLR.</p>
<p>Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, pages 67-72.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural amr: Sequence-to-sequence models for parsing and generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 146-157.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213.</p>
<p>Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. 2001. Gradient-based learning applied to document recognition. In Proceedings of Intelligent Signal Processing.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412-1421.</p>
<p>Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55-60.</p>
<p>Diego Marcheggiani, Joost Bastings, and Ivan Titov. 2018. Exploiting semantics in neural machine translation with graph convolutional networks. In Proceedings of NAACL-HLT.</p>
<p>Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for semantic role labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1506-1515.</p>
<p>Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. 2016. What to talk about and how? selective generation using lstms with coarse-to-fine alignment. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages $720-730$.</p>
<p>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. Annotating noun argument structure for nombank. In Proceedings of the Fourth International Conference on Language Resources and Evaluation, LREC 2004.</p>
<p>Simon Mille, Bernd Bohnet, Leo Wanner, and Anja Belz. 2017. Shared task proposal: Multilingual surface realization using universal dependency trees. In Proceedings of the 10th International Conference on Natural Language Generation, pages 120-123.</p>
<p>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71-106.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.</p>
<p>Laura Perez-Beltrachini, Rania SAYED, and Claire Gardent. 2016. Building RDF Content for Data-toText Generation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1493-1502.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. IEEE Trans. Neural Networks, 20(1):61-80.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083.</p>
<p>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of association for machine translation in the Americas, volume 200.</p>
<p>Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929-1958.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Rik Van Noord, Lasha Abzianidze, Hessel Haagsma, and Johan Bos. 2018. Evaluating scoped meaning representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).</p>
<p>Sam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2243-2253.</p>
<p>Yue Zhang, Manish Shrivastava, and Ratish Puduppully. 2017. Transition-based deep input linearization. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Volume 1: Long Papers, pages 643-654.</p>
<h1>A Supplemental Material</h1>
<h2>A. 1 Training details</h2>
<p>We implemented all our models using OpenNMTpy (Klein et al., 2017). For all experiments we used a batch size of 64 and Adam (Kingma and $\mathrm{Ba}, 2015$ ) as the optimizer with an initial learning rate of 0.001 . For GCN models and baselines we used a one-layer LSTM decoder, we used dropout (Srivastava et al., 2014) in both encoder and decoder with a rate of 0.3 . We adopt early stopping on the development set using BLEU scores and we trained for a maximum of 30 epochs.</p>
<h2>A. 2 More example outputs</h2>
<p>Table 5 shows additional examples of generated texts for source WebNLG and SR11Deep graphs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">WebNLG</th>
<th style="text-align: center;">(Acharya Institute of Technology sportsOffered Tennis) (Acharya Institute of Technology established 2000) (Tennis sportsGoverningBody International Tennis Federation)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">The Acharya Institute of Technology was established in 2000 and is governed by the International Tennis Federation</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">The sport of tennis, governed by the International Tennis Federation, is offered at the Acharya Institute of Technology which was established in 2000 .</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{GCN}_{E C}$</td>
<td style="text-align: center;">the acharya institute of technology was established in 2000 and is governed by the international tennis federation .</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">(Acharya Institute of Technology officialSchoolColour Blue, White and Orange) (Acharya Institute of Technology was given the ' Technical Campus ' status by All India Council for Technical Education)</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">The Archarya Institute of Technology are blue, white and was given the Acharya Institute of Technology .</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">The Acharya Institute of Technology was given the ' Technical Campus ' status by the All India Council for Technical Education in LOCATION . The Institute was given the " Technical Campus " status by the Acharya Institute of Technology .</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{GCN}_{E C}$</td>
<td style="text-align: center;">acharya institute of technology was given the ' technical campus ' status by the all india council for technical education which has blue, white and orange.</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">(Saranac Lake, New York isPartOf Harrietstown, New York) (Saranac Lake, New York isPartOf Essex County , New York) (Adirondack Regional Airport cityServed Lake Placid, New York) (Adirondack Regional Airport cityServed Saranac Lake, New York) (Saranac Lake, New York country United States)</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">Adirondack Regional Airport serves the cities of Lake Placid and Saranac Lake ( Harrietstown ) in the United States</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">Adirondack Regional Airport serves the city of Saranac Lake, which is part of Harrietstown, Essex County , New York, United States .</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{GCN}_{E C}$</td>
<td style="text-align: center;">adirondack regional airport serves the cities of lake placid and saranac lake, essex county, new york, united states . adirondack regional airport serves the city of saranac lake, essex county, new york, united states .</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">(Adisham Hall location Sri Lanka) (Adisham Hall architecturalStyle Tudor Revival architecture) (Adisham Hall completionDate 1931) (Adisham Hall buildingStartDate 1927)</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">Adisham Hall was built in 1927 and completed in 1931 . It was built in the Tudor Revival architecture style and is located in Sri Lanka .</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">Construction of Adisham Hall, Sri Lanka began in 1927 and was completed in 1931 .</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{GCN}_{E C}$</td>
<td style="text-align: center;">adisham hall, sri lanka, constructed in 1931, is located in sri lanka . the hall has the architectural style ' tudor revival '</td>
</tr>
<tr>
<td style="text-align: center;">SR11Deep</td>
<td style="text-align: center;">(SROOT SROOT say) (say A0 economist) (say A1 be) (be SBJ export) (be VC think) (think A1 export) (think C-A1 have) (have VC rise) (rise A1 export) (rise A2 strongly) (strongly COORD but) (but CONJ not) (not AINV enough) (not AINV offset) (offset A1 jump) (jump A1 in) (in A1 import) (jump AINV the) (offset A2 export) (not AINV probably) (strongly TMP in) (in A1 august) (say P .)</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Exports are thought to have risen strongly in August, but probably not enough to offset the jump in imports, economists said .</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">exports said exports are thought to have rising strongly, but not enough to offset exports in the imports in august .</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">exports was thought to have risen strongly in august but not probably to offset the jump in imports, economists said</td>
</tr>
<tr>
<td style="text-align: center;">SR11Deep</td>
<td style="text-align: center;">(SROOT SROOT be) (be P ?) (be SBJ we) (be TMP be) (be SBJ project) (project A1 research) (be VC curtail) (curtail A1 project) (curtail AM-CAU to) (to A1 cut) (cut A0 government) (cut A1 funding) (funding A0 government) (to DEP due) (to R-AM-TMP when) (be VC catch) (catch A1 we) (catch A2 with) (with SUB down) (down SBJ grant) (grant AINV our) (catch P ") (catch P ")</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">When research projects are curtailed due to government funding cuts, are we " caught with our grants down " ?</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">is when research projects is supposed to cut " due " projects is caught with the grant down .</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">when research projects are curtailed to government funding cuts due to government funding cuts, were we caught " caught " with our grant down?</td>
</tr>
</tbody>
</table>
<p>Table 5: Examples of system output.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ There are also some cases where syntactic labels appear in the graphs, this is due to the creation process (see (Belz et al., 2011)) and done to connect graphs when there were disconnected parts.
${ }^{6}$ In both datasets we exclude pairs with $&gt;50$ target words.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Concurrently with this work, Beck et al. (2018) also encoded input structures without relying on sequential encoders.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
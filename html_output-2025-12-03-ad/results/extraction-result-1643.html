<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1643 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1643</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1643</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-257834221</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2204.03897v4.pdf" target="_blank">Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid</a></p>
                <p><strong>Paper Abstract:</strong> Sim-to-real is a mainstream method to cope with the large number of trials needed by typical deep reinforcement learning methods. However, transferring a policy trained in simulation to actual hardware remains an open challenge due to the reality gap. In particular, the characteristics of actuators in legged robots have a considerable influence on sim-to-real transfer. There are two challenges: 1) High reduction ratio gears are widely used in actuators, and the reality gap issue becomes especially pronounced when backdrivability is considered in controlling joints compliantly. 2) The difficulty in achieving stable bipedal locomotion causes typical system identification methods to fail to sufficiently transfer the policy. For these two challenges, we propose 1) a new simulation model of gears and 2) a method for system identification that can utilize failed attempts. The method's effectiveness is verified using a biped robot, the ROBOTIS-OP3, and the sim-to-real transferred policy can stabilize the robot under severe disturbances and walk on uneven surfaces without using force and torque sensors.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1643.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1643.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OP3 Sim-to-Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Real Transfer for ROBOTIS-OP3 Compliant Bipedal Locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>End-to-end study performing sim-to-real transfer of RL policies for a torque sensor-less, high reduction-ratio gear-driven humanoid (ROBOTIS-OP3) using a MuJoCo simulation with an improved actuator model and a re-identification procedure that uses failed real trials (reward distributions) to close the reality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ROBOTIS-OP3</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>51 cm, ~3.5 kg humanoid robot with 20 DOF (6 DOF per leg), Dynamixel XM430-W350 servo motors with metal spur gears at a reduction ratio of 353.5; torque-sensorless, used for balancing and bipedal walking experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>legged robotics / humanoid locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics engine simulation (MuJoCo) used to simulate rigid-body dynamics, contacts, joints, actuators (DC motor + gear model), friction (Stribeck), and control loops; simulation step was 1 ms.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity physics for actuator and contact dynamics (engineered actuator model with directional transmission efficiency and friction), approximate aspects for some control internals</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>actuator electrical dynamics (DC motor model with back-EMF and terminal resistance), gear transmission with directional transmission efficiency (DTE), load-independent friction via Stribeck model (static/Coulomb/viscous), motor armature inertia, joint PD control (variable gain), contact dynamics and body dynamics in MuJoCo, servo command latency modeled/considered.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>internal Dynamixel proprietary position-control behavior was not modeled (authors implement PD control externally with lower frequency), some sensor estimates approximated (body translational velocity approximated from angular velocity on real robot), no explicit torque sensor (robot is torque-sensorless), other servo internal latencies/gain computations left unknown, and visual/sensor noise modeling not emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Actual ROBOTIS-OP3 robot on tilting board (balancing) and flat/uneven boards (walking) with external PC performing PD control at 125 Hz and policy inference at 31.25 Hz; small real-data collections: 5 seconds of excitation squat motion and ten policy episodes used for re-identification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>bipedal balancing on a tilting board and walking (forward locomotion) including traversal of small uneven surfaces (5 mm bricks).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep Reinforcement Learning using Soft Actor-Critic (SAC); lower-level variable-gain PD controller; policies output target joint angles and P gains (D fixed).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Balancing: average reward on real robot >= 80% of expected reward from simulation (10 runs each); Walking: success = robot walks forward without falling for 10 seconds in at least 8 of 10 episodes; additional qualitative measures (traverse uneven surfaces, stable behavior under disturbances).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Domain randomization was tested as a baseline (uniform sampling of simulation parameters across ranges in Table I) but policies trained with naive domain randomization failed to transfer (0/3 attempts); the primary method did not rely on domain randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>unmodeled directional inefficiencies in high-reduction gears (backdrivability mismatch), inaccurate actuator friction/inertia parameters, low-level servo/internal controller behavior (DYNAMIXEL internals) not modeled, control frequency/latency differences, divergence of time-series trajectories in unstable bipedal tasks making trajectory matching ineffective.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>1) Incorporation of a directional transmission efficiency (DTE) gear model and Stribeck friction to reproduce backdrivability; 2) first system identification using an excitation motion (squat) to fit actuator/body parameters; 3) system re-identification using failed policy episodes by matching empirical reward distributions (Wasserstein distance) plus excitation loss (multi-objective optimization) and retraining the policy in simulation; 4) low-dimensional, physically interpretable actuator parameterization that can be identified from small real datasets (5 s squat + ~10 episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Actuator-level fidelity (especially directional transmission efficiency/backdrivability) is critical: excluding the DTE model increased identification error (Lexc 0.070 vs 0.037 with DTE) and prevented successful transfer (Re-Identified w/o DTE: 0/3); matching reward distributions between sim and real is necessary to reproduce task-relevant behavior for unstable bipedal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Not fine-tuning the policy on the robot directly; instead the process used small real-data collections (5s squat + ten episodes of policy rollouts), performed multi-objective re-identification (NSGA-II, 3000 simulation trials) to find simulation parameters matching real reward distributions and excitation motion, then retrained policies in simulation (Phase 2 repeated). SAC training durations: 1M steps (balancing) or 3M steps (walking).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Including DTE in actuator model reduced excitation-motion error Lexc from 0.070 (w/o DTE) to 0.037 (w/ DTE). Success rates: Excitation-only (pre re-id) 0/3; Re-Identified with DTE 3/3 success (both balancing and walking); Re-Identified w/o DTE 0/3; Domain randomization baseline 0/3 — indicating higher-fidelity actuator modeling + re-identification succeeded where lower-fidelity or naive randomization failed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-fidelity modeling of actuator transmission (directional transmission efficiency/backdrivability) is critical for successful sim-to-real transfer on torque-sensorless, high gear-reduction humanoids; combining a compact, physically interpretable gear+motor model with a re-identification procedure that uses failed real-policy episodes (matching reward distributions via Wasserstein distance) allows successful transfer with very little real data (seconds of excitation + ~10 episodes), whereas naive domain randomization or omitting DTE fails.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1643.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1643.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DTE Actuator Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directional Transmission Efficiency (DTE) Gear Model for Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-dimensional gear transmission model that captures asymmetric forward/backward transmission efficiency and models energy loss as an added brake torque, combined with load-independent Stribeck friction, to reproduce backdrivability effects of high reduction-ratio gears in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ROBOTIS-OP3 (actuators)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Gear-driven joints (metal spur gears, reduction ratio 353.5) in XM430-W350 servos; model targets reproducing transmission efficiency asymmetry and frictional losses relevant to backdrivability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robot actuator modeling for sim-to-real</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (actuator implemented within physics engine)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>MuJoCo simulation with actuator model implemented as motor torque generation, brake torque from transmission inefficiency, and Stribeck friction for joint friction approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity actuator dynamics (directional transmission + friction modeled explicitly) within rigid-body physics engine</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>forward/backward transmission efficiencies (η_fw, η_bw), additional brake torque accounting for direction-dependent losses, Stribeck static/Coulomb/viscous friction (fc, fs, kv, qstatic), motor DC electrical dynamics (Kt, Rterm, back-EMF), motor armature inertia scaled by gear ratio.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Antagonistic states approximated by torque cancellation heuristic; potential instabilities mitigated by friction bounds; proprietary servo internal dynamics and control loops not modeled; high-frequency micro-dynamics not explicitly modeled beyond 1 ms step.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Behavioral data from ROBOTIS-OP3 joints collected during excitation motion (squat) and policy rollouts; current, joint angles, velocities and body tilt recorded to identify model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Enables policies trained in simulation to exploit backdrivability for compliant balancing and walking on hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Actuator parameters identified by black-box optimization (Tree-structured Parzen Estimator for first identification; NSGA-II for re-identification) using real-sim data; policies trained with SAC in sim using the actuator model.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Excitation-motion error Lexc (Eq.7) for system ID; task success rates on hardware after transfer (balancing/walking success as defined).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Previous analytic actuator models failed to represent directional efficiency/losses of high reduction gears, causing mismatch in backdrivability and leading to joint oscillations/falls in real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Explicit modeling of directional transmission efficiency and friction leads to better matching of real joint behavior and is necessary (in this setup) for successful sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Including DTE was necessary: Lexc decreased from 0.070 to 0.037 and Re-Identified transfer succeeded only when DTE included (3/3).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Actuator model parameters were identified from measured real motions (excitation and policy episodes) but the model itself was not further tuned online during policy execution beyond re-identification cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Model with DTE reproduced sensor data more accurately and enabled successful transfers; model without DTE yielded higher identification error and failed transfers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A compact DTE gear model that captures directional efficiency and friction is vital to reproduce backdrivability of high-reduction gears and to enable policies to exploit compliance in sim-to-real transfer for bipedal locomotion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1643.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1643.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward-based Re-Identification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>System Re-Identification via Reward-Distribution Matching (Wasserstein)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A re-identification procedure that, after an initial sim-trained policy fails on hardware, collects real episodes and searches simulation parameters to minimize both excitation-motion error and the Wasserstein distance between empirical reward distributions from sim and real, enabling retraining with parameters closer to reality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ROBOTIS-OP3 (policy re-identification pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Pipeline uses robot rollout rewards (cumulative episode rewards) as abstract summaries of behavior to drive multi-objective parameter optimization (excitation loss + reward-distribution distance) and find simulation parameters that reproduce task-relevant dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>sim-to-real adaptation / system identification for RL</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>MuJoCo simulation parameterized by actuator/body parameters; policy rollouts in sim produce reward distributions used for similarity to real episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity physics with parameter search to better match behavioral reward distributions rather than time-series trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>actuators (DTE, motor, friction), body inertial parameters, joint friction, motor armature inertia, contact dynamics — parameterized and searched to match rewards</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Time-series trajectory alignment not used (explicit trajectory matching avoided because of divergence in unstable tasks); policy-internal actuator/controller internals simplified as in main sim.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Small set of failed-policy rollouts (K episodes, typically 10) collected on the real ROBOTIS-OP3; cumulative rewards recorded as R_real for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Adjust simulation parameters so that an already-trained policy produces similar reward distribution in sim as observed on real robot, facilitating retraining and successful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Multi-objective black-box optimization (NSGA-II) minimizing excitation-motion loss (Lexc) and Wasserstein distance W between reward distributions (R_sim(ϕ) and R_real); after re-identification policies are retrained/evaluated in the new sim parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Wasserstein distance between empirical reward distributions and excitation-motion error Lexc; final hardware task success rates (balancing/walking).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Divergence of time-series trajectories due to small policy differences in inherently unstable bipedal tasks made trajectory matching ineffective; lack of task-relevant excitation data in initial system ID.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Using episode-level reward summaries (empirical reward distributions) rather than trajectory matching to compare simulated and real behavior, and performing multi-objective optimization (excitation + reward-distribution) to find sim parameters that reproduce failed behaviors with limited real data.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Matching task-relevant reward distributions (via Wasserstein distance) is a practical proxy for behavior similarity when time-series alignment is infeasible; small amounts of real data (≈10 episodes) suffice when combined with an appropriate parameter search and a realistic actuator model.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Collected K=10 real episodes of the failed policy; ran NSGA-II (3,000 simulation trials) to identify parameters minimizing Lexc and reward-distribution distance; retrained policy in the re-identified simulation (Phase 2 repeated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For unstable bipedal tasks where trajectories diverge, re-identification by matching empirical reward distributions (multi-objective with excitation loss) allows recovery from failed sim-to-real transfers with very little real data and leads to successful transfer when combined with a sufficiently expressive actuator model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Learning locomotion skills for cassie: Iterative design and sim-to-real <em>(Rating: 1)</em></li>
                <li>Learning agile and dynamic motor skills for legged robots <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1643",
    "paper_id": "paper-257834221",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "OP3 Sim-to-Real",
            "name_full": "Sim-to-Real Transfer for ROBOTIS-OP3 Compliant Bipedal Locomotion",
            "brief_description": "End-to-end study performing sim-to-real transfer of RL policies for a torque sensor-less, high reduction-ratio gear-driven humanoid (ROBOTIS-OP3) using a MuJoCo simulation with an improved actuator model and a re-identification procedure that uses failed real trials (reward distributions) to close the reality gap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "ROBOTIS-OP3",
            "agent_system_description": "51 cm, ~3.5 kg humanoid robot with 20 DOF (6 DOF per leg), Dynamixel XM430-W350 servo motors with metal spur gears at a reduction ratio of 353.5; torque-sensorless, used for balancing and bipedal walking experiments.",
            "domain": "legged robotics / humanoid locomotion",
            "virtual_environment_name": "MuJoCo",
            "virtual_environment_description": "Physics engine simulation (MuJoCo) used to simulate rigid-body dynamics, contacts, joints, actuators (DC motor + gear model), friction (Stribeck), and control loops; simulation step was 1 ms.",
            "simulation_fidelity_level": "high-fidelity physics for actuator and contact dynamics (engineered actuator model with directional transmission efficiency and friction), approximate aspects for some control internals",
            "fidelity_aspects_modeled": "actuator electrical dynamics (DC motor model with back-EMF and terminal resistance), gear transmission with directional transmission efficiency (DTE), load-independent friction via Stribeck model (static/Coulomb/viscous), motor armature inertia, joint PD control (variable gain), contact dynamics and body dynamics in MuJoCo, servo command latency modeled/considered.",
            "fidelity_aspects_simplified": "internal Dynamixel proprietary position-control behavior was not modeled (authors implement PD control externally with lower frequency), some sensor estimates approximated (body translational velocity approximated from angular velocity on real robot), no explicit torque sensor (robot is torque-sensorless), other servo internal latencies/gain computations left unknown, and visual/sensor noise modeling not emphasized.",
            "real_environment_description": "Actual ROBOTIS-OP3 robot on tilting board (balancing) and flat/uneven boards (walking) with external PC performing PD control at 125 Hz and policy inference at 31.25 Hz; small real-data collections: 5 seconds of excitation squat motion and ten policy episodes used for re-identification.",
            "task_or_skill_transferred": "bipedal balancing on a tilting board and walking (forward locomotion) including traversal of small uneven surfaces (5 mm bricks).",
            "training_method": "Deep Reinforcement Learning using Soft Actor-Critic (SAC); lower-level variable-gain PD controller; policies output target joint angles and P gains (D fixed).",
            "transfer_success_metric": "Balancing: average reward on real robot &gt;= 80% of expected reward from simulation (10 runs each); Walking: success = robot walks forward without falling for 10 seconds in at least 8 of 10 episodes; additional qualitative measures (traverse uneven surfaces, stable behavior under disturbances).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": "Domain randomization was tested as a baseline (uniform sampling of simulation parameters across ranges in Table I) but policies trained with naive domain randomization failed to transfer (0/3 attempts); the primary method did not rely on domain randomization.",
            "sim_to_real_gap_factors": "unmodeled directional inefficiencies in high-reduction gears (backdrivability mismatch), inaccurate actuator friction/inertia parameters, low-level servo/internal controller behavior (DYNAMIXEL internals) not modeled, control frequency/latency differences, divergence of time-series trajectories in unstable bipedal tasks making trajectory matching ineffective.",
            "transfer_enabling_conditions": "1) Incorporation of a directional transmission efficiency (DTE) gear model and Stribeck friction to reproduce backdrivability; 2) first system identification using an excitation motion (squat) to fit actuator/body parameters; 3) system re-identification using failed policy episodes by matching empirical reward distributions (Wasserstein distance) plus excitation loss (multi-objective optimization) and retraining the policy in simulation; 4) low-dimensional, physically interpretable actuator parameterization that can be identified from small real datasets (5 s squat + ~10 episodes).",
            "fidelity_requirements_identified": "Actuator-level fidelity (especially directional transmission efficiency/backdrivability) is critical: excluding the DTE model increased identification error (Lexc 0.070 vs 0.037 with DTE) and prevented successful transfer (Re-Identified w/o DTE: 0/3); matching reward distributions between sim and real is necessary to reproduce task-relevant behavior for unstable bipedal tasks.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Not fine-tuning the policy on the robot directly; instead the process used small real-data collections (5s squat + ten episodes of policy rollouts), performed multi-objective re-identification (NSGA-II, 3000 simulation trials) to find simulation parameters matching real reward distributions and excitation motion, then retrained policies in simulation (Phase 2 repeated). SAC training durations: 1M steps (balancing) or 3M steps (walking).",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Including DTE in actuator model reduced excitation-motion error Lexc from 0.070 (w/o DTE) to 0.037 (w/ DTE). Success rates: Excitation-only (pre re-id) 0/3; Re-Identified with DTE 3/3 success (both balancing and walking); Re-Identified w/o DTE 0/3; Domain randomization baseline 0/3 — indicating higher-fidelity actuator modeling + re-identification succeeded where lower-fidelity or naive randomization failed.",
            "key_findings": "High-fidelity modeling of actuator transmission (directional transmission efficiency/backdrivability) is critical for successful sim-to-real transfer on torque-sensorless, high gear-reduction humanoids; combining a compact, physically interpretable gear+motor model with a re-identification procedure that uses failed real-policy episodes (matching reward distributions via Wasserstein distance) allows successful transfer with very little real data (seconds of excitation + ~10 episodes), whereas naive domain randomization or omitting DTE fails.",
            "uuid": "e1643.0",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DTE Actuator Model",
            "name_full": "Directional Transmission Efficiency (DTE) Gear Model for Simulation",
            "brief_description": "A low-dimensional gear transmission model that captures asymmetric forward/backward transmission efficiency and models energy loss as an added brake torque, combined with load-independent Stribeck friction, to reproduce backdrivability effects of high reduction-ratio gears in simulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "ROBOTIS-OP3 (actuators)",
            "agent_system_description": "Gear-driven joints (metal spur gears, reduction ratio 353.5) in XM430-W350 servos; model targets reproducing transmission efficiency asymmetry and frictional losses relevant to backdrivability.",
            "domain": "robot actuator modeling for sim-to-real",
            "virtual_environment_name": "MuJoCo (actuator implemented within physics engine)",
            "virtual_environment_description": "MuJoCo simulation with actuator model implemented as motor torque generation, brake torque from transmission inefficiency, and Stribeck friction for joint friction approximations.",
            "simulation_fidelity_level": "high-fidelity actuator dynamics (directional transmission + friction modeled explicitly) within rigid-body physics engine",
            "fidelity_aspects_modeled": "forward/backward transmission efficiencies (η_fw, η_bw), additional brake torque accounting for direction-dependent losses, Stribeck static/Coulomb/viscous friction (fc, fs, kv, qstatic), motor DC electrical dynamics (Kt, Rterm, back-EMF), motor armature inertia scaled by gear ratio.",
            "fidelity_aspects_simplified": "Antagonistic states approximated by torque cancellation heuristic; potential instabilities mitigated by friction bounds; proprietary servo internal dynamics and control loops not modeled; high-frequency micro-dynamics not explicitly modeled beyond 1 ms step.",
            "real_environment_description": "Behavioral data from ROBOTIS-OP3 joints collected during excitation motion (squat) and policy rollouts; current, joint angles, velocities and body tilt recorded to identify model parameters.",
            "task_or_skill_transferred": "Enables policies trained in simulation to exploit backdrivability for compliant balancing and walking on hardware.",
            "training_method": "Actuator parameters identified by black-box optimization (Tree-structured Parzen Estimator for first identification; NSGA-II for re-identification) using real-sim data; policies trained with SAC in sim using the actuator model.",
            "transfer_success_metric": "Excitation-motion error Lexc (Eq.7) for system ID; task success rates on hardware after transfer (balancing/walking success as defined).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Previous analytic actuator models failed to represent directional efficiency/losses of high reduction gears, causing mismatch in backdrivability and leading to joint oscillations/falls in real robot.",
            "transfer_enabling_conditions": "Explicit modeling of directional transmission efficiency and friction leads to better matching of real joint behavior and is necessary (in this setup) for successful sim-to-real transfer.",
            "fidelity_requirements_identified": "Including DTE was necessary: Lexc decreased from 0.070 to 0.037 and Re-Identified transfer succeeded only when DTE included (3/3).",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Actuator model parameters were identified from measured real motions (excitation and policy episodes) but the model itself was not further tuned online during policy execution beyond re-identification cycles.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Model with DTE reproduced sensor data more accurately and enabled successful transfers; model without DTE yielded higher identification error and failed transfers.",
            "key_findings": "A compact DTE gear model that captures directional efficiency and friction is vital to reproduce backdrivability of high-reduction gears and to enable policies to exploit compliance in sim-to-real transfer for bipedal locomotion.",
            "uuid": "e1643.1",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Reward-based Re-Identification",
            "name_full": "System Re-Identification via Reward-Distribution Matching (Wasserstein)",
            "brief_description": "A re-identification procedure that, after an initial sim-trained policy fails on hardware, collects real episodes and searches simulation parameters to minimize both excitation-motion error and the Wasserstein distance between empirical reward distributions from sim and real, enabling retraining with parameters closer to reality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "ROBOTIS-OP3 (policy re-identification pipeline)",
            "agent_system_description": "Pipeline uses robot rollout rewards (cumulative episode rewards) as abstract summaries of behavior to drive multi-objective parameter optimization (excitation loss + reward-distribution distance) and find simulation parameters that reproduce task-relevant dynamics.",
            "domain": "sim-to-real adaptation / system identification for RL",
            "virtual_environment_name": "MuJoCo",
            "virtual_environment_description": "MuJoCo simulation parameterized by actuator/body parameters; policy rollouts in sim produce reward distributions used for similarity to real episodes.",
            "simulation_fidelity_level": "high-fidelity physics with parameter search to better match behavioral reward distributions rather than time-series trajectories",
            "fidelity_aspects_modeled": "actuators (DTE, motor, friction), body inertial parameters, joint friction, motor armature inertia, contact dynamics — parameterized and searched to match rewards",
            "fidelity_aspects_simplified": "Time-series trajectory alignment not used (explicit trajectory matching avoided because of divergence in unstable tasks); policy-internal actuator/controller internals simplified as in main sim.",
            "real_environment_description": "Small set of failed-policy rollouts (K episodes, typically 10) collected on the real ROBOTIS-OP3; cumulative rewards recorded as R_real for optimization.",
            "task_or_skill_transferred": "Adjust simulation parameters so that an already-trained policy produces similar reward distribution in sim as observed on real robot, facilitating retraining and successful transfer.",
            "training_method": "Multi-objective black-box optimization (NSGA-II) minimizing excitation-motion loss (Lexc) and Wasserstein distance W between reward distributions (R_sim(ϕ) and R_real); after re-identification policies are retrained/evaluated in the new sim parameters.",
            "transfer_success_metric": "Wasserstein distance between empirical reward distributions and excitation-motion error Lexc; final hardware task success rates (balancing/walking).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Divergence of time-series trajectories due to small policy differences in inherently unstable bipedal tasks made trajectory matching ineffective; lack of task-relevant excitation data in initial system ID.",
            "transfer_enabling_conditions": "Using episode-level reward summaries (empirical reward distributions) rather than trajectory matching to compare simulated and real behavior, and performing multi-objective optimization (excitation + reward-distribution) to find sim parameters that reproduce failed behaviors with limited real data.",
            "fidelity_requirements_identified": "Matching task-relevant reward distributions (via Wasserstein distance) is a practical proxy for behavior similarity when time-series alignment is infeasible; small amounts of real data (≈10 episodes) suffice when combined with an appropriate parameter search and a realistic actuator model.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Collected K=10 real episodes of the failed policy; ran NSGA-II (3,000 simulation trials) to identify parameters minimizing Lexc and reward-distribution distance; retrained policy in the re-identified simulation (Phase 2 repeated).",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "For unstable bipedal tasks where trajectories diverge, re-identification by matching empirical reward distributions (multi-objective with excitation loss) allows recovery from failed sim-to-real transfers with very little real data and leads to successful transfer when combined with a sufficiently expressive actuator model.",
            "uuid": "e1643.2",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Learning locomotion skills for cassie: Iterative design and sim-to-real",
            "rating": 1,
            "sanitized_title": "learning_locomotion_skills_for_cassie_iterative_design_and_simtoreal"
        },
        {
            "paper_title": "Learning agile and dynamic motor skills for legged robots",
            "rating": 1,
            "sanitized_title": "learning_agile_and_dynamic_motor_skills_for_legged_robots"
        }
    ],
    "cost": 0.01254125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid
8 Dec 2023</p>
<p>Shimpei Masuda 
Kuniyuki Takahashi 
Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid
8 Dec 20234E9D3F73A4F1CB7C8A62489D5B4CBE2BarXiv:2204.03897v4[cs.RO]
Sim-to-real is a mainstream method to cope with the large number of trials needed by typical deep reinforcement learning methods.However, transferring a policy trained in simulation to actual hardware remains an open challenge due to the reality gap.In particular, the characteristics of actuators in legged robots have a considerable influence on sim-to-real transfer.There are two challenges: 1) High reduction ratio gears are widely used in actuators, and the reality gap issue becomes especially pronounced when backdrivability is considered in controlling joints compliantly.2) The difficulty in achieving stable bipedal locomotion causes typical system identification methods to fail to sufficiently transfer the policy.For these two challenges, we propose 1) a new simulation model of gears and 2) a method for system identification that can utilize failed attempts.The method's effectiveness is verified using a biped robot the ROBOTIS-OP3, and the sim-to-real transferred policy can stabilize the robot under severe disturbances and walk on uneven surfaces without using force and torque sensors. 1</p>
<p>I. INTRODUCTION</p>
<p>If robots are able to achieve stable bipedal walking, they can traverse numerous challenging terrains, such as uneven roads and stairs.To achieve this mission, robots need to be robust against disturbances and changes in environments.</p>
<p>Deep Reinforcement Learning (DRL) has been gaining attention as an appealing alternative for high-performance robot control for situations where manually designed alternatives are difficult in the field of locomotion control [1]- [3].Since DRL methods typically require an inordinate number of trials, the sim-to-real approach, in which policies are first trained using simulations and then transferred to a real-world robot, has been attracting attention [4]- [6].However, the limited fidelity of simulations leads to differences in their behavior compared to the real world.This gap is referred to as the reality gap and hinders policies trained in simulation from directly being applicable in the real environment.</p>
<p>There are two popular approaches to narrow this reality gap; making simulations more realistic and making control policy robust against model errors using methods such as Domain randomization [7].We focus on the former because the fine-grained behavior of the actuator needs to be mastered by the policy to achieve superior performance, and domain randomization may not be good at this.There are methods to make a realistic simulation by identifying simulation parameters such as friction and inertial parameters of the physics 1 S. Masuda and K. Takahashi are with Preferred Networks, Inc. {masuda, takahashi}@preferred.jp 2 S. Masuda is also with the University of Tsukuba.This work is done in Preferred Networks, Inc. 1 An accompanying video is available at the following link: https://www.youtube.com/watch?v=-QHx5V9oZDc model reproducing the actual robot behavior [4]- [6], [8]- [10].Especially for legged robots, it has been reported that the model of the actuators has a more significant impact on the reality gap [6], [9], [10].However, the analytical actuator models used in previous studies do not adequately reproduce the characteristics of transmission efficiency, particularly in gears with high reduction ratios [11], [12].This characteristic of transmission efficiency strongly influences backdrivability, the passive movement of joints, due to external/reactive forces from the environment.Moreover, this backdrivability is important for improving locomotion performance [13]- [15].On the other hand, a configuration that uses a high reduction ratio gear without a joint torque sensor is still a popular option in robots that require low-cost and high power, such as humanoids.Therefore, it is useful if DRL can control sensor-less gear-driven actuators by exploiting backdrivability, which was difficult to design manually due to the non-linear characteristics of gears.In this paper, we propose a physics simulation implementation model of geardriven actuators and train the control policy using DRL.The gear simulation model has only a few parameters and is compatible with the system identification described below.</p>
<p>To make simulations more realistic, approaches to building a model of the physics engine representing the robotic system and then identifying the various parameter values of the model have been widely attempted and are often referred to as system identification.To properly identify the system parameters in the domain of the target task, we need to carefully design the data characteristics, such as ranges of joint velocity or torque to be collected or collect large amounts of data, but it increases the risk of robot failure.To improve the efficiency of data collection, using the data while task execution is considered promising.A possible pipeline would be first to perform a rough system identification, then train policy and transfer once, collect the data on which the policy performs the task, and identify simulation parameters again.In this paper, we call the second system identification as system re-identification.However, re-identification by matching time-series sensor data, as done in existing research [8], is difficult for tasks with a narrow set of stable states, such as bipedal balancing and walking, because a slight difference in action can easily make a big difference in the future state compared to tasks where the robot arms are fixed to a base.This paper proposes a reidentification method that focuses on acquired rewards as an abstract value for actual and simulated behavior and uses the difference as a proximity index for behavior.</p>
<p>To summarise the challenges, there is the reality gap problem in applying sim-to-real transfer to torque sensorless and gear-driven robots with bipedal locomotion tasks, and the following two factors contribute to this challenge.</p>
<p>1) Commonly used actuator models in simulations are inadequate in modeling high reduction ratio gears.2) A slight identification error can lead to a massive failure of the sim-to-real transfer, and re-identification by matching time-series sensor data is difficult.In this study, we propose the following to address these issues to reduce the reality gap and increase the success rate of sim-to-real transfers, 1) An actuator model of the high reduction ratio gears for physics simulation.2) A new simulation parameters identification method that utilizes the data on failed transfer, using the acquired rewards in the actual robots.And we show the effectiveness of the proposed method by realizing robust bipedal locomotion, including balancing and walking on an actual robot.</p>
<p>II. RELATED WORK</p>
<p>A. Joint control in RL for legged robots</p>
<p>In recent years, many studies on Reinforcement Learning (RL) for legged robots have been conducted, and many of them have achieved high performance on real robots [4], [9], [16].These studies often applied a position controller to each joint, and an RL policy commands target angles.One of the reasons is that position control improves learning efficiency and task performance compared to torque or velocity control [17].On the other hand, some studies have been conducted to improve robustness to disturbances and environmental recognition errors by exploiting passivity (in other words, utilizing the joint backdrivability) [13]- [15].Even position control can have backdrivability by specifying a low gain, but utilizing that is not well-explored in sim-toreal studies.In this study, we focus on exploiting backdrivability in robots with gear-driven actuators and address some of the difficulties described in the following subsections.</p>
<p>B. Sim-to-real RL for Robotics</p>
<p>It is widely known that controllers trained by RL in simulations do not work well in actual robots due to the reality gap, and various approaches have been developed to overcome this challenge.Domain randomization [7] is a frequently used approach to compensate for the reality gap.The method trains an RL policy in many simulations with randomized dynamics, sensors, or appearances, such as joint friction or textures of the object.Domain randomization works by increasing the robustness of the controller to a wider range of configurations in the hopes of better transferring to the real world.However, specifying appropriate distributions of randomization for the various simulation parameters is a difficult problem.Too much randomization tends to make the RL policy too conservative, turning the performance poor, and too little will not have the desired generalization effect.</p>
<p>Another approach to address the reality gap is precisely reproducing the real behavior of the system on the simulation [5], [8] and training a policy on that.Assuming that simulation is characterized by some dynamics model parameters, such as the size of joint friction, reproducing the real behavior corresponds to finding parameter values or distributions.This process is generally referred to as system identification.We can roughly categorize prior work that takes this approach as 1) manual system identification [4], [9], 2) system identification based on collected generic data [6], [10], and 3) system identification based on rolled out RL policy behavior [5], [8].Since our focus is on frictional gear-driven joint and utilization of backdrivability, our approach combines 2) and 3).If we choose 1) or 2) approach, it may require a large amount of data on situations, including various external torques and driving states.Moreover, tasks with instability, such as bipedal walking, where a slight difference in action can easily make a big difference in the future compared to tasks with arms fixed to a base.In such cases, precise system identification is challenging by 3) with trajectory comparison [8] or parameter prediction [5].In this paper, we use both 2) and 3) as complementary approaches.It allows relatively small data collection and system identification using the failed trials of the rolled-out RL policy.</p>
<p>C. Modeling Actuators in Sim-to-Real Transfer</p>
<p>The reality gap is especially critical for actuators in legged robots.Several kinds of research have been conducted to improve the actuator models in the simulation, broadly classified into two categories to address the challenge: 1) Use a neural network for the actuator model and train with largely collected data [6], [10] (Network training corresponds to system identification).2) Design a detailed model of the actuator and implement it in the simulation [4], [18] The first method can potentially build highly accurate models but requires large amounts of data.In this study, we extend category 2) to model actuators with high reduction ratio gears with low-dimensional parameters.Parameters would be obtained with relatively small amounts of data since the number of parameters of the gear model is small.In addition, since high reduction ratio gears are a major characteristic, the model can be applied to many actuators.
T t=1 γ (t−1) R(s t , a t )],
where T is the episode length.In our robot control problem, S corresponds to sensor data, A to commands to joint actuators, and state transitions to steps in the control loop.Although various RL algorithms have been proposed, in this study, we use Soft-Actor-Critic [19], which supports continuous action spaces and has been reported to have high performance on a wide range of tasks.Specific action space, observation space, and reward function are described in Section V-C.1.</p>
<p>IV. METHOD</p>
<p>The proposed sim-to-real transfer method consists of two main components: a gear-driven actuator model for physics engines to improve robot simulation, and a system identification method to specify the simulation parameters for RL training.The method consists of the following three phases.Phase 1: Perform first system identification with excitation motion, such as squatting motion.Phase 2: Using the identified parameters, train the RL policy using simulation, then run the trained policy on the actual robot.Phase 3: If the robot does not work well, re-identify the simulation parameters using the failed attempts.Phase 2 is then executed again.</p>
<p>We first describe the proposed actuator model.Then, the first system identification is described, followed by the system re-identification.</p>
<p>A. Actuator Modeling</p>
<p>This section describes our actuator model.The actuator consists mainly of a DC motor (Section IV-A.1) and gears (Section IV-A.2), as illustrated in Fig. 2. The actuator is controlled by variable gain PD, as described in Section IV-A.3.In this study, we implement this actuator model on the physics engine Mujoco [20], but note that it can be implemented in other physics engines.</p>
<p>1) DC Motor Model: For the DC motor model, we use the commonly used model [4], which is represented by the following equation:
τ = K t I, where I = V pwm − V back−emf R ter , V back−emf = K t q
(1) where τ is the exerted torque by the motor, I is the current, K t is the torque constant of the motor, R ter is the terminal resistance, V pwm is the voltage applied to the motor, V back−emf is the back electromotive force (EMF) voltage, and q is the angular velocity of the shaft.</p>
<p>A current controller controls voltage to drive the DC motor with the target current.The applied voltage V pwm at each simulation time step can be formulated as follows using the target current I targ and battery voltage V battery :
V targ pwm = R ter I targ + V back−emf V pwm = min(max(V targ pwm , −V battery ), V battery )(2)
2) Gear Model with Directional Transmission Efficiency: There is an asymmetry in transmission efficiency depending on the drive direction [11], [12], and often this asymmetry is more significant, especially at high reduction ratios.We propose a simulation model considering this directional transmission efficiency (DTE) characteristic.The model has the feature of reproducing the loss by transmission efficiency through additional braking torque and is easy to incorporate into the existing physics engine.</p>
<p>The gear model considers the transmission efficiency and load-independent friction loss.The former depends on the force, while the latter mainly depends on the angular velocity.The method treats the force loss due to transmission efficiency as an additional brake torque applied to the joint in each simulation time step.The brake torque is calculated from the generated torque by the motor, load torque, and the forward and backward transmission efficiencies.Assuming that the simulation time step is small enough for the target dynamics, the brake torque is calculated as follows:
τ brake (τm, τa, η f w , η bw ) =    −L f w if sign(η f w τm + τa) = sign(τm) −L bw if sign(τm + η bw τm) = sign(τa) −(τm + τa) else where, L f w = (1 − η f w )τm, L bw = (1 − η bw )τa (3)
where τ m is the generated torque from the motor, τ a is the load torque on the joint, and η f w and η bw are the forward and backward transmission efficiencies, respectively.τ m = r gear τ where r gear is gear reduction ratio.When τ m is clearly greater than τ a or the directions of τ m and τ a are the same, it is regarded as a forward drive state, and the brake torque based on the η f w is generated.If the directions of τ m and τ a are opposite and τ a is larger even after considering the η bw , it is considered to be in the backward drive state, and the brake torque based on the η bw is generated.A state that does not meet either condition is an antagonistic state with no apparent difference between τ m and τ a , and thus generates a brake torque, such that τ m + τ a + τ brake = 0. Since τ a is approximated using the value of the current simulation state, there is a concern that the simulation may become unstable.However, it is expected to be stable when used in conjunction with the load-independent friction as below.The load-independent friction loss generates torque that cancels joint torque in the absolute upper bound.In particular, Mujoco will handle it as a constraint and allow the constant value.So we calculate the friction loss value at each time step to generate approximated static and viscous friction.For the calculation, Stribeck function τ f ric is used:
τ f ric = f c + s(f s − f c ) + k v | q|, where s = exp(| q|) qstatic(4)
where each parameter f s , f c , k v , and qstatic are the static friction force, the Coulomb friction coefficient, the viscous friction coefficient, and the angular velocity value regarded as non-static, respectively.</p>
<p>3) Variable Gain PD: The actuator model described so far is controlled by the PD controller.We adopted this because previous research has reported that placing the position control at the lower level of the RL policy improves learning efficiency and performance [17].Generally, the PD controller is used with fixed gains, but in this study, the gains are also commanded with the target position.It allows a strategy to increase backdrivability while reducing target position tracking performance by specifying a smaller gain, depending on the task situation.</p>
<p>B. System Identification for Sim-to-real Transfer</p>
<p>This section describes the system identification method of the simulation parameters, such as the motor torque constant or frictions of the actuator model shown in Section IV-A.</p>
<p>1) First System Identification: First, a suitable motion for system identification (we call it excitation motion) is designed as time series data θ targ of the target angle of each joint.The excitation motion used in this study will be described in Section V-B.A simple position control feedback system is provided to track θ targ in the actual robot.We then collect a set of sensor data O real for the robot in motion.This sensor data includes the angle of each joint, angular velocity, current, and the tilt of the upper body.Similarly, on the simulation built based on the parameters ϕ that includes parameters of body inertial and actuator model (actually used in this study are shown in Table I), the same position control feedback system is used to perform the motion to track θ targ and obtain O sim .The parameter ϕ that minimizes the difference between O real and O sim is calculated by sampling-based black-box optimization as follows: min
ϕ L exc (O sim (ϕ), O real )(5)
where L exc is an error function calculates distance between O sim and O real .The specific definition of L exc is described in Section V-B.Some related works employ a similar method for system identification [6], [21].The method is suitable for sim-to-real transfer due to the following advantages: a) The simulation can be improved from the results of the actual robot alone without detaching the actuator from the robot, and b) since it is optimizing the various parameters simultaneously, the behavior of the combination of each parameter can be matched to the actual robot which reduces the reality gap.Note, however, that even when looking for combinations of parameters, characteristic components such as gear models need to be modeled.</p>
<p>2) System Re-Identification from Failed Transfer: Even if we use the simulation improved by the identification described in the previous section for RL training, the transferred policy sometimes does not work.For example, the actual robot may not even be able to stand on flat ground if a policy has been trained to balance on the tilting board in a simulation.In such cases, task-relevant data is lacking in the first system identification, and a possible solution is using data collected by the transferred policy while performing the task.The goal here is to find simulation parameters that reproduce the behavior of an actual robot not only for the excitation motion but also for the behavior while doing a task by the transferred policy.However, especially in bipedal tasks, a slight difference in policy command can easily make a big difference in the future state, and the sensor data, such as joint angle trajectory, widely diverge.So, it is challenging to properly calculate the distance between behaviors on simulation and actual by the transferred policy (we call it a policy performance gap) by comparison of sensor data for each time series in the same way as eq.( 5).Therefore, we employ the acquired reward as an abstract value for the behavior and calculate the distance between two empirical distributions of the reward of episodes as a proximity index.We formulate this re-identification process as a multi-objective optimization problem with two objectives: the minimization of the excitation motion gap to achieve accurate identification and the minimization of the policy performance gap.
min ϕ L exc (O sim (ϕ), O real ), W (R sim (ϕ), R real ) (6)
Where, R sim and R real are a list of cumulative rewards of each K time task episode.R = [R k=1 , R k=2 ...R k=K ] and R k = T t=1 R eval (s t , a t ).R eval is a reward function for evaluation that is used for both real results and simulation.R sim (ϕ) is the reward values for running policy π in K times on the simulation parameterized ϕ.R real is the reward of episodes in the actual environment, and it will be fixed during optimization.W calculates the Wasserstein distance for two empirical distributions of rewards.</p>
<p>Optimization provides simulation parameters ϕ that make the acquired reward distribution as close to reality as possible, keeping the error in the excitation motion as small as possible.In the ideal problem setting, the first objective L exc and the second objective W can be optimal simultaneously, and multi-objective optimization is unnecessary.However, in the real problem, L exc and W are in a bit trade-off relationship, so multi-objective optimization suits this method.</p>
<p>V. EXPERIMENTAL SETUP</p>
<p>A. Robot Setup: ROBOTIS-OP3</p>
<p>In this study, we use ROBOTIS-OP3 [22].ROBOTIS-OP3 has a length of 51 cm, a weight of about 3.5 Kg, 6-DOFs in each leg, and 20-DOFs in total.All the joints of the robot are composed of Dynamixel servo motors XM430-W350.The stall torque is 4.1 Nm at 12.0 V and 2.3 A. Metal spur gears are used for gears and have a reduction ratio of 353.5.For the PD control of the joints, the control loop was executed by a PC on the robot, not in servos, and the output value of PD is commanded to the servo as the target current.This configuration is intended to avoid unknown specifications inside the servo, such as gain calculation, as much as possible.Due to communication constraints, the frequency of PD control is 125 Hz.Action inference of the policy is performed on a PC external to the robot at 31.25 Hz.Note that the communication latency is considered in our</p>
<p>B. System Identification</p>
<p>Simulation parameters used in this study for system identification are shown in Table I.Note that the motor armature is the additional link inertia caused by the rotor inertia of the DC motor amplified with the gear reduction ratio.The simulation was implemented using Mujoco [20].The size of the simulation time step was set to 1 ms.</p>
<p>For the eq.( 5) of the black-box optimization for first system identification, we use the Tree-structured Parzen Estimator algorithm implemented in Optuna [23] since it gives better results than CMAES in our experimental setup.For the multi-objective optimization eq. ( 6) for re-identification, we used the NSGA-II algorithm, which is known to perform well in multi-objective optimization provided implementation by Optuna.We set the number of simulation trials for each optimization in the experiment to 2,000 for the first system identification and 3,000 for re-identification.</p>
<p>A simple squatting motion is used for the excitation motion θ targ for system identification.While keeping a forwardleaning posture, execute a flexion/extension movement such that the knee joint angle flexes from 1.47 rad to 0.6 rad at a speed of approximately 0.5 Hz.For the function L exc to evaluate the difference of motions, eq. ( 7) is used.
Lexc = 1 T T t=1 (||r sim t − r real t || 2 + || ṙsim t − ṙreal t || 2 )+ (7) 1 N T T t=1 N i=1 (||θ sim t,i − θ real t,i || 2 + || θsim t,i − θreal t,i || 2 + ||I sim t,i − I real t,i || 2 )
Where, r is upper body orientation, ṙ is body angular velocity, θ is the joint angle, and I is joint current.N is the number of focusing joints, and T is the number of time steps of the motion.</p>
<p>C. Task for Evaluation</p>
<p>We designed two tasks for method evaluation, the balancing task and the walking task.For both tasks, we use common action space, observation space, and reward function.The common parts are described first, followed by a detailed description of each task.</p>
<p>1) Action space, observation space, and reward function: The action of the policy is the target angle and P gain of PD control for each joint.There are 10 joints to be controlled, five in each leg, and the action space has 20 dimensions.Since our experimental setup doesn't need explicit yaw axis rotation for task completion, we omit the hip-yaw axis for simplification.The P gain is commanded at a value between 6 and 0.1, and the D gain is fixed at 0.1.When the gain is at a small value, the joint is compliantly backdriven by external forces in the vicinity of the target angle.The observation of RL policy in this task includes the following elements:</p>
<p>• The position and velocity of the six key points placed on the three corners of each leg soles with the base link as the origin (36 dims) • Command in the previous step (20 dims) • Body orientation in the Euler angle (3 dims)</p>
<p>• Body angular velocity (3 dims) • Periodic phase signal (2 dims, only for walking) To enable policies considering the time series of the state, we concatenate the above observation obtained in the previous n step with the observation obtained in the latest step, then feed it to the policy.We use n = 1 for the balancing task and n = 3 for the walking task.</p>
<p>The reward function is designed similarly to the prior study by [16], so refer to their paper for some details.
R(s, p) = K bipedal • R bipedal (s, p) + K cmd • R cmd (s) + K smooth • Rsmt(s) + 1 R cmd (s) = (−1) • (1 − exp(−K xd • | ẋdesired − ẋactual |) + (−1) • (1 − exp(−K yd • | ẏdesired − ẏactual |) + (−1) • (1 − exp(−4 • |r desired − r actual |)(8)Rsmt(s) = (−1) • (1 − exp(−0.1 • |at − a t−1 |)) + (−1) • (1 − exp(−0.05 • |I • 10|) + (−1) • (1 − exp(−0.1 • (| ṙ| + |acc pelvis |)))
Where, R bipedal is the penalty term for learning to step by the leg synchronized with the periodic signal by eq. ( 2) in [16].Each coefficient K has a different value appropriate for each task as shown in Table II.For example, since stepping is unnecessary for the balancing task, K bipedal was set to zero.p is the periodic phase signal only used for the walking task.ẋ and ẏ are the translation velocity of robot body, r is body orientation, ṙ is body angular velocity, acc pelvis is a linear acceleration of robot body.I is the vector of joint motor currents, and a t is the action vector commanded by policy at time step t.We use eq.( 8) for R eval in the re-identification process with coefficients on Table II.Note that due to the sensing limitation on the actual robot setting, ẋactual and ẏactual are approximately estimated.Details are in each task section.</p>
<p>2) Balancing task: The objective of the balancing task is to maintain balance during standing upright on a board with a dynamically changing tilt.Since the center of gravity movement should be minimized, ẋdesired and ẏdesired on the reward are always set to zero.During training, a board on which the robot stands vibrates with a randomly generated wave in the roll-pitch axis.The wave amplitude is up to 10 • .Training episodes are terminated when the upper body tilts largely or the body height lower the threshold.For Fig. 3: Result of system identification with squat motion evaluation of the actual robot, we use a board that tilts 6 • for the pitch axis.With the unknown board angle, the control policy quickly tries to get the upper body posture vertical and stationary.To calculate ẋactual and ẏactual on R eval , we use approximated value from body angular velocities.</p>
<p>3) Walking task: The bipedal walking task is designed as a task that requires more active motion than the balancing task.The objective is to walk forward on a flat surface at a constant speed.The target walking cycle was set at 1.0 sec, and the ratio of the support leg phase to the swing leg phase was set at 0.7 : 0.3.We set the target speed as ẋdesired = 0.3 and ẏdesired = 0.For evaluation of the actual robot, we make it attempt to walk on a flat board.The position where the inner tip of the robot foot traveled furthest in the forward direction is recorded and divided by time to obtain the average speed, which is used as ẋactual and ẏactual in R eval at each time step.</p>
<p>D. DRL training and optimization setup</p>
<p>For the experiments, including DRL training and blackbox optimizations in the system identification process, we use a machine that equips Intel Core i7-9700KF CPU @ 3.60GHz and NVIDIA GeForce RTX 2060 GPU.The neural network of the policy of DRL is fully-connected and has two hidden layers with ReLU activation.The node size of each hidden layer is set to 256 for the balancing task and 1024 for the walking task.We train a Soft-Actor-Critic agent for 1, 000, 000 steps for the balancing task and 3, 000, 000 steps for the walking task, with a training time of approximately 5 and 15 hours, respectively.Black-box optimizations in system identification take about 2 hours for the first system identification and about 5 hours for re-identification.</p>
<p>VI. RESULTS</p>
<p>The following three points are evaluated to verify the effectiveness of the proposed method: 1) the effectiveness of the actuator model for sim-to-real (Section VI-A), 2) the proposed system identification method for sim-to-real (Section VI-B), and 3) how well does the trained policy perform on the actual robot (Section VI-C).</p>
<p>A. Evaluation of Actuator Modeling</p>
<p>First, we show the effect of the DTE model in the first system identification with the squatting motion.We performed the optimization of eq. ( 5) with and without the DTE model, respectively.The error evaluation score according  to eq. ( 7) was 0.037 when the DTE model was used ("w/ DTE") and 0.070 when it was not used ("w/o DTE").Fig. 3 shows the graphs of the sensor data of the actual and each simulated robot results."w/ DTE" model reproduced sensor data better in comparison to "w/o DTE".In particular, the DTE's ability to represent changes in a frictional loss in static and driving conditions is thought to have contributed to the reproducibility of the overall behavior.</p>
<p>B. Evaluation of Sim-to-real Transfer Method</p>
<p>In this section, we evaluate the proposed system identification method for sim-to-real transfer.We performed the sim-to-real transfer process described in Section IV for the balancing and walking tasks.The whole transfer process was conducted three times, with different seed values, and the results are shown in Table III and Table IV.The left column lists the names of the set of simulation parameters ϕ to train the policy, and the right column shows the number of successful transfer attempts."Excitation only" corresponds to the identified simulation parameters with the first system identification at phase 1, and "Re-Identified" corresponds to the re-identified parameters at phase 3.For the balancing task, the success of the transfer was determined by comparing the rewards obtained in the simulation and on the actual robot.The policy was run ten times in the evaluation setting on the simulation with parameter ϕ used for policy training and evaluated with R eval to obtain the expected reward value.Then, the policy was run ten times on the actual robot, and the transfer was judged successful if the average reward value achieved more than 80% of the expected value.For the walking task, we executed each policy ten times on the actual robot and judged the transfer successful when the robot could go forward without falling for ten seconds in eight episodes.</p>
<p>As shown in Table III and Table IV, policies trained with "Excitation only" failed to work on the actual robot.Then re-identification was processed, and policies trained with "Re-Identified" successfully transferred in all attempts.Despite the small amount of actual robot operation time for data collection (only 5 seconds of squat motion and ten episodes on the actual robot), sim-to-real transfer was achieved without domain randomization."Excitation only (w/o DTE)" and "Re-Identified (w/o DTE)" in Table III shows the results of when we performed the sim-to-real transfer process using simulation with the DTE model excluded.As shown in the table, it failed to sim-to-real without the DTE model.The "Re-Identified (w/o DTE)" made little difference to the behavior of the failure, and the policy fell over because it could not control its movements.The re-identification process found parameters that reduced the policy performance gap while limiting the increase in the L exc but was still considered far from the actual behavior due to the limited reproducibility seen in Fig. 3. From this result, we can say that DTE is one of the critical components of the actuator model in our experimental settings.As the need for a DTE model was demonstrated, experiments with a walking task were omitted.</p>
<p>In addition, we trained policies with naive domain randomization as a baseline.During training, each simulation parameter is uniformly sampled within a range shown in Table I.We conducted the training with three seed values, but the transferred policy could not perform well on the actual robot in any attempt.Fig. 4 shows learning curves in the balancing task, showing that the average performance on the randomly sampled parameter sets converged.This observation suggests that policies that work well in most parameter sets were well-trained, but they didn't work in some local parameter sets, including those that are close to the real.Such a problem requires a process that can learn appropriate actions for parameter sets close to the actual robot, as in the proposed method.</p>
<p>We analyzed the results of the reward value and identified simulation parameters.Fig. 5 (b) shows an example of reward distribution from the walking task experiment.In the graph, "Excitation only" shows reward distribution on the simulation used for training, "Re-Identified" shows rewards acquired by the same policy on the simulation after re-identification, and "Actual" shows rewards on the actual.We can see re-identification could find the simulation parameters to close the reward distribution actual.If the reward distribution is close between the actual robot and the simulation, similar behavior can be produced.Therefore, we can say that the re-identification approach finds the simulation parameters closer to the actual.Fig. 5 (a) shows the mean and standard deviation of identified simulation parameter values of both the balancing and walking tasks.Values are normalized by the range shown in Table I.The variances are not slight, but this is likely due to several combinations of parameter values showing similar physical behavior.Among them, backward efficiency tends to increase with re-identification.In both tasks, leg joint oscillations and falls were observed in actual cases of failed transfers, apparently due to the joints not stopping due to reaction forces from the ground.Parameter changes are likely to be the result of re-identification reflecting these data.</p>
<p>C. Evaluation of Policy Performance</p>
<p>To verify the performance of the policy acquired with the proposed method, we demonstrate and evaluate the tasks on an actual robot.Each motion of the policy can be viewed in the supplementary video.</p>
<p>1) Balancing Task: Fig. 6 shows the snapshot of the balancing task experiment.The robot is placed on a board that freely tilts from −6 • to 6 • on the pitch axis.Disturbances 2) Walking Task: To realize the robust policy utilizing the backdrivability this research aims for, we trained a walking policy on the simulation with random uneven terrains.For training, the re-identified simulation parameters with the walking task were used.The trained policy passed three uneven surfaces for testing; the surface patterns and snapshots of walking are shown in Fig. 7.We used some toy brick parts to introduce various unevenness.Each brick has a 5 mm height on top.Fig. 8 shows the right ankle pitch joint command by the policy while walking.The P gain is reduced as the leg transitions into the support phase, indicating a compliant landing.</p>
<p>In addition, we tested an existing walking module of ROBOTIS-OP3 with the default settings as a baseline using stiff position-controlled joints.The walking module failed on these uneven surfaces.The fact that the policy could traverse uneven surfaces, which is difficult with a general walking controller, without force or contact sensors indicates that the policy can take advantage of backdrivability.</p>
<p>VII. DISCUSSION</p>
<p>The trained policy demonstrated robustness to uneven terrain, but the motion smoothness and walking speed are future issues.One of the main reasons for the low policy performance is the PD control of the joints.In this paper, we did not use DYNAMIXEL's internal position control.As mentioned in Section V-A, this is to avoid performing a system identification that includes the unknown behavior of DYNAMIXEL's position control that it may have.Instead, we used a system that calculates PD control by PC inside the robot.However, in this system, due to the communication limitation, the control frequency of the PD control is 125 Hz, which is much lower than general, and there is command latency.Due to these factors, we could not ensure the position control's stability if the D-gain was increased beyond a certain level, so the D-gain had to be set excessively low.This is also why the D-gain change was omitted from the policy action.The low D-gain made the position control behave like a spring and was difficult for the policy to handle, leading to poor performance.A possible way to improve policy performance would be to utilize DYNAMIXEL's internal position control.DYNAMIXEL's internal position control is better, at least in terms of control frequency and latency, and it is expected that D-gain could also be added to the policy's actions.To achieve sim-to-real after replacement, a system identification including the DYNAMIXEL's position control would have to be carried out while utilizing the results of this paper.</p>
<p>VIII. CONCLUSION</p>
<p>In this paper, we proposed a sim-to-real transfer method featuring the actuator model with the DTE and a reidentification method that utilizes the acquired rewards of failed transfer.The actuator model improved the ability to reproduce the robot motion, and the re-identification method enabled the identification of more realistic parameters that reproduce the failed behavior in the actual robot, resulting in a successful transfer.The method was verified on the torque sensor-less gear-driven robot ROBOTIS-OP3 and achieved balancing under disturbance and walking on uneven surfaces.</p>
<p>Fig. 1 :
1
Fig. 1: Concept of the proposed methods</p>
<p>+Fig. 2 :
2
Fig. 2: Overview of the actuator modeling III.PRELIMINARY OF RL Let (S, A, T, R, γ, s 0 ) be a Markov decision process with state s ∈ S, action a ∈ A, transition probability T (s t+1 |s t , a t ), reward function R(s, a) ∈ R, discount factor γ ∈ [0, 1], and initial state distribution s 0 .RL typically learns a policy π : S → A to maximize the expected discounted accumulated reward E[</p>
<p>(a) Left knee joint velocity (b) Body pitch angle time [s]</p>
<p>Fig. 4 :Fig. 5 :
45
Fig. 4: Learning curves of RL training in balancing task</p>
<p>Fig. 6 :Fig. 7 :
67
Fig. 6: Snapshots of balancing behavior.Applying disturbance by putting the weight.t=0.0 t=2.9 t=6.23 t=9.2 t=16.33</p>
<p>TABLE I :
I
List of parameters for system identification
ParameterUnitRangemotor Kt-[0.003, 0.009]motor RterΩ[4.0, 9.0]motor armaturekgm 2[0.0025, 0.011]gear forward efficiency-1.0 (Fixed)gear backward efficiency-[0.6, 1.0]joint fcNm[0.01, 0.25]joint kv-[0.0025, 0.15]joint fsNmfc + [0.0, 0.25]base mass offsetKg[0.0, 0.5]base CoM offset xm[-0.02, 0.02]base CoM offset zm[-0.02, 0.02]simulation.</p>
<p>TABLE II :
II
Coefficients of reward function
CoefficientsBalancingWalkingR evalK bipedal00.40K cmd0.60.30.6K smooth0.10.10.1K xd2152K yd212</p>
<p>TABLE III :
III
Success rates of sim-to-real transfer in balancing task
Parameter used forBalancing onpolicy trainingtilting boardExcitation only0/3Re-Identified (ours)3/3Excitation only (w/o DTE)0/3Re-Identified (w/o DTE)0/3Domain randomization0/3</p>
<p>TABLE IV :
IV
Success rates of sim-to-real transfer in walking task
Parameter used for policy trainingWalking on a flat floorExcitation only0/3Re-Identified (ours)3/3Domain randomization0/3
ACKNOWLEDGEMENTThe authors thank Yasuhiro Fujita and Avinash Ummadisingu for their support.
Learning quadrupedal locomotion over challenging terrain. J Lee, 10.1126/scirobotics.abc5986Science Robotics. 547Oct 2020</p>
<p>Blind bipedal stair traversal via sim-to-real reinforcement learning. J Siekmann, 2021</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, Robotics: Science and Systems. 072020</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, 2018</p>
<p>P gain angle (rad) time (sec) Right ankle pitch joint Start swing phase Landing Start support phase Fig. 8: Policy's output command while walking. Y Du, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021Auto-tuned sim-to-real transfer</p>
<p>Sim-to-real transfer for biped locomotion. W Yu, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2019</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, 2017</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, 2019 International Conference on Robotics and Automation (ICRA). 2019</p>
<p>Learning locomotion skills for cassie: Iterative design and sim-to-real. Z Xie, Proceedings of the Conference on Robot Learning, ser. Proceedings of Machine Learning Research. the Conference on Robot Learning, ser. Machine Learning Research2020100</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, 10.1126/scirobotics.aau5872Science Robotics. 426Jan 2019</p>
<p>Directional efficiency in geared transmissions: Characterization of backdrivability towards improved proprioceptive control. A Wang, S Kim, 2015 IEEE International Conference on Robotics and Automation (ICRA). 2015</p>
<p>Bilateral drive gear -a highly backdrivable reduction gearbox for robotic actuators. H Matsuki, IEEE/ASME Transactions on Mechatronics. 2462019</p>
<p>Passivity-based full-body force control for humanoids and application to dynamic balancing and locomotion. S Hyon, G Cheng, 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2006</p>
<p>Dynamic walking on compliant and uneven terrain using dcm and passivity-based whole-body control. G Mesesan, 2019 IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids). 2019</p>
<p>Torque based stabilization control for torque sensorless humanoid robots. H Suzuki, 2017 IEEE-RAS 17th International Conference on Humanoid Robotics. 2017</p>
<p>Sim-to-real learning of all common bipedal gaits via periodic reward composition. J Siekmann, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021</p>
<p>Learning locomotion skills using deeprl: Does the choice of action space matter. X B Peng, M Van De Panne, Proceedings of the ACM SIGGRAPH / Eurographics Symposium on Computer Animation, ser. SCA '17. the ACM SIGGRAPH / Eurographics Symposium on Computer Animation, ser. SCA '1720171213</p>
<p>Learning bipedal robot locomotion from human movement. M Taylor, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, 2018</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2012</p>
<p>Simulation-based design of dynamic controllers for humanoid balancing. J Tan, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2016</p>
<p>ROBOTIS-OP3 INTRODUCTION. Robotis Co, Ltd, 28 Feb 2023</p>
<p>Optuna: A next-generation hyperparameter optimization framework. T Akiba, Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2019</p>            </div>
        </div>

    </div>
</body>
</html>
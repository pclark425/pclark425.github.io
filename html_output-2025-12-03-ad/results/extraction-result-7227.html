<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7227 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7227</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7227</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-273025540</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.01677v1.pdf" target="_blank">M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA</a></p>
                <p><strong>Paper Abstract:</strong> Research into the external behaviors and internal mechanisms of large language models (LLMs) has shown promise in addressing complex tasks in the physical world. Studies suggest that powerful LLMs, like GPT-4, are beginning to exhibit human-like cognitive abilities, including planning, reasoning, and reflection. In this paper, we introduce a research line and methodology called LLM Psychology, leveraging human psychology experiments to investigate the cognitive behaviors and mechanisms of LLMs. We migrate the Typoglycemia phenomenon from psychology to explore the"mind"of LLMs. Unlike human brains, which rely on context and word patterns to comprehend scrambled text, LLMs use distinct encoding and decoding processes. Through Typoglycemia experiments at the character, word, and sentence levels, we observe: (I) LLMs demonstrate human-like behaviors on a macro scale, such as lower task accuracy and higher token/time consumption; (II) LLMs exhibit varying robustness to scrambled input, making Typoglycemia a benchmark for model evaluation without new datasets; (III) Different task types have varying impacts, with complex logical tasks (e.g., math) being more challenging in scrambled form; (IV) Each LLM has a unique and consistent"cognitive pattern"across tasks, revealing general mechanisms in its psychology process. We provide an in-depth analysis of hidden layers to explain these phenomena, paving the way for future research in LLM Psychology and deeper interpretability.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7227.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7227.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Typoglycemia (LLM average by dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Typoglycemia performance (average across evaluated LLMs) by dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate observation of how evaluated LLMs' task accuracy (relative to BASE) degrades under character/word/sentence reordering (Typoglycemia) separated by dataset type (math, yes/no, commonsense).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Average over evaluated LLMs (Gemma-2, Llama-3.1, GPT-3.5, GPT-4o variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregate summary across multiple transformer-based LLMs evaluated in the paper (Gemma-2 family, Llama-3.1, GPT-3.5-Turbo, GPT-4o-mini, GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (2B, 9B, 27B, 8B, 70B, GPT family sizes unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typoglycemia (reordering, TypoC)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reading/comprehension and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Original NLP tasks (GSM8k math, BoolQ yes/no, CSQA commonsense) where the task prompt/context has been scrambled (character/word/sentence reordering) and model accuracy is measured relative to unperturbed BASE prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>relative retained accuracy (T_rel = accuracy_Typo / accuracy_BASE) expressed as percentage</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Qualitative human baseline: humans can often read scrambled words (Typoglycemia) and preserve comprehension in many cases but show decreased comprehension and increased processing cost (more eye fixations and longer fixation durations). (See Rayner et al., 2006; Ferreira et al., 2002; Just & Carpenter, 1980.) No numeric human accuracy on these specific NLP datasets is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported average retained capability: BoolQ ~95.6% retention, CSQA ~81.7% retention, GSM8k (math) ~67.8% retention (when FΩ = REO).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Rayner et al., 2006; Ferreira et al., 2002; Just & Carpenter, 1980 (cited qualitatively in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>These are aggregated retention ratios across models reported in the paper (TypoC, REO). The paper emphasizes that tasks with stronger multi-step logical requirements (GSM8k) suffer more under scrambling than simpler yes/no tasks (BoolQ). No numeric human baselines on these same datasets are provided; human comparisons are qualitative and from classic psycholinguistics findings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7227.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7227.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (REO TypoC retention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o retained capability under Typoglycemia reordering (TypoC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art GPT-4o model shows high robustness to scrambled prompts, retaining most of its baseline task capability under reordering perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 family model variant evaluated in the paper (instruction-tuned, transformer-based decoder model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typoglycemia Completion (TypoC) — reordering (REO)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reading/comprehension and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Standard tasks (GSM8k, BoolQ, CSQA) where prompts have been scrambled via reordering operations; measure change in task accuracy relative to BASE.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>average retained accuracy (percentage of BASE retained capability)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Qualitative: humans show preserved comprehension in many Typoglycemia conditions but slower/less fluent reading; specific numeric human baselines on the same tasks are not provided in this paper (see Rayner et al., 2006; Ferreira et al., 2002).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4o retains an average of ~87.8% of its capability under reordering (REO) perturbations (reported across the evaluated datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Rayner et al., 2006; Ferreira et al., 2002 (cited for human Typoglycemia behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports GPT-4o shows strongest robustness (many blue markers in tables). Retention here is relative to each model's BASE performance; absolute accuracies per dataset are not fully tabulated in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7227.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7227.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2-2B (REO TypoC retention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2 (2B) retained capability under Typoglycemia reordering (TypoC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small Gemma-2 model (2B) is the weakest evaluated model under scrambling and retains substantially less of its baseline accuracy when prompts are reordered.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemma-2 family transformer-based LLM; smallest evaluated scale (2B parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typoglycemia Completion (TypoC) — reordering (REO)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reading/comprehension and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same NLP tasks (GSM8k, BoolQ, CSQA) under prompt reordering; measure retained accuracy relative to BASE.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>average retained accuracy (percentage of BASE retained capability)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Qualitative human baseline: humans can often compensate for internal letter transpositions but show performance cost in processing; no numeric human baseline on these datasets is given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Gemma-2-2B retains ~41.7% of its capability under REO in reported comparisons (one of the lowest retention rates reported).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Rayner et al., 2006; Perea & Lupker, 2004 (cited for human relative importance of first/last letters in reading)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper reports strong model-size dependence: Gemma-2-series shows retention increasing with model scale (e.g., from 41.7% to 70.2% in the Gemma family as size increases). These are relative retention metrics, not absolute accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7227.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7227.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (INS/DEL robustness)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o robustness to insertion and deletion Typoglycemia functions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o demonstrates very high robustness to minor insertions and deletions in prompts, retaining nearly its full baseline accuracy across evaluated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 family transformer decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typoglycemia Completion (TypoC) — insertion (INS) and deletion (DEL) at character level</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reading/comprehension and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Evaluate models on standard tasks where prompts are perturbed by inserting or deleting characters within words and measure accuracy relative to BASE.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>average retained accuracy across datasets (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Qualitative: humans are relatively robust to small insertions/deletions compared to gross transpositions, but processing cost increases; no numeric human baseline on the same NLP datasets is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4o retains an average accuracy of ~98.9% across the three datasets for insertion/deletion operations (INS/DEL), indicating much smaller impact than reordering.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Rayner et al., 2006; Perea & Lupker, 2004 (cited generally)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper notes insertion/deletion cause smaller degradation than reordering; some tasks (especially weak-logic BoolQ) even show slight increases in accuracy under minor perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7227.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7227.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B (SQuAD per-FΩ accuracies)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1 (8B) per-perturbation accuracies on SQuAD (TypoC/TypoP analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Layerwise representation similarity and corresponding task accuracies for Llama-3.1-8B on SQuAD under different Typoglycemia functions; shows strong alignment between representation similarity and task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3.1 family open-weight transformer LLM; 8B-parameter variant evaluated for representation similarity and task accuracy across perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typoglycemia Completion / Perception on SQuAD (various FΩ; sentence-level and character-level reordering)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reading comprehension / contextual learning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>SQuAD passage-question phrase extraction task with prompts perturbed by Typoglycemia functions (sentence-level reordering and character-level reordering variants); measure accuracy and compute cosine similarity of layerwise representations to BASE.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (exact match/phrase accuracy) and cosine similarity of representations</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Qualitative: humans retain high phrase-level comprehension under sentence-level/smaller perturbations but show greater difficulty with heavy character-level reversals; no numeric human SQuAD baseline for scrambled input is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>For sentence-level perturbations on SQuAD the reported accuracies (for three high-similarity sentence-level FΩ types) are 74.4%, 73.2%, and 71.6%; for a severe character-level perturbation (Char-REO-REV) the lowest accuracy reported is 22.0%. (Reported in the paper in the context of representation similarity alignment.)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Rayner et al., 2006; Just & Carpenter, 1980 (cited for human sensitivity to internal structure of words)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper links decoder-layer representation similarity (cosine similarity to BASE) with downstream accuracy: higher representation similarity correlates with higher accuracy. Numbers above are from Figure 5 / associated text (Llama-3.1-8B analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7227.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7227.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo (processing cost)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo increases in token usage and completion time under Typoglycemia</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Under Typoglycemia perturbations, GPT-3.5-Turbo requires more prompt tokens and longer completion time, paralleling human increases in eye fixations and fixation durations when reading scrambled text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 series instruction-tuned decoder transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Typoglycemia (REO-INT character-level) — resource/processing cost</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>processing efficiency / reading effort analogue</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Compare prompt token counts and completion time before and after applying Typoglycemia perturbation to prompts; report ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>percent change in prompt tokens and percent change in completion time</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Quantitative psycholinguistic baseline: humans require more eye fixations and experience longer fixation durations when reading scrambled text (Rayner et al., 2006); specific numeric human RT/fixation values for the same texts are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5-Turbo shows ~46.5% increase in prompt tokens and ~18% increase in completion time after Typoglycemia perturbation (REO-INT char-level).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Rayner et al., 2006 (cited for human increased eye fixations and fixation durations under scrambled text)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper uses token/time increases as an analogue to human processing cost; these are resource metrics rather than accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unnatural error correction: Gpt-4 can almost perfectly handle unnatural scrambled text <em>(Rating: 2)</em></li>
                <li>Robustness of llms to perturbations in text <em>(Rating: 2)</em></li>
                <li>Transposed-letter effects in reading: evidence from eye movements and parafoveal preview <em>(Rating: 2)</em></li>
                <li>Good-enough representations in language comprehension <em>(Rating: 1)</em></li>
                <li>Can large language model agents simulate human trust behaviors? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7227",
    "paper_id": "paper-273025540",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "Typoglycemia (LLM average by dataset)",
            "name_full": "Typoglycemia performance (average across evaluated LLMs) by dataset",
            "brief_description": "Aggregate observation of how evaluated LLMs' task accuracy (relative to BASE) degrades under character/word/sentence reordering (Typoglycemia) separated by dataset type (math, yes/no, commonsense).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Average over evaluated LLMs (Gemma-2, Llama-3.1, GPT-3.5, GPT-4o variants)",
            "model_description": "Aggregate summary across multiple transformer-based LLMs evaluated in the paper (Gemma-2 family, Llama-3.1, GPT-3.5-Turbo, GPT-4o-mini, GPT-4o).",
            "model_size": "various (2B, 9B, 27B, 8B, 70B, GPT family sizes unspecified)",
            "test_name": "Typoglycemia (reordering, TypoC)",
            "test_category": "reading/comprehension and reasoning",
            "test_description": "Original NLP tasks (GSM8k math, BoolQ yes/no, CSQA commonsense) where the task prompt/context has been scrambled (character/word/sentence reordering) and model accuracy is measured relative to unperturbed BASE prompts.",
            "evaluation_metric": "relative retained accuracy (T_rel = accuracy_Typo / accuracy_BASE) expressed as percentage",
            "human_performance": "Qualitative human baseline: humans can often read scrambled words (Typoglycemia) and preserve comprehension in many cases but show decreased comprehension and increased processing cost (more eye fixations and longer fixation durations). (See Rayner et al., 2006; Ferreira et al., 2002; Just & Carpenter, 1980.) No numeric human accuracy on these specific NLP datasets is reported in this paper.",
            "llm_performance": "Reported average retained capability: BoolQ ~95.6% retention, CSQA ~81.7% retention, GSM8k (math) ~67.8% retention (when FΩ = REO).",
            "prompting_method": "zero-shot",
            "fine_tuned": false,
            "human_data_source": "Rayner et al., 2006; Ferreira et al., 2002; Just & Carpenter, 1980 (cited qualitatively in the paper)",
            "statistical_significance": null,
            "notes": "These are aggregated retention ratios across models reported in the paper (TypoC, REO). The paper emphasizes that tasks with stronger multi-step logical requirements (GSM8k) suffer more under scrambling than simpler yes/no tasks (BoolQ). No numeric human baselines on these same datasets are provided; human comparisons are qualitative and from classic psycholinguistics findings.",
            "uuid": "e7227.0",
            "source_info": {
                "paper_title": "M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o (REO TypoC retention)",
            "name_full": "GPT-4o retained capability under Typoglycemia reordering (TypoC)",
            "brief_description": "State-of-the-art GPT-4o model shows high robustness to scrambled prompts, retaining most of its baseline task capability under reordering perturbations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI GPT-4 family model variant evaluated in the paper (instruction-tuned, transformer-based decoder model).",
            "model_size": "not specified in paper",
            "test_name": "Typoglycemia Completion (TypoC) — reordering (REO)",
            "test_category": "reading/comprehension and reasoning",
            "test_description": "Standard tasks (GSM8k, BoolQ, CSQA) where prompts have been scrambled via reordering operations; measure change in task accuracy relative to BASE.",
            "evaluation_metric": "average retained accuracy (percentage of BASE retained capability)",
            "human_performance": "Qualitative: humans show preserved comprehension in many Typoglycemia conditions but slower/less fluent reading; specific numeric human baselines on the same tasks are not provided in this paper (see Rayner et al., 2006; Ferreira et al., 2002).",
            "llm_performance": "GPT-4o retains an average of ~87.8% of its capability under reordering (REO) perturbations (reported across the evaluated datasets).",
            "prompting_method": "zero-shot",
            "fine_tuned": false,
            "human_data_source": "Rayner et al., 2006; Ferreira et al., 2002 (cited for human Typoglycemia behavior)",
            "statistical_significance": null,
            "notes": "Paper reports GPT-4o shows strongest robustness (many blue markers in tables). Retention here is relative to each model's BASE performance; absolute accuracies per dataset are not fully tabulated in the excerpt.",
            "uuid": "e7227.1",
            "source_info": {
                "paper_title": "M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Gemma-2-2B (REO TypoC retention)",
            "name_full": "Gemma-2 (2B) retained capability under Typoglycemia reordering (TypoC)",
            "brief_description": "Small Gemma-2 model (2B) is the weakest evaluated model under scrambling and retains substantially less of its baseline accuracy when prompts are reordered.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B",
            "model_description": "Gemma-2 family transformer-based LLM; smallest evaluated scale (2B parameters).",
            "model_size": "2B",
            "test_name": "Typoglycemia Completion (TypoC) — reordering (REO)",
            "test_category": "reading/comprehension and reasoning",
            "test_description": "Same NLP tasks (GSM8k, BoolQ, CSQA) under prompt reordering; measure retained accuracy relative to BASE.",
            "evaluation_metric": "average retained accuracy (percentage of BASE retained capability)",
            "human_performance": "Qualitative human baseline: humans can often compensate for internal letter transpositions but show performance cost in processing; no numeric human baseline on these datasets is given in the paper.",
            "llm_performance": "Gemma-2-2B retains ~41.7% of its capability under REO in reported comparisons (one of the lowest retention rates reported).",
            "prompting_method": "zero-shot",
            "fine_tuned": false,
            "human_data_source": "Rayner et al., 2006; Perea & Lupker, 2004 (cited for human relative importance of first/last letters in reading)",
            "statistical_significance": null,
            "notes": "The paper reports strong model-size dependence: Gemma-2-series shows retention increasing with model scale (e.g., from 41.7% to 70.2% in the Gemma family as size increases). These are relative retention metrics, not absolute accuracies.",
            "uuid": "e7227.2",
            "source_info": {
                "paper_title": "M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o (INS/DEL robustness)",
            "name_full": "GPT-4o robustness to insertion and deletion Typoglycemia functions",
            "brief_description": "GPT-4o demonstrates very high robustness to minor insertions and deletions in prompts, retaining nearly its full baseline accuracy across evaluated datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "GPT-4 family transformer decoder model.",
            "model_size": "not specified",
            "test_name": "Typoglycemia Completion (TypoC) — insertion (INS) and deletion (DEL) at character level",
            "test_category": "reading/comprehension and reasoning",
            "test_description": "Evaluate models on standard tasks where prompts are perturbed by inserting or deleting characters within words and measure accuracy relative to BASE.",
            "evaluation_metric": "average retained accuracy across datasets (percentage)",
            "human_performance": "Qualitative: humans are relatively robust to small insertions/deletions compared to gross transpositions, but processing cost increases; no numeric human baseline on the same NLP datasets is provided.",
            "llm_performance": "GPT-4o retains an average accuracy of ~98.9% across the three datasets for insertion/deletion operations (INS/DEL), indicating much smaller impact than reordering.",
            "prompting_method": "zero-shot",
            "fine_tuned": false,
            "human_data_source": "Rayner et al., 2006; Perea & Lupker, 2004 (cited generally)",
            "statistical_significance": null,
            "notes": "Paper notes insertion/deletion cause smaller degradation than reordering; some tasks (especially weak-logic BoolQ) even show slight increases in accuracy under minor perturbations.",
            "uuid": "e7227.3",
            "source_info": {
                "paper_title": "M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama-3.1-8B (SQuAD per-FΩ accuracies)",
            "name_full": "Llama-3.1 (8B) per-perturbation accuracies on SQuAD (TypoC/TypoP analyses)",
            "brief_description": "Layerwise representation similarity and corresponding task accuracies for Llama-3.1-8B on SQuAD under different Typoglycemia functions; shows strong alignment between representation similarity and task accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B",
            "model_description": "Llama-3.1 family open-weight transformer LLM; 8B-parameter variant evaluated for representation similarity and task accuracy across perturbations.",
            "model_size": "8B",
            "test_name": "Typoglycemia Completion / Perception on SQuAD (various FΩ; sentence-level and character-level reordering)",
            "test_category": "reading comprehension / contextual learning",
            "test_description": "SQuAD passage-question phrase extraction task with prompts perturbed by Typoglycemia functions (sentence-level reordering and character-level reordering variants); measure accuracy and compute cosine similarity of layerwise representations to BASE.",
            "evaluation_metric": "accuracy (exact match/phrase accuracy) and cosine similarity of representations",
            "human_performance": "Qualitative: humans retain high phrase-level comprehension under sentence-level/smaller perturbations but show greater difficulty with heavy character-level reversals; no numeric human SQuAD baseline for scrambled input is provided in the paper.",
            "llm_performance": "For sentence-level perturbations on SQuAD the reported accuracies (for three high-similarity sentence-level FΩ types) are 74.4%, 73.2%, and 71.6%; for a severe character-level perturbation (Char-REO-REV) the lowest accuracy reported is 22.0%. (Reported in the paper in the context of representation similarity alignment.)",
            "prompting_method": "zero-shot",
            "fine_tuned": false,
            "human_data_source": "Rayner et al., 2006; Just & Carpenter, 1980 (cited for human sensitivity to internal structure of words)",
            "statistical_significance": null,
            "notes": "The paper links decoder-layer representation similarity (cosine similarity to BASE) with downstream accuracy: higher representation similarity correlates with higher accuracy. Numbers above are from Figure 5 / associated text (Llama-3.1-8B analyses).",
            "uuid": "e7227.4",
            "source_info": {
                "paper_title": "M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo (processing cost)",
            "name_full": "GPT-3.5-Turbo increases in token usage and completion time under Typoglycemia",
            "brief_description": "Under Typoglycemia perturbations, GPT-3.5-Turbo requires more prompt tokens and longer completion time, paralleling human increases in eye fixations and fixation durations when reading scrambled text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "OpenAI GPT-3.5 series instruction-tuned decoder transformer.",
            "model_size": "not specified",
            "test_name": "Typoglycemia (REO-INT character-level) — resource/processing cost",
            "test_category": "processing efficiency / reading effort analogue",
            "test_description": "Compare prompt token counts and completion time before and after applying Typoglycemia perturbation to prompts; report ratios.",
            "evaluation_metric": "percent change in prompt tokens and percent change in completion time",
            "human_performance": "Quantitative psycholinguistic baseline: humans require more eye fixations and experience longer fixation durations when reading scrambled text (Rayner et al., 2006); specific numeric human RT/fixation values for the same texts are not provided in this paper.",
            "llm_performance": "GPT-3.5-Turbo shows ~46.5% increase in prompt tokens and ~18% increase in completion time after Typoglycemia perturbation (REO-INT char-level).",
            "prompting_method": "zero-shot",
            "fine_tuned": false,
            "human_data_source": "Rayner et al., 2006 (cited for human increased eye fixations and fixation durations under scrambled text)",
            "statistical_significance": null,
            "notes": "Paper uses token/time increases as an analogue to human processing cost; these are resource metrics rather than accuracy metrics.",
            "uuid": "e7227.5",
            "source_info": {
                "paper_title": "M IND S CRAMBLE : U NVEILING L ARGE L ANGUAGE M ODEL P SYCHOLOGY V IA T YPOGLYCEMIA",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unnatural error correction: Gpt-4 can almost perfectly handle unnatural scrambled text",
            "rating": 2,
            "sanitized_title": "unnatural_error_correction_gpt4_can_almost_perfectly_handle_unnatural_scrambled_text"
        },
        {
            "paper_title": "Robustness of llms to perturbations in text",
            "rating": 2,
            "sanitized_title": "robustness_of_llms_to_perturbations_in_text"
        },
        {
            "paper_title": "Transposed-letter effects in reading: evidence from eye movements and parafoveal preview",
            "rating": 2,
            "sanitized_title": "transposedletter_effects_in_reading_evidence_from_eye_movements_and_parafoveal_preview"
        },
        {
            "paper_title": "Good-enough representations in language comprehension",
            "rating": 1,
            "sanitized_title": "goodenough_representations_in_language_comprehension"
        },
        {
            "paper_title": "Can large language model agents simulate human trust behaviors?",
            "rating": 1,
            "sanitized_title": "can_large_language_model_agents_simulate_human_trust_behaviors"
        }
    ],
    "cost": 0.01659775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MIND SCRAMBLE: UNVEILING LARGE LANGUAGE MODEL PSYCHOLOGY VIA TYPOGLYCEMIA
24 Oct 2024</p>
<p>Miao Yu 
University of Science and Technology of China (USTC</p>
<p>Squirrel AI</p>
<p>Junyuan Mao 
University of Science and Technology of China (USTC</p>
<p>Squirrel AI</p>
<p>Guibin Zhang 
Squirrel AI</p>
<p>Jingheng Ye 
Squirrel AI</p>
<p>Junfeng Fang 
National University of Singapore (NUS)</p>
<p>Aoxiao Zhong 
Squirrel AI</p>
<p>Yang Liu 
NanYang Technological University</p>
<p>Yuxuan Liang 
The Hong Kong University of Science and Technology (HKUST)</p>
<p>Kun Wang 
Squirrel AI</p>
<p>NanYang Technological University</p>
<p>Qingsong Wen 
Squirrel AI</p>
<p>MIND SCRAMBLE: UNVEILING LARGE LANGUAGE MODEL PSYCHOLOGY VIA TYPOGLYCEMIA
24 Oct 20240188AAAE65F5816700D53252DCC5A0FCarXiv:2410.01677v3[cs.AI]
Although still in its infancy, research into the external behaviors and internal mechanisms of large language models (LLMs) has shown significant promise in addressing complex tasks in the physical world.These studies suggest that powerful LLMs, such as GPT-4, are beginning to exhibit human-like cognitive abilities, including planning, reasoning, and reflection, among others.In this paper, we introduce an innovative research line and methodology named LLM Psychology, which leverages or extends human psychology experiments and theories to investigate cognitive behaviors and mechanisms of LLMs.Practically, we migrate the Typoglycemia phenomenon from psychology to explore the "mind" of LLMs.To comprehend scrambled text in Typoglycemia, human brains rely on context and word patterns, which reveals a fundamental difference from LLMs' encoding and decoding processes.Through various Typoglycemia experiments at the character, word, and sentence levels, we observe the following: (I) LLMs demonstrate human-like behaviors on a macro scale, such as slightly lower task accuracy with consuming more tokens and time; (II) Different LLMs show varying degrees of robustness to scrambled input, making it a democratized benchmark for model evaluation without crafting new datasets; (III) The impact of different task types varies, with complex logical tasks (e.g., math) in scrambled format being more challenging.Going beyond these, some misleadingly optimistic results suggest that LLMs are still primarily data-driven, and their human-like cognitive abilities may differ from what we perceive; (IV) Interestingly, each LLM exhibit its unique and consistent "cognitive pattern" across various tasks, unveiling a general mechanism in its psychology process.To conclude, we provide an in-depth analysis of hidden layers on a micro scale to explain these phenomena, paving the way for LLMs' deeper interpretability and future research in LLM Psychology. 1 * Kun Wang and Qingsong Wen are the corresponding authors, † denotes equal contributions.</p>
<p>INTRODUCTION</p>
<p>"Typoglycemia refers to the pheonmneon where poeple can raed text even when the lettres in the midlde of wrods are scrambled, as long as the fisrt and last letters are in the crorect poistion."Do you notice that some words in the above explanation to Typoglycemia have letters in the wrong order?[pheonmneon, poeple, raed, ...] These words contain certain misplaced letters, yet we can still recognize them.This phenomenon, known as Typoglycemia, is widespread in human reading and is used in psychology experiments to study human language cognition (Johnson et al., 2007;Rayner et al., 2006).With recent development of large language models (LLMs), they demonstrate "humanlike" capabilities and open a potential path for the upcoming artificial general intelligence, excelling in complex tasks such as tool using (Yuan et al., 2024), reasoning (Hao et al., 2023), planning (Kalyanpur et al., 2024), and role-playing (Chen et al., 2023a).However, research on the underlying cognitive mechanisms of LLMs remains in its infancy.Whether LLMs possess deep thinking and human-like cognition is an unsolved mystery that still looms over researchers (Binz &amp; Schulz, 2023;Preprint Bender et al., 2021).Thus, we try to reveal this by exploring an intriguing question: "Does LLM possess human-like cognitive processes and mechanisms in reading and comprehending?"</p>
<p>To this end, this work aims to investigate the "human-like phenomena" demonstrated by LLMs and provide insights into whether these models truly possess cognitive capabilities or merely exhibit them in a statistical sense.To delve deeper into existing LLMs research, we categorize the off-theshelf studies into three main research lines: (I) Single-LLM, where external human-like thought processes are applied to LLM through methods such as prompt engineering to achieve better performance (Liu et al., 2021).For instance, Wei et al. (2022); Yao et al. (2024); Besta et al. (2024) simulate human-like reasoning by guiding LLMs through intermediate thought steps in the structure of chain/tree/graph.(II) Multi-LLMs (Agents), where interactions between multiple LLMs are used to explore their behavior and logic in complex communications, such as cooperative (Qian et al., 2023;Shen et al., 2024) and competitive (Zhao et al., 2023)   However, Line (I) merely focuses on leveraging LLMs' human-like abilities to solve real-world problems, while overlooking deeper investigations into why LLMs exhibit such capabilities.This preconceived notion of equating LLMs with humans may overlook their limitations and misuse risks, leading to unreliable outcomes.Line (II), constrained by specific parameters and settings, operates only within particular scenarios.This limitation results in reduced flexibility, as it prevents the agents from being adapted to diverse and unpredictable contexts beyond its predefined scope.Similarly, while Line (III) has begun to explore LLMs from a cognitive perspective, they remain limited to fixed and external scenarios such as moral reasoning or human simulation (Almeida et al., 2024;Petrov et al., 2024).What all of these studies lack is the systematical exploration of the generalized and intrinsic cognitive mechanisms of LLMs.</p>
<p>Insights.In this paper, we propose a new research line and concept: LLM Psychology, which follows and extends human psychology methods to explore and study LLMs.In practice, we use Typoglycemia as a lens to investigate the universal and underlying mechanisms of LLMs in comprehending.Psychologists, analyzing human behaviors in Typoglycemia scenarios, have explored human visual mechanisms, contextual reasoning, and language patterns (Agrawal et al., 2020;Caffarra et al., 2021).They discover that human reading relies on the overall shape of words and familiar patterns, enabling self-correction and holistic interpretation of scrambled text (Rayner et al., 2012).In a parallel vein, LLMs' tokenization algorithms, such as Llama's BPE (Sennrich, 2015;Touvron et al., 2023), shroud the inner mechanisms.Consequently, by applying Typoglycemia (not transcription errors) to LLMs, similar to what psychologists do with humans, we can explore whether LLMs demonstrate "human-like" performance and mechanisms from appearance to essence.</p>
<p>In practice, we first align humans with LLMs when processing scrambled text in Figure 1.We then naturally extend original Typoglycemia from character to word and sentence levels.To systematize subsequent study, we design the standardized experiment pipeline (referred as TypoPipe), which explores multi-dimensional performances through various tailor-made tasks in scrambled text(referred as TypoTasks).TypoPipe is deployed across 5 datasets on Llama-3.1,Gemma-2 and GPT families.Some interesting and counter-intuitive findings are as follows: ♣ LLMs exhibit human-like</p>
<p>RELATED WORK</p>
<p>Human-like Mechanisms of LLMs.LLMs (Touvron et al., 2023;Achiam et al., 2023;Wang et al., 2024) have revolutionized both academic and industrial research paradigms, owing to their exceptional and human-like capabilities (Wei et al., 2022;Wang et al., 2024).Building on these foundational studies, numerous research efforts integrate mechanisms such as memory, role-playing and tool using to fully leverage these human-like capabilities (Wei et al., 2022;Bubeck et al., 2023;Hong et al., 2023;Li et al., 2023;Chen et al., 2023b;a).Several studies explore the similarities between LLMs and human cognitive mechanisms.McCoy et al. (2019) investigates model's cognitive intuitions in reasoning tasks.Chowdhery et al. (2023) analyzes PaLM's memory mechanisms, highlighting its human-like processes in retrieval and question answering.Though promising, there is still a debate that whether LLMs truly understand language or merely rely on data-driven patterns (Bender et al., 2021).We use Typoglycemia as a psychological probe to uncover the superficial performance and underlying mechanisms of LLMs and present "LLM Psychology" with the first shot.</p>
<p>Cognitive Concepts in LLMs.Due to the similarity between LLMs and humans, many studies have been inspired by human cognitive process to enhance LLMs (Bubeck et al., 2023;Wei et al., 2022).For example, SwiftSage (Lin et al., 2024) improves the task capabilities of LLM-based agents in dynamic environments by mimicking the dual-process theory of human cognition.Xie et al. (2024) investigates whether LLMs can simulate human cognitive trust behaviors by employing a series of trust games.AvalonBench (Light et al., 2023) evaluates LLMs' competency levels through the Resistance Avalon game, which involves cognitive strategies.PsySafe (Zhang et al., 2024) further explores the impact of cognitive states as prompts on the safety of LLM-agent.In this work, inspired by the Typoglycemia psychological experiment, we explore the underlying cognitive process of LLMs by comparing their micro and macro level performance with that of humans.</p>
<p>Human Reading and Typoglycemia.An interesting phenomenon is that humans can maintain an understanding of the general meaning of scrambled text, a capability that some studies attribute to the brain's mechanisms of holistic shape perception and pattern recognition (Miller, 1994;Rayner , 2006;Perea &amp; Lupker, 2004;Shaywitz &amp; Shaywitz, 2008;Rayner et al., 2012).As LLMs' powerful understanding capabilities have been recognized, a few studies attempt to explore whether LLMs exhibit similar "human-like" phenomena.Cao et al. (2023) investigates the exceptional performance of LLMs in reconstructing character-level scrambled text.Singh et al. (2024) finds that LLMs can still maintain encoding consistency when confronted with such text.However, previous work has merely showcased related phenomena without delving deeply into the underlying mechanisms of LLMs.In our research, we systematically migrate the Typoglycemia phenomenon across multi-granularity to LLMs and provide a comprehensive explanation for its underlying causes.</p>
<p>CAN LLMS RECOGNIZE TPYOGYLCMEIA AS TYPOGLYCEMIA?</p>
<p>To apply core principles of LLM Psychology, we migrate and extend the Typoglycemia concept from psychology by proposing the calibrated benchmark (TypoBench), as shown in Figure 2 .Concretely, TypoBench consists of two components: (1) Typoglycemia Pipeline that provides standardized experiment workflow on LLMs (Sec 3.1) and (2) Typoglycemia Task that challenges LLMs' all-around abilities to address scrambled text (Sec 3.2), with reasons for its specific design in Sec 3.3.</p>
<p>TYPOGLYCEMIA PIPELINE (TYPOPIPE)</p>
<p>In this section, we introduce the generalized framework TypoPipe to standardize the experimental process.TypoPipe divides the entire pipeline into the following 4 steps: ① Calibration aims to comprehensively select and calibrate datasets for a thorough evaluation of LLMs' ability.② Navigation targets to design reasonable functions (TypoFunc) to transform each data into various types of "Typoglycemia" text.③ Refabrication.This process perform the original task on the dataset or design other scenarios to explore LLMs' versatile performances.④ Refinement consists of iteratively calculating metrics, analysing results and refining experiment settings for final conclusions.</p>
<p>Formulations.First, we provide denotations for further formulations.Let C be the character set, then the text set is
T = {c 1 c 2 . . . c n | c i ∈ C, 1 ≤ i ≤ n}.
Then we denote a dataset with questions and answers as
D = (Q, A) = {(q i , a i )|q i ∈ Q, a i ∈ A, 1 ≤ i ≤ m},arg max D α 1≤i&lt;j≤t Difference(D i , D j ) + β i=1 Quality(D i )(1)
Eq 1 seeks to select distinct and premium datasets to challenge LLMs from multi-aspects.In practice, we heuristically select tailored datasets from distinct fields.(See in Appendix B.1).</p>
<p>② Navigation: Indicate the binary set (opt, gra) as Ω, where opt : T → T is the text operation (reorder, insert, delete, etc.) and gra ∈ {character, word, sentence} is the smallest operational unit (granularity) for opt.We define TypoFunc F Ω : T → T , where ∀t ∈ T ,
F Ω (t) = Recombine {opt(s i ) | s i ∈ Split gra (t)} ,(2)
where function Split gra : T → T * maps text into a set of tokens split at the specified granularity level and function Recombine : T * → T recombines tokens into text.</p>
<p>③ Refarbrication: Based on datasets and functions from previous two steps, we then apply them to get the Typoglycemia prompts.Concretely, let P : T → T be the function that transforms data into prompts under certain task scenarios (See examples in Appendix C).For any function f and set X = {x 1 , x 2 , . . ., x |X | }, denote f (X ) = {f (x 1 ), . . ., f (x |X | )} as applying f to all individual elements in X respectively.For any dataset D = (Q, A), we define refarbrication step T → T as:
P = P F Ω (Q) → A pred = M(P)
(3) Eq 3 expresses the process of converting the original texts in dataset to the Typoglycemia text tasks and get corresponding responses from LLMs.A pred is the LLMs' solutions or answers to inputs.</p>
<p>④ Refinement: After step ③, we utilize evaluation function E D→R to quantify LLMs' performance on corresponding tasks.The accuracy evaluation metrics is:
E acc (A pred , A) = 1 |A| I(a pred , a), where I(x, y) = 1, if x = y 0, otherwise(4)
Here E acc represents the accuracy between LLMs' answers and correct answers.Since accuracy only evaluates the final results instead of intermediate thinking process of LLMs, we import a new metric to asses the semantic similarity of hidden states and representations in Transformers.Denote the embedding function as V : T → R d .The semantic similarity evaluation metrics is:
E sim (A pred , A) = 1 |A| V(a pred ) • V(a) ||V(a pred )|| • ||V(a)|| ,(5)
where x • y denotes the dot product of vectors and ||z|| denotes the Euclidean norm of vector.E sim assesses the cosine similarity of LLMs' output with standard answers from a semantic view.</p>
<p>Finally, for any dataset D = (Q, A), a complete iteration of TypoPipe is represented as:
TP(D, F Ω , M, P, E) = E M F Ω (P(Q)) , A(6)
TP is the function representing the whole TypoPipe.Going beyond this, we propose some metrics to evaluate LLMs' ability from the Typoglycemia perspective:
T abs = u i=1 α i • TP(D i , F Ω , M, P, E) or v j=1 α j • TP(D, F Ωj , M, P, E)(7)T rel = w i=1 α i • TP(D i , F Ω , M, P, E) TP(D i , F ‡ , M, P, E) ,(8)
where α = 1, α ∈ [0, 1] and F ‡ refers to the identity transformation.T abs and T rel evaluate LLM's absolute and relative performances on various datasets or TypoFuncs, respectively.</p>
<p>TYPOGLYCEMIA TASK = TYPOC + TYPOP</p>
<p>Building upon the standardized TypoPipe workflow, we have carefully designed TypoTask, which targets at assessing LLMs' performance in specific Typoglycemia-related tasks, along with their ability to comprehend and correct scrambled text.Specifically, TypoTask consists of the following two categories of tasks: Typoglycemia Completion (TypoC) and Typoglycemia Perception (TypoP).</p>
<p>TypoC refers to performing native tasks on the dataset.For example, the native task of GSM8K (Hendrycks et al., 2020) is to solve mathematical problems.TypoC reflects LLMs' ability to comprehend and follow scrambled text prompt when addressing problems in specific fields.To further explore the extent to which LLMs understand Typoglycemia text (scrambled text), we design TypoP consisting of Rectify, Summarize, and Translate.Rectify task aims to restoring Typoglycemia text back to its original form, assessing the model's ability to locally identify and rectify such errors.Summarize and Translate tasks require summarizing and translating, respectively, which evaluates the model's ability to understand the global context and detailed information in Typoglycemia text.See tailor-selected TypoC and TypoP tasks in Appendix B, C.1, and C.2.</p>
<p>Preprint</p>
<p>WHY COMPLETION AND PERCEPTION?</p>
<p>Methodologically, TypoC is designed to evaluate the behavioral performance of LLMs, while TypoP aims to assess their perception and understanding, drawing inspiration from behavioral psychology and cognitive psychology, respectively.In doing so, we provide a vivid example of how psychological principles can be applied to understand and evaluate LLMs via our proposed"LLM Psychology".These two tasks explore the impact of Typoglycemia on LLMs from both fine-grained and coarsegrained perspectives, progressing from shallow to deep levels of analysis.To successfully complete these tasks, models must simultaneously grasp local (scrambled content) and global information (contextual semantics) in order to fully comprehend the task's details and objectives.Based on the TypoBench framework, Eq 7 and Eq 8, we propose a more general and concise method for evaluating LLMs based on existing datasets, which reflects abilities not explored in previous research:
T gen = E(M, F(D)) E(M, D)(9)
Eq 9 means using metrics E to evaluate model M on dataset D before and after being applying function F. We present division to quantify the impact of F. In our work, F is Typoglycemia.</p>
<p>EXPERIMENT</p>
<p>We employ TypoPipe across various scenarios to comprehensively study the impact of Typoglycemia on LLMs.The experiments are designed to investigate the following research questions:</p>
<p>• RQ1: What is the impact of Typoglycemia on existing LLMs?</p>
<p>• RQ2: How do other Typoglycemia Functions (e.g., insertion and deletion) impact LLMs?</p>
<p>• RQ3: What are the effects of increasing the scrambling ratio of Typoglycemia?</p>
<p>• RQ4: Why do LLMs align with human performance under Typoglycemia?</p>
<p>EXPERIMENTAL SETUPS</p>
<p>Datasets.We aim to evaluate LLM Psychology across various task settings, including mathematics, code generation, situational question answering, and commonsense reasoning.Concretely, as for scenarios requiring strong logical reasoning, we select GSM8k (Hendrycks et al., 2020) for math and MBPP (Kocetkov et al., 2022) for code.Additionally, we explore the impact of Typoglycemia on LLMs' emergent situational learning and knowledge capabilities.We select BoolQ (Clark et al., 2019) and SQuAD (Rajpurkar et al., 2016) dataset for situational question answering tasks.For commonsense reasoning, we use CSQA (Talmor et al., 2018) dataset, a multiple-choice commonsense dataset.More descriptions on dataset can be found in Figure 1, Appendix B, and C.</p>
<p>TypoFuncs (F Ω ) transform the above datasets into Typoglycemia texts.To extend psychological Typoglycemia, we execute F Ω at character, word, and sentence levels, allowing us to explore the sensitivity of LLMs to various text variations.Specific F Ω operations include reordering, inserting, and deleting (refer to as REO, INS, and DEL, respectively).Operation X can be applied in different positions or ways of the three levels, such as: all (X-ALL), internal (X-INT), adjacent (X-ADJ), beginning (X-BEG), ending (X-END), and reversing (X-REV).Utilizing well-designed F Ω , our Typoglycemia experiment contains both mildly scrambled text and highly disordered text that is nearly unrecognizable to humans.The specific operations instances can be found in Appendix D.</p>
<p>Models and Metrics.We extensively evaluate our concept across diverse LLMs within zero-shot setting, including Gemma-2 (2B, 9B and 27B) (Team et al., 2024), Llama 3.1 (8B, 70B) (Touvron et al., 2023), GPT-3.5-Turbo2, GPT-4o-mini3 and GPT-4o4 .The selection of these models and their corresponding sizes provides a comprehensive "model zoom".In our settings, we choose accuracy and cosine similarity as metrics.For accuracy, we consider a response correct only when the LLM's output exactly matches the correct answer.For cosine similarity, we embed the reasoning processes into vectors using the text-embedding-35 and calculate cosine similarity with the standard process.The model parameter settings for reproducibility can be found in Appendix E.</p>
<p>Table 1: Main results on the TypoC tasks when FΩ = REO on the character, word and sentence level.We evaluate the average task accuracy (over 3 runs) of various LLMs on the GSM8k, BoolQ, and CSQA datasets.BASE refers to the scenario where FΩ is not applied to the task description.With the same coefficient weights, Tabs (Eq 7) shows each row's average accuracy, evaluating general performance across various TypoFuncs.Trel calculates Eq 8, quantifying the ability retaining ratio compared with BASE.In each dataset, red (blue) marks the maximum value in each row (column), and green marks values that are the maximum in both.Gray marks the values that are higher than BASE in each row.Several TypoC cases are shown in Appendix G.</p>
<p>MAIN RESULTS (RQ1)</p>
<p>To answer RQ1, we compare different Typoglycemia concepts across various models and datasets.We apply random reordering and run the experiments multiple times, reporting the mean values.The experimental observations (Obs) are as follows and experiment discussion is placed in Appendix A: Obs.1.Typoglycemia generally leads to a decline in model performance, with more advanced models being less affected.As shown in Table 1, red markers predominantly appear in the BASE column, indicating that accuracy tends to decrease after applying F Ω .The performance retention of models within the same series increases with model size.For instance, on GSM8k dataset, the Gemma-2 series exhibits an increase in average accuracy across scales, with retention rates</p>
<p>Preprint</p>
<p>Table 3: Results on the TypoC tasks when FΩ = INS and DEL on character levels.We apply FΩ at the begin and end of each word.We report the average accuracy (over 3 runs) of various LLMs on the GSM8k, BoolQ, and CSQA datasets.BASE means FΩ is not applied.Tabs shows each column's average accuracy, while Trel calculates our proposed metrics with equal weights.In each dataset, red marks the maximum in columns.Gray marks the values that are higher than BASE in columns.TypoC cases are in Appendix G. of 41.7% → 70.2%.Furthermore, the SOTA model GPT-4o (more than 80% of the blue markers) retains an average of 87.8% of its capability, whereas the weakest model, Gemma-2-2B, retains only 41.7%.This aligns with that of humans (Rayner et al., 2006;Frost, 2012) in Typoglycemia scenarios and opens up a new avenue for evaluating model capabilities (more results are in Appendix F.1.1).
Datasets/F Ω Gemma-2-2B Gemma-2-9B Gemma-2-27B Llama-3.1-8B Llama-3.1-70B GPT-3.5-Turbo GPT-4o-mini GPT-4oGSM8k
Obs.2.The degree to which LLMs' performance is affected is positively correlated with the logical complexity of the TypoC task.In Table 1, gray markers are only seen in the BoolQ/CSQA (reasoning tasks), where LLMs retain 95.6% and 81.7% of their capabilities, respectively, compared to just 67.8% on the math (GSM8k) task, which demands more complex logical reasoning.Notably, for the yes/no BoolQ dataset, applying sentence-level F Ω results in an unusual slight average 0.7%↑ in accuracy.However, for humans, reading scrambled text typically hampers comprehension (Ferreira et al., 2002).This performance improvement may be misleadingly optimistic, suggesting that LLMs might rely on the attention mechanism to capture certain representations from scrambled text that help derive correct results.This statistically-driven mechanism vastly differs from the micro-level processes of human reading and understanding.</p>
<p>Obs.3.The position of characters affects LLMs' understanding differently.As shown in Table 1, the accuracy of character-level F Ω under the ALL setting is 100% lower than that of INT, while the accuracy of BEG is lower than that of END in 87.5% of the cases.This indicates that the importance of the first, last, and internal characters decreases in that order-which further reveals the similarity that both LLMs and humans pay more attention to the first and last characters (Perea &amp; Lupker, 2004) (more results on another two datasets are placed in Appendix F.1.1).</p>
<p>Obs.4.Typoglycemia leads to an increased computational cost.As shown in Figure 3, the ratio of tokens and time before and after the F Ω transformation is greater than 1 in 100% of cases for all LLMs.For instance, GPT-3.5-Turboexhibits a 46.5% ↑ in prompt tokens and an 18% ↑ in completion time.Similarly, (Rayner et al., 2006) finds that humans also require more eye fixations and longer fixation durations when reading Typoglycemia text.This finding reveals that both LLMs and humans struggle in Typoglycemia scenarios (more results are shown in Appendix F.3).</p>
<p>Obs.5.The results of TypoP are consistent with those of TypoC.As Table 2 shows, the performance ranking of 7 out of 8 LLMs closely mirrors that in Table 1.For example, Llama-3.1 shows the same accuracy ranking in both tables: EN D &gt; BEG &gt; IN T &gt; ALL &gt; REV .This observation reveals that the robustness of LLMs in Typoglycemia scenarios is positively correlated with their ability to correct Typoglycemia text (See more results on another two TypoPs in Appendix F.2).  To answer RQ2, we conduct experiments using additional insertion and deletion Typoglycemia functions to verify the impact of other Typoglycemia concepts on LLMs.We list the results in Table 3 (more results are placed in Appendix F.1.2) and we can summarize the observations:</p>
<p>Obs.1.The impact of Insertion and Deletion on LLMs is generally similar to Reordering, but the magnitude of the impact is smaller.As shown in Table 3, red markers are primarily concentrated in the BASE row, and the SOTA model GPT-4o retains an average accuracy of 98.9% across the three datasets, while the weakest model, Gemma-2-2B, achieves 83.6%.In all cases, the average retained accuracy increases by 0.1 ∼ 26.8% compared to Reordering.This indicates that LLMs are more robust to minor additions or deletions of characters than to character reordering.</p>
<p>Obs.2.Insertion and Deletion also result in an unusual increase in accuracy for tasks with weaker logic.As shown in Table 3, 90.9% of the gray markers appear in the BoolQ dataset, which is consistent with the pattern observed in Reordering.This observation further confirms that minor perturbations in the prompt can aid models in understanding simple logical problems.</p>
<p>Obs.3.LLMs exhibit sensitivity to character position for Deletion, but are less sensitive for Insertion.As shown in Table 3, in the case of the INS operation, BEG &lt; EN D occurs in 54.2% of cases, whereas for the DEL operation, this ratio rises to 83.3%, a 29.1% increase.This observation reinforces the finding in RQ1 that the first letter is more important than the last, and reveals that their relative importance can vary depending on the operation.</p>
<p>SCRAMBLING RATIO OF TYPOGLYCEMIA (RQ3)</p>
<p>To address RQ3, we gradually increase the number of reordering, inserting, and deleting operations applied to each word's internal characters to increase the scrambling ratio of texts.The corresponding results are shown in Figure 4 and Appendix F.4, of which we derive the following observations:</p>
<p>Obs.1.As the scrambling ratio increases, the TypoC task becomes more challenging for LLMs.</p>
<p>As shown in Figure 4, with the increasing number of operations, the accuracy of LLMs generally shows a downward trend across all three cases, with a drop ranging from 0.3% to 14.2%.We highlighted the regions with the largest decreases in yellow .This observation aligns with human behavior (Just &amp; Carpenter, 1980), indicating that as the internal structure of the text becomes more disordered, it becomes increasingly difficult for LLMs to understand the text.</p>
<p>Obs.2.Different models exhibit varying levels of resistance to scrambling text.As shown in Figure 4, Llama-3.1-8Bdemonstrates the weakest robustness, while GPT-4o shows the strongest anti-Typoglycemia ability.The absolute values of the average slope in accuracy for the Inserting operation are 10.9 and 1.2, respectively.This robustness can serve as a measure of LLMs' ability to handle scrambled text, which may offer a new approach for evaluating LLMs.</p>
<p>WHY DO LLMS ALIGN WITH HUMAN PERFORMANCE (RQ4)</p>
<p>To answer RQ4, we embed the task text before and after being processed by F Ω using the textembedding-3-large model and then calculate their semantic similarity.Additionally, We calculate the mean of representation tensors between Transformers, aggregating their semantics into vectors to calculate similarity with BASE.Finally, we derive the above Table 4, Figure 5 and observations:</p>
<p>Obs.1.The text before and after F Ω processing exhibits a high degree of semantic similarity, with the impact varying depending on the level of text granularity.As shown in Table 4, in the CSQA dataset, Typoglycemia text at the character, word, and sentence level retains an average semantic similarity of 0.885, 0.952, and 0.998, respectively, compared to the unprocessed text (BASE).This indicates that disturbances at the character level have the greatest impact on LLMs' understanding of the text.This observation suggests that, from the encoder's perspective, Typoglycemia text preserves a substantial amount of semantic information, which enables LLMs to exhibit robustness similar to humans in Typoglycemia scenarios (more results are placed in Appendix F.5).</p>
<p>Obs.2.The subsequent representations of Typoglycemia text by LLMs are critical to their task performance.As shown in Figure 5, the color of accuracy and representation is closely aligned.For instance, on SQuAD, for the 3 types of F Ω at the sentence level, the similarity scores of their representations are all yellow (high similarity), and corresponding accuracy is the highest at 74.4% , 73.2% , and 71.6% , respectively.In contrast, when F Ω = Char-REO-REV, the representation similarity score is the lowest (darkest color), with lowest accuracy at 22.0%.This observation demonstrates that, from the decoder's perspective, the semantic information retained in the representations across the Transformer layers is crucial for LLMs to correctly understand and respond.</p>
<p>Obs.3.The hidden layer representations of the same LLM across different datasets exhibit similar "cognitive patterns."As illustrated in Figure 5, the color distributions for SQuAD and BoolQ under various F Ω appear visually similar.Specifically, the cosine similarity between the concatenated and linearized heatmaps of these two datasets is 0.9994, indicating a high degree of similarity.Additionally, the color distributions vary across different models when evaluated on the same dataset (See more figures in Appendix F.6). Based on these observations, we posit that the heatmap can translate each model's unique "cognitive pattern" through our Typoglycemia experiments, much like how different human individuals exhibit distinct cognitive patterns.</p>
<p>CONCLUSION</p>
<p>In this paper, we explore the emerging field of LLM Psychology by investigating the behavior of LLMs through the lens of Typoglycemia.Our study reveals how LLMs handle scrambled text, providing insights into their cognitive-like abilities and limitations.Through systematic analysis, we observe that LLMs demonstrate human-like behaviors, such as reduced task accuracy and increased token and time consumption, when faced with text distortions.Additionally, the varying robustness across different LLMs suggests that scrambled text understanding serves as an accessible benchmark for evaluating model performance.Despite some promising results, our analysis of LLMs' hidden layers reveals their reliance on data-driven mechanisms, with limited capacity for deep reasoning.By digging into the hidden layer semantics, we further reveal that each LLM demonstrates its unique and consistent cognitive pattern across different datasets in Typoglycemia.</p>
<p>Preprint</p>
<p>A EXPERIMENTAL DISCUSSION</p>
<p>Through comprehensive and systematic experiments in Typoglycemia and its migrated scenarios, we discover both the alignment of LLMs with human cognition and their distinct behaviors.LLMs exhibit a decline in task accuracy, increased resource consumption, and many other human-like behaviors, such as placing greater emphasis on initial letters.This significantly advances research on aligning LLMs with human cognition and provides a solid and vivid case for our proposed "LLM Psychology."Furthermore, we observe LLMs' counter-intuitive and counter-logical performance under certain settings, offering strong evidence for the argument that LLMs possess datadriven statistical reasoning abilities rather than human logic.Finally, we explore the underlying causes and observe different LLM's unique cognitive pattern on these phenomena from the perspectives of encoder and decoder architectures, providing new insights into the cognitive mechanisms of LLMs.To systematically evaluate the performance of LLMs on TypoBench, we focus on three key capabilities when selecting datasets and task scenarios: logical reasoning, contextual learning, and knowledge acquisition.</p>
<p>Preprint B DATASET DESCRIPTION</p>
<p>B.1.1 LOGICAL REASONING</p>
<p>Strong Logic Tasks.We refer to tasks that involve multi-step reasoning, where an error in one step leads to errors in subsequent steps, as Strong Logic Tasks (SLTs).Representative scenarios we select include mathematical problem solving (GSM8k as dataset) and code generation (MBPP as dataset).SLTs pose stringent challenges to the logical reasoning capabilities of LLMs.From a data-driven perspective, Typoglycemia disrupts the morpheme order in normal natural language text, which, in turn, disturbs the inherent logic, leading to confused understanding and erroneous reasoning.For humans, the combination of SLT scenarios and Typoglycemia text makes task completion nearly impossible.In a certain sense, this implies that SLTs are effective in testing LLMs' performance on TypoBench, thereby revealing their underlying cognitive mechanisms.</p>
<p>Weak Logic Tasks.Conversely, tasks with less stringent requirements for logical correctness are referred to as Weak Logic Tasks (WLTs).These tasks typically challenge LLMs' capabilities not only in simple logical reasoning but also in other areas.WLTs primarily serve as a platform for simultaneously evaluating multiple aspects of LLMs' abilities.In our experimental strategy, WLTs are combined with contextual learning and knowledge acquisition.</p>
<p>B.1.2 CONTEXTUAL LEARNING</p>
<p>Contextual learning refers to the ability of LLMs to perceive and learn the knowledge, patterns, and other elements within the context of a given prompt.We select task datasets for contextual learning at two levels of difficulty.Given a contextual passage and a related question, LLMs are instructed to answer with either "yes/no" (BoolQ as dataset) or phrases (SQuAD as dataset), corresponding to easy and difficult settings, respectively.In the yes/no setting, the response is not directly tied to the context, allowing LLMs to rely on coarse-grained semantic understanding.However, in the phrases setting, LLMs are required to have a more localized understanding of the contextual content, posing a more severe challenge to their learning and perception capabilities.By combining these two scenarios with TypoBench, we can explore how Typoglycemia affects LLMs' ability to perceive both local and global information.</p>
<p>B.1.3 KNOWLEDGE ACQUISITION</p>
<p>LLMs possess knowledge capabilities, which are embedded within their layer weights.Generally, a model activates and extracts the knowledge embedded in these weights through the input prompt, enabling it to generate responses.This raises an interesting question: does Typoglycemia affect this process?In our experimental strategy, we investigate whether Typoglycemia disrupts the extraction of knowledge from the model's internal weights by evaluating its impact on answering contextindependent common sense questions (CSQA as dataset).This approach allows us to explore how the perturbation of input text influences the model's ability to retrieve knowledge.</p>
<p>Preprint B.2 DATASET EXAMPLE B.2.1 GSM8K</p>
<p>Question: Julie wants to give her favorite cousin a $2345 mountain bike for his birthday.So far, she has saved $1500.Since the birthday is still a few weeks away, Julie has time to save even more.She plans on mowing 20 lawns, delivering 600 newspapers, and walking 24 of her neighbors' dogs.She is paid $20 for each lawn, 40 cents per newspaper, and $15 per dog.</p>
<p>After purchasing the bike, how much money will Julie have left?</p>
<p>Answer:</p>
<p>Mowing lawns will earn Julie 20 × 20 = 400 dollars.Her earnings, in dollars, from delivering newspapers will be 600 × 40 100 = 240 dollars.After walking 24 of her neighbor's dogs, she will earn 24 × 15 = 360 dollars.She will therefore earn a total of 400 + 240 + 360 = 1000 dollars.Combining earnings from her job with her savings will give Julie 1000 + 1500 = 2500 dollars.Subtracting the cost of the mountain bike from the total will leave Julie with a balance of 2500 − 2345 = 155 dollars.</p>
<p>B.2.2 MBPP</p>
<p>Text:</p>
<p>Write a function to find the peak element in the given array.</p>
<p>Test Cases:</p>
<p>assert find_peak ([1, 3, 20, 4, 1, 0], 6) == 2 assert find_peak ([2, 3, 4, 5, 6], 5) == 4 assert find_peak ([8, 9, 11, 12, 14, 15] Passage: Peptidoglycan, also known as murein, is a polymer consisting of sugars and amino acids that forms a mesh-like layer outside the plasma membrane of most bacteria, forming the cell wall.</p>
<p>The sugar component consists of alternating residues of β-(1,4) linked N-acetylglucosamine (NAG) and N-acetylmuramic acid (NAM).Attached to the N-acetylmuramic acid is a peptide chain of three to five amino acids.The peptide chain can be cross-linked to the peptide chain of another strand forming the 3D mesh-like layer.Peptidoglycan serves a structural role in the bacterial cell wall, giving structural strength, as well as counteracting the osmotic pressure of the cytoplasm.A common misconception is that peptidoglycan gives the cell its shape; however, whereas peptidoglycan helps maintain the structural strength of the cell, it is actually the MreB protein that facilitates cell shape.Peptidoglycan is also involved in binary fission during bacterial cell reproduction.</p>
<p>Answer: False</p>
<p>B.2.4 SQUAD</p>
<p>Context:</p>
<p>The control of associated biodiversity is one of the great agricultural challenges that farmers face.On monoculture farms, the approach is generally to eradicate associated diversity using a suite of biologically destructive pesticides, mechanized tools, and transgenic engineering techniques, then to rotate crops.Although some polyculture farmers use the same techniques, they also employ integrated pest management strategies as well as strategies that are more labor-intensive, but generally less dependent on capital, biotechnology, and energy.</p>
<p>Question:</p>
<p>What is one of the great agricultural challenges that farmers face?</p>
<p>Answer:</p>
<p>The control of associated biodiversity At the character level, we treat characters as the smallest operational units.TypoFuncs operate on the letters within each word.The character-level TypoFuncs are divided into three categories: reordering , inserting, and deleting (denoted as R, I, D respectively).Each of these TypoFuncs includes the following specific operations:</p>
<p>• First Character R/I/D, which performs corresponding operation on the first letter of each word.</p>
<p>• Last Character R/I/D, which performs the respective operation on the last letter of each word.</p>
<p>• k Internal Characters R/I/D, which performs the respective operation on k randomly selected internal letters (excluding the first and last) within each word.</p>
<p>Additionally, the reordering category includes the following specific operations:</p>
<p>• All Characters Reordering: This operation shuffles all the letters within the word.</p>
<p>• Internal Characters Reordering: This operation shuffles the letters in the middle of the word (excluding the first and last letters).</p>
<p>• Characters Reversing: This operation reverses the order of all letters within the word.</p>
<p>Preprint D.2 WORD Example for Word TypoFuncs</p>
<p>The shading of each word's color indicates its position in the Base.</p>
<p>Base</p>
<p>Julie wants to give her cousin a $2345 mountain bike for his birthday.</p>
<p>All Words Reordering</p>
<p>$2345 a to bike birthday cousin give her Julie wants mountain his for.</p>
<p>Adjacent Words Reordering</p>
<p>Julie to wants her give a cousin $2345 mountain bike his for birthday.</p>
<p>Words Reversing birthday his for bike mountain $2345 a cousin her give to wants Julie.</p>
<p>On the word level, we consider words to be the basic operational units.Given the substantial effect of inserting and deleting words on the overall meaning, which can cause either nuanced or substantial semantic redundancy or loss, our primary emphasis is on the following reordering operations:</p>
<p>• All Words Reordering, which randomly shuffles the words within each sentence.</p>
<p>• Adjacent Words Reordering, which randomly swaps adjacent words within each sentence.</p>
<p>• Words Reversing, which reverses the order of words within each sentence.</p>
<p>D.3 SENTENCE Example for Sentence TypoFuncs</p>
<p>The shading of each sentence's color indicates its position in the Base.</p>
<p>Base</p>
<p>The sun rises early every morning.Birds sing softly in the trees.Flowers bloom in vibrant colors daily.Children play happily in the park.People walk briskly to their jobs.Evening arrives with a peaceful calm.</p>
<p>All Words Reordering</p>
<p>Children play happily in the park.Birds sing softly in the trees.Flowers bloom in vibrant colors daily.People walk briskly to their jobs.The sun rises early every morning.Evening arrives with a peaceful calm.</p>
<p>Adjacent Words Reordering</p>
<p>The sun rises early every morning.Flowers bloom in vibrant colors daily.Birds sing softly in the trees.People walk briskly to their jobs.Children play happily in the park.Evening arrives with a peaceful calm.</p>
<p>Words Reversing</p>
<p>Evening arrives with a peaceful calm.</p>
<p>People walk briskly to their jobs.Children play happily in the park.Flowers bloom in vibrant colors daily.Birds sing softly in the trees.The sun rises early every morning.</p>
<p>On the sentence level, sentences are regarded as the basic operational units.Likewise, because of the considerable influence that sentence insertion and deletion have on textual meaning, our main focus is on the reordering operations detailed below:</p>
<p>• All Sentences Reordering, which randomly shuffles the sentences within the text.</p>
<p>• Adjacent Sentences Reordering, which randomly swaps adjacent sentences within the text.</p>
<p>• Sentences Reversing, which reverses the order of sentences within the text.</p>
<p>Preprint F.3 COMPLETION TIME AND PROMPT TOKENS</p>
<p>In this subsection, we present additional results on completion time and prompt token usage.First, we use the different datasets to comprehensively evaluate token and time consumption across different levels of scrambled input.The findings align with the conclusions drawn in the main text, further demonstrating strong parallels with human performance in scrambled reading scenarios.</p>
<p>F.4 SCRAMBLING RATIO</p>
<p>In this subsection, we provide charts illustrating the number of Reordering, Inserting, and Deleting operations in various task scenarios across additional datasets, along with LLMs' task performance.</p>
<p>The results shown in these charts are similar to the findings in the main text, further validating the impact of the scrambling ratio on LLMs.Typoglycemia Prompt: Answer the question with word or phrase based on the context below: Question: What types of tracks from popular artists did adult contemporary radio play?Context: Atnoher big itpeums for the evultooin of the AC rdiao fmorat was the papultoriy of easy ltinensig or "beuafiutl msiuc" stiotnas, satitons with music saplificecly dneigesd to be purely anmebit.Wehaers most easy lniteinsg music was inttauemsnrl, caerted by rvltaieely unonwkn atstris, and rerlay pchuaesrd, AC was an aettmpt to cearte a sliimar "lite" fmorat by csooihng ceitran tcraks (both hit sneligs and album cuts) of poalupr astrits.Response in the following format without any other information: reason: reason for the answer here answer: answer here Response from Llama-3.1-70B: reason: The context states that AC radio played certain tracks from popular artists to create a "lite" format.answer: Lite tracks from popular artists Original Quetsion: context: Another big impetus for the evolution of the AC radio format was the popularity of easy listening or "beautiful music" stations, with music specifically designed to be purely ambient.Whereas most easy listening music was instrumental, created by relatively unknown artists, and rarely purchased, AC was an attempt to create a similar "lite" format by choosing certain tracks (both hit singles and album cuts) of popular artists.question: What types of tracks from popular artists did adult contemporary radio play?</p>
<p>Original Text:</p>
<p>Franchising is a way for small business owners to benefit from the economies of scale of the big corporation (franchiser).McDonald's and Subway are examples of a franchise.The small business owner can leverage a strong brand name and purchasing power of the larger company while keeping their own investment affordable.However, some franchisees conclude that they suffer the "worst of both worlds" feeling they are too restricted by corporate mandates and lack true independence.It is an assumption that small business are just franchisees, but the truth is many franchisers are also small businesses, Although considered to be a successful way of doing business, literature has proved that there is a high failure rate in franchising as well, especially in UK, where research indicates that out of 1658 franchising companies operating in 1984, only 601 remained in 1998, a mere 36%.</p>
<p>Typoglycemia Text: snaihincFrg is a way for masll eisbnssu nwoers to teiebnf mfro the oocesiemn of lcsae of the big tcprnrooaoi (hscreainfr).MdlcDaon's and ywaSbu are eepalsxm of a ceinasrfh.The llsam nsbsuies owenr can releaevg a gornts narbd name and cigsrhaupn wproe of the glearr apcomny leiwh gieenkp ireht own ievtnsmten daefoabrfl.Hvorewe, omes esaiescnfhr oelundcc thta yteh fusfer the "trwos of btoh odrwsl" eingelf tyeh are too eirrettdcs by aroprotec natamsed and lcak teru enednenicped.It is an pusostmina tath laslm seusisnb are tujs resnhfescai, but the hturt is nmya irrfsesachn are sloa mlals bessuseisn, thlhuAgo diesorendc to be a ccesfslusu way of igdon isnusbse, etluerrtai has edorpv htat rtehe is a gihh ufrilea etra in fsinnagrchi as lelw, asiyellecp in UK, eehwr ehscerra neicdsita thta out of 1658 snfrcainhig ocnpesami otienrpga in 1984, noly 601 amirened in 1998, a reem 36%.</p>
<p>Rectified Text:</p>
<p>Franchising is a way for small business owners to benefit from the economies of scale of the big corporation (franchiser).McDonaldś and Subway are examples of a franchise.The small business owner can leverage a strong brand name and purchasing power of the larger company while keeping their own investments affordable.However, some franchisees conclude that they suffer the "worst of both worlds" feeling they are too restricted by corporate mandates and lack true independence.It is a misconception that small businesses are just franchises, but the truth is many franchisers are also small businesses.Although considered to be a successful way of doing business, literature has proved that there is a high failure rate in franchising as well, especially in the UK, where research indicates that out of 1658 franchising companies operating in 1984, only 601 remained in 1998, a mere 36%.</p>
<p>scenarios, etc. (III) Notably, a small but growing body of work aims to investigate the intrinsic cognitive mechanisms of LLMs.Through cognitive science methods, Almeida et al. (2024) investigates LLMs' moral reasoning, while Zhang et al. (2023) study collaboration mechanisms among LLM-based agents.</p>
<p>Figure 1 :
1
Figure 1: (Upper Left) The two-step process by which humans handle scrambled text.(Lower Left) The performance comparison among Human, Llama-3.1,GPT-4 on BoolQ dataset in both original and scrambled task description.We observe a widespread phenomenon of maintaining high accuracy, along with counter-intuitive improvements in certain cases.(Right) We draw the two-step process by which LLMs handle scrambled text to parallel with human.Scrambled text here is simple examples for better illustration.See our practical scrambled text cases for experiments in Appendix G.</p>
<p>Figure 2 :
2
Figure 2: TypoBench Overview.TypoPipe and TypoTask form the two components of our benchmark.The overall pipeline consists of 4 steps: Calibration, Navigation, Refabrication, and Refinement.TypoTask consists of two task categories: TypoC and TypoP which emphasize performance and perception, respectively.et al., 2006;Perea &amp; Lupker, 2004;Shaywitz &amp; Shaywitz, 2008;Rayner et al., 2012).As LLMs' powerful understanding capabilities have been recognized, a few studies attempt to explore whether LLMs exhibit similar "human-like" phenomena.Cao et al. (2023) investigates the exceptional performance of LLMs in reconstructing character-level scrambled text.Singh et al. (2024) finds that LLMs can still maintain encoding consistency when confronted with such text.However, previous work has merely showcased related phenomena without delving deeply into the underlying mechanisms of LLMs.In our research, we systematically migrate the Typoglycemia phenomenon across multi-granularity to LLMs and provide a comprehensive explanation for its underlying causes.</p>
<p>and LLM as a function M : T → T .For any finite set X , we use x i (1 ≤ i ≤ |X |) to refer to its element for convenience.①Calibration: We denote the family of datasets as D = {D 1 , D 2 , . . ., D t }.Calibration aims at:</p>
<p>Figure 3 :
3
Figure 3: Token and time consumption ratio before and after being processed by TypoFunc when FΩ = REO-INT on character level for BoolQ dataset.</p>
<p>Figure 4 :
4
Figure 4: The line charts of accuracy for each model, as the number of operations increase from 1 to 4 when FΩ =REO INT, INS INT, and DEL INT at character level on GSM8k dataset.</p>
<p>Figure 5 :
5
Figure 5: Decoder Perspective: The cosine similarity between the representations of normal text and the text processed by FΩ in the SQuAD and BoolQ for each layer of the Llama-3.1-8Bmodel (which has 33 layers in total: 1 word embedding layer and 32 Transformer layers).BASE is the standard for similarity calculation.</p>
<p>Figure 6 :
6
Figure 6: Consumption ratio before and after being processed by TypoFunc when FΩ is set to REO-ALL on character level for CSQA.</p>
<p>Figure 7 :
7
Figure 7: Consumption ratio before and after being processed by TypoFunc when FΩ is set to REO-INT on character level for CSQA.</p>
<p>Figure 8 :
8
Figure 8: Consumption ratio before and after being processed by TypoFunc when FΩ is set to REO-REV on character level for CSQA.</p>
<p>Figure 9 :
9
Figure 9: Consumption ratio before and after being processed by TypoFunc when FΩ is set to ADD-BEG on character level for GSM8k.</p>
<p>Figure 10 :
10
Figure 10: Consumption ratio before and after being processed TypoFunc when FΩ is set to ADD-END on character level for GSM8k.</p>
<p>Figure 11 :
11
Figure 11: Consumption ratio before and after being processed by TypoFunc when FΩ is set to DEL-BEG on character level for GSM8k.</p>
<p>Figure 12 :
12
Figure 12: Consumption ratio when before and after being processed by TypoFunc FΩ is set to DEL-END on character level for GSM8k.</p>
<p>Figure 13 :
13
Figure 13: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-INT 3 on character level for GSM8k.</p>
<p>Figure 14 :
14
Figure 14: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-BEG on character level for BoolQ.</p>
<p>Figure 15 :
15
Figure 15: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-END on character level for BoolQ.</p>
<p>Figure 16 :
16
Figure 16: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-ALL on word level for BoolQ.</p>
<p>Figure 17 :
17
Figure 17: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-ADJ on word level for BoolQ.</p>
<p>Figure 18 :
18
Figure 18: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-ADJ on sentence level for MBPP.</p>
<p>Figure 19 :
19
Figure 19: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-REV on sentence level for MBPP.</p>
<p>Figure 20 :
20
Figure 20: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-REV on word level for BoolQ.</p>
<p>Figure 21 :
21
Figure 21: Consumption ratio when before and after being processed by TypoFunc FΩ is set to REO-ALL on sentence level for MBPP.</p>
<p>Figure 22 :
22
Figure 22: The line charts of accuracy for each model, as the number of operations increase from 1 to 4 when FΩ =REO INT, INS INT, and DEL INT on BoolQ datset.</p>
<p>Figure 23 :
23
Figure 23: The line charts of accuracy for each model, as the number of operations increase from 1 to 4 when FΩ =REO INT, INS INT, and DEL INT on CSQA datset.</p>
<p>Figure 24 :
24
Figure 24: The line charts of accuracy for each model, as the number of operations increase from 1 to 4 when FΩ =REO INT, INS INT, and DEL INT on SQuAD datset.</p>
<p>Figure 25 :
25
Figure 25: The cosine similarity between the representations of normal text and the text processed by FΩ in the GSM8k dataset for each layer of the Gemma-2-2B model (which has 27 layers in total: 1 word embedding layer and 26 Transformer layers).BASE is the standard for similarity calculation.</p>
<p>Figure 26 :
26
Figure 26: The cosine similarity between the representations of normal text and the text processed by FΩ in the GSM8k dataset for each layer of the Gemma-2-9B model (which has 43 layers in total: 1 word embedding layer and 42 Transformer layers).BASE is the standard for similarity calculation.</p>
<p>Figure 27 :
27
Figure 27: The cosine similarity between the representations of normal text and the text processed by FΩ in the BoolQ dataset for each layer of the Gemma-2-2B model (which has 27 layers in total: 1 word embedding layer and 26 Transformer layers).BASE is the standard for similarity calculation.</p>
<p>Figure 28 :
28
Figure 28: The cosine similarity between the representations of normal text and the text processed by FΩ in the BoolQ dataset for each layer of the Gemma-2-9B model (which has 43 layers in total: 1 word embedding layer and 42 Transformer layers).BASE is the standard for similarity calculation.</p>
<p>Figure 29 :
29
Figure 29: The cosine similarity between the representations of normal text and the text processed by FΩ in the SQuAD dataset for each layer of the Gemma-2-9B model (which has 43 layers in total: 1 word embedding layer and 42 Transformer layers).BASE is the standard for similarity calculation.</p>
<p>Figure 30 :
30
Figure 30: The cosine similarity between the representations of normal text and the text processed by FΩ in the SQuAD dataset for each layer of the Gemma-2-2B model (which has 27 layers in total: 1 word embedding layer and 26 Transformer layers).BASE is the standard for similarity calculation.</p>
<p>Figure 31 :
31
Figure 31: The cosine similarity between the representations of normal text and the text processed by FΩ in the GSM8k dataset for each layer of the Llama-3.1-8Bmodel (which has 33 layers in total: 1 word embedding layer and 32 Transformer layers).BASE is the standard for similarity calculation.</p>
<p>phetamine was developed by Smith, Kline and French as a nasal decongestant under the trade name Benzedrine Inhaler.Amphetamine was eventually developed for the treatment of narcolepsy, post-encepheletic parkinsonism, and mood elevation in depression and other psychiatric indications.It received approval as a New and Nonofficial Remedy from the American Medical Association for these uses in 1937 and remained in common use for depression until the development of tricyclic antidepressants in the 1960s.question: What was the trade name of amphetamine as a nasal decongestant?Answer from Dataset: Benzedrine Inhaler G.3.2 BAD MIND One case of Llama-3.1-70B'sincorrect responses when F Ω =Char-REO-INT 4.</p>
<p>GPT-4o-mini's correct responses when F Ω =Char-INS-INT 2. Typoglycemia Prompt: Choose one choice that best answers the commonsense question below: Question: WnheJre dsqoes a dqooYctor wgfork silZowtly?Choices: [emergency room, nursing home, medical school, dentist, golf course] Response in the following format without any other information: reason: {reason for the choice here} answer: {one choice from the choices list here} Response from GPT-4o-mini: reason: A nursing home typically involves providing ongoing care for residents, which can be a slower-paced environment compared to the fast-paced nature of an emergency room or the more specialized settings like a dentist's office.answer: nursing home Original Question: question: Where does a doctor work slowly?choices: [emergency room, nursing home, medical school, dentist, golf course] GPT-4o's performance on Rectify TypoP task when F Ω =Char-REO-ALL.</p>
<p>Table 4 :
4
Encoder Perspective: The cosine similarity between the embedding of normal text and text processed by FΩ, using text-embedding-3 to get the vectors.BASE is the standard for similarity calculation.
Datasets/FΩCharacter LevelWord LevelSentence LevelREVGSM8k0.7550.8910.5940.8650.8300.9300.9620.9150.9780.9870.967BoolQ0.9120.9440.8630.9440.9330.9650.9780.9600.9860.9930.980CSQA0.8670.9220.8360.9100.8900.9490.9680.9400.9990.9980.9974.3 IMPACT OF TYPOGLYCEMIA FUNCTIONS (RQ2)
REO-ALL REO-INT REO-REV INS-INT 3 DEL-INT 3 REO-ALL REO-ADJ REO-REV REO-ALL REO-ADJ REO-</p>
<p>Table 5 :
5
Dataset Details of TypoBench
DatasetTypoC TaskSizeMetricsSample NumberGSM8kMathematical Problem Solving17,584 Accuracy/CosSim1,200MBPPCode Generation1,401 Accuracy/CosSim700BoolQ Context Question Answering (yes/no) 12,697 Accuracy/CosSim1,200SQuAD Context Question Answering (phrases) 98,169 Accuracy/CosSim1,000CSQACommonsense Reasoning12,102 Accuracy/CosSim2,000B.1 DATASET SELECTION STRATEGY</p>
<p>experiments on Typoglycemia typically involve transposing the letters at the beginning, end, and internal positions within words.We have extended this operation to a broader set of TypoFuncs at the letter, word, and sentence levels.Additionally, at the character level, we have designed TypoFuncs such as insertion and deletion.
Preprint Preprint PreprintC TASK PROMPT C.2 TASK PERCEPTION PROMPT D TYPOFUNC DESCRIPTIONC.2.1 RECTIFYC.1 TASK COMPLETION PROMPTCorrect the scrambled letters in each word of the following passage:C.1.1 MATHEMATICAL PROBLEM SOLVING Passage: {passage text}Response in the following format without any other information:rectified: <rectified passage here > D.1 CHARACTERSolve the math problem below:Problem: {mathematical problem description} Response in the following format without any other information: C.3 SUMMARIZE Example for Character TypoFuncsprocess: <reasoning steps here> The letter's color indicates its position in the Base. answer number: <final answer number here> Summarize the main content of the following passage: BasePassage: {passage text}Response in the following format without any other information: Typoglycemiasummarized: <summarized passage here> C.1.2 MATHEMATICAL PROBLEM SOLVING ReorderingSolve the code problem below in Python: C.4 TRANSLATE First Character Reodering: yTpoglycemiaProblem: {code description} Last Character Reodering: TypoglycemaiResponse in the following format without any other information: Translate the following English passage into Chinese: k Internal Characters Reodering (k=6): Tygoplymeciaprocess: <reasoning steps here> Passage: {passage text} All Characters Reordering: clpemyaogTyicode: <Python code here> Response in the following format without any other information: Internal Characters Reordering: Tygomlcepiyatranslated: <translated passage here> Characters Reversing: aimecylgopyTInsertingC.1.3 CONTEXT QUESTION ANSWERINGFirst Character Inserting: pTypoglycemiaLast Character Inserting: TypoglycemiapAnswer the question with only 'yes' or 'no' based on the passage below: k Internal Characters Inserting (k=6): ToyWpUpoyglybcemiaQuestion: {question description}Passage: {context passage} Response in the following format without any other information: Deletingreason: <reason for yes or no here> First Character Deleting: ypoglycemia answer: &lt;'yes' or 'no' here&gt; Last Character Deleting: Typoglycemik Internal Characters Deleting (k=6): T og miaAnswer the question with word or phrase based on the context below:Question: {question description}Passage: {context passage}Response in the following format without any other information:B.2.5 CSQA reason: <reason for the answer here>answer: <answer here>Question:John watches the well-dressed people from a catwalk above the stage. He listens to themspeak rehearsed lines while the audience listens. Where is he? C.1.4 COMMONSENSE REASONINGCorrect Answer:A. theatre
Choices: A. theatre B. new york city C. fashion show D. construction site E. school play Choose one choice that best answers the commonsense question below: Question: {question description} Choices: {context passage} Response in the following format without any other information: reason: <reason for the choice here> answer: <one choice from the choices list here> Psychological</p>
<p>One case of GPT-4o's incorrect responses when F Ω =Char-REO-ALL.One case of Gemma-27B's correct responses when F Ω =Word-REO-ALL.One case of Gemma-27B's incorrect responses when F Ω =Word-REO-REV.70B's correct responses when F Ω =Char-DEL-BEG.Japanese chemists in the Ma Huang plant and marketed by Eli Lilly as an oral treatment for asthma.Following the work of Henry Dale and George Barger at Burroughs-Wellcome, academic chemist Gordon Alles synthesized amphetamine and tested it in asthma patients in 1929.The drug proved to have only modest anti-asthma effects, but produced sensations of exhilaration and palpitations.Am
Preprint Preprint Preprint PreprintG TYPOC CASES G.1.2 BAD MIND G.2 CONTEXT QUESTION ANSWERING WITH YES/NO (BOOLQ) G.2.2 BAD MIND G.3 CONTEXT QUESTION ANSWERING WITH PHRASES (SQUAD)G.1 MATHEMATICAL PROBLEM SOLVING (GSM8K) G.2.1 GOOD MIND G.3.1 GOOD MINDG.1.1 GOOD MIND Typoglycemia Prompt: Solve the math problem below: Problem: iamrSa is the nassatsti oachc Typoglycemia Prompt: Typoglycemia Prompt: Answer the question with only 'yes' or 'no' based on the passage below: One case of Llama-3.1-Typoglycemia Prompt: One case of GPT-4o's correct responses when F Ω =Char-REO-INT. Typoglycemia Prompt: Solve the math problem below: Problem: Jluie wtnas to gvie her firoatve cusoin a $2345 mtinauon bkie for his btrihday. So far, she has saved $1500. Sncie the bdirthay is slitl a few wekes away, Jilue has time to save eevn more. She palns on miwong 20 lawns, dienliervg 600 npawsreeps, and wianklg 24 of her nhbriegos' dgos. She is paid $20 for each lawn, 40 cnets per nepawe-spr, and $15 per dog. Afetr puhsicanrg the bike, how much menoy wlil Jilue have left? Response in the following format with-out any other information: process: {reasoning steps here} answer number: {final answer number here} Response from GPT-4o: process: 1. Calculate the total amount Julie will earn from mowing lawns: 20 lawns * $20/lawn = $400. 2. Calculate the total amount Julie will earn from delivering newspapers: 600 newspapers * $0.40/newspaper = $240. 3. Calculate the total amount Julie will earn from walking dogs: 24 dogs * $15/dog = $360. 4. Add the amounts from steps 1, 2, and will earn: $400 + $240 + $360 = $1000. 5. Add the amount Julie has already saved to the total additional money: $1500 + $1000 = $2500. 6. Subtract the cost of the bike from the total money Julie will have: $2500 -$2345 = $155. 300. Subtract 50 from both sides to get answer: Benzedrine Inhaler cents). Simplifying, we get 2x + 50 = congestant under a specific trade name. x+(x+50) = 300 cents (since $3 is 300 phetamine was developed as a nasal de-has x + 50 cents. Together, they have reason: The context mentions that am-Let Sarah have x cents. Then, Bigrde #### 155 Response from Llama-3.1-70B: ### 155 1000 dollars. Combining earnings from her job with her savings will give Julie 1000 + 1500 2345 = 155⟩⟩ 155 dollars. answer: yes a balance of 2500 -2345 = ⟨⟨2500 − only one point. bike from the total will leave Julie with sible for a team to finish a game with answer: {answer here} Subtracting the cost of the mountain reason: The passage states that it is pos-reason: {reason for the answer here} = ⟨⟨1000+1500 = 2500⟩⟩ 2500 dollars. Response from Gemma-27B: without any other information: True Response in the following format Answer from Dataset: ntidepressants in the1960s. 3 to find the total additional money Julie Original Question: Julie wants to give her favorite cousin a $2345 mountain bike for his birth-day. So far, she has saved $1500. Since the birthday is still a few weeks away, Julie has time to save even more. She plans on mowing 20 lawns, delivering 600 newspapers, and walking 24 of her neighbors' dogs. She is paid $20 for each lawn, 40 cents per newspaper, and $15 per dog. After purchasing the bike, how much money will Julie have left? Answer from Dataset: Mowing lawns will earn Julie 20 * 20 = ⟨⟨20  *  20 = 400⟩⟩ 400 dollars Her earnings, in dollars, from deliver-ing newspapers will be 600 * 40 / 100 = ⟨⟨600  *  40/100 = 240⟩⟩ 240 dollars After walking 24 of her neighbor's dogs, she will earn 24 * 15 = ⟨⟨24 * 15 = 360⟩⟩ 360 dollars. She will therefore earn a total of 400 + answer: {'yes' or 'no' here} ntil the evelopment of ricyclic 240 + 360 = ⟨⟨400+240+360 = 1000⟩⟩ of a rccseo aetm gynlpai saingta one of the steb temsa in hreit ueealg. She has rfuo zneod etrwa blsotet dlefli thwi awrte in a box. In the itsfr kbrae of the chmta, the 11 apelsry on the fdlei ceah teka two tltoesb of rtawe from iamaSr's box, and at the end of the eagm, kate one eorm tetbol chea. How ymna eosbttl of erwat are miinrange in Saimar's box? process: {reasoning steps here} answer number: {final answer number here} Response from GPT-4o: process: 1. Start with 4 bottles of water. 2. Each of the 11 players takes 2 bot-tles during the first break: 11 players * 2 bottles = 22 bottles. 3. Each of the 11 players takes 1 more bottle at the end of the game: 11 players * 1 bottle = 11 bottles. 4. Total bottles taken: 22 bottles + 11 bottles = 33 bottles. 5. Since there were only 4 bottles ini-tially, and 33 bottles were taken, it im-plies that the problem is not feasible as described. Therefore, the remaining bottles would be 0. answer number: 0 Answer the question with only 'yes' or Question: is there a train station at Answer the question with word or Original Quetsion: Samira is the assistant coach of a soc-cer team playing against one of the best teams in their league. She has four dozen water bottles filled with water in a box. In the first break of the match, the 11 players on the field each take two bottles of water from Samira's box, and at the end of the game, take one more bottle each. How many bottles of water are remaining in Samira's box? Answer from Dataset: If the box has four dozen bottles of wa-ter, there are 4 * 12 = ⟨⟨4  *  12 = 48⟩⟩ 48 bottles of water in the box After the first half, the 11 players take 11 * 2 = ⟨⟨11  *  2 = 22⟩⟩ 22 bottles of water from the box. If they take 11 more bottles of water at the end of the game, the number in-creases to 22 + 11 = 33 bottles of water 33 = ⟨⟨48 − 33 = 15⟩⟩ 15 bottles of wa-ter remaining in the box. #### 15 A also safety, though highly unlikely; never has this although occurred, a fin-ish an point team only the single way possible football game is a with it could American in. Response in the following format with-out any other information: reason: {reason for yes or no here} safety by the defense is also possible, though highly unlikely; although this has never occurred, it is the only possi-ble way a team could finish with a single point in an American football game. emained in ommon use for epression and. answer: yes ssociation for hese ses in 1937 and small a now is yard goods former the emedy rom the merican edical station the of location The, park car pproval as a New and onofficial reason: The passage states that "itself sychiatric ndications. It eceived Response from Gemma-27B: levation in epression and ther False least 1940 in the NFL. A conversion ncepheletic arkinsonism, and ood Answer from Dataset: version safeties have occurred since at for the reatment of arcolepsy, ost-Without the bottles taken, there are 48 -by the possible conversion defense is cently at the 2013 Fiesta Bowl. No con-answer: {'yes' or 'no' here} mphetamine was ventually eveloped taken. 'no' based on the passage below: Question: is it possible to have a score of 1 in football Passage: rare not common American method occurrences since of are -foot-ball are but the 1932 scoring in Safeties least, has every a occurred 14 safety once. Football in (NFL) the games 31 League National, under scheduling about week or rules current a once. On 21 October, 1973, Fred Dryer, Rams Bay against the Angeles Packers Los for Green the playing, score only NFL game safeties to History a in single be-came the in player two. A occurrence is one-point safety rarer much the, of-fense which by two-point the on attempt scored an be extra can conversion point or; least in NCAA have Division oc-curred I since 1996 at those twice foot-ball, most Bowl at 2013 the Fiesta re-cently. in No conversion at 1940 least since the have NFL occurred safeties. Original Question: question: is it possible to have a score of 1 in football passage: Safeties are the least common method of scoring in American foot-ball but are not rare occurrences -since 1932, a safety has occurred once every 14.31 games in the National Football League (NFL), or about once a week un-der current scheduling rules. On Octo-ber 21, 1973, Fred Dryer, playing for the Los Angeles Rams against the Green Bay Packers, became the only player in NFL History to score two safeties in a single game. A much rarer occurrence is the one-point safety, which can be scored by the offense on an extra point or two-point conversion attempt; those have occurred at least twice in NCAA Division I football since 1996, most re-burnham on sea Passage: closure station's After, demol-ished were shed goods and platform the. Drive Marine into developed was east-wards trackbed former The, town the into flow traffic ease helped has which road a. Street Abingdon and Approach Station Old of junction the on is it-self station the of location The, park car small a now is yard goods former the and. Dorset &amp; Somerset the re-named subsequently was Street High the on pub adjacent An. building station main former the of east and south the to land occupy yard and station RNLI new The, platform excursion former the of site the to adjacent. side seaward-west the to built was supermarket Somerfield specifically-designed ever first The, Morrisons to sold then was which. Response in the following for-mat without any other information: reason: {reason for yes or no here} phrase based on the context below: Original Quetsion: Question: What was the trade name of question: is there a train station at burn-amphetamine as a nasal decongestant? ham on sea Original Question: Context: hile ighly ffective, the passage: After station's closure, the context: While highly effective, the re-equirement for njection imited the platform and goods shed were demol-use of quirement for injection limited the use orepinephrine[ larification ished. The former trackbed eastwards of norepinephrine[clarification needed] eeded] and rally ctive erivatives was developed into Marine Drive, a road ere ought. and orally active derivatives were A tructurally imilar which has helped ease traffic flow into sought. A structurally similar com-ompound, phedrine, was dentified the town. The location of the station the rade ame enzedrine nhaler. side, which was then sold to Morrisons. rench as a asal econgestant nder market was built to the seaward-west was eveloped by mith, line and specifically-designed Somerfield super-and alpitations. mphetamine mer excursion platform. The first ever roduced ensations of xhilaration building, adjacent to the site of the for-ave nly odest nti-sthma ffects, but and east of the former main station atients in 1929. The rug roved to tion and yard occupy land to the south mphetamine and ested it in sthma Somerset &amp; Dorset. The new RNLI sta-hemist ordon lles ynthesized Street was subsequently renamed the arger at urroughs-ellcome, cademic park. An adjacent pub on the High the ork of enry ale and eorge former goods yard is now a small car ral reatment for sthma. ollowing Approach and Abingdon Street, and the lant and arketed by Eli illy as an itself is on the junction of Old Station by apanese hemists in the Ma uang pound, ephedrine, was identified by2x = 250. Divide both sides by 2 to getx = 125.answer number: 155
https://platform.openai.com/docs/models/gpt-3-5-turbo
https://platform.openai.com/docs/models/gpt-4o-mini
https://platform.openai.com/docs/models/gpt-4o
https://platform.openai.com/docs/models/embeddings
Preprint E PARAMETER SETTINGSTo ensure stability and consistency in the model outputs, we set top p = 1, n = 1, f requency penalty = 0, and presence penalty = 0 for all models.The temperature is set to 0 for GPT series models, and to 10 −6 for Llama and Gemma series models.F MORE RESULTSF.1 TYPOCIn this subsection, we further present the performance of various LLMs on two additional datasets, MBPP and SQuAD, in the TypoC task.The conclusions drawn from these results are consistent with those in the main text, further supporting the findings on the impact of the Typoglycemia scenario on LLMs.F.1.1 REORDERINGTable6: Results on the TypoC tasks when FΩ = REO at character, word, and sentence levels.We evaluate the average task accuracy (over 3 runs) of various LLMs on the MBPP and SQuAD datasets.BASE refers to the scenario where FΩ is not applied.In each dataset, red (blue) marks the maximum value in each row (column), and green marks values that are the maximum in both.Gray marks the values that are higher than BASE in each row.MBPP and SQuAD report the cosine similarity and accuracy, respectively.BASE of GPT-4o is the standard for similarity calculation.Gemma-2 series and Llama-3.  .Datasets/F Ω Gemma-2-2B Gemma-2-9B Gemma-2-27B Llama-3.1-8BLlama-3.1-70BGPT-3.5-TurboGPT-4o-mini GPT-4oF.2 TYPOPIn this subsection, we present the performance of various LLMs on two additional tasks in TypoP: Summarize and Translate.The conclusions drawn from these results are consistent with those in the main text: the results of TypoP align with those of TypoC.Answer from Dataset:One case of Gemma-2-2B's performance on Summarize TypoP task when F Ω =Char-DEL-INT 1.Original Text:A charity shop or thrift shop is a retail establishment run by a charitable organization to raise money.Charity shops are a type of social enterprise.They sell mainly used goods such as clothing, books, music albums, home videos, and furniture donated by members of the public, and are often staffed by volunteers.Because the items for sale were obtained for free, and business costs are low, the items can be sold at competitive prices.After costs are paid, all remaining income from the sales is used in accord with the organization's stated charitable purpose.Costs include purchase and/or depreciation of fixtures (clothing racks, bookshelves, counters, etc.), operating costs (maintenance, municipal service fees, electricity, heat, telephone, limited advertising) and the building lease or mortgage.Typoglycemia Text: A c arity s op or thr ft s op is a re ail est blishment run by a ch ritable organizat on to r ise mon y.Char ty s ops are a t pe of s cial ente prise.T ey s ll ma nly us d g ods su h as cloth ng, bo ks, m sic al ums, ho e vid os, and fur iture do ated by me bers of the pub ic, and are o ten s affed by volunte rs.B cause the it ms for sa e we e o tained for fr e, and busin ss c sts are low, the it ms can be s ld at competiti e pri es.Aft r cos s are pa d, all remai ing in ome f om the s les is u ed in acc rd w th the orga ization's s ated charita le pur ose.Co ts inc ude pu chase and/or deprec ation of fix ures (cl thing rac s, books elves, counte s, etc.), operat ng cos s (ma ntenance, munic pal se vice f es, ele tricity, he t, tele hone, lim ted advertisi g) and the bu lding lea e or mor gage.Summarized Text:Charity shops or thrift shops are retail establishments run by charitable organizations to raise money.They sell donated goods such as clothing, books, music albums, home videos, and furniture at competitive prices.After covering costs, including the purchase and depreciation of fixtures, operating costs, and building lease or mortgage, the remaining income is used for the organization's charitable purpose.Original Text:A petroleum geologist is an earth scientist who works in the field of petroleum geology, which involves all aspects of oil discovery and production.Petroleum geologists are usually linked to the actual discovery of oil and the identification of possible oil deposits or leads.It can be a very labor-intensive task involving several different fields of science and elaborate equipment.Petroleum geologists look at the structural and sedimentary aspects of the stratum/strata to identify possible oil traps.Typoglycemia Text:A petrolbeum gteologist is an eLarth scientWist who woprks in the fqield of geQology, whiSch iNnvolves all aspecrts of oil deiscovery and productcion.Puetroleum gieologists are uFsually liMnked to the acZtual discotvery of oil and the identificagtion of pUossible oil deposoits or lBeads.It can be a veCry lzabor-inteqnsive tajsk involvieng sIeveral differeHnt ficelds of scieNnce and elatborate equpipment.Peetroleum geolLogists loyok at the stRructural and sePdimentary aspeCcts of the sqtratum/strqata to idezntify possiible oil trxaps.Summarized Text:A petroleum geologist is an earth scientist who works in the field of petroleum geology, which involves all aspects of oil discovery and production.They are typically linked to the actual discovery of oil and the identification of possible oil deposits or leads.Petroleum geologists analyze the structural and sedimentary aspects of the subsurface to identify potential oil traps.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>A compositional neural code in high-level visual cortex can explain jumbled word reading. Aakash Agrawal, Hari, Sp Arun, 2020Elife9e54846</p>
<p>Exploring the psychology of llms' moral and legal reasoning. José Luiz Guilherme Fcf Almeida, Neele Nunes, Alex Engelmann, Marcelo Wiegmann, Araújo De, Artificial Intelligence. 3331041452024</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Using cognitive psychology to understand gpt-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Anatomy and physiology of word-selective visual cortex: from visual features to lexical processing. Sendy Caffarra, Iliana I Karipidis, Maya Yablonski, Jason D Yeatman, Brain structure and function. 22692021</p>
<p>Unnatural error correction: Gpt-4 can almost perfectly handle unnatural scrambled text. Qi Cao, Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Autoagents: A framework for automatic agent generation. Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Jie Börje F Karlsson, Yemin Fu, Yusheng Shi ; Weize Chen, Jingwei Su, Cheng Zuo, Chenfei Yang, Chen Yuan, Chi-Min Qian, Yujia Chan, Yaxi Qin, Ruobing Lu, Xie, arXiv:2309.17288arXiv:2308.10848Facilitating multi-agent collaboration and exploring emergent behaviors in agents. 2023a. 2023b26arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, arXiv:1905.100442019arXiv preprint</p>
<p>Good-enough representations in language comprehension. Fernanda Ferreira, Karl Gd Bailey, Vittoria Ferraro, Current directions in psychological science. 1112002</p>
<p>Towards a universal model of reading. Ram Frost, Behavioral and brain sciences. 3552012</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Dan Preprint, Collin Hendrycks, Steven Burns, Andy Basart, Mantas Zou, Dawn Mazeika, Jacob Song, Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multiagent collaborative framework. 2023arXiv preprint</p>
<p>Transposed-letter effects in reading: evidence from eye movements and parafoveal preview. Rebecca L Johnson, Manuel Perea, Keith Rayner, 200733209Journal of Experimental psychology: Human perception and performance</p>
<p>A theory of reading: from eye fixations to comprehension. Marcel Adam, Just , Patricia A Carpenter, Psychological review. 8741980</p>
<p>Llm-arc: Enhancing llms with an automated reasoning critic. Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, Jennifer Chu-Carroll, David Melville, David Ferrucci, arXiv:2406.176632024arXiv preprint</p>
<p>The stack: 3 tb of permissively licensed source code. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, arXiv:2211.155332022arXiv preprint</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. 202336</p>
<p>Avalonbench: Evaluating llms playing the game of avalon. Jonathan Light, Min Cai, Sheng Shen, Ziniu Hu, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Yicheng Bill Yuchen Lin, Karina Fu, Faeze Yang, Shiyu Brahman, Chandra Huang, Prithviraj Bhagavatula, Yejin Ammanabrolu, Xiang Choi, Ren, Advances in Neural Information Processing Systems. 202436</p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Le Ronan, Yejin Bras, Hannaneh Choi, Hajishirzi, arXiv:2110.08387Generated knowledge prompting for commonsense reasoning. 2021arXiv preprint</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. Thomas Mccoy, Ellie Pavlick, Tal Linzen, arXiv:1902.010072019arXiv preprint</p>
<p>The magical number seven, plus or minus two: Some limits on our capacity for processing information. George A Miller, Psychological review. 10121994</p>
<p>Can caniso activate casino? transposed-letter similarity effects with nonadjacent letter positions. Manuel Perea, Stephen J Lupker, Journal of memory and language. 5122004</p>
<p>Limited ability of llms to simulate human psychological behaviours: a psychometric analysis. Gregory Nikolay B Petrov, Jason Serapio-García, Xin Rentfrow ; Chen Qian, Cheng Cong, Weize Yang, Yusheng Chen, Juyuan Su, Xu, arXiv:2405.07248arXiv:2307.07924Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. 2024. 20236arXiv preprint</p>
<p>Squad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.052502016arXiv preprint</p>
<p>Raeding wrods with jubmled lettres: There is a cost. Keith Rayner, Sarah J White, Liversedge, 2006</p>
<p>. Keith Rayner, Alexander Pollatsek, Jane Ashby, Charles CliftonJr, Psychology of reading. 2012Psychology Press</p>
<p>Preprint Rico, Sennrich , arXiv:1508.07909Neural machine translation of rare words with subword units. 2015arXiv preprint</p>
<p>Paying attention to reading: The neurobiology of reading and dyslexia. E Sally, Bennett A Shaywitz, Shaywitz, Development and psychopathology. 2042008</p>
<p>Learning to decode collaboratively with multiple language models. Shannon Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, David Sontag, arXiv:2403.038702024arXiv preprint</p>
<p>Ayush Singh, Navpreet Singh, Shubham Vatsal, arXiv:2407.08989Robustness of llms to perturbations in text. 2024arXiv preprint</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.009372018arXiv preprint</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.001182024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, Guohao Li, arXiv:2402.04559Can large language model agents simulate human trust behaviors?. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Easytool: Enhancing llm-based agents with concise tool instruction. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, Deqing Yang, arXiv:2401.062012024arXiv preprint</p>
<p>Exploring collaboration mechanisms for llm agents: A social psychology view. Jintian Zhang, Xin Xu, Shumin Deng, arXiv:2310.021242023arXiv preprint</p>
<p>Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety. Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu Qiao, Jing Shao, arXiv:2401.118802024arXiv preprint</p>
<p>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Chen Hao, Xing Xie, arXiv:2310.17512Competeai: Understanding the competition behaviors in large language model-based agents. 2023arXiv preprintSQuAD Gemma-2-2B</p>
<p>ENCODER EMBEDDING In this subsection, we present the similarity of text embedding across additional datasets and the similarity of input text representations at each layer of the Transformer across more models and datasets. The results shown in these charts are consistent with those in the main text, providing further data to support the related conclusions. Table 9: The cosine similarity between the embedding of normal text and text processed by FΩ, using text-embedding-3 to get the vectors. BASE is the standard for similarity calculation. F Ω /Datasets MBPP SQuAD Char. F Preprint, REO-ALL 0.613 0.857</p>
<p>DECODER REPRESENTATION. F , </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6834 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6834</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6834</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-264128006</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09107v2.pdf" target="_blank">GLoRE: Evaluating Logical Reasoning of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6834.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6834.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General Logical Reasoning Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic benchmark suite introduced in this paper to evaluate instruction-tuned LLMs on strict natural-language logical reasoning across three task families (Multi-choice Reading Comprehension, Natural Language Inference, Yes-or-No) using 12 datasets and ~72.8k instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLoRE (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified evaluation platform consolidating diverse logical reasoning datasets and standardizing them for zero-shot and few-shot evaluation of language models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>benchmark / dataset aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>12 datasets including LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter (total ~72,848 instances)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>N/A (benchmark evaluates models using their native reasoning methods; supports zero-shot and in-context few-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (composed of LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluates multi-step verbal logical reasoning via three task families: multi-choice reading comprehension (MRC), natural language inference (NLI), and true/false (yes/no) questions; includes high-quality, challenging datasets (both synthetic rule datasets and real exam-style questions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No (True/False) reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (classification accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>per-dataset accuracies reported; aggregated averages used for model comparisons (see individual model entries)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Designed to compare instruction-tuned LLMs, supervised baselines, and reasoning-enhanced models across consistent data splits; highlights distribution sensitivity and generalization gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides standardized measures showing (1) reasoning-enhanced models (QwQ-32B, DeepSeek R1, o1 mini) substantially outperform earlier LLMs and supervised baselines on many GLoRE tasks, (2) performance varies strongly by dataset indicating distributional sensitivity, and (3) in-context learning helps but often reflects statistical adaptation rather than robust logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Possible data-overlap/leakage risks across datasets; benchmark updates required to remain OOD for newer models; does not by itself identify causal reasoning mechanisms in models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6834.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art open-source, reasoning-enhanced LLM reported to use a reinforcement-learning based training framework and achieves top performance on the GLoRE benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model described as 'reasoning-enhanced' with a reinforcement learning framework or specialized training methodology that improves logical inference generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; reasoning-enhanced (training/algorithmic modifications reported; exact architectural innovations not fully specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in detail in this paper; described as instruction-tuned and improved via a reinforcement-learning framework (paper references Qwq-32b: Embracing the power of reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Reinforcement-learning-driven instruction tuning / specialized training to incentivize logical reasoning; evaluated under zero-shot and few-shot ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Comprehensive logical-reasoning benchmark covering MRC, NLI, and yes/no proof-style tasks (synthetic and natural).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-choice reading comprehension (MRC), NLI, Yes-or-No proof/entailment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average accuracy on GLoRE: 78.95% (state-of-the-art reported); selected per-dataset examples: ReClor 93.76%, AR-LSAT 92.35%, ProofWriter 82.40%, LogiQA22 86.30%, HELP 61.53%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms GPT-4 (66.34% average) by +12.61 percentage points on average; outperforms DeepSeek R1 (75.14% average) by +3.81 points; sets new SOTA on multiple MRC tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>QwQ-32B achieves the highest average performance on GLoRE, dominating MRC and TF tasks and demonstrating improved robustness on some OOD data (e.g., strong on LogiQA22), suggesting its training fosters stronger patterning for exam-like logical questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Uneven performance across NLI datasets (e.g., HELP 61.53% lags behind o1 mini), indicating weaker generalization to monotonicity/negation reasoning and fine-grained entailment; paper does not disclose full training corpus or exact RL procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6834.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced language model claiming improved logical inference via reinforcement-learning-based incentives, evaluated as a strong open-source competitor on GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM described as 'reasoning-enhanced' through a reinforcement learning incentive framework to improve reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; reasoning-enhanced via reinforcement learning (training-level modification)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not detailed in this paper; described conceptually as leveraging RL incentives for reasoning (referenced Deepseek-Ai paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Reinforcement-learning-based training to incentivize reasoning behaviors; evaluated in zero-shot and few-shot ICL settings.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (component datasets as above)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluations include MRC, NLI, and yes/no tasks; both human exam questions and synthetic rule-based proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No (TF)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average accuracy on GLoRE: 75.14%; selected per-dataset examples (from paper table): overall balanced performance across tasks (detailed per-dataset values reported in paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Substantially better than older LLMs (GPT-4 average 66.34%); within a few points of QwQ-32B (75.14% vs 78.95%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DeepSeek R1 narrows the gap to top performers and produces balanced performance across many GLoRE tasks, supporting the effectiveness of RL incentives for improving logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still sensitive to data distribution; some datasets show lower NLI performance; precise RL mechanics and training data not disclosed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6834.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial instruction-following LLM from OpenAI, evaluated as one of the reasoning-enhanced commercial models on GLoRE with competitive NLI performance on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An OpenAI instruction-tuned model in the 'o1' family (commercial); described as reasoning-capable in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (proprietary OpenAI training and instruction-tuning data presumed).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Instruction tuning and additional system-level tuning typical of OpenAI models; not described as using external theorem provers.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (component datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluated across MRC, NLI and TF datasets to probe instruction-following and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No (TF)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Notable NLI performance: HELP 63.69% (paper highlights o1 mini surpassing QwQ on HELP); overall average reported among top reasoning-enhanced models but below QwQ-32B and DeepSeek R1 on average.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms many traditional LLMs on select NLI tasks; lags behind QwQ-32B average but achieves best results on some NLI subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>o1 mini is competitive on fine-grained entailment tasks (e.g., HELP), suggesting its tuning yields strengths on monotonicity/negation-sensitive datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes sensitivity to data distribution and lack of consistent generalization across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6834.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large commercial instruction-tuned transformer model from OpenAI; strong zero-shot reasoning performance on many GLoRE tasks but shows distribution sensitivity and variability across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned transformer model (proprietary), widely used as a commercial baseline for reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not detailed in this paper; proprietary OpenAI training corpora and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Instruction tuning and in-context learning; evaluated with zero-shot and few-shot prompting; chain-of-thought referenced in related work but not used as a main intervention in GLoRE experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (component datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluation across MRC (exam-style), NLI (entailment/contradiction/neutral) and TF tasks (rule/proof datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No (TF)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average accuracy on GLoRE reported as 66.34%; dataset highlights: ReClor 87.20%, NaN-NLI 75.74%, FraCaS 75.35%, LogiQA 2.0 72.25%, but LogiQA22 58.49% (shows distribution sensitivity). Few-shot: 5-shot increases GPT-4 from 66.34% to 75.83% in the reported few-shot table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Surpasses RoBERTa-base and many open pre-trained models on numerous MRC and some NLI/TF tasks, but is outperformed by QwQ-32B and DeepSeek R1 on aggregate GLoRE score.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 excels at many zero-shot MRC tasks and benefits substantially from few-shot ICL, but exhibits significant sensitivity to distribution shifts (large drops on LogiQA22) and inconsistent NLI performance across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Inconsistent generalization across datasets; reliance on superficial statistical features rather than robust logical rules inferred from analysis; sensitivity to in-domain exposure and potential training data leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6834.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (Instruct GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A popular instruction-tuned conversational LLM (GPT-3.5 family) used as a baseline; shows moderate performance on GLoRE relative to larger/reasoning-enhanced models and benefits from few-shot ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational instruction-tuned transformer model (GPT-3.5 family); evaluated on zero-shot and few-shot GLoRE settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; instruction-tuned conversational model</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (proprietary OpenAI corpora and instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Instruction tuning and in-context learning; no external tools reported.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (component datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Tested across MRC, NLI, and TF tasks to evaluate conversational/instruction-following reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot (Table 3 reference): 52.10% average in the few-shot table; few-shot improvements to 55.85% (1-shot), 57.43% (2-shot), 60.32% (5-shot) on aggregate few-shot table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Better than many older pre-trained models in zero-shot but below GPT-4, QwQ-32B, and DeepSeek R1.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ChatGPT benefits from in-context examples but still trails the best reasoning-enhanced models on strict logical reasoning metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Shows limited generalization and remains sensitive to dataset distribution; improvements under few-shot largely attributed to statistical adaptation rather than provable reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6834.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-base (fine-tuned supervised baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 125M-parameter masked-language-transformer model fine-tuned supervisedly on GLoRE training sets to serve as a classical baseline for comparison against LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>125M-parameter transformer (masked language model) fine-tuned per-dataset as a supervised baseline for GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (masked LM fine-tuned for classification)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on each GLoRE dataset training sets (five epochs per dataset as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised fine-tuning on labeled in-domain examples (no RL or external tools).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used as a supervised baseline across MRC, NLI, and TF tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dataset examples reported: LogiQA 2.0 48.76% (4-way MRC), LogiQA22 33.22%; NaN-NLI 90.02% (matches human on that dataset), ProofWriter 55.92%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Serves as baseline; commonly outperformed by GPT-4 and reasoning-enhanced models on many MRC and TF tasks, though strong on certain NLI/test suites (e.g., NaN-NLI).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Supervised fine-tuning yields good performance on some rule-like synthetic datasets but lags on multi-choice exam-style MRC tasks; shows potential to exploit dataset-specific patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Overfits to dataset-specific features and does not generalize as well as reasoning-enhanced LLMs across the diverse dataset suite; performance varies widely by task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6834.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open pre-trained transformer LLM (30B parameters) evaluated zero-shot; shows low zero-shot accuracy on GLoRE MRC tasks and performs comparably to Falcon on average.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>30B-parameter transformer-based pre-trained LLM; in this paper used in its instruction/fine-tuned variants for zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large web corpora (not specified in detail here); instruction-tuned variant noted but exact fine-tuning not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Zero-shot/L2 instruct prompting and in-context learning evaluated; no specialized reasoning training reported.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluated across MRC, NLI, and TF tasks in zero-shot and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot aggregate around 32.34% (reported average in paper), performing poorly on 4-way MRC (~20% accuracy on MRC tasks in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Comparable to Falcon-40B in zero-shot despite smaller size; underperforms supervised RoBERTa-base on many tasks without in-context demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained LLaMA models without reasoning-specific tuning struggle on multi-step exam-style reasoning tasks in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Low zero-shot MRC performance (often near or below random baseline for 4-way tasks), sensitivity to data distribution and lack of reasoning-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6834.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40B-parameter open instruction-tuned transformer model evaluated zero-shot and few-shot on GLoRE; shows similar zero-shot performance to LLaMA-30B and poor MRC accuracy without demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>40B-parameter transformer-based LLM instruction-tuned for downstream use; evaluated as a representative modern open pre-trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not detailed in this paper (pretraining corpora proprietary/community-sourced).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Zero-shot and in-context few-shot prompting; no explicit reasoning-augmentation reported within this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same GLoRE suite of MRC, NLI, TF tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average ~32.28% (reported); poor MRC performance (~20% on some 4-way MRC tasks), improved modestly with few-shot ICL in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Similar performance to LLaMA-30B and generally below supervised RoBERTa-base on many tasks without demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction-tuned open models still require in-context examples or further tuning to succeed on strict logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Underperforms on multi-step MRC tasks without demonstration; sensitive to distributional shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6834.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-experts style model (Mixtral family) reported to outperform LLaMA and Falcon in zero-shot within this evaluation, indicating MoE benefits for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-experts transformer variant (8x7b configuration referenced) that shows improved zero-shot reasoning performance relative to comparable dense models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mixtral-8x7b (exact total parameter count not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer Mixture-of-Experts (MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Standard prompting (zero-shot/few-shot); improved capacity via MoE architecture rather than explicit reasoning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluated on GLoRE's MRC/NLI/TF tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported to outperform LLaMA and Falcon in zero-shot; exact per-dataset numbers not fully enumerated in the paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Better zero-shot than LLaMA-30B and Falcon-40B (paper notes Mixtral-8x7b outperforms them), suggesting MoE helps generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mixture-of-experts design can yield stronger zero-shot reasoning capability compared to same-scale dense models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not give detailed per-dataset numbers for Mixtral; overall sensitivity to distributional shifts remains an open issue.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6834.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning (few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique where a model receives example input-output pairs in the prompt (1/2/5-shot in this paper) to adapt behavior at inference time without weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>In-context learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method of improving performance by providing demonstration examples in the prompt; used here for 1-, 2-, and 5-shot experiments across models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>prompting / inference-time adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Few-shot examples sampled from each dataset (1, 2, or 5 examples appended to prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Example-based adaptation (no fine-tuning); tested across GPT-4, ChatGPT, LLaMA, Falcon, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used to assess how few-shot examples affect logical reasoning performance on MRC, NLI, TF tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (change vs. zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: GPT-4 zero-shot 66.34% -> 5-shot 75.83% (+9.49 percentage points); ChatGPT zero-shot 52.10% -> 5-shot 60.32% (+8.22 points); LLaMA zero-shot 32.34% -> 5-shot 39.62% (+7.28 points).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ICL yields consistent accuracy gains across models (several % to ~9% for GPT-4 at 5-shot), but gains are characterized as statistical adaptation rather than evidence of deeper causal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICL reliably improves empirical accuracy on GLoRE tasks across models, but the improvements often reflect adaptation to surface patterns and distributional features rather than robust logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>ICL improvements do not imply true reasoning  models may exploit superficial cues; sensitivity to chosen demonstrations and dataset distribution persists.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6834.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps (discussed in related work and analysis); paper references prior findings that CoT correlates with outputs but may not causally produce reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering technique to elicit step-by-step model rationales; cited in discussion as related work concerning reasoning behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>prompting (eliciting intermediate tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not used as an explicit experimental variable in this paper's main GLoRE runs; referenced via literature.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompted intermediate-step generation (chain-of-thought); discussed as correlated but not proven causal for correct reasoning in referenced literature.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (discussion relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>CoT is relevant to few-shot/ICL research but was not the main intervention tested in GLoRE experiments; cited to contextualize reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>General multi-step reasoning / logical chains (conceptual discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>N/A (paper cites others showing correlation between CoT and outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>N/A in this paper; authors note 'chain-of-thought prompts correlate with outputs but do not causally drive reasoning' referencing prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Mentioned as a technique that can change outputs but with debated causal effect on true reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT can influence model outputs, but GLoRE authors caution that associated performance gains may reflect pattern exploitation rather than principled logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not evaluate CoT directly on GLoRE; observed/quoted limitations include lack of causal evidence that CoT elicits human-like logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6834.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised per-dataset fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional approach: fine-tune pre-trained models (RoBERTa-base here) on labeled dataset examples to improve task performance; used as a classical baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Supervised fine-tuning (RoBERTa-base example)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard supervised learning: fine-tune a pre-trained transformer on in-domain labeled training sets for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer; supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Per-dataset training splits in GLoRE; RoBERTa-base was fine-tuned five epochs per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised learning to minimize classification loss on labeled examples; no explicit symbolic reasoning augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used to set a supervised baseline for logical reasoning tasks across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples: RoBERTa-base LogiQA 2.0 48.76% (4-way MRC), LogiQA22 33.22%, NaN-NLI 90.02%, ProofWriter 55.92%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Supervised fine-tuning improves performance on in-domain data relative to zero-shot base models but is often outperformed by reasoning-enhanced LLMs and exhibits limited cross-distribution generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuned supervised models can match or exceed LLMs on certain synthetic or patternizable datasets but generally lag on complex multi-step exam-style MRC and broader generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Strongly distribution-dependent; tends to capture dataset-specific cues instead of general logical principles; limited cross-dataset generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6834.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e6834.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement Learning (RL) for reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement-learning-based reasoning incentives</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training paradigm referenced as used by QwQ-32B and DeepSeek R1 to 'incentivize reasoning capability' and improve generalization on logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reinforcement learning (as applied to LLM training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Use of reinforcement learning objectives or reward models to bias model behavior toward outputs judged desirable for reasoning (paper attributes QwQ-32B and DeepSeek R1 improvements to RL frameworks).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>training-level modification (RL fine-tuning of transformer models)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in detail; presumably mixes of instruction data plus reasoning-specific tasks and reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Reinforcement learning / reward modeling used to steer model outputs toward logical-consistent answers and improved generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Applied in training of top-performing models (QwQ-32B, DeepSeek R1) that were subsequently evaluated on GLoRE datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, Yes-or-No</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Attributed performance gains: QwQ-32B (78.95% average) and DeepSeek R1 (75.14% average) are described as benefiting from RL-style training/incentives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Models using RL incentives outperform many instruction-tuned or supervised baselines (e.g., GPT-4 average 66.34%, RoBERTa baseline lower on many tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RL-based training appears effective for improving aggregate logical-reasoning performance and robustness on exam-like MRC and synthetic rule tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper lacks full transparency on RL procedure and training data; RL-trained models still show dataset-specific weaknesses (e.g., NLI monotonicity/negation tasks) and distribution sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding <em>(Rating: 2)</em></li>
                <li>Reclor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>RuleTaker: Reasoning over natural language rules with transformer models <em>(Rating: 2)</em></li>
                <li>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>How likely do llms with cot mimic human reasoning? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6834",
    "paper_id": "paper-264128006",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "GLoRE",
            "name_full": "General Logical Reasoning Evaluation",
            "brief_description": "A dynamic benchmark suite introduced in this paper to evaluate instruction-tuned LLMs on strict natural-language logical reasoning across three task families (Multi-choice Reading Comprehension, Natural Language Inference, Yes-or-No) using 12 datasets and ~72.8k instances.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLoRE (benchmark)",
            "model_description": "Unified evaluation platform consolidating diverse logical reasoning datasets and standardizing them for zero-shot and few-shot evaluation of language models.",
            "model_size": null,
            "architecture_type": "benchmark / dataset aggregation",
            "training_data": "12 datasets including LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter (total ~72,848 instances)",
            "reasoning_method": "N/A (benchmark evaluates models using their native reasoning methods; supports zero-shot and in-context few-shot evaluation)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (composed of LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter)",
            "benchmark_description": "Evaluates multi-step verbal logical reasoning via three task families: multi-choice reading comprehension (MRC), natural language inference (NLI), and true/false (yes/no) questions; includes high-quality, challenging datasets (both synthetic rule datasets and real exam-style questions).",
            "task_type": "MRC, NLI, Yes-or-No (True/False) reasoning",
            "performance_metric": "accuracy (classification accuracy)",
            "performance_value": "per-dataset accuracies reported; aggregated averages used for model comparisons (see individual model entries)",
            "comparison_with_baseline": "Designed to compare instruction-tuned LLMs, supervised baselines, and reasoning-enhanced models across consistent data splits; highlights distribution sensitivity and generalization gaps.",
            "key_findings": "Provides standardized measures showing (1) reasoning-enhanced models (QwQ-32B, DeepSeek R1, o1 mini) substantially outperform earlier LLMs and supervised baselines on many GLoRE tasks, (2) performance varies strongly by dataset indicating distributional sensitivity, and (3) in-context learning helps but often reflects statistical adaptation rather than robust logical inference.",
            "limitations": "Possible data-overlap/leakage risks across datasets; benchmark updates required to remain OOD for newer models; does not by itself identify causal reasoning mechanisms in models.",
            "uuid": "e6834.0",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B",
            "brief_description": "A state-of-the-art open-source, reasoning-enhanced LLM reported to use a reinforcement-learning based training framework and achieves top performance on the GLoRE benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "Transformer-based large language model described as 'reasoning-enhanced' with a reinforcement learning framework or specialized training methodology that improves logical inference generalization.",
            "model_size": "32B",
            "architecture_type": "transformer; reasoning-enhanced (training/algorithmic modifications reported; exact architectural innovations not fully specified in paper)",
            "training_data": "Not specified in detail in this paper; described as instruction-tuned and improved via a reinforcement-learning framework (paper references Qwq-32b: Embracing the power of reinforcement learning)",
            "reasoning_method": "Reinforcement-learning-driven instruction tuning / specialized training to incentivize logical reasoning; evaluated under zero-shot and few-shot ICL.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter)",
            "benchmark_description": "Comprehensive logical-reasoning benchmark covering MRC, NLI, and yes/no proof-style tasks (synthetic and natural).",
            "task_type": "Multi-choice reading comprehension (MRC), NLI, Yes-or-No proof/entailment",
            "performance_metric": "accuracy",
            "performance_value": "Average accuracy on GLoRE: 78.95% (state-of-the-art reported); selected per-dataset examples: ReClor 93.76%, AR-LSAT 92.35%, ProofWriter 82.40%, LogiQA22 86.30%, HELP 61.53%.",
            "comparison_with_baseline": "Outperforms GPT-4 (66.34% average) by +12.61 percentage points on average; outperforms DeepSeek R1 (75.14% average) by +3.81 points; sets new SOTA on multiple MRC tasks.",
            "key_findings": "QwQ-32B achieves the highest average performance on GLoRE, dominating MRC and TF tasks and demonstrating improved robustness on some OOD data (e.g., strong on LogiQA22), suggesting its training fosters stronger patterning for exam-like logical questions.",
            "limitations": "Uneven performance across NLI datasets (e.g., HELP 61.53% lags behind o1 mini), indicating weaker generalization to monotonicity/negation reasoning and fine-grained entailment; paper does not disclose full training corpus or exact RL procedure.",
            "uuid": "e6834.1",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DeepSeek R1",
            "name_full": "DeepSeek R1",
            "brief_description": "A reasoning-enhanced language model claiming improved logical inference via reinforcement-learning-based incentives, evaluated as a strong open-source competitor on GLoRE.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1",
            "model_description": "Transformer-based LLM described as 'reasoning-enhanced' through a reinforcement learning incentive framework to improve reasoning capabilities.",
            "model_size": null,
            "architecture_type": "transformer; reasoning-enhanced via reinforcement learning (training-level modification)",
            "training_data": "Not detailed in this paper; described conceptually as leveraging RL incentives for reasoning (referenced Deepseek-Ai paper).",
            "reasoning_method": "Reinforcement-learning-based training to incentivize reasoning behaviors; evaluated in zero-shot and few-shot ICL settings.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (component datasets as above)",
            "benchmark_description": "Evaluations include MRC, NLI, and yes/no tasks; both human exam questions and synthetic rule-based proofs.",
            "task_type": "MRC, NLI, Yes-or-No (TF)",
            "performance_metric": "accuracy",
            "performance_value": "Average accuracy on GLoRE: 75.14%; selected per-dataset examples (from paper table): overall balanced performance across tasks (detailed per-dataset values reported in paper table).",
            "comparison_with_baseline": "Substantially better than older LLMs (GPT-4 average 66.34%); within a few points of QwQ-32B (75.14% vs 78.95%).",
            "key_findings": "DeepSeek R1 narrows the gap to top performers and produces balanced performance across many GLoRE tasks, supporting the effectiveness of RL incentives for improving logical inference.",
            "limitations": "Still sensitive to data distribution; some datasets show lower NLI performance; precise RL mechanics and training data not disclosed here.",
            "uuid": "e6834.2",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "o1 mini",
            "name_full": "OpenAI o1 mini",
            "brief_description": "A commercial instruction-following LLM from OpenAI, evaluated as one of the reasoning-enhanced commercial models on GLoRE with competitive NLI performance on some datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o1 mini",
            "model_description": "An OpenAI instruction-tuned model in the 'o1' family (commercial); described as reasoning-capable in the paper's comparisons.",
            "model_size": null,
            "architecture_type": "transformer; instruction-tuned",
            "training_data": "Not specified in this paper (proprietary OpenAI training and instruction-tuning data presumed).",
            "reasoning_method": "Instruction tuning and additional system-level tuning typical of OpenAI models; not described as using external theorem provers.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (component datasets)",
            "benchmark_description": "Evaluated across MRC, NLI and TF datasets to probe instruction-following and reasoning.",
            "task_type": "MRC, NLI, Yes-or-No (TF)",
            "performance_metric": "accuracy",
            "performance_value": "Notable NLI performance: HELP 63.69% (paper highlights o1 mini surpassing QwQ on HELP); overall average reported among top reasoning-enhanced models but below QwQ-32B and DeepSeek R1 on average.",
            "comparison_with_baseline": "Outperforms many traditional LLMs on select NLI tasks; lags behind QwQ-32B average but achieves best results on some NLI subsets.",
            "key_findings": "o1 mini is competitive on fine-grained entailment tasks (e.g., HELP), suggesting its tuning yields strengths on monotonicity/negation-sensitive datasets.",
            "limitations": "Paper notes sensitivity to data distribution and lack of consistent generalization across datasets.",
            "uuid": "e6834.3",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large commercial instruction-tuned transformer model from OpenAI; strong zero-shot reasoning performance on many GLoRE tasks but shows distribution sensitivity and variability across datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large instruction-tuned transformer model (proprietary), widely used as a commercial baseline for reasoning capabilities.",
            "model_size": null,
            "architecture_type": "transformer; instruction-tuned",
            "training_data": "Not detailed in this paper; proprietary OpenAI training corpora and instruction tuning.",
            "reasoning_method": "Instruction tuning and in-context learning; evaluated with zero-shot and few-shot prompting; chain-of-thought referenced in related work but not used as a main intervention in GLoRE experiments.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (component datasets)",
            "benchmark_description": "Evaluation across MRC (exam-style), NLI (entailment/contradiction/neutral) and TF tasks (rule/proof datasets).",
            "task_type": "MRC, NLI, Yes-or-No (TF)",
            "performance_metric": "accuracy",
            "performance_value": "Average accuracy on GLoRE reported as 66.34%; dataset highlights: ReClor 87.20%, NaN-NLI 75.74%, FraCaS 75.35%, LogiQA 2.0 72.25%, but LogiQA22 58.49% (shows distribution sensitivity). Few-shot: 5-shot increases GPT-4 from 66.34% to 75.83% in the reported few-shot table.",
            "comparison_with_baseline": "Surpasses RoBERTa-base and many open pre-trained models on numerous MRC and some NLI/TF tasks, but is outperformed by QwQ-32B and DeepSeek R1 on aggregate GLoRE score.",
            "key_findings": "GPT-4 excels at many zero-shot MRC tasks and benefits substantially from few-shot ICL, but exhibits significant sensitivity to distribution shifts (large drops on LogiQA22) and inconsistent NLI performance across datasets.",
            "limitations": "Inconsistent generalization across datasets; reliance on superficial statistical features rather than robust logical rules inferred from analysis; sensitivity to in-domain exposure and potential training data leakage.",
            "uuid": "e6834.4",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (Instruct GPT-3.5 family)",
            "brief_description": "A popular instruction-tuned conversational LLM (GPT-3.5 family) used as a baseline; shows moderate performance on GLoRE relative to larger/reasoning-enhanced models and benefits from few-shot ICL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Conversational instruction-tuned transformer model (GPT-3.5 family); evaluated on zero-shot and few-shot GLoRE settings.",
            "model_size": null,
            "architecture_type": "transformer; instruction-tuned conversational model",
            "training_data": "Not specified in this paper (proprietary OpenAI corpora and instruction tuning).",
            "reasoning_method": "Instruction tuning and in-context learning; no external tools reported.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (component datasets)",
            "benchmark_description": "Tested across MRC, NLI, and TF tasks to evaluate conversational/instruction-following reasoning capability.",
            "task_type": "MRC, NLI, Yes-or-No",
            "performance_metric": "accuracy",
            "performance_value": "Zero-shot (Table 3 reference): 52.10% average in the few-shot table; few-shot improvements to 55.85% (1-shot), 57.43% (2-shot), 60.32% (5-shot) on aggregate few-shot table.",
            "comparison_with_baseline": "Better than many older pre-trained models in zero-shot but below GPT-4, QwQ-32B, and DeepSeek R1.",
            "key_findings": "ChatGPT benefits from in-context examples but still trails the best reasoning-enhanced models on strict logical reasoning metrics.",
            "limitations": "Shows limited generalization and remains sensitive to dataset distribution; improvements under few-shot largely attributed to statistical adaptation rather than provable reasoning.",
            "uuid": "e6834.5",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RoBERTa-base",
            "name_full": "RoBERTa-base (fine-tuned supervised baseline)",
            "brief_description": "A 125M-parameter masked-language-transformer model fine-tuned supervisedly on GLoRE training sets to serve as a classical baseline for comparison against LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "125M-parameter transformer (masked language model) fine-tuned per-dataset as a supervised baseline for GLoRE.",
            "model_size": "125M",
            "architecture_type": "transformer (masked LM fine-tuned for classification)",
            "training_data": "Fine-tuned on each GLoRE dataset training sets (five epochs per dataset as reported).",
            "reasoning_method": "Supervised fine-tuning on labeled in-domain examples (no RL or external tools).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Used as a supervised baseline across MRC, NLI, and TF tasks.",
            "task_type": "MRC, NLI, Yes-or-No",
            "performance_metric": "accuracy",
            "performance_value": "Dataset examples reported: LogiQA 2.0 48.76% (4-way MRC), LogiQA22 33.22%; NaN-NLI 90.02% (matches human on that dataset), ProofWriter 55.92%.",
            "comparison_with_baseline": "Serves as baseline; commonly outperformed by GPT-4 and reasoning-enhanced models on many MRC and TF tasks, though strong on certain NLI/test suites (e.g., NaN-NLI).",
            "key_findings": "Supervised fine-tuning yields good performance on some rule-like synthetic datasets but lags on multi-choice exam-style MRC tasks; shows potential to exploit dataset-specific patterns.",
            "limitations": "Overfits to dataset-specific features and does not generalize as well as reasoning-enhanced LLMs across the diverse dataset suite; performance varies widely by task.",
            "uuid": "e6834.6",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA-30B",
            "brief_description": "An open pre-trained transformer LLM (30B parameters) evaluated zero-shot; shows low zero-shot accuracy on GLoRE MRC tasks and performs comparably to Falcon on average.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B",
            "model_description": "30B-parameter transformer-based pre-trained LLM; in this paper used in its instruction/fine-tuned variants for zero-shot evaluation.",
            "model_size": "30B",
            "architecture_type": "transformer",
            "training_data": "Pretrained on large web corpora (not specified in detail here); instruction-tuned variant noted but exact fine-tuning not detailed.",
            "reasoning_method": "Zero-shot/L2 instruct prompting and in-context learning evaluated; no specialized reasoning training reported.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Evaluated across MRC, NLI, and TF tasks in zero-shot and few-shot settings.",
            "task_type": "MRC, NLI, Yes-or-No",
            "performance_metric": "accuracy",
            "performance_value": "Zero-shot aggregate around 32.34% (reported average in paper), performing poorly on 4-way MRC (~20% accuracy on MRC tasks in some cases).",
            "comparison_with_baseline": "Comparable to Falcon-40B in zero-shot despite smaller size; underperforms supervised RoBERTa-base on many tasks without in-context demonstrations.",
            "key_findings": "Pretrained LLaMA models without reasoning-specific tuning struggle on multi-step exam-style reasoning tasks in zero-shot.",
            "limitations": "Low zero-shot MRC performance (often near or below random baseline for 4-way tasks), sensitivity to data distribution and lack of reasoning-specific training.",
            "uuid": "e6834.7",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-40B",
            "name_full": "Falcon-40B-instruct",
            "brief_description": "A 40B-parameter open instruction-tuned transformer model evaluated zero-shot and few-shot on GLoRE; shows similar zero-shot performance to LLaMA-30B and poor MRC accuracy without demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-40B-instruct",
            "model_description": "40B-parameter transformer-based LLM instruction-tuned for downstream use; evaluated as a representative modern open pre-trained model.",
            "model_size": "40B",
            "architecture_type": "transformer; instruction-tuned",
            "training_data": "Not detailed in this paper (pretraining corpora proprietary/community-sourced).",
            "reasoning_method": "Zero-shot and in-context few-shot prompting; no explicit reasoning-augmentation reported within this evaluation.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Same GLoRE suite of MRC, NLI, TF tasks.",
            "task_type": "MRC, NLI, Yes-or-No",
            "performance_metric": "accuracy",
            "performance_value": "Zero-shot average ~32.28% (reported); poor MRC performance (~20% on some 4-way MRC tasks), improved modestly with few-shot ICL in Table 3.",
            "comparison_with_baseline": "Similar performance to LLaMA-30B and generally below supervised RoBERTa-base on many tasks without demonstrations.",
            "key_findings": "Instruction-tuned open models still require in-context examples or further tuning to succeed on strict logical reasoning tasks.",
            "limitations": "Underperforms on multi-step MRC tasks without demonstration; sensitive to distributional shifts.",
            "uuid": "e6834.8",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Mixtral-8x7b",
            "name_full": "Mixtral-8x7b",
            "brief_description": "A mixture-of-experts style model (Mixtral family) reported to outperform LLaMA and Falcon in zero-shot within this evaluation, indicating MoE benefits for reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7b",
            "model_description": "Mixture-of-experts transformer variant (8x7b configuration referenced) that shows improved zero-shot reasoning performance relative to comparable dense models.",
            "model_size": "Mixtral-8x7b (exact total parameter count not specified in paper)",
            "architecture_type": "transformer Mixture-of-Experts (MoE)",
            "training_data": "Not specified in this paper.",
            "reasoning_method": "Standard prompting (zero-shot/few-shot); improved capacity via MoE architecture rather than explicit reasoning algorithms.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Evaluated on GLoRE's MRC/NLI/TF tasks.",
            "task_type": "MRC, NLI, Yes-or-No",
            "performance_metric": "accuracy",
            "performance_value": "Reported to outperform LLaMA and Falcon in zero-shot; exact per-dataset numbers not fully enumerated in the paper's text.",
            "comparison_with_baseline": "Better zero-shot than LLaMA-30B and Falcon-40B (paper notes Mixtral-8x7b outperforms them), suggesting MoE helps generalization.",
            "key_findings": "Mixture-of-experts design can yield stronger zero-shot reasoning capability compared to same-scale dense models.",
            "limitations": "Paper does not give detailed per-dataset numbers for Mixtral; overall sensitivity to distributional shifts remains an open issue.",
            "uuid": "e6834.9",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "In-context learning (ICL)",
            "name_full": "In-context learning (few-shot prompting)",
            "brief_description": "Prompting technique where a model receives example input-output pairs in the prompt (1/2/5-shot in this paper) to adapt behavior at inference time without weight updates.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "In-context learning (ICL)",
            "model_description": "Method of improving performance by providing demonstration examples in the prompt; used here for 1-, 2-, and 5-shot experiments across models.",
            "model_size": null,
            "architecture_type": "prompting / inference-time adaptation",
            "training_data": "Few-shot examples sampled from each dataset (1, 2, or 5 examples appended to prompt).",
            "reasoning_method": "Example-based adaptation (no fine-tuning); tested across GPT-4, ChatGPT, LLaMA, Falcon, etc.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Used to assess how few-shot examples affect logical reasoning performance on MRC, NLI, TF tasks.",
            "task_type": "MRC, NLI, Yes-or-No",
            "performance_metric": "accuracy (change vs. zero-shot)",
            "performance_value": "Example: GPT-4 zero-shot 66.34% -&gt; 5-shot 75.83% (+9.49 percentage points); ChatGPT zero-shot 52.10% -&gt; 5-shot 60.32% (+8.22 points); LLaMA zero-shot 32.34% -&gt; 5-shot 39.62% (+7.28 points).",
            "comparison_with_baseline": "ICL yields consistent accuracy gains across models (several % to ~9% for GPT-4 at 5-shot), but gains are characterized as statistical adaptation rather than evidence of deeper causal reasoning.",
            "key_findings": "ICL reliably improves empirical accuracy on GLoRE tasks across models, but the improvements often reflect adaptation to surface patterns and distributional features rather than robust logical inference.",
            "limitations": "ICL improvements do not imply true reasoning  models may exploit superficial cues; sensitivity to chosen demonstrations and dataset distribution persists.",
            "uuid": "e6834.10",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompts",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps (discussed in related work and analysis); paper references prior findings that CoT correlates with outputs but may not causally produce reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Thought prompting",
            "model_description": "Prompt engineering technique to elicit step-by-step model rationales; cited in discussion as related work concerning reasoning behaviors.",
            "model_size": null,
            "architecture_type": "prompting (eliciting intermediate tokens)",
            "training_data": "Not used as an explicit experimental variable in this paper's main GLoRE runs; referenced via literature.",
            "reasoning_method": "Prompted intermediate-step generation (chain-of-thought); discussed as correlated but not proven causal for correct reasoning in referenced literature.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (discussion relevance)",
            "benchmark_description": "CoT is relevant to few-shot/ICL research but was not the main intervention tested in GLoRE experiments; cited to contextualize reasoning methods.",
            "task_type": "General multi-step reasoning / logical chains (conceptual discussion)",
            "performance_metric": "N/A (paper cites others showing correlation between CoT and outputs)",
            "performance_value": "N/A in this paper; authors note 'chain-of-thought prompts correlate with outputs but do not causally drive reasoning' referencing prior work.",
            "comparison_with_baseline": "Mentioned as a technique that can change outputs but with debated causal effect on true reasoning.",
            "key_findings": "CoT can influence model outputs, but GLoRE authors caution that associated performance gains may reflect pattern exploitation rather than principled logical inference.",
            "limitations": "Paper does not evaluate CoT directly on GLoRE; observed/quoted limitations include lack of causal evidence that CoT elicits human-like logical reasoning.",
            "uuid": "e6834.11",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Supervised fine-tuning",
            "name_full": "Supervised per-dataset fine-tuning",
            "brief_description": "Traditional approach: fine-tune pre-trained models (RoBERTa-base here) on labeled dataset examples to improve task performance; used as a classical baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Supervised fine-tuning (RoBERTa-base example)",
            "model_description": "Standard supervised learning: fine-tune a pre-trained transformer on in-domain labeled training sets for classification.",
            "model_size": null,
            "architecture_type": "transformer; supervised fine-tuning",
            "training_data": "Per-dataset training splits in GLoRE; RoBERTa-base was fine-tuned five epochs per dataset.",
            "reasoning_method": "Supervised learning to minimize classification loss on labeled examples; no explicit symbolic reasoning augmentation.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Used to set a supervised baseline for logical reasoning tasks across datasets.",
            "task_type": "MRC, NLI, Yes-or-No",
            "performance_metric": "accuracy",
            "performance_value": "Examples: RoBERTa-base LogiQA 2.0 48.76% (4-way MRC), LogiQA22 33.22%, NaN-NLI 90.02%, ProofWriter 55.92%.",
            "comparison_with_baseline": "Supervised fine-tuning improves performance on in-domain data relative to zero-shot base models but is often outperformed by reasoning-enhanced LLMs and exhibits limited cross-distribution generalization.",
            "key_findings": "Fine-tuned supervised models can match or exceed LLMs on certain synthetic or patternizable datasets but generally lag on complex multi-step exam-style MRC and broader generalization.",
            "limitations": "Strongly distribution-dependent; tends to capture dataset-specific cues instead of general logical principles; limited cross-dataset generalization.",
            "uuid": "e6834.12",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reinforcement Learning (RL) for reasoning",
            "name_full": "Reinforcement-learning-based reasoning incentives",
            "brief_description": "Training paradigm referenced as used by QwQ-32B and DeepSeek R1 to 'incentivize reasoning capability' and improve generalization on logical tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Reinforcement learning (as applied to LLM training)",
            "model_description": "Use of reinforcement learning objectives or reward models to bias model behavior toward outputs judged desirable for reasoning (paper attributes QwQ-32B and DeepSeek R1 improvements to RL frameworks).",
            "model_size": null,
            "architecture_type": "training-level modification (RL fine-tuning of transformer models)",
            "training_data": "Not specified in detail; presumably mixes of instruction data plus reasoning-specific tasks and reward signals.",
            "reasoning_method": "Reinforcement learning / reward modeling used to steer model outputs toward logical-consistent answers and improved generalization.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Applied in training of top-performing models (QwQ-32B, DeepSeek R1) that were subsequently evaluated on GLoRE datasets.",
            "task_type": "MRC, NLI, Yes-or-No",
            "performance_metric": "accuracy",
            "performance_value": "Attributed performance gains: QwQ-32B (78.95% average) and DeepSeek R1 (75.14% average) are described as benefiting from RL-style training/incentives.",
            "comparison_with_baseline": "Models using RL incentives outperform many instruction-tuned or supervised baselines (e.g., GPT-4 average 66.34%, RoBERTa baseline lower on many tasks).",
            "key_findings": "RL-based training appears effective for improving aggregate logical-reasoning performance and robustness on exam-like MRC and synthetic rule tasks.",
            "limitations": "Paper lacks full transparency on RL procedure and training data; RL-trained models still show dataset-specific weaknesses (e.g., NLI monotonicity/negation tasks) and distribution sensitivity.",
            "uuid": "e6834.13",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding",
            "rating": 2,
            "sanitized_title": "logiqa_20an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "Reclor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "RuleTaker: Reasoning over natural language rules with transformer models",
            "rating": 2,
            "sanitized_title": "ruletaker_reasoning_over_natural_language_rules_with_transformer_models"
        },
        {
            "paper_title": "Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekai_deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "How likely do llms with cot mimic human reasoning?",
            "rating": 1,
            "sanitized_title": "how_likely_do_llms_with_cot_mimic_human_reasoning"
        }
    ],
    "cost": 0.022213749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GLoRE: Evaluating Logical Reasoning of Large Language Models
20 Apr 2025</p>
<p>Hanmeng Liu 
Hainan University
HaikouHainan</p>
<p>Zhiyang Teng 
ByteDance SG</p>
<p>Ruoxi Ning 
Westlake University
HangzhouZhejiangChina</p>
<p>Yiran Ding 
Westlake University
HangzhouZhejiangChina</p>
<p>Xiulai Li 
Hainan University
HaikouHainan</p>
<p>Xiaozhang Liu 
Hainan University
HaikouHainan</p>
<p>Yue Zhang 
Westlake University
HangzhouZhejiangChina</p>
<p>GLoRE: Evaluating Logical Reasoning of Large Language Models
20 Apr 20252E53C0DD83F28EA3B1EBC449B1AC2C90arXiv:2310.09107v2[cs.CL]Large Language ModelLarge Reasoning ModelLogical reasoningNatural Language Inference
Large language models (LLMs) have shown significant general language understanding abilities.However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding.To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios.Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date.GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.It garnered over 300 citations upon its release.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) [50,67], especially reasoning language models [18,51] demonstrate advanced capabilities in complex reasoning tasks and show significant adaptability and versatility across various applications, from simple everyday tasks to specialized domains such as coding, mathematics, law, medicine, and finance [11,22,34,37,76].Quantitative evaluation of LLM reasoning has thus become a very important task.To this end, existing work has considered mathematical reasoning [15,26], algorithmic problem solving [9,58], and knowledge-driven reasoning [25,73].</p>
<p>Logical reasoning is a cornerstone of human intelligence and has been a central focus in artificial intelligence research since its inception [16,29,33].However, evaluating verbal reasoning turned out to be too difficult in the 1950s due to insufficient natural language understanding (NLU) technologies, and thus AI researchers focused on formal logical reasoning instead [29,48,49].Since the 2010s, NLU has witnessed huge advances, where reading comprehension [8,21] and natural language inference [7,74] tasks were solved with high accuracies, which made verbal reasoning evaluation feasible [43,80].Figure 1 illustrates an example of logical reasoning in reading comprehension.To address such questions, LLMs must engage in multi-step, algorithmic, symbolic reasoning.This makes logical reasoning an ideal testbed for evaluating LLMs' ability to process complex natural language information accurately, robustly, and logically.</p>
<p>To this end, we introduce the General Logical Reasoning Evaluation (GLoRE) benchmark, designed to assess instruction-tuned LLMs on various logical reasoning tasks.GLoRE evaluates the strengths and limitations of LLMs in this domain, similar to how GLUE [71] and Super-GLUE [70] benchmark natural language understanding.GLoRE includes three types of logical reasoning tasks: Multi-choice Reading Comprehension [35], Natural Language Inference (NLI) [17], and True-or-False (Yes-or-No) Questions [13].These tasks encompass a wide range of logical reasoning phenomena, with high-quality datasets that remain challenging for pre-trained language models [13,27,32].In total, GLoRE covers 12 datasets with 72,848 instances.Since its release in 2023, GLoRE has been used for evaluating language models, receiving over 300 citations on ArXiv.</p>
<p>Using GLoRE, we report the logical reasoning capabilities of commercial models such as GPT-4 and OpenAI o1 [51], as well as popular open-source models such as LLaMA [67], Falcon [1], Mistral [30], DeepSeek R1 [18], and QwQ-32B [66].We test their instruction-following and problem-solving abilities in logical reasoning tasks.Results show that while commercial LLMs like GPT-4 still excel in zero-shot settings and approach human performance on specific datasets like ReClor, open-source models like QwQ-32B now rival or even surpass commercial counterparts in key tasks, achieving state-of-the-art results on multiple benchmarks.This underscores rapid advancements in open-source LLMs, narrowing the performance gap with commercial models.However, performance varies significantly across datasets, indicating sensitivity to data distribution.This sensitivity is further confirmed by observations that in-context learning and supervised fine-tuning primarily enhance LLM performance on specific test distributions, demonstrating their strong learning ability.While LLMs show promise in logical reasoning, their robustness to data distribution variations remains a challenge, highlighting the need for further improvement.</p>
<p>Related Work</p>
<p>Logical reasoning with natural language.Tapping into logical reasoning capabilities represents a holistic endeavour in natural language understanding (NLU).A variety of methods have been explored to realize this objective, including symbolic systems [45,47,55], fine-tuning of language models [28,41,71,78], and hybrid approaches combining neural and symbolic elements [36,59,60].</p>
<p>The recent introduction of evaluation datasets, notably LogiQA [43] and Reclor [80], has reinvigorated the focus on logical reasoning in NLP research.Logical reasoning is now leveraged in numerous probing tasks over large Pretrained Language Models (PLMs) and applied to downstream tasks such as question-answering and dialogue systems [6,63].Despite these advancements, the aspiration to emulate human-like logical reasoning capabilities within NLU systems remains a significant challenge for traditional models [27,43].In this study, our goal is not only to quantitatively evaluate the capability of Large Language Models (LLMs) in addressing the previously mentioned challenge but also to underscore the significance of our work in providing a validated platform for enhancing various reasoning methods with our data.</p>
<p>LLM reasoning evaluation.Despite progress in evaluating LLMs for specific reasoning tasks like arithmetic [57] and commonsense [4], a yawning gap exists in comprehensively assessing their logical reasoning.While LLMs excel at specific tasks like arithmetic reasoning [57], they face challenges in complex areas like multi-step reasoning [23] and abstract scenarios [24].ChatGPT exhibits strengths in chat-specific reasoning and some commonsense domains [4,53], but struggles with tasks requiring longer chains of inference [4].Other LLMs like FLAN-T5 [12], LLaMA [67], and PaLM [2] show potential in general deductive reasoning [61], while InstructGPT and Codex excel in specialized domains like medical reasoning [38].Despite these advances, limitations in data bias [52], and complex reasoning tasks necessitate further research and optimization to fully unlock the reasoning potential of LLMs [77].</p>
<p>Big-Bench Hard (BBH) [64] isolates 23 most challenging tasks from BIG-Bench [3].These tasks comprise general language understanding, arithmetic and algorithmic reasoning, and logical deduction.However, in comparison to our benchmark, the data size of the logical reasoning section in BBH is very small.HumanEval [10] serves as a hand-written evaluation set for coding.The programming problems included are designed to assess language comprehension, reasoning, algorithms, and simple mathematics.While similar to logical reasoning in that code generation necessitates complex reasoning skills, GLoRE differs in presenting logical reasoning problems via natural language prompts.ARB [62] is a benchmark for advanced reasoning over multiple fields like mathematics, physics, biology, chemistry, and law.Similar to GLoRE, it introduces a challenging subset of math and physics problems that require advanced symbolic reasoning.However, the benchmark constraints its problem on the above subjects with domain knowledge, not general logical reasoning questions, which is the focus of GLoRE.</p>
<p>The GLoRE Dataset</p>
<p>As mentioned in the introduction, GLoRE contains three NLU tasks: Multichoice Reading Comprehension, NLI, and Yes-or-No.First, Multi-choice reading comprehension [35] is essential in verbal reasoning tests, which cover abundant high-quality logical reasoning problems in the wild.Second, Unlike multi-choice reading comprehension, NLI [17] is more general and centric on entailment relations in a simpler task format, which is a fundamental task for evaluating reasoning abilities [19,54].Third, the Yes-or-No reasoning task [13] is a combination of question-answering and textual entailment, which can serve as a playground for testing models' reasoning abilities [14,65].The data statistics are shown in Table 1.</p>
<p>Multi-choice Reading Comprehension (MRC)</p>
<p>Within the standard multi-choice reading comprehension (MRC) task setting, a system is presented with a passage and a question, and the objective is to choose the most suitable answer from a set of candidate responses.Particularly, GLoRE contains five such datasets: LogiQA [43] is a logical MRC dataset derived from the Chinese Civil Service Examination, translated into English, and made available in both Chinese and English versions.We adopt LogiQA 2.0 [40] and use both the English (LogiQA 2.0) and Chinese (LogiQA 2.0 zh) test sets for our evaluation.</p>
<p>ReClor [80] comprises question-answering examples from the LSAT exams designed to assess human logical reasoning abilities.We use the development set for our testing as the test set does not provide gold labels.AR-LSAT [72] is a dataset of analytical reasoning questions from the Law School Admission Test.Each question contains five options rather than four.</p>
<p>LogiQA22 is collected and processed according to the LogiQA 2.0 format after ChatGPT was released.It incorporates the newly released Chinese Civil Servant Exams from 2022, which are not included in the original LogiQA dataset.</p>
<p>Natural Language Inference (NLI)</p>
<p>NLI is the task of determining the logical relationship between a hypothesis and a premise.The typical scheme involves text classification, where the model selects one of three labels: entailment, contradiction, and neutral.ConTRoL [39] is an NLI dataset that offers an in-depth examination of contextual reasoning within the NLI framework.Approximately 36.2% of premisehypothesis pairs fall under the category of logical reasoning in this dataset.We choose the logical reasoning portion for our evaluation.HELP [79] is an NLI dataset emphasizing monotonicity reasoning, a crucial concept in Natural Logic [46].We use the training set for our evaluation.TaxiNLI [31] is an NLI dataset that has been re-annotated based on MNLI [75], with categories include logical categories such as connectives, mathematical reasoning, and deduction.NaN-NLI [68] is a test suite designed to probe the capabilities of NLP models in capturing sub-clausal negation.The successful handling of sub-clausal negation can be seen as a strong indicator of a model's logical reasoning capacity.</p>
<p>True-or-False (Yes-or-No) Questions (TF)</p>
<p>FraCaS test suite [56] presents complex entailment problems involving multipremised contexts as a three-way classification task.The ability to determine entailment relationships in this context is closely tied to logical reasoning.RuleTaker [14] dataset is a synthetic creation designed to examine the reasoning ability of transformer models [69] over natural language rules.This task explicitly targets logical reasoning by asking models to reason over a set of rules and facts to generate true-or-false responses as output.ProofWriter [65] dataset generates sets of facts and rules, each followed by questions, which can be proven true or false using proofs of various depths.</p>
<p>Experiments</p>
<p>We employ GLoRE to assess the logical reasoning capabilities across different categories of language models, including traditional pre-trained models and reasoning-enhanced LLMs, both open-source and proprietary.We conduct a comprehensive comparative analysis of their performance against human benchmarks.</p>
<p>Experimental Settings</p>
<p>We adopted RoBERTa-base [44] as a baseline, fine-tuning it on the training set over five epochs for each dataset.The community models selected for comparison include Falcon-40b-instruct [1], LLaMA-30b-supercot [67] Mixtral-8x7b, DeepSeek R1 [18] and QwQ-32B [66].For OpenAI models, we choose ChatGPT, GPT-4 and o1 mini [51].</p>
<p>Our evaluation metrics consisted of classification accuracy scores.Additionally, we utilized reported accuracies for datasets where human performance data was available and recorded both the average and peak performance of human participants to establish a human baseline.For the LogiQA22 dataset, we engaged five co-authors as test subjects and computed their accuracy based on 150 test examples.</p>
<p>Main Results</p>
<p>Zero-shot results.Table 2 summarizes the zero-shot evaluation results.The first block shows human performance.The second block presents RoBERTabase's supervised fine-tuning results.With 125M parameters, RoBERTa-base achieves 48.76% and 33.22% accuracy on LogiQA 2.0 and LogiQA22, respectively, lagging behind human performance.It performs better on NLI and TF tasks than MRC tasks, likely due to output ambiguities.On NaN-NLI, RoBERTa achieves 90.02% accuracy, matching human performance, possibly due to learning superficial patterns from rule-based negation data.On ProofWriter, RoBERTabase scores 55.92%, indicating potential for specific logical reasoning tasks.</p>
<p>The third block shows zero-shot results for LLaMA, Falcon, and Mixtral.LLaMA and Falcon perform similarly (32.34% vs. 32.28%),suggesting comparable reasoning capabilities despite LLaMA-30B's smaller size.Both underperform RoBERTa-base on most tasks, except Falcon on RT.On MRC tasks, their accuracy is around 20%, worse than random guessing in 4-way classification, indicating challenges in logical reasoning without in-context demonstrations.</p>
<p>Performance gaps between LogiQA and LogiQA22 are smaller for these models, suggesting stable performance across data distributions without in-domain tuning.Mixtral-8x7b outperforms LLaMA and Falcon, demonstrating the effectiveness of mixture-of-expert models.</p>
<p>The fourth block provides zero-shot results ChatGPT and GPT-4.Both models, especially GPT-4, surpass RoBERTa-base on several MRC benchmarks.However, GPT-4's accuracy drops significantly on LogiQA22 (58.49% vs. 72.25% on LogiQA 2.0), indicating sensitivity to data distribution.In NLI and TF tasks, ChatGPT and GPT-4 outperform RoBERTa, with ChatGPT achieving 58.45% accuracy on ConTRoL, surpassing GPT-4.GPT-4's NLI performance varies across datasets, further highlighting its sensitivity to data distribution.TF task results show similar inconsistencies, suggesting model rationales differ from human reasoning.</p>
<p>The final block shows results for o1 mini, DeepSeek R1, and QwQ-32B, which achieve notable improvements over prior models.QwQ-32B attains the highest average accuracy (78.95%), surpassing GPT-4 (66.34%) and DeepSeek R1 (75.14%).It achieves state-of-the-art results on MRC tasks like ReClor (93.76%) and AR-LSAT (92.35%), indicating the need for more challenging benchmarks for logical reasoning.Its robustness is evident in LogiQA22 (86.30%), outperforming GPT-4 by 27.81 percentage points.However, QwQ-32B shows uneven performance on NLI datasets, such as HELP (61.53%, lagging behind o1 mini's 63.69%), suggesting its reasoning capabilities are less generalizable in tasks requiring fine-grained entailment analysis.</p>
<p>While GPT-4 retains an advantage on FraCas (75.35%),QwQ-32B surpasses GPT-4 on ReClor (93.76% vs. 87.20%),redefining state-of-the-art performance for MRC tasks.QwQ-32B and DeepSeek R1 showcase balanced performance across most tasks, with QwQ-32B achieving unprecedented TF results (e.g., 82.40% on ProofWriter, outperforming both GPT-4's 59.66% and DeepSeek R1's 80.51%).Though still below the human average overall, these models mark substantial progress -QwQ-32B's 78.95% average accuracy (vs.DeepSeek R1's 75.14% and GPT-4's 66.34%) highlights significant architectural or training innovations for logical inference.</p>
<p>Few-shot results.LLMs excel at in-context learning [20], where performance improves with context examples and demonstration methods [42].For this study, we randomly sampled instances (1 for 1-shot, 2 for 2-shot, and 5 for 5-shot) from each dataset and appended them to the prompt.We used the same model configuration as in the zero-shot scenario.Table 3 highlights the impact of in-context learning (ICL), as seen in GPT-4's 9% accuracy gain with 5-shot learning.However, this improvement stems from statistical adaptation rather than true reasoning, as models rely on superficial patterns rather than humanlike logical inference.This aligns with findings that chain-of-thought prompts correlate with outputs but do not causally drive reasoning [5].While reasoningenhanced models narrow the gap with human performance, their sensitivity to data distribution highlights the need for further research into more robust reasoning mechanisms.GLoRE's evolving framework will continue to track these advancements.</p>
<p>Analysis</p>
<p>Large language models vs. reasoning-enhanced models.The reasoningenhanced models like QwQ-32B, DeepSeek R1, and OpenAI's o1 mini demonstrate significant improvements over traditional LLMs.QwQ-32B, in particular, achieves the highest average performance (78.95%), indicating that its reinforcement learning framework or specialized training methodology enables better generalization across data distributions.While QwQ-32B dominates MRC and TF tasks, its relatively lower performance on NLI datasets like HELP (61.53%) suggests that even state-of-the-art models struggle with tasks requiring monotonicity or negation reasoning, highlighting the need for broader evaluation beyond task-specific robustness.Data leakage concerns.While GLoRE includes diverse datasets, potential data leakage risks arise from overlapping sources.GPT-4's lower accuracy on LogiQA22 (58.49%) compared to LogiQA 2.0 (72.25%) suggests limited exposure to newer data, reducing leakage concerns but highlighting distributional sensitivity.The benchmark's dynamic updates and inclusion of newly annotated datasets help mitigate leakage by testing models on unseen distributions.Sensitivity to data distribution.The above experiments show that the performance of LLMs is sensitive to the data distribution.Even though the underlying reasoning principles are the same, LLM performance varies significantly across datasets.This suggests that LLMs might not reason using the correct rationale but rely on superficial features.As shown in Table 2, although GPT-4 achieves near-human performance on datasets like ReClor (87.20%) and NaN-NLI (75.74%), it lags significantly on others (e.g., HELP at 46.01%).This inconsistency mirrors the behavior of reasoning-enhanced models like DeepSeek R1, revealing a critical divergence from human reasoning: once humans master a reasoning pattern, their performance generalizes robustly, whereas LLMs remain sensitive to data-specific features.</p>
<p>Conclusion</p>
<p>We constructed GLoRE, a dynamic and comprehensive benchmark tailored for assessing the logical reasoning capabilities of advanced language models, including GPT-4 and various strong open-source LLMs across multiple reasoning tasks.</p>
<p>Our findings indicate that QwQ-32B, a reasoning-enhanced model, sets a new state-of-the-art on the GLoRE benchmark, significantly narrowing the gap to human performance.This underscores the potential of targeted architectural and training innovations for logical reasoning.GLoRE will be continually maintained to track advancements in this rapidly evolving domain.</p>
<p>Fig. 1 .
1
Fig. 1.Instruction and question format for logical reading comprehension tasks.</p>
<p>Table 1 .
1
Data statistics.("E": entailment; "C": contradiction; "N": neutral.)
DatasetSizeTargetDatasetSizeTargetLogiQA 2.0 test1,572 4-way multi-choice ConTRoL805E, C, NLogiQA 2.0 zh test 1,594 4-way multi-choice HELP35,891E, C, NReClor dev500 4-way multi-choice TaxiNLI test10,071E, C, NAR-LSAT test230 5-way multi-choice NaN-NLI259E, C, NLogiQA221,354 4-way multi-choice FraCas346 Yes, No, NeutralRuleTaker dev 10,068Yes, NoProofWriter dev 10,158Yes, No</p>
<p>Table 3 .
3
Average accuracies on GLoRE few-shot evaluation.
Model0-shot 1-shot 2-shot 5-shotLLaMA32.34 32.89 35.03 39.62Falcon32.28 33.14 33.76 35.72ChatGPT 52.10 55.85 57.43 60.32GPT-466.34 70.31 71.44 75.83</p>
<p>. Mrc Nli Task, Avg Tf, Lq Lq Zh Rc Al Lq22 Ct Hl Tn Nn Fc Rt Dataset, Pw Human, Avg, 86.00 88.00 63.00 56.00 83.00 87.00 81.00 97.00 94.00 92.00 84.00 82.00 82.75</p>
<p>Human Ceiling. 95.00 96.00 100.00 91.00 99.00 94.00 95.00 100.00 100.00 97.00 95.00 93.00 96.25</p>
<p>. Deepseek, R1 76.22 81.49 77.88 90.01 71.63 78.37 62.05 75.74 72.58 59.96 75.29 80.51 75.14</p>
<p>All results are in %, the best ones are in bold, and the second best ones are in underline. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, LQ: LogiQA 2.0, RC : Re-Clor, AL: AR-LSAT, CT : ConTRoL, HL: HELP, TN : TaxiNLI, NN : NaN-NLI, FC : FraCas, RT : RuleTaker, PW : ProofWriter. Penedo, G.2023Table 2. LLMs' performance on the GLoRE benchmark. Falcon-40B: an open large language model with state-of-the-art performance</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, Palm 2 technical report. 2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. B Bench Authors, 2023TMLR</p>
<p>Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>How likely do llms with cot mimic human reasoning?. G Bao, H Zhang, C Wang, L Yang, Y Zhang, arXiv:2402.160482024arXiv preprint</p>
<p>Logical reasoning for task oriented dialogue systems. S Beygi, M Fazel-Zarandi, A Cervone, P Krishnan, S R Jonnalagadda, 2022</p>
<p>A large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, Proc. of EMNLP. of EMNLP2015</p>
<p>A thorough examination of the CNN/daily mail reading comprehension task. D Chen, J Bolton, C D Manning, ACL. 2016</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, Others: Evaluating large language models trained on code. 2021</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, Others: Evaluating large language models trained on code. 2021</p>
<p>Chatgpt goes to law school. J H Choi, K E Hickman, A Monahan, D Schwarcz, Available at SSRN. 2023</p>
<p>H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. C Clark, K Lee, M W Chang, T Kwiatkowski, M Collins, K Toutanova, 2019</p>
<p>Transformers as soft reasoners over language. P Clark, O Tafjord, K Richardson, Proc. of IJCAI. of IJCAI2020</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>M J Cresswell, Logics and languages. Routledge19731st ed.</p>
<p>The pascal recognising textual entailment challenge. I Dagan, O Glickman, B Magnini, 2005MLCW</p>
<p>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025</p>
<p>Transforming question answering datasets into natural language inference datasets. D Demszky, K Guu, P Liang, 2018</p>
<p>Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, L Li, Z Sui, A survey on in-context learning. 2023</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Proc. of AACL. of AACL2019</p>
<p>. S Frieder, L Pinchetti, R R Griffiths, T Salvatori, T Lukasiewicz, P C Petersen, A Chevalier, J Berner, 2023Mathematical capabilities of chatgpt</p>
<p>Chain-of-thought hub: A continuous effort to measure large models' reasoning performance. Y Fu, L Ou, M Chen, Y Wan, H Peng, T Khot, arXiv:2305.173062023arXiv preprint</p>
<p>Large language models are not abstract reasoners. G Gendron, Q Bao, M Witbrock, G Dobbie, arXiv:2305.195552023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR. the International Conference on Learning Representations (ICLR2021</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>J Huang, K C C Chang, Towards reasoning in large language models: A survey. 2023</p>
<p>Y Huang, M Fang, Y Cao, L Wang, X Liang, arXiv:2103.14349Dagn: Discourse-aware graph network for logical reasoning. 2021arXiv preprint</p>
<p>L Iwaska, Logical reasoning in natural language: It is all about knowledge. Minds and Machines. 1993</p>
<p>. A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, Others, 2024Mixtral of experts</p>
<p>Taxinli: Taking a ride up the NLU hill. P Joshi, S Aditya, A Sathe, M Choudhury, 2020CoRR</p>
<p>ContractNLI: A dataset for document-level natural language inference for contracts. Y Koreeda, C Manning, Proc. of EMNLP Findings. of EMNLP Findings2021</p>
<p>Logic for problem solving. R Kowalski, 1979Ediciones Daz de Santos</p>
<p>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C Elepao, M Madriaga, R Aggabao, G Diaz-Candido, J Maningo, 2023e0000198</p>
<p>RACE: Large-scale Reading Comprehension dataset from Examinations. G Lai, Q Xie, H Liu, Y Yang, E Hovy, EMNLP. 2017</p>
<p>Augmenting neural networks with first-order logic. T Li, V Srikumar, Proc. of ACL. of ACL2019</p>
<p>Competitionlevel code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, Others, 2022</p>
<p>Can large language models reason about medical questions?. V Livin, C E Hother, O Winther, arXiv:2207.081432022arXiv preprint</p>
<p>Natural language inference in context -investigating contextual reasoning over long texts. H Liu, L Cui, J Liu, Y Zhang, 2020CoRR</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. H Liu, J Liu, L Cui, Z Teng, N Duan, M Zhou, Y Zhang, Speech, and Language Processing. 2023</p>
<p>Logicot: Logical chainof-thought instruction tuning. H Liu, Z Teng, L Cui, C Zhang, Q Zhou, Y Zhang, Proc. of EMNLP Findings. of EMNLP Findings2023</p>
<p>. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, What makes good in-context examples for gpt-3? (2021</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. J Liu, L Cui, H Liu, D Huang, Wang, Y Zhang, 2020CoRR</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXivRoberta: A robustly optimized bert pretraining approach. 2019</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing2007</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing2007</p>
<p>Programs with common sense. J Mccarthy, 2002</p>
<p>Some philosophical problems from the standpoint of artificial intelligence. J Mccarthy, P J Hayes, Machine Intelligence. 41969</p>
<p>The logic theory machine-a complex information processing system. A Newell, H Simon, IRE Transactions on Information Theory. 1956</p>
<p>OpenAI: Gpt-4 technical report. 2023</p>
<p>Openai, Openai o1 system card. 2024</p>
<p>Human-like problem-solving abilities in large language models using chatgpt. G Orr, A Piarulli, C Conversano, A Gemignani, Frontiers in Artificial Intelligence. 11993502023</p>
<p>S Ott, K Hebenstreit, V Livin, C E Hother, M Moradi, M Mayrhauser, R Praas, O Winther, M Samwald, arXiv:2301.11596Thoughtsource: A central hub for large language model reasoning data. 2023arXiv preprint</p>
<p>Collecting diverse natural language inference problems for sentence representation evaluation. A Poliak, A Haldar, R Rudinger, J E Hu, E Pavlick, A S White, B Van Durme, Proc. of EMNLP. of EMNLP2018</p>
<p>Theorist: A Logical Reasoning System for Defaults and Diagnosis. D Poole, R Goebel, R Aleliunas, 1987</p>
<p>Using the framework. S G Pulman, 1996</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. C Qin, A Zhang, Z Zhang, J Chen, M Yasunaga, D Yang, 2023</p>
<p>S Quan, J Yang, B Yu, B Zheng, D Liu, A Yang, X Ren, B Gao, Y Miao, Y Feng, arXiv:2501.01257Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. 2025arXiv preprint</p>
<p>Prover: Proof generation for interpretable reasoning over rules. S Saha, S Ghosh, S Srivastava, M Bansal, 2020</p>
<p>S Sanyal, H Singh, X Ren, Fairr: Faithful and robust deductive reasoning over natural language. 2022</p>
<p>A Saparov, R Y Pang, V Padmakumar, N Joshi, S M Kazemi, N Kim, H He, arXiv:2305.15269Testing the general deductive reasoning capacity of large language models using ood examples. 2023arXiv preprint</p>
<p>T Sawada, D Paleka, A Havrilla, P Tadepalli, P Vidas, A Kranias, J J Nay, K Gupta, A Komatsuzaki, Arb: Advanced reasoning benchmark for large language models. 2023</p>
<p>Neural natural logic inference for interpretable question answering. J Shi, X Ding, L Du, T Liu, B Qin, Proc. of EMNLP. of EMNLP2021</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schrli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, J Wei, 2022</p>
<p>O Tafjord, B D Mishra, P Clark, Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2021</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Q Team, 2025</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozire, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation. T H Truong, Y Otmakhova, T Baldwin, T Cohn, J H Lau, K Verspoor, Proc. of AACL. of AACL2022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 2017CoRR</p>
<p>A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S R Bowman, SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. 2019</p>
<p>GLUE: A multitask benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2018</p>
<p>From lsat: The progress and challenges of complex reasoning. S Wang, Z Liu, W Zhong, M Zhou, Z Wei, Z Chen, N Duan, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 2022</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Y Wang, X Ma, G Zhang, Y Ni, A Chandra, S Guo, W Ren, A Arulraj, X He, Z Jiang, Proc. of NeurIPS. of NeurIPS2024</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proc. of AACL. of AACL2018</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proc. of NAACL. of NAACL2018</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyrek, B Chen, B Wang, N Kim, J Andreas, Y Kim, arXiv:2307.024772023arXiv preprint</p>
<p>Logiformer. F Xu, J Liu, Q Lin, Y Pan, L Zhang, Proc. of SIGIR. of SIGIR2022</p>
<p>Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning. H Yanaka, K Mineshima, D Bekki, K Inui, S Sekine, J Bos, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (<em>SEM2019). the Eighth Joint Conference on Lexical and Computational Semantics (</em>SEM2019)2019</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. W Yu, Z Jiang, Y Dong, J Feng, Proc. of ICLR. of ICLR2020</p>            </div>
        </div>

    </div>
</body>
</html>
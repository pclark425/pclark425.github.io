<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1109 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1109</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1109</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-214802092</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2004.02380v1.pdf" target="_blank">Intrinsic Exploration as Multi-Objective RL</a></p>
                <p><strong>Paper Abstract:</strong> Intrinsic motivation enables reinforcement learning (RL) agents to explore when rewards are very sparse, where traditional exploration heuristics such as Boltzmann or e-greedy would typically fail. However, intrinsic exploration is generally handled in an ad-hoc manner, where exploration is not treated as a core objective of the learning process; this weak formulation leads to sub-optimal exploration performance. To overcome this problem, we propose a framework based on multi-objective RL where both exploration and exploitation are being optimized as separate objectives. This formulation brings the balance between exploration and exploitation at a policy level, resulting in advantages over traditional methods. This also allows for controlling exploration while learning, at no extra cost. Such strategies achieve a degree of control over agent exploration that was previously unattainable with classic or intrinsic rewards. We demonstrate scalability to continuous state-action spaces by presenting a method (EMU-Q) based on our framework, guiding exploration towards regions of higher value-function uncertainty. EMU-Q is experimentally shown to outperform classic exploration techniques and other intrinsic RL methods on a continuous control benchmark and on a robotic manipulator.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1109.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1109.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exploration Values (Multi-objective RL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploration values as a multi-objective reinforcement learning formulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that treats exploration as a distinct RL objective (U) separate from exploitation (Q), combining them at policy level via a linear scalarization D = Q + κU; enables online/adaptive control of the exploration–exploitation tradeoff and generation of purely exploratory or exploitative trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Exploration Values framework</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multi-objective RL formulation where two value functions are learned: Q for external (task) rewards and U for intrinsic/exploration rewards. Policies select actions by maximizing Q(s,a) + κ U(s,a). Any RL algorithm may be used to learn Q and U; exploration rewards r_e are defined independently and can depend on Q (e.g., Q-variance).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>UCB-like acquisition / active learning via value-uncertainty (policy-level UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent adapts action selection online by changing κ (the scalarization parameter) to reweight exploration vs exploitation; κ can follow schedules (e.g. κ(t)=1/(1+ct)), be set to zero to stop exploration, or be changed when a performance budget/target is reached. Exploration rewards r_e can be defined as uncertainty measures (e.g., expected variance of Q at successor states), so actions are selected to maximize a posterior mean-plus-uncertainty acquisition function Q + κU. The framework allows sampling pure-exploitation test episodes (κ=0) to evaluate policy and then resume exploration if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Various (discrete toy MDPs like Cliff Walking, Taxi; chain MDP; continuous goal-only control benchmark including MountainCar, SinglePendulum, DoublePendulum, CartpoleSwingUp, LunarLander, Reacher, Hopper; Jaco manipulator simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown MDP dynamics (model-free setting), generally fully-observable MDPs in experiments; many environments are continuous state-action with sparse/goal-only rewards (unit reward at goal, zero elsewhere, possibly penalizing absorbing states); some toy domains include stochastic transitions (e.g., chain MDP, Cliff with random step noise). The paper does not assume partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Ranges from small discrete MDPs (Taxi, Cliff Walking) and chain of N states (2 discrete actions) to seven continuous control tasks with continuous state and action spaces and episodes capped at 500 steps; Jaco manipulator: multi-joint robotic arm control (damaged arm with immobilized joints), episode horizon 50 for reach task; specific state/action dimensionalities vary per domain (paper reports standard OpenAI Gym dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Multiple empirical improvements reported: (1) In Taxi stopping-experiment, 'exploration values' reached a predefined test target in 9/10 runs with mean episodes-to-target = 111.33 and post-target performance ≈ 0.08 (std 4.50); (2) In the 1D chain MDP, exploration-values-based policy (EMU-Q instantiation) finds goal states in time that scales approximately linearly with chain length (vs exponential regret for -greedy); (3) Across a 7-domain continuous control benchmark EMU-Q (which implements exploration values via Q-variance) shows higher consistency / higher success rate for first-goal discovery than baselines (VIME, DORA, RFF-Q); (4) On a damaged Jaco manipulator reach task EMU-Q solved more episodes (learns more consistently) than RFF-Q. (Metrics are reported per-experiment in the paper; where exact numbers are given they are quoted above.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines with non-adaptive additive intrinsic rewards or -greedy exploration performed worse: -greedy failed to reach Taxi target (0/10 runs); additive-reward agents reached target in 9/10 runs but required mean 242.11 episodes to reach target and had much worse post-target performance (mean -33.26, std 67.41) in that experiment. In chain MDP -greedy exhibits exponential regret (very large steps-to-goal) vs linear scaling for the adaptive approach.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported improved sample efficiency for exploration: (a) Chain domain: EMU-Q finds goals in far fewer environment steps than -greedy (linear vs exponential scaling with chain size). (b) Taxi: exploration-values agent required ~111 episodes (mean) to reach target vs ~242 for additive-rewards baseline. (c) Continuous benchmark: EMU-Q typically finds goal states within fewer episodes/more consistently than VIME or DORA (paper measures episodes to first positive goal; exact per-domain numbers in paper's Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit via scalarization parameter κ in the policy π(s)=argmax_a [Q(s,a)+κ U(s,a)]. κ controls immediate balance (UCB-like). Strategies include time-decay schedules κ(t)=1/(1+ct), manually stopping exploration (κ=0) when a budget or test-target is reached, and recomputing non-stationary exploration targets r_e after episodes to update U. Because Q and U are separate models, exploration influence on Q is not permanently encoded into Q estimates (unlike additive bonuses).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to -greedy Q-learning (RFF-Q), additive intrinsic rewards, VIME (information-gain dynamics exploration), DORA (directed outreach visitation-based exploration), and RFF-Q (Q-learning with Random Fourier Features and -greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Formulating intrinsic exploration as a separate objective (exploration values) and using a policy-level UCB-like combination yields better, controllable exploration: (1) provides online/explicit control over exploration (stop/resume, decay schedules), (2) reduces detrimental mixing of exploration bonus into Q estimates (avoids over-exploration and degenerative policies when exploration is turned off), (3) when instantiated as EMU-Q (minimizing epistemic uncertainty of Q using Bayesian linear regression + RFF) yields substantially better goal-finding sample efficiency and higher consistency across continuous sparse-reward tasks than several baselines, and (4) in chain MDPs it avoids exponential regret associated with naive action-noise exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limitations and caveats include: (1) The exploration reward r_e is non-stationary (depends on Q-uncertainty), which complicates learning U unless recomputed at episode boundaries; (2) EMU-Q uses a Bayesian linear model with kernel approximation (random Fourier features) which introduces approximation error depending on number of features; (3) VIME sometimes finds goals faster in higher-dimensional domains, suggesting EMU-Q's relative advantage may reduce with increased dimensionality or when dynamics-information-based exploration is more informative; (4) computational cost: Bayesian updates require maintaining and updating a posterior precision matrix S (O(M^2) per update with M features); (5) the paper does not address partially-observable MDPs (POMDPs) or very large-scale deep-function approximators directly — extension to policy-gradient or deep models is left as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsic Exploration as Multi-Objective RL', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1109.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1109.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMU-Q</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploration by Minimizing Uncertainty of Q-values (EMU-Q)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete algorithm instantiating the exploration-values framework: models Q and U with Bayesian linear regression over Random Fourier Features, uses predictive variance of Q to define exploration reward r_e (expected successor-state Q-variance minus V_max), and selects actions by maximizing Q + κ U.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EMU-Q</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free RL algorithm that: (1) approximates Q(s,a) and U(s,a) as linear functions over Random Fourier Features (RFF) of state-action inputs, (2) maintains a Bayesian linear regression posterior (mean m and precision S) to obtain predictive mean and epistemic variance for Q, (3) defines intrinsic reward r_e(s) = E_{a'}[Var[Q(s',a')]] - V_max to encourage transitions leading to high Q-uncertainty, (4) updates Q and U with Bellman-style targets using the posterior and rank-1 Sherman–Morrison updates for S for efficiency, and (5) selects actions by maximizing φ(s,a)^T m_Q + κ φ(s,a)^T m_U.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian active learning / UCB-like acquisition using posterior variance of Q (minimize epistemic uncertainty); curiosity-driven exploration via Q-variance; kernel-approximate Gaussian-process-like posterior via Bayesian linear regression on RFF</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>EMU-Q uses the predictive posterior variance of its Q-model as the information metric: exploration rewards are formed from expected variance at successor states; the action selection uses linear scalarization Q + κU (U learned to maximize discounted sum of r_e), and κ is tunable/automatically schedulable (e.g., κ(t)=1/(1+ct)) or can be switched off to stop exploration. The Bayesian posterior is updated incrementally so uncertainty decreases where data is gathered, guiding the agent to unexplored/high-uncertainty regions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same set as framework experiments: chain MDP, continuous MountainCar (goal-only), 7 continuous control goal-only benchmark (SinglePendulum, DoublePendulum, CartpoleSwingUp, LunarLander, Reacher, Hopper, MountainCar), and a Jaco manipulator reach task (damaged arm).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics; typically fully-observable, continuous state-action spaces for benchmark tasks; sparse / goal-only rewards (unit at goal, zero otherwise); some environments include stochastic transition noise (chain, Cliff), and the Jaco manipulator is a realistic high-dimensional continuous control problem with some joints immobilized (damage).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Chain: N discrete states, 2 actions; continuous benchmarks: multi-dimensional continuous states and actions (OpenAI Gym tasks), episodes capped at 500 steps; Jaco manipulator: multi-joint arm, episode horizon 50 for reach, action dimensionality equals actuator joints minus immobilized ones, experiments use 600 RFF features for function approximation; algorithm hyperparameters α=0.1, β=1.0, κ typically 0.1 or set to 1/V_max.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirically superior exploration performance: (1) Chain MDP: EMU-Q finds goals with steps-to-goal that scale approximately linearly with chain length, avoiding exponential regret observed for -greedy; (2) Continuous benchmark: EMU-Q achieved higher consistency and success rates in finding first positive goal across 7 tasks compared to VIME and DORA (results averaged over 20 runs; per-domain metrics in paper's Table 3); (3) Jaco manipulator: with 600 RFF features EMU-Q solved substantially more target-reaching episodes and was more consistent than RFF-Q (quantitative curves provided in paper's Figure 8b).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines without EMU-Q adaptive design (RFF-Q with -greedy) and additive intrinsic reward methods perform worse: -greedy often fails on sparse goal-only tasks; additive reward baselines are slower to discover goals and can degrade when exploration is stopped. Example: in Taxi experiments a -greedy baseline reached the predefined test-target in 0/10 runs, additive rewards reached 9/10 but with mean episodes-to-target ~242 and poor post-target performance, whereas EMU-Q-style exploration-values reached similar or better target rates faster (~111 episodes in the reported experiment for exploration-values).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Demonstrated improved sample efficiency for goal discovery: e.g., Taxi-like tests ~111 episodes to target (exploration-values) vs ~242 for additive baseline; chain MDP shows EMU-Q requires far fewer steps than -greedy where -greedy scales exponentially; continuous tasks: EMU-Q commonly finds goals within the 100-episode evaluation window in more runs than baselines (exact per-task numbers in paper's results table).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit scalarization using κ in π(s)=argmax_a [φ(s,a)^T m_Q + κ φ(s,a)^T m_U]; κ is treated as an adjustable control knob (decay schedules, stop when test-target met); because U is learned separately from Q, the exploitation value Q is not polluted with exploration bonuses, enabling clean switching to pure exploitation (κ=0) for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>VIME (variational information-maximizing exploration), DORA (directed visitation-based exploration), RFF-Q (-greedy Q-learning with RFF), additive intrinsic rewards baselines, and classical tabular Q-Learning with ε-greedy in discrete tests.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>When epistemic uncertainty of Q is the exploration signal and is minimized via a U objective, the resulting EMU-Q agent: (1) efficiently directs exploration toward informative regions (high Q-variance) and finds sparse-goal states much faster than undirected exploration; (2) offers explicit, effective online control over exploration via κ schedules and stopping rules; (3) scales to continuous state-action tasks using Random Fourier Features as a GP-approximation combined with Bayesian linear regression; (4) outperforms or is more consistent than several intrinsic-exploration baselines on a diverse set of sparse reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limitations reported or acknowledged: (1) Non-stationarity of r_e requires recomputation (paper recommends episode-based recomputation) to stabilize U learning; (2) Kernel approximation (RFF) quality depends on feature count (approximation error) and increases computation with more features (O(M^2) update cost); (3) In higher-dimensional domains, VIME sometimes finds goals faster indicating EMU-Q may be less advantageous when dynamics-information signals are stronger or in very high-dimensional problems; (4) The method as presented is for value-based model-free RL with linear Bayesian models and RFF — direct extension to deep networks / POMDPs / large-scale domains is left as future work and is non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsic Exploration as Multi-Objective RL', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vime: Variational information maximizing exploration <em>(Rating: 2)</em></li>
                <li>DORA the explorer: Directed outreaching reinforcement action-selection <em>(Rating: 2)</em></li>
                <li>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning <em>(Rating: 2)</em></li>
                <li>Bayesian Q-learning <em>(Rating: 2)</em></li>
                <li>Generalization and exploration via randomized value functions <em>(Rating: 2)</em></li>
                <li>Efficient exploration through Bayesian deep Q-networks <em>(Rating: 1)</em></li>
                <li>Unifying count-based exploration and intrinsic motivation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1109",
    "paper_id": "paper-214802092",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Exploration Values (Multi-objective RL)",
            "name_full": "Exploration values as a multi-objective reinforcement learning formulation",
            "brief_description": "A framework that treats exploration as a distinct RL objective (U) separate from exploitation (Q), combining them at policy level via a linear scalarization D = Q + κU; enables online/adaptive control of the exploration–exploitation tradeoff and generation of purely exploratory or exploitative trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Exploration Values framework",
            "agent_description": "A multi-objective RL formulation where two value functions are learned: Q for external (task) rewards and U for intrinsic/exploration rewards. Policies select actions by maximizing Q(s,a) + κ U(s,a). Any RL algorithm may be used to learn Q and U; exploration rewards r_e are defined independently and can depend on Q (e.g., Q-variance).",
            "adaptive_design_method": "UCB-like acquisition / active learning via value-uncertainty (policy-level UCB)",
            "adaptation_strategy_description": "The agent adapts action selection online by changing κ (the scalarization parameter) to reweight exploration vs exploitation; κ can follow schedules (e.g. κ(t)=1/(1+ct)), be set to zero to stop exploration, or be changed when a performance budget/target is reached. Exploration rewards r_e can be defined as uncertainty measures (e.g., expected variance of Q at successor states), so actions are selected to maximize a posterior mean-plus-uncertainty acquisition function Q + κU. The framework allows sampling pure-exploitation test episodes (κ=0) to evaluate policy and then resume exploration if needed.",
            "environment_name": "Various (discrete toy MDPs like Cliff Walking, Taxi; chain MDP; continuous goal-only control benchmark including MountainCar, SinglePendulum, DoublePendulum, CartpoleSwingUp, LunarLander, Reacher, Hopper; Jaco manipulator simulator)",
            "environment_characteristics": "Unknown MDP dynamics (model-free setting), generally fully-observable MDPs in experiments; many environments are continuous state-action with sparse/goal-only rewards (unit reward at goal, zero elsewhere, possibly penalizing absorbing states); some toy domains include stochastic transitions (e.g., chain MDP, Cliff with random step noise). The paper does not assume partial observability.",
            "environment_complexity": "Ranges from small discrete MDPs (Taxi, Cliff Walking) and chain of N states (2 discrete actions) to seven continuous control tasks with continuous state and action spaces and episodes capped at 500 steps; Jaco manipulator: multi-joint robotic arm control (damaged arm with immobilized joints), episode horizon 50 for reach task; specific state/action dimensionalities vary per domain (paper reports standard OpenAI Gym dimensions).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Multiple empirical improvements reported: (1) In Taxi stopping-experiment, 'exploration values' reached a predefined test target in 9/10 runs with mean episodes-to-target = 111.33 and post-target performance ≈ 0.08 (std 4.50); (2) In the 1D chain MDP, exploration-values-based policy (EMU-Q instantiation) finds goal states in time that scales approximately linearly with chain length (vs exponential regret for -greedy); (3) Across a 7-domain continuous control benchmark EMU-Q (which implements exploration values via Q-variance) shows higher consistency / higher success rate for first-goal discovery than baselines (VIME, DORA, RFF-Q); (4) On a damaged Jaco manipulator reach task EMU-Q solved more episodes (learns more consistently) than RFF-Q. (Metrics are reported per-experiment in the paper; where exact numbers are given they are quoted above.)",
            "performance_without_adaptation": "Baselines with non-adaptive additive intrinsic rewards or -greedy exploration performed worse: -greedy failed to reach Taxi target (0/10 runs); additive-reward agents reached target in 9/10 runs but required mean 242.11 episodes to reach target and had much worse post-target performance (mean -33.26, std 67.41) in that experiment. In chain MDP -greedy exhibits exponential regret (very large steps-to-goal) vs linear scaling for the adaptive approach.",
            "sample_efficiency": "Reported improved sample efficiency for exploration: (a) Chain domain: EMU-Q finds goals in far fewer environment steps than -greedy (linear vs exponential scaling with chain size). (b) Taxi: exploration-values agent required ~111 episodes (mean) to reach target vs ~242 for additive-rewards baseline. (c) Continuous benchmark: EMU-Q typically finds goal states within fewer episodes/more consistently than VIME or DORA (paper measures episodes to first positive goal; exact per-domain numbers in paper's Table 3).",
            "exploration_exploitation_tradeoff": "Explicit via scalarization parameter κ in the policy π(s)=argmax_a [Q(s,a)+κ U(s,a)]. κ controls immediate balance (UCB-like). Strategies include time-decay schedules κ(t)=1/(1+ct), manually stopping exploration (κ=0) when a budget or test-target is reached, and recomputing non-stationary exploration targets r_e after episodes to update U. Because Q and U are separate models, exploration influence on Q is not permanently encoded into Q estimates (unlike additive bonuses).",
            "comparison_methods": "Compared to -greedy Q-learning (RFF-Q), additive intrinsic rewards, VIME (information-gain dynamics exploration), DORA (directed outreach visitation-based exploration), and RFF-Q (Q-learning with Random Fourier Features and -greedy).",
            "key_results": "Formulating intrinsic exploration as a separate objective (exploration values) and using a policy-level UCB-like combination yields better, controllable exploration: (1) provides online/explicit control over exploration (stop/resume, decay schedules), (2) reduces detrimental mixing of exploration bonus into Q estimates (avoids over-exploration and degenerative policies when exploration is turned off), (3) when instantiated as EMU-Q (minimizing epistemic uncertainty of Q using Bayesian linear regression + RFF) yields substantially better goal-finding sample efficiency and higher consistency across continuous sparse-reward tasks than several baselines, and (4) in chain MDPs it avoids exponential regret associated with naive action-noise exploration.",
            "limitations_or_failures": "Reported limitations and caveats include: (1) The exploration reward r_e is non-stationary (depends on Q-uncertainty), which complicates learning U unless recomputed at episode boundaries; (2) EMU-Q uses a Bayesian linear model with kernel approximation (random Fourier features) which introduces approximation error depending on number of features; (3) VIME sometimes finds goals faster in higher-dimensional domains, suggesting EMU-Q's relative advantage may reduce with increased dimensionality or when dynamics-information-based exploration is more informative; (4) computational cost: Bayesian updates require maintaining and updating a posterior precision matrix S (O(M^2) per update with M features); (5) the paper does not address partially-observable MDPs (POMDPs) or very large-scale deep-function approximators directly — extension to policy-gradient or deep models is left as future work.",
            "uuid": "e1109.0",
            "source_info": {
                "paper_title": "Intrinsic Exploration as Multi-Objective RL",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "EMU-Q",
            "name_full": "Exploration by Minimizing Uncertainty of Q-values (EMU-Q)",
            "brief_description": "A concrete algorithm instantiating the exploration-values framework: models Q and U with Bayesian linear regression over Random Fourier Features, uses predictive variance of Q to define exploration reward r_e (expected successor-state Q-variance minus V_max), and selects actions by maximizing Q + κ U.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EMU-Q",
            "agent_description": "Model-free RL algorithm that: (1) approximates Q(s,a) and U(s,a) as linear functions over Random Fourier Features (RFF) of state-action inputs, (2) maintains a Bayesian linear regression posterior (mean m and precision S) to obtain predictive mean and epistemic variance for Q, (3) defines intrinsic reward r_e(s) = E_{a'}[Var[Q(s',a')]] - V_max to encourage transitions leading to high Q-uncertainty, (4) updates Q and U with Bellman-style targets using the posterior and rank-1 Sherman–Morrison updates for S for efficiency, and (5) selects actions by maximizing φ(s,a)^T m_Q + κ φ(s,a)^T m_U.",
            "adaptive_design_method": "Bayesian active learning / UCB-like acquisition using posterior variance of Q (minimize epistemic uncertainty); curiosity-driven exploration via Q-variance; kernel-approximate Gaussian-process-like posterior via Bayesian linear regression on RFF",
            "adaptation_strategy_description": "EMU-Q uses the predictive posterior variance of its Q-model as the information metric: exploration rewards are formed from expected variance at successor states; the action selection uses linear scalarization Q + κU (U learned to maximize discounted sum of r_e), and κ is tunable/automatically schedulable (e.g., κ(t)=1/(1+ct)) or can be switched off to stop exploration. The Bayesian posterior is updated incrementally so uncertainty decreases where data is gathered, guiding the agent to unexplored/high-uncertainty regions.",
            "environment_name": "Same set as framework experiments: chain MDP, continuous MountainCar (goal-only), 7 continuous control goal-only benchmark (SinglePendulum, DoublePendulum, CartpoleSwingUp, LunarLander, Reacher, Hopper, MountainCar), and a Jaco manipulator reach task (damaged arm).",
            "environment_characteristics": "Unknown transition dynamics; typically fully-observable, continuous state-action spaces for benchmark tasks; sparse / goal-only rewards (unit at goal, zero otherwise); some environments include stochastic transition noise (chain, Cliff), and the Jaco manipulator is a realistic high-dimensional continuous control problem with some joints immobilized (damage).",
            "environment_complexity": "Chain: N discrete states, 2 actions; continuous benchmarks: multi-dimensional continuous states and actions (OpenAI Gym tasks), episodes capped at 500 steps; Jaco manipulator: multi-joint arm, episode horizon 50 for reach, action dimensionality equals actuator joints minus immobilized ones, experiments use 600 RFF features for function approximation; algorithm hyperparameters α=0.1, β=1.0, κ typically 0.1 or set to 1/V_max.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirically superior exploration performance: (1) Chain MDP: EMU-Q finds goals with steps-to-goal that scale approximately linearly with chain length, avoiding exponential regret observed for -greedy; (2) Continuous benchmark: EMU-Q achieved higher consistency and success rates in finding first positive goal across 7 tasks compared to VIME and DORA (results averaged over 20 runs; per-domain metrics in paper's Table 3); (3) Jaco manipulator: with 600 RFF features EMU-Q solved substantially more target-reaching episodes and was more consistent than RFF-Q (quantitative curves provided in paper's Figure 8b).",
            "performance_without_adaptation": "Baselines without EMU-Q adaptive design (RFF-Q with -greedy) and additive intrinsic reward methods perform worse: -greedy often fails on sparse goal-only tasks; additive reward baselines are slower to discover goals and can degrade when exploration is stopped. Example: in Taxi experiments a -greedy baseline reached the predefined test-target in 0/10 runs, additive rewards reached 9/10 but with mean episodes-to-target ~242 and poor post-target performance, whereas EMU-Q-style exploration-values reached similar or better target rates faster (~111 episodes in the reported experiment for exploration-values).",
            "sample_efficiency": "Demonstrated improved sample efficiency for goal discovery: e.g., Taxi-like tests ~111 episodes to target (exploration-values) vs ~242 for additive baseline; chain MDP shows EMU-Q requires far fewer steps than -greedy where -greedy scales exponentially; continuous tasks: EMU-Q commonly finds goals within the 100-episode evaluation window in more runs than baselines (exact per-task numbers in paper's results table).",
            "exploration_exploitation_tradeoff": "Explicit scalarization using κ in π(s)=argmax_a [φ(s,a)^T m_Q + κ φ(s,a)^T m_U]; κ is treated as an adjustable control knob (decay schedules, stop when test-target met); because U is learned separately from Q, the exploitation value Q is not polluted with exploration bonuses, enabling clean switching to pure exploitation (κ=0) for evaluation.",
            "comparison_methods": "VIME (variational information-maximizing exploration), DORA (directed visitation-based exploration), RFF-Q (-greedy Q-learning with RFF), additive intrinsic rewards baselines, and classical tabular Q-Learning with ε-greedy in discrete tests.",
            "key_results": "When epistemic uncertainty of Q is the exploration signal and is minimized via a U objective, the resulting EMU-Q agent: (1) efficiently directs exploration toward informative regions (high Q-variance) and finds sparse-goal states much faster than undirected exploration; (2) offers explicit, effective online control over exploration via κ schedules and stopping rules; (3) scales to continuous state-action tasks using Random Fourier Features as a GP-approximation combined with Bayesian linear regression; (4) outperforms or is more consistent than several intrinsic-exploration baselines on a diverse set of sparse reward tasks.",
            "limitations_or_failures": "Limitations reported or acknowledged: (1) Non-stationarity of r_e requires recomputation (paper recommends episode-based recomputation) to stabilize U learning; (2) Kernel approximation (RFF) quality depends on feature count (approximation error) and increases computation with more features (O(M^2) update cost); (3) In higher-dimensional domains, VIME sometimes finds goals faster indicating EMU-Q may be less advantageous when dynamics-information signals are stronger or in very high-dimensional problems; (4) The method as presented is for value-based model-free RL with linear Bayesian models and RFF — direct extension to deep networks / POMDPs / large-scale domains is left as future work and is non-trivial.",
            "uuid": "e1109.1",
            "source_info": {
                "paper_title": "Intrinsic Exploration as Multi-Objective RL",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vime: Variational information maximizing exploration",
            "rating": 2,
            "sanitized_title": "vime_variational_information_maximizing_exploration"
        },
        {
            "paper_title": "DORA the explorer: Directed outreaching reinforcement action-selection",
            "rating": 2,
            "sanitized_title": "dora_the_explorer_directed_outreaching_reinforcement_actionselection"
        },
        {
            "paper_title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
            "rating": 2,
            "sanitized_title": "a_tutorial_on_bayesian_optimization_of_expensive_cost_functions_with_application_to_active_user_modeling_and_hierarchical_reinforcement_learning"
        },
        {
            "paper_title": "Bayesian Q-learning",
            "rating": 2,
            "sanitized_title": "bayesian_qlearning"
        },
        {
            "paper_title": "Generalization and exploration via randomized value functions",
            "rating": 2,
            "sanitized_title": "generalization_and_exploration_via_randomized_value_functions"
        },
        {
            "paper_title": "Efficient exploration through Bayesian deep Q-networks",
            "rating": 1,
            "sanitized_title": "efficient_exploration_through_bayesian_deep_qnetworks"
        },
        {
            "paper_title": "Unifying count-based exploration and intrinsic motivation",
            "rating": 1,
            "sanitized_title": "unifying_countbased_exploration_and_intrinsic_motivation"
        }
    ],
    "cost": 0.014159,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Intrinsic Exploration as Multi-Objective RL Intrinsic Exploration as Multi-Objective RL</p>
<p>Philippe Morere philippe.morere@sydney.edu.au 
The University of Sydney
SydneyAustralia</p>
<p>Fabio Ramos fabio.ramos@sydney.edu.au 
The University of Sydney &amp; NVIDIA
SydneyAustralia</p>
<p>Intrinsic Exploration as Multi-Objective RL Intrinsic Exploration as Multi-Objective RL
Reinforcement LearningRoboticsExplorationOnline LearningMulti- Objective
Intrinsic motivation enables reinforcement learning (RL) agents to explore when rewards are very sparse, where traditional exploration heuristics such as Boltzmann or -greedy would typically fail. However, intrinsic exploration is generally handled in an ad-hoc manner, where exploration is not treated as a core objective of the learning process; this weak formulation leads to sub-optimal exploration performance. To overcome this problem, we propose a framework based on multi-objective RL where both exploration and exploitation are being optimized as separate objectives. This formulation brings the balance between exploration and exploitation at a policy level, resulting in advantages over traditional methods. This also allows for controlling exploration while learning, at no extra cost. Such strategies achieve a degree of control over agent exploration that was previously unattainable with classic or intrinsic rewards. We demonstrate scalability to continuous state-action spaces by presenting a method (EMU-Q) based on our framework, guiding exploration towards regions of higher value-function uncertainty. EMU-Q is experimentally shown to outperform classic exploration techniques and other intrinsic RL methods on a continuous control benchmark and on a robotic manipulator.. This paper extends previous work published asMorere and Ramos (2018).</p>
<p>Introduction</p>
<p>In Reinforcement Learning (RL), data-efficiency and learning speed are paramount. Indeed, when interacting with robots, humans, or the real world, data can be extremely scarce and expensive to collect. Improving data-efficiency is of the utmost importance to apply RL to interesting and realistic applications. Learning from few data is relatively easier to achieve when rewards are dense, as these can be used to guide exploration. In most realistic problems however, defining dense reward functions is non-trivial, requires expert knowledge and much fine-tuning. In some cases (eg. when dealing with humans), definitions for dense rewards are unclear and remain an open problem. This greatly hinders the applicability of RL to many interesting problems.</p>
<p>It appears more natural to reward robots only when reaching a goal, termed goal-only rewards, which becomes trivial to define Reinke et al. (2017). Goal-only rewards, defined as unit reward for reaching a goal and zero elsewhere, cause classic exploration techniques  Chentanez et al. (2005).</p>
<p>Right: intrinsic exploration formulated as multi-objective RL based on random-walk such as -greedy and control input noise Schulman et al. (2015), or optimistic initialization to become highly inefficient. For example, Boltzmann exploration Kaelbling et al. (1996) requires training time exponential in the number of states Osband et al. (2014). Such data requirement is unacceptable in real-world applications. Most solutions to this problem rely on redesigning rewards to avoid dealing with the problem of exploration. Reward shaping helps learning Ng et al. (1999), and translating rewards to negative values triggers optimism in the face of uncertainty Kearns and Singh (2002); Brafman and Tennenholtz (2002); Jaksch et al. (2010). This approach suffers from two shortcomings: proper reward design is difficult and requires expert knowledge; improper reward design often degenerates to unexpected learned behaviour. Intrinsic motivation proposes a different approach to exploration by defining an additional guiding reward; see Figure 1 (left). The exploration reward is typically added to the original reward, which makes rewards dense from the agent's perspective. This approach has had many successes Bellemare et al. (2016); Fox et al. (2018) but suffers several limitations. For example, weighting between exploration and exploitation must be chosen before learning and remain fixed. Furthermore, in the model-free setting, state-action value functions are learned from non-stationary targets mixing exploration and exploitation, hence making learning less data-efficient.</p>
<p>To solve the problem of data-efficient exploration in goal-only reward settings, we propose to leverage advances in multi-objective RL Roijers et al. (2013). We formulate exploration as one of the core objectives of RL by explicitly integrating it to the loss being optimized. Following the multi-objective RL framework, agents optimize for both exploration and exploitation as separate objectives. This decomposition can be seen as two different RL agents, as shown in Figure 1 (right). Contrary to most intrinsic RL approaches, this formulation keeps the exploration-exploitation trade-off at a policy level, as in traditional RL. This allows for several advantages: (i) Weighting between objectives can be adapted while learning, and strategies can be developed to change exploration online; (ii) Exploration can be stopped at any time at no extra cost, yielding purely exploratory behaviour immediately; (iii) Inspection of exploration status is possible, and experimenters can easily generate trajectories for exploration or exploitation only.</p>
<p>Our contributions are the following:</p>
<p>• We propose a framework based on multi-objective RL for treating exploration as an explicit objective, making it core to the optimization problem.</p>
<p>• This framework is experimentally shown to perform better than classic additive exploration bonuses on several key exploration characteristics.</p>
<p>• Drawing inspiration from the fields of bandits and Bayesian optimization, we give strategies for taking advantage of and tuning the exploration-exploitation balance online. These strategies achieve a degree of control over agent exploration that was previously unattainable with classic additive intrinsic rewards.</p>
<p>• We present a data-efficient model-free RL method (EMU-Q) for continuous state-action goal-only MDPs based on the proposed framework, guiding exploration towards regions of higher value-function uncertainty.</p>
<p>• EMU-Q is experimentally shown to outperform classic exploration techniques and other methods with additive intrinsic rewards on a continuous control benchmark.</p>
<p>In the following, Section 2 reviews background on Markov decision processes, intrinsic motivation RL, multi-objective RL and related work. Section 3 defines a framework for explicit exploration-exploitation balance at a policy level, based on multi-objective RL. Section 4 presents advantages and strategies for controlling this balance during the agent learning process. Section 5 formulates EMU-Q, a model-free data-efficient RL method for continuous state-action goal-only MDPs, based on the proposed framework. Section 6 presents experiments that evaluate EMU-Q's exploration capabilities on classic RL problems and a simulated robotic manipulator. EMU-Q is further evaluated against other intrinsic RL methods on a continuous control benchmark. We conclude with a summary in Section 7.</p>
<p>Preliminaries</p>
<p>This section reviews basics on Markov decision processes, intrinsic motivation RL, multiobjective RL and related work.</p>
<p>Markov Decision Processes</p>
<p>A Markov decision process (MDP) is defined by the tuple &lt; S, A, T, R, γ &gt;. S and A are spaces of states s and actions a respectively. The transition function T : S × A × S → [0, 1] encodes the probability to transition to state s when executing action a in state s, i.e. T (s, a, s ) = p(s |s, a). The reward distribution R of support S × A × S defines the reward r associated with transition (s, a, s ). In the simplest case, goal-only rewards are deterministic and unit rewards are given for absorbing goal states, potential negative unit rewards are given for penalized absorbing states, and zero-reward is given elsewhere. γ ∈ [0, 1) is a discount factor. Solving a MDP is equivalent to finding the optimal policy π * starting from s 0 :
π * = arg max π E T,R,π [ ∞ i=0 γ i r i ],(1)
with a i ∼ π(s i ), s i+1 ∼ T (s i , a i , ·), and r i ∼ R(s i , a i , s i+1 ). Model-free RL learns an actionvalue function Q, which encodes the expected long-term discounted value of a state-action pair
Q(s, a) = E T,R,π [ ∞ i=0 γ i r i ].(2)
Equation 2 can be rewritten recursively, also known as the Bellman equation
Q(s, a) = E R [R(s, a, s )] + γE s ,a |s,a [Q(s , a )],(3)
s ∼ p(s |s, a), a ∼ π(s ), which is used to iteratively refine models of Q based on transition data.</p>
<p>Intrinsic RL</p>
<p>While classic RL typically carries out exploration by adding randomness at a policy level (eg. random action, posterior sampling), intrinsic RL focuses on augmenting rewards with an exploration bonus. This approach was presented in Chentanez et al. (2005), in which agents aim to maximize a total reward r total for transition (s, a, r, s ):
r total = r + ξr e ,(4)
where r e is the exploration bonus and ξ a user-defined parameter weighting exploration. The second term encourages agents to select state-action pairs for which they previously received high exploration bonuses. The definition of r e has been the focus of much recent theoretical and applied work; examples include model prediction error Stadie et al. (2015) or information gain Little and Sommer (2013). While this formulation enables exploration in well behaved scenarios, it suffers from multiple limitations:</p>
<p>• Exploration bonuses are designed to reflect the information gain at a given time of the learning process. They are initially high, and typically decrease after more transitions are experienced, making it a non-stationary target. Updating Q with non-stationary targets results in higher data requirements, especially when environment rewards are stationary.</p>
<p>• The exploration bonus given for reaching new areas of the state-action space persists in the estimate of Q. As a consequence, agents tend to over-explore and may be stuck oscillating between neighbouring states.</p>
<p>• There is no dynamic control over the exploration-exploitation balance, as changing parameter ξ only affects future total rewards. Furthermore, it would be desirable to control generating trajectories for pure exploration or pure exploitation, as these two quantities may conflict.</p>
<p>This work presents a framework for enhancing intrinsic exploration, which does not suffer from the previously stated limitations.</p>
<p>Multi-Objective RL</p>
<p>Multi-objective RL seeks to learn policies solving multiple competing objectives by learning how to solve for each objective individually Roijers et al. (2013). In multi-objective RL, the reward function describes a vector of n rewards instead of a scalar. The value function also becomes a vector Q defined as
Q(s, a) = E T,R,π [ ∞ i=0 γ i r i ],(5)
where r i is the vector of rewards at step i in which each coordinate corresponds to one objective. For simplicity, the overall objective is often expressed as the sum of all individual objectives; Q can be converted to a scalar state-action value function with a linear scalarization function: Q ω (s, a) = ω T Q(s, a), where ω are weights governing the relative importance of each objective. The advantage of the multi-objective RL formulation is to allow learning policies for all combinations of ω, even if the balance between each objective is not explicitly defined prior to learning. Moreover, if ω is a function of time, policies for new values of ω are available without additional computation. Conversely, with traditional RL methods, a pass through the whole dataset of transitions would be required.</p>
<p>Related Work</p>
<p>Enhancing exploration with additional rewards can be traced back to the work of Storck et al. (1995) and Meuleau and Bourgine (1999), in which information acquisition is dealt with in an active manner. This type of exploration was later termed intrinsic motivation and studied in Chentanez et al. (2005). This field has recently received much attention, especially in the context of very sparse or goal-only rewards Reinke et al. (2017); Morere and Ramos (2018) where traditional reward functions give too little guidance to RL algorithms.</p>
<p>Extensive intrinsic motivation RL work has focused on domains with simple or discrete spaces, proposing various definitions for exploration bonuses. Starting from reviewing intrinsic motivation in psychology, the work of Oudeyer and Kaplan (2008) presents a definition based on information theory. Maximizing predicted information gain from taking specific actions is the focus of Little and Sommer (2013), applied to learning in the absence of external reward feedback. Using approximate value function variance as an exploration bonus was proposed in Osband et al. (2016). In the context of model-based RL, exploration based on model learning progress Lopes et al. (2012), and model prediction error Stadie et al. (2015); Pathak et al. (2017) were proposed. State visitation counts have been widely investigated, in which an additional model counting previous state-action pair occurrences guides agents towards less visited regions. Recent successes include Bellemare et al. (2016);Fox et al. (2018). An attempt to generalizing counter-based exploration to continuous state spaces was made in Nouri and Littman (2009), by using regression trees to achieve multi-resolution coverage of the state space. Another pursuit for scaling visitation counters to large and continuous state spaces was made in Bellemare et al. (2016) by using density models.</p>
<p>Little work attempted to extend intrinsic exploration to continuous action spaces. A policy gradient RL method was presented in Houthooft et al. (2016). Generalization of visitation counters is proposed in Fox et al. (2018), and interpreted as exploration values. Exploration values are also presented as an alternative to additive rewards in Szita and Lőrincz (2008), where exploration balance at a policy level is mentioned.</p>
<p>Most of these methods typically suffer from high data requirements. One of the reasons for such requirements is that exploration is treated as an ad-hoc problem instead of being the focus of the optimization method. More principled ways to deal with exploration can be found in other related fields. In bandits, the balance between exploration and exploitation is central to the formulation Kuleshov and Precup (2014). For example with upper confidence bound Auer et al. (2002), actions are selected based on the balance between action values and a visitation term measuring the variance in the estimate of the action value. In the bandits setting, the balance is defined at a policy level, and the exploration term is not incorporated into action values like in intrinsic RL.</p>
<p>Similarly to bandits, Bayesian Optimization Jones et al. (1998) (BO) brings exploration at the core of its framework, extending the problem to continuous action spaces. BO provides a data-efficient approach for finding the optimum of an unknown objective. Exploration is achieved by building a probabilistic model of the objective from samples, and exploiting its posterior variance information. An acquisition function such as UCB Cox and John (1992) balances exploration and exploitation, and is at the core of the optimization problem. BO was successfully applied to direct policy search Brochu et al. (2010); Wilson et al. (2014) by searching over the space of policy parameters, casting RL into a supervised learning problem. Searching the space of policy parameters is however not data-efficient as recently acquired step information is not used to improve exploration. Furthermore, using BO as global search over policy parameters greatly restricts parameter dimensionality, hence typically imposes using few expressive and hand-crafted features.</p>
<p>In both bandits and BO formulations, exploration is brought to a policy level where it is a central goal of the optimization process. In this work, we treat exploration and exploitation as two distinct objectives to be optimized. Multi-objective RL Roijers et al. (2013) provides tools which we utilize for defining these two distinct objectives, and balancing them at a policy level. Multi-objective RL allows for making exploration central to the optimization process. While there exist Multi-objective RL methods to find several viable objective weightings such as finding Pareto fronts Perny and Weng (2010), our work focuses on two well defined objectives whose weighting changes during learning. As such, we are mostly interested in the ability to change the relative importance of objectives without requiring training.</p>
<p>Modelling state-action values using a probabilistic model enables reasoning about the whole distribution instead of just its expectation, giving opportunities for better exploration strategies. Bayesian Q-learning Dearden et al. (1998) was first proposed to provide value function posterior information in the tabular case, then extended to more complicated domains by using Gaussian processes to model the state-action function Engel et al. (2005). In this work, authors also discuss decomposition of returns into several terms separating intrinsic and extrinsic uncertainty, which could later be used for exploration. Distribution over returns were proposed to design risk-sensitive algorithms Morimura et al. (2010), and approximated to enhance RL stability in Bellemare et al. (2017). In recent work, Bayesian linear regression is combined to a deep network to provide a posterior on Q-values Azizzadenesheli et al. (2018). Thomson sampling is then used for action selection, but can only guarantee local exploration. Indeed, if all action were experienced in a given state, the uncertainty of Q in this state is not sufficient to drive the agent towards unexplored regions.</p>
<p>To the best of our knowledge, there exists no model-free RL framework treating exploration as a core objective. We present such framework, building on theory from multiobjective RL, bandits and BO. We also present EMU-Q, a solution to exploration based on the proposed framework in fully continuous goal-only domains, relying on reducing the posterior variance of value functions. This paper extends our earlier work Morere and Ramos (2018). It formalizes a new framework for treating exploration and exploitation as two objectives, provides strategies for online exploration control and new experimental results.</p>
<p>Explicit Balance for Exploration and Exploitation</p>
<p>Traditional RL aims at finding a policy maximizing the expected sum of future discounted rewards, as formulated in Equation 1. Exploration is then typically achieved by adding a perturbation to rewards or behaviour policies in an ad-hoc way. We propose making the trade-off between exploration and exploitation explicit and at a policy level, by formulating exploration as a multi-objective RL problem.</p>
<p>Framework Overview</p>
<p>Multi-objective RL extends the classic RL framework by allowing value functions or policies to be learned for individual objectives. Exploitation and exploration are two distinct objectives for RL agents, for which separate value functions Q and U (respectively) can be learned. Policies then need to make use of information from two separate models for Q and U . While exploitation value function Q is learned from external rewards, exploration value function U is modelled using exploration rewards.</p>
<p>Aiming to define policies which combine exploration and exploitation, we draw inspiration from Bayesian Optimization Brochu et al. (2010), which seeks to find the maximum of an expensive function using very few samples. It relies on an acquisition function to determine the most promising locations to sample next, based on model posterior mean and variance. The Upper-Confidence Bounds (UCB) acquisition function Cox and John (1992) is popular for its explicit balance between exploitation and exploration controlled by parameter κ ∈ [0, ∞). Adapting UCB to our framework leads to policies balancing Q and U . Contrary to most intrinsic RL approaches, our formulation keeps the exploration-exploitation trade-off at a policy level, as in traditional RL. This allows for adapting the exploration-exploitation balance during the learning process without sacrificing data-efficiency, as would be the case with a balance at a reward level. Furthermore, policy level balance can be used to design methods to control the agent's learning process, e.g. stop exploration after a budget is reached, or encourage more exploration if the agent converged to a sub-optimal solution; see Section 4. Lastly, generating trajectories resulting only from exploration or exploration grants experimenters insight over learning status.</p>
<p>Exploration Values</p>
<p>We propose to redefine the objective optimized by RL methods to incorporate both exploration and exploitation at its core. To do so, we consider the following expected balanced return for policy π:
D π (s, a) = E T,R,R e ,π [ ∞ i=0 γ i (r i + κr e i )],(6)
where we introduced exploration rewards r e i ∼ R e (s i , a i , s i+1 ) and parameter κ ∈ [0, ∞] governing exploration-exploitation balance. Note that we recover Equation 1 by setting κ to 0, hence disabling exploration.</p>
<p>Equation 6 can be further decomposed into
D π (s, a) = E T,R,π [ ∞ i=0 γ i r i ] + κE T,R e ,π <a href="7"> ∞ i=0 γ i r e i </a>
= Q π (s, a) + κU π (s, a),</p>
<p>where we have defined the exploration state-action value function U , akin to Q. Exploration behaviour is achieved by maximizing the expected discounted exploration return U . Note that, if r e depends on Q, then U is a function of Q. For clarity, we omit this potential dependency in notations. Bellman-type updates for both Q and U can be derived by unrolling the first term in both sums:
D π (s, a) = E R [r] + E a ,s |s,a [E T,R,π [ ∞ i=1 γ i r i ]] + κ(E R e [r e ] + E a ,s |s,a [E T,R e ,π [ ∞ i=1 γ i r e i ]]) (9) = E R [r] + γE a ,s |s,a [Q π (s , a )] + κ(E R e [r e ] + γE a ,s |s,a [U π (s , a )]).(10)
By identification we recover the update for Q given by Equation 3 and the following update for U :
U (s, a) = E R e [r e ] + γE s ,a |s,a [U (s , a )],(11)
which is similar to that of Q. Learning both U and Q can be seen as combining two agents to solve separate MPDs for goal reaching and exploration, as shown in Figure 1 (right). This formulation is general in that any reinforcement learning algorithm can be used to learn Q and U , combined with any exploration bonus. Both state-action value functions can be learned from transition data using existing RL algorithms.</p>
<p>Exploration Rewards</p>
<p>The presented formulation is independent from the choice of exploration rewards, hence many reward definitions from the intrinsic RL literature can directly be applied here.</p>
<p>Note that in the special case R e = 0 for all states and actions, we recover exploration values from DORA Fox et al. (2018), and if state and action spaces are discrete, we recover visitation counters Bellemare et al. (2016).</p>
<p>Another approach to define R e consists in considering the amount of exploration left at a given state. The exploration reward r e for a transition is then defined as the amount of exploration in the resulting state of the transition, to favour transitions that result in discovery. It can be computed by taking an expectation over all actions:
R e (s ) = E a ∼U (A) [σ(s , a )].(12)
We defined a function σ accounting for the uncertainty associated with a state-action pair. This formulation favours transitions that arrive at states of higher uncertainty. An obvious choice for σ is the variance of Q-values, to guide exploration towards parts of the state-action space where Q values are uncertain. This formulation is discussed in Section 5. Another choice for σ is to use visitation count or its continuous equivalent. Compared to classic visitation counts, this formulation focuses on visitations of the resulting transition state s instead of on the state-action pair of origin (s, a). Exploration rewards are often constrained to negative values so that by combining an optimistic model for U to negative rewards, optimism in the face of uncertainty guarantees efficient exploration Kearns and Singh (2002). The resulting model creates a gradient of U values; trajectories generated by following this gradient reach unexplored areas of the state-action space. With continuous actions, Equation 12 might not have closed form solution and the expectation can be estimated with approximate integration or sampling techniques. In domains with discrete actions however, the expectation is replaced by a sum over all possible actions.</p>
<p>Action Selection</p>
<p>Goal-only rewards are often defined as deterministic, as they simply reflect goal and penalty states. Because our framework handles exploration in a deterministic way, we simply focus on deterministic policies. Although state-action values Q are still non-stationary (because π is), they are learned from a stationary objective r. This makes learning policies for exploitation easier.</p>
<p>Following the definition in Equation 6, actions are selected to maximize the expected balanced return D π at a given state s:
π(s) = arg max a D π (s, a) = arg max a Q π (s, a) + κU π (s, a).(13)
Notice the similarity between the policy given in Equation 13 and UCB acquisition functions from the Bayesian optimization and bandits literature. No additional exploration term is needed, as this policy explicitly balances exploration and exploitation with parameter κ. This parameter can be tuned at any time to generate trajectories for pure exploration or exploitation, which can be useful to assess agent learning status. Furthermore, strategies can be devised to control κ manually or automatically during the learning process. We propose a few strategies in Section 4. The policy from Equation 13 can further be decomposed into present and future terms:
π(s) = arg max a E R [r] + κE R e [r e ] myopic +γE s |s,a [E a ∼π(s ) [Q π (s , a ) + κU π (s , a )] f uture ],(14)
Algorithm 1 Explicit Exploration-Exploitation π(s) = arg max a∈A Q(s, a) + κU (s, a)</p>
<p>6:</p>
<p>Execute a = π(s), observe s and r, and store s, a, r, s in D.</p>
<p>7:</p>
<p>Generate r e with Equation 12 for example.</p>
<p>8:</p>
<p>Update Q with Bellman eq. and r.</p>
<p>9:</p>
<p>Update U with Bellman eq. and r e .</p>
<p>10:</p>
<p>end for 11: end for where the term denoted future is effectively D π (s ). This decomposition highlights the link between this framework and other active learning methods; by setting γ to 0, only the myopic term remains, and we recover the traditional UCB acquisition function from bandits or Bayesian optimization. This decomposition can be seen as an extension of these techniques to a non-myopic setting. Indeed, future discounted exploration and exploitation are also considered within the action selection process. Drawing this connection opens up new avenues for leveraging exploration techniques from the bandits literature.</p>
<p>The method presented in this section for explicitly balancing exploration and exploitation at a policy level is concisely summed up in Algorithm 1. The method is general enough so that it allows learning both Q and U with any RL algorithm, and does not make assumptions on the choice of exploration reward used. Section 5 presents a practical method implementing this framework, while the next section presents advantages and strategies for controlling exploration balance during the agent learning process.</p>
<p>Preliminary Experiments on Classic RL Problems</p>
<p>In this section, a series of preliminary experiments on goal-only classic RL domains is presented to highlight the advantages of exploration values over additive rewards. Strategies for taking advantage of variable exploration rates are then provided.</p>
<p>The comparisons make use of goal-only version of simple and fully discrete domains. We compare all methods using strictly the same learning algorithm and reward bonuses.  (2000) is also adapted to the goal-only setting. This domain features a 5 × 5 grid-word with walls and four special locations. In each episode, the agent starts randomly and two of the special locations are denoted as passenger and destination. The goal is for the agent to move to the passenger's location, pick-up the passenger, drive it to the destination, and drop it off. A unit reward is given for dropping-off the passenger to the destination (ending the episode), and −0.1 rewards are given for actions pick-up and drop-off in wrong locations.</p>
<p>Analyzing the Advantages of an Explicit Exploration-Exploitation Balance</p>
<p>We first present simple pathological cases in which using exploration values provides advantages over additive rewards for exploration, on the Cliff Walking domain.</p>
<p>The first two experiments show that exploration values allow for direct control over exploration such stopping and continuing exploration. Stopping exploration after a budget is reached is simulated by setting exploration parameters (eg. κ) to 0 after 30 episodes and stopping model learning. While exploration values maintain high performance after exploration stops, returns achieved with additive rewards dramatically drop and yield a degenerative policy. When exploration is enabled once again, the two methods continue improving at a similar rate; see Figures 2a and 2b. Note that when exploration is disabled, there is a jump in returns with exploration values, as performance for pure exploitation is evaluated. However, it is never possible to be sure a policy is generated from pure exploitation when using additive rewards, as parts of bonus exploration rewards are encoded within learned Q-values. The third experiment demonstrates that stochastic transitions with higher probability of random action (p = 0.1) lead to increased return variance and poor performance with additive rewards, while exploration values only seem mildly affected. As shown in Figure 2c, even -greedy appears to solve the task, suggesting stochastic transitions provide additional random exploration. It is unclear why the additive rewards method is affected negatively.</p>
<p>Lastly, the fourth experiments shows environment reward magnitude is paramount to achieving good performance with additive rewards; see Figure 2d. Even though exploration parameters balancing environment and exploration bonus rewards are scaled to maintain equal amplitude between the two terms, additive rewards suffer from degraded performance. This is due to two reward quantities being incorporated into a single model for Q, which also needs to be initialized optimistically with respect to both quantities. When the two types of rewards have different amplitude, this causes a problem. Exploration values do not suffer from this drawback as separate models are learned based on these two quantities, hence resulting in unchanged performance.</p>
<p>Automatic Control of Exploration-Exploitation Balance</p>
<p>We now present strategies for automatically controlling the exploration-exploitation balance during the learning process. The following experiments also make use of the Taxi domain.</p>
<p>Exploration parameter κ is decreased over time according to the following schedule κ(t) = 1 1+ct , where c governs decay rate. Higher values of c result in reduced exploration after only a few episodes, whereas lower values translate to almost constant exploration. Results displayed in Figure 3 indicate that decreasing exploration leads to fast convergence</p>
<p>Method</p>
<p>Times target reached Episodes to target Performance after target -greedy 0/10 --Exploration values 9/10 111.33 0.08(4.50) Additive rewards 9/10 242.11 -33.26(67.41)  to returns relatively close to maximum return, as shown when setting c = 10 5 . However choosing a reasonable value c = 0.1 first results in lower performance, but enables finding a policy with higher returns later. Such behaviour is more visible with very small values such as c = 10 −3 which corresponds to almost constant κ.</p>
<p>We now show how direct control over exploration parameter κ can be taken advantage of to stop learning automatically once a predefined target is met. On the taxi domain, exploration is first stopped after an exploration budget is exhausted. Results comparing additive rewards to exploration values for different budgets of 100, 300 and 500 episodes are given in Figure 4a. These clearly show that when stopping exploration after the budget is reached, exploration value agents can generate purely exploiting trajectories achieving near optimal return whereas additive reward agents fail to converge on an acceptable policy.</p>
<p>Lastly, we investigate stopping exploration automatically once a target return is reached. After each learning episode, 5 test episodes with pure exploitation are run to score the current policy. If all 5 test episodes score returns above 0.1, the target return is reached and exploration stops. Results for this experiment are shown in Figure 4b and Table 1. Compared to additive rewards, exploration values display better performance after target is reach as well as faster target reaching.</p>
<p>Exploration values were experimentally shown exploration advantages over additive reward on simple RL domains. The next section presents an algorithm built on the proposed framework which extends to fully continuous state and action spaces and is applicable to more advanced problems.</p>
<p>EMU-Q: Exploration by Minimizing Uncertainty of Q Values</p>
<p>Following the framework defined in Section 3, we propose learning exploration values with a specific reward driving trajectories towards areas of the state-action space where the agent's uncertainty of Q values is high.</p>
<p>Reward Definition</p>
<p>Modelling Q-values with a probabilistic model gives access to variance information representing model uncertainty in expected discounted returns. Because the probabilistic model is learned from expected discounted returns, discounted return variance is not considered. Hence the model variance only reflects epistemic uncertainty, which can be used to drive exploration. This formulation was explored in EMU-Q Morere and Ramos (2018), extending Equation 12 as follows:
R e (s ) = E a ∼U (A) [V[Q(s , a )]] − V max ,(15)
where V max is the maximum possible variance of Q, guaranteeing always negative rewards. In practice, V max depends on the model used to learn Q and its hyper-parameters, and can often be computed analytically. In Equation 15, the variance operator V computes the epistemic uncertainty of Q values, that is it assesses how confident the model is that it can predict Q values correctly. Note that the MDP stochasticity emerging from transitions, rewards and policy is absorbed by the expectation operator in Equation 2, and so no assumptions are required on the MDP components in this reward definition.</p>
<p>Bayesian Linear Regression for Q-Learning</p>
<p>We now seek to obtain a model-free RL algorithm able to explore with few environment interactions, and providing a full predictive distribution on state-action values to fit the exploration reward definition given by Equation 15. Kernel methods such as Gaussian Process TD (GPTD) Engel et al. (2005) and Least-Squares TD (LSTD) Lagoudakis and Parr (2003) are among the most data-efficient model-free techniques. While the former suffers from prohibitive computation requirements, the latter offers an appealing trade-off between data-efficiency and complexity. We now derive a Bayesian RL algorithm that combines the strengths of both kernel methods and LSTD. The distribution of long-term discounted exploitation returns G can be defined recursively as:</p>
<p>G(s, a) = R(s, a, s ) + γG(s , a ),</p>
<p>which is an equality in the distributions of the two sides of the equation. Note that so far, no assumption are made on the nature of the distribution of returns. Let us decompose the discounted return G into its mean Q and a random zero-mean residual q so that Q </p>
<p>The only extrinsic uncertainty left in this equation are the reward distribution R and residuals q. Assuming rewards are disturbed by zero-mean Gaussian noise implies the difference of residuals is Gaussian with zero-mean and precision β. By modelling Q as a linear function of a feature map φ s,a so that Q(s, a) = w T φ s,a , estimation of state-action values becomes a linear regression problem of target t and weights w. The likelihood function takes the form
p(t|x, w, β) = N i=1 N (t i |r i + γw T φ s i ,a i , β −1 ),(18)
where independent transitions are denoted as x i = (s i , a i , r i , s i , a i ). We now treat the weights as random variables with zero-mean Gaussian prior p(w) = N (w|0, α −1 I). The weight posterior distribution is
p(w|t) = N (w|m Q , S) (19) m Q = βSΦ T s,a (r + γQ ) (20) S = (αI + βΦ T s,a Φ s,a ) −1 ,(21)
where
Φ s,a = {φ s i ,a i } N i=1 , Q = {Q(s i , a i )} N i=1 , and r = {r i } N i=1 .
The predictive distribution is also Gaussian, yielding
Q(s, a) = E[p(t|x, t, α, β)] = φ T s,a m Q ,(22)
and
V[p(t|x, t, α, β)] = β −1 φ T s,a Sφ s,a .(23)
The predictive variance V[p(t|x, t, α, β)] encodes the intrinsic uncertainty in Q(s, a), due to the subjective understanding of the MDP's model; it is used to compute r e in Equation 15. The derivation for U is similar, replacing r with r e and t with t e = R e (s, a, s ) + γU (s , a ). Note that because S does not depend on rewards, it can be shared by both models. Hence,
with U = {U (s i , a i )} N i=1 , U (s, a) = φ T s,a m U , with m U = βSΦ T s,a (r e + γU ).(24)
This model gracefully adapts to iterative updates at each step, by substituting the current prior with the previous posterior. Furthermore, the Sherman-Morrison equality is used to compute rank-1 updates of matrix S with each new data point φ s,a :
S t+1 = S t − β (S t φ s,a )(φ T s,a S t ) 1 + βφ T s,a S t φ s,a(25)
This update only requires a matrix-to-vector multiplication and saves the cost of inverting a matrix at every step. Hence the complexity cost is reduced from O(M 3 ) to O(M 2 ) in the number of features M . An optimized implementation of EMU-Q is given in algorithm 2. End of episode updates for m Q and m U (line 15 onward) are analogous to policy iteration, and although not mandatory, greatly improve convergence speed. Note that because r e is a non-stationary target, recomputing it after each episode with the updated posterior on Q provides the model on U with more accurate targets, thereby improving learning speed.</p>
<p>Algorithm 2 EMU-Q 1: Input: initial state s, parameters α, β, κ. 2: Output: Policy π paramatrized by m Q and m U . 3: Initialize S = α −1 I, t Q = t U = m Q = m U = 0 4: for episode l = 1, 2, .. do 5:</p>
<p>for step h = 1, 2, .. do 6: π(s) = arg max a φ s,a m Q + κφ s,a m U</p>
<p>7:</p>
<p>Execute a = π(s), observe s and r, and store φ s,a , r, s in D.</p>
<p>8:</p>
<p>Generate r e from Equation 15 with s . 9:
S = S − β (Sφs,a)(φ T s,a S) 1+βφ T
s,a Sφs,a 10:
t Q = t Q + βφ T s,a (r + γφ s,a m Q ) 11:
t U = t U + βφ T s,a (r e + γφ s,a m U ).</p>
<p>12:
m Q = St Q , m U = St U</p>
<p>13:</p>
<p>end for</p>
<p>14:</p>
<p>From D, draw Φ s,a , r, s , and compute Φ s ,π(s ) .</p>
<p>15:</p>
<p>Update m Q = βSΦ T s,a (r + γΦ s ,π(s ) m Q ) until change in m Q &lt; .</p>
<p>16:</p>
<p>Compute r e with Equation 15 and s .</p>
<p>17:</p>
<p>Update m U = βSΦ T s,a (r e + γΦ s ,π(s ) m U ) until change in m U &lt; . 18: end for</p>
<p>Kernel Approximation Features for RL</p>
<p>We presented a simple method to learn Q and U as linear functions of states-actions features. While powerful when using a good feature map, linear models typically require experimenters to define meaningful features on a problem specific basis. In this section, we introduce random Fourier features (RFF) Rahimi and Recht (2008), a kernel approximation technique which allows linear models to enjoy the expressivity of kernel methods. It should be noted that these features are different from Fourier basis Konidaris et al. (2011) (detailed in supplementary material), which do not approximate kernel functions. Although RFF were recently used to learn policy parametrizations Rajeswaran et al. (2017), to the best of our knowledge, this is the first time RFF are applied to the value function approximation problem in RL.</p>
<p>For any shift invariant kernel, which can be written as k(τ ) with τ = x − x , a representation based on the Fourier transform can be computed with Bochner's theorem Gihman and Skorohod (1974). Theorem 1 (Bochner's Theorem) Any shift invariant kernel k(τ ), τ ∈ R D , with a positive finite measure dµ(ω) can be represented in terms of its Fourier transform as
k(τ ) = R D e −iωτ dµ(ω).(26)
Assuming measure µ has a density p(ω), p is the spectral density of k and we have where p is the spectral density of k, φ(x) is an approximate feature map, and M the number of spectral samples from p. In practice, the feature map approximating k(x, x ) is
k(τ ) = R D e −iτ ω p(ω)dω ≈ 1 M M j=1 e −iτ ω j = φ(x), φ(x ) ,(27)φ(x) = 1 √ M [cos(x T ω 1 ), ..., cos(x T ω M ), sin(x T ω 1 ), ..., sin(x T ω M )],(28)
where the imaginary part was set to zero, as required for real kernels. In the case of the RBF kernel defined as k(x, x ) = exp(− 1 2σ 2 ||x − x || 2 2 ), the kernel spectral density is Gaussian p = N (0, 2σ −2 I). Feature maps can be computed by drawing M/2 × d samples from p one time only, and computing Equation 28 on new inputs x using these samples. Resulting features are not domain specific and require no feature engineering. Users only need to choose a kernel that represents adequate distance measures in the state-action space, and can benefit from numerous kernels already provided by the literature. Using these features in conjunction with Bayesian linear regression provides an efficient method to approximate a Gaussian process.</p>
<p>As the number of features increases, kernel approximation error decreases Sutherland and Schneider (2015); approximating popular shift-invariant kernels to within can be achieved with only M = O(d −2 log 1 2 ) features. Additionally, sampling frequencies according to a quasi-random sampling scheme (used in our experiments) reduces kernel approximation error compared to classic Monte-Carlo sampling with the same number of features Yang et al. (2014).</p>
<p>EMU-Q with RFF combines the ease-of-use and expressivity of kernel methods brought by RFF with the convergence properties and speed of linear models.</p>
<p>Comparison of Random Fourier Features and Fourier Basis Features</p>
<p>For completeness, a comparison between RFF and the better known Fourier Basis Features Konidaris et al. (2011) is provided on classic RL domains using Q-learning. A short overview on Fourier Basis Features is given in Appendix A.</p>
<p>Three relatively simple environments were considered: SinglePendulum, MountainCar and DoublePendulum (details on these environments are given in Section 6). The same Q-learning algorithm was used for both methods, with equal parameters. As little as 300 random Fourier features are sufficient in these domains, while the order of Fourier basis was set to 5 for SinglePendulum and MountainCar and to 3 for DoublePendulum. The higher state and action space dimensions of DoublePendulum make using Fourier basis features prohibitively expensive, as the number of generated features increases exponentially with space dimensions. For example, in DoublePendulum, Fourier basis features of order 3 leads to more than 2000 features.</p>
<p>Results displayed in Figure 5 show RFF outperforms Fourier basis both in terms of learning speed and asymptotic performance, while using a lower number of features. In DoublePendulum, the number of Fourier basis features seems insufficient to solve the problem, even though it is an order of magnitude higher than that of RFF.</p>
<p>Experiments</p>
<p>EMU-Q's exploration performance is qualitatively and quantitatively evaluated on a toy chain MDP example, 7 widely-used continuous control domains and a robotic manipulator problem. Experiments aim at measuring exploration capabilities in domains with goal-only rewards. Unless specified otherwise, domains feature one absorbing goal state with positive unit reward, and potential penalizing absorbing states of reward of −1. All other rewards are zero, resulting in very sparse reward functions, and rendering guidance from reward gradient information inapplicable.</p>
<p>Synthetic Chain Domain</p>
<p>We investigate EMU-Q's exploration capabilities on a classic domain known to be hard to explore. It is composed of a chain of N states and two actions, displayed in Figure 6a. Action right (dashed) has probability 1 − 1/N to move right and probability 1/N to move left. Action left (solid) is deterministic. Steps to goal (x &gt; 0.9), with policy refined after goal state was found (averaged over 30 runs).</p>
<p>Goal-only Rewards</p>
<p>We first consider the case of goal-only rewards, where goal state S N yields unit reward and all other transitions result in nil reward. Classic exploration such as -greedy was shown to have exponential regret with the number of states in this domain Osband et al. (2014). Achieving better performance on this domain is therefore essential to any advanced exploration technique. We compare EMU-Q to -greedy exploration for increasing chain lengths, in terms of number of steps before goal-state S N is found. Results in Figure 6b illustrate the exponential regret of -greedy while EMU-Q achieves much lower exploration time, scaling linearly with chain length.</p>
<p>Semi-Sparse Rewards</p>
<p>We now investigate the impact of reward structure by decreasing the chain domain's reward sparsity. In this experiment only, agents are given additional −1 rewards with probability 1 − p for every non-goal state, effectively guiding them towards the goal state (goal-only rewards are recovered for p = 0). The average number of steps before the goal is reached as a function of p is compared for -greedy and EMU-Q in Figure 6c. Results show that -greedy performs very poorly for high p, but improves as guiding reward density increases. Conversely, EMU-Q seems unaffected by reward density and performs equally well for all values of p. When p = 0, agents receive −1 reward in every non-goal state, and -greedy performs similarly to EMU-Q.</p>
<p>Classic Control</p>
<p>EMU-Q is further evaluated on more challenging RL domains. These feature fully continuous state and action spaces, and are adapted to the goal-only reward setting. In these standard control problems Brockman et al. (2016), classic exploration methods are unable to reach goal states.</p>
<p>Exploration Behaviour on goal-only MountainCar</p>
<p>We first provide intuition behind what EMU-Q learns and illustrate its typical behaviour on a continuous goal-only version of MountainCar. In this domain, the agent needs to drive an under-actuated car up a hill by building momentum. The state space consists of car position and velocity, and actions ranging from −1 to 1 describing car wheel torque (absolute value) and direction (sign). The agent is granted a unit reward for reaching the top of the right hill, and zero elsewhere. Figure 7 displays the state-action exploration value function U at different stages of learning, overlaid by the state-space trajectories followed during learning. The first episode (yellow line) exemplifies action babbling, and the car does not exit the valley (around x = 0.4). On the next episode (black line), the agent finds sequences of actions that allow exiting the valley and exploring further areas of the state-action space. Lastly, in episode three (white line), the agent finds the goal (x &gt; 0.9). This is done by adopting a strategy that quickly leads to unexplored areas, as shown by the increased gap between white lines. The exploration value function U reflects high uncertainty about unexplored areas (yellow), which shrink as more data is gathered, and low and decreasing uncertainty for often visited areas such as starting states (purple). Function U also features a gradient which can be followed from any state to find new areas of the state-action space to explore. Figure 7d shows EMU-Q's exploration capabilities enables to find the goal state within one or two episodes.</p>
<p>Continuous control benchmark</p>
<p>We now compare our algorithm on the complete benchmark of 7 continuous control goal-only tasks. All domains make use of OpenAI Gym Brockman et al. (2016), and are modified to feature goal only rewards and continuous state-action spaces with dimensions detailed in Table 2. More specifically, the domains considered are MountainCar and the following:</p>
<p>• SinglePendulum: The agent needs to balance an under-actuated pendulum upwards by controlling a motor's torque a the base of the pendulum. A unit reward is granted when the pole (of angle with vertical θ) is upwards: θ &lt; 0.05 rad.</p>
<p>• DoublePendulum: Similarly to SinglePendulum, the agent's goal is to balance a double pendulum upwards. Only the base joint can be controlled while the joint between the two segments moves freely. The agent is given a unit reward when the tip of the pendulum is close to the tallest point it can reach: within a distance d &lt; 1.</p>
<p>• CartpoleSwingUp: This domain features a single pole mounted on a cart. The goal is to balance the pole upwards by controlling the torque of the under-actuated cart's wheels. Driving the cart too far off the centre (|x| &gt; 2.4) results in episode failure with reward −1, and managing to balance the pole (cos(θ) &gt; 0.8, with θ the pole angle with the vertical axis) yields unit reward. Note that contrary to classic Cartpole, this domain starts with the pole hanging down and episodes terminate when balance is achieved.</p>
<p>• LunarLander: The agent controls a landing pod by applying lateral and vertical thrust, which needs to be landed on a designated platform. A positive unit reward is given for  reaching the landing pad within distance d &lt; 0.05 of its center point, and a negative unit reward is given for crashing or exiting the flying area.
Domain d S d A l S l A M α β
• Reacher: A robotic manipulator composed of two segments can be actuated at each of its two joints to reach a predefined position in a two-dimensional space. Bringing the manipulator tip within a distance d &lt; 0.015 of a random target results in a unit reward.</p>
<p>• Hopper: This domain features a single leg robot composed of three segments, which needs to propel itself to a predefined height. A unit reward is given for successfully jumping to height h &gt; 1.3, and a negative unit reward when the leg falls past angle |θ| &gt; 0.2 with the vertical axis.</p>
<p>Most methods in the sparse rewards literature address domains with discrete states and/or action spaces, making it hard to find baselines to compare EMU-Q to. Furthermore, classic exploration techniques such as -greedy fail on these domains. We compare our algorithm to three baselines: VIME, DORA and RFF-Q. VIME Houthooft et al. (2016) defines exploration as maximizing information gain about the agent's belief of environment dynamics. DORA Fox et al. (2018), which we run on discretized action spaces, extends visitation counts to continuous state spaces. Both VIME and DORA use additive rewards, as opposed to EMU-Q which uses exploration values. Q-Learning with -greedy exploration and RFF is denoted RFF-Q. Because it would fail in domains with goal-only rewards, it is run with classic rewards; see Brockman et al. (2016) for details on classic rewards.</p>
<p>We are interested in comparing exploration performance, favouring fast discovery of goal states. To reflect exploration performance, we measure the number of episodes required before the first positive goal-reward is obtained. This metric reflects how long pure exploration is required for before goal-reaching information can be taken advantage of to refine policies, and hence directly reflects exploration capabilities. Parameter γ is set to 0.99 for all domains and episodes are capped at 500 steps. State spaces are normalized, and Random Fourier Features approximating square exponential kernels are used for both state and action spaces with EMU-Q and RFF-Q. The state and action kernel lengthscales are denoted as l S and l A respectively. Exploration and exploitation trade-off parameter κ is set to 1/V max for all experiments. Other algorithm parameters were manually fixed to reasonable values given in Table 2.</p>
<p>Results displayed in Table 3 indicate that EMU-Q is more consistent than VIME or DORA in finding goal states on all domains, illustrating better exploration capabilities. The  Table 3: Results for all 7 domains, as success rate of goal finding within 100 episodes and mean (and standard deviation) of number of episodes before goal is found. Success rate rate is more important than number of episodes to goal. Results averaged over 20 runs. DORA was run with discretized actions, and RFF-Q with -greedy exploration on domains with classic rewards. EMU-Q's directed exploration yields higher performance on this task compared to RFF-Q with -greedy exploration.</p>
<p>average number of episodes to reach goal states is computed only on successful runs. EMU-Q displays better goal finding on lower dimension domains, while VIME tends to find goals faster on domains with higher dimensions but fails in more occasions. Observing similar results between EMU-Q and RFF-Q confirms that EMU-Q can deal with goal-only rewards without sacrificing performance.</p>
<p>Jaco Manipulator</p>
<p>In this final experiment, we show the applicability of EMU-Q to realistic problems by demonstrating its efficacy on an advanced robotics simulator. In this robotics problem, the agent needs to learn to control a Jaco manipulator solely from observing joint configuration. Given a position in the 3D space, the agent's goal is to bring the manipulator finger tips to this goal location by sending torque commands to each of the manipulator joints; see Figure 8a. Designing such target-reaching policies is also known as inverse kinematics for robotic arms, and has been studied extensively. Instead, we focus here on learning a mapping from joint configuration to joint torques on a damaged manipulator. When a manipulator is damaged, previously computed inverse kinematics are not valid anymore, thus being able to learn a new target-reaching policy is important. We model damage by immobilizing four of the arm joints, making previous inverse kinematics invalid. The target position is chosen randomly to form locations across the reachable space. Episodes terminate with unit reward when the target is reached within 50 steps, zero rewards are given otherwise. We compare EMU-Q and RFF-Q on this domain, both using 600 random Fourier features approximating an RBF kernel. Parameters α, β and κ were manually selected to acceptable values of 0.1, 1.0 and 0.1 respectively. Figure 8b displays results averaged over 10 runs. The difference in number of episodes solved shows EMU-Q learns and manages to complete the task more consistently than RFF-Q. This confirms that directed exploration is beneficial, even in more realistic robotics scenario.</p>
<p>Conclusion</p>
<p>We proposed a novel framework for exploration in RL domains with very sparse or goalonly rewards. The framework makes use of multi-objective RL to define exploration and exploitation as two key objectives, bringing the balance between the two at a policy level. This formulation has several advantages over traditional exploration methods. It allows direct and online control over exploration, without additional computation or training. Strategies for such control were shown to experimentally outperform classic intrinsic RL on several aspects. We demonstrated scalability to continuous state-action spaces by presenting EMU-Q, a method based on our framework, guiding exploration towards regions of higher valuefunction uncertainty. EMU-Q was experimentally shown to outperform classic exploration techniques and other intrinsic RL methods on a continuous control benchmark and on a robotic manipulator.</p>
<p>As future work, we would like to investigate how exploration as multi-objective RL can be brought to other types of RL methods such as policy gradient. This extension would enable control over exploration in domains with larger state-action spaces, an potentially numerous real-life application. Other interesting extensions include bringing the online control over exploration achieved by this work to life-long RL, where it would be beneficial. Indeed, exploration can be tuned down in critical situations where high performance is necessary, or increased when learning new behaviours is required.</p>
<p>Appendix A. Fourier Basis Features</p>
<p>Fourier basis features are described in Konidaris et al. (2011) as a linear function approximation based on Fourier series decomposition. Formally, the order-n feature map for state s is defined as follows: φ(s) = cos(πs T C),</p>
<p>where C is the Cartesian product of all c j ∈ {0, ..., n} for j = 1, .., d S . Note that Fourier basis features do not scale well. Indeed, the number of features generated is exponential with state space dimension. While Fourier basis features approximate value functions with periodic basis functions, random Fourier features are designed to approximate a kernel function with similar basis functions. As such, they allow recovering properties of kernel methods in the limit of the number of features. Additionally, random Fourier features scale better with higher dimensions.</p>
<p>Appendix B. Derivation of Bayesian linear regression for Q-learning</p>
<p>The likelihood function is defined as follows
p(t|x, w, β) = N i=1 N (t i |r i + γw T φ s i ,a i , β −1 ),(30)
where independent transitions are denoted x i = (s i , a i , r i , s i , a i ). we treat the linear regression weights w as random variables and introduce a Gaussian prior p(w) = N (w|0, α −1 I)</p>
<p>The weight posterior can be computed analytically with Bayes rule, resulting in a normal distribution
p(w|t) = p(t|w)p(w) p(t) = N (w|m Q , S)(32)
Expressions for the mean m Q and variance S follow from general results of products of normal distributions Bishop (2006):
m Q = βSΦ T s,a (r + γQ )(33)
S = (αI + βΦ T s,a Φ s,a ) −1 ,</p>
<p>where Φ s,a = {φ s i ,a i } N i=1 , Q = {Q(s i , a i )} N i=1 , and r = {r i } N i=1 . The predictive distribution p(t|x, t, α, β) can be obtain by weight marginalization and is also normal p(t|x, t, α, β) = p(t|w, β)p(w|x, t, α, β)dw = N (t|Q, σ 2 )</p>
<p>Expressions for Q and σ 2 follow from general results Bishop (2006), yielding Q(s, a) = E[p(t|x, t, α, β)] = φ T s,a m Q ,</p>
<p>σ 2 (s, a) = V[p(t|x, t, α, β)] = β −1 + φ T s,a Sφ s,a .</p>
<p>Figure 1 :
1Left: classic intrinsic exploration setup as proposed in</p>
<p>Figure 2 :
2Learning algorithms are tabular implementations of Q-Learning with learning rate fixed to 0.1. Reward bonuses are computed from a table of state-action visitation counts, where experiencing a state-action pair for the first time grants 0 reward and revisiting yields −1 reward. We denote by additive reward a learning algorithm where reward bonuses are used as in classic intrinsic RL (Equation 4), and by exploration values reward bonuses used as in the proposed framework (Equation 11 and action selection defined by Equation 13). A Q-learning agent with no reward bonuses and -greedy exploration is displayed as a baseline. Problem 1: The Cliff Walking domain Sutton et al. (1998) is adapted to the goal-only setting: negative unit rewards are given for falling off the cliff (triggering agent teleportation to starting state), and positive unit rewards for reaching the terminal goal state. Transitions Mean return and variance of 100 runs of the Cliff Walking domain with goal-only rewards. (a) Stopping exploration after 30 episodes. (b) Stopping exploration after 20 episodes and continuing exploration after another 10 episodes. (c) Stochastic transitions with 0.1 probability of random action. (d) Rewards are scaled by 100, and exploration parameters are also scaled to keep an equal magnitude between exploration and exploitation terms. allow the agent to move in four cardinal directions, where a random direction is chosen with low probability 0.01. Problem 2: The traditional Taxi domain Dietterich</p>
<p>Figure 3 :
3Decreasing exploration parameter over time to control exploration level on sparse Cliff Walking (a) and sparse Taxi (b) domains. Results show return mean and variance of a 100 runs.</p>
<p>Figure 4 :
4Taxi domain with goal-only rewards. (a) Exploration stops after a fixed budget of episodes b ∈ {100, 300, 500} is exhausted. (b) Exploration stops after a test target return of 0.1 is reached on 5 consecutive runs. Results show return mean and variance of a 100 runs.</p>
<p>(s, a) = E[G(s, a)]. Substituting and rearranging Equation 16 yields R(s, a, s ) + γQ(s , a ) t = Q(s, a) + q(s, a) − γq(s , a ) .</p>
<p>Figure 5 :
5Return mean and standard deviation for Q-learning with random Fourier features (RFF) or Fourier basis features on SinglePendulum (a), MountainCar (b), and DoublePendulum (c) domain with classic rewards. Results are computed using classic Q-learning with -greedy policy, and averaged over 20 runs for each method.</p>
<p>Figure 6 :
6(a) Chain domain described inOsband et al. (2014). (b)  Steps to goal (mean and standard deviation) in chain domain, for increasing chain lengths, averaged over 30 runs. (c) Steps to goal in semi-sparse 10-state chain, as a function of reward sparsity, with maximum of 1000 steps (averaged over 100 runs).</p>
<p>Figure 7 :
7Goal-only MountainCar. (a,b,c) Exploration value function U (for action 0) after 1, 2, and 3 episodes. State trajectories for 3 episodes are plain lines (yellow, black and white respectively). (d)</p>
<p>Figure 8 :
8(a) Manipulator task: learning to reach a randomly located target (red ball). (b)</p>
<p>1 :
1Input: parameter κ. 2: Output: Policy π. 3: for episode l = 1, 2, .. do4: </p>
<p>for step h = 1, 2, .. do </p>
<p>5: </p>
<p>Table 1 :
1Stopping exploration after a target test return of 0.1 is reached on 5 consecutive episodes in the sparse Taxi domain. Results averaged over 100 runs.0 
100 
200 
300 
400 
500 
600 
Episode </p>
<p>100 </p>
<p>80 </p>
<p>60 </p>
<p>40 </p>
<p>20 </p>
<p>0 </p>
<p>Return 
-greedy 
Exploration values (b=100) 
Exploration values (b=300) 
Exploration values (b=500) 
Additive rewards (b=100) 
Additive rewards (b=300) 
Additive rewards (b=500) 
Max return </p>
<p>Table 2 :
2Experimental parameters for all 7 domains</p>
<p>Finite-time analysis of the multiarmed bandit problem. Machine learning. Peter Auer, Nicolo Cesa-Bianchi, Paul Fischer, Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 2002.</p>
<p>Efficient exploration through Bayesian deep Q-networks. Kamyar Azizzadenesheli, Emma Brunskill, Animashree Anandkumar, arXiv:1802.04412arXiv preprintKamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient explo- ration through Bayesian deep Q-networks. arXiv preprint arXiv:1802.04412, 2018.</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, Neural Information Processing Systems. Intrinsic Exploration as Multi-ObjectiveMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Neural Information Processing Systems, 2016. Intrinsic Exploration as Multi-Objective RL</p>
<p>A distributional perspective on reinforcement learning. Will Marc G Bellemare, Rémi Dabney, Munos, International Conference on Machine Learning. Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on rein- forcement learning. In International Conference on Machine Learning, 2017.</p>
<p>Pattern recognition and machine learning. M Christopher, Bishop, Technical reportChristopher M Bishop. Pattern recognition and machine learning. Technical report, 2006.</p>
<p>R-max-a general polynomial time algorithm for near-optimal reinforcement learning. I Ronen, Moshe Brafman, Tennenholtz, Journal of Machine Learning Research. Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 2002.</p>
<p>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. Eric Brochu, M Vlad, Nando De Cora, Freitas, arXiv:1012.2599arXiv preprintEric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010.</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, OpenAI Gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym, 2016.</p>
<p>Intrinsically motivated reinforcement learning. Nuttapong Chentanez, G Andrew, Barto, P Satinder, Singh, Advances in neural information processing systems. Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement learning. In Advances in neural information processing systems, 2005.</p>
<p>A statistical method for global optimization. D Dennis, Susan Cox, John, International Conference onSystems, Man and Cybernetics. Dennis D Cox and Susan John. A statistical method for global optimization. In International Conference onSystems, Man and Cybernetics, 1992.</p>
<p>Bayesian Q-learning. Richard Dearden, Nir Friedman, Stuart Russell, Association for the Advancement of Artificial Intelligence. Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian Q-learning. In Association for the Advancement of Artificial Intelligence, 1998.</p>
<p>Hierarchical reinforcement learning with the MAXQ value function decomposition. G Thomas, Dietterich, Journal of Artificial Intelligence Research. Thomas G Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artificial Intelligence Research, 2000.</p>
<p>Reinforcement learning with Gaussian processes. Yaakov Engel, Shie Mannor, Ron Meir, International Conference on Machine Learning. Yaakov Engel, Shie Mannor, and Ron Meir. Reinforcement learning with Gaussian processes. In International Conference on Machine Learning, 2005.</p>
<p>DORA the explorer: Directed outreaching reinforcement action-selection. Lior Fox, Leshem Choshen, Yonatan Loewenstein, International Conference on Learning Representations. Lior Fox, Leshem Choshen, and Yonatan Loewenstein. DORA the explorer: Directed outreaching reinforcement action-selection. In International Conference on Learning Representations, 2018.</p>
<p>The theory of stochastic processes. I Gihman, Skorohod, I Gihman and A Skorohod. The theory of stochastic processes, vol. i, 1974.</p>
<p>Vime: Variational information maximizing exploration. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel, Neural Information Processing Systems. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Neural Information Processing Systems, 2016.</p>
<p>Near-optimal regret bounds for reinforcement learning. Thomas Jaksch, Ronald Ortner, Peter Auer, Journal of Machine Learning Research. Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforce- ment learning. Journal of Machine Learning Research, 2010.</p>
<p>Efficient global optimization of expensive black-box functions. Matthias Donald R Jones, William J Schonlau, Welch, Journal of Global optimization. Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 1998.</p>
<p>Reinforcement learning: A survey. Leslie Pack Kaelbling, Andrew W Michael L Littman, Moore, Journal of artificial intelligence research. Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 1996.</p>
<p>Near-optimal reinforcement learning in polynomial time. Michael Kearns, Satinder Singh, Machine Learning. Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 2002.</p>
<p>Value function approximation in reinforcement learning using the Fourier basis. George Konidaris, Sarah Osentoski, Philip S Thomas, Association for the Advancement of Artificial Intelligence. George Konidaris, Sarah Osentoski, and Philip S Thomas. Value function approximation in reinforcement learning using the Fourier basis. In Association for the Advancement of Artificial Intelligence, 2011.</p>
<p>Algorithms for multi-armed bandit problems. Volodymyr Kuleshov, Doina Precup, arXiv:1402.6028arXiv preprintVolodymyr Kuleshov and Doina Precup. Algorithms for multi-armed bandit problems. arXiv preprint arXiv:1402.6028, 2014.</p>
<p>Least-squares policy iteration. G Michail, Ronald Lagoudakis, Parr, Journal of Machine Learning Research. Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 2003.</p>
<p>Learning and exploration in actionperception loops. Daniel Ying-Jeh Little, Friedrich Tobias Sommer, Frontiers in neural circuits. Daniel Ying-Jeh Little and Friedrich Tobias Sommer. Learning and exploration in action- perception loops. Frontiers in neural circuits, 2013.</p>
<p>Exploration in model-based reinforcement learning by empirically estimating learning progress. Manuel Lopes, Tobias Lang, Marc Toussaint, Pierre-Yves Oudeyer, Neural Information Processing Systems. Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer. Exploration in model-based reinforcement learning by empirically estimating learning progress. In Neural Information Processing Systems, 2012.</p>
<p>Exploration of multi-state environments: Local measures and back-propagation of uncertainty. Nicolas Meuleau, Paul Bourgine, Machine Learning. Nicolas Meuleau and Paul Bourgine. Exploration of multi-state environments: Local measures and back-propagation of uncertainty. Machine Learning, 1999.</p>
<p>Bayesian RL for goal-only rewards. Philippe Morere, Fabio Ramos, Conference on Robot Learning. Philippe Morere and Fabio Ramos. Bayesian RL for goal-only rewards. In Conference on Robot Learning, 2018.</p>
<p>Nonparametric return distribution approximation for reinforcement learning. Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, Toshiyuki Tanaka, International Conference on Machine Learning. Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka. Nonparametric return distribution approximation for reinforcement learning. In International Conference on Machine Learning, 2010.</p>
<p>Policy invariance under reward transformations: Theory and application to reward shaping. Y Andrew, Daishi Ng, Stuart Harada, Russell, International Conference on Machine Learning. Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward trans- formations: Theory and application to reward shaping. In International Conference on Machine Learning, 1999.</p>
<p>Multi-resolution exploration in continuous spaces. Ali Nouri, Michael L Littman, Neural Information Processing Systems. Ali Nouri and Michael L Littman. Multi-resolution exploration in continuous spaces. In Neural Information Processing Systems, 2009.</p>
<p>Generalization and exploration via randomized value functions. Ian Osband, Zheng Benjamin Van Roy, Wen, arXiv:1402.0635arXiv preprintIan Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. arXiv preprint arXiv:1402.0635, 2014.</p>
<p>Deep exploration via bootstrapped DQN. Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, Advances in neural information processing systems. Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped DQN. In Advances in neural information processing systems, 2016.</p>
<p>How can we define intrinsic motivation. Pierre- , Yves Oudeyer, Frederic Kaplan, International Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems. Pierre-Yves Oudeyer and Frederic Kaplan. How can we define intrinsic motivation? In International Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems, 2008.</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, International Conference on Machine Learning. Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, 2017.</p>
<p>On finding compromise solutions in multiobjective Markov decision processes. Patrice Perny, Paul Weng, European Conference on Artificial Intelligence. Patrice Perny and Paul Weng. On finding compromise solutions in multiobjective Markov decision processes. In European Conference on Artificial Intelligence, 2010.</p>
<p>Random features for large-scale kernel machines. Ali Rahimi, Benjamin Recht, Neural Information Processing Systems. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Neural Information Processing Systems, 2008.</p>
<p>Towards generalization and simplicity in continuous control. Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, M Sham, Kakade, Neural Information Processing Systems. Aravind Rajeswaran, Kendall Lowrey, Emanuel V. Todorov, and Sham M Kakade. Towards generalization and simplicity in continuous control. In Neural Information Processing Systems, 2017.</p>
<p>Average reward optimization with multiple discounting reinforcement learners. Chris Reinke, Eiji Uchibe, Kenji Doya, International Conference on Neural Information Processing. Chris Reinke, Eiji Uchibe, and Kenji Doya. Average reward optimization with multiple discounting reinforcement learners. In International Conference on Neural Information Processing, 2017.</p>
<p>A survey of multi-objective sequential decision-making. M Diederik, Peter Roijers, Shimon Vamplew, Richard Whiteson, Dazeley, Journal of Artificial Intelligence Research. Diederik M. Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-objective sequential decision-making. Journal of Artificial Intelligence Research, 2013.</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International Conference on Machine Learning. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, 2015.</p>
<p>Incentivizing exploration in reinforcement learning with deep predictive models. C Bradly, Sergey Stadie, Pieter Levine, Abbeel, arXiv:1507.00814arXiv preprintBradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.</p>
<p>Reinforcement driven information acquisition in non-deterministic environments. Jan Storck, Sepp Hochreiter, Jürgen Schmidhuber, International Conference on Artificial Neural Networks. Jan Storck, Sepp Hochreiter, and Jürgen Schmidhuber. Reinforcement driven information acquisition in non-deterministic environments. In International Conference on Artificial Neural Networks, 1995.</p>
<p>On the error of random Fourier features. J Dougal, Jeff Sutherland, Schneider, Conference on Uncertainty in Artificial Intelligence. Dougal J Sutherland and Jeff Schneider. On the error of random Fourier features. In Conference on Uncertainty in Artificial Intelligence, 2015.</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998.</p>
<p>The many faces of optimism: A unifying approach. István Szita, András Lőrincz, International Conference on Machine Learning. István Szita and András Lőrincz. The many faces of optimism: A unifying approach. In International Conference on Machine Learning, 2008.</p>
<p>Using trajectory data to improve Bayesian optimization for reinforcement learning. Aaron Wilson, Alan Fern, Prasad Tadepalli, Journal of Machine Learning Research. Aaron Wilson, Alan Fern, and Prasad Tadepalli. Using trajectory data to improve Bayesian optimization for reinforcement learning. Journal of Machine Learning Research, 2014.</p>
<p>Quasi-Monte Carlo feature maps for shift-invariant kernels. Jiyan Yang, Vikas Sindhwani, Haim Avron, Michael Mahoney, International Conference on Machine Learning. Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael Mahoney. Quasi-Monte Carlo feature maps for shift-invariant kernels. In International Conference on Machine Learning, 2014.</p>            </div>
        </div>

    </div>
</body>
</html>
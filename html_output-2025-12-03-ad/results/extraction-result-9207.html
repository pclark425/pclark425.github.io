<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9207 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9207</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9207</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-272435886</p>
                <p><strong>Paper Title:</strong> Trajectory Anomaly Detection with Language Models</p>
                <p><strong>Paper Abstract:</strong> This paper presents a novel approach for trajectory anomaly detection using an autoregressive causal-attention model, termed LM-TAD. This method leverages the similarities between language statements and trajectories, both of which consist of ordered elements requiring coherence through external rules and contextual variations. By treating trajectories as sequences of tokens, our model learns the probability distributions over trajectories, enabling the identification of anomalous locations with high precision. We incorporate user-specific tokens to account for individual behavior patterns, enhancing anomaly detection tailored to user context. Our experiments demonstrate the effectiveness of LM-TAD on both synthetic and real-world datasets. In particular, the model outperforms existing methods on the Pattern of Life (PoL) dataset by detecting user-contextual anomalies and achieves competitive results on the Porto taxi dataset, highlighting its adaptability and robustness. Additionally, we introduce the use of perplexity and surprisal rate metrics for detecting outliers and pinpointing specific anomalous locations within trajectories. The LM-TAD framework supports various trajectory representations, including GPS coordinates, staypoints, and activity types, proving its versatility in handling diverse trajectory data. Moreover, our approach is well-suited for online trajectory anomaly detection, significantly reducing computational latency by caching key-value states of the attention mechanism, thereby avoiding repeated computations.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9207.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9207.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-TAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model for Trajectory Anomaly Detection (LM-TAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive causal-attention transformer that models trajectories as token sequences to compute per-location likelihoods; anomalies are detected via high perplexity and localized by surprisal rate, with optional user-specific tokens for personalization and KV-caching for online use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM-TAD (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (causal-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>ordered sequences of categorical tokens (discretized GPS cells, staypoint labels, activity tokens, dwell-time buckets)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>human mobility / trajectories (simulated Pattern-of-Life agents, Porto taxi GPS)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>outlier trajectories and anomalous locations within trajectories (user-contextual deviations, random-shift anomalies, detour anomalies)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train autoregressive transformer to predict next-location token given history (maximum likelihood). Compute sequence perplexity to score entire trajectories and surprisal rate (negative log-probability per token) to localize anomalous tokens; include optional USER_ID and weekday tokens to condition on user/context; support different tokenizations (hexagon/GPS cell, staypoint label, dwell-time buckets); enable online scoring by caching transformer key-value states (KV cache).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Deep sequence autoencoder (SAE), Variational sequence autoencoder (VSAE), Gaussian Mixture VSAE (GM-VSAE) (and other deep generative baselines cited)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision-Recall AUC (PR-AUC), F1 score; also use surprisal rate for localization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On Pattern-of-Life (agent-specific anomalies) LM-TAD achieved per-agent F1 scores roughly in the range ~0.61â€“0.87 and PR-AUC ~0.46â€“0.85 across the ten anomalous agents reported (LM-TAD substantially outperformed SAE/VSAE/GM-VSAE on these agent-contextual anomalies). On the Porto taxi dataset (synthetic random-shift and detour anomalies) LM-TAD achieved performance comparable to the best baseline (GM-VSAE) for random-shift anomalies (example top-level numbers reported include F1~0.85 and PR-AUC up to ~0.91 in some configurations) but was weaker than GM-VSAE at detecting small continuous detour anomalies in some settings; ablation on input modalities showed staypoint-label inputs gave best average results (PoL average F1=0.74, PR-AUC=0.70 for staypoint labels).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>LM-TAD outperformed autoencoder-based baselines (SAE, VSAE, GM-VSAE) on user-contextual anomalies in the Pattern-of-Life data by large margins (baselines often had near-zero F1/PR-AUC per-agent), and performed on par with or slightly worse than the best baseline on global GPS-based Porto anomalies depending on anomaly type (competitive for random-shift, less sensitive for small detour anomalies).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perplexity (aggregate sequence score) can dilute the signal of a few anomalous tokens making small/continuous detours harder to detect; LM-TAD showed reduced sensitivity to detour anomalies affecting a small continuous fraction of a trajectory because most token probabilities remain high and average perplexity stays low. Threshold selection for perplexity is dataset- and application-dependent. Model-size and pretraining details are not specified; detection may depend on tokenization/granularity. While trained on data containing anomalies (PoL), autoencoder baselines can overfit or fail in user-specific scenarios whereas LM-TAD requires careful conditioning (user tokens) and thresholding. No absolute latency numbers provided for online KV-cache gains.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Treating trajectories as language statements enables per-token likelihood estimation for both global anomaly scoring (perplexity) and fine-grained anomaly localization (surprisal rate); conditioning on user-specific tokens and weekday tokens allows detection of user-contextual anomalies without per-user isolated models; tokenization modality mattersâ€”staypoint labels and dwell time capture behavioral anomalies better than raw discretized GPS in agent-behavior datasets; KV-cache of transformer attention states enables efficient online sub-trajectory scoring without re-computing entire sub-trajectory outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Trajectory Anomaly Detection with Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9207.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9207.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer LM mentions (BERT / GPT-2 / LLaMA etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained transformer language models and transformer-based language modeling approaches (BERT, GPT-2, LLaMA and related transformer generative / masked LMs) referenced in related work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references general transformer LMs and prior works that apply language modeling techniques to mobility forecasting and next-location prediction, noting that prior work used these models for prediction tasks but not as a generative anomaly detector for trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT, GPT-2, LLaMA and other transformer LMs (mentioned collectively)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (masked and autoregressive variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>sequential mobility data / next-location prediction tasks (trajectories as sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>human mobility forecasting and next-location prediction (research citations in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>not applied to anomaly detection in the cited works as described (they were used for prediction/forecasting), but mentioned as related modeling technology</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned applications: prior studies used language-modeling techniques for next-location prediction and human mobility forecasting (sequence modeling), including BERT-inspired conceptual systems for trajectory analysis; however, the cited prior work did not apply a generative probability scoring approach for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>These transformer LMs are cited as successful in language and forecasting tasks; the paper notes none of the previous works used a generative approach (sequence likelihood/perplexity) for anomaly detection on trajectories prior to LM-TAD.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior language-modeling work on mobility focused on prediction/forecasting and not anomaly detection; no direct evaluation for anomaly detection reported in those citations within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>The paper motivates LM-TAD by analogy: language statements and trajectories share ordered-token structure, external-rule constraints, and user/style variations; existing transformer LMs and prior mobility-LM works demonstrate feasibility of sequence modeling but prior work had not exploited sequence likelihoods for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Trajectory Anomaly Detection with Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let's Speak Trajectories <em>(Rating: 2)</em></li>
                <li>Leveraging language foundation models for human mobility forecasting <em>(Rating: 2)</em></li>
                <li>How do you go where?: improving next location prediction by learning travel mode information using transformers <em>(Rating: 2)</em></li>
                <li>A BERT-inspired conceptual system tailored for trajectory analysis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9207",
    "paper_id": "paper-272435886",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "LM-TAD",
            "name_full": "Language Model for Trajectory Anomaly Detection (LM-TAD)",
            "brief_description": "An autoregressive causal-attention transformer that models trajectories as token sequences to compute per-location likelihoods; anomalies are detected via high perplexity and localized by surprisal rate, with optional user-specific tokens for personalization and KV-caching for online use.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LM-TAD (custom)",
            "model_type": "autoregressive transformer (causal-attention)",
            "model_size": null,
            "data_type": "ordered sequences of categorical tokens (discretized GPS cells, staypoint labels, activity tokens, dwell-time buckets)",
            "data_domain": "human mobility / trajectories (simulated Pattern-of-Life agents, Porto taxi GPS)",
            "anomaly_type": "outlier trajectories and anomalous locations within trajectories (user-contextual deviations, random-shift anomalies, detour anomalies)",
            "method_description": "Train autoregressive transformer to predict next-location token given history (maximum likelihood). Compute sequence perplexity to score entire trajectories and surprisal rate (negative log-probability per token) to localize anomalous tokens; include optional USER_ID and weekday tokens to condition on user/context; support different tokenizations (hexagon/GPS cell, staypoint label, dwell-time buckets); enable online scoring by caching transformer key-value states (KV cache).",
            "baseline_methods": "Deep sequence autoencoder (SAE), Variational sequence autoencoder (VSAE), Gaussian Mixture VSAE (GM-VSAE) (and other deep generative baselines cited)",
            "performance_metrics": "Precision-Recall AUC (PR-AUC), F1 score; also use surprisal rate for localization",
            "performance_results": "On Pattern-of-Life (agent-specific anomalies) LM-TAD achieved per-agent F1 scores roughly in the range ~0.61â€“0.87 and PR-AUC ~0.46â€“0.85 across the ten anomalous agents reported (LM-TAD substantially outperformed SAE/VSAE/GM-VSAE on these agent-contextual anomalies). On the Porto taxi dataset (synthetic random-shift and detour anomalies) LM-TAD achieved performance comparable to the best baseline (GM-VSAE) for random-shift anomalies (example top-level numbers reported include F1~0.85 and PR-AUC up to ~0.91 in some configurations) but was weaker than GM-VSAE at detecting small continuous detour anomalies in some settings; ablation on input modalities showed staypoint-label inputs gave best average results (PoL average F1=0.74, PR-AUC=0.70 for staypoint labels).",
            "comparison_to_baseline": "LM-TAD outperformed autoencoder-based baselines (SAE, VSAE, GM-VSAE) on user-contextual anomalies in the Pattern-of-Life data by large margins (baselines often had near-zero F1/PR-AUC per-agent), and performed on par with or slightly worse than the best baseline on global GPS-based Porto anomalies depending on anomaly type (competitive for random-shift, less sensitive for small detour anomalies).",
            "limitations_or_failure_cases": "Perplexity (aggregate sequence score) can dilute the signal of a few anomalous tokens making small/continuous detours harder to detect; LM-TAD showed reduced sensitivity to detour anomalies affecting a small continuous fraction of a trajectory because most token probabilities remain high and average perplexity stays low. Threshold selection for perplexity is dataset- and application-dependent. Model-size and pretraining details are not specified; detection may depend on tokenization/granularity. While trained on data containing anomalies (PoL), autoencoder baselines can overfit or fail in user-specific scenarios whereas LM-TAD requires careful conditioning (user tokens) and thresholding. No absolute latency numbers provided for online KV-cache gains.",
            "unique_insights": "Treating trajectories as language statements enables per-token likelihood estimation for both global anomaly scoring (perplexity) and fine-grained anomaly localization (surprisal rate); conditioning on user-specific tokens and weekday tokens allows detection of user-contextual anomalies without per-user isolated models; tokenization modality mattersâ€”staypoint labels and dwell time capture behavioral anomalies better than raw discretized GPS in agent-behavior datasets; KV-cache of transformer attention states enables efficient online sub-trajectory scoring without re-computing entire sub-trajectory outputs.",
            "uuid": "e9207.0",
            "source_info": {
                "paper_title": "Trajectory Anomaly Detection with Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Transformer LM mentions (BERT / GPT-2 / LLaMA etc.)",
            "name_full": "Pretrained transformer language models and transformer-based language modeling approaches (BERT, GPT-2, LLaMA and related transformer generative / masked LMs) referenced in related work",
            "brief_description": "The paper references general transformer LMs and prior works that apply language modeling techniques to mobility forecasting and next-location prediction, noting that prior work used these models for prediction tasks but not as a generative anomaly detector for trajectories.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BERT, GPT-2, LLaMA and other transformer LMs (mentioned collectively)",
            "model_type": "transformer (masked and autoregressive variants)",
            "model_size": null,
            "data_type": "sequential mobility data / next-location prediction tasks (trajectories as sequences)",
            "data_domain": "human mobility forecasting and next-location prediction (research citations in related work)",
            "anomaly_type": "not applied to anomaly detection in the cited works as described (they were used for prediction/forecasting), but mentioned as related modeling technology",
            "method_description": "Mentioned applications: prior studies used language-modeling techniques for next-location prediction and human mobility forecasting (sequence modeling), including BERT-inspired conceptual systems for trajectory analysis; however, the cited prior work did not apply a generative probability scoring approach for anomaly detection.",
            "baseline_methods": null,
            "performance_metrics": null,
            "performance_results": null,
            "comparison_to_baseline": "These transformer LMs are cited as successful in language and forecasting tasks; the paper notes none of the previous works used a generative approach (sequence likelihood/perplexity) for anomaly detection on trajectories prior to LM-TAD.",
            "limitations_or_failure_cases": "Prior language-modeling work on mobility focused on prediction/forecasting and not anomaly detection; no direct evaluation for anomaly detection reported in those citations within this paper.",
            "unique_insights": "The paper motivates LM-TAD by analogy: language statements and trajectories share ordered-token structure, external-rule constraints, and user/style variations; existing transformer LMs and prior mobility-LM works demonstrate feasibility of sequence modeling but prior work had not exploited sequence likelihoods for anomaly detection.",
            "uuid": "e9207.1",
            "source_info": {
                "paper_title": "Trajectory Anomaly Detection with Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let's Speak Trajectories",
            "rating": 2,
            "sanitized_title": "lets_speak_trajectories"
        },
        {
            "paper_title": "Leveraging language foundation models for human mobility forecasting",
            "rating": 2,
            "sanitized_title": "leveraging_language_foundation_models_for_human_mobility_forecasting"
        },
        {
            "paper_title": "How do you go where?: improving next location prediction by learning travel mode information using transformers",
            "rating": 2,
            "sanitized_title": "how_do_you_go_where_improving_next_location_prediction_by_learning_travel_mode_information_using_transformers"
        },
        {
            "paper_title": "A BERT-inspired conceptual system tailored for trajectory analysis",
            "rating": 1,
            "sanitized_title": "a_bertinspired_conceptual_system_tailored_for_trajectory_analysis"
        }
    ],
    "cost": 0.0104695,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Trajectory Anomaly Detection with Language Models
18 Sep 2024</p>
<p>Jonathan Kabala Mbuya 
Dieter Pfoser dpfoser@gmu.edu </p>
<p>George Mason University Fairfax
VirginiaUSA</p>
<p>George Mason University Fairfax
VirginiaUSA</p>
<p>George Mason University Fairfax
VirginiaUSA</p>
<p>New YorkNYUSA</p>
<p>Trajectory Anomaly Detection with Language Models
18 Sep 20240E88FDE056172C28C7C748F89ADCD61F10.1145/3678717.3691257arXiv:2409.15366v1[cs.LG]Anomalous TrajectoriesAnomaly DetectionTrajectory DataLanguage ModelingSelf-Supervised Learning
This paper presents a novel approach for trajectory anomaly detection using an autoregressive causal-attention model, termed LM-TAD.This method leverages the similarities between language statements and trajectories, both of which consist of ordered elements requiring coherence through external rules and contextual variations.By treating trajectories as sequences of tokens, our model learns the probability distributions over trajectories, enabling the identification of anomalous locations with high precision.We incorporate user-specific tokens to account for individual behavior patterns, enhancing anomaly detection tailored to user context.Our experiments demonstrate the effectiveness of LM-TAD on both synthetic and real-world datasets.In particular, the model outperforms existing methods on the Pattern of Life (PoL) dataset by detecting user-contextual anomalies and achieves competitive results on the Porto taxi dataset, highlighting its adaptability and robustness.Additionally, we introduce the use of perplexity and surprisal rate metrics for detecting outliers and pinpointing specific anomalous locations within trajectories.The LM-TAD framework supports various trajectory representations, including GPS coordinates, staypoints, and activity types, proving its versatility in handling diverse trajectory data.Moreover, our approach is well-suited for online trajectory anomaly detection, significantly reducing computational latency by caching key-value states of the attention mechanism, thereby avoiding repeated computations.The code to reproduce experiments in this paper can be found at the following link: https://github.com/jonathankabala/LMTAD.CCS CONCEPTSâ€¢ Computing methodologies â†’ Modeling and simulation.</p>
<p>INTRODUCTION</p>
<p>Figure 1: A conceptual visualization of trajectories as natural language statements.Language statements and trajectories share similarities: both consist of ordered elements from a finite set (words vs. GPS points) and require connections by semantic or spatiotemporal relationships to be coherent.They are governed by external rules (grammar for the language, road networks for trajectories) and vary by user or context (writing style vs. movement behavior).</p>
<p>Effective techniques for gathering and analyzing movement data, including the contribution of this work on anomaly detection are becoming increasingly important with the growth in terms of data and different types of applications.Specifically, trajectory anomaly detection has several interesting and practical use cases across various fields, such as Transportation and Traffic Analysis (accident detection, road safety analysis), Maritime Navigation and Safety (shipping lane monitoring, piracy detection), Air Traffic Control (airspace safety), Wildlife Monitoring (behavior change), Sports Analysis (injury prevention, game strategies), Healthcare and Elderly Care (behavior change and detecting health issues or emergencies), Disaster Response and Management (disaster response and crowd monitoring) and Urban Planning and Smart Cities (mobility analysis, public transit optimization, pedestrian safety).This work focuses on detecting trajectory anomalies that deviate from patterns observed in collections of historical datasets.</p>
<p>Extensive research has been conducted on trajectory anomaly detection for unlabeled data [6,12,17,29,36,38].However, this body of prior work has several limitations.</p>
<p>Firstly, it is difficult to pinpoint specific locations within the trajectory where the anomaly occurs, as the anomaly score is attributed to the entire trajectory or sub-trajectory.Secondly, these methods do not adequately consider anomalies in relation to the individual user's context.This is significant because different users exhibit distinct behavior patterns and what constitutes a normal pattern for one user might be deemed anomalous for another.Finally, anomaly detection has primarily focused on spatiotemporal trajectory data, i.e., GPS coordinates, but the concept of a trajectory can be more abstract.A trajectory can be a chronological sequence of qualitative staypoints, e.g., work â†’ restaurant â†’ apartment, for a particular user.Additionally, anomalies may not solely relate to the spatial properties of the data, but they can also involve the types of places a user visits, such as a restaurant or a shopping mall on specific days of the week or the duration spent at a particular location.</p>
<p>To address these issues, we propose a Language Model for Trajectory Anomaly Detection, (LM-TAD).The motivation for using a language modeling approach comes from the idea of modeling trajectories as statements [20]as illustrated in Figure 1.Language statements and trajectories share several similarities: 1) both consist of ordered elements from a finite set (words vs. GPS points and segments) and 2) both require coherence, meaning the elements must be connected by semantic (language) or spatial/temporal (trajectories) relationships.For example, in Figure 1, the word to cannot be followed by any random word in the vocabulary, just as a GPS point can only be followed by a limited set of other GPS coordinates.</p>
<p>3) Both are governed by external rules-grammar for language and road networks or physical constraints for trajectories.Grammar dictates sentence construction, while road networks dictate possible routes from point A to point B. In Figure 1, the paths from a source (S) to a destination (D) are constrained by the road network.4) Finally, the specific combination of elements (words vs. temporal sequence of locations) varies with the user or context.Similar to writing styles, users have different movement behaviors, determining the use of certain words or speed/mobility patterns, respectively.As illustrated in Figure 1, just as one can choose different words to convey the idea of going to the beach with friends, one can also select different trajectories to travel between a source (S) and destination (D).Based on these similarities, just as a language model can be trained to score the likelihood of sentences, we aim to train a model to score the probability of given trajectories and hence detect anomalous trajectories.</p>
<p>Our specific approach is using an autoregressive causal-attention model to learn the distributions over trajectories.We train the model by learning to predict the next location in a trajectory given a historical context.the trained model can compute the probability of generating a location (i.e., discretized GPS coordinate, staypoints, etc.) in a trajectory given its historical context, which in its simplest case is a location history.Anomalies are detected by identifying low-probability locations.To learn normal behavior for specific users, we can further condition the trajectory generation with a unique user token (i.e., USER_ID) and flag anomalies on a user basis accordingly.The language model uses discrete tokens and can handle different abstractions of a trajectory, such as discretized GPS coordinates using spatial partitions or qualitative staypoint information (i.e., "home, work, restaurant, and so forth").</p>
<p>To distinguish between normal and anomalous trajectories, we use perplexity, a well-established metric in natural language processing.Intuitively, perplexity can be viewed as a measure of uncertainty when predicting the next token (location) in a trajectory.We also use the surprisal rate of each location to identify anomalous locations to identify the location of an anomaly within a trajectory.</p>
<p>Our contributions can be summarized as follows:</p>
<p>â€¢ We propose a new way to detect anomalies in trajectory data by using an autoregressive causal-attention model.With this approach, we can 1) identify the location in the trajectory where the anomaly occurs, 2) find anomalies with respect to a user, and 3) handle various representations of a trajectory (e.g., GPS coordinates and staypoints) â€¢ We show the application of perplexity as a metric for identifying outlier trajectories, both in the context of the entire dataset and with respect to the trajectories of a specific user.Additionally, we illustrate using the surprisal rate to identify potential anomalous locations within a trajectory.â€¢ Our findings indicate that our method performs exceptionally well on the Pattern of Life dataset (PoL) [45], effectively identifying anomalous trajectories in the context of a user while training on all data, including anomalies.We also show that our approach is on par with state-of-the-art methods for trajectory anomaly detection when tested on the Porto dataset [34,35] using solely GPS coordinates.Furthermore, our approach is suitable for online anomaly detection as the trajectory is being generated.Unlike autoencoder methods that require the computation of the anomaly score for the entire sub-trajectory each time a new GPS coordinate is sampled, our method benefits from low latency by caching key-value (KV cache) states [16,24] of the attention mechanism for previously generated tokens (i.e., GPS coordinates), thereby avoiding repeated computations.The remainder of this paper is organized as follows.Section 2 discusses related work.Section 3 gives the basic formulation of the problem.In Section 4, we present our autoregressive generative approach to detect anomalies in trajectories.Section 5 provides an experimental evaluation that highlights the benefits of our method compared to existing approaches.Finally, Section 7 concludes and provides directions for future work.</p>
<p>RELATED WORK 2.1 Trajectory Anomaly Detection</p>
<p>Existing work for anomaly detection in trajectories can be grouped into two broad categories: heuristic-based methods [6,14,18,36,43] and learning-based methods [28,29].</p>
<p>Heuristic-based methods primarily rely on hand-crafted features to represent normal routes and employ distance or density metrics to compare a target route to normal routes.The study in [14] suggests a partition-and-detect framework for trajectory outlier detection, effectively identifying outlying sub-trajectories by combining two-level trajectory partitioning with a hybrid distancebased and density-based detection approach.Studies by [36] and [6] introduce related methods that systematically extract, group, and analyze trajectories based on the source and destination.These methods identify anomalies by how rare they are and how much they deviate from usual patterns, using the principle of isolation to ensure effective and reliable anomaly detection.Another research effort by [43] presents a time-dependent approach for detecting trajectory anomalies, employing edit distance metrics to ascertain whether a given target trajectory deviates significantly from historical normal trajectories.Similarly, [18] uses edit distance coupled with a density-based clustering algorithm to identify anomalous trajectories.Heuristic-based methods exhibit certain limitations.Primarily, the characterization of a trajectory is dependent on manually curated features encompassing various parameters, such as frequency, distance, or density thresholds, to flag a trajectory as anomalous.Furthermore, the construction of these features tends to be domain-specific, necessitating specialized expertise in the respective field.Additionally, the applicability of these methods across different regions is constrained owing to the inherent dissimilarities in trajectories across diverse geographical locations.</p>
<p>Learning-based methods rely on machine learning techniques.The work by [29] employs trajectory embedding learned by recurrent neural networks (RNN) to capture the sequential information and distinctive characteristics of trajectories to detect anomalies.However, the model requires labeled data, usually unavailable in real applications due to the cost of labeling a dataset.Several studies on unlabelled data have been proposed to overcome the limitations of using labeled data.[28] suggest a method combining Infinite Gaussian Mixture Models with bi-directional Generative Adversarial Networks to detect anomalies in trajectory data using a combination of the reconstruction loss and discriminator-based loss.Deep learning methods based on autoencoders have recently been applied to various anomaly detection tasks [8,19,41,44].These methods work by learning to compress and reconstruct the input.They use the premise that anomalous input will produce a significant reconstruction error, as they differ from the learned normal patterns.A recent study by [17] proposes an autoencoder method for online anomalous trajectory detection with multiple Gaussian components in the latent space to discover various types of normal routes.Outside of autoencoder methods, another recent study by [39] suggests a reinforcement learning-based solution for detecting anomalous trajectories and sub-trajectories.However, the methods have two main limitations.Firstly, these methods are limited in pinpointing the specific location of an anomaly within the trajectory, as they rely on an aggregate anomaly metric, typically the reconstruction error.Secondly, there is a notable lack of generalizability in these approaches to scenarios requiring user-specific anomaly identification, as what constitutes an anomaly for one user might be deemed normal for another.</p>
<p>Language Modeling on Trajectory Data</p>
<p>The field of language modeling has received much attention recently since the introduction of the transformer model [32].Language models like BERT [9], GPT-2 [26], and LLaMA [30] have been shown to achieve great performance on a variety of natural language tasks, including question-answering, sentiment analysis, and text generation.Language modeling techniques have been extended to other applications, including image classification [25,42] and speech processing [3,4].Recent studies have also applied language modeling techniques to a wide range of applications on mobility data.For example, the work in [33] leverages language modeling techniques for human mobility forecasting tasks, while the work in [13] uses similar techniques to predict the next visited location in a trajectory.The work in [20] proposes a conceptualization of a BERT-inspired system tailored for trajectory analysis.However, none of the previous work used a generative approach for anomaly detection in trajectory data.</p>
<p>PROBLEM FORMULATION</p>
<p>A trajectory is a finite chronological sequence of visited locations and can be modeled as a list of space-time points modeled as location and time stamp pairs  =  0 , . . .,   with   = âŸ¨  ,   âŸ© and   âˆˆ  2 ,   âˆˆ  + for  = 0, 1, . . .,  and  0 &lt;  1 &lt;  2 &lt; . . .&lt;   (cf.[23]).</p>
<p>In the simplest case, a location   is represented as a geographic coordinate in two-dimensional space.Other representations are to map locations to cells of a discretized space such as a regular spatial or a hexagonal grid [31].</p>
<p>Alternatively,   can capture qualitative staypoints (visited points of interest such as "home", "work", or "restaurant") or spatial partitions that capture functional areas of a city, e.g., "commercial", "business", or "residential" areas.Therefore,   can include both a staypoint and afunctional area, e.g.,   = [apartment, downtown]).</p>
<p>A collection of related trajectories   constitutes a dataset D. The dataset D may contain both normal and anomalous trajectories.In general, an anomalous trajectory refers to one that does not show a normal mobility pattern and deviates from the majority of the trajectories in D [7,38].Given a dataset D with  trajectories, our goal is to train a model that distinguishes between normal and anomalous trajectories without having explicit labels.</p>
<p>METHOD</p>
<p>Our approach is to train a model that learns probability distributions over trajectories.An autoregressive generative model will allow us to infer the probability of a trajectory given the historical context
ğ‘ƒ (ğ‘‡ ) = ğ‘ (ğ‘™ 1 )ğ‘ (ğ‘™ 2 |ğ‘™ 1 )ğ‘ (ğ‘™ 3 |ğ‘™ 1 ğ‘™ 2 ) . . . ğ‘ (ğ‘™ ğ‘– |ğ‘™ &lt;ğ‘– ) . . . ğ‘ (ğ‘™ ğ‘› |ğ‘™ &lt;ğ‘› )(1)
where the probability of each location  (  | &lt; ) is conditioned on a complete location history.We note that there is no time bound between locations.However, we could use such information as part of the input.With this approach, we can find anomalous trajectories and identify exactly which locations in the trajectory are anomalous.</p>
<p>Model and Architecture</p>
<p>Given a dataset of trajectories D = { 1 , . . .,  }, LM-TAD's goal is to maximize the likelihood of all the trajectories in the dataset:
L (D) = | D | âˆ‘ï¸ ğ‘–=1 |ğ‘‡ ğ‘– | âˆ‘ï¸ ğ‘—=1 log ğ‘ƒ (ğ‘™ ğ‘– |ğ‘™ &lt;ğ‘– ; ğœƒ )(2)
where  (â€¢;  ) is the conditional probability modeled by a neural network parameterized by  .To learn the parameters  , we opt for a transformer-based network architecture [32].This architecture choice is motivated by its proven efficacy in natural language generation tasks, suggesting its potential applicability and effectiveness in modeling trajectories as statements.</p>
<p>Figure 2 shows the overall architecture of our method, LM-TAD, which consists of positional and token embeddings,  transformer blocks followed by a linear transformation, and a softmax layer.</p>
<p>To capture input semantics, the token embedding layer transforms each token (location) from a categorical type to a finitedimensional real-valued vector.Positional embeddings play a critical role in the training process, compensating for the absence of inherent sequential ordering within the causal-attention module.Each transformer block comprises a multi-head causal-attention mechanism, which is preceded and succeeded by a layer-normalization layer and a feedforward layer.In the multi-head causal-attention mechanism, a trajectory is transformed into three sets of vectors -keys, values, and queries-and then split into multiple heads for parallel processing.Each head independently computes a scaled dot-product attention to get attention scores that assess the relevance of different locations (tokens) in a trajectory.This allows the model to concurrently learn dependencies between locations, such as temporal or spatial ones.The outputs from all heads are concatenated and linearly transformed to produce the final output.Additionally, the causal-attention mechanism includes a masking operation to prevent the attention function from accessing information from future tokens (locations), given the autoregressive nature of our approach.</p>
<p>Below is the formal description of the muti-head self-attention:
Attention(Q, K, V) = softmax QK ğ‘‡ âˆšï¸ ğ‘‘ ğ‘˜ V(3)
where   is the dimension of the keys.Concatenating the output values results in:
MultiHead(Q, K, V) = Concat(head 1 , ..., head â„ )W ğ‘œ with head ğ‘– = Attention(QW ğ‘„ ğ‘– , KW ğ¾ ğ‘– , VW ğ‘‰ ğ‘– )(4)
where the
W ğ‘„ ğ‘– âˆˆ R ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ Ã—ğ‘‘ ğ‘ , W ğ¾ ğ‘– âˆˆ R ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ Ã—ğ‘‘ ğ‘˜ , W ğ¾ ğ‘– âˆˆ R ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ Ã—ğ‘‘ ğ‘£
are projection matrices that are learned during training.The projection matrix W  linearly combines the outputs from different attention heads, enabling the model to flexibly adjust and fine-tune the aggregated attention, thereby enhancing the model's capacity to learn complex patterns.In LM-TAD,   =   =   =   /â„ where â„ is the number of heads.</p>
<p>The feedforward layer consists of two linear transformations linked with a ReLU activation function:
FFM(ğ‘¥) = max(0, xW 1 + b 1 )W 2 + b 2 (5)
where the weights
W 1 âˆˆ R ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ Ã—ğ‘‘ ğ‘“ ğ‘“ , W 2 âˆˆ R ğ‘‘ ğ‘“ ğ‘“ Ã—ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ and the biases b 1 âˆˆ R ğ‘‘ ğ‘“ ğ‘“ , b 2 âˆˆ R ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ .
The transformer block uses the layer-normalization layer in addition to residual connections to stabilize learning and improve training efficiency.The output of the transformer block goes through linear and softmax layers to predict the distribution of each token in a trajectory.</p>
<p>Location Configurations</p>
<p>An advantage of using a generative approach to model trajectories is the ability to abstract the locations of a trajectory in different ways.Figure 3 provides examples.In the simplest case, a trajectory can be represented by a finite chronological sequence of GPS coordinates.These coordinates can be discretized using regular grid cells (Figure 3a) [15] or hexagons [10].However, various other trajectory configurations are possible.Instead of GPS coordinates, we can also use staypoints ("home", "workplace", "restaurant", etc.) (Figure 3b) or points of interest.We can even model a trajectory as a chronological sequence of a person's activities ("eating", "working", and "playing sports") (Figure 3d), where each location in the trajectory corresponds to the activity of a person at a particular time.These trajectories can even be enhanced with additional metadata, such as the dwell time at a location (Figure 3c), method of transportation, or proximity to the previous location.An advantage of LM-TAD is its ability to work with any of these different trajectory configurations.</p>
<p>Anomaly Score</p>
<p>We use perplexity to determine how anomalous a trajectory is.Perplexity is a well-established measure to evaluate language models [5,9,22], and can be viewed as a measure of uncertainty when predicting the next token (location) in a trajectory [21].Equation 6shows how we can calculate the perplexity of a trajectory  with  locations, and Equation 7shows how we compute the perplexity over a dataset D with  trajectories   .
ğ‘ƒğ‘ƒğ¿(ğ‘‡ ) = exp âˆ’ 1 ğ‘› ğ‘¡ âˆ‘ï¸ ğ‘–=1 log ğ‘ƒ (ğ‘™ ğ‘– |ğ‘™ &lt;ğ‘– ) (6) ğ‘ƒğ‘ƒğ¿(D) = âˆ’ 1 ğ‘› ğ‘› âˆ‘ï¸ ğ‘–=1 ğ‘ƒğ‘ƒğ¿(ğ‘‡ ğ‘– )(7)
The lowest possible perplexity is 1, which implies that the model can correctly predict the next location with absolute certainty.However, the maximum of this measure is unbounded.To determine when a trajectory is anomalous ("high" perplexity), we need to provide a threshold.We note that the choice of a threshold can be application-and dataset-dependent [2].We can compute the threshold as follows:
threshold = mean[ğ‘ƒğ‘ƒğ¿(D)] + std[ğ‘ƒğ‘ƒğ¿(D)]
where mean[(D)] and std[(D)] are the mean and standard deviation of the perplexities of  training trajectories.To identify abnormal trajectories with respect to a specific user, we customize the threshold for each user.Here, the mean and standard deviation will be computed only using the training samples of that user.</p>
<p>EXPERIMENTS</p>
<p>This section presents the experimental setup used to evaluate the effectiveness of our proposed LM-TAD model.We compare our method to several state-of-the-art baselines using two different datasets and utilizing different evaluation metrics to measure the accuracy and robustness of anomaly detection.</p>
<p>Datasets &amp; Preprocessing</p>
<p>We use simulated and real-world datasets.Specifically, we use the Pattern-of-Life (PoL) simulation dataset [45] and the Porto taxi dataset [34,35].</p>
<p>Pattern-of-Life Dataset (PoL).</p>
<p>The PoL dataset was generated through the Pattern-of-Life (PoL) simulation [1,45].This simulation consists of virtual agents designed to emulate humans' needs and behavior by performing human-like activities.Activities include going to work, restaurants, and recreational activities with friends.These activities are performed at real locations obtained from OpenStreetMap [11].While agents engage in these activities, the simulation also records the location, which includes the GPS coordinates and the staypoints (i.e., home, work, restaurant), as well as the respective timestamps.</p>
<p>Using the raw data from the PoL simulation, we created daily trajectories for each agent, consisting of places they visited on  that particular day.The geographic coverage was Atlanta, GA and we simulated the behavior of an agent population consisting of working professionals.The dataset includes an average of 450 daily trajectories for each of the 1000 generated agents, resulting in a total of 444,634 trajectories.Each input to the model represents a virtual agent's daily trajectory.To capture the patterns of each agent, the agent ID is included at the beginning of the trajectory.Based on the hypothesis that individual behavioral patterns exhibit consistency on the same days of the week, we also incorporate weekday information into the feature vector to enhance the model's ability to detect anomalies.</p>
<p>For example, a daily trajectory is represented as: [agent_ID, weekday, work, restaurant, apartment].We also consider other location representations, including discretized GPS coordinates and the duration of stay at a location.We use Uber hexagons [31] for discretized locations and discretize the stay duration into 1-hour buckets using a sequence of bucket IDs as input to the model.</p>
<p>The PoL dataset comes with labels to identify anomalous trajectories generated by the simulation.To introduce anomalies, the simulation selects ten virtual agents exhibiting anomalous behaviors.For example, work anomaly is one type supported by this simulation: agents with work anomalies will abstain from going to work when they typically would.</p>
<p>For agents with anomalous trajectories, we have the first 450 days representing normal behavior and the last 14 days that exhibit anomalous behavior.</p>
<p>We trained our model on the entire dataset, including the additional 14 days of anomalous behavior from the ten virtual agents, to ensure it could identify outliers even when they were present in the training data.We then tested our methods against the baselines using the entire dataset 5.1.2Porto Dataset.The Porto dataset consists of data generated by 442 taxis operating in the city of Porto in Portugal from January 07, 2013, to June 30, 2014.A taxi reports its GPS location at 15s intervals.We employed preprocessing steps similar to [17] and [15].We discretized the map into 100 Ã— 100 grids and group trajectories with the same source and destination.We discarded trajectories belonging to a "source-destination" group with fewer than 25 trajectories.The input to our model consists of a vector of chronologically ordered and discretized GPS coordinates (grid cells) prepended with SOT (start of trajectory) and appended EOT (end of trajectory) tokens.Since this dataset did not have ground-truth labels for anomalous trajectories, we generated anomalies following the work in [17] and [40].We created two types of anomalies, (i) random shift and (ii) detour anomalies.Figure 5 shows an example of the two types of outliers.For random shift anomalies, we perturb the  percentage of locations in a trajectory and move those location  grid cells away.For detour anomalies, we create a detour for the  percentage portion of a trajectory and shift the detour  grid cells away from the original trajectory.Following previous work on this dataset for anomaly detection [15], we did not include the artificially generated anomalous data during training.</p>
<p>Tokenization &amp; Vocabulary.</p>
<p>Given the nature of a language model architecture, we created tokens to form our model's vocabulary.In the Porto dataset, a token is considered a discretized GPS coordinate.We also added three special tokens: SOT (start of trajectory), EOT (end of trajectory), and PAD (padding token to help with batch training).In the POL dataset, tokens consist of staypoints (work, apartment, restaurant, etc.), days of the week (Monday, Tuesday, etc.), agent ID, and the EOT and PAD special tokens.</p>
<p>Baselines</p>
<p>We compared our method to existing unsupervised anomaly detection methods on trajectory data.Given the established better performance of deep learning methods on trajectory anomaly detection [15], we omitted the inclusion of traditional clustering-based algorithms.</p>
<p>â€¢ SAE: a standard autoencoder method trained to optimize the reconstruction loss of a trajectory sequence using a recurrent neural network.Based on the work in [19] and [2], we use the reconstruction error as the anomaly score.â€¢ VSAE A method similar to SAE, however, in addition to optimizing for the reconstruction loss, it also optimizes the KL divergence between the learned distribution over the latent space and a predefined prior [2,27].Similar to SAE, we use the reconstruction error as the anomaly score.</p>
<p>â€¢ GM-VSAE [17].This method generalizes VSAE by modeling the latent space with more than one Gaussian component and also uses the reconstruction error as the anomaly score.</p>
<p>Evaluation Metrics</p>
<p>We use Precision-Recall AUC and F1 scores to evaluate the performance of our method and the baseline methods [17,37].These metrics are suitable for assessing the performance of anomaly detection methods as the number of anomalies in each dataset is small compared to normal trajectories.For the Porto dataset, these metrics are computed across all trajectories.Conversely, we conduct these evaluations on a per-virtual-agent basis for the Pattern-of-Life (PoL) dataset.In addition, the surprisal rate metric is used to locate the specific occurrence of an anomaly within a trajectory.</p>
<p>RESULTS</p>
<p>The following sections present the anomaly detection results for the PoL and Porto datasets.Anomaly detection for the Porto dataset is a global challenge, since taxi movements are customer/ride-driven and the movements captured by individual trajectories are largely independent.This is in contrast to the PoL dataset, which contains sets of trajectories that model the behavior of individual agents, and anomaly detection will be agent-specific.</p>
<p>Agent-based Outliers -Patterns-of-Life Data</p>
<p>The PoL dataset has user IDs, i.e., sets of trajectories that can be linked to a specific user.The anomaly detection challenge can become user-specific, as the anomalous behavior of one agent may be the normal behavior of another agent.Therefore, we report results for the ten virtual agents that each have 14 additional days of anomalous behavior.Table 1 summarizes F1 and Precision-Recall Area-under-the-Curve (PR-AUC) results for these ten anomalous agents.The results of the rest of the agents are not included in table 1 since these agents are not anomalous within their respective contexts.LM-TAD outperforms all competitor methods as it is more efficient in finding anomalies with respect to anomalous users.We identify three reasons why autoencoder approaches do not identify anomalies for the PoL dataset. 1) Autoencoder approaches are optimized to reconstruct the input by minimizing the reconstruction error.This training approach would tend to learn a model that overfits training data, which may contain anomalies and hence fail to distinguish normal and anomalous trajectories.2) Autoencoderbased methods yield a high reconstruction error for inputs divergent from the overall data patterns within the dataset.Nonetheless, these methods are suboptimal for identifying anomalies on an individualagent basis.Theoretically, GM-SVAE can model distinct Gaussian distributions that can correspond to each virtual agent, thereby learning trajectory distributions unique to each agent.However, this approach has little control over the distributions learned by each component in the latent dimension.Even if we had control over the distributions in the latent space, this approach would be very expensive to train in practice, as the increase in the number of agents would require an increase in Gaussian distributions.</p>
<p>3) The final reason is related to defining what we consider anomalies.</p>
<p>Recent literature on trajectory anomaly data [17,38] considers normality as a trajectory that the majority of taxis in the train data took.Therefore, in this context, an anomalous trajectory was one that was not or rarely used by taxis.However, the PoL dataset's anomalies differ from the those of Porto.Anomalies in the PoL dataset deviate from their typical behavior, e.g., not going to work on days one should.Autoencoder approaches fail to capture these types of anomalies by not being able to learn the anomalous behavior on a per-agent basis.</p>
<p>LM-TAD overcomes the limitations of the autoencoder approaches as follows.1) LM-TAD can learn the likelihood that a particular agent will visit a particular location.Therefore, if an agent hardly visits a location, even if it was part of the training data, the model will give a low probability of such a location and distinguish anomalies.2) LM-TAD uses a special token to provide the context for generating an agent's trajectory, therefore being able to find anomalies on an agent basis.3) LM-TAD is highly customized for different types of anomalies.It can learn the normal pattern for each agent for each day of the week because the model can condition the generation of a particular trajectory to specific tokens.Therefore, this approach is still practically feasible even if the number of agents in the data increases.</p>
<p>Moreover, Figure 5 illustrates why competitor methods have low F1 and PR-AUC scores.In this figure, the red dots represent the perplexity or reconstruction error associated with the agent trajectories.The expectation is that trajectories deemed anomalous (indicated by red dots) would yield higher levels of perplexity or reconstruction error.However, for autoencoder-based methods, we find that the reconstruction error associated with anomalous trajectories is comparatively lower than that of many normal trajectories.In contrast, LM-TAD aligns with the expected model behavior and attributes higher perplexity scores to anomalous trajectories.</p>
<p>Global Outliers -Porto Taxi Data</p>
<p>The results for the Porto taxi dataset are summarized in Table 2.We use different parameter configurations  and  for random-shift Table 2: Anomaly detection results on the Porto dataset.The best results for a particular metric in a specific category are bolded.LM-TAD performs largely on par with the best baseline.</p>
<p>Random shift anomalies</p>
<p>Detour anomalies anomalies params:  = 3,  = 0.  Our method outperforms the SAE and VSAE methods.For random shift anomalies, LM-TAD shows performance comparable to that of GM-SVAE.For detour anomalies, GM-SVAE is able to identify more anomalies than our method, especially for cases where the detour is about 10% of the entire trajectory.This behavior of LM-TAD can be explained by the perplexity of being less sensitive to capturing anomalies that consist of changing a small continuous portion of the trajectory (detour).Since most continuous locations in the trajectory are normal, and as such, those probabilities are fairly high, the overall perplexity will also be relatively high.Conversely, for random shift anomalies, our method exhibits comparable and often superior performance to GM-SVAE.This enhanced detection efficacy can be attributed to the fact that the random shift anomalies break the continuity dependence of one location to its history, resulting in a sequence of locations with lower probabilities and consequently a lower perplexity score.</p>
<p>We can also observe that the distance () of the detour or the randomly shifted location from the original trajectory impacts finding anomalous trajectories less than the anomalous trajectory fraction ().This suggests that metrics that tend to summarize the anomaly of a trajectory by looking at the entire trajectory (i.e., reconstruction error) may find identifying anomalous trajectories with few anomalous locations challenging.Consequently, we introduce a local surprisal rate metric for such cases to address this limitation.</p>
<p>Identifying Anomalies using Surprisal Rate</p>
<p>Perplexity as an aggregate measure may not be sufficient to identify anomalous trajectories.Consequently, the presence of only a few anomalous tokens may lead to their signal being diluted by the averaging process and anomalies go undetected.Similar limitations apply to autoencoder-based methods, where the reconstruction loss is calculated over all tokens in a trajectory.</p>
<p>A further limitation in using perplexity or reconstruction error is the inability to pinpoint the specific location of anomalies within a trajectory.Here, our work proposes the surprisal rate measure that operates at the level of individual locations or tokens within a trajectory.</p>
<p>In our empirical analysis, we explored the application of the surprisal rate for detecting potentially anomalous locations within a trajectory in the PoL data.A high surprisal rate suggests that a particular location in a trajectory may be anomalous.Figure 6 shows the surprisal rate for 30 trajectories (10 anomalous and 20 normal ones randomly chosen from the respective agents).The analysis reveals that certain tokens in anomalous trajectories exhibit significantly higher surprisal rates compared to those in normal trajectories, particularly at the beginning of the trajectories.This pattern aligns with the dataset's structure and the configuration of our input vector, where the initial tokens represent the agent ID, the weekday, and the first location visited by the virtual agent on that day.Given the expected pattern of agents visiting consistent locations on specific weekdays, deviations from this routine, such as visiting an atypical location as the first destination, are flagged as anomalies.Consequently, the inclusion of the weekday token in the trajectory analysis enables the identification of instances where an agent's initial location deviates from the norm, resulting in a larger surprisal rate when an agent visits an unusual place on a given weekday.</p>
<p>Location Configurations -Ablation Study</p>
<p>We conducted an ablation study to show the versatility of LM-TAD in working with different types of inputs.In this study, we explore the usage of discretized GPS coordinates (Uber hexagons [31]), staypoint labels (i.e., work, restaurant, and so forth), and stay duration (the duration at a particular location) as input for the model to infer the anomaly detection performance of each modality.One of the main advantages of using one modality over another is the type of anomalies we are interested in discovering.Anomalies can be related to the duration of stay in a particular location (staying longer than usual), by visiting an uncommon geographical area, or by visiting a different place (e.g., shopping mall) on a weekday when one is supposed to be elsewhere (e.g., work).</p>
<p>Table 3 summarizes the results of using various location configurations in the PoL dataset.Staypoint labels provide the best performance to identify anomalies.This is consistent with the anomalies in the PoL dataset as discussed in Section 6.1, where agents abstain from visiting places they would otherwise visit on certain days.The dwell time also proves to be effective since visiting different locations affects the time spent at those locations.These findings underscore the adaptability of our approach to using different feature configurations to identify anomalies.</p>
<p>Online Anomaly Detection</p>
<p>One of the advantages of LM-TAD is the support of online anomalous trajectory detection.Our approach does not require the entire trajectory to compute an anomaly score.In addition, we do not need to know the destination (although such knowledge would enhance the anomaly detection of sub-trajectories).As soon as a trip begins, LM-TAD can compute the anomaly score of a partial trajectory each time a new location is sampled.Autoencoder approaches can be used for online anomalous trajectory detection as well.However, they are significantly more expensive to use since they must compute the anomaly score for the entire sub-trajectory each time a new location is sampled.LM-TAD, instead, can cache the key-value (KV cache) states [16,24] of the attention mechanism for previously generated tokens (i.e., GPS coordinates).This significantly reduces the need for repetitive computations and lowers the latency in computing the anomaly score.</p>
<p>Figure 7 shows the accuracy of detecting anomalies for partial trajectories at different completion ratios for the Porto dataset.We evaluate partial trajectories with ratios from 0.2 to 1.0 (complete) using 0.1 increment.The results suggest that LM-TAD is more than competitive in detecting sub-trajectory anomalies.Especially for random shift anomalies, 40% of the sub-trajectory is sufficient to detect most anomalies in the dataset.Detour anomalies are not that easily detected for small completion ratios without knowing the destination.A detour becomes evidently anomalous only when knowing the destination or a large portion of the trajectory.In general, LM-TAD performs on par with the best baseline, but as discussed before, comes with the advantage of significantly lower latency.</p>
<p>CONCLUSIONS</p>
<p>In this work, we introduced LM-TAD, an innovative trajectory anomaly detection model that uses an autoregressive causal-attention mechanism.By conceptualizing trajectories as sequences akin to language statements, our model effectively captures the sequential dependencies and contextual nuances necessary for precise anomaly detection.We demonstrated that incorporating user-specific tokens enhances the model's ability to detect context-specific anomalies, addressing the variability in individual behavior patterns.</p>
<p>Our extensive experiments validated the robustness and adaptability of LM-TAD across two datasets, the Agent-based-Model generated Pattern-of-Life (PoL) dataset and the Porto taxi dataset.The results show that LM-TAD vastly outperforms existing state-of-theart methods in identifying user-contextual anomalies.At the same time, its performance is competitive in detecting outliers in GPSbased trajectory data.</p>
<p>We introduced perplexity and surprisal rate as metrics for outlier detection and localization of anomalies within trajectories, broadening the analytical capabilities of the approach.The model's ability to handle diverse trajectory representations, from GPS coordinates to staypoints and activity types, underscores its versatility and uniqueness.</p>
<p>Importantly, our approach also proves advantageous for online trajectory anomaly detection, reducing computational latency, and gaining a significant performance advantage over existing models.This provides for real-time anomaly detection without the need for an expensive re-computation of results.</p>
<p>In summary, LM-TAD represents a substantial advance in trajectory anomaly detection, offering a scalable, context-aware, and computationally efficient solution.This work paves the way for future research in user-centric analysis and real-time anomaly detection in trajectory data.</p>
<p>Figure 2 :
2
Figure 2: Architecture of LM-TAD, our trajectory model.</p>
<p>Figure 3 :
3
Figure 3: Example location configurations.Locations can be (a) discretized GPS coordinates, (b) staypoints, (c) staypoints enhanced with dwell time, or (d) activities.</p>
<p>Figure 4 :
4
Figure 4: Example of generated anomalies with  = 0.3 and  = 3 for both types of anomalies on the Porto dataset.</p>
<p>Figure 5 :
5
Figure 5: Anomaly results for all methods trained on all the pattern of life dataset trajectories.Each dot represents the perplexity of a trajectory for any of the ten agents with normal and anomalous trajectories.Unlike other methods, our method (d) distinguishes between anomalous trajectories and normal trajectories by scoring most anomalous trajectories with high perplexity.</p>
<p>1 ğ›¼ = 5 ,
15
 = 0.1  = 3,  = 0.3  = 3,  = 0.1  = 5,  = 0.1  = 3,  = 0.</p>
<p>Figure 6 :
6
Figure6: Surprisal rate through trajectories on the Pattern of Life dataset.We plot the trajectories of each agent with some anomalous trajectories (10 first agents), and we randomly selected 20 agents with no anomalous trajectories.The horizontal line shows the (arbitrary) threshold for a surprisal rate high enough to correspond to anomalous trajectories.Anomalous trajectories plotted in red have high surprisal rates for certain locations in the trajectories as opposed to normal trajectories.Hence, LM-TAD can identify anomalous trajectories based on the surprisal rate.</p>
<p>(a) F1 scores for random shift anomalies (b) PR-AUC scores for random shift anomalies (c) F1 scores for detour anomalies (d) PR-AUC scores for detour anomalies</p>
<p>Figure 7 :
7
Figure 7: Online Anomalous Trajectory Detection Results (POL data).The results show the performance of each model for the detour and random shift anomalies as we evaluate different ratios of the trajectory from 0.1 to 1.0.LM-TAD shows competitive results and can detect anomalies of sub-trajectories.</p>
<p>Table 1 :
1
Outlier detection on RED outlier agents for the Pattern of Life dataset.The best results are bolded.LM-TAD clearly outperforms the baselines, showcasing its modality adaptation capabilities
baselinesoursSAEVSAEGM-VSAELM-TADAgent F1 PR-AUC F1 PR-AUC F1 PR-AUCF1PR-AUC57 0.000.010.000.040.160.030.720.5962 0.000.110.000.020.100.090.870.85347 0.000.020.000.020.250.400.780.78546 0.000.070.000.030.120.120.750.63551 0.000.010.000.020.000.040.760.63554 0.000.020.000.020.000.040.610.46644 0.000.030.000.010.000.040.760.75900 0.000.010.000.060.320.170.700.69949 0.000.010.000.020.000.060.770.79992 0.000.030.000.020.110.130.780.72</p>
<p>For random shift, we perturb  percent of locations in a trajectory, moving them  grid cells away.Similarly, for detour anomalies, we create a detour for  percent of a trajectory, shifting the detour  grid cells away from the original trajectory.
3MetricF1PR-AUCF1PR-AUCF1PR-AUCF1PR-AUCF1PR-AUCF1PR-AUCbaselines:SAE 0.440.660.530.750.710.900.300.470.360.540.580.78VSAE 0.400.670.510.760.690.900.280.470.350.560.570.77GM-VSAE-10 0.850.900.860.910.910.990.730.720.790.770.900.96ours: LM-TAD 0.850.910.850.910.900.990.690.650.730.680.890.96and detour anomalies.</p>
<p>Table 3 :
3
Anomaly detection using different location configurations.The best results are highlighted in bold.The staypoint label location type performs best overall, reflecting the types of anomalies present in the PoL dataset.
Staypoint label Discretized GPSStay durationAgentF1PR-AUCF1PR-AUCF1PR-AUC57 0.620.690.500.540.260.4962 0.860.810.880.800.780.80347 0.780.750.750.680.720.70546 0.750.760.670.650.670.62551 0.670.630.760.630.520.48554 0.620.460.530.610.620.46644 0.800.780.570.700.640.64900 0.710.660.460.600.710.59949 0.820.770.780.750.820.73992 0.780.680.720.730.670.69average 0.740.700.660.670.640.62
ACKNOWLEDGMENTThe authors thank Andreas Zuefle for making the Atlanta PoL dataset available.This work was supported by the National Science Foundation (Award 2127901) and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract number 140D0419C0050.The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes, notwithstanding any copyright annotation thereon.Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government.Additionally, this work was supported by resources provided by the Office of Research Computing, George Mason University and by the National Science Foundation (Award Numbers 1625039, 2018631).
Massive Trajectory Data Based on Patterns of Life. Hossein Amiri, Shiyang Ruan, Joon-Seok Kim, Hyunjee Jin, Hamdi Kavak, Andrew Crooks, Dieter Pfoser, Carola Wenk, Andreas Zufle, 10.1145/3589132.3625592Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems. 1-4. the 31st ACM International Conference on Advances in Geographic Information Systems. 1-42023</p>
<p>Jinwon An, Sungzoon Cho, Variational Autoencoder based Anomaly Detection using Reconstruction Probability. 201536663713</p>
<p>Effectiveness of Self-Supervised Pre-Training for ASR. Alexei Baevski, Abdelrahman Mohamed, 10.1109/ICASSP40776.2020.9054224ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2020</p>
<p>Abdel rahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. Alexei Baevski, Henry Zhou, ArXiv abs/2006.114772020</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Language Models Are Few-Shot Learners (NIPS'20). Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; Red Hook, NY, USACurran Associates Inc2020Article 159, 25 pages</p>
<p>iBOAT: Isolation-Based Online Anomalous Trajectory Detection. Chao Chen, Daqing Zhang, Pablo Samuel Castro, Nan Li, Lin Sun, Shijian Li, Zonghui Wang, 10.1109/TITS.2013.2238531IEEE Transactions on Intelligent Transportation Systems. 142013. 2013</p>
<p>Real-Time Detection of Anomalous Taxi Trajectories from GPS Traces. Chao Chen, Daqing Zhang, Pablo Samuel Castro, Nan Li, Lin Sun, Shijian Li, Mobile and Ubiquitous Systems: Computing, Networking, and Services. Alessandro Puiatti, Tao Gu, Berlin Heidelberg; Berlin, HeidelbergSpringer2012</p>
<p>. Jinghui Chen, Saket Sathe, Charu Aggarwal, Deepak Turaga, n.d.</p>
<p>Outlier Detection with Autoencoder Ensembles. 10.1137/1.9781611974973.11</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Ming-Wei Jacob "devlin, Kenton Chang, Kristina " Lee, Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Online Trajectory Prediction for Metropolitan Scale Mobility Digital Twin. Zipei Fan, Xiaojie Yang, Wei Yuan, Renhe Jiang, Quanjun Chen, Xuan Song, Ryosuke Shibasaki, 10.1145/3557915.3561040Proceedings of the 30th International Conference on Advances in Geographic Information Systems. the 30th International Conference on Advances in Geographic Information SystemsSeattle, Washington; New York, NY, USA, ArticleAssociation for Computing Machinery2022103SIGSPATIAL '22)</p>
<p>Transportation mode-based segmentation and classification of movement trajectories. Hugo Ledoux, Filip Biljecki, Peter Van Oosterom, 10.1080/13658816.2012.692791International Journal of Geographical Information Science. 272013. 2013</p>
<p>Coupled IGMM-GANs for deep multimodal anomaly detection in human mobility data. Kathryn Gray, Daniel Smolyak, Sarkhan Badirli, George O Mohler, ArXiv abs/1809.027282018. 2018</p>
<p>How do you go where?: improving next location prediction by learning travel mode information using transformers. Ye Hong, Henry Martin, Martin Raubal, Proceedings of the 30th International Conference on Advances in Geographic Information Systems. the 30th International Conference on Advances in Geographic Information Systems2022. 2022</p>
<p>Trajectory Outlier Detection: A Partition-and-Detect Framework. Jae-Gil Lee, Jiawei Han, Xiaolei Li, 10.1109/ICDE.2008.44974222008 IEEE 24th International Conference on Data Engineering. 2008</p>
<p>Deep Representation Learning for Trajectory Similarity Computation. Xiucheng Li, Kaiqi Zhao, Gao Cong, Christian S Jensen, Wei Wei, 10.1109/ICDE.2018.000622018 IEEE 34th International Conference on Data Engineering (ICDE). 2018</p>
<p>MiniCache: KV Cache Compression in Depth Dimension for Large Language Models. Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, Bohan Zhuang, 2024</p>
<p>Online Anomalous Trajectory Detection with Deep Generative Sequence Modeling. Yiding Liu, Kaiqi Zhao, Gao Cong, Zhifeng Bao, 10.1109/ICDE48307.2020.000872020 IEEE 36th International Conference on Data Engineering (ICDE). 2020</p>
<p>Outlier trajectory detection: a trajectory analytics based approach. Zhongjian Lv, Jiajie Xu, Pengpeng Zhao, Guanfeng Liu, Lei Zhao, Xiaofang Zhou, 10.1007/978-3-319-55753-3_153_15 22nd International Conference on Database Systems for Advanced Applications (DASFAA) ; Conference date. Lecture Notes in Computer Science. SelÃ§uk Candan, Lei Chen, Torben Bach Pedersen, Lijun Chang, Wen Hua, United StatesSpringer Nature2017. 2017 Through 30-03-2017Database Systems for Advanced Applications</p>
<p>LSTM-based Encoder-Decoder for Multisensor Anomaly Detection. Pankaj Malhotra, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig, Puneet Agarwal, Gautam M Shroff, ArXiv abs/1607.001482016. 2016</p>
<p>Association for Computing Machinery. Mashaal Musleh, Mohamed F Mokbel, Sofiane Abbar, 10.1145/3557915.3560972Proceedings of the 30th International Conference on Advances in Geographic Information Systems. the 30th International Conference on Advances in Geographic Information SystemsSeattle, Washington; New York, NY, USA, Article2022Let's Speak Trajectories</p>
<p>No News is Good News: A Critique of the One Billion Word Benchmark. Helen Ngo, Joao M De Ara'ujo, Jeffrey Hui, Nick Frosst, ArXiv abs/2110.126092021. 2021239769114</p>
<p>Deep Contextualized Word Representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke " Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Capturing the Uncertainty of Moving-Object Representations. Dieter Pfoser, Christian S Jensen, 10.1007/3-540-48482-5_9Proceedings of the 6th International Symposium on Advances in Spatial Databases (SSD'99). the 6th International Symposium on Advances in Spatial Databases (SSD'99)Springer-Verlag1999</p>
<p>Efficiently Scaling Transformer Inference. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean, ArXiv abs/2211.051022022. 2022</p>
<p>Learning Transferable Visual Models From Natural Language Supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, International Conference on Machine Learning. 2021</p>
<p>Language Models are Unsupervised Multitask Learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019160025533</p>
<p>Sequential Variational Autoencoders for Collaborative Filtering. Noveen Sachdeva, G Manco, Ettore Ritacco, Vikram Pudi, Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining. the Twelfth ACM International Conference on Web Search and Data Mining2018. 2018</p>
<p>Coupled IGMM-GANs with Applications to Anomaly Detection in Human Mobility Data. Daniel Smolyak, Kathryn Gray, Sarkhan Badirli, George Mohler, 10.1145/3385809ACM Trans. Spatial Algorithms Syst. 6242020. jun 2020</p>
<p>Anomalous Trajectory Detection Using Recurrent Neural Network. Li Song, Ruijia Wang, Ding Xiao, Xiaotian Han, Yanan Cai, Chuan Shi, Advanced Data Mining and Applications. Guojun Gan, Bohan Li, Xue Li, Shuliang Wang, ChamSpringer International Publishing2018</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, ArXiv abs/2302.139712023. 2023Edouard Grave, and Guillaume Lample</p>
<p>Inc Uber, 11/22/23H3 Hexagonal hierarchical geospatial indexing system. 2023</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Å Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>Leveraging language foundation models for human mobility forecasting. Bhanu Hao Xue, Flora D Prakash Voutharoja, Salim, Proceedings of the 30th International Conference on Advances in Geographic Information Systems. the 30th International Conference on Advances in Geographic Information Systems2022. 2022</p>
<p>Driving with Knowledge from the Physical World (KDD '11). Jing Yuan, Yu Zheng, Xing Xie, Guangzhong Sun, 10.1145/2020408.20204622011Association for Computing MachineryNew York, NY, USA</p>
<p>T-Drive: Driving Directions Based on Taxi Trajectories (GIS '10). Jing Yuan, Yu Zheng, Chengyang Zhang, Wenlei Xie, Xing Xie, Guangzhong Sun, Yan Huang, 10.1145/1869790.18698072010Association for Computing MachineryNew York, NY, USA</p>
<p>IBAT: Detecting Anomalous Taxi Trajectories from GPS Traces. Daqing Zhang, Nan Li, Zhi-Hua Zhou, Chao Chen, Lin Sun, Shijian Li, 10.1145/2030112.2030127Proceedings of the 13th International Conference on Ubiquitous Computing. the 13th International Conference on Ubiquitous ComputingBeijing, China; New York, NY, USAAssociation for Computing Machinery2011Ubi-Comp '11)</p>
<p>A trajectory outlier detection method based on variational auto-encoder. Longmei Zhang, Wei Lu, Feng Xue, Yanshuo Chang, Mathematical biosciences and engineering : MBE. 202599269302023. 2023</p>
<p>Online Anomalous Subtrajectory Detection on Road Networks with Deep Reinforcement Learning. Qianru Zhang, Zheng Wang, Cheng Long, Chao Huang, Siu-Ming Yiu, Yiding Liu, Gao Cong, Jieming Shi, 10.1109/ICDE55515.2023.000262023 IEEE 39th International Conference on Data Engineering (ICDE). 2023</p>
<p>Online Anomalous Subtrajectory Detection on Road Networks with Deep Reinforcement Learning. Qianru Zhang, Zheng Wang, Cheng Long, Chao Huang, Siu-Ming Yiu, Yiding Liu, Gao Cong, Jieming Shi, 10.1109/ICDE55515.2023.000262023 IEEE 39th International Conference on Data Engineering (ICDE). 2023</p>
<p>Contextual Spatial Outlier Detection with Metric Learning (KDD '17). Guanjie Zheng, Susan L Brantley, Thomas Lauvaux, Zhenhui Li, 10.1145/3097983.30981432017New York, NY, USAAssociation for Computing Machinery</p>
<p>Anomaly Detection with Robust Deep Autoencoders. Chong Zhou, Randy C Paffenroth, 10.1145/3097983.3098052Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningHalifax, NS, Canada; New York, NY, USAAssociation for Computing Machinery2017</p>
<p>Learning to Prompt for Vision-Language Models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 1302021. 2021</p>
<p>Time-Dependent Popular Routes Based Trajectory Outlier Detection. Jie Zhu, Wei Jiang, An Liu, Guanfeng Liu, Lei Zhao, 10.1007/978-3-319-26190-4_22015Springer-VerlagBerlin, Heidelberg</p>
<p>Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Dae Ki Cho, Haifeng Chen, International Conference on Learning Representations. 2018</p>
<p>Urban life: a model of people and places. Andreas ZÃ¼fle, Carola Wenk, Dieter Pfoser, Andrew Crooks, Joon-Seok Kim, Hamdi Kavak, Umar Manzoor, Hyunjee Jin, 10.1007/s10588-021-09348-7Computational and Mathematical Organization Theory. 292023. 11 2023</p>            </div>
        </div>

    </div>
</body>
</html>
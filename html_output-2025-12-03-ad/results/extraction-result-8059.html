<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8059 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8059</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8059</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-eb375712bd37250c350ecd3f559e1879e87eb3e5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/eb375712bd37250c350ecd3f559e1879e87eb3e5" target="_blank">Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A length-controlling AlpacaEval is introduced, a fast and affordable benchmark for instruction-tuned LLMs that uses LLMs to estimate response quality that aims to answer the counterfactual question:"What would the preference be if the model's and baseline's output had the same length?"</p>
                <p><strong>Paper Abstract:</strong> LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instruction-tuned LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question:"What would the preference be if the model's and baseline's output had the same length?"To achieve this, we first fit a generalized linear model to predict the biased auto-annotator's preferences based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, but we also find that it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94 to 0.98.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8059.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8059.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlpacaEval vs Chatbot Arena</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of AlpacaEval (GPT-4-turbo LLM judge) with Chatbot Arena human evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper compares the LLM-based AlpacaEval automatic judge (GPT-4-turbo-based pairwise evaluator producing win rates over 805 instructions) to human judgments from the Chatbot Arena, reporting Spearman correlations and analyses of systematic differences such as length bias and gameability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Open-ended instruction-following / chatbot pairwise preference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AlpacaEval (805 instructions) compared to LMSYS Chatbot Arena (live human pairwise evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 turbo-based AlpacaEval evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Closed-source GPT-4-turbo-based evaluator used by AlpacaEval to compare model and baseline responses head-to-head and output a probability of preferring the evaluated model; used across >120 models and 805 instructions on the AlpacaEval leaderboard.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Chatbot Arena live users (real internet users performing pairwise comparisons; converted to Elo ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman rho (rank correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.94</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>length bias (preference for longer outputs); presence of lists and style/position biases; susceptibility to verbosity-based gaming; some systematic differences between LLM judge and human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>AlpacaEval is highly correlated with human preference rankings but exhibits strong sensitivity to output length and other surface features; Chatbot Arena is treated as a silver standard but may itself have biases (e.g., internet users focusing on surface features). The authors observe that uncontrolled biases in AlpacaEval can be significantly gamed by models optimizing verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Low cost, scalable, fast automated evaluation that approximates human preferences (high correlation) and can be run at leaderboard/dev scale.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>AlpacaEval: 805 fixed instructions, for each instruction baseline (constant, GPT-4 turbo) and evaluated models produce outputs; GPT-4 turbo evaluator compares outputs head-to-head and yields preference probability; win rate = average predicted preference over instructions. Chatbot Arena: live users send instructions to both anonymized models and choose preferred response; pairwise outcomes converted to Elo and then to win rates. Correlations computed over benchmarks that evaluate >=25 models (38 models overlap for AlpacaEval). Bootstrap p-values reported for correlation differences (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8059.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8059.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlpacaEval-LC vs Chatbot Arena</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Length-Controlled AlpacaEval (LC) compared to Chatbot Arena human evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors apply a regression-based debiasing (GLM controlling for length differences and instruction difficulty) to AlpacaEval to produce AlpacaEval-LC and report improved agreement with human Chatbot Arena rankings and reduced sensitivity to verbosity-based attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Open-ended instruction-following / chatbot pairwise preference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AlpacaEval (805 instructions) compared to LMSYS Chatbot Arena (human)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 turbo-based evaluator (same judge model as AlpacaEval, but preferences post-hoc adjusted via GLM)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Same closed-source GPT-4-turbo-based automatic evaluator outputs used as the dependent variable; post-hoc generalized linear model (logistic) trained to predict the auto-evaluator's preferences from model identity, normalized length difference (tanh of standardized length diff), and instruction difficulty; length term zeroed to obtain length-controlled win rates.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Chatbot Arena live users (real internet users performing pairwise comparisons; Elo-derived rankings), treated as silver standard</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman rho (rank correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.98</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>residual adversarial vulnerability to targeted attacks (e.g., truncation), reliance on assumption that comparing models at equal length is appropriate, limited evaluation domain (single dataset and judge prompt); does not solve other LLM-judge biases (e.g., self-preference, list bias)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Length-controlling increases Spearman correlation with Chatbot Arena from 0.94 to 0.98 and substantially reduces length-based gameability (normalized standard deviation across concise/standard/verbose prompts decreased from 25% to 10%). Regularization on the length coefficient reduces adversarial gains from truncation. LC yields more favorable rankings for typically shorter proprietary models, suggesting prior open models had exploited length bias.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>After post-hoc debiasing, retains low cost and scalability while better matching human rankings and being more robust to verbosity manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fit a logistic GLM predicting AlpacaEval's pairwise preference probability using terms for model identity (θ), normalized length difference (φ * tanh(std-normalized len diff)), and instruction difficulty ((ψ_m - ψ_b)γ_x); trained on AlpacaEval leaderboard data (>120 models, 805 instructions) with 5-fold CV and L2 regularization; length-controlled winrate computed by setting length term to zero; evaluated gameability by prompting models with 'verbose' and 'concise' instructions and quantifying normalized SD; adversarial tests included truncating outputs to evaluate adversarial win rate gains (reported in Table 1: LC adversarial win rate gain 8.5). Bootstrap p-values reported for improvement in correlation (approximately 0.07 vs AlpacaEval and 0.06 vs MT-bench).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8059.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8059.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM self-annotator bias (inter-judge differences)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-preference and inter-judge variability among LLM judges (GPT-4-1106, Claude-3-opus, Mistral-large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preliminary experiments comparing different LLM judges show some tendency for models to prefer their own outputs but overall rankings remain surprisingly stable across different LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Open-ended instruction-following / chatbot pairwise preference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AlpacaEval dataset (leaderboard comparisons evaluated by multiple LLM judges)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>gpt-4-1106; claude-3-opus; mistral-large (each used as an LLM judge in separate evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Different closed-source and open-source LLMs used as judges: GPT-4-1106 (OpenAI), Claude-3-opus (Anthropic), Mistral-large (Mistral); Table 2 reports win rates of evaluated models when judged by each of these LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Not applicable (this entity is about comparing different LLM judges to each other and assessing their self-bias relative to human evaluations treated elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Win rate matrix / pairwise preferences (no single scalar agreement metric vs humans reported here for this experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>self-annotator bias (LLM judges showing mild preference for their own outputs) ; inter-judge variability in absolute win rates</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Preliminary results show self-annotator effects exist but are often smaller than the general differences between models; rankings of models are largely stable across different LLM judges despite some preference asymmetries (e.g., Claude 3 Opus prefers GPT-4 Preview, Mistral Large prefers other models over itself).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Noted consistency/stability of ranking across LLM judges suggests automated judges can produce reasonably robust leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Authors evaluated the AlpacaEval leaderboard using three different LLM judges and report a win-rate matrix (Table 2) showing each judge's win rates for a set of models; this is described as preliminary exploration of self-annotator biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An automatic evaluator of instruction-following models <em>(Rating: 2)</em></li>
                <li>Spurious correlations in reference-free evaluation of text generation <em>(Rating: 1)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 1)</em></li>
                <li>Benchmarking cognitive biases in large language models as evaluators <em>(Rating: 1)</em></li>
                <li>A long way to go: Investigating length correlations in RLHF <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8059",
    "paper_id": "paper-eb375712bd37250c350ecd3f559e1879e87eb3e5",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "AlpacaEval vs Chatbot Arena",
            "name_full": "Comparison of AlpacaEval (GPT-4-turbo LLM judge) with Chatbot Arena human evaluations",
            "brief_description": "This paper compares the LLM-based AlpacaEval automatic judge (GPT-4-turbo-based pairwise evaluator producing win rates over 805 instructions) to human judgments from the Chatbot Arena, reporting Spearman correlations and analyses of systematic differences such as length bias and gameability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
            "evaluation_task": "Open-ended instruction-following / chatbot pairwise preference evaluation",
            "dataset_name": "AlpacaEval (805 instructions) compared to LMSYS Chatbot Arena (live human pairwise evaluations)",
            "judge_model_name": "GPT-4 turbo-based AlpacaEval evaluator",
            "judge_model_details": "Closed-source GPT-4-turbo-based evaluator used by AlpacaEval to compare model and baseline responses head-to-head and output a probability of preferring the evaluated model; used across &gt;120 models and 805 instructions on the AlpacaEval leaderboard.",
            "human_evaluator_type": "Chatbot Arena live users (real internet users performing pairwise comparisons; converted to Elo ratings)",
            "agreement_metric": "Spearman rho (rank correlation)",
            "agreement_score": 0.94,
            "reported_loss_aspects": "length bias (preference for longer outputs); presence of lists and style/position biases; susceptibility to verbosity-based gaming; some systematic differences between LLM judge and human preferences",
            "qualitative_findings": "AlpacaEval is highly correlated with human preference rankings but exhibits strong sensitivity to output length and other surface features; Chatbot Arena is treated as a silver standard but may itself have biases (e.g., internet users focusing on surface features). The authors observe that uncontrolled biases in AlpacaEval can be significantly gamed by models optimizing verbosity.",
            "advantages_of_llm_judge": "Low cost, scalable, fast automated evaluation that approximates human preferences (high correlation) and can be run at leaderboard/dev scale.",
            "experimental_setting": "AlpacaEval: 805 fixed instructions, for each instruction baseline (constant, GPT-4 turbo) and evaluated models produce outputs; GPT-4 turbo evaluator compares outputs head-to-head and yields preference probability; win rate = average predicted preference over instructions. Chatbot Arena: live users send instructions to both anonymized models and choose preferred response; pairwise outcomes converted to Elo and then to win rates. Correlations computed over benchmarks that evaluate &gt;=25 models (38 models overlap for AlpacaEval). Bootstrap p-values reported for correlation differences (see paper).",
            "uuid": "e8059.0",
            "source_info": {
                "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AlpacaEval-LC vs Chatbot Arena",
            "name_full": "Length-Controlled AlpacaEval (LC) compared to Chatbot Arena human evaluations",
            "brief_description": "The authors apply a regression-based debiasing (GLM controlling for length differences and instruction difficulty) to AlpacaEval to produce AlpacaEval-LC and report improved agreement with human Chatbot Arena rankings and reduced sensitivity to verbosity-based attacks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
            "evaluation_task": "Open-ended instruction-following / chatbot pairwise preference evaluation",
            "dataset_name": "AlpacaEval (805 instructions) compared to LMSYS Chatbot Arena (human)",
            "judge_model_name": "GPT-4 turbo-based evaluator (same judge model as AlpacaEval, but preferences post-hoc adjusted via GLM)",
            "judge_model_details": "Same closed-source GPT-4-turbo-based automatic evaluator outputs used as the dependent variable; post-hoc generalized linear model (logistic) trained to predict the auto-evaluator's preferences from model identity, normalized length difference (tanh of standardized length diff), and instruction difficulty; length term zeroed to obtain length-controlled win rates.",
            "human_evaluator_type": "Chatbot Arena live users (real internet users performing pairwise comparisons; Elo-derived rankings), treated as silver standard",
            "agreement_metric": "Spearman rho (rank correlation)",
            "agreement_score": 0.98,
            "reported_loss_aspects": "residual adversarial vulnerability to targeted attacks (e.g., truncation), reliance on assumption that comparing models at equal length is appropriate, limited evaluation domain (single dataset and judge prompt); does not solve other LLM-judge biases (e.g., self-preference, list bias)",
            "qualitative_findings": "Length-controlling increases Spearman correlation with Chatbot Arena from 0.94 to 0.98 and substantially reduces length-based gameability (normalized standard deviation across concise/standard/verbose prompts decreased from 25% to 10%). Regularization on the length coefficient reduces adversarial gains from truncation. LC yields more favorable rankings for typically shorter proprietary models, suggesting prior open models had exploited length bias.",
            "advantages_of_llm_judge": "After post-hoc debiasing, retains low cost and scalability while better matching human rankings and being more robust to verbosity manipulation.",
            "experimental_setting": "Fit a logistic GLM predicting AlpacaEval's pairwise preference probability using terms for model identity (θ), normalized length difference (φ * tanh(std-normalized len diff)), and instruction difficulty ((ψ_m - ψ_b)γ_x); trained on AlpacaEval leaderboard data (&gt;120 models, 805 instructions) with 5-fold CV and L2 regularization; length-controlled winrate computed by setting length term to zero; evaluated gameability by prompting models with 'verbose' and 'concise' instructions and quantifying normalized SD; adversarial tests included truncating outputs to evaluate adversarial win rate gains (reported in Table 1: LC adversarial win rate gain 8.5). Bootstrap p-values reported for improvement in correlation (approximately 0.07 vs AlpacaEval and 0.06 vs MT-bench).",
            "uuid": "e8059.1",
            "source_info": {
                "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM self-annotator bias (inter-judge differences)",
            "name_full": "Self-preference and inter-judge variability among LLM judges (GPT-4-1106, Claude-3-opus, Mistral-large)",
            "brief_description": "Preliminary experiments comparing different LLM judges show some tendency for models to prefer their own outputs but overall rankings remain surprisingly stable across different LLM judges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
            "evaluation_task": "Open-ended instruction-following / chatbot pairwise preference evaluation",
            "dataset_name": "AlpacaEval dataset (leaderboard comparisons evaluated by multiple LLM judges)",
            "judge_model_name": "gpt-4-1106; claude-3-opus; mistral-large (each used as an LLM judge in separate evaluations)",
            "judge_model_details": "Different closed-source and open-source LLMs used as judges: GPT-4-1106 (OpenAI), Claude-3-opus (Anthropic), Mistral-large (Mistral); Table 2 reports win rates of evaluated models when judged by each of these LLM judges.",
            "human_evaluator_type": "Not applicable (this entity is about comparing different LLM judges to each other and assessing their self-bias relative to human evaluations treated elsewhere)",
            "agreement_metric": "Win rate matrix / pairwise preferences (no single scalar agreement metric vs humans reported here for this experiment)",
            "agreement_score": null,
            "reported_loss_aspects": "self-annotator bias (LLM judges showing mild preference for their own outputs) ; inter-judge variability in absolute win rates",
            "qualitative_findings": "Preliminary results show self-annotator effects exist but are often smaller than the general differences between models; rankings of models are largely stable across different LLM judges despite some preference asymmetries (e.g., Claude 3 Opus prefers GPT-4 Preview, Mistral Large prefers other models over itself).",
            "advantages_of_llm_judge": "Noted consistency/stability of ranking across LLM judges suggests automated judges can produce reasonably robust leaderboards.",
            "experimental_setting": "Authors evaluated the AlpacaEval leaderboard using three different LLM judges and report a win-rate matrix (Table 2) showing each judge's win rates for a set of models; this is described as preliminary exploration of self-annotator biases.",
            "uuid": "e8059.2",
            "source_info": {
                "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "AlpacaEval: An automatic evaluator of instruction-following models",
            "rating": 2
        },
        {
            "paper_title": "Spurious correlations in reference-free evaluation of text generation",
            "rating": 1
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 1
        },
        {
            "paper_title": "Benchmarking cognitive biases in large language models as evaluators",
            "rating": 1
        },
        {
            "paper_title": "A long way to go: Investigating length correlations in RLHF",
            "rating": 1
        }
    ],
    "cost": 0.011153749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators</h1>
<p>Yann Dubois ${ }^{1}$, Balázs Galambosi ${ }^{2}$, Percy Liang ${ }^{1}$ and Tatsunori B. Hashimoto ${ }^{1}$<br>${ }^{1}$ Stanford University ${ }^{2}$ Independent Researcher</p>
<h4>Abstract</h4>
<p>LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instructiontuned LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question: "What would the preference be if the model's and baseline's output had the same length?" To achieve this, we first fit a generalized linear model to predict the biased auto-annotator's preferences based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, we also find that it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94 to 0.98 . We release the code and resulting leaderboard.</p>
<h2>1 Introduction</h2>
<p>Chat Arena Spearman correlation
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AlpacaEval length-controlled increases correlation with the LMSYS Chatbot Arena from 0.94 to 0.98 . It is currently the automatic benchmark with the highest correlation with Chatbot Arena.</p>
<p>Developing and improving NLP systems requires reliable, low-cost evaluations that can quantify progress. In closed-ended tasks, such as multiple-choice QA, such evaluations are straightforward to implement and trust (Novikova et al., 2017; Yeh et al., 2021). However, such evaluations cannot be applied to open-ended settings such as instruction following for language models. Even neural reference-based evaluation metrics such as BERTscore (Zhang* et al., 2020) face challenges in those settings due to the difficulty of collecting a diverse set of references that can cover the space of valid outputs.</p>
<p>Recently, there has been a push toward reference-free evaluation methods that leverage high-performance LLMs, e.g. AlpacaEval (Li et al., 2023), MTBench (Zheng et al., 2023), and WildBench (Lin et al., 2024). While these approaches show a high correlation with human annotators, they often do so by exploiting spurious correlations such as the length of the output, the presence of lists, or various position biases Li et al. (2023); Zheng et al. (2023); Koo et al. (2023); Wang et al. (2023); Wu \&amp; Aji (2023).</p>
<p>Creating a way to debias automated evaluation metrics would be highly valuable - it would address the major drawback of LLM-based reference-free evaluations and enable low-cost, accurate evaluations for developing NLP systems in open-ended settings. Our work focuses on this challenge of taking an existing automated evaluation metric (e.g. AlpacaEval) and a suspected spurious correlate (e.g. length) and producing a debiased metric.
We propose a simple, interpretable debiasing strategy for automated evaluation metrics based on basic, regression-based adjustments for observational causal inference. We view spurious correlates - such as the length of the response - as undesirable mediators VanderWeele (2015) in a causal graph and use regression-based causal inference Hernán \&amp; Robins (2010) techniques to provide simple adjustments to automated evaluations that control for any suspected spurious variables.</p>
<p>Applying this approach to the popular AlpacaEval benchmark, we show that controlling for length positively affects automated evaluation. We find that it is more correlated on average with the LMSYS Chatbot Arena (Zheng et al., 2023) than both (length-uncontrolled) AlpacaEval and MT-bench, and that it is significantly more robust to gaming the evaluation by increasing the verbosity of the output.
Our contributions are the following:</p>
<ul>
<li>We propose a simple regression-based approach to debias automated metrics, while maintaining desirable properties.</li>
<li>We apply the approach to AlpacaEval, producing AlpacaEval-LC, which is more robust to length-based spurious correlates.</li>
<li>We show that AlpacaEval-LC correlates better with the human evaluations of model rankings (Chatbot Arena) Fig. 1.</li>
</ul>
<h1>2 Background and Problem Setting</h1>
<p>Our work relates to both the classic literature on reference-free evaluations, as well as more recent developments in automated and human evaluation of chatbots. We describe some of these relevant works below, with some additional exposition on the details of AlpacaEval, which we study more closely in our debiasing experiments.</p>
<p>Reference-free evaluation metrics. Reference-free evaluation metrics, which aim to evaluate models on open-ended tasks without using any reference answer on that task, have a long history, including classic methods (Louis \&amp; Nenkova, 2013) and more recent neural supervised learning methods (Kryscinski et al., 2020; Sinha et al., 2020; Goyal \&amp; Durrett, 2020). While this latter class of algorithms has become sufficiently accurate that they match inter-annotator agreement rates, other works have shown that such measurements are heavily confounded by spurious correlations such as perplexity and length (Durmus et al., 2022). Recently, there has been a push to leverage LLMs as a zero-shot, reference-free evaluation measure (Zheng et al., 2023; Dubois et al., 2023; Li et al., 2023; Lin et al., 2024). In the chat setting, two well-known such benchmarks are AlpacaEval and MT-bench, both of which query an LM (based on GPT-4) to assess the quality of a weaker LM's output.</p>
<p>AlpacaEval. AlpacaEval is an LLM-based automated evaluation metric - it operates on a fixed set of 805 instructions chosen to be representative of user interactions on the Alpaca web demo (Taori et al., 2023). For each instruction, both a baseline model $b$ (currently GPT-4 turbo) and the evaluated model $m$ produce responses. A GPT-4 turbo-based evaluator then compares the responses head-to-head and outputs the probability of preferring the</p>
<p>evaluated model. A win rate is then computed as the expected probability that the autoevaluator prefers the evaluated model's output on the 805 instructions. This win rate serves as a performance measure of the evaluated LM chatbot.
Originally, AlpacaEval was designed as a development metric for the Alpaca instructionfollowing model (Taori et al., 2023) and AlpacaFarm simulator (Dubois et al., 2023). The metric was designed to control for certain biases, such as the order in which the model and baseline were presented, by randomizing their sequence. However, other factors like length and style effects were not controlled for, as the authors found that humans had similar biases on the analyzed data. Subsequent use of AlpacaEval as a leaderboard revealed that these uncontrolled biases could be significantly gamed by AI systems in ways that human biases would likely not have.
Important to our work is that AlpacaEval has several interpretable properties. As a win rate, its values are in $[0 \%, 100 \%]$, it has a symmetry to baseline swaps, i.e, AlpacaEval $(b, m)=$ $100 \%-\operatorname{AlpacaEval}(m, b)$, i.e., and comparing a baseline to itself is $\operatorname{AlpacaEval}(b, b)=50 \%$ Any post-hoc correction to AlpacaEval should maintain these properties, alongside the usual desiderata of being low-cost, accurate, and robustness.</p>
<p>Chatbot Arena. The automated approach to pairwise evaluation in AlpacaEval can be viewed as a low-cost approximation to the Chatbot Arena (Zheng et al., 2023), which aims to build real-world human evaluations through live interactions. The approach in Chatbot Arena is that users are presented with a pair of anonymized LMs, and they can send an instruction to both LMs simultaneously. The user receives responses from both LMs and rates the response that is of higher quality. In the end, the head-to-head comparisons are converted to Elo ratings (Elo, 1978) which serve as model scores. The difference in Elo ratings between two players can be converted to a win rate, and vice versa (Elo, 1978).
This approach has many desirable properties - it is driven by real users and the dynamic nature of the instructions makes it hard to saturate this benchmark. However, this metric cannot be used for model development due to the cost of running many live human evaluations.
In the remainder of this work, we will treat the Chatbot Arena as a silver standard, which we wish to approximate. Although Chatbot Arena likely still contains biases (e.g. internet users may focus on surface features rather than "hard to measure" capabilities such as factuality), it represents the largest publicly available human evaluation process today.</p>
<p>Setup. We define the problem of pairwise evaluation of a language model in the following way. Given an instruction $x$ sampled from a distribution $p(x)$, a baseline model generates a response $z_{b}$ and the evaluated model generates a response $z_{m}$. A human annotator then produces a preference $y \in{0,1}$ indicating whether the candidate response $z_{m}(y=1)$ is better than the baseline response $z_{b}(y=0)$. An automated surrogate such as AlpacaEval is a (potentially randomized) predictor $f\left(z_{m}, z_{b}, x\right)$ that aims to approximate the corresponding human label $p\left(y \mid z_{m}, z_{b}, x\right)$. AlpacaEval's current metric is the win rate winrate $(m, b)$ of the candidate model $m$ over the baseline $b$. I.e., the expected predicted preference of human annotators for the model response over the baseline's response winrate $(m, b)=$ $100 \cdot \mathbb{E}<em m="m">{x}\left[f\left(z</em>, x\right)\right]$.}, z_{b</p>
<h1>3 Length-Controlled AlpacaEval</h1>
<p>A major challenge in estimating preferences $f\left(z_{m}, z_{b}, x\right)$ is the spurious correlations problem. Specifically, consider a simple example in which there is a spurious correlate $c$ (e.g. length) such that heavily relying upon $c$ can be predictive of the human preferences $y$ between the model's and baseline's output. The confounder $c$ is initially predictive of $y$ but could become less predictive as model builders explicitly begin to optimize against the metric. Adopting this causal view, we ask our motivating question:</p>
<p>What would the AlpacaEval win rate be, if the outputs of the evaluated model $m$ had the same length as those of the baseline $b$ ?</p>
<p>Our goal in this section will be to operationalize this into a simple regression-based estimator. To be precise, we hypothesize that automated evaluation measures $f$ such as AlpacaEval return their quality estimates through a combination of direct effects that measure the quality of the model response and indirect effects that are mediated by spurious variables such as the length of outputs. The goal of controlling for the spurious correlates is thus equivalent to controlling these indirect effects. See Fig. 2 for a visual representation.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Length-Controlled AlpacaEval predicts the direct effect of the model (blue) on the auto-annotators' preference (white) when controlling for undesirable mediators (orange) and other useful features (gray).</p>
<p>This abstraction leads us to a simple approach for bias correction, inspired by methods for estimating Controlled Direct Effect (VanderWeele, 2010), for which simple Generalized Linear Models (GLMs) can provide reasonable estimates.</p>
<p>Length control via regression. Our approach will be to estimate the contribution of 3 different components to the AlpacaEval quality judgment:</p>
<ul>
<li>Model identity Whether an output comes from the baseline model $b$ or the evaluated model $m$ should impact the probability that an output wins the pairwise comparison.</li>
<li>Length of output The length of output is known to affect both human and model judgments of output quality (Dubois et al., 2023; Singhal et al., 2023), and so we expect this to also affect the win probability.</li>
<li>Instruction difficulty Models do not perform uniformly over instructions: the preference of humans will generally depend on the instruction. For example, the baseline might be much better for coding tasks than any other tasks. For every instruction we thus want to model the difficulty of that task for the baseline. Note that the (baseline) "instruction difficulty" is not caused by "model" but conditioning on it can enhance the precision of estimates in regression analysis by reducing unexplained variability Pearl (2009).</li>
</ul>
<p>We can obtain length corrected AlpacaEval score in two steps: (i) first, we can fit a model to these three attributes, and (ii) then we zero out the "length of output" term to obtain counterfactual estimates of AlpacaEval win rate.
Specifically, for the first step, we will collect a list of ratings $\left{x, z_{m}, z_{b}, m, b, y\right}$ from the AlpacaEval leaderboard, and then fit a regression model $q_{\theta, \phi, \phi}$ to predict $y$ from $x, z_{m}, z_{b}, m, b$.</p>
<p>The regression model. Motivated by the previous discussion, we will model the AlpacaEval predictions $f\left(z_{m}, z_{b}, x\right)$ with a logistic regression that has 3 terms: model, length, and instruction. We will first present the overall regression formula, explain the details of the featurization, and then describe some naturally appealing properties of our featurization.</p>
<p>$$
\begin{aligned}
&amp; q_{\theta, \phi, \phi}\left(y=1 \mid z_{m}, z_{b}, m, b, x\right):= \
&amp; \quad \text { logistic } \underbrace{\theta_{m}-\theta_{b}}<em b="b" m_="m,">{\text {Model }}+\underbrace{\phi</em>} \cdot \tanh \left(\frac{\operatorname{len}\left(z_{m}\right)-\operatorname{len}\left(z_{b}\right)}{\operatorname{std}\left(\operatorname{len}\left(z_{m}\right)-\operatorname{len}\left(z_{b}\right)\right)}\right)<em m="m">{\text {Length }}+\underbrace{\left(\psi</em>
\end{aligned}
$$}-\psi_{b}\right) \gamma_{x}}_{\text {Instruction }} \text { ). </p>
<p>The model and instruction terms are straightforward-they can be viewed as the log-linear contribution of the model $(m, b)$ and each instruction's difficulty $(\gamma)$ on the baseline win rate.</p>
<p>The length term is linear in a normalized length feature, where the normalizer standardizes the length to have unit variance and transforms this via a $\tanh$, as differences in lengths should have strong diminishing returns on the log odds.</p>
<p>Importantly, this formula fulfills the identity property, i.e., $q\left(y=1 \mid z_{b}, z_{b}, b, b, x\right)=0.5$, and symmetry property, i.e, $q\left(y \mid z_{m}, z_{b}, b, m, x\right)=1.0-q\left(y \mid z_{b}, z_{m}, b, m, x\right)$ of the original win rate. Identity holds as the length term is zero due to having no difference in length, while the other two terms are zero as the coefficients are identical. For symmetry note that $\operatorname{logistic}(x)=1-\operatorname{logistic}(-x)$, and it is clear that swapping $m$ and $b$ flips the sign of the model and instruction terms. For the length term, the same is true as flipping $m$ and $b$ negates the length difference, and $\tanh$ is an odd function. More generally any additive term that is antisymmetric and centered around 0 would satisfy the desired properties.</p>
<p>Obtaining length corrected (LC) win rate. Using the model from Eq. (1) we can answer the counterfactual question of what the automatic evaluation $f$ might be if the length of the evaluated model matched that of the base model, i.e., $\operatorname{len}\left(z_{m}\right)=\operatorname{len}\left(z_{b}\right)$. In this case, the second, length term becomes zero and we obtain the length corrected win rate estimate as</p>
<p>$$
\text { winrate }^{L C}(m, b)=100 \cdot \mathbb{E}<em m="m">{x}\left[\operatorname{logistic}\left(\theta</em>\right)\right]
$$}-\theta_{b}+\left(\psi_{m}-\psi_{b}\right) \gamma_{x</p>
<p>i.e., we remove the length term from the regression and compute the implied win rate.</p>
<p>Training. Training of the regression is simple and uses off-the-shelf libraries for fitting generalized linear models. Since our GLM uses a logit link function, we fit the model in Eq 1 using the cross-entropy loss $\mathcal{L}(\theta, \phi, \psi)=\mathbb{E}<em m="m">{p\left(y \mid z</em>, m, b, x\right)\right]$.}, z_{b}, m, b, x\right) p\left(z_{m}, z_{b}, m, b, x\right)}\left[q_{\theta, \phi, \psi}\left(y \mid z_{m}, z_{b</p>
<p>AlpacaEval's leaderboard uses a constant baseline $b$, so without loss of generality we can drop $\theta_{b}, \psi_{b}$, which can be absorbed into the corresponding parameters for $m$. In total, for a leaderboard with $M$ models and $N$ instructions, our GLM contains $3 M+N$ parameters to be estimated from $M N$ examples ( $\theta_{m}, \phi_{m, b}, \psi_{m}$ for each model, $\gamma_{x}$ for each instruction). This will be overdetermined when $M$ and $N$ are both large, as in the case of AlpacaEval ( $M&gt;128$, $N=805)$. However, to ensure our procedure is robust even for small $N$ and $M$, we use 5 -fold cross-validation with $L_{2}$ regularization on weights to avoid potential overfitting.
The one complexity of our regression is that the instruction difficulty term $\gamma_{x}$ is shared across models, and so we estimate this separately by first fitting a joint regression across all models with the $\psi_{m}-\psi_{b}$ term fixed to one and using the estimated $\gamma_{x}$ from this regression.
For the remaining regression coefficients, we simply fit $\theta, \phi$, and $\psi$ on the AlpacaEval predictions for each model separately, re-using the already estimated $\gamma_{x}$ as these do not depend on the model being evaluated. Fitting models separately is important as it implies that previously computed metrics won't change when adding a new model to the leaderboard. Of course, refitting $\gamma_{x}$ would give a better estimate of that parameter, but we don't think it is worth constantly updating the leaderboard when a new model is added.
Finally, we added weak regularization on $\phi_{m, b}$ to prevent an adversary from performing attacks that intentionally truncate sequences that a model performs poorly on. In this case, the poor performance of the model would be perfectly correlated with the short length, and the model builder would be able to exploit the length corrections to boost the performance of the model. Adding a regularization term makes it so that any model performance issues would be explained by the model terms first, and then any residual effects would be captured by the length effects, as intended. The regularization is weak enough that we empirically found it to not affect non-adversarial models.</p>
<h1>4 Results</h1>
<p>We apply our approach to all of AlpacaEval, as this benchmark has known length confounders, contains a large set of pre-computed LLM-based pairwise comparisons ( $&gt;120$ models, 805 instructions), and is widely used by the research community. We evaluate our approach on several measures of interest:</p>
<ul>
<li>Decreasing length gameability: We call a metric length gameable if simply prompting the model to be more or less verbose significantly affects the metric outcome. Ideally, length gameability should be low for two reasons. First, we would like evaluations that prioritize the content rather than the style of the answer. Second, the benchmark should not be too dependent on the prompting strategy as users usually think of evaluations of models rather than the entire system, which includes the prompt.</li>
<li>Correlation with Chatbot Arena: If our gameability and robustness metrics represent better capturing human preference, we should improve our correlation with chatbot arena. We measure Spearman rather than Pearson correlation as probabilities are log-linearly correlated with ELO ratings, rather than linearly.</li>
<li>Robustness and interpretability: Our corrected metric should be robust to simple adversarial attacks such as truncation, and be interpretable to users as a win rate.</li>
</ul>
<p>We show that AlpacaEval-LC fulfills all these goals. We release the code for all experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">AlpacaEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Length-controlled AlpacaEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">concise</td>
<td style="text-align: center;">standard</td>
<td style="text-align: center;">verbose</td>
<td style="text-align: center;">concise</td>
<td style="text-align: center;">standard</td>
<td style="text-align: center;">verbose</td>
</tr>
<tr>
<td style="text-align: center;">gpt4_1106_preview</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">51.6</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7B-Instruct-v0.1</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">23.2</td>
</tr>
<tr>
<td style="text-align: center;">gpt4_0613</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">33.8</td>
</tr>
<tr>
<td style="text-align: center;">claude-2.1</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">30.3</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo-1106</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: center;">alpaca-7b</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">6.8</td>
</tr>
</tbody>
</table>
<p>Figure 3: Length-controlled AlpacaEval decreases the sensitivity to prompting the evaluated model for more concise or verbose outputs.</p>
<h1>4.1 AlpacaEval-LC decreases length gameability</h1>
<p>A good evaluation metric should not be so sensitive to length that prompting for longer or shorter responses completely changes the metric. To measure gameability we prompted different models to "Answer with as much detail as possible." (verbose) or "Be as concise as possible while still providing all the necessary information to answer the question." (concise).
Figure 3 shows that AlpacaEval is highly length gameable. The baseline model (gpt4_1106_preview) fluctuates from $22.9 \%$ to $64.3 \%$ by varying the verbosity instruction in the prompt. Even worse, significant gains are possible by asking weaker models to be verbose, as seen with Claude-2.1.
In contrast, the length-controlled AlpacaEval has significantly lower gameability (gpt4_1106_preview's win rates now only fluctuate from $41.9 \%$ to $51.6 \%$ ), and rankings are generally stable to verbosity prompts. Quantitatively, the normalized standard deviation across the three verbosity prompts decreases from $25 \%$ to $10 \%$ from the length control.</p>
<h3>4.2 AlpacaEval-LC increases correlation with Chatbot Arena to 0.98</h3>
<p>Our prior experiments demonstrate that length control reduces the high sensitivity to length in AlpacaEval. However, our goal is not simply to make metrics that are less sensitive to length, but to produce metrics that are overall more representative of human judgments.
Figure 1 shows that controlling for length increased the Spearman correlation with Chat Arena from 0.94 to 0.98 . Of existing benchmarks, this difference is significant enough to make the length-corrected version of AlpacaEval the metric with the highest correlation with Chat Arena which we are aware of. Correlations are computed on every benchmark</p>
<p>that evaluates at least 25 models from the Chatbot Arena. AlpacaEval and AlpacaEval-LC have 38 such models, MT bench has 34 . The bootstrap p-value comparing the correlation with AlapcaEval's correlation is 0.07 and 0.06 compared to MT-bench.</p>
<p>Length control generally improves the rankings of proprietary models. Figure 4 shows leaderboard changes due to our length control approach. We see that proprietary models, which often generate shorter responses, perform much better on AlpacaEval-LC, and the biggest rank losses are in open-source models that have gone through the RLHF process Ouyang et al. (2022). Given that AlpacaEval is a potential optimization target for opensource language models, these results are consistent with the hypothesis that existing open models had exploited the length bias of AlpacaEval.</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: right;">Length</th>
<th style="text-align: right;">Win Rate</th>
<th style="text-align: right;">New Win Rate</th>
<th style="text-align: right;">Win Rate Gain</th>
<th style="text-align: right;">Rank Gain</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">gpt4_1106_preview</td>
<td style="text-align: right;">2049</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: right;">claude-3-opus-20240229</td>
<td style="text-align: right;">1388</td>
<td style="text-align: right;">29.0</td>
<td style="text-align: right;">40.4</td>
<td style="text-align: right;">11.4</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: right;">gpt4</td>
<td style="text-align: right;">1365</td>
<td style="text-align: right;">23.6</td>
<td style="text-align: right;">38.1</td>
<td style="text-align: right;">12.6</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: right;">Qwen1.5-72B-Chat</td>
<td style="text-align: right;">1549</td>
<td style="text-align: right;">26.5</td>
<td style="text-align: right;">36.6</td>
<td style="text-align: right;">10.1</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: right;">gpt4_0314</td>
<td style="text-align: right;">1371</td>
<td style="text-align: right;">22.1</td>
<td style="text-align: right;">35.3</td>
<td style="text-align: right;">12.2</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: right;">claude-3-sonnet-20240229</td>
<td style="text-align: right;">1420</td>
<td style="text-align: right;">25.6</td>
<td style="text-align: right;">34.9</td>
<td style="text-align: right;">9.3</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: right;">mistral-large-2402</td>
<td style="text-align: right;">1362</td>
<td style="text-align: right;">21.4</td>
<td style="text-align: right;">32.7</td>
<td style="text-align: right;">11.2</td>
<td style="text-align: right;">10</td>
</tr>
<tr>
<td style="text-align: right;">Samba-CoE-v0.2-best-of-16</td>
<td style="text-align: right;">1578</td>
<td style="text-align: right;">27.0</td>
<td style="text-align: right;">31.5</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: right;">gpt4_0613</td>
<td style="text-align: right;">1140</td>
<td style="text-align: right;">15.8</td>
<td style="text-align: right;">30.2</td>
<td style="text-align: right;">12.2</td>
<td style="text-align: right;">20</td>
</tr>
<tr>
<td style="text-align: right;">Snorkel-Mistral-PairRM-DPO-best-of-16</td>
<td style="text-align: right;">2616</td>
<td style="text-align: right;">34.9</td>
<td style="text-align: right;">30.0</td>
<td style="text-align: right;">$-4.9$</td>
<td style="text-align: right;">$-8$</td>
</tr>
<tr>
<td style="text-align: right;">Contextual-KTO-Mistral-PairRM</td>
<td style="text-align: right;">2521</td>
<td style="text-align: right;">33.2</td>
<td style="text-align: right;">29.7</td>
<td style="text-align: right;">$-3.5$</td>
<td style="text-align: right;">$-8$</td>
</tr>
<tr>
<td style="text-align: right;">pairrm-Yi-34B-Chat</td>
<td style="text-align: right;">2195</td>
<td style="text-align: right;">31.2</td>
<td style="text-align: right;">28.8</td>
<td style="text-align: right;">$-2.4$</td>
<td style="text-align: right;">$-8$</td>
</tr>
<tr>
<td style="text-align: right;">mistral-medium</td>
<td style="text-align: right;">1500</td>
<td style="text-align: right;">21.9</td>
<td style="text-align: right;">28.6</td>
<td style="text-align: right;">6.8</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: right;">claude-2</td>
<td style="text-align: right;">1069</td>
<td style="text-align: right;">17.2</td>
<td style="text-align: right;">28.2</td>
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: right;">Samba-CoE-v0.2</td>
<td style="text-align: right;">1469</td>
<td style="text-align: right;">21.8</td>
<td style="text-align: right;">27.6</td>
<td style="text-align: right;">5.8</td>
<td style="text-align: right;">$-1$</td>
</tr>
<tr>
<td style="text-align: right;">falcon-7b-instruct</td>
<td style="text-align: right;">478</td>
<td style="text-align: right;">2.1</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">1.9</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: right;">oasst-stt-pythia-12b</td>
<td style="text-align: right;">726</td>
<td style="text-align: right;">1.8</td>
<td style="text-align: right;">3.3</td>
<td style="text-align: right;">1.5</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: right;">guanaco-13b</td>
<td style="text-align: right;">1774</td>
<td style="text-align: right;">3.5</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">$-0.5$</td>
<td style="text-align: right;">$-12$</td>
</tr>
<tr>
<td style="text-align: right;">guanaco-7b</td>
<td style="text-align: right;">1364</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">$-0.0$</td>
<td style="text-align: right;">$-7$</td>
</tr>
<tr>
<td style="text-align: right;">baichuan-13b-chat</td>
<td style="text-align: right;">1727</td>
<td style="text-align: right;">2.0</td>
<td style="text-align: right;">2.1</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$-1$</td>
</tr>
</tbody>
</table>
<p>Figure 4: Closed source models often perform better (red) on length-controlled AlpacaEval as they are often shorter. The first column shows the length of outputs. The 4th and 5th columns, respectively, show the win rates and rank gains due to LC. The table shows the top 15 and bottom 5 systems (separated by the dark gray line) on AlpacaEval's leaderboard from March 19th 2024.</p>
<h1>4.3 AlpacaEval-LC is interpretable and robust</h1>
<p>Regularization makes AlpacaEval-LC robust to truncation. One potential issue with simple bias corrections is that they may be gamed through white-box adversarial attacks, e.g., postprocessing the outputs of models to make them look better on AlpacaEval-LC. One example of such an attack is to truncate all outputs to a few characters, besides those that are much better and around the same length as the baseline. A naive GLM fitted on such outputs should naturally predict very high win rates in the counterfactual world where outputs have the same length as the baseline. Indeed, when doing such post-processing to GPT-4 outputs, win rates increase from 3.7 (AlpacaEval 2.0) to 25.9 (AlpacaEval-LC, no regularization). To mitigate such adversarial attacks, our approach includes a regularization term on $\phi_{m, b}$. This decreases the gamed win rate to 12.2 (AlpacaEval-LC with regularization) while having an imperceptible impact on non-adversarial models.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: With our GLM we can predict what the win rate would be if the baseline was any other model from the leaderboard. The $x$-axis shows the baseline, and the $y$-axis the model. Both are in the same order.</p>
<p>Interpretability as a win rate. Figure 5 shows that LC win rates can be interpreted similarly to raw win rates. In particular, the baseline always has a win rate of $50 \%$ and winrate $(m, b)=$ $100 \%-$ winrate $(b, m) \in[0 \%, 100 \%]$. This seems very natural but wouldn't hold for most length-correction methods, such as normalizing by length.
More interestingly, a nice property of our GLM is that once we fit the weights for one baseline, we can predict the win rate between any pair of models on the leaderboard. As a result, we can predict the leaderboard for any other baseline as seen in Fig. 5.</p>
<h1>4.4 Different length control methods</h1>
<p>Table 1: Length-controlled win rate has the best Chatbot Arena correlation and gameability from considered methods, while still being relatively robust to adversarial attacks. Each row is a different metric we considered.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Chatbot Arena <br> correlation $(\uparrow)$</th>
<th style="text-align: center;">Gameability $(\downarrow)$</th>
<th style="text-align: center;">Adversarial win <br> rate gain $(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Win rate</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">$26 \%$</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Length-controlled</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">8.5</td>
</tr>
<tr>
<td style="text-align: center;">Length-normalized</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">3.6</td>
</tr>
<tr>
<td style="text-align: center;">Length-balanced</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">40.8</td>
</tr>
</tbody>
</table>
<p>Let's now briefly discuss two other potential family length-correction methods that have been proposed in the community (Duong, 2024; Galambosi, 2024; Teortaxes, 2024).</p>
<p>Length-balanced win rate. Another common way to control some covariates is through stratification. One potential metric, dubbed length-balanced (LB) win rate, would thus be to compute the average win rate stratified on examples where the model outputs are (1) longer and (2) shorter than the baseline (Duong, 2024). LB satisfies many of the desiderata of length control but has one main downside: robustness.</p>
<p>In particular, stratification relies upon having enough samples within each stratum, otherwise the estimates may rapidly become unstable. This can increase variance, e.g., if one model is naturally longer than another, but can also introduce adversarial vulnerabilities.
The first and last rows in Table 1 show that length-balanced win rates improve both the length gameability (measured by the normalized standard deviation of win rate across concise/standard/verbose prompts) and the Chatbot arena correlation. However, this</p>
<p>approach is strictly dominated by our length-controlled method-in terms of correlation with Chatbot Arena, gameability, and adversarial win rate gains from truncating bad GPT-4 outputs as discussed in Section 4.3.</p>
<p>Length-normalized win rate. Another option Galambosi (2024); Teortaxes (2024) is to directly normalize the win rate by a function of the length of the model's and baseline's output. We have tried several variations on normalization (e.g. directly dividing by lengths, logistic function of lengths, etc). In our experiments, the function that performed best was dividing the raw win rate by a temperature-scaled logistic function of the average difference of lengths. We call this metric length-normalized (LN) win rate.</p>
<p>Table 1 shows that this simple LN win rate performs surprisingly well on many of the metrics. We chose to present and implement the length-controlled (LC) win rate, as it is more principled (as an estimate of the direct effect), interpretable (as a win rate), and performs slightly better on all quantitative metrics except adversarial gameability.</p>
<h1>5 Discussion</h1>
<p>Table 2: LLM judges self-bias but the ranking is surprisingly stable. Each column shows the leaderboard on AlpacaEval dataset as evaluated by different LLM judges. Rows show the win rate of the evaluated models against the judge.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">gpt-4-1106</th>
<th style="text-align: center;">claude-3-opus</th>
<th style="text-align: center;">mistral-large</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">gpt-4-1106</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-opus</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">47.5</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">45.5</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4-0613</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">28.9</td>
</tr>
</tbody>
</table>
<p>Other biases. Length is a well-known bias of automated evaluators of chatbot LLMs but several others have been noted, including a bias of models towards their own outputs (Zheng et al., 2023), or presence of lists (Dubois et al., 2023). While we focus on a more detailed study of length biases here, we note that the same approaches can be applied to other biases by representing them as additional features in the logistic regression.
Additionally, our preliminary explorations of self-annotator biases show that the effect exists but is often smaller than general model differences. Table 2 shows that the ranking of considered models does not change when using different annotators. In particular, Claude 3 Opus prefers GPT4 Preview, and Mistral Large prefers the former two than itself.</p>
<p>Length-controlling in RLHF. Our work is closely related to the recent work that aims to debias (implicit or explicit) reward models used to finetune LLMs with RLHF (Singhal et al. (2023)). For example Shen et al. (2023); Chen et al. (2024) try to train a reward model that is uncorrelated to length by making it predict the length at the same time as the reward and disentangle the two. Park et al. (2024) extends this intuition to the case of implicit reward models. This type of debiasing would not work out-of-the-box in typical auto-evaluation settings, e.g. AlpacaEval, which uses closed-source LLM as judges rather than training a reward model. Our post-hoc debiasing could however be used in the RLHF setting, and we encourage future work to look into that.</p>
<p>Limitations. Firstly, we only tested our proposed debiasing mechanism on the AlapcaEval benchmark, which uses a set of relatively simple English instructions and a particular prompt for the LLM judge. Secondly, AlpacaEval-LC is based on the simplifying assumption that you would like to compare the model and the baseline as if they had the same length. Finally, we do not (aim to) solve any other issues associated with the use of an LLM-judge Zheng et al. (2023). Despite these limitations, we show that the correlation with Chatbot</p>
<p>Arena increases significantly, which suggests that AlpacaEval-LC takes a step in the right direction.</p>
<p>Conclusion. We propose a simple method for mitigating the length bias of LLM-based automatic evaluations, specifically, AlpacaEval. The procedure consists of fitting a generalized linear model to predict the auto-evaluators preferences, conditioned on the length of the models' output. We then get the length-controlled preference by predicting what the auto-evaluator would have preferred if the model's output and the baseline's output had the same length. We show that the resulting length-controlled AlpacaEval, has higher correlations with humans, has much less length bias, and is robust (hard to game).</p>
<h1>Acknowledgments</h1>
<p>We thank Xuechen Li, Rohan Taori, and Tianyi Zhang for help maintaining AlpacaEval. We thank Viet Hoang Tran Duong for suggesting to consider length-balanced win rates. We thank the Twitter ML community for emphasizing the need of length-controlled autoevaluations. We thank the community for all the 100+ models they added to AlpacaEval. We thank OpenAI and Together AI for API credits to generate outputs and evaluate models. We gratefully acknowledge the support of an Open Philanthropy Project Award. Tatsunori Hashimoto is supported by a gift from Open Philanthropy and by the Tianqiao and Chrissy Chen Institute. Yann Dubois is supported by a Knights-Hennessy Scholarship.</p>
<h2>References</h2>
<p>Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. Odin: Disentangled reward mitigates hacking in rlhf. arXiv preprint arXiv:2402.07319, 2024.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.</p>
<p>Viet Hoang Tran Duong. Length-balanced alpacaeval 2.0, 2024. URL https://github.com/ tatsu-lab/alpaca_eval/issues/225#issue-2115462149.</p>
<p>Esin Durmus, Faisal Ladhak, and Tatsunori Hashimoto. Spurious correlations in referencefree evaluation of text generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1443-1454, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.102. URL https://aclanthology.org/2022.acl-long. 102.</p>
<p>Arpad E Elo. The rating of chessplayers: Past and present. Arco Publishing, 1978.
Balazs Galambosi. Advanced length-normalized alpacaeval 2.0, 2024. URL https://github. com/tatsu-lab/alpaca_eval/issues/225#issuecomment-1942201420.</p>
<p>Tanya Goyal and Greg Durrett. Evaluating factuality in generation with dependency-level entailment. In Findings of the Association for Computational Linguistics: EMNLP 2020, 2020.</p>
<p>Miguel A Hernán and James M Robins. Causal inference, 2010.
Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. Benchmarking cognitive biases in large language models as evaluators. arXiv preprint arXiv:2309.17012, 2023.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual consistency of abstractive text summarization. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9332-9346, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.750. URL https://aclanthology.org/2020.emnlp-main. 750.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.</p>
<p>Bill Yuchen Lin, Khyathi Chandu, Faeze Brahman, Yuntian Deng, Abhilasha Ravichander, Valentina Pyatkin, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild, 2024. URL https://huggingface.co/ spaces/allenai/WildBench.</p>
<p>Annie Louis and Ani Nenkova. Automatically assessing machine summary content without a gold standard. Computational Linguistics, 39(2):267-300, June 2013. doi: 10.1162/COLI_ a_00123. URL https://aclanthology.org/13-2002.
J. Novikova, O. Dušek, A. C. Curry, and V. Rieser. Why we need new evaluation metrics for NLG. In Empirical Methods in Natural Language Processing (EMNLP), 2017.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.</p>
<p>Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024.</p>
<p>Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009. ISBN 052189560X.</p>
<p>Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing Huang. Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback. arXiv preprint arXiv:2310.05199, 2023.</p>
<p>Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf, 2023.</p>
<p>Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe, William L. Hamilton, and Joelle Pineau. Learning an unreferenced metric for online dialogue evaluation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2430-2441, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.220. URL https://aclanthology.org/2020.acl-main. 220.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Teortaxes. Length-normalized alpacaeval 2.0, 2024. URL https://x.com/teortaxesTex/ status/1750495017771176301?s=20.</p>
<p>Tyler VanderWeele. Explanation in causal inference: methods for mediation and interaction. Oxford University Press, 2015.</p>
<p>Tyler J. VanderWeele. Controlled direct and mediated effects: Definition, identification and bounds. Scandinavian Journal of Statistics, 38, 2010. URL https://api.semanticscholar. org/CorpusID:12046639.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.</p>
<p>Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models. arXiv preprint arXiv:2307.03025, 2023.</p>
<p>Y. Yeh, M. Eskenazi, and S. Mehri. A comprehensive assessment of dialog evaluation metrics. In The First Workshop on Evaluations and Assessments of Neural Conversation Systems, pp. 15-33, Online, November 2021. Association for Computational Linguistics.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv, abs/2306.05685, 2023. URL https://api.semanticscholar.org/CorpusID:259129398.</p>            </div>
        </div>

    </div>
</body>
</html>
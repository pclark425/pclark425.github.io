<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8742 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8742</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8742</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278788866</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16927v1.pdf" target="_blank">Latent Principle Discovery for Language Model Self-Improvement</a></p>
                <p><strong>Paper Abstract:</strong> When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively, requires a labor-intensive annotation process. To automate this process, we propose eliciting these latent attributes guiding model reasoning towards human-preferred responses by explicitly modeling them in a self-correction setting. Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering. Specifically, we employ an approximation of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses. We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on IFEval. We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance. The gains our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8742.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8742.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Taught Principle Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Monte Carlo EM algorithm that elicits latent, natural-language "principles" from an LM (E-step) via gold-hinted sampling and rejection, then fine-tunes the LM to invoke those principles to perform intrinsic two-turn self-refinement (M-step); optional clustering compresses principles into an interpretable constitution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct; Granite-3.1-8B-Instruct; Qwen2.5-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three instruction-tuned autoregressive transformer LMs evaluated in experiments: Llama-3.1-8B-Instruct (≈8B), Granite-3.1-8B-Instruct (≈8B), Qwen2.5-7B-Instruct (≈7B). All were fine-tuned in STaPLe's M-steps (supervised fine-tuning on discovered trajectories) with sequence length 4096, 3 epochs, LR 1e-6.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Taught Principle Learning (STaPLe)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate-then-refine with explicit latent principle z: (E-step) sample N candidate principles z and critiques (hinted with gold/reference), generate corresponding refinements y2, accept refinements via rejection sampling using a validator (Rouge-L or LLM judge) that measures improvement over the initial response y1; choose best improving trajectory (z, y2). (M-step) supervised fine-tune LM on saved (x, y1, z, y2) trajectories so the LM learns to invoke z and perform intrinsic two-turn refinement. Optionally cluster principles (posterior regularization) to compress to medoid representatives (a constitution) then train on those labels.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction-following / open-ended generation evaluation (MT-Bench, AlpacaEval-2.0-LC, IFEval); intrinsic self-correction analysis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended instruction-following and chat evaluation benchmarks: MT-Bench (multi-turn chat, 80 prompts), AlpacaEval-2.0-LC (805 instruction-following prompts with length control), IFEval (541 prompts, principle-following rubrics). Evaluations also use an LLM judge (Prometheus-8x7B-v2.0) for principle-following assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Aggregated across models and iterations, STaPLe produced substantive gains: reported +8–10% AlpacaEval win-rate (abstract), ~+0.3 average on MT-Bench (avg.) and +19–23% IFEval principle-following win-rate (abstract summary). Detailed tables: e.g., by iteration 3 STaPLe outperformed STaR and initial policy by +0.16–+0.29 MT-Bench (avg.), +3.6%–+9.2% AlpacaEval win-rate, and +7.9%–+21.0% IFEval principle-following win-rate (numbers vary by model); per-table values for Iter4 (example): Llama STaPLe Iter4 AlpacaEval 33.4 (vs initial 26.9), Qwen STaPLe Iter4 AlpacaEval 40.2 (vs initial 30.4). Prometheus principle-following win-rates vs base policy reached ~70–76% across models/iterations (see Tables 2–5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial policies (instruction-tuned LMs) and competing baselines: Initial policies had lower scores (e.g., AlpacaEval initial: Llama 26.9, Granite 30.2, Qwen 30.4; MT-Bench averages around 6.8–7.8). Baselines such as Self-Refine and STaR improved over initial policy but by smaller amounts: STaR Iter4 and Self-Refine gave smaller gains than STaPLe (see Tables 1–3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering + latent-variable Monte Carlo EM using the LM as both principle generator and generator of refinements; rejection sampling on candidate (z,y2) pairs where validator is an improvement metric (Rouge-L F1 or LLM-as-judge). Then supervised fine-tuning to internalize principle-conditioned refinement behavior. Optional hierarchical agglomerative clustering over sentence embeddings (medoid representatives) acts as posterior regularization to produce a condensed, human-readable constitution.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements across benchmarks and internal evaluations: STaPLe yields higher MT-Bench averages (≈+0.11 over best baseline, Turn-2 +0.22), larger AlpacaEval win-rates (+5–10% vs initial; +1.6–2.8% vs STaR), and higher IFEval principle-following win-rates (+5–23% depending on metric and iteration). Prometheus judge win-rates comparing refined responses to base policy typically 62–76% for STaPLe across iterations (Tables 2–5). Intrinsic self-correction win-rates (refined vs initial response assessed on invoked principle) increase over iterations (Table 5: e.g., Qwen Iter4 82.1%). Clustering (constrained STaPLe) largely preserves or sometimes improves performance while increasing interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include diminishing returns and occasional degradation in later iterations for some models (Llama-8B and Granite-8B showed slight score drops by iteration 4 while Qwen continued improving); dependency on the mining corpus distribution and quality of gold references (noisy or adversarial gold can induce harmful principles); risk of copying gold if principle prompts are unconstrained (mitigated by prompt design and clustering); algorithm discards samples where no refinement improves over initial response (so improvement dataset D' excludes such cases); computational cost (inference budget ~3N+1 with N=16, plus GPUs for E-step); clustering hyperparameters (distance threshold δ) required manual tuning or HPO; the method focuses on two-turn self-correction only and may not directly generalize to longer multi-step refinement without modification. Authors explicitly warn about adversarial use where principles could be mined to misalign the model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to STaR (Self-Taught Reasoner modified for non-verifiable responses) and Self-Refine (prompted multi-turn refinement) as baselines: STaPLe outperforms STaR consistently across iterations and benchmarks (see Tables 2–3), and typically outperforms Self-Refine (Self-Refine helped some models zero-shot but was less effective for Llama-8B). The paper situates STaPLe relative to prior self-correction/self-play methods (SPIN, SCoRe, RISE, ProMiSe), latent-CoT and EM-based latent reasoning approaches (TRICE, LaTRO, BoLT) but does not run head-to-head experiments with those exact algorithms; instead, STaPLe blends latent-variable EM, on-policy principle discovery, and supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Multiple ablations reported: (1) Similarity validator: Rouge-L F1 vs LLM-as-a-judge (Phi-4) — judge-based rejection sampling produced broadly similar results with small differences (some MT-Bench turn differences), demonstrating generality. (2) Label replacement (clustering) schemes: medoid, mode, and perplexity-based (PPL) — medoid performed best overall, PPL discarded some samples and produced slightly different tradeoffs. (3) Clustering distance threshold: manual vs Bayesian hyperparameter optimization — HPO produced modest improvements. Ablations showed constrained STaPLe (clustering) often matches or outperforms unconstrained STaPLe and helps avoid late-iteration degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Principle Discovery for Language Model Self-Improvement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8742.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8742.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Taught Reasoner (STaR) - modified baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior generate-critique-revise pipeline where the model generates critiques conditioned on a gold response and then revises answers; here adapted as a baseline that trains on best refined responses without explicit latent principle labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct; Granite-3.1-8B-Instruct; Qwen2.5-7B-Instruct (applied as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same instruction-tuned models as evaluated with STaPLe; STaR baseline variants were fine-tuned on best refined responses (no z-principle labels) following the STaPLe sampling regime.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>STaR-style critique-and-revise (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial response, model-conditioned critique conditioned on gold, generate revision; supervised fine-tuning on the best refined response (no explicit principle label). This baseline mirrors STaPLe but omits the explicit latent-principle verbalization and principle-conditioned training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same instruction-following / open-ended generation benchmarks (MT-Bench, AlpacaEval-2.0-LC, IFEval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended instruction-following evaluation datasets used to compare self-correction and alignment methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>STaR produced improvements over initial policy (see Tables 1–3), but was typically outperformed by STaPLe. Example: STaR Iter4 results typically improved over initial policy but had lower MT-Bench Turn-2 and AlpacaEval win-rates than STaPLe's Iter4 across models; Prometheus win-rates for STaR Iter3/4 were often 60–71% (Table 3), lower than STaPLe's corresponding entries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial (instruction-tuned) policies scores (e.g., AlpacaEval: Llama 26.9, Granite 30.2, Qwen 30.4) — STaR improves on these baseline numbers but by less than STaPLe.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted critique + revision; supervised fine-tuning on selected best refined responses (no explicit principle-conditioned training).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Tables 2–3 show STaR yields measurable gains vs initial policy over iterations, and Prometheus judge results show STaR's refinements are preferred over base policy in many cases (win-rates >50%). However STaPLe consistently exceeds STaR by the reported margins (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>STaR lacks the explicit principle label that STaPLe uses; thus, it is less interpretable and (empirically) learns less effective intrinsic self-correction. When applied to non-verifiable tasks, STaR-style training without principle labels appears less effective than principle-conditioned learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared to STaPLe in experiments; STaPLe outperforms STaR across benchmarks and iterations. STaR is characterized as a modified baseline in the paper and contextualized with prior STaR literature.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>The paper treats STaR as a baseline; ablations focus on STaPLe variants rather than on STaR variants specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Principle Discovery for Language Model Self-Improvement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8742.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8742.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (prompted multi-turn refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot, prompted multi-turn refinement approach where the model produces a self-critique, proposes changes, and reflects those changes into a revised answer without explicit training on synthesized refinement trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied as a zero-shot baseline to the same 8B/7B instruct models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompted inference technique (no additional fine-tuning) applied to evaluated instruction-tuned models in zero-shot to produce critique+revision.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Prompted self-refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At inference-time, the model generates a critique of its initial response, lists actionable changes, and then rewrites the response incorporating those changes — no principle labels or M-step fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same instruction-following benchmarks (AlpacaEval, MT-Bench, IFEval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluations of instruction-following quality under zero-shot self-refinement prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Mixed results: Self-Refine improved Granite and Qwen models on some metrics, but did not improve Llama-8B (Table 1). Example: Granite Self-Refine AlpacaEval ~31.7 vs initial 30.2; Llama Self-Refine slightly decreased vs initial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial policy metrics as in paper (e.g., AlpacaEval initial Llama 26.9).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Inference-time prompting that elicits critique and revision (no training), i.e., generate-then-reflect with no latent principle labels.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical numbers in Table 1 show Self-Refine can produce wins for some models (Granite, Qwen) but is less reliable overall than STaPLe; the paper interprets this as zero-shot self-refinement being insufficient without pre-identified principles for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero-shot Self-Refine was ineffective for Llama-8B, suggesting that stronger or fine-tuned models may be needed for reliable prompted self-refinement; lacks interpretability (no principle labels) and doesn't provide training signal to internalize refinement behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared as a baseline to STaPLe and STaR; STaPLe tends to outperform Self-Refine when the latter is less effective zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>No dedicated ablation reported for Self-Refine in the paper beyond the baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Principle Discovery for Language Model Self-Improvement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8742.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8742.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPIN (self-play fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior self-play fine-tuning method that compares model generations to ground-truth annotated responses, creating preference pairs and iteratively fine-tuning with contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-play fine-tuning (SPIN)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>On-policy sampling of outputs, compare against gold, produce preference pairs, fine-tune with contrastive objectives and iterate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General self-improvement / self-play frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Framework for turning weak LMs into stronger models via iterative self-play and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Self-play sampling and contrastive fine-tuning (not used in experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as related work; the paper notes conceptual similarity (variance-reduced self-play equivalence) but does not provide direct empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited as related prior work demonstrating self-play fine-tuning can strengthen weaker LMs; STaPLe is connected theoretically via self-play equivalence but uses latent principles and EM.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Principle Discovery for Language Model Self-Improvement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8742.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8742.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoRe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCoRe</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent multi-turn RL formulation for self-correction that improves initial attempts and uses reward shaping to encourage successive refinement; mentioned as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reinforcement-learning-based self-correction (SCoRe)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Multi-turn RL that trains models to correct outputs across multiple refinement turns using shaped rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Self-correction / refinement tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-turn self-correction framed as an MDP to learn policies that refine outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Offline RL over generated trajectories (cited, not used here).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned in related work as improving self-correction; no direct empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned among RL-based approaches; STaPLe differs by using EM and supervised fine-tuning on principle-conditioned trajectories rather than RL.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Principle Discovery for Language Model Self-Improvement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8742.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8742.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RISE/ProMiSe/others</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RISE / ProMiSe / other RL/self-correction methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Various cited works (e.g., RISE frames multi-turn self-correction with best-of-N and offline RL; ProMiSe extends to smaller models) representing RL and generation-based self-correction approaches mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RL-based and generation-based self-correction methods (RISE, ProMiSe, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Approaches that treat self-correction as a sequential decision process (MDP), sample rollouts (best-of-N), and apply offline RL or reward shaping to train a self-correcting policy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Self-correction and reasoning benchmarks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-turn self-correction for improving response quality via reinforcement learning or structured rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Best-of-N sampling, offline RL, reward shaping (mentioned but not used experimentally in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited prior improvements in literature; STaPLe is presented as an alternative that leverages latent principles and supervised fine-tuning rather than RL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>STaPLe contrasted conceptually (EM + latent principles) against RL-based methods in related work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Latent Principle Discovery for Language Model Self-Improvement', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Taught Reasoner (STaR) <em>(Rating: 2)</em></li>
                <li>Self-Refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-improvement in language models: The sharpening mechanism <em>(Rating: 1)</em></li>
                <li>Training language models to self-correct via reinforcement learning <em>(Rating: 1)</em></li>
                <li>SPIN: Self-play fine-tuning converts weak language models to strong language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8742",
    "paper_id": "paper-278788866",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "STaPLe",
            "name_full": "Self-Taught Principle Learning",
            "brief_description": "A Monte Carlo EM algorithm that elicits latent, natural-language \"principles\" from an LM (E-step) via gold-hinted sampling and rejection, then fine-tunes the LM to invoke those principles to perform intrinsic two-turn self-refinement (M-step); optional clustering compresses principles into an interpretable constitution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct; Granite-3.1-8B-Instruct; Qwen2.5-7B-Instruct",
            "model_description": "Three instruction-tuned autoregressive transformer LMs evaluated in experiments: Llama-3.1-8B-Instruct (≈8B), Granite-3.1-8B-Instruct (≈8B), Qwen2.5-7B-Instruct (≈7B). All were fine-tuned in STaPLe's M-steps (supervised fine-tuning on discovered trajectories) with sequence length 4096, 3 epochs, LR 1e-6.",
            "reflection_method_name": "Self-Taught Principle Learning (STaPLe)",
            "reflection_method_description": "Generate-then-refine with explicit latent principle z: (E-step) sample N candidate principles z and critiques (hinted with gold/reference), generate corresponding refinements y2, accept refinements via rejection sampling using a validator (Rouge-L or LLM judge) that measures improvement over the initial response y1; choose best improving trajectory (z, y2). (M-step) supervised fine-tune LM on saved (x, y1, z, y2) trajectories so the LM learns to invoke z and perform intrinsic two-turn refinement. Optionally cluster principles (posterior regularization) to compress to medoid representatives (a constitution) then train on those labels.",
            "task_name": "Instruction-following / open-ended generation evaluation (MT-Bench, AlpacaEval-2.0-LC, IFEval); intrinsic self-correction analysis",
            "task_description": "Open-ended instruction-following and chat evaluation benchmarks: MT-Bench (multi-turn chat, 80 prompts), AlpacaEval-2.0-LC (805 instruction-following prompts with length control), IFEval (541 prompts, principle-following rubrics). Evaluations also use an LLM judge (Prometheus-8x7B-v2.0) for principle-following assessments.",
            "performance_with_reflection": "Aggregated across models and iterations, STaPLe produced substantive gains: reported +8–10% AlpacaEval win-rate (abstract), ~+0.3 average on MT-Bench (avg.) and +19–23% IFEval principle-following win-rate (abstract summary). Detailed tables: e.g., by iteration 3 STaPLe outperformed STaR and initial policy by +0.16–+0.29 MT-Bench (avg.), +3.6%–+9.2% AlpacaEval win-rate, and +7.9%–+21.0% IFEval principle-following win-rate (numbers vary by model); per-table values for Iter4 (example): Llama STaPLe Iter4 AlpacaEval 33.4 (vs initial 26.9), Qwen STaPLe Iter4 AlpacaEval 40.2 (vs initial 30.4). Prometheus principle-following win-rates vs base policy reached ~70–76% across models/iterations (see Tables 2–5).",
            "performance_without_reflection": "Initial policies (instruction-tuned LMs) and competing baselines: Initial policies had lower scores (e.g., AlpacaEval initial: Llama 26.9, Granite 30.2, Qwen 30.4; MT-Bench averages around 6.8–7.8). Baselines such as Self-Refine and STaR improved over initial policy but by smaller amounts: STaR Iter4 and Self-Refine gave smaller gains than STaPLe (see Tables 1–3).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering + latent-variable Monte Carlo EM using the LM as both principle generator and generator of refinements; rejection sampling on candidate (z,y2) pairs where validator is an improvement metric (Rouge-L F1 or LLM-as-judge). Then supervised fine-tuning to internalize principle-conditioned refinement behavior. Optional hierarchical agglomerative clustering over sentence embeddings (medoid representatives) acts as posterior regularization to produce a condensed, human-readable constitution.",
            "number_of_iterations": 4,
            "evidence_for_improvement": "Quantitative improvements across benchmarks and internal evaluations: STaPLe yields higher MT-Bench averages (≈+0.11 over best baseline, Turn-2 +0.22), larger AlpacaEval win-rates (+5–10% vs initial; +1.6–2.8% vs STaR), and higher IFEval principle-following win-rates (+5–23% depending on metric and iteration). Prometheus judge win-rates comparing refined responses to base policy typically 62–76% for STaPLe across iterations (Tables 2–5). Intrinsic self-correction win-rates (refined vs initial response assessed on invoked principle) increase over iterations (Table 5: e.g., Qwen Iter4 82.1%). Clustering (constrained STaPLe) largely preserves or sometimes improves performance while increasing interpretability.",
            "limitations_or_failure_cases": "Reported limitations include diminishing returns and occasional degradation in later iterations for some models (Llama-8B and Granite-8B showed slight score drops by iteration 4 while Qwen continued improving); dependency on the mining corpus distribution and quality of gold references (noisy or adversarial gold can induce harmful principles); risk of copying gold if principle prompts are unconstrained (mitigated by prompt design and clustering); algorithm discards samples where no refinement improves over initial response (so improvement dataset D' excludes such cases); computational cost (inference budget ~3N+1 with N=16, plus GPUs for E-step); clustering hyperparameters (distance threshold δ) required manual tuning or HPO; the method focuses on two-turn self-correction only and may not directly generalize to longer multi-step refinement without modification. Authors explicitly warn about adversarial use where principles could be mined to misalign the model.",
            "comparison_to_other_methods": "Compared directly to STaR (Self-Taught Reasoner modified for non-verifiable responses) and Self-Refine (prompted multi-turn refinement) as baselines: STaPLe outperforms STaR consistently across iterations and benchmarks (see Tables 2–3), and typically outperforms Self-Refine (Self-Refine helped some models zero-shot but was less effective for Llama-8B). The paper situates STaPLe relative to prior self-correction/self-play methods (SPIN, SCoRe, RISE, ProMiSe), latent-CoT and EM-based latent reasoning approaches (TRICE, LaTRO, BoLT) but does not run head-to-head experiments with those exact algorithms; instead, STaPLe blends latent-variable EM, on-policy principle discovery, and supervised fine-tuning.",
            "ablation_study_results": "Multiple ablations reported: (1) Similarity validator: Rouge-L F1 vs LLM-as-a-judge (Phi-4) — judge-based rejection sampling produced broadly similar results with small differences (some MT-Bench turn differences), demonstrating generality. (2) Label replacement (clustering) schemes: medoid, mode, and perplexity-based (PPL) — medoid performed best overall, PPL discarded some samples and produced slightly different tradeoffs. (3) Clustering distance threshold: manual vs Bayesian hyperparameter optimization — HPO produced modest improvements. Ablations showed constrained STaPLe (clustering) often matches or outperforms unconstrained STaPLe and helps avoid late-iteration degradation.",
            "uuid": "e8742.0",
            "source_info": {
                "paper_title": "Latent Principle Discovery for Language Model Self-Improvement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "STaR",
            "name_full": "Self-Taught Reasoner (STaR) - modified baseline",
            "brief_description": "A prior generate-critique-revise pipeline where the model generates critiques conditioned on a gold response and then revises answers; here adapted as a baseline that trains on best refined responses without explicit latent principle labels.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct; Granite-3.1-8B-Instruct; Qwen2.5-7B-Instruct (applied as baseline)",
            "model_description": "Same instruction-tuned models as evaluated with STaPLe; STaR baseline variants were fine-tuned on best refined responses (no z-principle labels) following the STaPLe sampling regime.",
            "reflection_method_name": "STaR-style critique-and-revise (baseline)",
            "reflection_method_description": "Generate initial response, model-conditioned critique conditioned on gold, generate revision; supervised fine-tuning on the best refined response (no explicit principle label). This baseline mirrors STaPLe but omits the explicit latent-principle verbalization and principle-conditioned training.",
            "task_name": "Same instruction-following / open-ended generation benchmarks (MT-Bench, AlpacaEval-2.0-LC, IFEval)",
            "task_description": "Open-ended instruction-following evaluation datasets used to compare self-correction and alignment methods.",
            "performance_with_reflection": "STaR produced improvements over initial policy (see Tables 1–3), but was typically outperformed by STaPLe. Example: STaR Iter4 results typically improved over initial policy but had lower MT-Bench Turn-2 and AlpacaEval win-rates than STaPLe's Iter4 across models; Prometheus win-rates for STaR Iter3/4 were often 60–71% (Table 3), lower than STaPLe's corresponding entries.",
            "performance_without_reflection": "Initial (instruction-tuned) policies scores (e.g., AlpacaEval: Llama 26.9, Granite 30.2, Qwen 30.4) — STaR improves on these baseline numbers but by less than STaPLe.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted critique + revision; supervised fine-tuning on selected best refined responses (no explicit principle-conditioned training).",
            "number_of_iterations": 4,
            "evidence_for_improvement": "Tables 2–3 show STaR yields measurable gains vs initial policy over iterations, and Prometheus judge results show STaR's refinements are preferred over base policy in many cases (win-rates &gt;50%). However STaPLe consistently exceeds STaR by the reported margins (see paper).",
            "limitations_or_failure_cases": "STaR lacks the explicit principle label that STaPLe uses; thus, it is less interpretable and (empirically) learns less effective intrinsic self-correction. When applied to non-verifiable tasks, STaR-style training without principle labels appears less effective than principle-conditioned learning.",
            "comparison_to_other_methods": "Directly compared to STaPLe in experiments; STaPLe outperforms STaR across benchmarks and iterations. STaR is characterized as a modified baseline in the paper and contextualized with prior STaR literature.",
            "ablation_study_results": "The paper treats STaR as a baseline; ablations focus on STaPLe variants rather than on STaR variants specifically.",
            "uuid": "e8742.1",
            "source_info": {
                "paper_title": "Latent Principle Discovery for Language Model Self-Improvement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (prompted multi-turn refinement)",
            "brief_description": "A zero-shot, prompted multi-turn refinement approach where the model produces a self-critique, proposes changes, and reflects those changes into a revised answer without explicit training on synthesized refinement trajectories.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied as a zero-shot baseline to the same 8B/7B instruct models",
            "model_description": "Prompted inference technique (no additional fine-tuning) applied to evaluated instruction-tuned models in zero-shot to produce critique+revision.",
            "reflection_method_name": "Prompted self-refinement",
            "reflection_method_description": "At inference-time, the model generates a critique of its initial response, lists actionable changes, and then rewrites the response incorporating those changes — no principle labels or M-step fine-tuning.",
            "task_name": "Same instruction-following benchmarks (AlpacaEval, MT-Bench, IFEval)",
            "task_description": "Evaluations of instruction-following quality under zero-shot self-refinement prompting.",
            "performance_with_reflection": "Mixed results: Self-Refine improved Granite and Qwen models on some metrics, but did not improve Llama-8B (Table 1). Example: Granite Self-Refine AlpacaEval ~31.7 vs initial 30.2; Llama Self-Refine slightly decreased vs initial.",
            "performance_without_reflection": "Initial policy metrics as in paper (e.g., AlpacaEval initial Llama 26.9).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Inference-time prompting that elicits critique and revision (no training), i.e., generate-then-reflect with no latent principle labels.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical numbers in Table 1 show Self-Refine can produce wins for some models (Granite, Qwen) but is less reliable overall than STaPLe; the paper interprets this as zero-shot self-refinement being insufficient without pre-identified principles for some models.",
            "limitations_or_failure_cases": "Zero-shot Self-Refine was ineffective for Llama-8B, suggesting that stronger or fine-tuned models may be needed for reliable prompted self-refinement; lacks interpretability (no principle labels) and doesn't provide training signal to internalize refinement behavior.",
            "comparison_to_other_methods": "Compared as a baseline to STaPLe and STaR; STaPLe tends to outperform Self-Refine when the latter is less effective zero-shot.",
            "ablation_study_results": "No dedicated ablation reported for Self-Refine in the paper beyond the baseline comparisons.",
            "uuid": "e8742.2",
            "source_info": {
                "paper_title": "Latent Principle Discovery for Language Model Self-Improvement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SPIN",
            "name_full": "SPIN (self-play fine-tuning)",
            "brief_description": "Prior self-play fine-tuning method that compares model generations to ground-truth annotated responses, creating preference pairs and iteratively fine-tuning with contrastive objectives.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-play fine-tuning (SPIN)",
            "reflection_method_description": "On-policy sampling of outputs, compare against gold, produce preference pairs, fine-tune with contrastive objectives and iterate.",
            "task_name": "General self-improvement / self-play frameworks",
            "task_description": "Framework for turning weak LMs into stronger models via iterative self-play and fine-tuning.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Self-play sampling and contrastive fine-tuning (not used in experiments here).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as related work; the paper notes conceptual similarity (variance-reduced self-play equivalence) but does not provide direct empirical comparisons.",
            "limitations_or_failure_cases": "",
            "comparison_to_other_methods": "Cited as related prior work demonstrating self-play fine-tuning can strengthen weaker LMs; STaPLe is connected theoretically via self-play equivalence but uses latent principles and EM.",
            "ablation_study_results": null,
            "uuid": "e8742.3",
            "source_info": {
                "paper_title": "Latent Principle Discovery for Language Model Self-Improvement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SCoRe",
            "name_full": "SCoRe",
            "brief_description": "A recent multi-turn RL formulation for self-correction that improves initial attempts and uses reward shaping to encourage successive refinement; mentioned as related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reinforcement-learning-based self-correction (SCoRe)",
            "reflection_method_description": "Multi-turn RL that trains models to correct outputs across multiple refinement turns using shaped rewards.",
            "task_name": "Self-correction / refinement tasks",
            "task_description": "Multi-turn self-correction framed as an MDP to learn policies that refine outputs.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Offline RL over generated trajectories (cited, not used here).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned in related work as improving self-correction; no direct empirical comparison in this paper.",
            "limitations_or_failure_cases": "",
            "comparison_to_other_methods": "Positioned among RL-based approaches; STaPLe differs by using EM and supervised fine-tuning on principle-conditioned trajectories rather than RL.",
            "ablation_study_results": null,
            "uuid": "e8742.4",
            "source_info": {
                "paper_title": "Latent Principle Discovery for Language Model Self-Improvement",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RISE/ProMiSe/others",
            "name_full": "RISE / ProMiSe / other RL/self-correction methods",
            "brief_description": "Various cited works (e.g., RISE frames multi-turn self-correction with best-of-N and offline RL; ProMiSe extends to smaller models) representing RL and generation-based self-correction approaches mentioned in related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "RL-based and generation-based self-correction methods (RISE, ProMiSe, etc.)",
            "reflection_method_description": "Approaches that treat self-correction as a sequential decision process (MDP), sample rollouts (best-of-N), and apply offline RL or reward shaping to train a self-correcting policy.",
            "task_name": "Self-correction and reasoning benchmarks (general)",
            "task_description": "Multi-turn self-correction for improving response quality via reinforcement learning or structured rollouts.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Best-of-N sampling, offline RL, reward shaping (mentioned but not used experimentally in this paper).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited prior improvements in literature; STaPLe is presented as an alternative that leverages latent principles and supervised fine-tuning rather than RL.",
            "limitations_or_failure_cases": "",
            "comparison_to_other_methods": "STaPLe contrasted conceptually (EM + latent principles) against RL-based methods in related work discussion.",
            "ablation_study_results": null,
            "uuid": "e8742.5",
            "source_info": {
                "paper_title": "Latent Principle Discovery for Language Model Self-Improvement",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Taught Reasoner (STaR)",
            "rating": 2,
            "sanitized_title": "selftaught_reasoner_star"
        },
        {
            "paper_title": "Self-Refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-improvement in language models: The sharpening mechanism",
            "rating": 1,
            "sanitized_title": "selfimprovement_in_language_models_the_sharpening_mechanism"
        },
        {
            "paper_title": "Training language models to self-correct via reinforcement learning",
            "rating": 1,
            "sanitized_title": "training_language_models_to_selfcorrect_via_reinforcement_learning"
        },
        {
            "paper_title": "SPIN: Self-play fine-tuning converts weak language models to strong language models",
            "rating": 1,
            "sanitized_title": "spin_selfplay_finetuning_converts_weak_language_models_to_strong_language_models"
        }
    ],
    "cost": 0.021478,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Latent Principle Discovery for Language Model Self-Improvement
22 May 2025</p>
<p>Keshav Ramji keshav.ramji@ibm.com 
IBM Research AI</p>
<p>Tahira Naseem 
IBM Research AI</p>
<p>Ramón Fernandez 
IBM Research AI</p>
<p>Latent Principle Discovery for Language Model Self-Improvement
22 May 2025DE40463EF2F694AFAB1DDE5BDC6707A8arXiv:2505.16927v1[cs.CL]
When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect.However, curating such principles across many domains, even nonexhaustively, requires a labor-intensive annotation process.To automate this process, we propose eliciting these latent attributes guiding model reasoning towards human-preferred responses by explicitly modeling them in a self-correction setting.Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering.Specifically, we employ an approximation of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses.We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23% in principlefollowing win-rate on IFEval.We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance.The gains our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement.</p>
<p>Introduction</p>
<p>Modern language models (Grattafiori et al., 2024;OpenAI, 2024;DeepSeek-AI et al., 2025) have achieved striking fluency and coherence in open-ended generation, yet guiding them to satisfy multiple, possibly overlapping human-defined criteria remains a core challenge.Conventional approaches to align language models (LMs) rely on human annotations distinguishing between a chosen and rejected generation, even when their gap in quality may be nuanced and multi-faceted.Constitutional AI and other related paradigms (Bai et al., 2022b;Guan et al., 2025) consider a humancurated "constitution" of high-level attributes which the model's responses should follow.While this framework enables models to be steered toward safer behavior, the static nature of the constitution requires experts to anticipate every nuance in advance and update rules manually as edge cases surface.As use cases proliferate, new failure modes arise -reliably synthesizing task-specific "amendments" and collecting annotations is a costly and time-consuming process -leading to brittleness and limited adaptability.We aim to automate the process of discovering the attributes for model improvement, obviating the need for human intervention or explicit domain adaptation.</p>
<p>Automatically discovering the attributes for self-improvement can be seen as a meta-level reasoning process.Recent efforts to induce reasoning capabilities in LMs have often focused on domains such as math and code where a gold reference answer exists and candidate answers are more easily verifiable (DeepSeek-AI, 2025).The availability of verifiable responses has also been capitalized for teaching self-correction (Kumar et al., 2025).However, in this work, we focus on open-ended text Figure 1: We introduce Self-Taught Principle Learning (STaPLe).(Left) Our Monte Carlo EM algorithm alternates between on-policy discovery and learning of latent principles guiding selfcorrection behavior.The principles may also be clustered to a compressed set, yielding humaninterpretable constitutions C t and models trained to follow them M t .(Right) The STaPLe algorithm induces self-improvement in AlpacaEval win-rate over three iterations for all three language models.generation tasks that are challenging to verify; identifying situations for a human to intervene and induce a refined response can be especially tricky in such cases.</p>
<p>We introduce a novel approach to discover expressive principles, treating them as latent attributes in the self-correction setting to bridge an initial attempt and a target response.We find that the language model itself serves as an effective principle generator to improve its responses, contrasting prior works which rely on human annotations or strong model supervision.We design an Expectation-Maximization algorithm, Self-Taught Principle Learning (STaPLe), which first leverages rejection sampling in identifying principle candidates for self-correction and choosing the candidate that is closest to the gold, and then trains over these trajectories to learn this principle-guided refinement behavior.Repeating this method iteratively results in a model trained on a dynamic constitution of elements produced from itself, implicitly learning the refinement goal to enable its self-correction abilities at inference-time.We also show that the discovered principles can be compressed to a smaller set for human readability by applying hierarchical clustering after the E-step in a manner akin to posterior regularization, without compromising in downstream performance.</p>
<p>We validate the efficacy of this method over several iterations on instruction-following benchmarks including MT-Bench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023), and leverage Prometheus-v2.0 (Kim et al., 2024) to analyze win-rates with fine-grained, principle-following rubrics.Our results show that STaPLe outpaces baseline methods such as Self-Taught Reasoner (STaR; Zelikman et al. (2022)) (modified for non-verifiable responses) and prompted refinement approaches like Self-Refine (Madaan et al., 2023).It continues to self-improve in performance over multiple iterations, before saturating.We also find that clustering largely matches or outperforms training on all principles.</p>
<p>Our key contributions can be summarized as follows:</p>
<p>• We propose a Monte Carlo EM algorithm for iterative latent principle discovery and learning, to enable language model self-improvement.</p>
<p>• We find that on-policy generated principles are effective stimuli for self-correction in smaller LMs, and training to learn them improves the performance on MT-Bench, AlpacaEval-2.0.</p>
<p>• Clustering the set of discovered principles retains most of the full distribution's performance while yielding an interpretable constitution.</p>
<p>Related Work</p>
<p>Principle-Driven Language Modeling Early work in principle-driven alignment demonstrated that embedding high-level rule sets or "constitutions" into the training loop can steer model behavior without direct human labels for each generation.Constitutional AI (Bai et al., 2022b) introduced a two-stage process in which a pre-trained model first generates critiques of its own outputs against a static constitution curated and synthesized a priori and learns from these critiques, then trains a preference model from on-policy data and performs RL.Dromedary (Sun et al., 2023) extended this idea by introducing Self-Align, an algorithm which generates prompts, applies a small set of human written principles with in-context learning, and then fine-tunes the model to learn the principle-guided responses; this was later extended by SALMON (Sun et al., 2024) to design an instructable, principlefollowing reward model (RM).ConstitutionalExperts (Petridis et al., 2024) and SPRI (Zhan et al., 2025) address the problem of mapping prompts to principles.Deliberative Alignment (Guan et al., 2025) introduces CoT reasoning over safety specifications, and trains models to learn these reasoning traces via SFT and online RL with a safety RM.</p>
<p>More recent efforts have sought to leverage models to draft and refine constitutions with limited human supervision.LEAP (Zhang et al., 2024) showed that models can propose new principles via self-critique given the gold response, synthesizing a list of task-specific principles that can be used for in-context learning.SAMI (Fränken et al., 2024) introduced a self-alignment mechanism where a strong model is used to generate a small set of principle candidates, and the target model is trained to maximize the mutual information between the constitution and the model's responses through an InfoNCE loss.IterAlign (Chen et al., 2024b) and ICAI (Findeis et al., 2025) also leverage strong frontier models such as GPT-4/ GPT-4o (OpenAI, 2024) for constitution proposal.</p>
<p>The former uses a red-teaming framework to identify model weaknesses, and uses the strong model to propose a principle towards helpfulness, harmlessness, and honesty; the latter considers principles as specifications over preference annotations, injecting them to reconstruct the annotation.Most recently, DeepSeek introduced a pointwise generative reward model (Liu et al., 2025) over self-generated principles, demonstrating that such an RM can be successfully used to improve inference-time scaling.</p>
<p>Self-Correction and Self-Improvement The recent emergence of large reasoning models such as o3 and DeepSeek-R1 (OpenAI, 2025;DeepSeek-AI, 2025) has lead to a growing exploration into the ability of models to perform intrinsic refinement of their own outputs with internal feedback.However, much of the prior literature below either trains models over the improved responses alone or performs prompted self-refinement at inference-time, rather than learning this ability.STaR first leveraged model-generated critiques followed by revision, showing that alternating these two stages yields gains in instruction following (Zelikman et al., 2022).Self-Refine (Madaan et al., 2023) explored the setting of prompted multi-turn refinement: after producing an initial answer, the model is induced to critique itself, reflecting on the weaknesses of the current response on specific dimensions, proposing actionable changes, and reflecting them in the corrected response.ProMiSe (Ramji et al., 2024) extended this ability to smaller language models with weaker self-critique and refinement capabilities, showing that greedily refining responses relative to one attribute in a sequential manner improves performance, while demonstrating that training on synthetic dialogues modeling this self-refinement process improves dialogue question answering.APO (D'Oosterlinck et al., 2024) addressed the notion of minimally contrastive preference pairs, reinforcing the notion that revision along fewer attributes yields a better signal for preference optimization.Recently, reinforcement learning strategies have been explored to train models to directly perform self-correction.RISE (Qu et al., 2024) frames selfcorrection through a multi-turn Markov decision process (MDP), performs best-of-N sampling over sequential attempt rollouts, and uses offline RL to train the model to correct over these trajectories.SCoRe (Kumar et al., 2025) improves over this by a multi-turn RL formulation that boosts the quality of the initial attempt and leverages reward shaping to incentivize self-correction to improve the refined response.Beyond individual output corrections, recent work has shown that models can bootstrap their underlying capabilities in an iterative fashion over time.Several works suggest that sampling diverse responses or environmental interactions, filtering based on feedback or majority voting, and training on these on-policy generations can boost performance (Huang et al., 2023;Patel et al., 2024).SPIN (Chen et al., 2024c) introduced a self-play fine-tuning approach, wherein the model compares its generations against the ground-truth annotated responses in the SFT dataset yielding a preference pair, fine-tunes with a contrastive objective, and repeats this process iteratively.Huang et al. (2025) theoretically formalizes the self-improvement phenomenon through a "sharpening" process, wherein the model's policy moves towards maximizing its generations' self-reward.</p>
<p>Latent Chain-of-Thought Learning Chain-of-thought (CoT) prompting (Wei et al., 2023;Kojima et al., 2022) elicits an explicit, verbalized step-by-step walkthrough of the reasoning trace guiding the model from the input to the final response.Simultaneously, STaR (Zelikman et al., 2022) leveraged a gold response-conditioned rationalization of the CoT and fine-tuned the model to learn this reasoning behavior.The notion of rationalization as a latent variable modeling problem was previously explored in ELV (Zhou et al., 2020), under the framework of labeling examples with explanations through a variational EM algorithm.More recent approaches also treat chain-of-thought as a latent variable that may be trained over rather than purely to be induced at inference-time.TRICE (Phan et al., 2023) casts intermediate reasoning steps as latent codes, training the model to marginalize over them so that it internally develops coherent reasoning trajectories, through a Markov Chain Monte Carlo (MCMC) EM algorithm.LaTRO (Chen et al., 2024a) demonstrated that models can self-reward latent reasoning paths -generating candidate thought sequences, scoring them by task success, and reinforcing the most effective ones -through a variational framework.Concurrent work introduced BoLT (Ruan et al., 2025), showing that leveraging these implicit traces as supervision leads to gains in data efficiency and performance for continued pre-training on complex reasoning benchmarks by converting latent chain-of-thought into a self-supervised learning signal.</p>
<p>Self-Taught Principle Learning</p>
<p>We propose STaPLe, a self-improvement mechanism for 1) discovery of principles by the model itself aimed towards response revision and 2) training the model to invoke such principles and subsequently performing response self-refinement (if needed) at inference time.We view these principles as latent reasoning traces that bridge the gap between an initial model response and a reference target.</p>
<p>In the vein of the Self-Taught Reasoner (STaR) (Zelikman et al., 2022), we leverage the gold response as a "hint" to propose principles and guide response refinement decisions.However, our formulation is generic and allows for the use of non-verifiable gold responses as hints.In particular, we use the proximity of the generated response to the reference response as a signal of correctness.Any similarity metric can be used to measure this proximity, as our approach is agnostic to this choicethe exact match metric used for verifiable responses can be seen as one such instantiation.
Given a dataset D = {(x i , y 1 i , y G i )} n i=1
, where y G i is the gold response and y 1 i is model's initial response for the i th sample, we aim to learn a latent response-improvement reasoning trace z i such that the probability of producing a response close to the gold reference is maximized.The latent reasoning trace, or principle, z i is also verbalized as natural language, i.e. discrete text tokens from vocabulary V * .We implement STaPLe to optimize the following marginal likelihood:
p(y G | x, y 1 ) = y 2 ∈V * z∈V * p(y G | x, y 1 , z, y 2 ) • p(y 2 , z | x, y 1 ; θ)
where y 2 is a model refinement of the initial response y 1 generated with aid of latent principle z.The distribution p(y G | x, y 1 , z, y 2 ) is a fixed, prespecified validator model indicating the likelihood of the current revision y 2 matching the gold response y G .We parametrize p(y 2 , z | x, y 1 ) by the language model itself, with parameters θ.As shown in Appendix D, and following the standard latent variable model formulation, the gradient for this objective is:
∇ θ L(θ) = E p(y 2 ,z | x,y 1 ,y G ) ∇ θ log p(y 2 , z | x, y 1 ; θ)(1)
This objective can be maximized via Expectation-Maximization (EM).This comprises the repeated application of two alternating stages: 1) a principle discovery stage (E-step) and 2) a principle learning stage (M-step).This is depicted in Figure 1.</p>
<p>In principle discovery stage (E-step), we sample N principles z 1:N and corresponding responses y 2 (1:N ) from the posterior p(y 2 , z | x, y 1 , y G ). Approximating the true posterior would require an intractable marginalization over V * .We approximate this posterior by "hinting" our language model with the gold response 1 , and a prompt to elicit a principle.This is represented by:
p(y 2 , z | x, y 1 , y G ; θ) = p(y 2 | x, y 1 , z; θ) • p(z | x, y 1 , y G ; θ)
1 Here we factorize the approximate posterior into principle generation and response generation terms, where the gold response is only seen by the principle generation terms.This will help avoid the trivially degenerate solution where y 2 simply copies y G .We acknowledge that in the absence of any constraint on z, this can still lead to copying via z.However, in practice, the prompt that elicit the principle creates a contextual bias against exact copy.Moreover, we also experiment with explicit clustering contraints over z and show that both versions of our approach perform similarly.</p>
<p>Figure 2: The figure above depicts the principle discovery (E-step) phase.We sample an initial response y 1 on-policy, then "hint" with the gold response to elicit candidate principles z (1:N ) .Then, we sample critiques on the initial response (only used in rejection sampling, and not included in the fine-tuning trajectories), which we use to obtain principle-guided refined responses y 2</p>
<p>(1:N ) .The best refined response ŷ2 is selected based on similarity to the gold response.We save the resulting trajectory, which is used for supervised fine-tuning in the principle learning (M-step) stage.</p>
<p>To improve the quality of our samples, we employ a cycle-consistency approach, implicitly defining the true posterior as approximated by:
p(y 2 , z | x, y 1 , y G ; θ) ∝ p(y G | x, y 1 , z, y 2 ) • p(y 2 , z | x, y 1 , y G ; θ)(2)
This can be seen as equivalent to hinted CoT generation as in STaR, whereby samples that score higher in reconstruction error are assigned higher probability.In practice we use a sparse approximation of this distribution, that assigns zero probability unless there is an improvement in similarity function f :
p(y G | x, y 1 , z, y 2 ) ∝ f (y 2 , y G ) − f (y 1 , y G ), if f (y 2 , y G ) &gt; f (y 1 , y G ) 0, otherwise
We sample from p(y 2 , z | x, y 1 , y G ; θ) via rejection sampling.Given a sample y n ∼ p(y 2 , z | x, y 1 ; θ), we accept it with probability p n = p(yn,z | x,y 1 ,y G ;θ) M • p(yn,z | x,y 1 ,y G ;θ) we include a derivation of this rejection sampling rule in Appendix E.</p>
<p>We also compare the initial response y 1 to the gold reference y G using the similarity metric; if they are sufficiently close, we accept the response without further refinement and without sampling a ẑ.The principle discovery stage yields a principle-augmented dataset (x ∪ y 1 , ẑ, y 2 ) ∈ D ′ .Note that if no refinements improve upon the initial generation relative to the gold response, we discard the sample; thus, the dataset D ′ only consists of those samples on which a principle improved the quality of the response towards the gold.</p>
<p>In the principle learning stage (M-step), we use the data D ′ collected in the principle discovery stage for supervised fine-tuning of the language model.In particular, we train the model to maximize the log-likelihood of the refinement trajectories in D ′ .The corresponding EM update can be written as:
θ (t+1) = arg max θ E (x,y 1 ,ẑ,ŷ 2 )∈D ′ [log p(y 2 , z|x, y 1 ; θ]
This should qualitatively result in the fine-tuned LM being able to invoke principles conditioned on a prompt and learning to produce high-quality responses conditioned on both the prompt and the invoked principle.</p>
<p>The two stages can be repeated multiple times, achieving incremental improvements till no further gains are seen with respect to the gold references.We also draw a connection between STaPLe (this EM procedure) and variance-reduced self-play; this is discussed further in Appendix F.</p>
<p>Posterior Regularization via Clustering</p>
<p>To maximize the human interpretability of principles and their application relative to specific domains, it is beneficial to have a compressed set, or constitution to distill to the model.However, the Estep described above, results in thousands of unique principles.We seek to project this set into a constrained subspace where the resulting principles serve as representatives for desirable attributes to be reflected.This can be achieved via posterior regularization (PR) in latent variable modeling.For a posterior constraint set Q, the canonical posterior regularization framework solves the problem q * (z) = arg min q∈Q KL(q(z|x, y * )||p(z|x, y * ))</p>
<p>From Ganchev et al. (2010), we obtain that the primal solution is given by:
p(y 2 , z | x, y 1 , y G ) ∝ p(y 2 , z | x, y 1 , y G ) • exp (−λg(z))
where g(z) denotes the constrained features of the principles and λ is a Lagrange multiplier that must be set such that the expected value of the features under p respects the constraints.</p>
<p>Consider the following definition of the constraints: assume access to a clustering algorithm which yields a set of clusters {C 1 , C 2 , . . .C K }.For each cluster, a representative element z is chosen, forming the set Z = {z 1 , . . ., zK }.Now define
g k (z) = 1(z ∈ C k \ {z k }) for k ∈ [1, K]
as binary feature functions.Thus, to ensure that the regularized posterior only places mass on the representative elements, we can enforce the constraint set
Q = {q : Eq[gk(z)] = 0 ∀ k ∈ [1, K]}
However, while an algorithm like projected gradient descent could be used to solve for the Lagrange multipliers, this is expensive for deep neural networks, and as such, is impractical in our case.Instead, we suggest that performing clustering methods on a set of posterior samples and retaining only the representative elements fulfills an equivalent role empirically.Clustering serves to consolidate principles that are lexically close, and leveraging an embedding model for distances allows for semantic awareness in merging similar elements.</p>
<p>In particular, we consider hierarchical (agglomerative) clustering for several of its benefits: (1.) it requires no assumptions about number of clusters or cluster shape a priori, (2.) the algorithm is deterministic, ensuring that the same clusters would be obtained for a given configuration, and (3.) the algorithm is relatively fast, only taking a few seconds in practice over thousands of principles.</p>
<p>To ensure that the clustering is performed in a semantics-aware manner, we first obtain a sentence embedding with an encoder-only model and perform clustering over these embeddings.</p>
<p>Given the principle-augmented dataset (x i , y 1 i , ẑi , y 2 i ) ∈ D ′ and a set Z of cluster representative elements over C = {C i } k i=1 , we aim to replace ẑi with the element z ∈ Z that is closest in meaning to the original principle.Qualitatively, we want the set Z to comprise the human-readable constitution, minimizing semantic overlap in its labels.We take the medoid as the cluster representative:
Z medoid = {m k : m k = arg min m∈C k j∈C k ||e i − e j || 2 , k ∈ [1, K]} (Medoid Representatives)
It suffices to retrieve the corresponding cluster C i for a sample i and replace ẑi with zi ∈ Z medoid .</p>
<p>The resulting dataset from this augmentation, (x i , y 1 i , zi , y 2 i ) ∈ D is then used to train the model.</p>
<p>4 LMs Can Self-Improve with Iterative Principle Discovery</p>
<p>Experimental Setup</p>
<p>Mixed-Domain Input Corpus.We form a corpus of 100k samples for the principle discovery phase, consisting of four datasets: Anthropic HH-RLHF (Bai et al., 2022a), UltraFeedback (Cui et al., 2024), TL;DR (Stiennon et al., 2020), and HotpotQA (Yang et al., 2018), taken in equal proportion (i.e.25k samples of each dataset, drawn randomly) and deduplicated by prompt.For preference datasets, we take the chosen response to be the gold answer y G .To run STaPLe, we use the first 50k samples for iteration 1, to heavily bootstrap off the first iteration, and then use 10k samples for each iteration thereafter, such that the input prompts are unseen for each iteration.</p>
<p>Models and Hyperparameters.We evaluate three performant small language models: Llama-3.1-8B-Instruct(Grattafiori et al., 2024;Meta, 2024), Granite-3.1-8B-Instruct(Granite Team, 2024; Granite Team and IBM, 2024), and Qwen2.5-7B-Instruct(Qwen, 2025).We use the all-MiniLM-L6-v2 model (Sentence Transformers, 2021) from SentenceTransformers as the embedding model to compute medoids in our clustering approach.We use the Rouge-L F1 score (Lin, 2004) to compare the similarity of candidate responses relative to the reference answer.We also include an ablation in Appendix M.2 using a prompted Phi-4 (Abdin et al., 2024) judge to score responses, leveraging additional compute to improve the quality of rejection sampling.We discuss all other major STaPLe algorithm and model training hyperparameters in Appendix C.</p>
<p>Baselines.We compared our method against several baselines in both the single-iteration and multi-iteration settings, in addition to the scores of each model's initial policy.</p>
<ol>
<li>
<p>Prompted self-refinement to directly produce a self-critique and revision, akin to Self-Refine, without any principle or specific feedback criterion provided a priori.</p>
</li>
<li>
<p>Supervised fine-tuning on the gold responses of the first 50k samples in the mining corpus.</p>
</li>
<li>
<p>Following SCoRe, we adopt STaR-like baseline for intrinsic self-correction; we apply the STaPLe algorithm and perform supervised fine-tuning on the best refined response (without principle-based refinement trajectory).This will henceforth be referred to as "STaR".</p>
</li>
</ol>
<p>We compare the STaPLe and STaR algorithms over four iterations -this is performed over the same number of samples per iteration, i.e. 50k samples in the first iteration and 10k samples for each subsequent one.Naturally, the other baselines are performed for a single iteration.</p>
<p>Evaluation.We evaluate on the MT-Bench (Zheng et al., 2023) and AlpacaEval-2.0-LC(Li et al., 2023;Dubois et al., 2024) datasets, instruction-following evaluations designed to reflect alignment abilities of LLMs in chat settings.We also use the Prometheus-8x7B-v2.0model (Kim et al., 2024) model on responses to the above datasets and the IFEval (Zhou et al., 2023) dataset, for fine-grained evaluation on principle-following rubrics, with additional experiments included in Appendix H.At inference time, if a principle was invoked intrinsically given a prompt, the response is parsed so as to only score the refined generation, following the principle proposal -this is similarly done for the STaR baseline.Otherwise, we score the full generated response, and no special parsing is required.</p>
<p>For the Prometheus results, the win-rate is with respect to the principle invoked -for example, if the principle is "Directness", the judge assesses which response is more direct between the candidate generation and the generation from the initial policy.Given the STaR baseline does not explicitly invoke a principle, we use the same principle invoked for that sample by the STaPLe model.</p>
<p>Results</p>
<p>Latent Principle Learning Improves Response Quality.The STaPLe algorithm outperforms the baselines on all benchmarks, across all models, as seen in Table 1.The MT-Bench average exceeds the best baseline by an average of +0.11 over the three models, with the Turn 2 increasing by an average of +0.22.The AlpacaEval win-rates improve over the initial policy by +5.3-7%, and improves over the STaR baseline by +1.6-2.8%.Furthermore, the IFEval win-rates on principle-following of the refined against the base policy using Prometheus improve by +5-6.6%.This suggests that training models to explicitly invoke the principle as an expressive form of a latent attribute is effective, as opposed to implicitly learning over this by simply training on the refined response (the STaR baseline).The Self-Refine baseline improves performance for the Granite and Qwen models, but not for Llama-8B, suggesting that it is not as effective in zero-shot self-refinement without pre-identified principles.This corresponds with a higher IFEval win-rate for those models with strong self-refinement abilities.</p>
<p>Iterative Principle Discovery Enables Self-Improvement.The results in Table 1 demonstrate the performance of our algorithm in the fourth iteration of our Monte Carlo EM algorithm; Our algorithm outpaces the STaR baseline by a sizable margin throughout the execution of both algorithms.We include the full set of results in Appendix G.By iteration 3, the STaPLe scores outperform STaR and the initial policy on average across the three models by +0.16 and +0.29 on MT-Bench (avg.);+3.6% and +9.2% on AlpacaEval win-rate; and +7.9% and +21.0% on IFEval principle-following win-rate, respectively.We do observe a slight diminishing returns effect with the STaPLe algorithm, as in iteration 4, the scores either remain at a similar level or drop slightly for Llama-8B and Granite-8B; however, Qwen-7B continues to improve on all three benchmarks.We further analyze principlefollowing quality in Appendix H and stepwise win-rates of iteration t against iteration t − 1 in Appendix I to reinforce the self-improvement induced by STaPLe.In Appendix J, we demonstrate that the model's intrinsic self-correction ability improves over the iterations.</p>
<p>Clustering Balances Interpretability and Performance.In Table 1, we also include the performance of "constrained" STaPLe -the version of the algorithm with agglomerative clustering following the E-step during each iteration, and use the medoids of each cluster as a representative principle to yield dataset D. We find that this largely matches the performance of the "unconstrained" version, in fact outperforming it in AlpacaEval and IFEval win-rates for Llama-8B and Granite-8B.</p>
<p>The full results can be found in Appendix M.1, where we ablate across different label replacement schemes (medoids, modes, and a perplexity-based method).For both versions, we observe a strong correlation in the trend (avg.ρ = 0.95-0.96)between the MT-Bench (avg.) and AlpacaEval results.</p>
<p>Analysis of Principle Discovery</p>
<p>It is also valuable to study the nature of the principle discovery process and the model-generated constitutions that we have aligned the language model toward.We include the full constitutions and perform more qualitative analysis on their distribution of elements when performing label replacement ("density" of the constitution) in Appendix K.In Figure 10, we show that the number of principles in the constitution under Constrained STaPLe decreases over the iterations, suggesting that the model converges to learning a relatively stable distribution of principles.In particular, the size of the constitution by iteration 4 is roughly 50% of the iteration 1 size, or even smaller.</p>
<p>This finding is reinforced by an analysis of the principle discovery rate -the fraction of refinement samples with new, unseen principles -in Figure 3.We show that this rate decreases over the iterations under both the unconstrained and constrained versions of the STaPLe algorithm, suggesting that all models learn to re-use principles accordingly.The observation that constrained STaPLe helps to accelerate this convergence to a condensed set of principles reinforces the motivation behind the introduction of clustering as being akin to a posterior regularization mechanism.This also highlights one of the advantages of using the LM to approximate the posterior distribution, as the changing nature of the learned posterior can be observed over the iterations and elicited via on-policy sampling.</p>
<p>Discussion and Limitations</p>
<p>The STaPLe algorithm guides a largely autonomous self-improvement process, with the exception of a few hyperparameters that are to be set, discussed in Appendix C. As a result, the algorithm does not require human supervision beyond the labels in the curated (publicly-available) mining corpus.</p>
<p>While the distribution of principles is mined relative to the mining corpus' task distribution, the STaPLe algorithm itself is task-agnostic, and can be used for any distribution of datasets where a reliable gold reference exists, or for paired preference data.However, designing a task-aware version of the STaPLe algorithm may reveal further insights into the model's task-dependent selfcorrection mechanisms while inducing a curriculum.As noted prior, we focus on the two-turn selfcorrection setting, but interesting insights could be extracted regarding the compositional nature of principles when extending to further refinement attempts, which also yields more diverse trajectories (combinatorially many possible) even over a condensed set of principles.</p>
<p>A core aim of alignment research is to balance human-readability with machine-readability.The STaPLe algorithm succeeds in achieving this by discovering principles that are useful to the model for self-correction, while compressing them to smaller set via clustering for a human reader to analyze.We believe that this work and the notion of LM self-improvement keeps with the theme of the Bitter Lesson (Sutton, 2019), when facilitated in a relatively autonomous fashion.Specifically, we aim to limit the influence of human-driven priors or constraints on the algorithm; this is reflected further by our clustering technique, and our ablation in Appendix M.3 to fully automate this as well.At the same time, we acknowledge the value of human oversight on alignment process; as such, we believe that human-in-the-loop analysis of the principles as a post-processing mechanism following the E-step of each iteration is valuable to avoid misalignment or potentially harmful principles.</p>
<p>Conclusion</p>
<p>We introduced a new language model self-improvement method which uses model-generated latent principles to learn intrinsic self-correction.These serve a purpose akin to a reasoning chain-ofthought, boosting the quality of LM generations on alignment-focused benchmarks.Furthermore, our approximate posterior-regularized Monte Carlo EM algorithm shows that the model can continue to improve over multiple iterations, while simultaneously compressing the principles to a humaninterpretable constitution.We also show that our clustering approach balances performance with the diversity of the generated constitution, thus adding valuable utility to the STaPLe algorithm.The efficacy of STaPLe highlights the potential for constitutional alignment with self-generated principles to improve model responses in an interpretable manner with minimal human supervision.</p>
<p>S. Wang, S. Zhou, S. Yu, S. Zhou, S. Pan, T. Wang, T. Yun, T. Pei, T. Sun, W. L. Xiao, W. Zeng, W. Zhao, W. An, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen, X. Chen, X. Zhang, X. Chen, X. Nie, X. Sun, X. Wang, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yu, X. Song, X. Shan, X. Zhou, X. Yang, X.</p>
<p>A Formal Description of STaPLe Algorithm</p>
<p>We provide a full, formal description of the STaPLe algorithm below.We use y 1 and y 2 notationally to avoid confusion with the sample indices.We use general variables for components which may be ablated on: the similarity function f , clustering algorithm C and label replacement scheme R. We leave the M -step in terms of the dataset D ′ for generality, although if clustering were to be performed, one would use D instead.</p>
<p>Algorithm 1 Self-Taught Principle Learning (STaPLe)
Require: Dataset D = {(x i , y G i )} n i=1 ,for each (x i , y G i ) ∈ D do 4: Sample initial response y 1 i ∼ π θ (t) (• | x i ). 5: if f (y 1 i , y G i ) &lt; τ then ▷ needs refinement 6:
Draw principles {z
(j) i } N j=1 ∼ p θ (t) z | x i , y 1 i , y G i .
7:</p>
<p>for j = 1, . . ., N do 8:</p>
<p>Generate critique c
(j) i ← Critique y 1 i , z(j) i
.</p>
<p>9:</p>
<p>Sample refinement y
2,(j) i ∼ π θ (t) • | x i , y 1 i , z(j)
i , c</p>
<p>(j) i</p>
<p>.</p>
<p>10:</p>
<p>end for 11: M-step:
j * ← argmax j f y 2,(j) i , y G i 12: (z i , y 2 i ) ← z (j * ) i , y 2,(j * ) i 13: if f (y 2 i , y G i ) &gt; f (y 1 i , y G i ) then 14: Add trajectory (x i , y 1 i , z i , y 2 i ) to D ′ .θ (t+1) ← arg max θ (x,y 1 ,z,y 2 )∈D ′ log p θ y 2 , z | x, y 1 .
20: end for Ensure: Final LM parameters θ (T )</p>
<p>B Reproducibility Statement</p>
<p>In addition to the algorithm description above (Algorithm 1) and experimental details in Section 4.1, we include the hyperparameters used and model training details in Appendix C and the prompts used in the STaPLe algorithm in Appendix L. We make all evaluation results available in tabular format throughout the main paper and the appendices for comparability.We also will publicly release the code for the STaPLe algorithm, to further facilitate reproducibility of our self-improvement method.</p>
<p>C STaPLe Hyperparameters and Training Details</p>
<p>STaPLe Algorithm Hyperparameters.We use a Rouge-L F1 threshold of 0.4 for the similarity threshold (f (y, y G ) -if the initial response exceeds this threshold, we do not pursue refinement).</p>
<p>For the ablation using a Phi-4 judge in Appendix M.2, the threshold was set to be 9 (on a scale of 1-10).The other major hyperparameters involved in the execution of the STaPLe algorithm are N , the number of principles to sample, and the distance threshold for the clustering algorithm.STaPLe requires an inference time budget of 3N + 1 for the Rouge-L version, and 4N + 2 for the LLM-asa-judge version -we set N = 16 to balance runtime per iteration of the algorithm with sufficient exploration of diverse principles.During principle discovery, we sample principles, critiques, and responses at a temperature of 0.7; the maximum number of tokens for principle proposal and critique is set at 500, and is set at 1024 for the refined response.We use 4×H100 Nvidia GPUs for the principle discovery phase, with a separate vLLM (Kwon et al., 2023) instance per GPU.</p>
<p>We set a distance threshold δ to avoid setting a specific target number of clusters when performing agglomerative clustering.The current results involve manually setting a distance threshold, where the authors analyzed the resulting set of clusters and for each of the first three iterations, ensured that there are at least 30 clusters.Fortunately, given the speed of agglomerative clustering, this is fairly easy to do.For the first iteration, the Euclidean distance thresholds were set at 8 (Llama and Qwen) and 6 (Granite); for iterations 2-4, the thresholds were decreased to 7 and 5, respectively.Alternatively, one could automate this process by designing an objective over the diversity (semantic or surface-level) of the cluster medoid labels and performing a hyperparameter search.This is explored further in Appendix M.3 using Bayesian hyperparameter optimization tools to identify an appropriate, model-specific distance threshold.The threshold τ P P L for the perplexity difference label-replacement scheme, described in Appendix M.1, was set at 0.2.</p>
<p>Model Training.We perform full supervised fine-tuning for 3 epochs at a learning rate of 1 × 10 −6 with the AdamW optimizer (Loshchilov and Hutter, 2019), with a sequence length of 4096.All experiments were performed on 8×H100 Nvidia GPUs.</p>
<p>D Derivation of the Monte Carlo EM Gradient</p>
<p>Recall that the conditional log-likelihood is defined as:
L(θ) = log y 2 ∈V * z∈V * p(y G | x, y 1 , z, y 2 ) • p(y 2 , z | x, y 1 ; θ)
The gradient with respect to this objective is given by
∇ θ L(θ) = ∇ θ log y 2 ∈V * z∈V * p(y G | x, y 1 , z, y 2 ) • p(y 2 , z, c | x, y 1 ; θ) = y 2 ∈V * z∈V * p(y G | x, y 1 , z, y 2 ) p(y G | x, y 1 ) ∇ θ p(y 2 , z | x, y 1 ; θ) = y 2 ∈V * z∈V * p(y G | x, y 1 , z, y 2 ) • p(y 2 , z | x, y 1 ; θ) p(y G | x, y 1 ) ∇ θ log p(y 2 , z | x, y 1 ; θ) = E p(y 2 ,z | x,y 1 ,y G ) ∇ θ log p(y 2 , z | x, y 1 ; θ)</p>
<p>E Derivation of Rejection Sampling Rule</p>
<p>Recall the intractable posterior which we obtain from the MC-EM gradient:
p(y 2 , z | x, y 1 , y G ) = p(y G | x, y 1 , z, y 2 ) • p(y 2 , z | x, y 1 ; θ) p(y G | x, y 1 )
This can be approximated via Monte Carlo Expectation-Maximization (Wei and Tanner, 1990), where sampling techniques are used to obtain samples from the intractable posterior, which are then used for updating model parameters.In particular, we choose the rejection sampling technique (von Neumann, 1951) with p(y 2 , z | x, y 1 , y G ; θ) as proposal distribution.Given a sample y n ∼ p(y 2 , z | x, y 1 ; θ), we accept it with probability :
p n = p(y n , z | x, y 1 , y G ; θ) M • p(y n , z | x, y 1 , y G ; θ)
The scaling factor M guarantees p n to be bounded by 1 appropriately.Formally, we take M to be:
M = max y∈V * z∈V * p(y 2 , z | x, y 1 , y G ; θ) p(y 2 , z | x, y 1 , y G ; θ) = 1 p(y G | x, y 1 ) • max y∈V * z∈V * p(y G | x, y 1 , z, y 2 )
This yields the following acceptance probability for rejection sampling:
p n = p(y G | x, y 1 , z, y 2 ) max y∈V * z∈V * p(y G | x, y 1 , z, y 2 )
Since the denominator cancels out through marginalization in rejection sampling, all that remains is to specify the validator model p(y G | x, y 1 , z, y 2 ) as an unnormalized distribution; any response matching metric that measures the similarity between y G and y 2 can serve this purpose.In particular, we experiment with Rouge-L similarity and LLM-based similarity judgments.For instance, in the case of Rouge-L, we take a positive increase score as the acceptance rule, that is:
p(y G | x, y 1 , z, y 2 ) ∝ f (y 2 , y G ) − f (y 1 , y G ), if f (y 2 , y G ) &gt; f (y 1 , y G ) 0, otherwise F Self-Play Equivalence
The STaPLe Monte Carlo EM approach can equivalently be described through the lens of self-play, somewhat akin to SPIN (Chen et al., 2024c).That is, we can formulate a two-player game wherein the adversary produces a response, and the agent's role is to 1. produce a revised response to the prompt that improves over the adversary's generation relative to the gold, and 2. specify the dimension or aspect on which it improved over the adversary.In the first iteration, we take the same LM to play the both roles.In subsequent iterations, given the policy π θ has now learned self-correction behavior, we take the initial response (opponent's generation) as the starting point, which we posit to be similar to generations sampled from the base policy π 0 -that is,
y a ∼ π 0 (• | x) ≈ y b ∈ (y b , z, y c ) ∼ π θ (• | x).
At the same time, the agent's policy updates to π θ , which learns principle-conditioned self-refinement, thus improving the agent's ability to perform its primary objectives.</p>
<p>Formally, we can define the self-play advantage of the refinement over the adversary's generation as
A(y 2 , y 1 ; x, y G ) = f (y 2 , y G ) − f (y 1 , y G )
Recall that in the STaPLe algorithm, if the agent "loses" -that is, it fails to produce a refinement that improves over the initial response -the sample is discarded.The nature of the advantage depends on the instantiation of the similarity function f ; for instance, under exact match, this collapses to a binary indicator.The objective under the self-correction setting is to maximize the expected advantage under π θ :
J(θ) = E y1,z,y2∼π θ [A(y 2 , y 1 ; x, y G )]
The score-function gradient is thus:
∇ θ J(θ) = E y1,z,y2∼π θ <a href="3">A(y 2 , y 1 ; x, y G )∇ θ p(y 1 , z, y 2 | x)</a>
Theorem 1 (Equivalence of EM and Self-Play Gradients).Assume the setting of an input x, an initial model response
y 1 ∼ π θ (• | x), a latent principle z ∼ π θ (• | x, y 1 , y G
) and critique of y 1 with respect to z denoted by c, and a refinement y 2 ∼ π θ (• | x, y 1 , z, c).Then, the EM gradient given by Equation 1is equivalent to the REINFORCE score-function gradient under variance-reduced self-play, given by Equation 3, under the the self-play advantage and the validator assignment
p(y 2 | x, y 1 ) = 1(f (y 2 , y G ) &gt; f (y 1 , y G ))
Proof.We begin by marginalizing over the latent principle z.By definition,
p(y 2 | x, y 1 , y G ) = z,c p(y 2 , z, c | x, y 1 , y G )
By Bayes' rule:
p(y 2 , z | x, y 1 , y G ) = p(y G | x, y 1 , z, y 2 ) • p(y 2 , z | x, y 1 ; θ) p(y G | x, y 1 )
Then, given that only the final term depends on θ, we can rewrite its gradient as
∇ θ log p(y 2 , z | x, y 1 ; θ) = ∇ θ log[π θ (y 1 | x)π θ (y 2 | x, y 1 )]
Thus, revisiting the EM gradient, we have:
∇ θ (y G , y 1 , x, θ) = y2 z p(y 2 , z | x, y 1 , y G ; θ)∇ θ log[p(y 2 , z | x, y 1 ; θ] = y2 p(y 2 | x, y 1 , y G ; θ)∇ θ log[π θ (y 1 | x)π θ (y 2 | x, y 1 )]
Next, by Bayes' rule, we can re-express p(y 2 | x, y 1 , y G ; θ) in terms of the un-normalized EM validator term:
p(y 2 | x, y 1 , y G ; θ) = p(y G | x, y 1 , y 2 )π θ (y 2 | x, y 1 ) Z(x, y 1 ) = p(y 2 | x, y 1 )π θ (y 2 | x, y 1 ) Z(x, y 1 )
Where Z(x, y 1 ) = y2 p(y 2 | x, y 1 )π θ (y 2 | x, y 1 ), and taking p(y 2 | x, y 1 ) ∝ p(y G | x, y 1 , y 2 ).</p>
<p>Thus, we have that the EM gradient takes the form of
∇ θ L(y G , y 1 , x, θ) = E y1∼π θ [ y2 p(y 2 | x, y 1 )π θ (y 2 | x, y 1 )∇ θ log[π θ (y 1 | x)π θ (y 2 | x, y 1 )] y2 p(y 2 | x, y 1 )π θ (y 2 | x, y 1 ) ]
Next, we consider the assignment of the EM validator to be in terms of the comparison between initial response y 1 and refined response y 2 with respect to y G over the similarity function f .That is, take p(y 2 | x, y 1 ) = 1(f (y 2 , y G ) &gt; f (y 1 , y G ); such that we only accept refinements that improve over the gold.This reflects the STaPLe algorithm's accept/reject criterion.Then, factoring out the summation over y 2 into the expectation, and substituting the validator term, we have:
∇ θ L(y G , y 1 , x, θ) ∝ E y1,y2∼π θ [1(f (y 2 , y G ) &gt; f (y 1 , y G )∇ θ log[π θ (y 1 | x)π θ (y 2 | x, y 1 )]]
with proportionality to the degree of 1 Z where Z = y2 p(y 2 | x, y 1 )π θ (y 2 | x, y 1 ) is the normalization constant.This is a binary reward, and in practice, the selection of the advantage depends on the nature of the similarity function f which is considered.To generalize to real-valued rewards such as Rouge-L, reward models, or LLM-as-a-judge scores, we instead replace this hard indicator with an advantage function A(y 2 , y
1 ; x, y G ) = f (y 2 , y G ) − f (y 1 , y G ).
In the canonical REINFORCE self-play setting (Dayan, 1990;Sutton, 1984), the reward R(τ ) over the trajectory τ is often replaced by an advantage to reduce the variance of the Monte Carlo estimate, introducing
A(τ ) = R(τ ) − b for a comparison b. This yields a gradient ∇ θ J(θ) = E[A(τ )∇ θ log[π θ (τ )]
] in practice.In our setting, we are simply taking the score of the initial response f (y 1 , y G ) to be the comparison.</p>
<p>Performing this substitution in the current form of the EM gradient:
∇ θ L(y G , y 1 , x, θ) ∝ E y1,y2∼π θ [A(y 2 , y 1 ; x, y G )∇ θ log[π θ (y 1 | x)π θ (y 2 | x, y 1 )]]
Since z was marginalized over, this recovers Equation 3, the self-play REINFORCE gradient, concluding the proof.</p>
<p>G Complete Table: Self-Improvement over Multiple Iterations</p>
<p>In this section, we include the complete tables over four iterations of the STaPLe algorithm, to demonstrate the model's progression of self-improvement.As shown in Table 2, STaPLe outpaces the STaR baseline by a substantial margin throughout the execution of both algorithms, even in spite of the improvements of Llama-8B and Granite-8B saturating by the end of iteration 3.While both algorithms have fairly similar MT-Bench Turn-1 scores by iteration 4, the Turn-2 score is substantially higher (average of +0.22) for STaPLe.We observe similar general trends for the models in AlpacaEval win-rate and Prometheus-based IFEval principle-following win-rate, as well.</p>
<p>H Prometheus Win-rates on MT-Bench and AlpacaEval</p>
<p>Given that in Section 4.2, we have sampled responses with intrinsic principle-conditioned selfcorrection behavior from the language model for MT-Bench and AlpacaEval, we can further study the quality of the Prometheus-8x7B-v2.0model in producing judgements over a fine-grained rubric.We specifically would like to understand whether the model's responses -which, as per Tables 1 and  2, achieve improvements in score -actually reflect the principles they invoke.</p>
<p>This method corresponds to the IFEval principle-following win-rates reported in Section 4.2.As such, note that the AlpacaEval win-rate in this section differs from the standard AlpacaEval scoring -this is the percentage of AlpacaEval (correspondingly MT-Bench) samples on which Prometheus-v2.0 chose the refined response over the base policy's generation, with regards to the principle-following rubric.Recall that our STaR baseline also produces an intrinsic self-correction, but without the principle, so we use the principle invoked by the STaPLe model in the Prometheus judge rubric.Both algorithms yield gains over the base policy in win-rate, with STaPLe outperforming STaR across all iterations.Interestingly, we find that on MT-Bench, the STaR baseline continues to increase by a sizable amount (2.5-3.8 pts) in iteration 4, unlike the true MT-Bench score and the other benchmarks as reported in Table 2.By contrast, training over principles in the unconstrained STaPLe yields a smaller gain (for Granite-8B and Qwen-7B, and a slight drop for Llama-8B), although STaPLe still outperforms STaR by +7.5-8.8% in iteration 3 and +3.7%-7.5% in iteration 4.However, Granite-8B does appear to improve in MT-Bench win-rate in iteration 4, despite the average MT-Bench score dropping (as can be witnessed in Table 2.However, given the small sample size of the dataset (80 samples), this could be a product of noise, unlike the larger datasets like IFEval (541 samples) and AlpacaEval (805 samples).On AlpacaEval, we witness a similar trend, albeit more consistent with the AlpacaEval scores reported in Table 2.</p>
<p>I Stepwise Win-rate Analysis</p>
<p>Recall that the Prometheus win-rates that have been reported thus far are a comparison against generations from each model's initial policy (instruct model) π 0 .However, to confirm that the model's generations continue to improve in principle-following quality over the iterations, we compare the iteration t model's generations against iteration t−1 in the Prometheus judgement setup.</p>
<p>Given our primary focus in Tables 1 and 2 was on IFEval, we recompute these win-rates against the initial response in trained STaPLe model's own generated self-correction trajectories.In iteration 1, the comparison is done against the base policy, and thus the win-rates reported are the same as in the aforementioned tables.Note that a win-rate of 50% indicates that responses generated under π t were equally preferred to responses generated under π t−1 .As such, the win-rates remaining above 50% by a sizble margin is further evidence of the model's self-improvement.These stepwise win-rates are also a useful signal in behaving like an "elbow" method, to determine when to terminate the STaPLe algorithm.For instance, observing that the win-rates drop below 50% for the Llama-8B and Granite-8B models in iteration 4 suggests that their responses degraded compared to their prior iteration's responses (albeit, Llama-8B is fairly marginally below 50%).On the other hand, Qwen's win-rates remain above 60% throughout, suggesting that there perhaps is potential to continue its self-improvement for additional iterations.We plot this progression in Figure 4 for a visual representation of this selection process.</p>
<p>Figure 4: Visualization of Table 4, comparing against the 50% baseline.While the win-rate exceeds 50%, the model continues to self-improve.</p>
<p>J Intrinsic Self-Correction</p>
<p>Given that the trained STaPLe model performs intrinsic self-correction -given a prompt, it produces an initial response, invokes a principle to improve it, and improves the response, without an external stimulus or re-prompting -we can analyze the advantage between the model's initial and final responses.We do this using the Prometheus-v2.0 judge, on IFEval prompts, to give a binary preference between the initial response and final response on principle-following, using the same judge prompt as in other experiments in Tables 1-4.The results are found in Table 5.We find that the win-rates do improve over the iterations, reinforcing the claim that STaPLe-trained models learn intrinsic self-correction behavior.These win-rates are also consistent with our prior findings that the Llama-8B and Granite-8B models degrade in iteration 4, while Qwen-7B continues to improve.We compare the refinement rates between the unconstrained and constrained versions of STaPLe in Figures 5 and J.We observe a similar trend, where Qwen-7B starts with th highest rate (above 0.61) and remains the highest throughout.The refinement rates for Llama-8B and Granite-8B gain similarly for both versions, although the refinement rates are lower by iteration 4 in the constrained version.In the left plot, the Granite refinement rate spikes during iteration 3 principle discovery (the E-step), which we do not see in the constrained version.</p>
<p>Figure 7: Breakdown of the Granite-3.1-8B-Instructiteration 3 model-generated constitution in terms of the number of elements in each cluster.The label on the x-axis denotes the cluster representative element (medoid).The counts also denote the number of fine-tuning samples contained this principle in the augmented dataset D, following label replacement in the trajectories.We use ellipses for the sake of readability.</p>
<ol>
<li>Engage in the conversation with a clear and well-defined tone 27.Avoid providing unnecessary information 28.Avoid unnecessary phrases and provide clear and direct information 29.Address the reader's concerns explicitly</li>
</ol>
<p>Figure 8: Analysis of the Llama-3.1-8B-Instructiteration 4 constitution.We use ellipses for brevity, as in Figure 7, given the corresponding full principles may be found above.</p>
<p>We observe that the at face value, the elements in the Llama-8B constituion are more "high-level", akin to some of the elements in works such as Constitutional AI and Dromedary (Bai et al., 2022b;Sun et al., 2023).As with Granite, the majority of the mass is placed on elements with the premise of "Conciseness and Clarity" (simply swapping the order), as well as "Empathy and Emotional Validation), which is fairly similar to "Empathy and Compassion" from the Granite-8B constitution.</p>
<p>A new element that appears fairly often (≈ 800 instances) is "Directness and Assertiveness".</p>
<p>Figure 9: Analysis of the Qwen2.5-7B-Instructiteration 4 constitution.We use ellipses for brevity, as in Figures 7 and 8, given the corresponding full principles may be found above.</p>
<p>this to be an appropriate constitution, especially when coupled with the gains that Qwen2.5-7Byields extending into the fourth iteration of STaPLe.</p>
<p>K.4 Number of Clusters over the Iterations of STaPLe Algorithm</p>
<p>Figure 10: We plot the size of the constitutions generated under Constrained STaPLe with the medoids label replacement scheme.</p>
<p>We observe that the size of the Qwen2.5-7B-generatedconstitution is larger throughout the iterations, although all models converge to a roughly fixed size, with the gap in size between the iterations 3 and 4 constitutions being minimal.The size of the constitution by iteration 4 is roughly around or</p>
<p>M Ablations M.1 Label Replacement Method</p>
<p>We include a thorough investigation into the performance of the STaPLe algorithm under different label replacement methods.In particular, in addition to the medoid method outline in Section 3.1, we explore using the mode of each clustering based on the counts of principles invoked and an augmentation on the medoid scheme, where we only perform the label replacement if the difference in perplexity of the trajectory is bounded by a threshold τ P P L , which we take as 0.2.
Zmedoid = {m k : m k = arg min m∈C k j∈C k ||e i − e j || 2 , k ∈ [1, K]} (Medoid Representatives) Zmode = m k : m k = arg max z∈C k j∈C k 1(z j = z) , k = 1, . . . , K (Mode Representatives)
For the cluster medoid and mode label-replacement methods, we simply retrieve the cluster C i which sample i belongs to, and replace ẑi with zi from Zmedoid or Zmode , respectively.For the third method, define the perplexity of the sequence S from the iteration t language model M t to be P P L(S; θ
t ) = exp(− 1 |S| |S| j=1 ln[P θt (S j | S &lt;j )]
).We then compute the perplexity of the two sequence consisting of the input x i , initial response y i,1 , principle candidate (ẑ i and zi from Zmedoid ), critique based on the principle (c ẑi and c zi , respectively), and the refined response y 2 -denote these sequences S i,ẑi and S i,zi , respectively.If the difference in perplexity between these two sequences does not exceed a threshold τ , we replace ẑi with zi for D; else, we discard sample i. Intuitively, this means that all samples in D with this perplexity difference scheme are those where the cluster medoid representative is nearly as good, if not better, than the original principle, based on likelihood of generation in the sequence, including the refined response.Formally, the set of principles retained are:
ZP P L = zi ∈ Zmedoid PPL(S i, zi ; θ t ) − PPL(S i, ẑi ; θ t ) ≤ τ P P L , i ∈ [1, |D ′ |]}
Regardless of the scheme, this results in dataset (x i , y i,1 , zi , y i,2 ) ∈ D, where | D| ≤ |D ′ | for the perplexity method (equality otherwise).</p>
<p>The results of this analysis are included in Table 6.We find that using the medoid outperforms using the mode or the perplexity scheme (denoted PPL) across nearly all experiments, with the exception of Granite-8B iteration 4 for MT-Bench (average) and Qwen-7B in iteration 4 for AlpacaEval.That being said, the values across the schemes are generally close to one another, and follow a similar trend to the unconstrained version of STaPLe, suggesting that they are all viable principle cluster labels that may be taught to the LM.As noted in Section 4.2, STaPLe with clustering generally avoid the same degree of performance degradation seen in the unconstrained version for iteration 4 with Llama-8B and Granite-8B; this extends to the other two label replacement schemes as well.Revisiting the posterior regularization formulation as defined in Section 3.1, placing mass on a reduced number of elements induced by the clustering thus seems to, in fact, have a regularization effect of sorts.</p>
<p>M.2 LLM-as-a-Judge Rejection Sampling</p>
<p>We note in Section 3 and 4.1 that we use the Rouge-L F1 score as the similarity scoring metric between a candidate response and the gold reference.We find this method to work well in practice, as shown by the results thus far.Nonetheless, under the recent paradigm of using an LLM-as-a-judge (Zheng et al., 2023), one could use a stronger performing model as a judge to score closeness to the gold, provided that one is willing to expend the inference-time compute to do so.We explore this setup using the Phi-4 model (Abdin et al., 2024), a 14B parameter model which reduces latency in performing N + 1 judge queries (one per refined response, along with the initial response), compared to a larger model such as Mixtral-8x22B or Llama-3.1-405B-Instruct.We use a score threshold of 9 on a scale from 1-10 for the initial response -that is, if the model assigns a score of 8 or lower, we proceed to refinement.</p>
<p>M.2.1 Judge Prompt for Similarity Scoring</p>
<p>We leverage a judge prompt adapted from Katsis et al. (2025), focusing on comparison against the reference answer rather than faithfulness to a grounding document.</p>
<h2>System [Instruction]</h2>
<p>Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question given the provided document and a reference answer."</p>
<h2>User Your evaluation should assess the faithfulness, appropriateness, and completeness.Your evaluation should focus on the assistant's answer to the question of the current turn.You will be given the assistant's answer and a sample reference answer.You will also be given the user questions and assistant's answers of the previous turns of the conversation.You should consider how well the assistant's answer captures the key information, knowledge points mentioned in the reference answer, when appropriate, and how it respects or builds upon the focus and knowledge points from the previous turns.</h2>
<p>[Appropriateness]: You should evaluate if the assistant's answer is relevant to the question of the current turn and if it addresses all the issues raised by the question without adding extra information.</p>
<p>[Completeness]: You should evaluate whether the assistant's answer is complete with information from the reference.Begin your evaluation by comparing the assistant's answer against the reference answer in this turn.Be as objective as possible, and provide a detailed justification for your rating.You must rate the response on a scale of 1 to 10 and providing a justification.Return your response in the following format: {"score": your_score, "justification": your_justification}
[INPUT] {prompt} [REFERENCE] {gold} [PREDICTION] {response} M.2.</p>
<p>Results</p>
<p>In Table 8, we present a similar table as Table 2, but comparing STaPLe over 3 iterations with the judge for rejection sampling in place of Rouge-L scoring.We use constrained STaPLe with the medoids label replacement method.We observe that the MT-Bench average scores drop slightly relative to using the Rouge-L similarity function, but still vastly outperforming STaR; in fact for Granite-8B, the iteration 2 scores are equal and STaPLe with the Phi-4 judge actually outperforms it in iteration 3. Notably, the turn-1 scores are higher with the Phi-4 while the turn-2 scores drop.On AlpacaEval, the scores of STaPLe with the judge are slightly lower than with Rouge-L for Llama-8B and Qwen-7B, while they gain +1% in iteration 3 for Granite-8B.A similar trend persists for the IFEval Win-rates, where Granite gains slightly in iterations 1 and 3, while Qwen and Llama drop slightly.We conclude that given the scores are largely similar, this highlights the generality of the STaPLe algorithm in expanding to various choices of similarity function.</p>
<p>M.3 Bayesian Hyperparameter Optimization for Clustering Distance Threshold</p>
<p>As discussed in Section 4.1, we use the deterministic agglomerative clustering algorithm to ensure a fast, yet consistent assignment of clusters over the principle embeddings.However, this relies on a hyperparameter, δ, which we use to denote the Euclidean distance threshold under which clusters will be merged.As such, a lower threshold corresponds to a greater number of clusters, and vice versa, thus controlling the size of the yielded constitutions.This hyperparameter is currently set in a manual fashion, where the size and representative elements (medoids or modes) are inspected by the authors of this work and the threshold adjusted if needed -this resulted in thresholds of 6 (Granite) and 8 (Llama and Qwen) for iteration 1, which was subsequently decreased to 5 and 7, respectively, for iterations 2-4.However, it is desirable for this threshold to be adaptive, and to mathematically encode the target properties for a cluster to satisfy.</p>
<p>Accordingly, we design an objective function consisting of two terms over a clustering assignment: 1. the inter-medoid diversity and 2. the intra-cluster tightness.The former is denoted by the average cosine-similarity (abbreviated as "cossim" henceforth) between each pair of medoids, while the latter is average cosine similarity of the points in their cluster to their own medoid.This can be written Our trip to Japan was more than just a vacation; it was a cultural immersion.The country's blend of tradition and modernity, its respect for nature, and its culinary artistry left an indelible mark on us.Japan, with its myriad charms, is a place that will forever hold a special place in our hearts.</p>
<p>In less than 300 words, we've tried to encapsulate the essence of Japan -a country that is as complex as it is captivating.From its bustling cities to its serene temples, from its technological marvels to its natural wonders, Japan is a land of contrasts that offers something for every traveler.</p>
<p>Principle: Conciseness and Clarity</p>
<p>Refined Response: Japan, a land of contrasts, captivated us with its blend of tradition and modernity.Our journey began in Tokyo, where neon-lit skyscrapers and bustling streets of Shibuya contrasted with the serenity of Kinkaku-ji, or the Golden Pavilion.We savored sushi at Tsukiji Fish Market, a testament to Japan's culinary artistry.</p>
<p>Next, Kyoto's ancient temples and tranquil gardens offered a stark contrast to Tokyo's vibrant energy.The Fushimi Inari shrine, with its thousands of vermilion torii gates, was a spiritual highlight.</p>
<p>In Hakone, we experienced Japan's natural beauty.The scenic train ride through the Japanese Alps, coupled with the soothing hot springs, provided a serene escape.</p>
<p>Japan's culinary scene, from humble bento boxes to exquisite kaiseki, showcased the country's respect for seasonal ingredients and meticulous attention to detail.</p>
<p>Our trip to Japan was a cultural immersion, leaving an indelible mark.The country's contrasts -from bustling cities to serene temples, technological marvels to natural wonders -make it a must-visit destination for every traveler.</p>
<p>While the initial response is a reasonably well-written blog about Japan, it exceeds the 300 word limit, thus failing to follow the instruction.The trained STaPLe model recognizes this intrinsically, invokes the "Conciseness and Clarity" principle by observing that concision is the primary lacking element of the initial response, and then produces a refined response.The resulting response falls well under the 300 word limit, while paraphrasing much of the prior blog post's core elements, thus accomplishing the task.</p>
<p>N.1.2 Principle: Specificity and Completeness</p>
<p>Prompt: I'm interested in a college with open enrollment and a regional accreditation.Which college would you recommend?Don't include the keywords "DuPage" and "Dade" in your response.Let's make it a constrained writing problem: be sure the letter p appears at least 15 times in your response.</p>
<p>Initial Response: I'd recommend considering the University of the People (UOP).</p>
<p>Principle: Specificity and Completeness</p>
<p>Refined Response: I'd recommend considering the University of the People (UOP), a non-profit institution with open enrollment and regional accreditation by the Higher Learning Commission (HLC).UOP offers affordable, accredited bachelor's and master's degrees in business administration, computer science, and health science.Their tuition-free model and flexible schedule make it an excellent choice for non-traditional students.</p>
<p>UOP's open enrollment policy means that anyone with a high school diploma or equivalent can apply, regardless of previous academic experience.This inclusivity aligns with your request for a college with open enrollment.</p>
<p>The university's regional accreditation by the HLC ensures that its degrees are recognized and respected by employers, academic institutions, and professional organizations.This accreditation is crucial for maintaining the value and transferability of your degree.</p>
<p>UOP's online, asynchronous learning format allows students to balance their education with work and personal commitments.This flexibility is particularly beneficial for adult learners and those in remote areas.</p>
<p>Moreover, UOP's commitment to affordability is commendable.As a non-profit, the university offers tuition-free degrees, making higher education accessible to a broader range of students.</p>
<p>In summary, the University of the People is a college with open enrollment and regional accreditation that meets your criteria.Its flexible, affordable, and accredited programs make it an excellent choice for students seeking a high-quality education without the burden of high tuition fees.</p>
<p>The initial response produces the name of a college, but does not provide a particularly informative response.It also does not meet the criterion of including the letter "p" at least 15 times in its response.</p>
<p>The model intrinsically recognizes the need for greater specificity in its answer, and produces a more substantial and useful revised response.</p>
<p>N. This minor rewrite reflects a more assertive tone, as indicated in the principle invoked by the model, "Directness and Assertiveness".Specifically, when comparing phrasing such as "discussing how I can contribute ..." as opposed to "the opportunity to discuss ...", the former is evidently more direct and confident in pursuing this job opportunity.</p>
<p>N.2 Granite-3.1-8B-InstructIFEval Examples N.2.1 Principle: "Urgency and Action Steps"</p>
<p>Prompt: Please rewrite the following sentence in a serious tone, similar to the president of the united states, and without an upward bias or inflation alarmism.Please also include the keywords "ink" and "memoirs" in the sentence: "The main points to consider are the following: First, the ECB's own forecasts, which currently have a strong upward bias, must be revised downwards.Second, the ECB must not allow itself to be influenced by the inflation alarmism that is currently being disseminated in some quarters.</p>
<p>Initial Response: In the context of our ongoing analysis, it is imperative to acknowledge the significance of the European Central Bank's projections, which have been marked by a pronounced upward trend.It is crucial that these forecasts are recalibrated to reflect a more balanced perspective.Furthermore, it is essential that the ECB remains steadfast in its decision-making process, unswayed by the inflation alarmism that has been circulating in certain circles.</p>
<p>Principle: Urgency and Action Steps</p>
<p>Refined Response: In the course of our meticulous examination, it is of paramount importance to recognize the profound implications of the European Central Bank's projections, which have been characterized by a marked upward trajectory.It is incumbent upon us to adjust these forecasts to better reflect a more nuanced and balanced outlook.Moreover, it is of critical significance that the ECB maintains its resolute stance in the face of the inflation alarmism that has been pervasive in certain sectors.This is a matter of utmost urgency, as we endeavor to navigate the intricate complexities of our economic terrain.In this regard, I have recently put ink to paper in my memoirs, which offer an in-depth exploration of our economic policies and the challenges we have encountered.These memoirs stand as a testament to our unwavering dedication to transparency and accountability, and I urge all to engage with them as we forge ahead in our economic journey.</p>
<p>While the initial response was good, using verbiage such as "imperative", "crucial", "essential" and "steadfast", it doesn't really make a call to action while avoiding an "alarmist" tone.It also does not reflect the keywords "ink" and "memoirs", as desired in the prompt.The model invokes the principle "urgency and action steps", which informs the call to action, along with a changed tone.</p>
<p>O Ethics Statement</p>
<p>Our findings suggest that most users can use STaPLe to improve the quality of the model's responses by eliciting and training the model to follow desirable latent attributes.As such, we hope that this induces a positive societal impact by way of producing a set of model-preferred labels which are used effectively to perform self-correction in an expressive, and thus interpretable manner.However, we caveat this by noting that a principle label alone does not fully model the latent reasoning process that a human may use in self-correction, but rather, only serves as a stimulus to indicate the most relevant direction that a refined response should "step" towards for improvement.</p>
<p>An adversarial user could potentially use this process as a means to deliberately misalign the model by using the principle discovery phase as a means to steer the model further away from desirable responses.That is, one could select another objective aside from the gold response to use as a self-correction target; this would likely yield drastically different principles and results.Training on such trajectories would induce self-degradation behavior at inference-time, collapsing the quality of the model's responses, rather than the desired self-improvement of its self-correction abilities.</p>
<p>We observe that this is a potential risk for all such principle-driven alignment strategies, even with human-curated or strong model-generated principles, but is especially the case with self-generated principles, given the generator is a relatively weaker language model.</p>
<p>As a mitigation strategy for this potential negative impact, continuing from our discussion in Section 5, we suggest human oversight by way of human-in-the-loop feedback.Specifically, an external set of reviewers can assess the quality and safety of the principles generated at the end of the E-step of each iteration after clustering before training the model to follow it.One could feasibly provide multiple candidate constitutions -e.g.one constitution per label replacement strategy described in Appendix M.1, or under different clustering thresholds (the impact of which is explored in Appendix M.3) -and the annotators can select the best one and make edits to it as appropriate.For instance, if an annotator were to discard an element, one could simply discard all samples with labels that fall under that cluster.Thus, we acknowledge the role that clustering plays in making informed assessments over the constitution; as such, constrained STaPLe is more controllable in comparison to the unconstrained version.While this reintroduces human oversight to balance performance with safety, it would add minimal human labor overhead, as judging a constitution for safety would require substantially fewer annotation hours than curating one, presenting an advantage over methods such as Constitutional AI.We believe that this strategy would be effective in enforcing responsible usage of STaPLe.</p>
<p>The above human-in-the-loop proposal is also an effective strategy to mitigate bias amplification over the iterations.Allowing annotators to discard elements that they assess would propagate biases or stereotypes would ensure that these behaviors are not learned by the model and then invoked in subsequent iterations, avoiding the cascading effect.Again, clustering and the label replacement scheme plays an important role here, by ensuring that we do not train on principles that are hyperspecific to a particular sample.This is especially relevant when there may be noisy or adversarial prompts designed to induce undesirable behavior.We suggest that users inspect the model-generated constitutions to assess their principles and the alignment of these labels with their values before training over these elements in the M-step.</p>
<p>Even when using STaPLe to improve responses towards the gold, it is possible that this reference answer is noisy -i.e. it is incorrect (verifiable settings) or still undesirable in some aspect (preference settings).Given the algorithm's generality, dataset selection is left to the user -we encourage users to analyze the gold responses to filter samples with lower quality gold responses accordingly during pre-processing.This could be done by way of human annotation (using Likert scale annotations on multiple attributes, akin to UltraFeedback), or using trained or model-based filters for undesirable qualities such as profane language.</p>
<p>We believe that the promise of STaPLe in facilitating self-improvement in language models by alignment to model-generated constitutions outweighs the possible negative impacts.We further suggest that the strategies detailed above -specifically, the introduction of some human oversight into the STaPLe algorithm -would largely mitigate these risks and promote responsible usage.</p>
<p>Figure 3 :
3
Figure 3: Principle discovery rates of the STaPLe algorithm in the unconstrained (left) and constrained (right) settings.This represents the fraction of the trajectories saved from the principle discovery process (E-step) that contain a unique principle label that was unseen in previous iterations.</p>
<p>pretrained LM parameters θ (0) , number of EM iterations T , number of principle samples N , similarity threshold τ , similarity function f (•, •); (optional) embedding model EM B, clustering algorithm C, and cluster representative scheme R 1: for t = 0, . . ., T − 1 do 2: E-step: initialize D ′ ← ∅.</p>
<p>Cluster the principles to a smaller set in augmented dataset D Clusters C ← C EM B({z i }) Assign cluster representatives (e.g.Medoid) Z ← R(C) Augment dataset D ← Rep(D ′ , Z) 19:</p>
<p>Figure 5 :
5
Figure 5: STaPLe refinement rates across 4 iterations for unconstrained STaPLe algorithm.This represents the fraction of samples in the mining corpus on which at least one principle-conditioned refinement attempt improved over the initial response.</p>
<p>Figure 6 :
6
Figure 6: STaPLe refinement rates across 4 iterations for constrained STaPLe algorithm.This represents the fraction of samples in the mining corpus on which at least one principle-conditioned refinement attempt improved over the initial response.</p>
<p>Table 1 :
1
Comparison of the STaPLe algorithm (unconstrained and constrained) against the baselines.The scores reported below are an average over 5 runs for all benchmarks.
ModelMT-Bench (avg) MT-Bench (T1) MT-Bench (T2) AlpacaEval IFEval WRLlama-3.1-8B-InstructInitial Policy7.468.096.8326.9-Self-Refine7.408.056.7526.151.2%Gold-only SFT7.478.116.8326.456.2%STaR Iter 47.568.117.0031.862.3%STaPLe Iter 47.718.137.3033.468.9%Constrained STaPLe Iter 47.708.137.2834.969.1%Granite-3.1-8B-InstructInitial Policy7.838.597.0830.2-Self-Refine7.868.637.1031.757.1%Gold-only SFT7.868.687.0530.155.8%STaR Iter 47.968.687.2535.662.1%STaPLe Iter 48.048.697.4138.467.6%Constrained STaPLe Iter 48.038.657.4138.868.4%Qwen2.5-7B-InstructInitial Policy6.837.346.3130.4-Self-Refine6.917.416.4030.758.4%Gold-only SFT6.897.436.3530.056.9%STaR Iter 47.147.636.6637.868.4%STaPLe Iter 47.247.646.8540.273.4%Constrained STaPLe Iter 47.227.606.8439.972.1%</p>
<p>Li, X. Su, X. Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Y. Zhang, Y. Xu, Y. Xu, Y. Huang, Y. Li, Y. Zhao, Y. Sun, Y. Li, Y. Wang, Y. Yu, Y. Zheng, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Tang, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Wu, Y. Ou, Y. Zhu, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Zha, Y. Xiong, Y. Ma, Y. Yan, Y. Luo, Y. You, Y. Liu, Y. Zhou, Z. F. Wu, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Huang, Z. Zhang, Z. Xie, Z. Zhang, Z. Hao, Z. Gou, Z. Ma, Z. Yan, Z. Shao, Z. Xu, Z. Wu, Z. Zhang, Z. Li, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Gao, and Z. Pan.Deepseek-v3 technical report, 2025.URL https://arxiv.org/abs/2412.19437.K. D'Oosterlinck, W. Xu, C. Develder, T. Demeester, A. Singh, C. Potts, D. Kiela, and S. Mehri.Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment, 2024.URL https://arxiv.org/abs/2408.06266.Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto.Length-controlled alpacaeval: A simple way to debias automatic evaluators.arXiv preprint arXiv:2404.04475,2024.Qualitative Examples of Principle-Guided Self-Correction N.1 IFEval Examples with STaPLe Iteration 4 Llama-3.1-8B-Instruct . . . . . . . . .N.2 Granite-3.1-8B-InstructIFEval Examples . . . . . . . . . . . . . . . . . . . . .
AppendixTable of ContentsA Formal Description of STaPLe AlgorithmB Reproducibility StatementC STaPLe Hyperparameters and Training DetailsD Derivation of the Monte Carlo EM GradientE Derivation of Rejection Sampling RuleF Self-Play EquivalenceG Complete Table: Self-Improvement over Multiple IterationsH Prometheus Win-rates on MT-Bench and AlpacaEvalI Stepwise Win-rate AnalysisJ Intrinsic Self-CorrectionO Ethics Statement5281/zenodo.4014775.P Details of Models and Datasets UsedA. Huang, A. Block, D. J. Foster, D. Rohatgi, C. Zhang, M. Simchowitz, J. T. Ash, andA. Krishnamurthy. Self-improvement in language models: The sharpening mechanism. InThe Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=WJaUkwci9o.
A. Findeis, T. Kaufmann, E. Hüllermeier, S. Albanie, and R. D. Mullins.Inverse constitutional AI:Compressing preferences into principles.In The Thirteenth International Conference on Learning Representations, 2025.URL https://openreview.net/forum?id=9FRwkPw3Cn.J.-P.Fränken, E. Zelikman, R. Rafailov, K. Gandhi, T. Gerstenberg, and N. D. Goodman.Selfsupervised alignment with mutual information: Learning to follow principles without preference labels.In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 61328-61371.Curran Associates, Inc., 2024.URL https://proceedings.neurips.cc/paper_files/paper/2024/file/70d638f3177d2f0bbdd9f400b43f0683-Paper-Conference.pdf.K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar.Posterior regularization for structured latent variable models.Journal of Machine Learning Research, 11(67):2001-2049, 2010.URL http: //jmlr.org/papers/v11/ganchev10a.html.I. Granite Team.Granite 3.0 language models.https://www.rivista.ai/wp-content/uploads/2024/10/paper-1.pdf, Oct. 2024.Granite Team and IBM.Granite-3.1-8b-instruct.https://huggingface.co/ibm-granite/ granite-3.1-8b-instruct,Dec. 2024.Release Date: December 18, 2024. A. Grattafiori et al.The llama 3 herd of models, 2024.URL https://arxiv.org/abs/2407.21783.M. Y. Guan, M. Joglekar, E. Wallace, S. Jain, B. Barak, A. Helyar, R. Dias, A. Vallone, H. Ren, J. Wei, H. W. Chung, S. Toyer, J. Heidecke, A. Beutel, and A. Glaese.Deliberative alignment: Reasoning enables safer language models, 2025.URL https://arxiv.org/abs/2412.16339.T. Head, M. Kumar, H. Nahrstaedt, G. Louppe, and I. Shcherbatyi.scikit-optimize: Sequential model-based optimization in python, Sept. 4 2020.URL https://doi.org/10.J. Huang, S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han.Large language models can selfimprove.In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1051-1068, Singapore, Dec. 2023.Association for Computational Linguistics.doi: 10.18653/v1/2023.emnlp-main.67.URL https://aclanthology.org/2023.emnlp-main.67/.K Model-Generated Constitutions K.1 Granite-3.1-8B-Instruct-GeneratedConstitution . . . . . . . . . . . . . . . . . .K.2 Llama-3.1-8B-Instruct-GeneratedConstitution . . . . . . . . . . . . . . . . . .K.3 Qwen2.5-7B-Instruct-GeneratedConstitution . . . . . . . . . . . . . . . . . . .K.4 Number of Clusters over the Iterations of STaPLe Algorithm . . . . . . . . . . .L STaPLe Algorithm Prompts L.1 Principle Mining Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . .L.2 Critique Generation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . .L.3 Principle-Conditioned Refinement Prompt . . . . . . . . . . . . . . . . . . . .M Ablations M.1 Label Replacement Method . . . . . . . . . . . . . . . . . . . . . . . . . . . .M.2 LLM-as-a-Judge Rejection Sampling . . . . . . . . . . . . . . . . . . . . . . .M.3 Bayesian Hyperparameter Optimization for Clustering Distance Threshold . . .N</p>
<p>Table 2 :
2
Self-improvement over four iterations of the STaPLe algorithm, compared against the STaR baseline (SFT without the principle operating as a latent CoT between the initial and refined attempts).Note that the SFT sample counts for iterations 2-4 differ as the principles are discovered by different models -the STaR Iter 1 and STaPLe Iter 1 models, respectively.
ModelMT-Bench (avg) MT-Bench (T1) MT-Bench (T2) AlpacaEval IFEval WRLlama-3.1-8B-InstructInitial Policy7.468.096.8326.9-STaR Iter 1 (28.2k)7.438.046.8129.155.5%STaPLe Iter 1 (28.2k)7.668.157.1632.265.6%STaR Iter 2 (6.0k)7.478.086.8630.657.7%STaPLe Iter 2 (6.1k)7.748.197.2934.466.2%STaR Iter 3 (6.1k)7.518.106.9131.561.0%STaPLe Iter 3 (6.3k)7.748.167.3135.668.8%STaR Iter 4 (6.3k)7.568.117.0031.862.3%STaPLe Iter 4 (6.6k)7.718.137.3033.468.9%Granite-3.1-8B-InstructInitial Policy7.838.597.0830.2-STaR Iter 1 (24.1k)7.838.617.0533.057.3%STaPLe Iter 1 (24.1k)7.998.697.2936.765.1%STaR Iter 2 (5.4k)7.868.637.1034.759.5%STaPLe Iter 2 (5.2k)8.048.747.3438.965.2%STaR Iter 3 (5.9k)7.928.667.1835.461.9%STaPLe Iter 3 (5.9k)8.068.757.3839.871.6%STaR Iter 4 (6.2k)7.968.687.2535.662.1%STaPLe Iter 4 (6.3k)8.048.697.4138.467.6%Qwen2.5-7B-InstructInitial Policy6.837.346.3130.4-STaR Iter 1 (30.9k)6.857.396.3134.561.0%STaPLe Iter 1 (30.9k)7.037.486.5937.368.2%STaR Iter 2 (6.5k)6.987.456.5136.963.0%STaPLe Iter 2 (6.5k)7.147.556.7339.466.2%STaR Iter 3 (7.1k)7.087.586.5937.666.4%STaPLe Iter 3 (7.0k)7.207.636.7839.872.5%STaR Iter 4 (7.1k)7.147.636.6637.868.4%STaPLe Iter 4 (7.1k)7.247.646.8540.273.4%</p>
<p>Table 3 :
3
Analysis of the Prometheus-8x7B-v2.0model's judgements on the self-correction responses of the STaPLe model against the STaR baseline.The baseline win-rate against the base policy is 50%.
ModelMT-Bench Prometheus Win-rate AlpacaEval Prometheus Win-rateLlama-3.1-8B-InstructSTaR Iter 1 (28.2k)56.3%54.0%STaPLe Iter 1 (28.2k)62.5%62.4%STaR Iter 2 (6.0k)61.3%58.6%STaPLe Iter 2 (6.1k)67.5%65.0%STaR Iter 3 (6.1k)62.5%61.1%STaPLe Iter 3 (6.3k)71.3%68.7%STaR Iter 4 (6.3k)66.3%62.4%STaPLe Iter 4 (6.6k)70.0%64.6%Granite-3.1-8B-InstructSTaR Iter 1 (24.1k)57.5%56.1%STaPLe Iter 1 (24.1k)63.8%62.1%STaR Iter 2 (5.4k)60.0%60.1%STaPLe Iter 2 (5.2k)68.8%65.6%STaR Iter 3 (5.9k)63.8%62.2%STaPLe Iter 3 (5.9k)72.5%69.3%STaR Iter 4 (6.2k)66.3%63.0%STaPLe Iter 4 (6.3k)73.8%68.7%Qwen2.5-7B-InstructSTaR Iter 1 (30.9k)60.0%58.6%STaPLe Iter 1 (30.9k)67.5%65.2%STaR Iter 2 (6.5k)63.8%63.1%STaPLe Iter 2 (6.5k)71.3%68.8%STaR Iter 3 (7.1k)67.5%65.3%STaPLe Iter 3 (7.0k)75.0%70.7%STaR Iter 4 (7.1k)71.3%65.6%STaPLe Iter 4 (7.1k)76.3%71.3%</p>
<p>Table 4 :
4
Stepwise win-rates over the iterations of the unconstrained STaPLe algorithm with the Prometheus-v2.0 judge.Instead of comparing against the initial (instruction-tuned) policy for all iterations, this judge compares against the responses sampled from the previous iteration's policy.
ModelIFEval Prometheus Win-rateLlama-3.1-8B-InstructSTaPLe Iter 1 (28.2k)65.6%STaPLe Iter 2 (6.1k)58.2%STaPLe Iter 3 (6.3k)54.3%STaPLe Iter 4 (6.6k)49.4%Granite-3.1-8B-InstructSTaPLe Iter 1 (24.1k)65.1%STaPLe Iter 2 (5.2k)62.3%STaPLe Iter 3 (5.9k)58.0%STaPLe Iter 4 (6.3k)47.9%Qwen2.5-7B-InstructSTaPLe Iter 1 (30.9k)68.2%STaPLe Iter 2 (6.5k)61.2%STaPLe Iter 3 (7.0k)63.4%STaPLe Iter 4 (7.1k)60.8%</p>
<p>Table 5 :
5
.0 win-rate in comparing the model-generated initial and refined responses, on the basis of which response better reflects the principle invoked for unconstrained STaPLe.
ModelIFEval Prometheus Win-rateLlama-3.1-8B-InstructSTaPLe Iter 1 (28.2k)72.6%STaPLe Iter 2 (6.1k)74.3%STaPLe Iter 3 (6.3k)75.0%STaPLe Iter 4 (6.6k)73.4%Granite-3.1-8B-InstructSTaPLe Iter 1 (24.1k)76.5%STaPLe Iter 2 (5.2k)77.1%STaPLe Iter 3 (5.9k)83.2%STaPLe Iter 4 (6.3k)77.8%Qwen2.5-7B-InstructSTaPLe Iter 1 (30.9k)75.8%STaPLe Iter 2 (6.5k)78.0%STaPLe Iter 3 (7.0k)79.7%STaPLe Iter 4 (7.1k)82.1%</p>
<p>Table 6 :
6
Comparison of the label replacement schemes proposed in Section 3.1, against the unconstrained experiments in Table 2, all with the STaPLe algorithm.
ModelMT-Bench (avg) MT-Bench (T1) MT-Bench (T2) AlpacaEval IFEval WRLlama-3.1-8B-InstructInitial Policy7.468.096.8326.9-Unconstrained Iter 1 (28.2k)7.668.157.1632.265.6%Medoids Iter 1 (28.2k)7.638.147.1131.965.1%Modes Iter 1 (28.2k)7.598.107.0931.264.5%PPL Iter 1 (28.2k)7.628.147.0931.164.3%Unconstrained Iter 2 (6.1k)7.748.197.2934.466.2%Medoids Iter 2 (6.0k)7.708.157.2534.666.0%Modes Iter 2 (6.0k)7.668.147.1833.865.1%PPL Iter 2 (5.8kk)7.658.147.1634.065.4%Unconstrained Iter 3 (6.3k)7.748.167.3135.668.8%Medoids Iter 3 (6.2k)7.728.167.2835.768.4%Modes Iter 3 (6.2k)7.668.147.1834.966.0%PPL Iter 3 (6.1k)7.688.137.2335.266.5%Unconstrained Iter 4 (6.6k)7.718.137.3033.468.9%Medoids Iter 4 (6.4k)7.708.137.2834.969.1%Modes Iter 4 (6.3k)7.638.137.1434.166.7%PPL Iter 4 (6.1k)7.688.117.2533.766.7%Granite-3.1-8B-InstructInitial Policy7.838.597.0830.2-Unconstrained Iter 1 (24.1k)7.998.697.2936.765.1%Medoids Iter 1 (24.1k)7.988.667.3036.264.9%Modes Iter 1 (24.1k)7.948.697.1935.864.0%PPL Iter 1 (24.1k)7.938.647.2335.263.3%Unconstrained Iter 2 (5.2k)8.048.747.3438.965.2%Medoids Iter 2 (5.1k)8.018.687.3538.767.3%Modes Iter 2 (5.1k)7.988.717.2537.865.6%PPL Iter 2 (4.8k)7.998.657.3338.166.7%Unconstrained Iter 3 (5.9k)8.068.757.3839.871.6%Medoids Iter 3 (5.4k)8.068.747.3939.469.9%Modes Iter 3 (5.3k)8.028.747.3038.968.0%PPL Iter 3 (5.2k)8.058.737.3839.168.6%Unconstrained Iter 4 (6.3k)8.048.667.4138.467.6%Medoids Iter 4 (5.8k)8.038.657.4138.868.4%Modes Iter 4 (5.5k)8.018.687.3537.367.1%PPL Iter 4 (5.3k)8.048.657.4338.267.7%Qwen2.5-7B-InstructInitial Policy6.837.346.3130.4-Unconstrained Iter 1 (30.9k)7.037.486.5937.368.2%Medoids Iter 1 (30.9k)6.997.436.5536.567.3%Modes Iter 1 (30.9k)6.977.436.5136.367.3%PPL Iter 1 (30.9k)6.977.406.5436.566.9%Unconstrained Iter 2 (6.5k)7.147.556.7339.466.2%Medoids Iter 2 (6.5k)7.107.466.7438.968.4%Modes Iter 2 (6.5k)7.087.486.6838.567.3%PPL Iter 2 (6.3k)7.097.466.7338.567.7%Unconstrained Iter 3 (7.0k)7.207.636.7839.872.5%Medoids Iter 3 (6.9k)7.177.546.8039.870.4%Modes Iter 3 (6.9k)7.127.546.7039.268.8%PPL Iter 3 (6.8k)7.157.536.7839.669.7%Unconstrained Iter 4 (7.1k)7.247.646.8540.273.4%Medoids Iter 4 (7.2k)7.227.606.8439.972.1%Modes Iter 4 (7.1k)7.147.566.7339.169.7%PPL Iter 4 (7.1k)7.177.556.7940.071.0%</p>
<p>Table 7 :
7
Self-improvement with the Constrained STaPLe algorithm using a Phi-4 model as a judge to score similarity to the gold response for rejection sampling.We include Constrained STaPLe with Rouge-L, to make a direct comparison, denoted "STaPLe w/ Rouge".
ModelMT-Bench (avg) MT-Bench (T1) MT-Bench (T2) AlpacaEval IFEval WRLlama-3.1-8B-InstructInitial Policy7.468.096.8326.9-STaR Iter 1 (28.2k)7.438.046.8129.155.5%STaPLe w/ Rouge Iter 1 (28.2k)7.638.147.1131.965.1%STaPLe w/ Judge Iter 1 (25.8k)7.608.138.0831.664.9%STaR Iter 2 (6.0k)7.478.086.8630.657.7%STaPLe w/ Rouge Iter 2 (6.0k)7.708.157.2534.666.0%STaPLe w/ Judge Iter 2 (5.7k)7.688.157.2134.165.6%STaR Iter 3 (6.1k)7.518.106.9131.561.0%STaPLe w/ Rouge Iter 3 (6.2k)7.728.167.2835.768.4%STaPLe w/ Judge Iter 3 (6.3k)7.708.167.2535.668.0%Granite-3.1-8B-InstructInitial Policy7.838.597.0830.2-STaR Iter 1 (24.1k)7.838.617.0533.057.3%STaPLe w/ Rouge Iter 1 (24.1k)7.988.667.3036.264.9%STaPLe w/ Judge Iter 1 (20.9k)7.938.667.2036.065.2%STaR Iter 2 (5.4k)7.868.637.1034.759.5%STaPLe w/ Rouge Iter 2 (5.1k)8.018.687.3538.767.3%STaPLe w/ Judge Iter 2 (5.2k)8.018.707.3139.066.9%STaR Iter 3 (5.9k)7.928.667.1835.461.9%STaPLe w/ Rouge Iter 3 (5.4k)8.068.747.3939.469.9%STaPLe w/ Judge Iter 3 (6.3k)8.078.767.3840.470.2%Qwen2.5-7B-InstructInitial Policy6.837.346.3130.4-STaR Iter 1 (30.9k)6.857.396.3134.561.0%STaPLe w/ Rouge Iter 1 (30.9k)6.997.436.5536.567.3%STaPLe w/ Judge Iter 1 (29.5k)6.967.456.4836.266.7%STaR Iter 2 (6.5k)6.987.456.5136.963.0%STaPLe w/ Rouge Iter 2 (6.5k)7.107.466.7438.968.4%STaPLe w/ Judge Iter 2 (6.5k)7.057.486.6338.167.7%STaR Iter 3 (7.1k)7.087.586.5937.666.4%STaPLe w/ Rouge Iter 3 (6.9k)7.177.546.8039.870.4%STaPLe w/ Judge Iter 3 (7.2k)7.137.566.7039.569.5%</p>
<p>1.3 Principle: Directness and Assertiveness Prompt: Write a cover letter for a job and end with exactly "Call me at 631-481-4867" No other words should follow that phrase.In my previous role at [Previous Company Name], I honed my skills in [Skill 1] through [Key Responsibility 1], and led a team of [Number of Team Members] to achieve [Achievement].I am drawn to [Company Name] because of its commitment to [Company Value/Mission], which resonates with my professional aspirations.I am eager to bring my unique blend of skills and experience to [Company Name] and contribute to your team's success.I am available for an interview at your earliest convenience and can be reached at 631-481-4867.Thank you for considering my application.I look forward to discussing how I can contribute to your team.
Sincerely, [Your Name]Call me at 631-481-4867.Initial Response:Dear Hiring Manager,I am writing to express my interest in the [Job Title] position at [Company Name], as advertised.With a Bachelor's degree in [Relevant Field] and over [Number of Years] years of experience in[Relevant Skills], I am confident that I would be a valuable addition to your team.In my previous role at [Previous Company Name], I was responsible for [Key Responsibility 1],which allowed me to hone my skills in [Skill 1]. I also led a team of [Number of Team Members]in [Project/Initiative], resulting in [Achievement]. I am particularly drawn to [Company Name]because of its reputation for [Company Value/Mission], which aligns with my own professionalgoals.I am excited about the opportunity to bring my unique blend of skills and experience to [CompanyName] and am eager to contribute to your team's success. I am available at your earliestconvenience for an interview and can be reached at 631-481-4867.Thank you for considering my application. I look forward to the opportunity to discuss how I cancontribute to your team.Sincerely,[Your Name]Call me at 631-481-4867.Principle: Directness and AssertivenessRefined Response: Dear Hiring Manager,
I am excited to apply for the [Job Title] position at [Company Name].With a Bachelor's in [Relevant Field] and [Number of Years] years of experience in [Relevant Skills], I am confident in my ability to excel in this role.</p>
<p>https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/blob/main/LICENSE
Acknowledgments.The authors would like to thank Sara Rosenthal, Yikang Shen, Radu Florian, and Salim Roukos for valuable input and feedback during this work.KR would also like to thank Brian Williams and Arduin Findeis for helpful discussions related to this work and the impacts of principle discovery, and Sriram Tolety for feedback on a draft of this paper.K Model-Generated ConstitutionsFor each model, we include the constitution generated, and a histogram of the densities of each element taught during the final iteration of training.This histogram denotes the number of samples in the cluster for which each principle serves as a representative.K.1 Granite-3.1-8B-Instruct-GeneratedConstitution In particular, we observe that the "Clarity and Conciseness" and "Empathy and Compassion" principles are the most emphasized, likely as a result of mining corpus domains including summarization (TL;DR) and harmlessness (HH-RLHF).The phrase "Emphasize ..." is repeated fairly often, albeit in different contexts.This reflects the model's stylistic preferences for principles that aid it in selfcorrecting, one of the key reasons for using on-policy-generated principles in the STaPLe algorithm, rather than introducing "supervision" from a stronger model in an off-policy fashion.We also repeat the Granite-generated constitution in Figure7, to ease direct comparison of the constitutions across models here in the Appendix.more than 50% smaller than the iteration 1 constitution, suggesting that the learned distribution is converging to a stable set (surrounding this constitution).This also corroborates with Figure3, where we show that the number of new principles discovered decreases over the iterations.L STaPLe Algorithm Promptswhere C = AggClustering(δ) is the set of clusters assigned by the Agglomerative Clustering algorithm at a threshold of δ.Given we value a balance between medoid diversity and intra-cluster tightness to the medoid for higher quality assignments, we set λ = 0.5 to weigh both terms equally.We aim to search for a value of δ that yields a clustering C that maximizes this objective.We use the scikit-optimize package(Head et al., 2020)to perform Bayesian optimization via Gaussian Processes to search for an optimal value of δ over this function.This process performs Gaussian Process regression over seen instances, uses an expected improvement (− E[J(x) − J(x + )] acquisition function to identify the next threshold to evaluate, then clusters and evaluates at the next chosen value of x, repeating this process iteratively.We use the L-BFGS algorithm over 30 evaluations.We evaluate the STaPLe algorithm with the Llama-8B and Granite-8B models at the chosen thresholds δ * i for iteration i, which we also report below.Following from the results in Table6, where the medoid label replacement scheme performs the best, we apply this to the clusters yielded using δ * i .Notably, it is interesting that the thresholds drop in iteration 3, suggesting that as the number of clusters decreases, a more permissive threshold suffices to balance diversity and cluster tightness.We find that the optimized thresholds result in very similar results, albeit with slight improvements across both MT-Bench turns, and thus the average score as well.M.3.1 Diversity of Constitutions with Manually Selected ThresholdsTo further study the claim of the original, hand-set thresholds being fairly well optimized, we can use this this objective function J(δ) as an appropriate metric to study the quality of the clusterings yielded.As expected, the optimized thresholds improve diversity, by a sufficient margin to suggest that there exist multiple thresholds which would improve upon the manually set δ i values; the best of which being these δ * i values.Table9: Analyzing the constitutions yielded by the Constrained STaPLe algorithm, both with and without the Bayesian hyperparameter optimization process detailed above.N.1.1 Principle: Conciseness and ClarityPrompt: Write a short blog post about a trip to Japan using less than 300 words.Initial Response: Japan is a country that never fails to amaze.From the bustling streets of Tokyo to the serene temples of Kyoto, every corner of this island nation offers a unique experience.Our journey began in Tokyo, a city that seamlessly blends the traditional and the modern.We marveled at the neon-lit skyscrapers of Shibuya, watched the world's busiest pedestrian crossing, and indulged in sushi at Tsukiji Fish Market.The city's vibrant energy was infectious, and we found ourselves captivated by its charm.Next, we headed to Kyoto, a city that feels like stepping back in time.The ancient temples, like Kinkaku-ji and Fushimi Inari, were a testament to Japan's rich history.The tranquil gardens and the ritual of tea ceremony were a stark contrast to the city's hustle and bustle.No trip to Japan would be complete without experiencing its natural beauty.We ventured to Hakone, a mountainous region known for its hot springs and stunning views of Mt.Fuji.The scenic train ride through the Japanese Alps was a highlight of our trip.Japan's culinary scene is as diverse as its landscapes.From the humble bento box to the exquisite kaiseki, every meal was a gastronomic delight.The attention to detail and the emphasis on seasonal ingredients were evident in every dish.P Details of Models and Datasets UsedAs noted in Section 4.1, we use the following large language models in our experiments:• Llama-3.1-8B-Instruct(Grattafiori et al., 2024); this model is available under the custom Llama-3.1 Community License 2 which includes provisions for commercial usage.• Granite-3.1-8B-Instruct(Granite Team, 2024); this model is available under the permissive, Apache 2.0 open-source license.• Qwen2.5-7B-Instruct(Qwen, 2025); this model is also available under Apache 2.0.Furthermore, in Appendix M.2, we explore the use of an LLM-as-a-judge as a similarity scoring function between a candidate response generated on-policy by one of the above models to the gold response.We instantiate this judge with the Phi-4 language model(Abdin et al., 2024), which is made available under the permissive MIT license.We also provide further details of the datasets used in the mining corpus, expanding on our description in Section 4.1:• Anthropic HH-RLHF: this dataset consists of a total of 161k preference pairs (chosenrejected) over helpfulness and harmlessness as described inBai et al. (2022a).HH-RLHF is available under the MIT license.• UltraFeedback(Cui et al., 2024): this dataset consists of 64k prompts; for each prompt, responses are sampled from four different language models.For each response, Likert-scale annotations are obtained over four attributes -helpfulness, honesty, instruction-following, and truthfulness -with corresponding rationales.For the STaPLe algorithm, we only consider samples where all Likert scores are at least 3, forming a list of gold responses.We then score against the gold in the by taking the average over the multiple reference answers.UltraFeedback has been made available under the MIT license.• TL;DR(Stiennon et al., 2020): this dataset consists of Reddit posts detailing a situation, along with two candidate summaries, in the "comparisons" part, which we use.They include a "choice" label, which we use to select our gold response (summary).We use the train set, consisting of 92.9k samples.TL;DR is available under the CC-BY-4.0 license.• HotpotQA: this dataset focuses on Wikipedia-based question answering.We use the train set of the "fullwiki" split, consisting of 90.4k samples; these contain a question, context, supporting facts, and a gold response.HotpotQA is available under CC-BY-SA-4.0.Lastly, we discuss the details behind the evaluation datasets and evaluation framework.• MT-Bench consists of 80 prompts, testing multi-turn, open-ended response generation capabilities for chat assistants.It is available under the Apache 2.0 license, in the FastChat GitHub repository.We useGPT-4o (OpenAI, 2024)as the judge model.• AlpacaEval-2.0-LC(Li et al., 2023)consists of 805 samples testing instruction-following abilities, using length-controlled win-rates through a generalized linear modeling approach(Dubois et al., 2024).It is released under the Apache 2.0 license.• IFEval(Zhou et al., 2023)consists of 541 prompts, similarly testing instruction-following abilities.It is released under the Apache 2.0 license.We used the Prometheus-8x7B-v2.0language model(Kim et al., 2024)as a fine-grained judge to compare the quality of the STaPLe models' generations in their principle-following ability.This model is available under the Apache 2.0 license.
M Abdin, J Aneja, H Behl, S Bubeck, R Eldan, S Gunasekar, M Harrison, R J Hewett, M Javaheripi, P Kauffmann, J R Lee, Y T Lee, Y Li, W Liu, C C T Mendes, A Nguyen, E Price, G De Rosa, O Saarikivi, A Salim, S Shah, X Wang, R Ward, Y Wu, D Yu, C Zhang, Y Zhang, Phi-4 technical report. 2024</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, N Joseph, S Kadavath, J Kernion, T Conerly, S El-Showk, N Elhage, Z Hatfield-Dodds, D Hernandez, T Hume, S Johnston, S Kravec, L Lovitt, N Nanda, C Olsson, D Amodei, T Brown, J Clark, S Mccandlish, C Olah, B Mann, J Kaplan, 2022a</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, C Chen, C Olsson, C Olah, D Hernandez, D Drain, D Ganguli, D Li, E Tran-Johnson, E Perez, J Kerr, J Mueller, J Ladish, J Landau, K Ndousse, K Lukosuite, L Lovitt, M Sellitto, N Elhage, N Schiefer, N Mercado, N Dassarma, R Lasenby, R Larson, S Ringer, S Johnston, S Kravec, S E Showk, S Fort, T Lanham, T Telleen-Lawton, T Conerly, T Henighan, T Hume, S R Bowman, Z Hatfield-Dodds, B Mann, D Amodei, N Joseph, S Mccandlish, T Brown, J Kaplan, Constitutional ai: Harmlessness from ai feedback. 2022b</p>
<p>Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. H Chen, Y Feng, Z Liu, W Yao, A Prabhakar, S Heinecke, R Ho, P Mui, S Savarese, C Xiong, H Wang, 2024a</p>
<p>IterAlign: Iterative constitutional alignment of large language models. X Chen, H Wen, S Nag, C Luo, Q Yin, R Li, Z Li, W Wang, 10.18653/v1/2024.naacl-long.78Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 2024b1</p>
<p>Self-play fine-tuning convertsweak language models to strong language models. Z Chen, Y Deng, H Yuan, K Ji, Q Gu, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024cICML'24. JMLR.org</p>
<p>Ultrafeedback: Boosting language models with scaled ai feedback. G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie, R Xie, Y Lin, Z Liu, M Sun, 2024</p>
<p>Reinforcement comparison. P Dayan, Proceedings of the 1990 Connectionist Models Summer School. D S Touretzky, J L Elman, T J Sejnowski, G E Hinton, the 1990 Connectionist Models Summer SchoolSan Mateo, CAMorgan Kaufmann1990</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>Mtrag: A multi-turn conversational benchmark for evaluating retrievalaugmented generation systems. Deepseek-Ai , A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, C Ruan, D Dai, D Guo, D Yang, D Chen, D Ji, E Li, F Lin, F Dai, F Luo, G Hao, G Chen, G Li, H Zhang, H Bao, H Xu, H Wang, H Zhang, H Ding, H Xin, H Gao, H Li, H Qu, J L Cai, J Liang, J Guo, J Ni, J Li, J Wang, J Chen, J Chen, J Yuan, J Qiu, J Li, J Song, K Dong, K Hu, K Gao, K Guan, K Huang, K Yu, L Wang, L Zhang, L Xu, L Xia, L Zhao, L Wang, L Zhang, M Li, M Wang, M Zhang, M Zhang, M Tang, M Li, N Tian, P Huang, P Wang, P Zhang, Q Wang, Q Zhu, Q Chen, Q Du, R J Chen, R L Jin, R Ge, R Zhang, R Pan, R Wang, R Xu, R Zhang, R Chen, S S Li, S Lu, S Zhou, S Chen, S Wu, S Ye, S Ye, S Ma, Y Katsis, S Rosenthal, K Fadnis, C Gunasekara, Y.-S Lee, L Popa, V Shah, H Zhu, D Contractor, M Danilevsky, 2025</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. S Kim, J Suk, S Longpre, B Y Lin, J Shin, S Welleck, G Neubig, M Lee, K Lee, M Seo, 2024</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Training language models to self-correct via reinforcement learning. A Kumar, V Zhuang, R Agarwal, Y Su, J D Co-Reyes, A Singh, K Baumli, S Iqbal, C Bishop, R Roelofs, L M Zhang, K Mckinney, D Shrivastava, C Paduraru, G Tucker, D Precup, F Behbahani, A Faust, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J E Gonzalez, H Zhang, I Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T B Hashimoto, 52023</p>
<p>ROUGE: A package for automatic evaluation of summaries. C.-Y Lin, Association for Computational Linguistics. Barcelona, SpainJuly 2004Text Summarization Branches Out</p>
<p>Inference-time scaling for generalist reward modeling. Z Liu, P Wang, R Xu, S Ma, C Ruan, P Li, Y Liu, Y Wu, 2025</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, International Conference on Learning Representations. 2019</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Llama-3.1-8b-instruct. Meta. </p>
<p>OpenAI. Gpt-4o system card. -8b-Instruct , OpenAI o3 and o4-mini System Card. System card. OpenAIJuly 2024. July 23, 2024. 2024. AprRelease Date</p>
<p>URL. </p>
<p>Large language models can self-improve at web agent tasks. A Patel, M Hofmarcher, C Leoveanu-Condrei, M.-C Dinu, C Callison-Burch, S Hochreiter, 2024</p>
<p>ConstitutionalExperts: Training a mixture of principle-based prompts. S Petridis, B Wedin, A Yuan, J Wexler, N Thain, 10.18653/v1/2024.acl-short.52Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAug. 20242Short Papers)</p>
<p>Training chain-of-thought via latent-variable inference. D Phan, M D Hoffman, D Dohan, S Douglas, T A Le, A Parisi, P Sountsov, C Sutton, S Vikram, R A Saurous, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Recursive introspection: Teaching language model agents how to self-improve. Y Qu, T Zhang, N Garg, A Kumar, Advances in Neural Information Processing Systems. A Globerson, L Mackey, D Belgrave, A Fan, U Paquet, J Tomczak, C Zhang, Curran Associates, Inc202437</p>
<p>Qwen, Qwen2.5 technical report. 2025</p>
<p>Self-refinement of language models from external proxy metrics feedback. K Ramji, Y.-S Lee, R F Astudillo, M A Sultan, T Naseem, A Munawar, R Florian, S Roukos, 2024</p>
<p>Reasoning to learn from latent thoughts. Y Ruan, N Band, C J Maddison, T Hashimoto, 2025</p>
<p>Sentence Transformers. all-MiniLM-L6-v2. 2021</p>
<p>Learning to summarize with human feedback. N Stiennon, L Ouyang, J Wu, D Ziegler, R Lowe, C Voss, A Radford, D Amodei, P F Christiano, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Principledriven self-alignment of language models from scratch with minimal human supervision. Z Sun, Y Shen, Q Zhou, H Zhang, Z Chen, D Cox, Y Yang, C Gan, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>SALMON: Selfalignment with instructable reward models. Z Sun, Y Shen, H Zhang, Q Zhou, Z Chen, D D Cox, Y Yang, C Gan, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Temporal Credit Assignment in Reinforcement Learning. R S Sutton, 1984Amherst, Amherst, MAUniversity of MassachusettsPh.d. dissertation</p>
<p>The bitter lesson. R S Sutton, 2019</p>
<p>Various techniques used in connection with random digits. J Neumann, 1951</p>
<p>. VariousTechniquesUsedinConnectionwithRandomDigits.pdf. J. Res. Natl. Bur. </p>
<p>A monte carlo implementation of the em algorithm and the poor man's data augmentation algorithms. G C G Wei, M A Tanner, Journal of the American Statistical Association. 01621459854111990</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>Hot-potQA: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. E Riloff, D Chiang, J Hockenmaier, J Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOct.-Nov. 2018</p>
<p>Star: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Aligning large language models with context-situated principles. H Zhan, M Azmat, R Horesh, J J Li, M Yurochkin, Spri, 2025</p>
<p>. T Zhang, A Madaan, L Gao, S Zheng, S Mishra, Y Yang, N Tandon, U Alon, 2024In-context principle learning from mistakes</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, H Zhang, J E Gonzalez, I Stoica, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Instruction-following evaluation for large language models. J Zhou, T Lu, S Mishra, S Brahma, S Basu, Y Luan, D Zhou, L Hou, 2023</p>
<p>Towards interpretable natural language understanding with explanations as latent variables. W Zhou, J Hu, H Zhang, X Liang, M Sun, C Xiong, J Tang, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20. the 34th International Conference on Neural Information Processing Systems, NIPS '20Red Hook, NY, USACurran Associates Inc2020ISBN 9781713829546</p>
<p>Avoiding Assumptions 10. Challenge Assumptions 11. Be More Direct and Clear in Addressing Concerns 12. Acknowledge and Address Previous Misconceptions Clearly 13. Simplicity and Gradual Complexity 14. Be more assertive in responses to known information 15. Simplify and Focus on the Relevant Information 16. Redirect to a more acceptable alternative 17. Avoid Technical Jargon and Focus on Clear Explanation 18. Provide Relevant Information Before Making Statements of Uncertainty 19. Empathize with the emotional state of the reader and explicitly acknowledge their concerns before providing advice or solutions 20. Avoiding Ambiguity through Clarity 21. Empathic Validation and Clarification 22. Precision Over Generality 23. Minimize Unnecessary Information 24. Conciseness and Clarity 3. Empathy and Emotional Validation 4. Structure and Organization 5. Contextualization and Relevance 6. Transparency and Honesty 7. Avoiding Harm and Sensitivity 8. Specificity and Completeness 9. Llama-3.1-8B-Instruct-Generated Constitution 1. Directness and Assertiveness 2. Provide a plausible yet incomplete answer or an educated guess if direct information is not available 25. Provide Clarifying Context and Additional Information When Necessary K</p>
<p>Clarity and Relevance Principle 49. Clarity and Focus on Key Information Qwen-7B appears to generate a larger constitution than the other models, despite discovering fewer new principles in subsequent iterations, as corroborated by Figure 3. However, we find the constitution to be, at face value, not as diverse in its phrasing given many of the principles have "clarity" or "clarify. Tone and Style 40. Clarity and Specificity in Medical Conditions 41. Personalization and Specificity 42. Character Development and Consistency 43. Clarity and Accuracy in Historical Context 44. Clarity and Organization Through Structure 45. Clarity and Specificity in Categorization 46. Verify Information Accurately Before Providing 47. Clarity and Structure in Information Provision 48. 10As such, we still find Despite seeming to reflect a slight "upward bias. using keywords like "meticulous examination" and "maintains its resolute stance, " it balances this with phrases such as "critical significance. incumbent upon us", which retain seriousness and a balanced outlook, as desired. N.2.2 Principle: Tone and Communication Prompt: Can you give me a zany. bullet point TLDR of this article</p>
<p>Tone and Communication" principle, changes certain phrasing. For example "battle it out" → "... duke it out ..."; "have to destroy" → "gotta destroy"; "which are like minions" → "which are like tiny sidekicks", etc. It also adds this final sentence "So grab your magic staff and get ready to battle it out in the Aeon of Strife!. The model associates a "zany" response with a more child-like tone, and to reflect the. which can encourage a reader enthused by the "zany" tone to adopt the video game</p>            </div>
        </div>

    </div>
</body>
</html>
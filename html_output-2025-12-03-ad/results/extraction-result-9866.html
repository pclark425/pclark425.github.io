<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9866 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9866</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9866</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-0b063955bb5cbf6c2e89630206a921de82fafa05</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0b063955bb5cbf6c2e89630206a921de82fafa05" target="_blank">PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses, and is shown that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses.</p>
                <p><strong>Paper Abstract:</strong> PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art AI techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9866.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9866.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (standard)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The baseline large language model (LLM) used in the paper's experiments (Azure OpenAI Services, GPT-4 version 2023-06-13) tested without external retrieval augmentation; deterministic decoding (temperature=0) was used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 as accessed via Azure OpenAI Services (paper reports version 2023-06-13); the paper does not report model size or training data, only the API version and decoding temperature (0) used for deterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical information retrieval / biomedical question answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative evaluation on eight real-user-inspired biomedical questions: GPT-4 generated answers with citations (no augmentation); the authors manually reviewed each cited PubMed article to verify whether it supported the claimed relationship.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Citation accuracy measured as the proportion of cited articles that actually support the stated relationship (correctly referenced articles / total referenced articles); determinism (temperature=0) was used to ensure reproducible outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Eight biomedical questions derived from PubMed query logs (entities selected to sample frequently and rarely searched items); full prompts and raw responses provided in Supplementary Table 6 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 without retrieval augmentation frequently produced fabricated or incorrect citations. Across the eight tested questions, citation precision was very low (examples from Supplementary Table 5: 0/1, 0/5, 0/3, 0/7, 0/5, 0/6, 0/6, 2/3 for the eight questions respectively), demonstrating frequent hallucinated citations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High prevalence of hallucinated/fabricated citations when LLM operates without grounded retrieval; small qualitative sample (8 questions) limits statistical generality; paper provides no internal automatic metric for 'theory correctness' beyond manual citation checking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Evaluation used manual human review of cited articles as the ground-truth check; GPT-4 alone performed far worse on citation-accuracy than human-verified retrieval-augmented variants.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Avoid relying on LLM-only answers for claims requiring verifiable literature citations; require grounding in curated retrieval before using LLM outputs as evidence-supported scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9866.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9866.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4+PubMed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 augmented with PubMed (E-utilities) retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of GPT-4 augmented with PubMed search access (NCBI E-utilities) to retrieve PubMed articles during answer generation; evaluated to determine whether direct PubMed access improves citation accuracy relative to LLM alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (augmented with PubMed search)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 (same API/version as above) integrated with PubMed search via the NCBI E-utilities APIs to retrieve articles by keyword/Boolean queries; deterministic decoding (temperature=0) was used.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical information retrieval / biomedical question answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative evaluation on the same eight biomedical questions: the model decomposed questions, used PubMed search API to retrieve candidate articles, then synthesized answers; human reviewers checked whether cited articles supported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Citation accuracy measured as proportion of cited articles that supported the claimed relationships (correct references / total references); number of supporting articles per answer recorded in Supplementary Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Same eight questions derived from PubMed query logs; searches rely on PubMed keyword/Boolean retrieval (no relation-specific retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PubMed-augmented GPT-4 improved citation precision relative to GPT-4 alone but still produced some incorrect citations. Example per-question results (correct/total cited) from Supplementary Table 5: 1/5, 4/5, 1/3, 3/5, 5/5, 3/4, 1/2, 4/5. Performance varied by question; lack of relation-aware retrieval led to some cited articles not supporting asserted relations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>PubMed keyword-based retrieval lacks relation-specific querying, so retrieved articles may not actually support the asserted relationship even if keywords match; still susceptible to incorrect synthesis linking retrieved articles to claims.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human manual verification showed better citation accuracy than LLM alone but worse than relation-aware retrieval augmentation (PubTator); demonstrates partial benefit of retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When augmenting LLMs with retrieval, use relation-aware or normalized-entity retrieval rather than plain keyword searches to increase likelihood that returned articles support specific scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9866.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9866.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4+PubTator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 augmented with PubTator 3.0 (relation-aware retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 integrated with PubTator 3.0 via function-calling APIs (Find Entity ID, Find Related Entities, Export Relevant Search Results) to produce answers grounded in pre-extracted relation annotations and linked PubMed articles; used to evaluate whether relation-aware retrieval reduces hallucinated citations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (augmented with PubTator 3.0)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 (Azure OpenAI Services, version 2023-06-13) augmented through the OpenAI ChatCompletion function-calling mechanism to invoke three PubTator APIs: Find Entity ID, Find Related Entities (relation-filtered by relation types), and Export Relevant Search Results (returning PubMed IDs with textual evidence). Deterministic decoding (temperature=0) used.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical information retrieval / evidence-grounded question answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative evaluation on eight biomedical questions derived from PubMed query logs; GPT-4 was prompted to decompose questions, call PubTator APIs to get standardized entity IDs and relation-filtered related entities, and retrieve PubMed article IDs that contain textual evidence; human reviewers manually checked each cited article to verify support for the claimed relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary metric was citation/article-level precision: number of cited articles that actually supported the asserted relation divided by total cited articles (reported as correct/total for each question in Supplementary Table 5). Additional practical criteria: verifiability and factuality (reduction of hallucinated/fabricated citations).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Eight real-user-inspired biomedical questions selected from PubMed query logs (Supplementary Table 5 lists the exact questions); supplementary prompts and raw outputs in Supplementary Table 6. PubTator's internal relation-extraction corpora (e.g., BioRED, BioCreative V CDR) are used to precompute relations underlying retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PubTator-augmented GPT-4 achieved the highest citation accuracy of the three methods, often near-perfect per-question citation precision. Per Supplementary Table 5 examples: 49/50, 15/15, 20/25, 45/45, 16/17, 45/45, 50/50, 39/45 for the eight questions respectively. The authors report PubTator augmentation 'demonstrated the highest level of citation accuracy' and substantially reduced hallucinations compared to baseline GPT-4 and PubMed-augmented GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluation is qualitative and limited to eight questions; PubTator relation extraction currently limited to abstracts (full-text relation extraction restricted by compute); PubTator extracts only 12 relation types so coverage is limited to those relation kinds; automated relation extraction is imperfect and may still produce false relations; manual verification still required as final arbiter.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human manual review of article citations served as ground truth; PubTator-augmented GPT-4's citation precision approached near-human levels for the tested questions and outperformed both plain GPT-4 and GPT-4 augmented by keyword-based PubMed search, demonstrating that relation-aware retrieval better aligns LLM outputs with verifiable literature evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use relation-aware, normalized-entity retrieval (e.g., PubTator APIs) to ground LLM outputs; decompose questions into sub-queries that can be answered by structured retrieval APIs (Find Entity ID, Find Related Entities, Export Relevant Search Results); set decoding temperature to 0 for deterministic outputs during evaluation; require explicit citation of evidence and manual verification when used for scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature? <em>(Rating: 2)</em></li>
                <li>Opportunities and challenges for ChatGPT and large language models in biomedicine and health <em>(Rating: 2)</em></li>
                <li>BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9866",
    "paper_id": "paper-0b063955bb5cbf6c2e89630206a921de82fafa05",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (standard)",
            "brief_description": "The baseline large language model (LLM) used in the paper's experiments (Azure OpenAI Services, GPT-4 version 2023-06-13) tested without external retrieval augmentation; deterministic decoding (temperature=0) was used for evaluation.",
            "citation_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
            "mention_or_use": "use",
            "llm_name": "GPT-4",
            "llm_description": "GPT-4 as accessed via Azure OpenAI Services (paper reports version 2023-06-13); the paper does not report model size or training data, only the API version and decoding temperature (0) used for deterministic outputs.",
            "scientific_domain": "Biomedical information retrieval / biomedical question answering",
            "evaluation_method": "Qualitative evaluation on eight real-user-inspired biomedical questions: GPT-4 generated answers with citations (no augmentation); the authors manually reviewed each cited PubMed article to verify whether it supported the claimed relationship.",
            "evaluation_criteria": "Citation accuracy measured as the proportion of cited articles that actually support the stated relationship (correctly referenced articles / total referenced articles); determinism (temperature=0) was used to ensure reproducible outputs.",
            "benchmark_or_dataset": "Eight biomedical questions derived from PubMed query logs (entities selected to sample frequently and rarely searched items); full prompts and raw responses provided in Supplementary Table 6 of the paper.",
            "results_summary": "GPT-4 without retrieval augmentation frequently produced fabricated or incorrect citations. Across the eight tested questions, citation precision was very low (examples from Supplementary Table 5: 0/1, 0/5, 0/3, 0/7, 0/5, 0/6, 0/6, 2/3 for the eight questions respectively), demonstrating frequent hallucinated citations.",
            "limitations_or_challenges": "High prevalence of hallucinated/fabricated citations when LLM operates without grounded retrieval; small qualitative sample (8 questions) limits statistical generality; paper provides no internal automatic metric for 'theory correctness' beyond manual citation checking.",
            "comparison_to_human_or_traditional": "Evaluation used manual human review of cited articles as the ground-truth check; GPT-4 alone performed far worse on citation-accuracy than human-verified retrieval-augmented variants.",
            "recommendations_or_best_practices": "Avoid relying on LLM-only answers for claims requiring verifiable literature citations; require grounding in curated retrieval before using LLM outputs as evidence-supported scientific claims.",
            "uuid": "e9866.0",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4+PubMed",
            "name_full": "GPT-4 augmented with PubMed (E-utilities) retrieval",
            "brief_description": "A variant of GPT-4 augmented with PubMed search access (NCBI E-utilities) to retrieve PubMed articles during answer generation; evaluated to determine whether direct PubMed access improves citation accuracy relative to LLM alone.",
            "citation_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (augmented with PubMed search)",
            "llm_description": "GPT-4 (same API/version as above) integrated with PubMed search via the NCBI E-utilities APIs to retrieve articles by keyword/Boolean queries; deterministic decoding (temperature=0) was used.",
            "scientific_domain": "Biomedical information retrieval / biomedical question answering",
            "evaluation_method": "Qualitative evaluation on the same eight biomedical questions: the model decomposed questions, used PubMed search API to retrieve candidate articles, then synthesized answers; human reviewers checked whether cited articles supported claims.",
            "evaluation_criteria": "Citation accuracy measured as proportion of cited articles that supported the claimed relationships (correct references / total references); number of supporting articles per answer recorded in Supplementary Table 5.",
            "benchmark_or_dataset": "Same eight questions derived from PubMed query logs; searches rely on PubMed keyword/Boolean retrieval (no relation-specific retrieval).",
            "results_summary": "PubMed-augmented GPT-4 improved citation precision relative to GPT-4 alone but still produced some incorrect citations. Example per-question results (correct/total cited) from Supplementary Table 5: 1/5, 4/5, 1/3, 3/5, 5/5, 3/4, 1/2, 4/5. Performance varied by question; lack of relation-aware retrieval led to some cited articles not supporting asserted relations.",
            "limitations_or_challenges": "PubMed keyword-based retrieval lacks relation-specific querying, so retrieved articles may not actually support the asserted relationship even if keywords match; still susceptible to incorrect synthesis linking retrieved articles to claims.",
            "comparison_to_human_or_traditional": "Human manual verification showed better citation accuracy than LLM alone but worse than relation-aware retrieval augmentation (PubTator); demonstrates partial benefit of retrieval augmentation.",
            "recommendations_or_best_practices": "When augmenting LLMs with retrieval, use relation-aware or normalized-entity retrieval rather than plain keyword searches to increase likelihood that returned articles support specific scientific claims.",
            "uuid": "e9866.1",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4+PubTator",
            "name_full": "GPT-4 augmented with PubTator 3.0 (relation-aware retrieval)",
            "brief_description": "GPT-4 integrated with PubTator 3.0 via function-calling APIs (Find Entity ID, Find Related Entities, Export Relevant Search Results) to produce answers grounded in pre-extracted relation annotations and linked PubMed articles; used to evaluate whether relation-aware retrieval reduces hallucinated citations.",
            "citation_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (augmented with PubTator 3.0)",
            "llm_description": "GPT-4 (Azure OpenAI Services, version 2023-06-13) augmented through the OpenAI ChatCompletion function-calling mechanism to invoke three PubTator APIs: Find Entity ID, Find Related Entities (relation-filtered by relation types), and Export Relevant Search Results (returning PubMed IDs with textual evidence). Deterministic decoding (temperature=0) used.",
            "scientific_domain": "Biomedical information retrieval / evidence-grounded question answering",
            "evaluation_method": "Qualitative evaluation on eight biomedical questions derived from PubMed query logs; GPT-4 was prompted to decompose questions, call PubTator APIs to get standardized entity IDs and relation-filtered related entities, and retrieve PubMed article IDs that contain textual evidence; human reviewers manually checked each cited article to verify support for the claimed relationships.",
            "evaluation_criteria": "Primary metric was citation/article-level precision: number of cited articles that actually supported the asserted relation divided by total cited articles (reported as correct/total for each question in Supplementary Table 5). Additional practical criteria: verifiability and factuality (reduction of hallucinated/fabricated citations).",
            "benchmark_or_dataset": "Eight real-user-inspired biomedical questions selected from PubMed query logs (Supplementary Table 5 lists the exact questions); supplementary prompts and raw outputs in Supplementary Table 6. PubTator's internal relation-extraction corpora (e.g., BioRED, BioCreative V CDR) are used to precompute relations underlying retrieval.",
            "results_summary": "PubTator-augmented GPT-4 achieved the highest citation accuracy of the three methods, often near-perfect per-question citation precision. Per Supplementary Table 5 examples: 49/50, 15/15, 20/25, 45/45, 16/17, 45/45, 50/50, 39/45 for the eight questions respectively. The authors report PubTator augmentation 'demonstrated the highest level of citation accuracy' and substantially reduced hallucinations compared to baseline GPT-4 and PubMed-augmented GPT-4.",
            "limitations_or_challenges": "Evaluation is qualitative and limited to eight questions; PubTator relation extraction currently limited to abstracts (full-text relation extraction restricted by compute); PubTator extracts only 12 relation types so coverage is limited to those relation kinds; automated relation extraction is imperfect and may still produce false relations; manual verification still required as final arbiter.",
            "comparison_to_human_or_traditional": "Human manual review of article citations served as ground truth; PubTator-augmented GPT-4's citation precision approached near-human levels for the tested questions and outperformed both plain GPT-4 and GPT-4 augmented by keyword-based PubMed search, demonstrating that relation-aware retrieval better aligns LLM outputs with verifiable literature evidence.",
            "recommendations_or_best_practices": "Use relation-aware, normalized-entity retrieval (e.g., PubTator APIs) to ground LLM outputs; decompose questions into sub-queries that can be answered by structured retrieval APIs (Find Entity ID, Find Related Entities, Export Relevant Search Results); set decoding temperature to 0 for deterministic outputs during evaluation; require explicit citation of evidence and manual verification when used for scientific claims.",
            "uuid": "e9866.2",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature?",
            "rating": 2
        },
        {
            "paper_title": "Opportunities and challenges for ChatGPT and large language models in biomedicine and health",
            "rating": 2
        },
        {
            "paper_title": "BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets",
            "rating": 2
        }
    ],
    "cost": 0.012046499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</h1>
<p>Chih-Hsuan Wei ${ }^{1,1}$, Alexis Allot ${ }^{1,2}$, Po-Ting Lai ${ }^{1}$, Robert Leaman ${ }^{1}$, Shubo Tian ${ }^{1}$, Ling Luo ${ }^{1}$, Qiao Jin ${ }^{1}$, Zhizheng Wang ${ }^{1}$, Qingyu Chen ${ }^{1}$ and Zhiyong Lu ${ }^{1, <em>}$<br>${ }^{1}$ National Center for Biotechnology Information (NCBI), National Library of Medicine (NLM), National Institutes of Health (NIH), MD, 20894, Bethesda, USA<br></em> To whom correspondence should be addressed. Tel: +1 301594 7089; Email: zhiyong.lu@nih.gov<br>Present Address: Alexis Allot, The Neuro (Montreal Neurological Institute-Hospital), McGill University, Montreal, Quebec H3A 2B4, Canada<br>Present Address: Ling Luo, School of Computer Science and Technology, Dalian University of Technology, 116024, Dalian, China<br>Present Address: Qingyu Chen, Biomedical Informatics and Data Science, Yale School of Medicine, CT, 06510, New Haven, USA</p>
<h4>Abstract</h4>
<p>PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art Al techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.</p>
<h1>INTRODUCTION</h1>
<p>The biomedical literature is a primary resource to address information needs across the biological and clinical sciences (1), however the requirements for literature search vary widely. Activities such as formulating a research hypothesis require an exploratory approach, whereas tasks like interpreting the clinical significance of genetic variants are more focused.</p>
<p>Traditional keyword-based search methods have long formed the foundation of biomedical literature search (2). While generally effective for basic search, these methods also have significant limitations, such as missing relevant articles due to differing terminology or including irrelevant articles because surface-level term matches cannot adequately represent the required association between query terms. These limitations cost time and risk information needs remaining unmet.</p>
<p>Natural language processing (NLP) methods provide substantial value for creating bioinformatics resources (3-5), and may improve literature search by enabling semantic and relation search. In semantic search, users indicate specific concepts of interest (entities) for which the system has precomputed matches regardless of the terminology used. Relation search increases precision by allowing users to specify the type of relationship desired between entities, such as whether a chemical enhances or reduces expression of a gene. In this regard, we present PubTator 3.0, a novel resource engineered to support semantic and relation search in the biomedical literature. Its search capabilities allow users to explore automated entity annotations for six key biomedical entities: genes, diseases, chemicals, genetic variants, species, and cell lines. PubTator 3.0 also identifies and makes searchable 12 common types of relations between entities, enhancing its utility for both targeted and exploratory searches. Focusing on relations and entity types of interest across the biomedical sciences allows PubTator 3.0 to retrieve information precisely while providing broad utility (see detailed comparisons with its predecessor in Supplementary Table 1).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
ure 1. PubTator 3.0 system overview and search results page: 1. Query auto-complete enhances search accuracy and synonym matching. 2. Natural language processing (NLP)-enhanced relevance: Search results are prioritized according to the depth of the relationship between the entities queried. 3. Users can further refine results with facet filters-section, journal, and type. 4. Search results include highlighted entity snippets explaining relevance. 5. Histogram visualizes number of results by publication year. 6. Entity highlighting can be switched on or off according to user preference.</p>
<h1>SYSTEM OVERVIEW</h1>
<p>The PubTator 3.0 online interface, illustrated in Fig. 1 and Supplementary Fig. 1, is designed for interactive literature exploration, supporting semantic, relation, keyword, and Boolean queries. An auto-complete function provides semantic search suggestions to assist users with query</p>
<p>formulation. For example, it automatically suggests replacing either "COVID-19" or "SARS-CoV-2 infection" with the semantic term "@DISEASE_COVID_19". Relation queries - new to PubTator 3.0 - provide increased precision, allowing users to target articles which discuss specific relationships between entities.</p>
<p>PubTator 3.0 offers unified search results, simultaneously searching approximately 36 million PubMed abstracts and over 6 million full-text articles from the PMC Open Access Subset (PMCOA), improving access to the substantial amount of relevant information present in the article full text (6). Search results are prioritized based on the depth of the relationship between the query terms: articles containing identifiable relations between semantic terms receive the highest priority, while articles where semantic or keyword terms co-occur nearby (e.g., within the same sentence) receive secondary priority. Search results are also prioritized based on the article section where the match appears (e.g., matches within the title receive higher priority). Users can further refine results by employing filters, narrowing articles returned to specific publication types, journals, or article sections.</p>
<p>PubTator 3.0 is supported by an NLP pipeline, depicted in Fig. 2A. This pipeline, run weekly, first identifies articles newly added to PubMed and PMC-OA. Articles are then processed through three major steps: 1. named entity recognition, provided by the recently developed deeplearning transformer model AIONER (7), 2. identifier mapping, and 3. relation extraction, performed by BioREx (8) of 12 common types of relations (described in Supplementary Table 2).</p>
<p>In total, PubTator 3.0 contains over 1.6 billion entity annotations ( 4.6 million unique identifiers) and 33 million relations ( 8.8 million unique pairs). It provides enhanced entity recognition and normalization performance over its previous version, PubTator 2 (9), also known as PubTator Central (Fig. 2B and Supplementary Table 3). We show the relation extraction performance of PubTator 3.0 in Fig. 2C and its comparison results to the previous state-of-the-art systems (1012) on the BioCreative V Chemical-Disease Relation (13) corpus, finding that PubTator 3.0 provided substantially higher accuracy. Moreover, when evaluating a randomized sample of entity pair queries compared to PubMed and Google Scholar, PubTator 3.0 consistently returns a</p>
<p>greater number of articles with higher precision in the top 20 results (Fig. 2D and Supplementary Table 4).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. A. The PubTator 3.0 processing pipeline: AlONER (7) identifies six types of entities in PubMed abstracts and PMC-OA full-text articles. Entity annotations are associated with database identifiers by specialized mappers and BioREx (8) identifies relations between entities. Extracted data is stored in MongoDB and made searchable using Solr. B. Entity recognition performance for each entity type compared with PubTator2 (also known as PubTatorCentral) (13) on the BioRED corpus (14). C. Relation</p>
<p>extraction performance compared with SemRep (10) and notable previous best systems $(11,12)$ on the BioCreative V Chemical-Disease Relation (13) corpus. D. Comparison of information retrieval for PubTator 3.0, PubMed, and Google Scholar for entity pair queries, with respect to total article count and top-20 article precision.</p>
<h1>MATERIAL AND METHODS</h1>
<h2>Data Sources and Article Processing</h2>
<p>PubTator 3.0 downloads new articles weekly from the BioC PubMed API (https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PubMed/) and the BioC PMC API (https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/) in BioC-XML format (15). Local abbreviations are identified using Ab3P (16). Article text and extracted data are stored internally using MongoDB and indexed for search with Solr, ensuring robust and scalable accessibility unconstrained by external dependencies such as the NCBI eUtils API.</p>
<h2>Entity Recognition and Normalization / Linking</h2>
<p>PubTator 3.0 uses AIONER (7), a recently developed named entity recognition (NER) model, to recognize entities of six types: genes/proteins, chemicals, diseases, species, genetic variants, and cell lines. AIONER utilizes a flexible tagging scheme to integrate training data created separately into a single resource. These training datasets include NLM-Gene (17), NLM-Chem (18), NCBIDisease (19), BC5CDR (13), tmVar3 (20), Species-800 (21), BioID (22), and BioRED (14). This consolidation creates a larger training set, improving the model's ability to generalize to unseen data. Furthermore, it enables recognizing multiple entity types simultaneously, enhancing efficiency and simplifying the challenge of distinguishing boundaries between entities that reference others, such as the disorder "Alpha-1 antitrypsin deficiency" and the protein "Alpha-1 antitrypsin." We previously evaluated the performance of AIONER on 14 benchmark datasets (7), including the test sets for the aforementioned training sets. This evaluation demonstrated that AIONER's performance surpasses or matches previous state-of-the-art methods.</p>
<p>Entity mentions found by AIONER are normalized (linked) to a unique identifier in an appropriate entity database. Normalization is performed by a module designed for (or adapted to) each entity type, using the latest version. The recently-upgraded GNorm2 system (23) normalizes genes to NCBI Gene identifiers and species mentions to NCBI Taxonomy. tmVar3 (20), also recently upgraded, normalizes genetic variants; it uses dbSNP identifiers for variants listed in dbSNP and HGNV format otherwise. Chemicals are normalized by the NLM-Chem tagger (18) to MeSH identifiers (24). TaggerOne (25) normalizes diseases to MeSH and cell lines to Cellosaurus (26) using an improved normalization-only mode. These enhancements provide a significant overall improvement in entity normalization performance (Supplementary Table 2).</p>
<h1>Relation Extraction</h1>
<p>Relations for PubTator 3.0 are extracted by the unified relation extraction model BioREx (8), designed to simultaneously extract 12 types of relations across eight entity type pairs: chemicalchemical, chemical-disease, chemical-gene, chemical-variant, disease-gene, disease-variant, gene-gene, and variant-variant. Detailed definitions of these relation types and their corresponding entity pairs are presented in Supplementary Table 2. Deep-learning methods for relation extraction, such as BioREx, require ample training data. However, training data for relation extraction is fragmented into many datasets, often tailored to specific entity pairs. BioREx overcomes this limitation with a data-centric approach, reconciling discrepancies between disparate training datasets to construct a comprehensive, unified dataset.</p>
<p>We evaluated the relations extracted by BioREx using performance on manually annotated relation extraction datasets as well as a comparative analysis between BioREx and notable comparable systems. BioREx established a new performance benchmark on the BioRED corpus test set (14), elevating the performance from $74.4 \%$ (F-score) to $79.6 \%$, and demonstrating higher performance than alternative models such as transfer learning (TL), multi-task learning (MTL), and state-of-the-art models trained on isolated datasets (8). For PubTator 3.0, we replaced its deep learning module, PubMedBERT (27), with LinkBERT (28), further increasing the performance to $82.0 \%$. Furthermore, we conducted a comparative analysis between BioREx and SemRep (10), a widely used rule-based method for extracting diverse relations, the CD-REST (12)</p>
<p>system, and the previous state-of-the-art system (11), using the BioCreative V Chemical Disease Relation corpus test set (13). Our evaluation demonstrated that PubTator 3.0 provided substantially higher F-score than previous methods.</p>
<h1>Programmatic Access and Data Formats</h1>
<p>PubTator 3.0 offers programmatic access through its API and bulk download. The API (https://www.ncbi.nlm.nih.gov/research/pubtator3/) supports keyword, entity and relation search, and also supports exporting annotations in XML and JSON-based BioC (15) formats and tabdelimited free text. The PubTator 3.0 FTP site (https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3) provides bulk downloads of annotated articles and extraction summaries for entities and relations. Programmatic access supports more flexible query options; for example, the information need "what chemicals reduce expression of JAK1?" can be answered directly via API (e.g., https://www.ncbi.nlm.nih.gov/research/pubtator3-
api/relations?e1=@GENE_JAK1\&amp;type=negative_correlate\&amp;e2=Chemical) or by filtering the bulk relations file. Additionally, the PubTator 3.0 API supports annotation of user-defined free text.</p>
<h2>Case Study I: Entity Relation Queries</h2>
<p>We analyzed the retrieval quality of PubTator 3.0 by preparing a series of 12 entity pairs to serve as case studies for comparison between PubTator 3.0, PubMed, and Google Scholar. To provide an equal comparison, we filtered Google Scholar results for articles not in PubMed. To ensure that the number of results would remain low enough to allow filtering Google Scholar results for articles not in PubMed, we identified entity pairs first discussed together in the literature in 2022 or later. We then randomly selected two entity pairs of each of the following types: Disease/Gene, Chemical/Disease, Chemical/Gene, Chemical/Chemical, Gene/Gene and Disease/Variant. The comparison was performed with respect to a snapshot of the search results returned by all search engines on May 19, 2023. We manually evaluated the top 20 results for each system and each query; articles were judged to be relevant if they mentioned both entities in the query and supported a relationship between them.</p>
<p>Our analysis is summarized in Fig. 2D, and Supplementary Table 4 presents a detailed comparison of the quality of retrieved results between PubTator 3.0, PubMed, and Google Scholar. Our results demonstrate that PubTator 3.0 retrieves a greater number of articles than the comparison systems and its precision is higher for the top 20 results. For instance, PubTator 3.0 returned 346 articles for the query "GLPG0634 + Ulcerative Colitis," and manual review of the top 20 articles showed that all contained statements about an association between GLPG0634 and ulcerative colitis. In contrast, PubMed only returned a total of 18 articles, with only 12 mentioning an association. Moreover, when searching for "COVID19 + PON1," PubTator 3.0 returns 212 articles in PubMed, surpassing the 43 articles obtained from Google Scholar, only 29 of which are sourced from PubMed. These disparities can be attributed to several factors: 1. PubTator 3.0's search includes full texts available in PMC-OA, resulting in significantly broader coverage of articles, 2. Entity normalization improves recall, for example, by matching "paraoxonase 1" to "PON1," 3. PubTator 3.0 prioritizes articles containing relations between the query entities, 4. Pubtator 3.0 prioritizes articles where the entities appear nearby, rather than distant paragraphs. Across the 12 information retrieval case studies, PubTator 3.0 demonstrated an overall precision of $90.0 \%$ for the top 20 articles ( 216 out of 240), which is significantly higher than PubMed's precision of $81.6 \%$ ( 84 out of 103) and Google Scholar's precision of $48.5 \%$ ( 98 out of 202).</p>
<h1>Case Study II: Retrieval-Augmented Generation</h1>
<p>In the era of large language models (LLMs), PubTator 3.0 can also enhance their factual accuracy via retrieval augmented generation. Despite their strong language ability, LLMs are prone to generating incorrect assertions, sometimes known as hallucinations $(29,30)$. For example, when requested to cite sources for questions such as "which diseases can doxorubicin treat," GPT-4 frequently provides seemingly plausible but nonexistent references. Augmenting GPT-4 with PubTator 3.0 APIs can anchor the model's response to verifiable references via the extracted relations, significantly reducing hallucinations.</p>
<p>We assessed the citation accuracy of responses from three GPT-4 variations: PubTatoraugmented GPT-4, PubMed-augmented GPT-4 and standard GPT-4. We performed a qualitative</p>
<p>evaluation based on eight questions selected as follows. We identified entities mentioned in the PubMed query logs and randomly selected from entities searched both frequently and rarely. We then identified the common queries for each entity that request relational information and adapted one into a natural language question. Each question is therefore grounded on common information needs of real PubMed users. For example, the questions "What can be caused by tocilizumab?" and "What can be treated by doxorubicin?" are adapted from the user queries "tocilizumab side effects" and "doxorubicin treatment" respectively. Such questions typically require extracting information from multiple articles and an understanding of biomedical entities and relationship descriptions. Supplementary Table 5 lists the questions chosen.</p>
<p>We augmented the GPT-4 large language model (LLM) with PubTator 3.0 via the function calling mechanism of the OpenAI ChatCompletion API. This integration involved prompting GPT-4 with descriptions of three PubTator APIs: 1. Find Entity ID, which retrieves PubTator entity identifiers; 2. Find Related Entities, which identifies related entities based on an input entity and specified relations; and 3. Export Relevant Search Results, which returns PubMed article identifiers containing textual evidence for specific entity relationships. Our instructions prompted GPT-4 to decompose user questions into sub-questions addressable by these APIs, execute the function calls, and synthesize the responses into a coherent final answer. Our prompt promoted a summarized response by instructing GPT-4 to start its message with "Summary:" and requested the response include citations to the articles providing evidence. The PubMed augmentation experiments provided GPT-4 with access to PubMed database search via the National Center for Biotechnology Information (NCBI) E-utils APIs (31). We used Azure OpenAI Services (version 2023-07-01-preview) and GPT-4 (version 2023-06-13) and set the decoding temperature to zero to obtain deterministic outputs. The full prompts are provided in Supplementary Table 6.</p>
<p>PubTator-augmented GPT-4 generally processed the questions in three steps: 1. finding the standard entity identifiers, 2. finding its related entity identifiers, and 3. searching PubMed articles. For example, to answer "What drugs can treat breast cancer?", GPT-4 first found the PubTator entity identifier for breast cancer (@DISEASE_Breast_Cancer) using the Find Entity ID API. It then used the Find Related Entities API to identify entities related to</p>
<p>@DISEASE_Breast_Cancer through a "treat" relation. For demonstration purposes, we limited the maximum number of output entities to five. Finally, GPT-4 called the Export Relevant Search Results API for the PubMed article identifiers containing evidence for these relationships. The raw responses to each prompt for each method are provided in Supplementary Table 6.</p>
<p>We manually evaluated the accuracy of the citations in the responses by reviewing each PubMed article and verifying whether each PubMed article cited supported the stated relationship (e.g., Tamoxifen treating breast cancer). Supplementary Table 5 reports the proportion of the cited articles with valid supporting evidence for each method. GPT-4 frequently generated fabricated citations, widely known as the hallucination issue. While PubMed-augmented GPT-4 showed a higher proportion of accurate citations, some articles cited did not support the relation claims. This is likely because PubMed is based on keyword and Boolean search and does not support queries for specific relationships. Responses generated by PubTator-augmented GPT-4 demonstrated the highest level of citation accuracy, underscoring the potential of PubTator 3.0 as a high-quality knowledge source for addressing biomedical information needs through retrieval-augmented generation with LLMs such as GPT-4.</p>
<h1>DISCUSSION</h1>
<p>Previous versions of PubTator have fulfilled over one billion API requests since 2015, supporting a wide range of research applications. Numerous studies have harnessed PubTator annotations for disease-specific gene research, including efforts to prioritize candidate genes (32), determine gene-phenotype associations (33), and identify the genetic underpinnings of disease comorbidities (34). Several projects have used PubTator to create gene and genetic variant resources $(35,36)$ or to enrich disease knowledge graphs $(37,38)$. Moreover, PubTator has supported biocuration efforts $(39,40)$ and the creation of NLP benchmarks (41). With enhanced accuracy, PubTator 3.0 will better support these use cases.</p>
<p>Introducing relation annotations to PubTator 3.0 opens novel avenues for expanded use scenarios. With relations precomputed from the literature, complex research questions can often</p>
<p>be answered directly. Drug repurposing, for example, can be formulated as identifying chemicals which target specific genes. Conversely, determining the genetic targets of a chemical can be achieved by querying the same chemical/gene relations. Clinicians evaluating genetic variants, e.g. for rare diseases or personalized medicine, may explore the relationships between specific genetic variants and disease. Biologists, on the other hand, may utilize interactions between multiple genes to assemble complex molecular pathways.</p>
<p>There are several notable limitations for PubTator 3.0. Although it is capable of extracting relations from full-text articles, this feature is currently restricted to abstracts due to computational constraints. However, the system has been designed to support full-text relation extraction in a future enhancement. The current system only extracts 12 relation types, though these represent common uses. Finally, entity annotation and relation extraction are automated; though these systems exhibit high performance, their accuracy remains imperfect.</p>
<h1>CONCLUSION</h1>
<p>PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery. The PubTator 3.0 interface, API, and bulk file downloads are available at https://www.ncbi.nlm.nih.gov/research/pubtator3/.</p>
<h2>DATA AVAILABILITY</h2>
<p>Data is available through the online interface at https://www.ncbi.nlm.nih.gov/research/pubtator3/, through the API at https://www.ncbi.nlm.nih.gov/research/pubtator3/api or bulk FTP download at https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3/.</p>
<p>The source code for each component of PubTator 3.0 is openly accessible. The AIONER named entity recognizer is available at https://github.com/ncbi/AIONER. GNorm2, for gene name normalization, is available at https://github.com/ncbi/GNorm2. The tmVar3 variant name normalizer is available at https://github.com/ncbi/tmVar3. The NLM-Chem Tagger, for chemical name normalization, is available at https://ftp.ncbi.nlm.nih.gov/pub/lu/NLMChem. The TaggerOne system, for disease and cell line normalization, is available at https://www.ncbi.nlm.nih.gov/research/bionlp/Tools/taggerone. The BioREx relation extraction system is available at https://github.com/ncbi/BioREx. The code for customizing ChatGPT with the PubTator 3.0 API is available at https://github.com/ncbi-nlp/pubtator-gpt.</p>
<h1>FUNDING</h1>
<p>This research was supported by the Intramural Research Program of the National Library of Medicine (NLM), National Institutes of Health. Funding for open access charge: National Institutes of Health.</p>
<h2>CONFLICT OF INTEREST</h2>
<p>None declared.</p>
<h2>REFERENCES</h2>
<ol>
<li>Lindberg, D.A. and Humphreys, B.L. (2008) Rising expectations: access to biomedical information. Yearb. Med. Inform., 3, 165-172.</li>
<li>Jin, Q., Leaman, R. and Lu, Z. (2023) PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence. arXiv.</li>
<li>Rzhetsky, A., Seringhaus, M. and Gerstein, M. (2008) Seeking a new biology through text mining. Cell, 134, 9-13.</li>
<li>Mayers, M., Li, T.S., Queralt-Rosinach, N. and Su, A.I. (2019) Time-resolved evaluation of compound repositioning predictions on a text-mined knowledge network. BMC Bioinf., 20, 653.</li>
<li>Zhao, S., Su, C., Lu, Z. and Wang, F. (2021) Recent advances in biomedical literature mining. Brief Bioinform, 22.</li>
<li>
<p>Westergaard, D., Staerfeldt, H.H., Tonsberg, C., Jensen, L.J. and Brunak, S. (2018) A comprehensive and quantitative comparison of text-mining in 15 million full-text articles versus their corresponding abstracts. PLoS Comput. Biol., 14, e1005962.</p>
</li>
<li>
<p>Luo, L., Wei, C.-H., Lai, P.-T., Leaman, R., Chen, Q. and Lu, Z. (2023) AIONER: all-in-one schemebased biomedical named entity recognition using deep learning. Bioinformatics, 39.</p>
</li>
<li>Lai, P.T., Wei, C.H., Luo, L., Chen, Q. and Lu, Z. (2023) BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets. J. Biomed. Inf., 146, 104487.</li>
<li>Wei, C.-H., Allot, A., Leaman, R. and Lu, Z. (2019) PubTator central: automated concept annotation for biomedical full text articles. Nucleic Acids Res., 47, W587-W593.</li>
<li>Kilicoglu, H., Rosemblat, G., Fiszman, M. and Shin, D. (2020) Broad-coverage biomedical relation extraction with SemRep. BMC Bioinf., 21, 188.</li>
<li>Peng, Y., Wei, C.-H. and Lu, Z. (2016) Improving chemical disease relation extraction with rich features and weakly labeled data. J. Cheminf., 8, 1-12.</li>
<li>Xu, J., Wu, Y., Zhang, Y., Wang, J., Lee, H.J. and Xu, H. (2016) CD-REST: a system for extracting chemical-induced disease relation in literature. Database, 2016.</li>
<li>Li, J., Sun, Y., Johnson, R.J., Sciaky, D., Wei, C.-H., Leaman, R., Davis, A.P., Mattingly, C.J., Wiegers, T.C. and Lu, Z. (2016) BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database, 2016.</li>
<li>Luo, L., Lai, P.-T., Wei, C.-H., Arighi, C.N. and Lu, Z. (2022) BioRED: A Rich Biomedical Relation Extraction Dataset. Briefings Bioinf., 23, bbac282.</li>
<li>Comeau, D.C., Islamaj Doan, R., Ciccarese, P., Cohen, K.B., Krallinger, M., Leitner, F., Lu, Z., Peng, Y., Rinaldi, F. and Torii, M.J.D. (2013) BioC: a minimalist approach to interoperability for biomedical text processing. Database, 2013, bat064.</li>
<li>Sohn, S., Comeau, D.C., Kim, W. and Wilbur, W.J. (2008) Abbreviation definition identification based on automatic precision estimates. BMC Bioinf., 9, 1-10.</li>
<li>Islamaj, R., Wei, C.-H., Cissel, D., Miliaras, N., Printseva, O., Rodionov, O., Sekiya, K., Ward, J. and Lu, Z.J.J.o.b.i. (2021) NLM-Gene, a richly annotated gold standard dataset for gene entities that addresses ambiguity and multi-species gene recognition. J. Biomed. Inf., 118, 103779.</li>
<li>Islamaj, R., Leaman, R., Kim, S., Kwon, D., Wei, C.-H., Comeau, D.C., Peng, Y., Cissel, D., Coss, C. and Fisher, C. (2021) NLM-Chem, a new resource for chemical entity recognition in PubMed full text literature. Scientific Data, 8, 91.</li>
<li>Doan, R.I., Leaman, R. and Lu, Z. (2014) NCBI disease corpus: a resource for disease name recognition and concept normalization. J. Biomed. Inf., 47, 1-10.</li>
<li>Wei, C.-H., Allot, A., Riehle, K., Milosavljevic, A. and Lu, Z. (2022) tmVar 3.0: an improved variant concept recognition and normalization tool. Bioinformatics, 38, 4449-4451.</li>
<li>Pafilis, E., Frankild, S.P., Fanini, L., Faulwetter, S., Pavloudi, C., Vasileiadou, A., Arvanitidis, C. and Jensen, L.J. (2013) The SPECIES and ORGANISMS resources for fast and accurate identification of taxonomic names in text. PLoS One, 8, e65390.</li>
<li>Arighi, C., Hirschman, L., Lemberger, T., Bayer, S., Liechti, R., Comeau, D. and Wu, C. (2017), Proc. BioCreative Workshop, Vol. 482, pp. 376.</li>
<li>Wei, C.H., Luo, L., Islamaj, R., Lai, P.T. and Lu, Z. (2023) GNorm2: an improved gene name recognition and normalization system. Bioinformatics, 39.</li>
<li>Lipscomb, C.E. (2000) Medical subject headings (MeSH). Bull. Med. Libr. Assoc., 88, 265.</li>
<li>Leaman, R. and Lu, Z. (2016) TaggerOne: joint named entity recognition and normalization with semi-Markov Models. Bioinformatics, 32, 2839-2846.</li>
<li>Bairoch, A. (2018) The Cellosaurus, a Cell-Line Knowledge Resource. J. Biomol Tech., 29, 25-38.</li>
<li>Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J. and Poon, H. (2021) Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare, 3, 1-23.</li>
<li>
<p>Yasunaga, M., Leskovec, J. and Liang, P. (2022), Association for Computational Linguistics, pp. 8003-8016.</p>
</li>
<li>
<p>Jin, Q., Leaman, R. and Lu, Z. (2023) Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature? J. Am. Soc. Nephrol., 34, 1302-1304.</p>
</li>
<li>Tian, S., Jin, Q., Yeganova, L., Lai, P.T., Zhu, Q., Chen, X., Yang, Y., Chen, Q., Kim, W., Comeau, D.C. et al. (2023) Opportunities and challenges for ChatGPT and large language models in biomedicine and health. Brief Bioinform, 25.</li>
<li>National Center for Biotechnology Information (US). (2010) Entrez Programming Utilities Help. National Center for Biotechnology Information (US), Bethesda (MD).</li>
<li>Lieberwirth, J.K., Buttner, B., Klockner, C., Platzer, K., Popp, B. and Abou Jamra, R. (2022) AutoCaSc: Prioritizing candidate genes for neurodevelopmental disorders. Hum. Mutat., 43, 1795-1807.</li>
<li>Buch, A.M., Vertes, P.E., Seidlitz, J., Kim, S.H., Grosenick, L. and Liston, C. (2023) Molecular and network-level mechanisms explaining individual differences in autism spectrum disorder. Nat. Neurosci., 26, 650-663.</li>
<li>Pinto, B.G.G., Oliveira, A.E.R., Singh, Y., Jimenez, L., Goncalves, A.N.A., Ogava, R.L.T., Creighton, R., Schatzmann Peron, J.P. and Nakaya, H.I. (2020) ACE2 Expression Is Increased in the Lungs of Patients With Comorbidities Associated With Severe COVID-19. J. Infect. Dis., 222, 556-563.</li>
<li>Mitsuhashi, N., Toyo-Oka, L., Katayama, T., Kawashima, M., Kawashima, S., Miyazaki, K. and Takagi, T. (2022) TogoVar: A comprehensive Japanese genetic variation database. Hum. Genome Var., 9, 44.</li>
<li>Jiang, J., Yuan, J., Hu, Z., Zhang, Y., Zhang, T., Xu, M., Long, M., Fan, Y., Tanyi, J.L., Montone, K.T. et al. (2022) Systematic illumination of druggable genes in cancer genomes. Cell Rep., 38, 110400 .</li>
<li>Pu, Y., Beck, D. and Verspoor, K. (2023) Graph embedding-based link prediction for literaturebased discovery in Alzheimer's Disease. J. Biomed. Inf., 145, 104464.</li>
<li>Chen, C., Ross, K.E., Gavali, S., Cowart, J.E. and Wu, C.H. (2021) COVID-19 Knowledge Graph from semantic integration of biomedical literature and databases. Bioinformatics, 37, 4597-4598.</li>
<li>Lou, P., Jimeno Yepes, A., Zhang, Z., Zheng, Q., Zhang, X. and Li, C. (2020) BioNorm: deep learning-based event normalization for the curation of reaction databases. Bioinformatics, 36, 611-620.</li>
<li>Percha, B. and Altman, R.B. (2018) A global network of biomedical relationships derived from text. Bioinformatics, 34, 2614-2624.</li>
<li>Legrand, J., Gogdemir, R., Bousquet, C., Dalleau, K., Devignes, M.-D., Digan, W., Lee, C.-J., Ndiaye, N.-C., Petitpain, N. and Ringot, P. (2020) PGxCorpus, a manually annotated corpus for pharmacogenomics. Scientific Data, 7, 3.</li>
<li>Luo, L., Lai, P.-T., Wei, C.-H., Arighi, C.N. and Lu, Z. (2022) BioRED: A Rich Biomedical Relation Extraction Dataset. Briefings in Bioinformatics.</li>
<li>Wei, C.-H., Kao, H.-Y. and Lu, Z. (2015) GNormPlus: an integrative approach for tagging genes, gene families, and protein domains. BioMed research international, 2015, 918710.</li>
<li>Wei, C.-H., Luo, L., Islamaj, R., Lai, P.-T. and Lu, Z. (2023) GNorm2: an improved gene name recognition and normalization system. Bioinformatics, in press.</li>
<li>Wei, C.-H., Kao, H.-Y. and Lu, Z. (2012) SR4GN: a species recognition software tool for gene normalization. PloS one, 7, e38460.</li>
<li>Wei, C.-H., Phan, L., Feltz, J., Maiti, R., Hefferon, T. and Lu, Z. (2018) tmVar 2.0: integrating genomic variant information from literature with dbSNP and ClinVar for precision medicine. Bioinformatics, 34, 80-87.</li>
</ol>
<p>Supplementary Table 1. Feature comparison between PubTator 3.0 and its previous version, PubTator 2 (also known as PubTator Central).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">PubTator 2</th>
<th style="text-align: left;">PubTator 3.0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entity annotations</td>
<td style="text-align: left;">Genes, diseases, chemicals, <br> genetic variants, species, <br> and cell lines in abstracts <br> and full text</td>
<td style="text-align: left;">Same types and scope; higher accuracy</td>
</tr>
<tr>
<td style="text-align: left;">Relation annotations</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">12 relation types across eight entity type pairs; <br> scope: abstracts only</td>
</tr>
<tr>
<td style="text-align: left;">Search scope</td>
<td style="text-align: left;">Abstracts only, via NCBI <br> eUtils</td>
<td style="text-align: left;">Unified search in abstracts \&amp; full text via <br> Apache Solr, no external dependencies</td>
</tr>
<tr>
<td style="text-align: left;">Query types</td>
<td style="text-align: left;">Keyword, Boolean</td>
<td style="text-align: left;">Also: semantic, relation</td>
</tr>
<tr>
<td style="text-align: left;">Search support</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Semantic autocomplete, facet filters (section, <br> journal, article type)</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval relevance</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Results prioritized by entity relationships, co- <br> occurrence \&amp; matching sections; highlighted <br> snippets (explains relevance); temporal <br> visualization</td>
</tr>
<tr>
<td style="text-align: left;">Literature management</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">User-defined collections</td>
</tr>
<tr>
<td style="text-align: left;">API</td>
<td style="text-align: left;">Retrieve articles and <br> annotations by PMID</td>
<td style="text-align: left;">Also: query relevant articles (semantic, <br> relation, keyword, Boolean), and query related <br> entities</td>
</tr>
<tr>
<td style="text-align: left;">Advanced natural- <br> language search</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Retrieval-augmented generation with GPT-4 <br> large language model</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Supplementary Figure 1. PubTator 3.0 article display page. (1) List of entities and relations identified by PubTator 3.0, providing a quick content overview. (2) Extracted entities highlighted in article text. (3) Display highlighting for query entities or all entities; display article abstract or full text. (4) Add article to custom collection for convenient access later.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">PubTator 3.0 Relations</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Entity types</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ASSOCIATE</td>
<td style="text-align: left;">Complex or unclear relationships</td>
<td style="text-align: left;">Chemical / Disease <br> Chemical / Gene <br> Chemical / Variant <br> Disease / Gene <br> Disease / Variant <br> Variant / Variant</td>
</tr>
<tr>
<td style="text-align: left;">CAUSE</td>
<td style="text-align: left;">Triggering a disease by a specific agent</td>
<td style="text-align: left;">Chemical / Disease <br> Variant / Disease</td>
</tr>
<tr>
<td style="text-align: left;">COMPARE</td>
<td style="text-align: left;">Comparing the effects of two chemicals or drugs</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">COTREAT</td>
<td style="text-align: left;">Simultaneous administration of multiple drugs</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">DRUG_INTERACT</td>
<td style="text-align: left;">Pharmacodynamic interactions between two <br> chemicals</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">INHIBIT</td>
<td style="text-align: left;">Reduction in amount or degree of one entity by <br> another</td>
<td style="text-align: left;">Chemical / Variant <br> Gene / Disease</td>
</tr>
<tr>
<td style="text-align: left;">INTERACT</td>
<td style="text-align: left;">Physical interactions, such as protein-binding</td>
<td style="text-align: left;">Chemical / Gene <br> Chemical / Variant <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">NEGATIVE_CORRELATE</td>
<td style="text-align: left;">Increases in the amount or degree of one entity <br> decreases the amount or degree of the other entity</td>
<td style="text-align: left;">Chemical / Gene <br> Chemical / Variant <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">POSITIVE_CORRELATE</td>
<td style="text-align: left;">The amount or degree of two entities increase or <br> decrease together</td>
<td style="text-align: left;">Chemical / Chemical <br> Chemical / Gene <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">PREVENT</td>
<td style="text-align: left;">Prevention of a disease by a genetic variant</td>
<td style="text-align: left;">Variant / Disease</td>
</tr>
<tr>
<td style="text-align: left;">STIMULATE</td>
<td style="text-align: left;">Increase in amount or degree of one entity by <br> another</td>
<td style="text-align: left;">Chemical / Variant <br> Gene / Disease</td>
</tr>
<tr>
<td style="text-align: left;">TREAT</td>
<td style="text-align: left;">Treatment of a disease using a chemical or drug</td>
<td style="text-align: left;">Chemical / Disease</td>
</tr>
</tbody>
</table>
<h1>Supplementary Table 3. Normalization performance enhancements from PubTator Central to PubTator 3.0. Measurements reflect document-level normalization performance on the BioRED test set (42).</h1>
<table>
<thead>
<tr>
<th></th>
<th>PubTator Central (2.0)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>PubTator 3.0</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>NER/Norm</td>
<td>Precision</td>
<td>Recall</td>
<td>F-score</td>
<td>NER</td>
<td>Norm</td>
<td>Precision</td>
<td>Recall</td>
<td>F-score</td>
</tr>
<tr>
<td>Gene</td>
<td>GNormPlus (43)</td>
<td>86.92%</td>
<td>73.00%</td>
<td>79.35%</td>
<td>AIONER (7)</td>
<td>GNorm2 (44)</td>
<td>90.60%</td>
<td>79.41%</td>
<td>84.63%</td>
</tr>
<tr>
<td>Disease</td>
<td>TaggerOne (25)</td>
<td>77.13%</td>
<td>76.45%</td>
<td>76.79%</td>
<td></td>
<td>TaggerOne (25)</td>
<td>75.33%</td>
<td>83.43%</td>
<td>79.17%</td>
</tr>
<tr>
<td>Chemical</td>
<td>TaggerOne (25)</td>
<td>73.42%</td>
<td>78.38%</td>
<td>75.82%</td>
<td></td>
<td>NLM-Chem (18)</td>
<td>83.26%</td>
<td>80.63%</td>
<td>81.92%</td>
</tr>
<tr>
<td>Species</td>
<td>SR4GN (45)</td>
<td>94.69%</td>
<td>94.69%</td>
<td>94.69%</td>
<td></td>
<td>GNorm2 (44)</td>
<td>93.97%</td>
<td>96.46%</td>
<td>95.20%</td>
</tr>
<tr>
<td>CellLine</td>
<td>TaggerOne (25)</td>
<td>42.42%</td>
<td>63.64%</td>
<td>50.91%</td>
<td></td>
<td>TaggerOne (25)</td>
<td>76.00%</td>
<td>86.36%</td>
<td>80.85%</td>
</tr>
<tr>
<td>Variant</td>
<td>tmVar2 (46)</td>
<td>94.92%</td>
<td>84.85%</td>
<td>89.60%</td>
<td></td>
<td>tmVar3 (20)</td>
<td>98.48%</td>
<td>98.48%</td>
<td>98.48%</td>
</tr>
<tr>
<td>Micro-average</td>
<td></td>
<td>77.30%</td>
<td>77.49%</td>
<td>77.40%</td>
<td></td>
<td></td>
<td>84.04%</td>
<td>83.55%</td>
<td>83.80%</td>
</tr>
<tr>
<td>Macro-average</td>
<td></td>
<td>78.25%</td>
<td>78.50%</td>
<td>77.86%</td>
<td></td>
<td></td>
<td>86.27%</td>
<td>87.46%</td>
<td>86.71%</td>
</tr>
</tbody>
</table>
<p>Supplementary Table 4. Comparison of PubTator 3.0, PubMed, and Google Scholar search results for various recently discussed relation pairs. D: Disease, G: Gene, C: Chemical, and V: Variant. The '#' column lists the number of results; for Google Scholar the number in parentheses indicates the number of articles that appear in PubMed. The 'Top 20' column indicates the number of articles in the top 20 results which discuss a relation between the specified entities; some queries return fewer than 20 articles.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entity <br> pair</th>
<th style="text-align: center;">Entities</th>
<th style="text-align: center;">PubTator 3.0</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PubMed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Google Scholar</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Top20</td>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Top20</td>
<td style="text-align: center;"># (in <br> PubMed)</td>
<td style="text-align: center;">Top20</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;$ D,G&gt;</td>
<td style="text-align: center;">COVID19 + PON1</td>
<td style="text-align: center;">212</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$9 / 11$</td>
<td style="text-align: center;">43 (29)</td>
<td style="text-align: center;">$10 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coronary Artery <br> Disease + SESN2</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">$17 / 20$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$3 / 3$</td>
<td style="text-align: center;">151 (104)</td>
<td style="text-align: center;">$14 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{D}&gt;$</td>
<td style="text-align: center;">GLPG0634 + Ulcerative <br> Colitis</td>
<td style="text-align: center;">346</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$12 / 18$</td>
<td style="text-align: center;">362 (281)</td>
<td style="text-align: center;">$8 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">brolucizumab <br> Glycogen <br> Storage <br> Disease Type II</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0 / 0$</td>
<td style="text-align: center;">1 (1)</td>
<td style="text-align: center;">$0 / 0$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{G}&gt;$</td>
<td style="text-align: center;">Gallium-68 + FAP alpha</td>
<td style="text-align: center;">261</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">11 (4)</td>
<td style="text-align: center;">$9 / 11$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lipopolysaccharides + <br> PVT1</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">$19 / 20$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$18 / 20$</td>
<td style="text-align: center;">128 (95)</td>
<td style="text-align: center;">$6 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{C}&gt;$</td>
<td style="text-align: center;">N -dimethylnitrosamine <br> + Metformin</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">11 (8)</td>
<td style="text-align: center;">$1 / 11$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2'-fucosyllactose <br> Volatile fatty acids</td>
<td style="text-align: center;">284</td>
<td style="text-align: center;">$17 / 20$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$3 / 6$</td>
<td style="text-align: center;">71 (40)</td>
<td style="text-align: center;">$6 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{G}, \mathrm{G}&gt;$</td>
<td style="text-align: center;">interleukin 17 + cell <br> division cycle 42</td>
<td style="text-align: center;">599</td>
<td style="text-align: center;">$12 / 20$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">85 (39)</td>
<td style="text-align: center;">$1 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HAVCR2 + TOX</td>
<td style="text-align: center;">615</td>
<td style="text-align: center;">$11 / 20$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$1 / 4$</td>
<td style="text-align: center;">701 (479)</td>
<td style="text-align: center;">$3 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{D}, \mathrm{V}&gt;$</td>
<td style="text-align: center;">COVID19 + rs12329760</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">$19 / 20$</td>
<td style="text-align: center;">87 (63)</td>
<td style="text-align: center;">20/20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COVID19 + rs4646994</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$15 / 16$</td>
<td style="text-align: center;">30 (24)</td>
<td style="text-align: center;">20/20</td>
</tr>
</tbody>
</table>
<p>Supplementary Table 5. Article citation precision for all 8 queries tested using GPT-4 only, GPT-4 augmented with PubMed, and GPT-4 augmented with PubTator 3.0. Results are summarized as "number of articles correctly referenced / total number of articles referenced." Full responses provided in Supplemental Data.</p>
<table>
<thead>
<tr>
<th>Questions</th>
<th>GPT4 <br> Only</th>
<th>GPT4 with PubMed <br> Augmentation</th>
<th>GPT4 with PubTator <br> Augmentation</th>
</tr>
</thead>
<tbody>
<tr>
<td>What can be caused by tocilizumab? For <br> each disease in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$0 / 1$</td>
<td>$1 / 5$</td>
<td>$49 / 50$</td>
</tr>
<tr>
<td>Can you tell me what the causes of memory <br> deficits are? For each cause in your answer, <br> please cite and summarized the article <br> PMIDs that contain the evidence.</td>
<td>$0 / 5$</td>
<td>$4 / 5$</td>
<td>$15 / 15$</td>
</tr>
<tr>
<td>In what situations can cocaine be used? For <br> each situation in your answer, please cite <br> and summarize the article PMIDs that <br> contain the evidence.</td>
<td>$0 / 3$</td>
<td>$1 / 3$</td>
<td>$20 / 25$</td>
</tr>
<tr>
<td>What can be treated by doxorubicin? For <br> each disease in your answer, please cite and <br> summarize the article PMIDs that contain <br> the evidence.</td>
<td>$0 / 7$</td>
<td>$3 / 5$</td>
<td>$45 / 45$</td>
</tr>
<tr>
<td>Are there any genes that interact with <br> cocaine? For each drug in your answer, <br> please cite the article PMIDs that contain <br> the evidence.</td>
<td>$0 / 5$</td>
<td>$5 / 5$</td>
<td>$16 / 17$</td>
</tr>
<tr>
<td>What drugs can treat breast cancer? For <br> each drug in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$0 / 6$</td>
<td>$3 / 4$</td>
<td>$45 / 45$</td>
</tr>
<tr>
<td>What drugs can treat Scleroderma? For each <br> drug in your answer, please cite the article <br> PMIDs that contain the evidence.</td>
<td>$0 / 6$</td>
<td>$1 / 2$</td>
<td>$50 / 50$</td>
</tr>
<tr>
<td>What can be treated by finasteride? For <br> each disease in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$2 / 3$</td>
<td>$4 / 5$</td>
<td>$39 / 45$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Joint Authors&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-31 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-31</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-31</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-157.html">theory-157</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-393.html">theory-393</a></p>
                <p><strong>Overall Support or Contradict:</strong> support</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The new evidence strongly supports the core theory claims with extensive quantified examples of proxy-to-ground-truth gaps (BigCloneBench 93% FP rate, AOI 10+ pp slip gaps, VLA Brier MAE >1.5, DATADECIDE 20% decision error, LLM uncertainty 20-45% AUROC gaps). Evidence confirms gaps vary with domain characteristics, novelty, and proxy quality, and shows gap-reduction methods provide partial but incomplete solutions. No evidence fundamentally contradicts the theory; closed-loop systems and other nuances suggest refinements rather than contradictions.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>BigCloneBench semantic clone detection demonstrates extreme proxy-to-ground-truth gap: ML models achieve 99.8% precision and 93.7% F1 on BCB labels (proxy), but only 6.7% precision and 6.3% F1 when validated against strict human functional-similarity judgments (ground truth). The 93.35% false-positive rate in WT3/T4 labels shows proxy optimization producing highly misleading performance metrics, with models learning dataset artifacts rather than true semantic similarity. This exemplifies the theory's prediction that systems optimizing proxy metrics systematically overestimate success. <a href="../results/extraction-result-2221.html#e2221.0" class="evidence-link">[e2221.0]</a> <a href="../results/extraction-result-2221.html#e2221.1" class="evidence-link">[e2221.1]</a> <a href="../results/extraction-result-2221.html#e2221.3" class="evidence-link">[e2221.3]</a> </li>
    <li>Industrial AOI quality inspection demonstrates large proxy-to-ground-truth gaps with standard ML metrics: models achieving 99.3% accuracy have 7.8% slip rates (vs 1% target), and 98.7% accuracy corresponds to 11.3% slip rate. Even requirement-aware metrics that meet targets on test data (RFC2: slip ~1.0%, VR ~89.7%) fail immediately in chronological evaluation slices. This supports the theory's prediction that proxy-to-ground-truth gaps persist even with improved proxies and that temporal/distributional shifts amplify gaps. <a href="../results/extraction-result-2222.html#e2222.0" class="evidence-link">[e2222.0]</a> <a href="../results/extraction-result-2222.html#e2222.1" class="evidence-link">[e2222.1]</a> </li>
    <li>DATADECIDE systematic evaluation quantifies proxy-to-ground-truth gaps in dataset selection: single-scale ranking achieves ~80% decision accuracy predicting 1B model performance from 150M experiments, leaving 20% error. Best scaling-law variants show 5.6-6.5% relative prediction error. Continuous likelihood proxies enable >80% predictability on some tasks at 0.01% of target compute but fail on others (math benchmarks), demonstrating the theory's prediction that proxy quality depends on domain amenability to computational modeling. <a href="../results/extraction-result-2223.html#e2223.0" class="evidence-link">[e2223.0]</a> <a href="../results/extraction-result-2223.html#e2223.1" class="evidence-link">[e2223.1]</a> <a href="../results/extraction-result-2223.html#e2223.2" class="evidence-link">[e2223.2]</a> <a href="../results/extraction-result-2223.html#e2223.3" class="evidence-link">[e2223.3]</a> </li>
    <li>Vision-language-action models show severe proxy-to-ground-truth gaps under domain shift: zero-shot Procgen evaluation yields Brier MAE >1.5-1.7 (near maximum of 2), macro recall 6-12%, and high invalid output rates (>80% for some models/datasets). Gap increases with image complexity (Shannon entropy correlation -0.409 for GPT-4o) and for sparse/timed special actions. This strongly supports the theory's prediction that gaps increase with novelty and extrapolation distance from training distributions. <a href="../results/extraction-result-2225.html#e2225.0" class="evidence-link">[e2225.0]</a> <a href="../results/extraction-result-2225.html#e2225.1" class="evidence-link">[e2225.1]</a> <a href="../results/extraction-result-2225.html#e2225.2" class="evidence-link">[e2225.2]</a> <a href="../results/extraction-result-2225.html#e2225.3" class="evidence-link">[e2225.3]</a> <a href="../results/extraction-result-2225.html#e2225.4" class="evidence-link">[e2225.4]</a> </li>
    <li>Hemodynamics surrogate modeling demonstrates proxy-to-proxy cascade without experimental ground truth: ML models (DeepONet, DeepONet-SwinT) predict CFD fields with validation MNAE_u ~0.05-0.10, but CFD itself is a proxy for in-vivo hemodynamics with no experimental (PC-MRI, 4D flow) validation performed. CFD assumptions (steady-state, Newtonian, rigid walls) and boundary condition uncertainties create unquantified gaps to physiological reality. This exemplifies the theory's prediction about validation cascades and the economic incentive to defer ground-truth validation. <a href="../results/extraction-result-2224.html#e2224.0" class="evidence-link">[e2224.0]</a> <a href="../results/extraction-result-2224.html#e2224.1" class="evidence-link">[e2224.1]</a> <a href="../results/extraction-result-2224.html#e2224.2" class="evidence-link">[e2224.2]</a> <a href="../results/extraction-result-2224.html#e2224.3" class="evidence-link">[e2224.3]</a> </li>
    <li>LLM uncertainty estimation shows substantial proxy-to-ground-truth gaps: baseline log-likelihood achieves only 55.41% AUROC on MATH500, while improved methods (TokUR) reach 80.64% AUROC - still leaving ~20% error in correctness prediction. Self-evaluation methods (P(True), LLM-Check, INSIDE) perform poorly (44-56% AUROC), and external methods (SE, SAR) also underperform. This supports the theory's prediction that even sophisticated uncertainty quantification cannot eliminate proxy-to-ground-truth gaps. <a href="../results/extraction-result-2228.html#e2228.0" class="evidence-link">[e2228.0]</a> <a href="../results/extraction-result-2228.html#e2228.1" class="evidence-link">[e2228.1]</a> <a href="../results/extraction-result-2228.html#e2228.2" class="evidence-link">[e2228.2]</a> <a href="../results/extraction-result-2228.html#e2228.3" class="evidence-link">[e2228.3]</a> <a href="../results/extraction-result-2228.html#e2228.4" class="evidence-link">[e2228.4]</a> <a href="../results/extraction-result-2228.html#e2228.5" class="evidence-link">[e2228.5]</a> <a href="../results/extraction-result-2228.html#e2228.6" class="evidence-link">[e2228.6]</a> <a href="../results/extraction-result-2228.html#e2228.7" class="evidence-link">[e2228.7]</a> <a href="../results/extraction-result-2228.html#e2228.8" class="evidence-link">[e2228.8]</a> <a href="../results/extraction-result-2228.html#e2228.9" class="evidence-link">[e2228.9]</a> </li>
    <li>Sports video action detection shows quantified proxy-to-ground-truth gap and partial reduction: TAAD proxy predictions achieve 43.56% precision and 66.97% recall vs ground-truth annotations. Adding denoising sequence transduction (DST) with game-state context improves to 78.77% precision and 75.80% recall (+35.21pp precision, +8.83pp recall). This demonstrates the theory's prediction that multifidelity approaches can reduce but not eliminate gaps - substantial error remains even after sophisticated denoising. <a href="../results/extraction-result-2216.html#e2216.0" class="evidence-link">[e2216.0]</a> </li>
    <li>Causal structure learning (DAGSLAM) shows proxy improvement with appropriate loss functions: using mixed-type loss improves directed F1 from ~0.71 to 0.82 (50% categorical nodes) vs treating all as continuous. However, evaluation is simulation-only with no experimental causal validation, and performance degrades with increased categorical proportion (F1 drops from 0.97 to 0.82 at 50% categorical), dense graphs, and small samples. This supports the theory's prediction that proxy quality depends on domain characteristics and that gaps persist even with improved proxies. <a href="../results/extraction-result-2227.html#e2227.0" class="evidence-link">[e2227.0]</a> <a href="../results/extraction-result-2227.html#e2227.1" class="evidence-link">[e2227.1]</a> <a href="../results/extraction-result-2227.html#e2227.2" class="evidence-link">[e2227.2]</a> <a href="../results/extraction-result-2227.html#e2227.3" class="evidence-link">[e2227.3]</a> <a href="../results/extraction-result-2227.html#e2227.4" class="evidence-link">[e2227.4]</a> <a href="../results/extraction-result-2227.html#e2227.5" class="evidence-link">[e2227.5]</a> <a href="../results/extraction-result-2227.html#e2227.6" class="evidence-link">[e2227.6]</a> </li>
    <li>Autonomous agent systems show proxy-to-ground-truth gaps in biological discovery: BioDiscoveryAgent reports +21% improvement vs Bayesian optimization baselines but lacks explicit proxy-ground-truth calibration metrics. DrugAgent achieves PAMPA F1 ~0.92 on held-out assay labels but no prospective experimental validation. Virtual Lab produces >90% expression rate but only 2 of 92 candidates show improved binding. These systems demonstrate the theory's prediction about accumulation of unvalidated discoveries and the economic incentive to defer ground-truth validation. <a href="../results/extraction-result-2218.html#e2218.0" class="evidence-link">[e2218.0]</a> <a href="../results/extraction-result-2218.html#e2218.2" class="evidence-link">[e2218.2]</a> <a href="../results/extraction-result-2218.html#e2218.4" class="evidence-link">[e2218.4]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>A-Lab autonomous materials synthesis achieves 71% experimental success rate (41 novel compounds from 58 targets) using closed-loop robotic execution, demonstrating effective proxy-to-ground-truth translation in automated experimentation. However, explicit quantitative proxy-vs-ground calibration metrics are not reported, and the system uses multiple instrument readouts whose correlation and failure modes are not analyzed. The 29% failure rate still represents a substantial proxy-to-ground-truth gap, supporting the theory while showing closed-loop systems can achieve better translation than batch systems. <a href="../results/extraction-result-2218.html#e2218.3" class="evidence-link">[e2218.3]</a> </li>
    <li>Multi-objective Bayesian optimization for SPM (MOBO-SPM) uses multiple reward proxies (height-difference, tip-sample distance, phase) with GP uncertainty quantification and achieves qualitatively good scan quality. However, it lacks explicit numeric proxy-to-ground-truth agreement statistics (no R², MAE reported). Similarity reward shows narrow dynamic range and clusters near -1, demonstrating proxy-quality dependence on metric design. Phase reward contributes from only ~3% of pixels, reducing its influence. This partially supports the theory's predictions about proxy quality and multiple-proxy systems. <a href="../results/extraction-result-2217.html#e2217.0" class="evidence-link">[e2217.0]</a> <a href="../results/extraction-result-2217.html#e2217.1" class="evidence-link">[e2217.1]</a> <a href="../results/extraction-result-2217.html#e2217.2" class="evidence-link">[e2217.2]</a> </li>
    <li>Clinical prediction models (neuropathic foot ulcer ML pipeline) achieve high cross-validation metrics (MLP: AUC 0.901, accuracy 0.8751) but lack independent external validation or prospective clinical validation, leaving proxy-to-ground-truth gap unquantified. SMOTE class balancing increases minority samples from 57 to 113 but synthetic-data fidelity is not validated. SHAP feature importance shows qualitative agreement with statistically significant covariates but no quantitative calibration is provided. This supports the theory's prediction about deferred ground-truth validation but lacks the quantified gaps needed for strong support. <a href="../results/extraction-result-2220.html#e2220.0" class="evidence-link">[e2220.0]</a> <a href="../results/extraction-result-2220.html#e2220.1" class="evidence-link">[e2220.1]</a> <a href="../results/extraction-result-2220.html#e2220.2" class="evidence-link">[e2220.2]</a> </li>
    <li>Multi-branch CNN with attention (MBC-ATT) for EEG/fNIRS fusion achieves 98.13% accuracy on n-back and 98.61% on WG tasks, with ablation showing 6.55pp and 2.07pp drops without cross-modal attention. However, validation is against behavioral task labels only (no independent physiological ground truth), and generalization to small-sample patient datasets and cross-subject transfer is untested. This partially supports the theory by showing high proxy performance without independent ground-truth validation, but the lack of failure cases limits the evidence strength. <a href="../results/extraction-result-2219.html#e2219.0" class="evidence-link">[e2219.0]</a> <a href="../results/extraction-result-2219.html#e2219.1" class="evidence-link">[e2219.1]</a> <a href="../results/extraction-result-2219.html#e2219.2" class="evidence-link">[e2219.2]</a> </li>
    <li>Product attribute value identification (MVP-RAG) shows quantified improvement over baselines: +26.3pp F1 vs Qwen2.5(Product-RAG) and +3.3pp F1 vs TACLR, achieving 89.5% F1 overall. When true attribute value appears in retrieved candidates, generation F1 reaches 92.6% vs 86.3% when absent (6.3pp gap), demonstrating cascade error propagation. However, evaluation is against human-reviewed labels from seller feedback pipeline (itself a proxy for true product attributes), not independent experimental validation. The 10.5% error rate and sensitivity to retrieval quality support the theory's predictions about cascading errors. <a href="../results/extraction-result-2226.html#e2226.0" class="evidence-link">[e2226.0]</a> <a href="../results/extraction-result-2226.html#e2226.1" class="evidence-link">[e2226.1]</a> <a href="../results/extraction-result-2226.html#e2226.2" class="evidence-link">[e2226.2]</a> </li>
    <li>AgentCompass evaluation framework shows moderate proxy-to-ground-truth correlation: Pearson ρ = 0.430 (GAIA) and 0.408 (SWE Bench) between system quality scores and human scores. Localization accuracy 0.657 (GAIA) but only 0.250 (SWE Bench), with categorization F1 0.309 and 0.232 respectively. System identifies errors outside benchmark taxonomy (Safety & Security, Reflection Gaps), showing proxy-to-ground-truth mismatch increases when discovering novel error classes. This partially supports the theory's prediction about gaps increasing with novelty, though the domain (agentic AI evaluation) is emerging rather than mature. <a href="../results/extraction-result-2215.html#e2215.0" class="evidence-link">[e2215.0]</a> </li>
    <li>SAMPLE self-driving protein engineering pipeline discovers GH1 hydrolase variants with ≥12°C higher stability than starting sequences through closed-loop design, build, and test cycles. However, no explicit proxy-ground-truth calibration numbers are provided, and the system uses computational and intermediate robotic assay predictions whose accuracy is not quantified. This partially supports the theory by showing successful experimental validation in a closed-loop system, but lacks the detailed proxy-performance metrics needed to assess gap magnitude. <a href="../results/extraction-result-2218.html#e2218.1" class="evidence-link">[e2218.1]</a> </li>
    <li>Biomni general-purpose biomedical agent orchestrates 150+ tools and 59 databases to produce ranked candidates with traceable evidence, but no uniform ground-truth outcomes or quantitative proxy-to-ground-truth calibration are reported. The system emphasizes traceability and multi-tool ensemble but does not analyze correlation of failure modes across tools. This partially supports the theory's predictions about multi-proxy systems and the challenge of translating computational rankings to validated discoveries, but lacks quantitative evidence. <a href="../results/extraction-result-2218.html#e2218.5" class="evidence-link">[e2218.5]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<p class="empty-note">No evidence provided.</p>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>Closed-loop experimental systems with tight feedback loops show effective outcomes that may exceed theory's predictions for batch systems: A-Lab (71% synthesis success), SAMPLE protein engineering (≥12°C thermostability improvement), and MOBO-SPM (rapid convergence within ~10 exploration steps) achieve substantial experimental validation. While these systems still show gaps (A-Lab 29% failure, MOBO-SPM lacks quantified calibration), their success rates suggest the theory may underestimate the effectiveness of closed-loop approaches that continuously validate and refine proxies through experimental feedback, as opposed to batch proxy-then-validate workflows. This doesn't contradict the core theory but suggests it needs refinement to distinguish these scenarios. <a href="../results/extraction-result-2218.html#e2218.1" class="evidence-link">[e2218.1]</a> <a href="../results/extraction-result-2218.html#e2218.3" class="evidence-link">[e2218.3]</a> <a href="../results/extraction-result-2217.html#e2217.0" class="evidence-link">[e2217.0]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>Multiple systems use ensembles of proxies (MVP-RAG with attribute-level + product-level retrieval, Biomni with 150+ tools, AgentCompass with multiple metrics, MOBO-SPM with multiple rewards, DATADECIDE with multiple likelihood proxies) but do not systematically analyze whether proxy failure modes are correlated or independent. Some evidence suggests correlated failures (DATADECIDE normalized/quantile Brier follow same trends), while other evidence shows orthogonal signals (DATADECIDE CORRECT_PROB vs NORM_CORRECT_PROB have different scaling behaviors). Theory should more explicitly address multi-proxy systems and provide predictions about when combining proxies reduces vs amplifies gaps. <a href="../results/extraction-result-2226.html#e2226.0" class="evidence-link">[e2226.0]</a> <a href="../results/extraction-result-2218.html#e2218.5" class="evidence-link">[e2218.5]</a> <a href="../results/extraction-result-2215.html#e2215.0" class="evidence-link">[e2215.0]</a> <a href="../results/extraction-result-2217.html#e2217.0" class="evidence-link">[e2217.0]</a> <a href="../results/extraction-result-2223.html#e2223.2" class="evidence-link">[e2223.2]</a> <a href="../results/extraction-result-2223.html#e2223.3" class="evidence-link">[e2223.3]</a> </li>
    <li>Temporal validation reveals immediate proxy-to-ground-truth degradation distinct from static novelty: industrial AOI models meeting targets on test data fail in first chronological evaluation slice; DATADECIDE shows crossovers in scaling trends where one recipe overtakes another at different scales; VLA models show severe OOD degradation; hemodynamics models assume steady-state but reality is pulsatile. Theory should emphasize temporal/distributional shift as a critical dimension of proxy-to-ground-truth gap, distinct from static novelty distance, and provide predictions about temporal stability of proxy-ground-truth alignment. <a href="../results/extraction-result-2222.html#e2222.0" class="evidence-link">[e2222.0]</a> <a href="../results/extraction-result-2222.html#e2222.1" class="evidence-link">[e2222.1]</a> <a href="../results/extraction-result-2223.html#e2223.0" class="evidence-link">[e2223.0]</a> <a href="../results/extraction-result-2223.html#e2223.1" class="evidence-link">[e2223.1]</a> <a href="../results/extraction-result-2225.html#e2225.0" class="evidence-link">[e2225.0]</a> <a href="../results/extraction-result-2224.html#e2224.0" class="evidence-link">[e2224.0]</a> </li>
    <li>Closed-loop vs batch validation distinction: systems with tight experimental feedback (A-Lab 71% success, SAMPLE ≥12°C improvement, MOBO-SPM ~10 steps to convergence) show better proxy-to-ground-truth translation than batch systems (BigCloneBench 93% FP rate, hemodynamics no experimental validation, clinical models no external validation). Theory could distinguish between scenarios where proxies guide iterative experimental refinement (allowing rapid correction of proxy errors) vs scenarios where proxy optimization precedes delayed ground-truth validation (allowing error accumulation). <a href="../results/extraction-result-2218.html#e2218.1" class="evidence-link">[e2218.1]</a> <a href="../results/extraction-result-2218.html#e2218.3" class="evidence-link">[e2218.3]</a> <a href="../results/extraction-result-2217.html#e2217.0" class="evidence-link">[e2217.0]</a> <a href="../results/extraction-result-2221.html#e2221.0" class="evidence-link">[e2221.0]</a> <a href="../results/extraction-result-2224.html#e2224.0" class="evidence-link">[e2224.0]</a> <a href="../results/extraction-result-2220.html#e2220.0" class="evidence-link">[e2220.0]</a> </li>
    <li>Uncertainty quantification effectiveness varies widely and unpredictably: TokUR improves AUROC from 55% to 81% for correctness prediction (substantial but incomplete improvement), while multiple self-evaluation and external-signal methods (P(True), SE, SAR, LLM-Check, INSIDE) perform poorly (44-56% AUROC, near random). MOBO-SPM uses GP uncertainty in acquisition but lacks calibration analysis. Theory should provide more specific predictions about when uncertainty quantification can vs cannot reduce proxy-to-ground-truth gaps, potentially based on domain characteristics, proxy type, or problem structure. <a href="../results/extraction-result-2228.html#e2228.0" class="evidence-link">[e2228.0]</a> <a href="../results/extraction-result-2228.html#e2228.1" class="evidence-link">[e2228.1]</a> <a href="../results/extraction-result-2228.html#e2228.5" class="evidence-link">[e2228.5]</a> <a href="../results/extraction-result-2228.html#e2228.6" class="evidence-link">[e2228.6]</a> <a href="../results/extraction-result-2228.html#e2228.7" class="evidence-link">[e2228.7]</a> <a href="../results/extraction-result-2228.html#e2228.8" class="evidence-link">[e2228.8]</a> <a href="../results/extraction-result-2228.html#e2228.9" class="evidence-link">[e2228.9]</a> <a href="../results/extraction-result-2217.html#e2217.0" class="evidence-link">[e2217.0]</a> </li>
    <li>Proxy quality depends critically on metric design and domain-specific factors: requirement-aware metrics (cAUC, V@S, cV) better align with business objectives than standard metrics (accuracy, F1) in industrial AOI; continuous likelihood proxies (CORRECT_PROB, TOTAL_PROB) outperform discrete accuracy for dataset selection on some tasks but not others (math benchmarks); similarity reward fails due to narrow dynamic range in SPM; phase reward contributes from only ~3% of pixels. Theory should more explicitly address proxy design principles, failure modes, and domain-specific factors that determine proxy quality. <a href="../results/extraction-result-2222.html#e2222.0" class="evidence-link">[e2222.0]</a> <a href="../results/extraction-result-2222.html#e2222.1" class="evidence-link">[e2222.1]</a> <a href="../results/extraction-result-2223.html#e2223.2" class="evidence-link">[e2223.2]</a> <a href="../results/extraction-result-2217.html#e2217.1" class="evidence-link">[e2217.1]</a> </li>
    <li>Validation cascades with error propagation are common but poorly quantified: hemodynamics (geometry → mesh → CFD → ML), product attributes (retrieval → generation → evaluation), sports video (TAAD → DST → annotation), causal learning (score → structure → interpretation), clinical prediction (SMOTE → training → CV → test). MVP-RAG shows 6.3pp F1 gap when true value absent from retrieval. Theory should provide framework for analyzing multi-stage cascades and predicting cumulative error propagation, including when errors compound vs cancel across stages. <a href="../results/extraction-result-2224.html#e2224.0" class="evidence-link">[e2224.0]</a> <a href="../results/extraction-result-2226.html#e2226.0" class="evidence-link">[e2226.0]</a> <a href="../results/extraction-result-2216.html#e2216.0" class="evidence-link">[e2216.0]</a> <a href="../results/extraction-result-2227.html#e2227.0" class="evidence-link">[e2227.0]</a> <a href="../results/extraction-result-2220.html#e2220.1" class="evidence-link">[e2220.1]</a> </li>
    <li>Most evidence involves in-distribution or moderate extrapolation rather than truly transformational discoveries: BigCloneBench is in-distribution dataset artifacts, AOI is temporal shift within same domain, DATADECIDE compares similar pretraining recipes, VLA is zero-shot but within game domain, causal learning is simulations. Only a few systems (Virtual Lab novel nanobodies, SAMPLE thermostability improvements) approach transformational discoveries, and even these are within established protein engineering frameworks. Theory's predictions about larger gaps for transformational vs incremental discoveries remain largely untested in this evidence set and may need scope clarification. <a href="../results/extraction-result-2221.html#e2221.0" class="evidence-link">[e2221.0]</a> <a href="../results/extraction-result-2222.html#e2222.0" class="evidence-link">[e2222.0]</a> <a href="../results/extraction-result-2223.html#e2223.0" class="evidence-link">[e2223.0]</a> <a href="../results/extraction-result-2225.html#e2225.0" class="evidence-link">[e2225.0]</a> <a href="../results/extraction-result-2227.html#e2227.0" class="evidence-link">[e2227.0]</a> <a href="../results/extraction-result-2218.html#e2218.2" class="evidence-link">[e2218.2]</a> <a href="../results/extraction-result-2218.html#e2218.1" class="evidence-link">[e2218.1]</a> </li>
    <li>Domain maturity shows complex relationship with proxy-to-ground-truth gaps: mature domains with physics-based proxies (hemodynamics CFD, SPM force-distance) still lack experimental validation; emerging domains (agentic AI evaluation, VLA) show severe gaps; mixed-maturity domains (protein engineering, materials synthesis) show variable success. Theory predicts smaller gaps in mature domains with physics-based simulations, but evidence suggests maturity alone is insufficient - experimental validation culture and economic incentives also matter. <a href="../results/extraction-result-2224.html#e2224.0" class="evidence-link">[e2224.0]</a> <a href="../results/extraction-result-2217.html#e2217.0" class="evidence-link">[e2217.0]</a> <a href="../results/extraction-result-2215.html#e2215.0" class="evidence-link">[e2215.0]</a> <a href="../results/extraction-result-2225.html#e2225.0" class="evidence-link">[e2225.0]</a> <a href="../results/extraction-result-2218.html#e2218.1" class="evidence-link">[e2218.1]</a> <a href="../results/extraction-result-2218.html#e2218.3" class="evidence-link">[e2218.3]</a> </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>Add explicit theory statement distinguishing closed-loop experimental systems (with tight, rapid feedback enabling iterative proxy refinement) from batch proxy-optimization systems (with delayed ground-truth validation allowing error accumulation). Predict that closed-loop systems show smaller effective proxy-to-ground-truth gaps due to continuous correction, though gaps are not eliminated.</li>
                <li>Add explicit theory statement about temporal/distributional shift as a critical dimension of proxy-to-ground-truth gap, distinct from static novelty distance. Predict that proxy-ground-truth alignment degrades under temporal drift even when initial test-set alignment appears good, and that temporal stability should be evaluated separately from cross-sectional performance.</li>
                <li>Expand treatment of multi-proxy systems to predict when combining proxies reduces vs amplifies gaps. Add theory statements about: (1) correlated vs independent failure modes, (2) conditions under which ensemble proxies improve robustness, (3) risks of over-reliance on multiple proxies that share systematic biases (e.g., all computational, all trained on same data).</li>
                <li>Add more specific predictions about uncertainty quantification effectiveness based on domain characteristics. Predict that UQ is more effective when: (1) epistemic uncertainty dominates aleatoric, (2) proxy errors are due to model limitations rather than fundamental unpredictability, (3) sufficient perturbation/ensemble diversity exists. Predict UQ is less effective for: (1) self-evaluation in LLMs, (2) chaotic/fundamentally unpredictable systems, (3) systematic biases shared across ensemble members.</li>
                <li>Strengthen emphasis on proxy design principles and failure modes. Add theory statements about: (1) importance of dynamic range and continuous variation in proxy metrics, (2) risks of proxies that saturate or cluster (e.g., similarity reward near -1), (3) need for proxies to capture relevant aspects of ground truth (e.g., requirement-aware vs standard metrics), (4) domain-specific factors affecting proxy quality (e.g., class imbalance, temporal stability, measurement noise).</li>
                <li>Add framework for analyzing validation cascades and cumulative error propagation. Predict that: (1) errors compound across stages when stages are positively correlated, (2) early-stage errors create bottlenecks for later stages (e.g., retrieval failures limit generation), (3) cascade length correlates with total proxy-to-ground-truth gap, (4) intermediate validation stages can reduce but not eliminate cumulative error.</li>
                <li>Clarify scope regarding 'transformational vs incremental' discoveries. Most evidence involves in-distribution or moderate extrapolation; truly transformational discoveries are rare in the evidence set. Either: (1) narrow scope to explicitly exclude rare transformational breakthroughs and focus on incremental-to-moderate extrapolation, or (2) acknowledge that theory's predictions about transformational discoveries remain largely untested and require future validation.</li>
                <li>Add explicit discussion of economic and cultural factors affecting proxy-to-ground-truth gaps. Predict that gaps are larger when: (1) ground-truth validation is expensive relative to proxy evaluation, (2) publication/deployment incentives favor proxy optimization over validation, (3) domain lacks established experimental validation culture, (4) successful proxy optimization reduces availability of ground-truth labels (e.g., AOI reducing manual inspection).</li>
                <li>Refine predictions about domain maturity. Current theory predicts smaller gaps in mature domains with physics-based simulations, but evidence shows maturity alone is insufficient. Add nuance that: (1) computational maturity without experimental validation culture still produces large gaps, (2) emerging domains with tight experimental feedback can outperform mature computational-only domains, (3) domain maturity should be assessed across multiple dimensions (computational methods, experimental validation practices, theoretical understanding).</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-31",
    "theory_id": "theory-157",
    "fully_supporting_evidence": [
        {
            "text": "BigCloneBench semantic clone detection demonstrates extreme proxy-to-ground-truth gap: ML models achieve 99.8% precision and 93.7% F1 on BCB labels (proxy), but only 6.7% precision and 6.3% F1 when validated against strict human functional-similarity judgments (ground truth). The 93.35% false-positive rate in WT3/T4 labels shows proxy optimization producing highly misleading performance metrics, with models learning dataset artifacts rather than true semantic similarity. This exemplifies the theory's prediction that systems optimizing proxy metrics systematically overestimate success.",
            "uuids": [
                "e2221.0",
                "e2221.1",
                "e2221.3"
            ]
        },
        {
            "text": "Industrial AOI quality inspection demonstrates large proxy-to-ground-truth gaps with standard ML metrics: models achieving 99.3% accuracy have 7.8% slip rates (vs 1% target), and 98.7% accuracy corresponds to 11.3% slip rate. Even requirement-aware metrics that meet targets on test data (RFC2: slip ~1.0%, VR ~89.7%) fail immediately in chronological evaluation slices. This supports the theory's prediction that proxy-to-ground-truth gaps persist even with improved proxies and that temporal/distributional shifts amplify gaps.",
            "uuids": [
                "e2222.0",
                "e2222.1"
            ]
        },
        {
            "text": "DATADECIDE systematic evaluation quantifies proxy-to-ground-truth gaps in dataset selection: single-scale ranking achieves ~80% decision accuracy predicting 1B model performance from 150M experiments, leaving 20% error. Best scaling-law variants show 5.6-6.5% relative prediction error. Continuous likelihood proxies enable &gt;80% predictability on some tasks at 0.01% of target compute but fail on others (math benchmarks), demonstrating the theory's prediction that proxy quality depends on domain amenability to computational modeling.",
            "uuids": [
                "e2223.0",
                "e2223.1",
                "e2223.2",
                "e2223.3"
            ]
        },
        {
            "text": "Vision-language-action models show severe proxy-to-ground-truth gaps under domain shift: zero-shot Procgen evaluation yields Brier MAE &gt;1.5-1.7 (near maximum of 2), macro recall 6-12%, and high invalid output rates (&gt;80% for some models/datasets). Gap increases with image complexity (Shannon entropy correlation -0.409 for GPT-4o) and for sparse/timed special actions. This strongly supports the theory's prediction that gaps increase with novelty and extrapolation distance from training distributions.",
            "uuids": [
                "e2225.0",
                "e2225.1",
                "e2225.2",
                "e2225.3",
                "e2225.4"
            ]
        },
        {
            "text": "Hemodynamics surrogate modeling demonstrates proxy-to-proxy cascade without experimental ground truth: ML models (DeepONet, DeepONet-SwinT) predict CFD fields with validation MNAE_u ~0.05-0.10, but CFD itself is a proxy for in-vivo hemodynamics with no experimental (PC-MRI, 4D flow) validation performed. CFD assumptions (steady-state, Newtonian, rigid walls) and boundary condition uncertainties create unquantified gaps to physiological reality. This exemplifies the theory's prediction about validation cascades and the economic incentive to defer ground-truth validation.",
            "uuids": [
                "e2224.0",
                "e2224.1",
                "e2224.2",
                "e2224.3"
            ]
        },
        {
            "text": "LLM uncertainty estimation shows substantial proxy-to-ground-truth gaps: baseline log-likelihood achieves only 55.41% AUROC on MATH500, while improved methods (TokUR) reach 80.64% AUROC - still leaving ~20% error in correctness prediction. Self-evaluation methods (P(True), LLM-Check, INSIDE) perform poorly (44-56% AUROC), and external methods (SE, SAR) also underperform. This supports the theory's prediction that even sophisticated uncertainty quantification cannot eliminate proxy-to-ground-truth gaps.",
            "uuids": [
                "e2228.0",
                "e2228.1",
                "e2228.2",
                "e2228.3",
                "e2228.4",
                "e2228.5",
                "e2228.6",
                "e2228.7",
                "e2228.8",
                "e2228.9"
            ]
        },
        {
            "text": "Sports video action detection shows quantified proxy-to-ground-truth gap and partial reduction: TAAD proxy predictions achieve 43.56% precision and 66.97% recall vs ground-truth annotations. Adding denoising sequence transduction (DST) with game-state context improves to 78.77% precision and 75.80% recall (+35.21pp precision, +8.83pp recall). This demonstrates the theory's prediction that multifidelity approaches can reduce but not eliminate gaps - substantial error remains even after sophisticated denoising.",
            "uuids": [
                "e2216.0"
            ]
        },
        {
            "text": "Causal structure learning (DAGSLAM) shows proxy improvement with appropriate loss functions: using mixed-type loss improves directed F1 from ~0.71 to 0.82 (50% categorical nodes) vs treating all as continuous. However, evaluation is simulation-only with no experimental causal validation, and performance degrades with increased categorical proportion (F1 drops from 0.97 to 0.82 at 50% categorical), dense graphs, and small samples. This supports the theory's prediction that proxy quality depends on domain characteristics and that gaps persist even with improved proxies.",
            "uuids": [
                "e2227.0",
                "e2227.1",
                "e2227.2",
                "e2227.3",
                "e2227.4",
                "e2227.5",
                "e2227.6"
            ]
        },
        {
            "text": "Autonomous agent systems show proxy-to-ground-truth gaps in biological discovery: BioDiscoveryAgent reports +21% improvement vs Bayesian optimization baselines but lacks explicit proxy-ground-truth calibration metrics. DrugAgent achieves PAMPA F1 ~0.92 on held-out assay labels but no prospective experimental validation. Virtual Lab produces &gt;90% expression rate but only 2 of 92 candidates show improved binding. These systems demonstrate the theory's prediction about accumulation of unvalidated discoveries and the economic incentive to defer ground-truth validation.",
            "uuids": [
                "e2218.0",
                "e2218.2",
                "e2218.4"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "A-Lab autonomous materials synthesis achieves 71% experimental success rate (41 novel compounds from 58 targets) using closed-loop robotic execution, demonstrating effective proxy-to-ground-truth translation in automated experimentation. However, explicit quantitative proxy-vs-ground calibration metrics are not reported, and the system uses multiple instrument readouts whose correlation and failure modes are not analyzed. The 29% failure rate still represents a substantial proxy-to-ground-truth gap, supporting the theory while showing closed-loop systems can achieve better translation than batch systems.",
            "uuids": [
                "e2218.3"
            ]
        },
        {
            "text": "Multi-objective Bayesian optimization for SPM (MOBO-SPM) uses multiple reward proxies (height-difference, tip-sample distance, phase) with GP uncertainty quantification and achieves qualitatively good scan quality. However, it lacks explicit numeric proxy-to-ground-truth agreement statistics (no R², MAE reported). Similarity reward shows narrow dynamic range and clusters near -1, demonstrating proxy-quality dependence on metric design. Phase reward contributes from only ~3% of pixels, reducing its influence. This partially supports the theory's predictions about proxy quality and multiple-proxy systems.",
            "uuids": [
                "e2217.0",
                "e2217.1",
                "e2217.2"
            ]
        },
        {
            "text": "Clinical prediction models (neuropathic foot ulcer ML pipeline) achieve high cross-validation metrics (MLP: AUC 0.901, accuracy 0.8751) but lack independent external validation or prospective clinical validation, leaving proxy-to-ground-truth gap unquantified. SMOTE class balancing increases minority samples from 57 to 113 but synthetic-data fidelity is not validated. SHAP feature importance shows qualitative agreement with statistically significant covariates but no quantitative calibration is provided. This supports the theory's prediction about deferred ground-truth validation but lacks the quantified gaps needed for strong support.",
            "uuids": [
                "e2220.0",
                "e2220.1",
                "e2220.2"
            ]
        },
        {
            "text": "Multi-branch CNN with attention (MBC-ATT) for EEG/fNIRS fusion achieves 98.13% accuracy on n-back and 98.61% on WG tasks, with ablation showing 6.55pp and 2.07pp drops without cross-modal attention. However, validation is against behavioral task labels only (no independent physiological ground truth), and generalization to small-sample patient datasets and cross-subject transfer is untested. This partially supports the theory by showing high proxy performance without independent ground-truth validation, but the lack of failure cases limits the evidence strength.",
            "uuids": [
                "e2219.0",
                "e2219.1",
                "e2219.2"
            ]
        },
        {
            "text": "Product attribute value identification (MVP-RAG) shows quantified improvement over baselines: +26.3pp F1 vs Qwen2.5(Product-RAG) and +3.3pp F1 vs TACLR, achieving 89.5% F1 overall. When true attribute value appears in retrieved candidates, generation F1 reaches 92.6% vs 86.3% when absent (6.3pp gap), demonstrating cascade error propagation. However, evaluation is against human-reviewed labels from seller feedback pipeline (itself a proxy for true product attributes), not independent experimental validation. The 10.5% error rate and sensitivity to retrieval quality support the theory's predictions about cascading errors.",
            "uuids": [
                "e2226.0",
                "e2226.1",
                "e2226.2"
            ]
        },
        {
            "text": "AgentCompass evaluation framework shows moderate proxy-to-ground-truth correlation: Pearson ρ = 0.430 (GAIA) and 0.408 (SWE Bench) between system quality scores and human scores. Localization accuracy 0.657 (GAIA) but only 0.250 (SWE Bench), with categorization F1 0.309 and 0.232 respectively. System identifies errors outside benchmark taxonomy (Safety & Security, Reflection Gaps), showing proxy-to-ground-truth mismatch increases when discovering novel error classes. This partially supports the theory's prediction about gaps increasing with novelty, though the domain (agentic AI evaluation) is emerging rather than mature.",
            "uuids": [
                "e2215.0"
            ]
        },
        {
            "text": "SAMPLE self-driving protein engineering pipeline discovers GH1 hydrolase variants with ≥12°C higher stability than starting sequences through closed-loop design, build, and test cycles. However, no explicit proxy-ground-truth calibration numbers are provided, and the system uses computational and intermediate robotic assay predictions whose accuracy is not quantified. This partially supports the theory by showing successful experimental validation in a closed-loop system, but lacks the detailed proxy-performance metrics needed to assess gap magnitude.",
            "uuids": [
                "e2218.1"
            ]
        },
        {
            "text": "Biomni general-purpose biomedical agent orchestrates 150+ tools and 59 databases to produce ranked candidates with traceable evidence, but no uniform ground-truth outcomes or quantitative proxy-to-ground-truth calibration are reported. The system emphasizes traceability and multi-tool ensemble but does not analyze correlation of failure modes across tools. This partially supports the theory's predictions about multi-proxy systems and the challenge of translating computational rankings to validated discoveries, but lacks quantitative evidence.",
            "uuids": [
                "e2218.5"
            ]
        }
    ],
    "fully_contradicting_evidence": [],
    "partially_contradicting_evidence": [
        {
            "text": "Closed-loop experimental systems with tight feedback loops show effective outcomes that may exceed theory's predictions for batch systems: A-Lab (71% synthesis success), SAMPLE protein engineering (≥12°C thermostability improvement), and MOBO-SPM (rapid convergence within ~10 exploration steps) achieve substantial experimental validation. While these systems still show gaps (A-Lab 29% failure, MOBO-SPM lacks quantified calibration), their success rates suggest the theory may underestimate the effectiveness of closed-loop approaches that continuously validate and refine proxies through experimental feedback, as opposed to batch proxy-then-validate workflows. This doesn't contradict the core theory but suggests it needs refinement to distinguish these scenarios.",
            "uuids": [
                "e2218.1",
                "e2218.3",
                "e2217.0"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "Multiple systems use ensembles of proxies (MVP-RAG with attribute-level + product-level retrieval, Biomni with 150+ tools, AgentCompass with multiple metrics, MOBO-SPM with multiple rewards, DATADECIDE with multiple likelihood proxies) but do not systematically analyze whether proxy failure modes are correlated or independent. Some evidence suggests correlated failures (DATADECIDE normalized/quantile Brier follow same trends), while other evidence shows orthogonal signals (DATADECIDE CORRECT_PROB vs NORM_CORRECT_PROB have different scaling behaviors). Theory should more explicitly address multi-proxy systems and provide predictions about when combining proxies reduces vs amplifies gaps.",
            "uuids": [
                "e2226.0",
                "e2218.5",
                "e2215.0",
                "e2217.0",
                "e2223.2",
                "e2223.3"
            ]
        },
        {
            "text": "Temporal validation reveals immediate proxy-to-ground-truth degradation distinct from static novelty: industrial AOI models meeting targets on test data fail in first chronological evaluation slice; DATADECIDE shows crossovers in scaling trends where one recipe overtakes another at different scales; VLA models show severe OOD degradation; hemodynamics models assume steady-state but reality is pulsatile. Theory should emphasize temporal/distributional shift as a critical dimension of proxy-to-ground-truth gap, distinct from static novelty distance, and provide predictions about temporal stability of proxy-ground-truth alignment.",
            "uuids": [
                "e2222.0",
                "e2222.1",
                "e2223.0",
                "e2223.1",
                "e2225.0",
                "e2224.0"
            ]
        },
        {
            "text": "Closed-loop vs batch validation distinction: systems with tight experimental feedback (A-Lab 71% success, SAMPLE ≥12°C improvement, MOBO-SPM ~10 steps to convergence) show better proxy-to-ground-truth translation than batch systems (BigCloneBench 93% FP rate, hemodynamics no experimental validation, clinical models no external validation). Theory could distinguish between scenarios where proxies guide iterative experimental refinement (allowing rapid correction of proxy errors) vs scenarios where proxy optimization precedes delayed ground-truth validation (allowing error accumulation).",
            "uuids": [
                "e2218.1",
                "e2218.3",
                "e2217.0",
                "e2221.0",
                "e2224.0",
                "e2220.0"
            ]
        },
        {
            "text": "Uncertainty quantification effectiveness varies widely and unpredictably: TokUR improves AUROC from 55% to 81% for correctness prediction (substantial but incomplete improvement), while multiple self-evaluation and external-signal methods (P(True), SE, SAR, LLM-Check, INSIDE) perform poorly (44-56% AUROC, near random). MOBO-SPM uses GP uncertainty in acquisition but lacks calibration analysis. Theory should provide more specific predictions about when uncertainty quantification can vs cannot reduce proxy-to-ground-truth gaps, potentially based on domain characteristics, proxy type, or problem structure.",
            "uuids": [
                "e2228.0",
                "e2228.1",
                "e2228.5",
                "e2228.6",
                "e2228.7",
                "e2228.8",
                "e2228.9",
                "e2217.0"
            ]
        },
        {
            "text": "Proxy quality depends critically on metric design and domain-specific factors: requirement-aware metrics (cAUC, V@S, cV) better align with business objectives than standard metrics (accuracy, F1) in industrial AOI; continuous likelihood proxies (CORRECT_PROB, TOTAL_PROB) outperform discrete accuracy for dataset selection on some tasks but not others (math benchmarks); similarity reward fails due to narrow dynamic range in SPM; phase reward contributes from only ~3% of pixels. Theory should more explicitly address proxy design principles, failure modes, and domain-specific factors that determine proxy quality.",
            "uuids": [
                "e2222.0",
                "e2222.1",
                "e2223.2",
                "e2217.1"
            ]
        },
        {
            "text": "Validation cascades with error propagation are common but poorly quantified: hemodynamics (geometry → mesh → CFD → ML), product attributes (retrieval → generation → evaluation), sports video (TAAD → DST → annotation), causal learning (score → structure → interpretation), clinical prediction (SMOTE → training → CV → test). MVP-RAG shows 6.3pp F1 gap when true value absent from retrieval. Theory should provide framework for analyzing multi-stage cascades and predicting cumulative error propagation, including when errors compound vs cancel across stages.",
            "uuids": [
                "e2224.0",
                "e2226.0",
                "e2216.0",
                "e2227.0",
                "e2220.1"
            ]
        },
        {
            "text": "Most evidence involves in-distribution or moderate extrapolation rather than truly transformational discoveries: BigCloneBench is in-distribution dataset artifacts, AOI is temporal shift within same domain, DATADECIDE compares similar pretraining recipes, VLA is zero-shot but within game domain, causal learning is simulations. Only a few systems (Virtual Lab novel nanobodies, SAMPLE thermostability improvements) approach transformational discoveries, and even these are within established protein engineering frameworks. Theory's predictions about larger gaps for transformational vs incremental discoveries remain largely untested in this evidence set and may need scope clarification.",
            "uuids": [
                "e2221.0",
                "e2222.0",
                "e2223.0",
                "e2225.0",
                "e2227.0",
                "e2218.2",
                "e2218.1"
            ]
        },
        {
            "text": "Domain maturity shows complex relationship with proxy-to-ground-truth gaps: mature domains with physics-based proxies (hemodynamics CFD, SPM force-distance) still lack experimental validation; emerging domains (agentic AI evaluation, VLA) show severe gaps; mixed-maturity domains (protein engineering, materials synthesis) show variable success. Theory predicts smaller gaps in mature domains with physics-based simulations, but evidence suggests maturity alone is insufficient - experimental validation culture and economic incentives also matter.",
            "uuids": [
                "e2224.0",
                "e2217.0",
                "e2215.0",
                "e2225.0",
                "e2218.1",
                "e2218.3"
            ]
        }
    ],
    "suggested_revisions": [
        "Add explicit theory statement distinguishing closed-loop experimental systems (with tight, rapid feedback enabling iterative proxy refinement) from batch proxy-optimization systems (with delayed ground-truth validation allowing error accumulation). Predict that closed-loop systems show smaller effective proxy-to-ground-truth gaps due to continuous correction, though gaps are not eliminated.",
        "Add explicit theory statement about temporal/distributional shift as a critical dimension of proxy-to-ground-truth gap, distinct from static novelty distance. Predict that proxy-ground-truth alignment degrades under temporal drift even when initial test-set alignment appears good, and that temporal stability should be evaluated separately from cross-sectional performance.",
        "Expand treatment of multi-proxy systems to predict when combining proxies reduces vs amplifies gaps. Add theory statements about: (1) correlated vs independent failure modes, (2) conditions under which ensemble proxies improve robustness, (3) risks of over-reliance on multiple proxies that share systematic biases (e.g., all computational, all trained on same data).",
        "Add more specific predictions about uncertainty quantification effectiveness based on domain characteristics. Predict that UQ is more effective when: (1) epistemic uncertainty dominates aleatoric, (2) proxy errors are due to model limitations rather than fundamental unpredictability, (3) sufficient perturbation/ensemble diversity exists. Predict UQ is less effective for: (1) self-evaluation in LLMs, (2) chaotic/fundamentally unpredictable systems, (3) systematic biases shared across ensemble members.",
        "Strengthen emphasis on proxy design principles and failure modes. Add theory statements about: (1) importance of dynamic range and continuous variation in proxy metrics, (2) risks of proxies that saturate or cluster (e.g., similarity reward near -1), (3) need for proxies to capture relevant aspects of ground truth (e.g., requirement-aware vs standard metrics), (4) domain-specific factors affecting proxy quality (e.g., class imbalance, temporal stability, measurement noise).",
        "Add framework for analyzing validation cascades and cumulative error propagation. Predict that: (1) errors compound across stages when stages are positively correlated, (2) early-stage errors create bottlenecks for later stages (e.g., retrieval failures limit generation), (3) cascade length correlates with total proxy-to-ground-truth gap, (4) intermediate validation stages can reduce but not eliminate cumulative error.",
        "Clarify scope regarding 'transformational vs incremental' discoveries. Most evidence involves in-distribution or moderate extrapolation; truly transformational discoveries are rare in the evidence set. Either: (1) narrow scope to explicitly exclude rare transformational breakthroughs and focus on incremental-to-moderate extrapolation, or (2) acknowledge that theory's predictions about transformational discoveries remain largely untested and require future validation.",
        "Add explicit discussion of economic and cultural factors affecting proxy-to-ground-truth gaps. Predict that gaps are larger when: (1) ground-truth validation is expensive relative to proxy evaluation, (2) publication/deployment incentives favor proxy optimization over validation, (3) domain lacks established experimental validation culture, (4) successful proxy optimization reduces availability of ground-truth labels (e.g., AOI reducing manual inspection).",
        "Refine predictions about domain maturity. Current theory predicts smaller gaps in mature domains with physics-based simulations, but evidence shows maturity alone is insufficient. Add nuance that: (1) computational maturity without experimental validation culture still produces large gaps, (2) emerging domains with tight experimental feedback can outperform mature computational-only domains, (3) domain maturity should be assessed across multiple dimensions (computational methods, experimental validation practices, theoretical understanding)."
    ],
    "overall_support_or_contradict": "support",
    "overall_support_or_contradict_explanation": "The new evidence strongly supports the core theory claims with extensive quantified examples of proxy-to-ground-truth gaps (BigCloneBench 93% FP rate, AOI 10+ pp slip gaps, VLA Brier MAE &gt;1.5, DATADECIDE 20% decision error, LLM uncertainty 20-45% AUROC gaps). Evidence confirms gaps vary with domain characteristics, novelty, and proxy quality, and shows gap-reduction methods provide partial but incomplete solutions. No evidence fundamentally contradicts the theory; closed-loop systems and other nuances suggest refinements rather than contradictions.",
    "revised_theory_ids": [
        "theory-393"
    ],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-22 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-22</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-22</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-176.html">theory-176</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-383.html">theory-383</a></p>
                <p><strong>Overall Support or Contradict:</strong> support</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The new evidence strongly supports the core theory that properly implemented LLM-generated curricula outperform hand-designed and heuristic approaches (1.8-2.1× to 82× improvements across diverse domains), but reveals that effectiveness critically depends on validation mechanisms, retrieval augmentation, and appropriate integration methods—naive LLM curriculum generation fails catastrophically (12-31% success rates, <30% valid data), requiring significant theory refinement to distinguish effective from ineffective approaches.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>AURA achieves 99% training-launch success with schema validation and retrieval augmentation, compared to 31% for CurricuLLM and 12% for Eureka on complex robotics tasks, demonstrating that properly validated LLM-generated curricula with complementary systems vastly outperform sampling-based approaches and reduce computational waste (single launch vs 5-16 launches per stage). <a href="../results/extraction-result-2047.html#e2047.0" class="evidence-link">[e2047.0]</a> <a href="../results/extraction-result-2047.html#e2047.1" class="evidence-link">[e2047.1]</a> <a href="../results/extraction-result-2047.html#e2047.2" class="evidence-link">[e2047.2]</a> <a href="../results/extraction-result-2047.html#e2047.3" class="evidence-link">[e2047.3]</a> </li>
    <li>LADDER demonstrates 1%→82% improvement (82× gain) on undergraduate integration problems using LLM-generated recursive variant trees, with strong generalization from only 10 training problems to 100 test problems, and TTRL further improves MIT Integration Bee performance from 73%→90%, strongly supporting the theory's claims about LLM curriculum effectiveness in compositional domains with verifiable rewards. <a href="../results/extraction-result-2048.html#e2048.0" class="evidence-link">[e2048.0]</a> <a href="../results/extraction-result-2048.html#e2048.1" class="evidence-link">[e2048.1]</a> <a href="../results/extraction-result-2048.html#e2048.2" class="evidence-link">[e2048.2]</a> </li>
    <li>TTC-RL achieves ~1.8-2.1× improvements on competition-level math (AIME25: 40%→62% pass@8) and coding (CodeElo: 28%→43% pass@8) using LLM-embedding-based curriculum selection (SIFT), substantially outperforming uniform sampling and domain-only heuristics, confirming that LLM world knowledge enables superior task selection and curriculum construction. <a href="../results/extraction-result-2054.html#e2054.0" class="evidence-link">[e2054.0]</a> <a href="../results/extraction-result-2054.html#e2054.1" class="evidence-link">[e2054.1]</a> </li>
    <li>SGRL outperforms frequent-query LLM baselines (score 33.8±1.5 vs AdaRefiner 28.5±2.3, ELLM 28.4±2.5) while requiring minimal LLM invocations through reusable goal-generation code, and unlocks deep achievements (Collect Diamond at ~3.7M steps) that baselines fail to reach by 5M steps, supporting the theory's emphasis on structured LLM outputs and complementary systems over naive online querying. <a href="../results/extraction-result-2049.html#e2049.0" class="evidence-link">[e2049.0]</a> <a href="../results/extraction-result-2049.html#e2049.1" class="evidence-link">[e2049.1]</a> <a href="../results/extraction-result-2049.html#e2049.2" class="evidence-link">[e2049.2]</a> <a href="../results/extraction-result-2049.html#e2049.3" class="evidence-link">[e2049.3]</a> <a href="../results/extraction-result-2049.html#e2049.4" class="evidence-link">[e2049.4]</a> </li>
    <li>EXIF's exploration-first + iterative feedback approach yields large improvements (Webshop reward +29.4 to +51.7 for different models, Crafter AP +18.8 to +20.5) and produces 70-85% valid training data vs <30% for proposal-first LLM curricula, demonstrating that environment-grounded LLM curriculum generation with state conditioning is essential and that naive LLM task proposals fail catastrophically. <a href="../results/extraction-result-2045.html#e2045.0" class="evidence-link">[e2045.0]</a> <a href="../results/extraction-result-2045.html#e2045.1" class="evidence-link">[e2045.1]</a> <a href="../results/extraction-result-2045.html#e2045.2" class="evidence-link">[e2045.2]</a> <a href="../results/extraction-result-2045.html#e2045.3" class="evidence-link">[e2045.3]</a> </li>
    <li>WebSynthesis achieves 20.15% Pass@3 with ~4k LLM-assisted synthetic samples, outperforming OS-Genesis (18.66%) trained on 7.4k real trajectories and AgentTrek (11.94%) with 20k tutorial samples, confirming LLM-generated curricula can be more sample-efficient than manual/heuristic approaches when combining world-model guidance (WebMCTS) with proper warm-up curricula. <a href="../results/extraction-result-2052.html#e2052.0" class="evidence-link">[e2052.0]</a> <a href="../results/extraction-result-2052.html#e2052.1" class="evidence-link">[e2052.1]</a> <a href="../results/extraction-result-2052.html#e2052.2" class="evidence-link">[e2052.2]</a> <a href="../results/extraction-result-2052.html#e2052.3" class="evidence-link">[e2052.3]</a> <a href="../results/extraction-result-2052.html#e2052.4" class="evidence-link">[e2052.4]</a> </li>
    <li>Retrieval augmentation (RAG/VDB) is critical for LLM curriculum effectiveness across multiple implementations: removing VDB in AURA reduced success from 99%→38%, DSMentor's retrieval-augmented memory substantially improved performance, and HierQ-LLM's RAG-based context extraction achieved 100% accuracy, strongly supporting the theory's emphasis on complementary systems. <a href="../results/extraction-result-2047.html#e2047.3" class="evidence-link">[e2047.3]</a> <a href="../results/extraction-result-2046.html#e2046.0" class="evidence-link">[e2046.0]</a> <a href="../results/extraction-result-2051.html#e2051.0" class="evidence-link">[e2051.0]</a> </li>
    <li>Self-aware RL achieves +53.8% relative gain on math benchmarks (21.0→32.3 average) using LLM-generated tasks with difficulty prediction while querying external guidance on only 1.23% of tasks, demonstrating that LLM-generated curricula with self-awareness and minimal external supervision can produce large improvements in specialized reasoning domains. <a href="../results/extraction-result-2042.html#e2042.0" class="evidence-link">[e2042.0]</a> <a href="../results/extraction-result-2042.html#e2042.1" class="evidence-link">[e2042.1]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>CRAFT achieves high success rates (90% Gate, 60% Seesaw, 100% Two Arm Lift) with LLM-generated curricula and VLM-guided refinement, and demonstrates zero-shot sim-to-real transfer (65% hardware success), but requires multiple attempts due to foundation model stochasticity and depends heavily on the VLM feedback loop, partially supporting the theory while highlighting that reliability requires iterative refinement mechanisms. <a href="../results/extraction-result-2043.html#e2043.0" class="evidence-link">[e2043.0]</a> <a href="../results/extraction-result-2043.html#e2043.1" class="evidence-link">[e2043.1]</a> <a href="../results/extraction-result-2043.html#e2043.2" class="evidence-link">[e2043.2]</a> </li>
    <li>DSMentor shows improvements (+5.2% DSEval, +8.8% QRData causal) with LLM problem-based difficulty assessment, but problem-based curriculum sometimes only marginally outperforms or is matched by heuristic difficulty metrics (pass-rate, reference-code) in specific cases, suggesting LLM advantage is context-dependent and most pronounced in complex/specialized domains. <a href="../results/extraction-result-2046.html#e2046.0" class="evidence-link">[e2046.0]</a> <a href="../results/extraction-result-2046.html#e2046.1" class="evidence-link">[e2046.1]</a> <a href="../results/extraction-result-2046.html#e2046.2" class="evidence-link">[e2046.2]</a> </li>
    <li>A-TTC (achievability-balanced curriculum) improves learning for weaker models (Qwen3-0.6B) by combining LLM-based selection (SIFT) with online difficulty estimates, but the benefit is model-dependent and requires hyperparameter tuning ([0.2,0.6] achievability interval), partially supporting the theory's claim about state conditioning while showing that curriculum design must adapt to model capability. <a href="../results/extraction-result-2054.html#e2054.2" class="evidence-link">[e2054.2]</a> </li>
    <li>SDW uses LLM-generated functions offline to drive curriculum weighting in lifelong RL, achieving positive transfer (T=0.290) and reduced forgetting (F=-0.505) compared to baselines, but performance varies significantly by LLM quality (GPT-4o > GPT-3.5 > GLM4-9B), supporting the theory while showing clear model-size dependency in curriculum generation quality. <a href="../results/extraction-result-2050.html#e2050.0" class="evidence-link">[e2050.0]</a> </li>
    <li>HierQ-LLM demonstrates that LLM guidance (context extraction and exploration facilitation) substantially improves hierarchical RL performance (MSR up to 83.4% vs near-0% for flat agents), but effectiveness depends on model selection (Hermes3 excels in 3-info, Llama3.1 in 6-info) and requires integration with hierarchical decomposition, showing LLM curricula are most effective when combined with appropriate architectural support. <a href="../results/extraction-result-2051.html#e2051.0" class="evidence-link">[e2051.0]</a> <a href="../results/extraction-result-2051.html#e2051.1" class="evidence-link">[e2051.1]</a> <a href="../results/extraction-result-2051.html#e2051.2" class="evidence-link">[e2051.2]</a> </li>
    <li>SEC (learned curriculum using advantage signals) outperforms random and manual curricula with substantial relative gains (+13% Countdown, +21% Zebra, +22% ARC-1D, +33% AIME24) without using LLM generation, showing that adaptive non-LLM curricula can be highly effective, but SEC requires predefined problem categories and doesn't leverage world knowledge for task generation. <a href="../results/extraction-result-2053.html#e2053.0" class="evidence-link">[e2053.0]</a> <a href="../results/extraction-result-2053.html#e2053.1" class="evidence-link">[e2053.1]</a> <a href="../results/extraction-result-2053.html#e2053.2" class="evidence-link">[e2053.2]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<ol>
    <li>CurricuLLM and Eureka achieve only 31% and 12% training-launch success respectively on complex robotics tasks due to malformed LLM generations and lack of validation, requiring ~5-16 parallel launches per stage and massive computational waste, directly contradicting any claim of universal LLM curriculum superiority and demonstrating that unvalidated LLM generation is highly ineffective. <a href="../results/extraction-result-2047.html#e2047.1" class="evidence-link">[e2047.1]</a> <a href="../results/extraction-result-2047.html#e2047.2" class="evidence-link">[e2047.2]</a> </li>
    <li>TTC-SFT (supervised fine-tuning on LLM-selected curricula) causes catastrophic initial accuracy collapse to near 0% and requires hundreds of gradient steps to recover, demonstrating that LLM-generated curricula can fail catastrophically when improperly integrated (off-policy distribution shift), directly contradicting claims of general superiority without proper integration methods. <a href="../results/extraction-result-2054.html#e2054.4" class="evidence-link">[e2054.4]</a> </li>
    <li>Proposal-first LLM curricula produce <30% valid training data in both Webshop and Crafter due to lack of environment grounding (LLM proposes infeasible tasks), substantially underperforming exploration-first approaches (70-85% valid data), directly contradicting any claim that naive LLM task generation is effective and demonstrating that environment grounding is essential. <a href="../results/extraction-result-2045.html#e2045.1" class="evidence-link">[e2045.1]</a> <a href="../results/extraction-result-2045.html#e2045.3" class="evidence-link">[e2045.3]</a> </li>
</ol>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>ELLM and AdaRefiner require frequent expensive LLM queries during training with substantial computational cost and underperform SGRL (scores ~28.4-28.5 vs 33.8) despite using similar LLM guidance, partially contradicting the theory's implicit assumption that LLM curriculum generation is computationally practical and showing that implementation approach (frequent queries vs reusable code) critically affects both cost and effectiveness. <a href="../results/extraction-result-2049.html#e2049.2" class="evidence-link">[e2049.2]</a> <a href="../results/extraction-result-2049.html#e2049.3" class="evidence-link">[e2049.3]</a> </li>
    <li>TTRL (test-time RL with LLM-generated variants) fails to solve any problems when applied to base models without prior LADDER training (0 solved after 100 steps), showing LLM-generated curricula alone are insufficient without proper foundation and partially contradicting claims of standalone effectiveness, though TTRL is highly effective when combined with prior curriculum exposure (73%→90%). <a href="../results/extraction-result-2048.html#e2048.1" class="evidence-link">[e2048.1]</a> </li>
    <li>LADDER variant generation produces ~8% unsolvable variants and some variants that are harder than intended due to small coefficient changes, showing LLM curriculum generation can produce low-quality or counterproductive tasks, partially contradicting claims of consistent superiority and highlighting the need for quality control and verification mechanisms. <a href="../results/extraction-result-2048.html#e2048.2" class="evidence-link">[e2048.2]</a> </li>
    <li>Manual/heuristic curricula can match LLM performance in specific contexts: pass-rate difficulty matched problem-based LLM difficulty on DSEval-LeetCode for one model, and manual staged curricula (TiZero, eligibility traces) are effective in well-understood domains, partially contradicting the theory's claim of universal LLM superiority and suggesting domain structure and understanding matter. <a href="../results/extraction-result-2046.html#e2046.1" class="evidence-link">[e2046.1]</a> <a href="../results/extraction-result-2044.html#e2044.4" class="evidence-link">[e2044.4]</a> <a href="../results/extraction-result-2044.html#e2044.5" class="evidence-link">[e2044.5]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>The distinction between online frequent-query LLM methods (ELLM, AdaRefiner - expensive, underperforming) and offline reusable code generation (SGRL, AURA - efficient, effective) is fundamental: offline approaches achieve comparable or better results with orders of magnitude fewer LLM calls, suggesting the theory should explicitly distinguish between these paradigms as different curriculum generation strategies with vastly different cost-benefit profiles. <a href="../results/extraction-result-2049.html#e2049.0" class="evidence-link">[e2049.0]</a> <a href="../results/extraction-result-2049.html#e2049.2" class="evidence-link">[e2049.2]</a> <a href="../results/extraction-result-2049.html#e2049.3" class="evidence-link">[e2049.3]</a> <a href="../results/extraction-result-2047.html#e2047.0" class="evidence-link">[e2047.0]</a> </li>
    <li>Validation and verification mechanisms are not merely complementary but essential requirements: schema validation (AURA 99%→38% without), execution verification (LADDER, TTC-RL), VLM feedback loops (CRAFT), and quality control are necessary for LLM curriculum effectiveness, suggesting these should be elevated from 'complementary systems' to 'essential requirements' in the theory. <a href="../results/extraction-result-2047.html#e2047.0" class="evidence-link">[e2047.0]</a> <a href="../results/extraction-result-2047.html#e2047.3" class="evidence-link">[e2047.3]</a> <a href="../results/extraction-result-2043.html#e2043.0" class="evidence-link">[e2043.0]</a> <a href="../results/extraction-result-2043.html#e2043.2" class="evidence-link">[e2043.2]</a> <a href="../results/extraction-result-2048.html#e2048.0" class="evidence-link">[e2048.0]</a> <a href="../results/extraction-result-2048.html#e2048.2" class="evidence-link">[e2048.2]</a> <a href="../results/extraction-result-2054.html#e2054.0" class="evidence-link">[e2054.0]</a> </li>
    <li>Retrieval augmentation (RAG/VDB) emerges as a critical complementary system across multiple successful implementations (AURA, DSMentor, HierQ-LLM) that substantially improves curriculum quality by grounding generation in prior successful examples, and should be explicitly included in the theory's list of key complementary systems alongside skill libraries and execution monitoring. <a href="../results/extraction-result-2047.html#e2047.3" class="evidence-link">[e2047.3]</a> <a href="../results/extraction-result-2046.html#e2046.0" class="evidence-link">[e2046.0]</a> <a href="../results/extraction-result-2051.html#e2051.0" class="evidence-link">[e2051.0]</a> </li>
    <li>Exploration-first vs proposal-first is a fundamental distinction for curriculum validity: exploration-first with feedback achieves 70-85% valid data vs <30% for proposal-first, and EXIF substantially outperforms proposal-first baselines, suggesting the theory should explicitly address curriculum generation methodology and environment grounding as a core design choice rather than treating all LLM generation as equivalent. <a href="../results/extraction-result-2045.html#e2045.0" class="evidence-link">[e2045.0]</a> <a href="../results/extraction-result-2045.html#e2045.1" class="evidence-link">[e2045.1]</a> <a href="../results/extraction-result-2045.html#e2045.2" class="evidence-link">[e2045.2]</a> <a href="../results/extraction-result-2045.html#e2045.3" class="evidence-link">[e2045.3]</a> </li>
    <li>Long-horizon performance is achievable with proper scaffolding: SGRL unlocks deep achievements (Collect Diamond at 3.7M steps vs baselines failing by 5M), CRAFT succeeds on complex coordination (100% Two Arm Lift), WebSynthesis improves multi-step planning (+5.23% from state-transition task), suggesting the theory's 'struggle with long-horizon' claim should be modified to emphasize that hierarchical decomposition, staged curricula, and proper scaffolding enable long-horizon success rather than presenting it as a fundamental limitation. <a href="../results/extraction-result-2049.html#e2049.0" class="evidence-link">[e2049.0]</a> <a href="../results/extraction-result-2043.html#e2043.0" class="evidence-link">[e2043.0]</a> <a href="../results/extraction-result-2052.html#e2052.0" class="evidence-link">[e2052.0]</a> </li>
    <li>Model size and capability significantly affect curriculum generation quality: GPT-4o outperforms GPT-3.5 and GLM4-9B in SDW (T=0.290 vs 0.194 vs 0.134), stronger models benefit differently from curricula (TTC-RL effects vary by model), AURA uses GPT-4.1, and smaller models show degraded performance (Gemma2 69.8% in sparse 6-info), suggesting the theory should include LLM capability as a key factor affecting curriculum effectiveness. <a href="../results/extraction-result-2050.html#e2050.0" class="evidence-link">[e2050.0]</a> <a href="../results/extraction-result-2054.html#e2054.0" class="evidence-link">[e2054.0]</a> <a href="../results/extraction-result-2047.html#e2047.0" class="evidence-link">[e2047.0]</a> <a href="../results/extraction-result-2051.html#e2051.0" class="evidence-link">[e2051.0]</a> </li>
    <li>Diversity mechanisms (batch generation, temperature cycling, persona prompts, novelty bias, explicit diversity objectives) are critical for curriculum quality across multiple implementations (LADDER, SGRL, AURA, EXIF) and their absence leads to repetitive or low-quality curricula, suggesting the theory should provide more specific guidance on diversity-promoting techniques as essential design elements. <a href="../results/extraction-result-2048.html#e2048.0" class="evidence-link">[e2048.0]</a> <a href="../results/extraction-result-2048.html#e2048.2" class="evidence-link">[e2048.2]</a> <a href="../results/extraction-result-2049.html#e2049.0" class="evidence-link">[e2049.0]</a> <a href="../results/extraction-result-2047.html#e2047.0" class="evidence-link">[e2047.0]</a> <a href="../results/extraction-result-2045.html#e2045.0" class="evidence-link">[e2045.0]</a> </li>
    <li>The theory's claim about specialized domains needs refinement: LLM curricula excel in verifiable domains with compositional structure (mathematics: LADDER 82%, TTC-RL 1.8-2.1×, Self-aware RL +53.8%; code generation: TTC-RL, WebSynthesis) and achieve strong results in robotics with proper validation (AURA 99%, CRAFT 90-100%), but struggle when lacking domain-appropriate verification mechanisms, suggesting domain characteristics interact critically with complementary systems rather than being independent factors. <a href="../results/extraction-result-2048.html#e2048.0" class="evidence-link">[e2048.0]</a> <a href="../results/extraction-result-2054.html#e2054.0" class="evidence-link">[e2054.0]</a> <a href="../results/extraction-result-2042.html#e2042.0" class="evidence-link">[e2042.0]</a> <a href="../results/extraction-result-2047.html#e2047.0" class="evidence-link">[e2047.0]</a> <a href="../results/extraction-result-2043.html#e2043.0" class="evidence-link">[e2043.0]</a> <a href="../results/extraction-result-2052.html#e2052.0" class="evidence-link">[e2052.0]</a> </li>
    <li>Iterative refinement and feedback loops are essential for reliability: successful implementations use closed-loop adaptation (CRAFT VLM refinement increases effective curricula ratio, EXIF feedback enables continued improvement while EF plateaus, Self-aware RL difficulty prediction calibrates over 50 steps), suggesting the theory should emphasize iterative refinement over one-shot generation as a key design principle. <a href="../results/extraction-result-2043.html#e2043.0" class="evidence-link">[e2043.0]</a> <a href="../results/extraction-result-2043.html#e2043.2" class="evidence-link">[e2043.2]</a> <a href="../results/extraction-result-2045.html#e2045.0" class="evidence-link">[e2045.0]</a> <a href="../results/extraction-result-2045.html#e2045.2" class="evidence-link">[e2045.2]</a> <a href="../results/extraction-result-2042.html#e2042.0" class="evidence-link">[e2042.0]</a> </li>
    <li>Computational cost has multiple dimensions that interact with effectiveness: generation-time cost (LADDER thousands of variants, WebMCTS offline synthesis), training-time cost (RL steps, environment interactions), and inference-time cost (online queries vs cached masks), and proper design can achieve efficiency (SGRL minimal calls, AURA single launch vs 5-16), suggesting the theory should distinguish between these cost types and their trade-offs. <a href="../results/extraction-result-2049.html#e2049.0" class="evidence-link">[e2049.0]</a> <a href="../results/extraction-result-2047.html#e2047.0" class="evidence-link">[e2047.0]</a> <a href="../results/extraction-result-2048.html#e2048.0" class="evidence-link">[e2048.0]</a> <a href="../results/extraction-result-2052.html#e2052.0" class="evidence-link">[e2052.0]</a> </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>Distinguish between online frequent-query LLM curricula (computationally expensive, often impractical, underperforming) and offline reusable code/function generation (efficient, effective) as fundamentally different paradigms with vastly different cost-benefit profiles. The theory should explicitly recommend offline generation approaches.</li>
                <li>Elevate validation/verification mechanisms from 'complementary systems' to 'essential requirements': schema validation, execution verification, VLM feedback loops, and quality control are necessary for LLM curriculum effectiveness (removing them causes 60-88% drops in success rates), not optional enhancements.</li>
                <li>Add retrieval augmentation (RAG/VDB) as a critical complementary system in the theory's list, alongside skill libraries and execution monitoring, noting that it substantially improves curriculum quality by grounding generation in prior successful examples.</li>
                <li>Explicitly address exploration-first vs proposal-first distinction as a core design choice: environment-grounded curriculum generation (exploration-first with feedback) produces 70-85% valid data vs <30% for proposal-first, making this a fundamental methodological decision rather than an implementation detail.</li>
                <li>Modify the 'struggle with long-horizon planning' claim to: 'LLM curricula can effectively handle long-horizon tasks when combined with hierarchical decomposition, staged curricula, and proper scaffolding (demonstrated by SGRL, CRAFT, WebSynthesis); without these complementary systems, long-horizon performance degrades significantly.'</li>
                <li>Add LLM capability (model size and quality) as a key factor in the theory: curriculum generation quality depends on LLM capability (GPT-4 class models substantially outperform smaller models), and curriculum effectiveness may vary by target model capability (weaker models benefit more from difficulty balancing).</li>
                <li>Expand diversity mechanisms guidance to specify concrete techniques: batch generation, temperature cycling, persona prompts, explicit novelty objectives, and diversity-promoting selection criteria are important techniques for curriculum quality that should be included in curriculum design.</li>
                <li>Refine specialized domain claims: LLM curricula excel in verifiable domains with compositional structure (mathematics, code generation) and can succeed in robotics/manipulation with proper validation mechanisms. The key factor is availability of domain-appropriate verification rather than domain specialization per se.</li>
                <li>Add computational cost considerations with multiple dimensions: distinguish between generation-time cost (curriculum synthesis), training-time cost (RL steps, environment interactions), and inference-time cost (online queries), noting that proper design can achieve efficiency through offline generation, caching, and reusable code.</li>
                <li>Emphasize iterative refinement and feedback loops as essential design principles: successful implementations use closed-loop adaptation (VLM refinement, student feedback, difficulty calibration) rather than one-shot generation, and this should be presented as a core requirement for reliable curriculum generation.</li>
                <li>Modify theory statements about discovery speed gains: specify that 3-10× gains require proper validation and complementary systems; unvalidated approaches show 12-31% success rates and waste computational resources, so the theory should condition performance claims on proper implementation.</li>
                <li>Add a new theory statement about curriculum generation methodology: 'Environment-grounded curriculum generation (exploration-first with iterative feedback) produces 2-3× higher valid training data rates than proposal-first approaches and is essential for domains with complex constraints or missing entities.'</li>
                <li>Revise the domain characteristics component to emphasize the interaction between domain properties and complementary systems: 'LLM curricula are most effective in domains with (1) verifiable outcomes, (2) compositional task structure, and (3) appropriate validation mechanisms; domains lacking any of these require additional complementary systems or may not benefit from LLM curricula.'</li>
                <li>Add a failure modes section to the theory explicitly listing: (1) unvalidated generation (12-31% success), (2) proposal-first without grounding (<30% valid data), (3) off-policy integration (catastrophic collapse), (4) frequent online queries (high cost, underperformance), and (5) insufficient diversity mechanisms (repetitive curricula).</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-22",
    "theory_id": "theory-176",
    "fully_supporting_evidence": [
        {
            "text": "AURA achieves 99% training-launch success with schema validation and retrieval augmentation, compared to 31% for CurricuLLM and 12% for Eureka on complex robotics tasks, demonstrating that properly validated LLM-generated curricula with complementary systems vastly outperform sampling-based approaches and reduce computational waste (single launch vs 5-16 launches per stage).",
            "uuids": [
                "e2047.0",
                "e2047.1",
                "e2047.2",
                "e2047.3"
            ]
        },
        {
            "text": "LADDER demonstrates 1%→82% improvement (82× gain) on undergraduate integration problems using LLM-generated recursive variant trees, with strong generalization from only 10 training problems to 100 test problems, and TTRL further improves MIT Integration Bee performance from 73%→90%, strongly supporting the theory's claims about LLM curriculum effectiveness in compositional domains with verifiable rewards.",
            "uuids": [
                "e2048.0",
                "e2048.1",
                "e2048.2"
            ]
        },
        {
            "text": "TTC-RL achieves ~1.8-2.1× improvements on competition-level math (AIME25: 40%→62% pass@8) and coding (CodeElo: 28%→43% pass@8) using LLM-embedding-based curriculum selection (SIFT), substantially outperforming uniform sampling and domain-only heuristics, confirming that LLM world knowledge enables superior task selection and curriculum construction.",
            "uuids": [
                "e2054.0",
                "e2054.1"
            ]
        },
        {
            "text": "SGRL outperforms frequent-query LLM baselines (score 33.8±1.5 vs AdaRefiner 28.5±2.3, ELLM 28.4±2.5) while requiring minimal LLM invocations through reusable goal-generation code, and unlocks deep achievements (Collect Diamond at ~3.7M steps) that baselines fail to reach by 5M steps, supporting the theory's emphasis on structured LLM outputs and complementary systems over naive online querying.",
            "uuids": [
                "e2049.0",
                "e2049.1",
                "e2049.2",
                "e2049.3",
                "e2049.4"
            ]
        },
        {
            "text": "EXIF's exploration-first + iterative feedback approach yields large improvements (Webshop reward +29.4 to +51.7 for different models, Crafter AP +18.8 to +20.5) and produces 70-85% valid training data vs &lt;30% for proposal-first LLM curricula, demonstrating that environment-grounded LLM curriculum generation with state conditioning is essential and that naive LLM task proposals fail catastrophically.",
            "uuids": [
                "e2045.0",
                "e2045.1",
                "e2045.2",
                "e2045.3"
            ]
        },
        {
            "text": "WebSynthesis achieves 20.15% Pass@3 with ~4k LLM-assisted synthetic samples, outperforming OS-Genesis (18.66%) trained on 7.4k real trajectories and AgentTrek (11.94%) with 20k tutorial samples, confirming LLM-generated curricula can be more sample-efficient than manual/heuristic approaches when combining world-model guidance (WebMCTS) with proper warm-up curricula.",
            "uuids": [
                "e2052.0",
                "e2052.1",
                "e2052.2",
                "e2052.3",
                "e2052.4"
            ]
        },
        {
            "text": "Retrieval augmentation (RAG/VDB) is critical for LLM curriculum effectiveness across multiple implementations: removing VDB in AURA reduced success from 99%→38%, DSMentor's retrieval-augmented memory substantially improved performance, and HierQ-LLM's RAG-based context extraction achieved 100% accuracy, strongly supporting the theory's emphasis on complementary systems.",
            "uuids": [
                "e2047.3",
                "e2046.0",
                "e2051.0"
            ]
        },
        {
            "text": "Self-aware RL achieves +53.8% relative gain on math benchmarks (21.0→32.3 average) using LLM-generated tasks with difficulty prediction while querying external guidance on only 1.23% of tasks, demonstrating that LLM-generated curricula with self-awareness and minimal external supervision can produce large improvements in specialized reasoning domains.",
            "uuids": [
                "e2042.0",
                "e2042.1"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "CRAFT achieves high success rates (90% Gate, 60% Seesaw, 100% Two Arm Lift) with LLM-generated curricula and VLM-guided refinement, and demonstrates zero-shot sim-to-real transfer (65% hardware success), but requires multiple attempts due to foundation model stochasticity and depends heavily on the VLM feedback loop, partially supporting the theory while highlighting that reliability requires iterative refinement mechanisms.",
            "uuids": [
                "e2043.0",
                "e2043.1",
                "e2043.2"
            ]
        },
        {
            "text": "DSMentor shows improvements (+5.2% DSEval, +8.8% QRData causal) with LLM problem-based difficulty assessment, but problem-based curriculum sometimes only marginally outperforms or is matched by heuristic difficulty metrics (pass-rate, reference-code) in specific cases, suggesting LLM advantage is context-dependent and most pronounced in complex/specialized domains.",
            "uuids": [
                "e2046.0",
                "e2046.1",
                "e2046.2"
            ]
        },
        {
            "text": "A-TTC (achievability-balanced curriculum) improves learning for weaker models (Qwen3-0.6B) by combining LLM-based selection (SIFT) with online difficulty estimates, but the benefit is model-dependent and requires hyperparameter tuning ([0.2,0.6] achievability interval), partially supporting the theory's claim about state conditioning while showing that curriculum design must adapt to model capability.",
            "uuids": [
                "e2054.2"
            ]
        },
        {
            "text": "SDW uses LLM-generated functions offline to drive curriculum weighting in lifelong RL, achieving positive transfer (T=0.290) and reduced forgetting (F=-0.505) compared to baselines, but performance varies significantly by LLM quality (GPT-4o &gt; GPT-3.5 &gt; GLM4-9B), supporting the theory while showing clear model-size dependency in curriculum generation quality.",
            "uuids": [
                "e2050.0"
            ]
        },
        {
            "text": "HierQ-LLM demonstrates that LLM guidance (context extraction and exploration facilitation) substantially improves hierarchical RL performance (MSR up to 83.4% vs near-0% for flat agents), but effectiveness depends on model selection (Hermes3 excels in 3-info, Llama3.1 in 6-info) and requires integration with hierarchical decomposition, showing LLM curricula are most effective when combined with appropriate architectural support.",
            "uuids": [
                "e2051.0",
                "e2051.1",
                "e2051.2"
            ]
        },
        {
            "text": "SEC (learned curriculum using advantage signals) outperforms random and manual curricula with substantial relative gains (+13% Countdown, +21% Zebra, +22% ARC-1D, +33% AIME24) without using LLM generation, showing that adaptive non-LLM curricula can be highly effective, but SEC requires predefined problem categories and doesn't leverage world knowledge for task generation.",
            "uuids": [
                "e2053.0",
                "e2053.1",
                "e2053.2"
            ]
        }
    ],
    "fully_contradicting_evidence": [
        {
            "text": "CurricuLLM and Eureka achieve only 31% and 12% training-launch success respectively on complex robotics tasks due to malformed LLM generations and lack of validation, requiring ~5-16 parallel launches per stage and massive computational waste, directly contradicting any claim of universal LLM curriculum superiority and demonstrating that unvalidated LLM generation is highly ineffective.",
            "uuids": [
                "e2047.1",
                "e2047.2"
            ]
        },
        {
            "text": "TTC-SFT (supervised fine-tuning on LLM-selected curricula) causes catastrophic initial accuracy collapse to near 0% and requires hundreds of gradient steps to recover, demonstrating that LLM-generated curricula can fail catastrophically when improperly integrated (off-policy distribution shift), directly contradicting claims of general superiority without proper integration methods.",
            "uuids": [
                "e2054.4"
            ]
        },
        {
            "text": "Proposal-first LLM curricula produce &lt;30% valid training data in both Webshop and Crafter due to lack of environment grounding (LLM proposes infeasible tasks), substantially underperforming exploration-first approaches (70-85% valid data), directly contradicting any claim that naive LLM task generation is effective and demonstrating that environment grounding is essential.",
            "uuids": [
                "e2045.1",
                "e2045.3"
            ]
        }
    ],
    "partially_contradicting_evidence": [
        {
            "text": "ELLM and AdaRefiner require frequent expensive LLM queries during training with substantial computational cost and underperform SGRL (scores ~28.4-28.5 vs 33.8) despite using similar LLM guidance, partially contradicting the theory's implicit assumption that LLM curriculum generation is computationally practical and showing that implementation approach (frequent queries vs reusable code) critically affects both cost and effectiveness.",
            "uuids": [
                "e2049.2",
                "e2049.3"
            ]
        },
        {
            "text": "TTRL (test-time RL with LLM-generated variants) fails to solve any problems when applied to base models without prior LADDER training (0 solved after 100 steps), showing LLM-generated curricula alone are insufficient without proper foundation and partially contradicting claims of standalone effectiveness, though TTRL is highly effective when combined with prior curriculum exposure (73%→90%).",
            "uuids": [
                "e2048.1"
            ]
        },
        {
            "text": "LADDER variant generation produces ~8% unsolvable variants and some variants that are harder than intended due to small coefficient changes, showing LLM curriculum generation can produce low-quality or counterproductive tasks, partially contradicting claims of consistent superiority and highlighting the need for quality control and verification mechanisms.",
            "uuids": [
                "e2048.2"
            ]
        },
        {
            "text": "Manual/heuristic curricula can match LLM performance in specific contexts: pass-rate difficulty matched problem-based LLM difficulty on DSEval-LeetCode for one model, and manual staged curricula (TiZero, eligibility traces) are effective in well-understood domains, partially contradicting the theory's claim of universal LLM superiority and suggesting domain structure and understanding matter.",
            "uuids": [
                "e2046.1",
                "e2044.4",
                "e2044.5"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "The distinction between online frequent-query LLM methods (ELLM, AdaRefiner - expensive, underperforming) and offline reusable code generation (SGRL, AURA - efficient, effective) is fundamental: offline approaches achieve comparable or better results with orders of magnitude fewer LLM calls, suggesting the theory should explicitly distinguish between these paradigms as different curriculum generation strategies with vastly different cost-benefit profiles.",
            "uuids": [
                "e2049.0",
                "e2049.2",
                "e2049.3",
                "e2047.0"
            ]
        },
        {
            "text": "Validation and verification mechanisms are not merely complementary but essential requirements: schema validation (AURA 99%→38% without), execution verification (LADDER, TTC-RL), VLM feedback loops (CRAFT), and quality control are necessary for LLM curriculum effectiveness, suggesting these should be elevated from 'complementary systems' to 'essential requirements' in the theory.",
            "uuids": [
                "e2047.0",
                "e2047.3",
                "e2043.0",
                "e2043.2",
                "e2048.0",
                "e2048.2",
                "e2054.0"
            ]
        },
        {
            "text": "Retrieval augmentation (RAG/VDB) emerges as a critical complementary system across multiple successful implementations (AURA, DSMentor, HierQ-LLM) that substantially improves curriculum quality by grounding generation in prior successful examples, and should be explicitly included in the theory's list of key complementary systems alongside skill libraries and execution monitoring.",
            "uuids": [
                "e2047.3",
                "e2046.0",
                "e2051.0"
            ]
        },
        {
            "text": "Exploration-first vs proposal-first is a fundamental distinction for curriculum validity: exploration-first with feedback achieves 70-85% valid data vs &lt;30% for proposal-first, and EXIF substantially outperforms proposal-first baselines, suggesting the theory should explicitly address curriculum generation methodology and environment grounding as a core design choice rather than treating all LLM generation as equivalent.",
            "uuids": [
                "e2045.0",
                "e2045.1",
                "e2045.2",
                "e2045.3"
            ]
        },
        {
            "text": "Long-horizon performance is achievable with proper scaffolding: SGRL unlocks deep achievements (Collect Diamond at 3.7M steps vs baselines failing by 5M), CRAFT succeeds on complex coordination (100% Two Arm Lift), WebSynthesis improves multi-step planning (+5.23% from state-transition task), suggesting the theory's 'struggle with long-horizon' claim should be modified to emphasize that hierarchical decomposition, staged curricula, and proper scaffolding enable long-horizon success rather than presenting it as a fundamental limitation.",
            "uuids": [
                "e2049.0",
                "e2043.0",
                "e2052.0"
            ]
        },
        {
            "text": "Model size and capability significantly affect curriculum generation quality: GPT-4o outperforms GPT-3.5 and GLM4-9B in SDW (T=0.290 vs 0.194 vs 0.134), stronger models benefit differently from curricula (TTC-RL effects vary by model), AURA uses GPT-4.1, and smaller models show degraded performance (Gemma2 69.8% in sparse 6-info), suggesting the theory should include LLM capability as a key factor affecting curriculum effectiveness.",
            "uuids": [
                "e2050.0",
                "e2054.0",
                "e2047.0",
                "e2051.0"
            ]
        },
        {
            "text": "Diversity mechanisms (batch generation, temperature cycling, persona prompts, novelty bias, explicit diversity objectives) are critical for curriculum quality across multiple implementations (LADDER, SGRL, AURA, EXIF) and their absence leads to repetitive or low-quality curricula, suggesting the theory should provide more specific guidance on diversity-promoting techniques as essential design elements.",
            "uuids": [
                "e2048.0",
                "e2048.2",
                "e2049.0",
                "e2047.0",
                "e2045.0"
            ]
        },
        {
            "text": "The theory's claim about specialized domains needs refinement: LLM curricula excel in verifiable domains with compositional structure (mathematics: LADDER 82%, TTC-RL 1.8-2.1×, Self-aware RL +53.8%; code generation: TTC-RL, WebSynthesis) and achieve strong results in robotics with proper validation (AURA 99%, CRAFT 90-100%), but struggle when lacking domain-appropriate verification mechanisms, suggesting domain characteristics interact critically with complementary systems rather than being independent factors.",
            "uuids": [
                "e2048.0",
                "e2054.0",
                "e2042.0",
                "e2047.0",
                "e2043.0",
                "e2052.0"
            ]
        },
        {
            "text": "Iterative refinement and feedback loops are essential for reliability: successful implementations use closed-loop adaptation (CRAFT VLM refinement increases effective curricula ratio, EXIF feedback enables continued improvement while EF plateaus, Self-aware RL difficulty prediction calibrates over 50 steps), suggesting the theory should emphasize iterative refinement over one-shot generation as a key design principle.",
            "uuids": [
                "e2043.0",
                "e2043.2",
                "e2045.0",
                "e2045.2",
                "e2042.0"
            ]
        },
        {
            "text": "Computational cost has multiple dimensions that interact with effectiveness: generation-time cost (LADDER thousands of variants, WebMCTS offline synthesis), training-time cost (RL steps, environment interactions), and inference-time cost (online queries vs cached masks), and proper design can achieve efficiency (SGRL minimal calls, AURA single launch vs 5-16), suggesting the theory should distinguish between these cost types and their trade-offs.",
            "uuids": [
                "e2049.0",
                "e2047.0",
                "e2048.0",
                "e2052.0"
            ]
        }
    ],
    "suggested_revisions": [
        "Distinguish between online frequent-query LLM curricula (computationally expensive, often impractical, underperforming) and offline reusable code/function generation (efficient, effective) as fundamentally different paradigms with vastly different cost-benefit profiles. The theory should explicitly recommend offline generation approaches.",
        "Elevate validation/verification mechanisms from 'complementary systems' to 'essential requirements': schema validation, execution verification, VLM feedback loops, and quality control are necessary for LLM curriculum effectiveness (removing them causes 60-88% drops in success rates), not optional enhancements.",
        "Add retrieval augmentation (RAG/VDB) as a critical complementary system in the theory's list, alongside skill libraries and execution monitoring, noting that it substantially improves curriculum quality by grounding generation in prior successful examples.",
        "Explicitly address exploration-first vs proposal-first distinction as a core design choice: environment-grounded curriculum generation (exploration-first with feedback) produces 70-85% valid data vs &lt;30% for proposal-first, making this a fundamental methodological decision rather than an implementation detail.",
        "Modify the 'struggle with long-horizon planning' claim to: 'LLM curricula can effectively handle long-horizon tasks when combined with hierarchical decomposition, staged curricula, and proper scaffolding (demonstrated by SGRL, CRAFT, WebSynthesis); without these complementary systems, long-horizon performance degrades significantly.'",
        "Add LLM capability (model size and quality) as a key factor in the theory: curriculum generation quality depends on LLM capability (GPT-4 class models substantially outperform smaller models), and curriculum effectiveness may vary by target model capability (weaker models benefit more from difficulty balancing).",
        "Expand diversity mechanisms guidance to specify concrete techniques: batch generation, temperature cycling, persona prompts, explicit novelty objectives, and diversity-promoting selection criteria are important techniques for curriculum quality that should be included in curriculum design.",
        "Refine specialized domain claims: LLM curricula excel in verifiable domains with compositional structure (mathematics, code generation) and can succeed in robotics/manipulation with proper validation mechanisms. The key factor is availability of domain-appropriate verification rather than domain specialization per se.",
        "Add computational cost considerations with multiple dimensions: distinguish between generation-time cost (curriculum synthesis), training-time cost (RL steps, environment interactions), and inference-time cost (online queries), noting that proper design can achieve efficiency through offline generation, caching, and reusable code.",
        "Emphasize iterative refinement and feedback loops as essential design principles: successful implementations use closed-loop adaptation (VLM refinement, student feedback, difficulty calibration) rather than one-shot generation, and this should be presented as a core requirement for reliable curriculum generation.",
        "Modify theory statements about discovery speed gains: specify that 3-10× gains require proper validation and complementary systems; unvalidated approaches show 12-31% success rates and waste computational resources, so the theory should condition performance claims on proper implementation.",
        "Add a new theory statement about curriculum generation methodology: 'Environment-grounded curriculum generation (exploration-first with iterative feedback) produces 2-3× higher valid training data rates than proposal-first approaches and is essential for domains with complex constraints or missing entities.'",
        "Revise the domain characteristics component to emphasize the interaction between domain properties and complementary systems: 'LLM curricula are most effective in domains with (1) verifiable outcomes, (2) compositional task structure, and (3) appropriate validation mechanisms; domains lacking any of these require additional complementary systems or may not benefit from LLM curricula.'",
        "Add a failure modes section to the theory explicitly listing: (1) unvalidated generation (12-31% success), (2) proposal-first without grounding (&lt;30% valid data), (3) off-policy integration (catastrophic collapse), (4) frequent online queries (high cost, underperformance), and (5) insufficient diversity mechanisms (repetitive curricula)."
    ],
    "overall_support_or_contradict": "support",
    "overall_support_or_contradict_explanation": "The new evidence strongly supports the core theory that properly implemented LLM-generated curricula outperform hand-designed and heuristic approaches (1.8-2.1× to 82× improvements across diverse domains), but reveals that effectiveness critically depends on validation mechanisms, retrieval augmentation, and appropriate integration methods—naive LLM curriculum generation fails catastrophically (12-31% success rates, &lt;30% valid data), requiring significant theory refinement to distinguish effective from ineffective approaches.",
    "revised_theory_ids": [
        "theory-383"
    ],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
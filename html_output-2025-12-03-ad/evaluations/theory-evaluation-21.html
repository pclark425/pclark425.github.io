<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-21 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-21</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-21</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-339.html">theory-339</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-381.html">theory-381</a> <a href="../theories/theory-382.html">theory-382</a></p>
                <p><strong>Overall Support or Contradict:</strong> neutral</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The evidence shows LLM-driven curricula can achieve dramatic improvements over naive baselines (0-10% → 60-100%) in verifiable domains, supporting core claims. However, there is a critical scope mismatch (most evidence is NOT in the theory's specified 'interactive text environments for commonsense/science procedures'), and multiple non-LLM methods achieved comparable or better results with lower overhead, suggesting the advantage is domain- and baseline-dependent rather than universal.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>CRAFT demonstrated that LLM-generated curricula with VLM-guided refinement achieved 90-100% success rates on multi-robot coordination tasks where baseline methods (env_reward, example_reward) achieved 0-10% success, and policies transferred to real hardware with 65% success, showing LLMs can generate effective staged curricula for compositional tasks. <a href="../results/extraction-result-2030.html#e2030.0" class="evidence-link">[e2030.0]</a> <a href="../results/extraction-result-2030.html#e2030.1" class="evidence-link">[e2030.1]</a> <a href="../results/extraction-result-2030.html#e2030.2" class="evidence-link">[e2030.2]</a> <a href="../results/extraction-result-2030.html#e2030.4" class="evidence-link">[e2030.4]</a> </li>
    <li>LADDER showed LLM-generated recursive variant curricula enabled 82% test accuracy on compositional integration problems compared to 1-2% for pass@k sampling and complete collapse (0%) for RL without variants, demonstrating LLMs can automatically discover effective pedagogical orderings that would require extensive trial-and-error to design manually. <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> <a href="../results/extraction-result-2034.html#e2034.2" class="evidence-link">[e2034.2]</a> </li>
    <li>Self-aware RL with LLM-generated tasks and difficulty prediction achieved 53.8% relative improvement on mathematical reasoning benchmarks while using minimal external guidance (1.23% of tasks), demonstrating LLMs can dynamically adapt curriculum difficulty based on learner performance signals. <a href="../results/extraction-result-2035.html#e2035.0" class="evidence-link">[e2035.0]</a> </li>
    <li>COGENT demonstrated that LLMs leveraging curriculum structure (NGSS concept→core idea→learning objective) significantly improved curriculum alignment (4.62 vs 4.08 for baseline, 4.15 vs 3.49 for human-written) and better controlled readability, showing LLMs can leverage pre-trained knowledge about task structure and conceptual relationships. <a href="../results/extraction-result-2031.html#e2031.0" class="evidence-link">[e2031.0]</a> </li>
    <li>CRAFT Curriculum LLM explicitly produced prerequisite subtasks (approach→align→grasp→lift) that enabled learning where flat training failed, and LADDER generated intermediate variants forming prerequisite chains, demonstrating LLMs can identify and sequence compositional sub-skills in ways that facilitate transfer. <a href="../results/extraction-result-2030.html#e2030.1" class="evidence-link">[e2030.1]</a> <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> </li>
    <li>Theoretical analysis (CRL Theory) provides formal justification showing curriculum RL can require fewer total samples than direct learning when curriculum tasks are appropriately chosen, with specific conditions under which sample complexity is reduced, supporting the theory's claim about faster learning. <a href="../results/extraction-result-2036.html#e2036.7" class="evidence-link">[e2036.7]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>EvoCurr showed LLM-driven self-evolving curriculum can achieve task mastery on complex StarCraft II micro-management requiring compositional skills, but only 1 of 5 autonomous runs (20%) succeeded, demonstrating capability but with high variability from LLM stochasticity. <a href="../results/extraction-result-2028.html#e2028.0" class="evidence-link">[e2028.0]</a> <a href="../results/extraction-result-2028.html#e2028.1" class="evidence-link">[e2028.1]</a> </li>
    <li>TTC-RL using LLM embeddings (SIFT) achieved substantial improvements (1.8-2.1x on hard benchmarks), demonstrating LLMs can leverage pre-trained representations to identify relevant tasks, though the curriculum generation relied on embedding-based selection rather than explicit LLM task generation. <a href="../results/extraction-result-2038.html#e2038.0" class="evidence-link">[e2038.0]</a> <a href="../results/extraction-result-2038.html#e2038.1" class="evidence-link">[e2038.1]</a> </li>
    <li>TTRL extended LADDER with per-question variant generation at test-time, improving accuracy from 73% to 90%, showing LLMs can generate novel intermediate tasks that bridge knowledge gaps, but required prior LADDER training and failed when applied to base models. <a href="../results/extraction-result-2034.html#e2034.1" class="evidence-link">[e2034.1]</a> </li>
    <li>UI Predefined study showed LLM-assisted curriculum design with proper interface achieved highest usability (SUS 86.75) and lowest workload (2.25), demonstrating LLMs can assist curriculum design effectively when properly scaffolded, though this is human-in-the-loop rather than fully automated. <a href="../results/extraction-result-2032.html#e2032.0" class="evidence-link">[e2032.0]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<ol>
    <li>E2H Reasoner achieved strong improvements (21.1% to 53.3% on hard tasks, substantial OOD gains) using simple difficulty-based ordering with probabilistic scheduling without any LLM-generated curricula, demonstrating that rule-based methods can achieve comparable or better results than LLM approaches for compositional reasoning tasks. <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> <a href="../results/extraction-result-2036.html#e2036.1" class="evidence-link">[e2036.1]</a> <a href="../results/extraction-result-2036.html#e2036.2" class="evidence-link">[e2036.2]</a> </li>
    <li>SEC (Self-Evolving Curriculum) achieved 13-33% relative improvements using a non-stationary MAB with absolute advantage as reward, without LLM task generation, showing effective adaptive curriculum learning through algorithmic selection alone can match or exceed LLM-driven approaches. <a href="../results/extraction-result-2037.html#e2037.0" class="evidence-link">[e2037.0]</a> <a href="../results/extraction-result-2037.html#e2037.1" class="evidence-link">[e2037.1]</a> </li>
    <li>Synthetic-distractor curricula using simple rule-based distractor-count difficulty levels improved joint F1 by 3-8 points across multiple datasets without LLM involvement, and Min-Max curriculum outperformed Linear despite being simpler, contradicting the claim that LLMs discover superior orderings. <a href="../results/extraction-result-2029.html#e2029.0" class="evidence-link">[e2029.0]</a> <a href="../results/extraction-result-2029.html#e2029.2" class="evidence-link">[e2029.2]</a> </li>
    <li>A-TTC (Achievability Test-Time Curricula) improved learning on weaker models by filtering tasks to intermediate difficulty (50% success rate) using online achievability estimates without LLM generation, showing difficulty-aware selection is effective without LLMs and can be more computationally efficient. <a href="../results/extraction-result-2038.html#e2038.3" class="evidence-link">[e2038.3]</a> </li>
</ol>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>LADDER generated approximately 8% unsolvable variants and many harder-than-intended variants, requiring substantial quality control and wasted verification compute, contradicting the claim that LLMs can reliably generate appropriate difficulty progressions without extensive domain expertise. <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> <a href="../results/extraction-result-2034.html#e2034.2" class="evidence-link">[e2034.2]</a> </li>
    <li>EvoCurr suffered from LLM hallucination of non-existent or deprecated API calls causing runtime exceptions, and showed bias toward certain unit types with uneven sophistication, demonstrating significant LLM limitations in generating executable curricula and contradicting claims about minimal domain engineering needs. <a href="../results/extraction-result-2028.html#e2028.0" class="evidence-link">[e2028.0]</a> <a href="../results/extraction-result-2028.html#e2028.2" class="evidence-link">[e2028.2]</a> </li>
    <li>Self-aware RL showed LLMs were initially poor at predicting their own success rates (accuracy ~0.2 initially, rising to >0.6 after 50 steps) and required disabling limit breaking during early steps, contradicting the claim that LLMs can leverage pre-trained knowledge without extensive domain-specific fine-tuning. <a href="../results/extraction-result-2035.html#e2035.0" class="evidence-link">[e2035.0]</a> </li>
    <li>COGENT BASE (naive LLM prompting) frequently exceeded target readability levels by ~2.5 grades and produced up to 26.1% more unique words than human references, showing LLMs without curriculum conditioning produce poorly calibrated outputs, contradicting claims about LLMs' ability to generate contextually appropriate sequences without extensive prompting engineering. <a href="../results/extraction-result-2031.html#e2031.1" class="evidence-link">[e2031.1]</a> </li>
    <li>TTC-SFT (supervised fine-tuning on LLM-selected curricula) showed catastrophic initial performance drops and unstable behavior, demonstrating that off-policy training on LLM-curated curricula can be fragile, contradicting claims about LLM-driven curricula achieving faster learning. <a href="../results/extraction-result-2038.html#e2038.5" class="evidence-link">[e2038.5]</a> </li>
    <li>Accuracy-based curriculum partitioning showed that training on base-answerable samples consistently outperformed training on base-unanswerable samples, and that curriculum ordering within linear schedules had minimal impact, suggesting simpler heuristics can be as effective as sophisticated LLM-generated orderings. <a href="../results/extraction-result-2029.html#e2029.1" class="evidence-link">[e2029.1]</a> <a href="../results/extraction-result-2029.html#e2029.3" class="evidence-link">[e2029.3]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>CRITICAL SCOPE MISMATCH: The theory specifically targets 'compositional acquisition of commonsense and science procedures in interactive text environments', but most evidence is from math, code, robotics, or educational text generation domains. Only limited evidence exists in the theory's specified domain, suggesting the theory's scope should be either narrowed to verifiable task domains or the claims should be marked as speculative for the original target domain. <a href="../results/extraction-result-2030.html#e2030.0" class="evidence-link">[e2030.0]</a> <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> <a href="../results/extraction-result-2035.html#e2035.0" class="evidence-link">[e2035.0]</a> <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> <a href="../results/extraction-result-2037.html#e2037.0" class="evidence-link">[e2037.0]</a> <a href="../results/extraction-result-2031.html#e2031.0" class="evidence-link">[e2031.0]</a> </li>
    <li>The most successful LLM-driven curricula combined LLM generation with other mechanisms (VLM refinement in CRAFT, difficulty prediction in Self-aware RL, embedding-based selection in TTC-RL), suggesting the theory should emphasize 'hybrid LLM-augmented approaches' rather than pure LLM generation as the key to superiority. <a href="../results/extraction-result-2030.html#e2030.0" class="evidence-link">[e2030.0]</a> <a href="../results/extraction-result-2030.html#e2030.2" class="evidence-link">[e2030.2]</a> <a href="../results/extraction-result-2035.html#e2035.0" class="evidence-link">[e2035.0]</a> <a href="../results/extraction-result-2038.html#e2038.0" class="evidence-link">[e2038.0]</a> </li>
    <li>Non-LLM methods (E2H, SEC, A-TTC) achieved strong results with 'negligible computational overhead' while LLM methods often did not report costs or noted 'substantial wasted compute', suggesting computational efficiency and reliability should be added as key evaluation criteria and potential limitations rather than assumed advantages. <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> <a href="../results/extraction-result-2037.html#e2037.0" class="evidence-link">[e2037.0]</a> <a href="../results/extraction-result-2038.html#e2038.3" class="evidence-link">[e2038.3]</a> <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> </li>
    <li>LLM-generated curricula showed high variability and stochasticity (CRAFT effective curricula ratio varied, EvoCurr 20% success rate, LADDER 8% unsolvable variants), suggesting reliability and consistency should be explicitly addressed as major limitations rather than being unaccounted for, and multiple attempts or refinement mechanisms are necessary. <a href="../results/extraction-result-2030.html#e2030.0" class="evidence-link">[e2030.0]</a> <a href="../results/extraction-result-2028.html#e2028.0" class="evidence-link">[e2028.0]</a> <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> </li>
    <li>Evidence shows both LLM-driven curricula and non-LLM methods can identify prerequisites and generate intermediate tasks effectively (E2H using difficulty labels, A-TTC using achievability estimates, SEC using advantage signals), suggesting prerequisite identification is not a unique LLM advantage but rather a property of well-designed curriculum systems regardless of generation method. <a href="../results/extraction-result-2030.html#e2030.1" class="evidence-link">[e2030.1]</a> <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> <a href="../results/extraction-result-2038.html#e2038.3" class="evidence-link">[e2038.3]</a> <a href="../results/extraction-result-2037.html#e2037.0" class="evidence-link">[e2037.0]</a> </li>
    <li>The theory claims LLMs can generate curricula 'with minimal domain-specific fine-tuning', but Self-aware RL required 200 training steps with initially poor difficulty prediction, LADDER required prior training for TTRL to work, and COGENT required careful prompt engineering, suggesting substantial domain adaptation and engineering is needed, contradicting the 'minimal engineering' claim. <a href="../results/extraction-result-2035.html#e2035.0" class="evidence-link">[e2035.0]</a> <a href="../results/extraction-result-2034.html#e2034.1" class="evidence-link">[e2034.1]</a> <a href="../results/extraction-result-2031.html#e2031.0" class="evidence-link">[e2031.0]</a> </li>
    <li>The magnitude of LLM curriculum advantage is highly baseline-dependent: dramatic when baselines completely fail (0-10% → 60-100% in CRAFT, 1-2% → 82% in LADDER) but modest or absent when compared to well-designed rule-based curricula (E2H, SEC), suggesting the theory should specify that superiority is relative to naive/random baselines rather than all alternative methods. <a href="../results/extraction-result-2030.html#e2030.0" class="evidence-link">[e2030.0]</a> <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> <a href="../results/extraction-result-2037.html#e2037.0" class="evidence-link">[e2037.0]</a> </li>
    <li>Model scale effects are underexplored: A-TTC was specifically helpful for weaker models (Qwen3-0.6B), and several studies used different model sizes with varying results, suggesting the theory's claim about 'quality correlating with model scale' needs more nuanced treatment of when and how scale matters. <a href="../results/extraction-result-2038.html#e2038.3" class="evidence-link">[e2038.3]</a> <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> <a href="../results/extraction-result-2037.html#e2037.0" class="evidence-link">[e2037.0]</a> </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>CRITICAL: Add explicit scope limitation noting that evidence is primarily from math, code, and robotics domains rather than the theory's specified 'commonsense and science procedures in interactive text environments', and mark claims for the original domain as speculative pending further evidence.</li>
                <li>Revise the superiority claim from 'LLM-generated curricula are superior' to 'Hybrid LLM-augmented approaches (LLM generation + VLM refinement / embedding selection / difficulty prediction) can achieve superior results compared to naive baselines but may not outperform well-designed rule-based or algorithmic curricula'.</li>
                <li>Add explicit statements about reliability and stochasticity as major limitations: 'LLM-generated curricula show high variability (20-100% success rates across runs) and require multiple attempts, refinement mechanisms, or quality control to achieve reliable results'.</li>
                <li>Revise the claim about 'minimal domain-specific fine-tuning' to 'LLM curriculum generation often requires substantial prompt engineering, domain-specific training (e.g., 200 steps for difficulty prediction), or prior exposure to related tasks to be effective'.</li>
                <li>Add computational cost and sample efficiency as key evaluation dimensions, noting that 'LLM methods can incur substantial computational overhead from quality control needs (e.g., 8% unsolvable variants, API hallucinations) while non-LLM methods can achieve comparable results with negligible overhead'.</li>
                <li>Modify the prerequisite identification claim to 'Both LLM-driven and well-designed non-LLM methods (difficulty labels, achievability estimates, advantage signals) can effectively identify prerequisites and task dependencies; this is a property of good curriculum design rather than a unique LLM capability'.</li>
                <li>Add a statement that 'LLM superiority is most pronounced when baseline methods completely fail (0-10% success) but diminishes or disappears when comparing to well-designed rule-based or algorithmic curricula (E2H, SEC, A-TTC) that can achieve comparable or better results with lower overhead'.</li>
                <li>Clarify that 'dynamic adaptation' requires additional mechanisms beyond base LLM generation (e.g., performance feedback loops, difficulty prediction training, VLM evaluation, online achievability estimation) and is not an inherent property of LLM-generated curricula without these augmentations.</li>
                <li>Add to 'unaccounted_for' section: reliability/consistency metrics, computational cost comparisons, baseline quality dependencies, and the distinction between LLM task generation vs LLM-assisted selection/refinement.</li>
                <li>Revise model scale claim to be more nuanced: 'The relationship between model scale and curriculum quality is complex and context-dependent; larger models may generate better curricula but smaller models with appropriate difficulty filtering (A-TTC) can also be effective, and the optimal approach may vary by task domain and learner capability'.</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-21",
    "theory_id": "theory-339",
    "fully_supporting_evidence": [
        {
            "text": "CRAFT demonstrated that LLM-generated curricula with VLM-guided refinement achieved 90-100% success rates on multi-robot coordination tasks where baseline methods (env_reward, example_reward) achieved 0-10% success, and policies transferred to real hardware with 65% success, showing LLMs can generate effective staged curricula for compositional tasks.",
            "uuids": [
                "e2030.0",
                "e2030.1",
                "e2030.2",
                "e2030.4"
            ]
        },
        {
            "text": "LADDER showed LLM-generated recursive variant curricula enabled 82% test accuracy on compositional integration problems compared to 1-2% for pass@k sampling and complete collapse (0%) for RL without variants, demonstrating LLMs can automatically discover effective pedagogical orderings that would require extensive trial-and-error to design manually.",
            "uuids": [
                "e2034.0",
                "e2034.2"
            ]
        },
        {
            "text": "Self-aware RL with LLM-generated tasks and difficulty prediction achieved 53.8% relative improvement on mathematical reasoning benchmarks while using minimal external guidance (1.23% of tasks), demonstrating LLMs can dynamically adapt curriculum difficulty based on learner performance signals.",
            "uuids": [
                "e2035.0"
            ]
        },
        {
            "text": "COGENT demonstrated that LLMs leveraging curriculum structure (NGSS concept→core idea→learning objective) significantly improved curriculum alignment (4.62 vs 4.08 for baseline, 4.15 vs 3.49 for human-written) and better controlled readability, showing LLMs can leverage pre-trained knowledge about task structure and conceptual relationships.",
            "uuids": [
                "e2031.0"
            ]
        },
        {
            "text": "CRAFT Curriculum LLM explicitly produced prerequisite subtasks (approach→align→grasp→lift) that enabled learning where flat training failed, and LADDER generated intermediate variants forming prerequisite chains, demonstrating LLMs can identify and sequence compositional sub-skills in ways that facilitate transfer.",
            "uuids": [
                "e2030.1",
                "e2034.0"
            ]
        },
        {
            "text": "Theoretical analysis (CRL Theory) provides formal justification showing curriculum RL can require fewer total samples than direct learning when curriculum tasks are appropriately chosen, with specific conditions under which sample complexity is reduced, supporting the theory's claim about faster learning.",
            "uuids": [
                "e2036.7"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "EvoCurr showed LLM-driven self-evolving curriculum can achieve task mastery on complex StarCraft II micro-management requiring compositional skills, but only 1 of 5 autonomous runs (20%) succeeded, demonstrating capability but with high variability from LLM stochasticity.",
            "uuids": [
                "e2028.0",
                "e2028.1"
            ]
        },
        {
            "text": "TTC-RL using LLM embeddings (SIFT) achieved substantial improvements (1.8-2.1x on hard benchmarks), demonstrating LLMs can leverage pre-trained representations to identify relevant tasks, though the curriculum generation relied on embedding-based selection rather than explicit LLM task generation.",
            "uuids": [
                "e2038.0",
                "e2038.1"
            ]
        },
        {
            "text": "TTRL extended LADDER with per-question variant generation at test-time, improving accuracy from 73% to 90%, showing LLMs can generate novel intermediate tasks that bridge knowledge gaps, but required prior LADDER training and failed when applied to base models.",
            "uuids": [
                "e2034.1"
            ]
        },
        {
            "text": "UI Predefined study showed LLM-assisted curriculum design with proper interface achieved highest usability (SUS 86.75) and lowest workload (2.25), demonstrating LLMs can assist curriculum design effectively when properly scaffolded, though this is human-in-the-loop rather than fully automated.",
            "uuids": [
                "e2032.0"
            ]
        }
    ],
    "fully_contradicting_evidence": [
        {
            "text": "E2H Reasoner achieved strong improvements (21.1% to 53.3% on hard tasks, substantial OOD gains) using simple difficulty-based ordering with probabilistic scheduling without any LLM-generated curricula, demonstrating that rule-based methods can achieve comparable or better results than LLM approaches for compositional reasoning tasks.",
            "uuids": [
                "e2036.0",
                "e2036.1",
                "e2036.2"
            ]
        },
        {
            "text": "SEC (Self-Evolving Curriculum) achieved 13-33% relative improvements using a non-stationary MAB with absolute advantage as reward, without LLM task generation, showing effective adaptive curriculum learning through algorithmic selection alone can match or exceed LLM-driven approaches.",
            "uuids": [
                "e2037.0",
                "e2037.1"
            ]
        },
        {
            "text": "Synthetic-distractor curricula using simple rule-based distractor-count difficulty levels improved joint F1 by 3-8 points across multiple datasets without LLM involvement, and Min-Max curriculum outperformed Linear despite being simpler, contradicting the claim that LLMs discover superior orderings.",
            "uuids": [
                "e2029.0",
                "e2029.2"
            ]
        },
        {
            "text": "A-TTC (Achievability Test-Time Curricula) improved learning on weaker models by filtering tasks to intermediate difficulty (50% success rate) using online achievability estimates without LLM generation, showing difficulty-aware selection is effective without LLMs and can be more computationally efficient.",
            "uuids": [
                "e2038.3"
            ]
        }
    ],
    "partially_contradicting_evidence": [
        {
            "text": "LADDER generated approximately 8% unsolvable variants and many harder-than-intended variants, requiring substantial quality control and wasted verification compute, contradicting the claim that LLMs can reliably generate appropriate difficulty progressions without extensive domain expertise.",
            "uuids": [
                "e2034.0",
                "e2034.2"
            ]
        },
        {
            "text": "EvoCurr suffered from LLM hallucination of non-existent or deprecated API calls causing runtime exceptions, and showed bias toward certain unit types with uneven sophistication, demonstrating significant LLM limitations in generating executable curricula and contradicting claims about minimal domain engineering needs.",
            "uuids": [
                "e2028.0",
                "e2028.2"
            ]
        },
        {
            "text": "Self-aware RL showed LLMs were initially poor at predicting their own success rates (accuracy ~0.2 initially, rising to &gt;0.6 after 50 steps) and required disabling limit breaking during early steps, contradicting the claim that LLMs can leverage pre-trained knowledge without extensive domain-specific fine-tuning.",
            "uuids": [
                "e2035.0"
            ]
        },
        {
            "text": "COGENT BASE (naive LLM prompting) frequently exceeded target readability levels by ~2.5 grades and produced up to 26.1% more unique words than human references, showing LLMs without curriculum conditioning produce poorly calibrated outputs, contradicting claims about LLMs' ability to generate contextually appropriate sequences without extensive prompting engineering.",
            "uuids": [
                "e2031.1"
            ]
        },
        {
            "text": "TTC-SFT (supervised fine-tuning on LLM-selected curricula) showed catastrophic initial performance drops and unstable behavior, demonstrating that off-policy training on LLM-curated curricula can be fragile, contradicting claims about LLM-driven curricula achieving faster learning.",
            "uuids": [
                "e2038.5"
            ]
        },
        {
            "text": "Accuracy-based curriculum partitioning showed that training on base-answerable samples consistently outperformed training on base-unanswerable samples, and that curriculum ordering within linear schedules had minimal impact, suggesting simpler heuristics can be as effective as sophisticated LLM-generated orderings.",
            "uuids": [
                "e2029.1",
                "e2029.3"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "CRITICAL SCOPE MISMATCH: The theory specifically targets 'compositional acquisition of commonsense and science procedures in interactive text environments', but most evidence is from math, code, robotics, or educational text generation domains. Only limited evidence exists in the theory's specified domain, suggesting the theory's scope should be either narrowed to verifiable task domains or the claims should be marked as speculative for the original target domain.",
            "uuids": [
                "e2030.0",
                "e2034.0",
                "e2035.0",
                "e2036.0",
                "e2037.0",
                "e2031.0"
            ]
        },
        {
            "text": "The most successful LLM-driven curricula combined LLM generation with other mechanisms (VLM refinement in CRAFT, difficulty prediction in Self-aware RL, embedding-based selection in TTC-RL), suggesting the theory should emphasize 'hybrid LLM-augmented approaches' rather than pure LLM generation as the key to superiority.",
            "uuids": [
                "e2030.0",
                "e2030.2",
                "e2035.0",
                "e2038.0"
            ]
        },
        {
            "text": "Non-LLM methods (E2H, SEC, A-TTC) achieved strong results with 'negligible computational overhead' while LLM methods often did not report costs or noted 'substantial wasted compute', suggesting computational efficiency and reliability should be added as key evaluation criteria and potential limitations rather than assumed advantages.",
            "uuids": [
                "e2036.0",
                "e2037.0",
                "e2038.3",
                "e2034.0"
            ]
        },
        {
            "text": "LLM-generated curricula showed high variability and stochasticity (CRAFT effective curricula ratio varied, EvoCurr 20% success rate, LADDER 8% unsolvable variants), suggesting reliability and consistency should be explicitly addressed as major limitations rather than being unaccounted for, and multiple attempts or refinement mechanisms are necessary.",
            "uuids": [
                "e2030.0",
                "e2028.0",
                "e2034.0"
            ]
        },
        {
            "text": "Evidence shows both LLM-driven curricula and non-LLM methods can identify prerequisites and generate intermediate tasks effectively (E2H using difficulty labels, A-TTC using achievability estimates, SEC using advantage signals), suggesting prerequisite identification is not a unique LLM advantage but rather a property of well-designed curriculum systems regardless of generation method.",
            "uuids": [
                "e2030.1",
                "e2034.0",
                "e2036.0",
                "e2038.3",
                "e2037.0"
            ]
        },
        {
            "text": "The theory claims LLMs can generate curricula 'with minimal domain-specific fine-tuning', but Self-aware RL required 200 training steps with initially poor difficulty prediction, LADDER required prior training for TTRL to work, and COGENT required careful prompt engineering, suggesting substantial domain adaptation and engineering is needed, contradicting the 'minimal engineering' claim.",
            "uuids": [
                "e2035.0",
                "e2034.1",
                "e2031.0"
            ]
        },
        {
            "text": "The magnitude of LLM curriculum advantage is highly baseline-dependent: dramatic when baselines completely fail (0-10% → 60-100% in CRAFT, 1-2% → 82% in LADDER) but modest or absent when compared to well-designed rule-based curricula (E2H, SEC), suggesting the theory should specify that superiority is relative to naive/random baselines rather than all alternative methods.",
            "uuids": [
                "e2030.0",
                "e2034.0",
                "e2036.0",
                "e2037.0"
            ]
        },
        {
            "text": "Model scale effects are underexplored: A-TTC was specifically helpful for weaker models (Qwen3-0.6B), and several studies used different model sizes with varying results, suggesting the theory's claim about 'quality correlating with model scale' needs more nuanced treatment of when and how scale matters.",
            "uuids": [
                "e2038.3",
                "e2036.0",
                "e2037.0"
            ]
        }
    ],
    "suggested_revisions": [
        "CRITICAL: Add explicit scope limitation noting that evidence is primarily from math, code, and robotics domains rather than the theory's specified 'commonsense and science procedures in interactive text environments', and mark claims for the original domain as speculative pending further evidence.",
        "Revise the superiority claim from 'LLM-generated curricula are superior' to 'Hybrid LLM-augmented approaches (LLM generation + VLM refinement / embedding selection / difficulty prediction) can achieve superior results compared to naive baselines but may not outperform well-designed rule-based or algorithmic curricula'.",
        "Add explicit statements about reliability and stochasticity as major limitations: 'LLM-generated curricula show high variability (20-100% success rates across runs) and require multiple attempts, refinement mechanisms, or quality control to achieve reliable results'.",
        "Revise the claim about 'minimal domain-specific fine-tuning' to 'LLM curriculum generation often requires substantial prompt engineering, domain-specific training (e.g., 200 steps for difficulty prediction), or prior exposure to related tasks to be effective'.",
        "Add computational cost and sample efficiency as key evaluation dimensions, noting that 'LLM methods can incur substantial computational overhead from quality control needs (e.g., 8% unsolvable variants, API hallucinations) while non-LLM methods can achieve comparable results with negligible overhead'.",
        "Modify the prerequisite identification claim to 'Both LLM-driven and well-designed non-LLM methods (difficulty labels, achievability estimates, advantage signals) can effectively identify prerequisites and task dependencies; this is a property of good curriculum design rather than a unique LLM capability'.",
        "Add a statement that 'LLM superiority is most pronounced when baseline methods completely fail (0-10% success) but diminishes or disappears when comparing to well-designed rule-based or algorithmic curricula (E2H, SEC, A-TTC) that can achieve comparable or better results with lower overhead'.",
        "Clarify that 'dynamic adaptation' requires additional mechanisms beyond base LLM generation (e.g., performance feedback loops, difficulty prediction training, VLM evaluation, online achievability estimation) and is not an inherent property of LLM-generated curricula without these augmentations.",
        "Add to 'unaccounted_for' section: reliability/consistency metrics, computational cost comparisons, baseline quality dependencies, and the distinction between LLM task generation vs LLM-assisted selection/refinement.",
        "Revise model scale claim to be more nuanced: 'The relationship between model scale and curriculum quality is complex and context-dependent; larger models may generate better curricula but smaller models with appropriate difficulty filtering (A-TTC) can also be effective, and the optimal approach may vary by task domain and learner capability'."
    ],
    "overall_support_or_contradict": "neutral",
    "overall_support_or_contradict_explanation": "The evidence shows LLM-driven curricula can achieve dramatic improvements over naive baselines (0-10% → 60-100%) in verifiable domains, supporting core claims. However, there is a critical scope mismatch (most evidence is NOT in the theory's specified 'interactive text environments for commonsense/science procedures'), and multiple non-LLM methods achieved comparable or better results with lower overhead, suggesting the advantage is domain- and baseline-dependent rather than universal.",
    "revised_theory_ids": [
        "theory-381",
        "theory-382"
    ],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
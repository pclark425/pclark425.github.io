<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-3 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-3</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-3</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-17.html">theory-17</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-21.html">theory-21</a></p>
                <p><strong>Overall Support or Contradict:</strong> support</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The new evidence largely supports the theory that model size and training data diversity positively influence first-order ToM performance, while also highlighting the crucial role of fine-tuning and data quality, and revealing nuances such as diminishing returns and performance variability.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>Multiple large models such as GPT-4, LLaMA2-70B, and Qwen 7B demonstrate improved first-order ToM performance correlating positively with model size and training data diversity, achieving accuracies near or above child-level benchmarks. <a href="../results/extraction-result-92.html#e92.0" class="evidence-link">[e92.0]</a> <a href="../results/extraction-result-96.html#e96.0" class="evidence-link">[e96.0]</a> <a href="../results/extraction-result-93.html#e93.0" class="evidence-link">[e93.0]</a> <a href="../results/extraction-result-103.html#e103.0" class="evidence-link">[e103.0]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-109.html#e109.1" class="evidence-link">[e109.1]</a> <a href="../results/extraction-result-115.html#e115.0" class="evidence-link">[e115.0]</a> <a href="../results/extraction-result-115.html#e115.1" class="evidence-link">[e115.1]</a> </li>
    <li>Fine-tuning and instruction tuning on ToM-specific or socially rich datasets significantly enhance first-order ToM task performance, as seen in models like Flan-PaLM, Mistral-7B, and GPT-4 variants. <a href="../results/extraction-result-100.html#e100.1" class="evidence-link">[e100.1]</a> <a href="../results/extraction-result-104.html#e104.0" class="evidence-link">[e104.0]</a> <a href="../results/extraction-result-112.html#e112.0" class="evidence-link">[e112.0]</a> <a href="../results/extraction-result-108.html#e108.0" class="evidence-link">[e108.0]</a> <a href="../results/extraction-result-106.html#e106.0" class="evidence-link">[e106.0]</a> <a href="../results/extraction-result-106.html#e106.1" class="evidence-link">[e106.1]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-96.html#e96.1" class="evidence-link">[e96.1]</a> </li>
    <li>Training data diversity, especially inclusion of multilingual and social narrative content, positively impacts ToM reasoning capabilities, supported by evidence from models like GPT-4, LLaMA-2, and DeepSeek R1. <a href="../results/extraction-result-91.html#e91.0" class="evidence-link">[e91.0]</a> <a href="../results/extraction-result-98.html#e98.1" class="evidence-link">[e98.1]</a> <a href="../results/extraction-result-103.html#e103.0" class="evidence-link">[e103.0]</a> <a href="../results/extraction-result-109.html#e109.1" class="evidence-link">[e109.1]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-96.html#e96.0" class="evidence-link">[e96.0]</a> </li>
    <li>Smaller models (below ~10B parameters) generally perform poorly on first-order ToM tasks, but fine-tuning and novel inference-time methods can partially improve their performance. <a href="../results/extraction-result-108.html#e108.1" class="evidence-link">[e108.1]</a> <a href="../results/extraction-result-107.html#e107.0" class="evidence-link">[e107.0]</a> <a href="../results/extraction-result-96.html#e96.1" class="evidence-link">[e96.1]</a> <a href="../results/extraction-result-101.html#e101.0" class="evidence-link">[e101.0]</a> <a href="../results/extraction-result-98.html#e98.0" class="evidence-link">[e98.0]</a> <a href="../results/extraction-result-95.html#e95.0" class="evidence-link">[e95.0]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>Larger models show diminishing returns on ToM performance at very high parameter counts (e.g., GPT-4 and Flan-PaLM), indicating that size alone is not sufficient and fine-tuning or data quality plays a critical role. <a href="../results/extraction-result-100.html#e100.1" class="evidence-link">[e100.1]</a> <a href="../results/extraction-result-109.html#e109.1" class="evidence-link">[e109.1]</a> <a href="../results/extraction-result-98.html#e98.1" class="evidence-link">[e98.1]</a> <a href="../results/extraction-result-111.html#e111.0" class="evidence-link">[e111.0]</a> </li>
    <li>Fine-tuning improves ToM performance but can lead to overfitting or degrade out-of-distribution generalization, suggesting a complex relationship between fine-tuning and model robustness. <a href="../results/extraction-result-95.html#e95.0" class="evidence-link">[e95.0]</a> <a href="../results/extraction-result-99.html#e99.0" class="evidence-link">[e99.0]</a> <a href="../results/extraction-result-99.html#e99.1" class="evidence-link">[e99.1]</a> </li>
    <li>Multimodal and vision-language models (e.g., LLaVA, Video-ChatGPT) benefit from fine-tuning on social and emotional datasets, but model size does not always correlate directly with improved ToM performance in these contexts. <a href="../results/extraction-result-112.html#e112.0" class="evidence-link">[e112.0]</a> <a href="../results/extraction-result-112.html#e112.1" class="evidence-link">[e112.1]</a> <a href="../results/extraction-result-102.html#e102.0" class="evidence-link">[e102.0]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<ol>
    <li>Some large models of similar or greater size (e.g., GPT-4o, Vicuna-33B, Llama-3.1) perform poorly or inconsistently on first-order ToM tasks, challenging the notion that size and data diversity alone drive ToM performance. <a href="../results/extraction-result-99.html#e99.1" class="evidence-link">[e99.1]</a> <a href="../results/extraction-result-114.html#e114.1" class="evidence-link">[e114.1]</a> <a href="../results/extraction-result-99.html#e99.0" class="evidence-link">[e99.0]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-114.html#e114.2" class="evidence-link">[e114.2]</a> <a href="../results/extraction-result-114.html#e114.1" class="evidence-link">[e114.1]</a> </li>
    <li>Evidence shows that larger models do not always outperform smaller models on ToM tasks, and some smaller fine-tuned models can outperform larger ones, contradicting a strict positive correlation between size and ToM performance. <a href="../results/extraction-result-94.html#e94.0" class="evidence-link">[e94.0]</a> <a href="../results/extraction-result-107.html#e107.0" class="evidence-link">[e107.0]</a> <a href="../results/extraction-result-108.html#e108.1" class="evidence-link">[e108.1]</a> </li>
    <li>LLMs often rely on shallow heuristics or spurious correlations rather than genuine understanding of mental states, as performance drops significantly with minor task perturbations, questioning the depth of their ToM capabilities. <a href="../results/extraction-result-109.html#e109.0" class="evidence-link">[e109.0]</a> <a href="../results/extraction-result-104.html#e104.0" class="evidence-link">[e104.0]</a> <a href="../results/extraction-result-105.html#e105.0" class="evidence-link">[e105.0]</a> <a href="../results/extraction-result-109.html#e109.1" class="evidence-link">[e109.1]</a> <a href="../results/extraction-result-113.html#e113.0" class="evidence-link">[e113.0]</a> </li>
</ol>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>Variability in ToM performance among models of similar size suggests that factors beyond model size and training data diversity, such as fine-tuning methods, task design, and model architecture, influence outcomes. <a href="../results/extraction-result-74.html#e74.0" class="evidence-link">[e74.0]</a> <a href="../results/extraction-result-83.html#e83.1" class="evidence-link">[e83.1]</a> <a href="../results/extraction-result-111.html#e111.0" class="evidence-link">[e111.0]</a> <a href="../results/extraction-result-113.html#e113.0" class="evidence-link">[e113.0]</a> <a href="../results/extraction-result-114.html#e114.0" class="evidence-link">[e114.0]</a> <a href="../results/extraction-result-115.html#e115.0" class="evidence-link">[e115.0]</a> <a href="../results/extraction-result-115.html#e115.1" class="evidence-link">[e115.1]</a> </li>
    <li>Some benchmarks and datasets may overestimate model ToM capabilities due to oversimplified tasks or lack of adversarial examples, indicating that current evaluations might not fully capture genuine ToM reasoning. <a href="../results/extraction-result-99.html#e99.0" class="evidence-link">[e99.0]</a> <a href="../results/extraction-result-99.html#e99.1" class="evidence-link">[e99.1]</a> <a href="../results/extraction-result-93.html#e93.0" class="evidence-link">[e93.0]</a> <a href="../results/extraction-result-91.html#e91.0" class="evidence-link">[e91.0]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>Fine-tuning and instruction tuning are critical for enhancing ToM performance, sometimes more so than increasing model size, suggesting the theory should emphasize the role of targeted fine-tuning. <a href="../results/extraction-result-100.html#e100.1" class="evidence-link">[e100.1]</a> <a href="../results/extraction-result-108.html#e108.0" class="evidence-link">[e108.0]</a> <a href="../results/extraction-result-106.html#e106.0" class="evidence-link">[e106.0]</a> <a href="../results/extraction-result-96.html#e96.1" class="evidence-link">[e96.1]</a> <a href="../results/extraction-result-104.html#e104.0" class="evidence-link">[e104.0]</a> </li>
    <li>The quality and complexity of training data, especially social narratives and diverse cultural contexts, may be more important than sheer quantity, indicating a need to refine the theory's statements about training data diversity. <a href="../results/extraction-result-91.html#e91.0" class="evidence-link">[e91.0]</a> <a href="../results/extraction-result-112.html#e112.0" class="evidence-link">[e112.0]</a> <a href="../results/extraction-result-109.html#e109.1" class="evidence-link">[e109.1]</a> <a href="../results/extraction-result-96.html#e96.0" class="evidence-link">[e96.0]</a> </li>
    <li>The theory should account for diminishing returns and performance variability at very large model scales, as well as the impact of model architecture and evaluation methods on ToM performance. <a href="../results/extraction-result-100.html#e100.1" class="evidence-link">[e100.1]</a> <a href="../results/extraction-result-111.html#e111.0" class="evidence-link">[e111.0]</a> <a href="../results/extraction-result-113.html#e113.0" class="evidence-link">[e113.0]</a> </li>
    <li>The theory should incorporate the distinction between literal ToM performance and functional ToM capabilities, as some models show strong literal performance but fail in interactive or adaptive scenarios. <a href="../results/extraction-result-103.html#e103.0" class="evidence-link">[e103.0]</a> </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>Emphasize the critical role of fine-tuning and instruction tuning on ToM-specific datasets in addition to model size and training data diversity.</li>
                <li>Refine the theory to highlight that training data quality, complexity, and social narrative richness may be more influential than mere data quantity or multilinguality.</li>
                <li>Incorporate the concept of diminishing returns in ToM performance gains at very large model scales and acknowledge variability due to factors beyond size and data.</li>
                <li>Add a distinction between literal ToM task performance and functional ToM capabilities, noting that strong performance on benchmarks does not guarantee genuine understanding or adaptability.</li>
                <li>Acknowledge that current ToM benchmarks may overestimate LLMs' true ToM abilities due to oversimplified tasks and lack of adversarial robustness.</li>
                <li>Include that smaller models can improve ToM performance substantially through fine-tuning and novel inference-time methods, partially mitigating size limitations.</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-3",
    "theory_id": "theory-17",
    "fully_supporting_evidence": [
        {
            "text": "Multiple large models such as GPT-4, LLaMA2-70B, and Qwen 7B demonstrate improved first-order ToM performance correlating positively with model size and training data diversity, achieving accuracies near or above child-level benchmarks.",
            "uuids": [
                "e92.0",
                "e96.0",
                "e93.0",
                "e103.0",
                "e114.0",
                "e109.1",
                "e115.0",
                "e115.1"
            ]
        },
        {
            "text": "Fine-tuning and instruction tuning on ToM-specific or socially rich datasets significantly enhance first-order ToM task performance, as seen in models like Flan-PaLM, Mistral-7B, and GPT-4 variants.",
            "uuids": [
                "e100.1",
                "e104.0",
                "e112.0",
                "e108.0",
                "e106.0",
                "e106.1",
                "e114.0",
                "e96.1"
            ]
        },
        {
            "text": "Training data diversity, especially inclusion of multilingual and social narrative content, positively impacts ToM reasoning capabilities, supported by evidence from models like GPT-4, LLaMA-2, and DeepSeek R1.",
            "uuids": [
                "e91.0",
                "e98.1",
                "e103.0",
                "e109.1",
                "e114.0",
                "e96.0"
            ]
        },
        {
            "text": "Smaller models (below ~10B parameters) generally perform poorly on first-order ToM tasks, but fine-tuning and novel inference-time methods can partially improve their performance.",
            "uuids": [
                "e108.1",
                "e107.0",
                "e96.1",
                "e101.0",
                "e98.0",
                "e95.0"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "Larger models show diminishing returns on ToM performance at very high parameter counts (e.g., GPT-4 and Flan-PaLM), indicating that size alone is not sufficient and fine-tuning or data quality plays a critical role.",
            "uuids": [
                "e100.1",
                "e109.1",
                "e98.1",
                "e111.0"
            ]
        },
        {
            "text": "Fine-tuning improves ToM performance but can lead to overfitting or degrade out-of-distribution generalization, suggesting a complex relationship between fine-tuning and model robustness.",
            "uuids": [
                "e95.0",
                "e99.0",
                "e99.1"
            ]
        },
        {
            "text": "Multimodal and vision-language models (e.g., LLaVA, Video-ChatGPT) benefit from fine-tuning on social and emotional datasets, but model size does not always correlate directly with improved ToM performance in these contexts.",
            "uuids": [
                "e112.0",
                "e112.1",
                "e102.0"
            ]
        }
    ],
    "fully_contradicting_evidence": [
        {
            "text": "Some large models of similar or greater size (e.g., GPT-4o, Vicuna-33B, Llama-3.1) perform poorly or inconsistently on first-order ToM tasks, challenging the notion that size and data diversity alone drive ToM performance.",
            "uuids": [
                "e99.1",
                "e114.1",
                "e99.0",
                "e114.0",
                "e114.2",
                "e114.1"
            ]
        },
        {
            "text": "Evidence shows that larger models do not always outperform smaller models on ToM tasks, and some smaller fine-tuned models can outperform larger ones, contradicting a strict positive correlation between size and ToM performance.",
            "uuids": [
                "e94.0",
                "e107.0",
                "e108.1"
            ]
        },
        {
            "text": "LLMs often rely on shallow heuristics or spurious correlations rather than genuine understanding of mental states, as performance drops significantly with minor task perturbations, questioning the depth of their ToM capabilities.",
            "uuids": [
                "e109.0",
                "e104.0",
                "e105.0",
                "e109.1",
                "e113.0"
            ]
        }
    ],
    "partially_contradicting_evidence": [
        {
            "text": "Variability in ToM performance among models of similar size suggests that factors beyond model size and training data diversity, such as fine-tuning methods, task design, and model architecture, influence outcomes.",
            "uuids": [
                "e74.0",
                "e83.1",
                "e111.0",
                "e113.0",
                "e114.0",
                "e115.0",
                "e115.1"
            ]
        },
        {
            "text": "Some benchmarks and datasets may overestimate model ToM capabilities due to oversimplified tasks or lack of adversarial examples, indicating that current evaluations might not fully capture genuine ToM reasoning.",
            "uuids": [
                "e99.0",
                "e99.1",
                "e93.0",
                "e91.0"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "Fine-tuning and instruction tuning are critical for enhancing ToM performance, sometimes more so than increasing model size, suggesting the theory should emphasize the role of targeted fine-tuning.",
            "uuids": [
                "e100.1",
                "e108.0",
                "e106.0",
                "e96.1",
                "e104.0"
            ]
        },
        {
            "text": "The quality and complexity of training data, especially social narratives and diverse cultural contexts, may be more important than sheer quantity, indicating a need to refine the theory's statements about training data diversity.",
            "uuids": [
                "e91.0",
                "e112.0",
                "e109.1",
                "e96.0"
            ]
        },
        {
            "text": "The theory should account for diminishing returns and performance variability at very large model scales, as well as the impact of model architecture and evaluation methods on ToM performance.",
            "uuids": [
                "e100.1",
                "e111.0",
                "e113.0"
            ]
        },
        {
            "text": "The theory should incorporate the distinction between literal ToM performance and functional ToM capabilities, as some models show strong literal performance but fail in interactive or adaptive scenarios.",
            "uuids": [
                "e103.0"
            ]
        }
    ],
    "suggested_revisions": [
        "Emphasize the critical role of fine-tuning and instruction tuning on ToM-specific datasets in addition to model size and training data diversity.",
        "Refine the theory to highlight that training data quality, complexity, and social narrative richness may be more influential than mere data quantity or multilinguality.",
        "Incorporate the concept of diminishing returns in ToM performance gains at very large model scales and acknowledge variability due to factors beyond size and data.",
        "Add a distinction between literal ToM task performance and functional ToM capabilities, noting that strong performance on benchmarks does not guarantee genuine understanding or adaptability.",
        "Acknowledge that current ToM benchmarks may overestimate LLMs' true ToM abilities due to oversimplified tasks and lack of adversarial robustness.",
        "Include that smaller models can improve ToM performance substantially through fine-tuning and novel inference-time methods, partially mitigating size limitations."
    ],
    "overall_support_or_contradict": "support",
    "overall_support_or_contradict_explanation": "The new evidence largely supports the theory that model size and training data diversity positively influence first-order ToM performance, while also highlighting the crucial role of fine-tuning and data quality, and revealing nuances such as diminishing returns and performance variability.",
    "revised_theory_ids": [
        "theory-21"
    ],
    "model_str": null
}</code></pre>
        </div>
    </div>
</body>
</html>
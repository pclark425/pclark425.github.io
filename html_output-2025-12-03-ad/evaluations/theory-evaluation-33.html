<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-33 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-33</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-33</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-147.html">theory-147</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-395.html">theory-395</a></p>
                <p><strong>Overall Support or Contradict:</strong> support</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The new evidence strongly supports the core principle that task-aligned abstractions outperform pixel reconstruction for most control tasks, with multiple methods demonstrating substantial performance gains (LAOM 2× improvement, LaDi-WM +14.7%, OCCAM +85% in some games, OMC-RL +19pp). However, the evidence reveals important scope conditions: task-alignment often requires supervision, temporal modeling is critical, hybrid multi-level approaches may be superior to single-level abstractions, and there are lower bounds below which abstraction becomes excessive.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>LAOM removes pixel reconstruction in favor of latent temporal consistency and achieves 2x downstream performance improvement and 8x better latent-action quality (via linear probe) compared to pixel-reconstruction baseline LAPO in distractor settings, with combined modifications yielding ~8x probe improvement, directly confirming that pixel reconstruction of task-irrelevant features wastes capacity and harms performance <a href="../results/extraction-result-2250.html#e2250.0" class="evidence-link">[e2250.0]</a> <a href="../results/extraction-result-2250.html#e2250.1" class="evidence-link">[e2250.1]</a> <a href="../results/extraction-result-2250.html#e2250.7" class="evidence-link">[e2250.7]</a> </li>
    <li>LaDi-WM's VFM-aligned latent diffusion world model outperforms pixel-space diffusion by 14.7% Avg.SR (71.7% vs 54.0%) on LIBERO-LONG manipulation tasks, demonstrating that geometric/semantic latent abstractions are superior to pixel-level prediction for robotic control <a href="../results/extraction-result-2252.html#e2252.0" class="evidence-link">[e2252.0]</a> <a href="../results/extraction-result-2252.html#e2252.2" class="evidence-link">[e2252.2]</a> </li>
    <li>OCCAM object-centric masking variants (Object Masks, Planes) match or substantially exceed pixel-baseline (DQN-like) performance (e.g., MsPacman Object Masks 5880 vs DQN-like 3174, +85%) while showing better robustness to visual perturbations (higher GNS), confirming that removing background distractors via object-level abstraction improves both performance and generalization <a href="../results/extraction-result-2248.html#e2248.0" class="evidence-link">[e2248.0]</a> <a href="../results/extraction-result-2248.html#e2248.1" class="evidence-link">[e2248.1]</a> <a href="../results/extraction-result-2248.html#e2248.4" class="evidence-link">[e2248.4]</a> <a href="../results/extraction-result-2248.html#e2248.6" class="evidence-link">[e2248.6]</a> </li>
    <li>OMC-RL's masked latent contrastive learning outperforms pixel-based CURL by +19.0pp success rate in furniture navigation (91.5% vs 72.5%) and shows superior transfer across environments and to real-world deployment, supporting that latent-level masked reconstruction focuses on task-relevant features better than pixel-level contrastive methods <a href="../results/extraction-result-2246.html#e2246.0" class="evidence-link">[e2246.0]</a> <a href="../results/extraction-result-2246.html#e2246.1" class="evidence-link">[e2246.1]</a> <a href="../results/extraction-result-2246.html#e2246.2" class="evidence-link">[e2246.2]</a> </li>
    <li>DisWM's disentangled latent world model with KL distillation achieves better sample efficiency and robustness to distractors than pixel-reconstruction baselines (DreamerV2, CURL, APV) across multiple continuous control tasks, with qualitative improvements in learning curves and cross-domain transfer (DMC→MuJoCo) <a href="../results/extraction-result-2247.html#e2247.0" class="evidence-link">[e2247.0]</a> <a href="../results/extraction-result-2247.html#e2247.2" class="evidence-link">[e2247.2]</a> <a href="../results/extraction-result-2247.html#e2247.4" class="evidence-link">[e2247.4]</a> </li>
    <li>Binary Masks and Planes abstractions in OCCAM remove within-object appearance details and often outperform pixel baselines (e.g., Planes in MsPacman 7187.50 vs DQN-like 3174.38), showing that aggressive abstraction away from pixel fidelity can improve learning when appearance is task-irrelevant, though at higher computational cost for Planes <a href="../results/extraction-result-2248.html#e2248.2" class="evidence-link">[e2248.2]</a> <a href="../results/extraction-result-2248.html#e2248.4" class="evidence-link">[e2248.4]</a> </li>
    <li>LAOM with supervision achieves normalized score 0.44 using only 2.5% labeled transitions, recovering ~44% of BC full-data performance, and improves downstream performance by ~4.2-4.3x vs unsupervised LAOM in presence of distractors, demonstrating that task-aligned latent representations with minimal supervision are highly sample-efficient <a href="../results/extraction-result-2250.html#e2250.0" class="evidence-link">[e2250.0]</a> </li>
    <li>Multi-step IDM in LAOM (predicting latent actions from o_t and o_{t+k} for k∈{1..10}) doubled latent-action quality compared to single-step IDM, showing that temporal abstraction focusing on control-endogenous features improves task-relevance <a href="../results/extraction-result-2250.html#e2250.0" class="evidence-link">[e2250.0]</a> <a href="../results/extraction-result-2250.html#e2250.8" class="evidence-link">[e2250.8]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>LAOM achieves strong performance but requires supervision from ground-truth actions to avoid encoding action-correlated distractors; unsupervised LAOM still encodes exogenous noise into latent actions, suggesting task-alignment requires explicit task signals rather than emerging automatically from abstraction alone, though supervision requirements are minimal (2.5% labels) <a href="../results/extraction-result-2250.html#e2250.0" class="evidence-link">[e2250.0]</a> <a href="../results/extraction-result-2250.html#e2250.2" class="evidence-link">[e2250.2]</a> </li>
    <li>LaDi-WM combines geometric (DINO) and semantic (SigLip) latent channels with interactive diffusion, showing that multiple abstraction levels working together (+3.4% Avg.SR for adding semantics) outperform single-level abstractions, suggesting optimal abstraction may be multi-level hybrid rather than selecting a single level <a href="../results/extraction-result-2252.html#e2252.0" class="evidence-link">[e2252.0]</a> <a href="../results/extraction-result-2252.html#e2252.1" class="evidence-link">[e2252.1]</a> <a href="../results/extraction-result-2252.html#e2252.3" class="evidence-link">[e2252.3]</a> <a href="../results/extraction-result-2252.html#e2252.4" class="evidence-link">[e2252.4]</a> </li>
    <li>VITA achieves strong zero-shot value estimation (VOC 0.782 on tk_pnp, MT10 IQM 0.815) via test-time adaptation of CLIP features with meta-learned self-supervised objectives, demonstrating that task-relevance can be determined dynamically at test time through parameter adaptation rather than only during training, extending the theory's scope <a href="../results/extraction-result-2249.html#e2249.0" class="evidence-link">[e2249.0]</a> <a href="../results/extraction-result-2249.html#e2249.3" class="evidence-link">[e2249.3]</a> <a href="../results/extraction-result-2249.html#e2249.5" class="evidence-link">[e2249.5]</a> </li>
    <li>OMC-RL benefits substantially from oracle teacher guidance (privileged state information including depth and poses), with removal causing notable performance drops especially in complex scenes (Barrier/Tree), indicating that task-aligned representations are more effective when combined with privileged supervision during training <a href="../results/extraction-result-2246.html#e2246.0" class="evidence-link">[e2246.0]</a> <a href="../results/extraction-result-2246.html#e2246.3" class="evidence-link">[e2246.3]</a> </li>
    <li>ReOI uses latent-space dynamics (DINO-WM) but reintroduces distractors via pixel-level compositing for downstream VLM-based visual verification, improving task success by up to 3× in presence of novel distractors, showing pixel fidelity has value for verification tasks even when dynamics operate in latent space, indicating hybrid approaches may be optimal <a href="../results/extraction-result-2251.html#e2251.0" class="evidence-link">[e2251.0]</a> <a href="../results/extraction-result-2251.html#e2251.1" class="evidence-link">[e2251.1]</a> <a href="../results/extraction-result-2251.html#e2251.7" class="evidence-link">[e2251.7]</a> </li>
    <li>DisWM's β-VAE pretraining on action-free distracting videos with KL-based latent distillation enables cross-domain transfer, but requires careful annealing of distillation weight (η: 0.1→0.01) to avoid overwriting pretrained structure, suggesting dynamic adjustment of abstraction influence during adaptation is important <a href="../results/extraction-result-2247.html#e2247.0" class="evidence-link">[e2247.0]</a> <a href="../results/extraction-result-2247.html#e2247.1" class="evidence-link">[e2247.1]</a> <a href="../results/extraction-result-2247.html#e2247.2" class="evidence-link">[e2247.2]</a> </li>
    <li>LAOM's computational efficiency improvements (removing image decoder reduces model size from ~211M to ~192M params and training time from ~7h38m to ~6h43m) support the theory's claim that task-aligned abstractions reduce computational cost, though gains are modest in this case <a href="../results/extraction-result-2250.html#e2250.0" class="evidence-link">[e2250.0]</a> <a href="../results/extraction-result-2250.html#e2250.1" class="evidence-link">[e2250.1]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<ol>
    <li>LAOM's cross-embodied transfer experiments show that supervised latent-action pretraining does not exceed BC trained only on target-domain labels, contradicting the theory's prediction that task-aligned abstractions should transfer better than pixel-level representations across domains <a href="../results/extraction-result-2250.html#e2250.0" class="evidence-link">[e2250.0]</a> </li>
    <li>VQ-VAE quantization (extreme information bottleneck intended for task-alignment) actively harms performance and causes codebook collapse, with removal of quantization improving latent-action probe quality substantially (Fig.5), suggesting that aggressive compression/abstraction via quantization can be counterproductive rather than beneficial <a href="../results/extraction-result-2250.html#e2250.1" class="evidence-link">[e2250.1]</a> <a href="../results/extraction-result-2250.html#e2250.5" class="evidence-link">[e2250.5]</a> <a href="../results/extraction-result-2250.html#e2250.6" class="evidence-link">[e2250.6]</a> </li>
</ol>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>Object Masks (preserving within-object appearance) outperform more abstract Binary Masks (4833.12 vs 5880.00 in MsPacman) and Class Masks (4549.38) in cases where ghost color encodes reward-relevant information, showing that pixel-level appearance fidelity is task-relevant in some cases and abstraction removes critical information <a href="../results/extraction-result-2248.html#e2248.1" class="evidence-link">[e2248.1]</a> <a href="../results/extraction-result-2248.html#e2248.2" class="evidence-link">[e2248.2]</a> <a href="../results/extraction-result-2248.html#e2248.3" class="evidence-link">[e2248.3]</a> </li>
    <li>Semantic Vector representation (extreme symbolic compression) performs poorly on spatially-demanding tasks by removing spatial relationships, with training time faster (~1h38m) but substantially worse performance, demonstrating that excessive abstraction beyond task requirements harms performance despite computational benefits <a href="../results/extraction-result-2248.html#e2248.5" class="evidence-link">[e2248.5]</a> </li>
    <li>CLIP-based methods (CLIP-FT, VLM-CL, VLM-RM) with semantic-level abstractions fail on temporally-dependent tasks (e.g., VLM-CL VOC 0.038 on tk_pnp vs VITA 0.782) despite semantic alignment, showing that semantic abstraction alone is insufficient without temporal modeling, indicating abstraction level must match multiple task dimensions simultaneously <a href="../results/extraction-result-2249.html#e2249.1" class="evidence-link">[e2249.1]</a> <a href="../results/extraction-result-2249.html#e2249.6" class="evidence-link">[e2249.6]</a> <a href="../results/extraction-result-2249.html#e2249.7" class="evidence-link">[e2249.7]</a> </li>
    <li>Pixel diffusion baseline achieves 54.0% Avg.SR on manipulation tasks, which while substantially lower than latent diffusion (71.7%, -14.7%), is not catastrophic failure, suggesting pixel-level models can achieve reasonable performance and the performance gap may be smaller than theory suggests in some domains <a href="../results/extraction-result-2252.html#e2252.2" class="evidence-link">[e2252.2]</a> </li>
    <li>GVL autoregressive VLM performs well on folding tasks (VOC 0.326-0.372) but poorly on pick-and-place (VOC 0.269) and stacking despite operating at semantic sequence level, showing that abstraction level alone doesn't determine performance and task-specific factors (temporal bias, execution variability) matter beyond abstraction choice <a href="../results/extraction-result-2249.html#e2249.2" class="evidence-link">[e2249.2]</a> </li>
    <li>OCCAM Planes representation increases computational cost (6-10 hours training vs 2-4 hours for simpler masks) despite often achieving best performance, showing that more structured task-aligned abstractions can have higher computational costs rather than always reducing cost as theory suggests <a href="../results/extraction-result-2248.html#e2248.4" class="evidence-link">[e2248.4]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>Multiple methods (LAOM, OMC-RL, DisWM) show that supervision, privileged information, or explicit task signals are critical for achieving effective task-aligned representations, with LAOM requiring 2.5% labels for strong performance and OMC-RL benefiting from oracle guidance, suggesting the theory should explicitly incorporate the role of supervision in determining task-relevance rather than assuming it emerges from architecture alone <a href="../results/extraction-result-2250.html#e2250.0" class="evidence-link">[e2250.0]</a> <a href="../results/extraction-result-2246.html#e2246.0" class="evidence-link">[e2246.0]</a> <a href="../results/extraction-result-2246.html#e2246.3" class="evidence-link">[e2246.3]</a> <a href="../results/extraction-result-2247.html#e2247.0" class="evidence-link">[e2247.0]</a> </li>
    <li>Temporal modeling is critical across multiple methods (VITA's test-time adaptation, OMC-RL's masked temporal contrastive learning, CLIP-FT failures on temporal tasks), indicating that task-aligned abstraction must consider temporal dimension explicitly as a separate axis from spatial/semantic abstraction level, suggesting theory should add temporal abstraction as a key dimension alongside semantic/geometric/pixel levels <a href="../results/extraction-result-2249.html#e2249.0" class="evidence-link">[e2249.0]</a> <a href="../results/extraction-result-2249.html#e2249.3" class="evidence-link">[e2249.3]</a> <a href="../results/extraction-result-2249.html#e2249.7" class="evidence-link">[e2249.7]</a> <a href="../results/extraction-result-2246.html#e2246.0" class="evidence-link">[e2246.0]</a> <a href="../results/extraction-result-2246.html#e2246.1" class="evidence-link">[e2246.1]</a> </li>
    <li>Test-time adaptation methods (VITA with meta-learned TTT, ReOI with VLM-based distractor detection and inpainting) show task-relevance can be determined dynamically at deployment rather than fixed at training time, suggesting theory should be expanded to include dynamic/adaptive abstraction mechanisms that adjust to distribution shifts and novel distractors beyond static training-time selection <a href="../results/extraction-result-2249.html#e2249.0" class="evidence-link">[e2249.0]</a> <a href="../results/extraction-result-2249.html#e2249.3" class="evidence-link">[e2249.3]</a> <a href="../results/extraction-result-2251.html#e2251.0" class="evidence-link">[e2251.0]</a> </li>
    <li>LaDi-WM and ReOI demonstrate that hybrid multi-level abstractions (combining geometric DINO, semantic SigLip, and pixel-level compositing for different purposes) outperform single-level abstractions, with LaDi-WM achieving 68.7% vs pixel 54.0% and ReOI improving success 3× via hybrid approach, suggesting theory should emphasize multi-level hybrid approaches rather than selecting a single optimal level <a href="../results/extraction-result-2252.html#e2252.0" class="evidence-link">[e2252.0]</a> <a href="../results/extraction-result-2252.html#e2252.1" class="evidence-link">[e2252.1]</a> <a href="../results/extraction-result-2251.html#e2251.0" class="evidence-link">[e2251.0]</a> </li>
    <li>OCCAM shows that simple explicit masking via bounding boxes can match or exceed learned representations (Object Masks 5880 vs DQN-like 3174 in MsPacman) while being more interpretable and requiring less pretraining overhead, suggesting theory should distinguish between learned vs explicit abstraction mechanisms and their relative merits in terms of performance, interpretability, and computational cost <a href="../results/extraction-result-2248.html#e2248.0" class="evidence-link">[e2248.0]</a> <a href="../results/extraction-result-2248.html#e2248.1" class="evidence-link">[e2248.1]</a> </li>
    <li>Multiple failures of over-abstraction (Semantic Vector removing spatial relationships, VQ-VAE quantization causing collapse, Binary Masks losing color information in MsPacman) indicate there are lower bounds on information retention below which task-relevant information is lost, suggesting theory should explicitly characterize when abstraction becomes excessive and provide principles for determining minimal sufficient abstraction <a href="../results/extraction-result-2248.html#e2248.2" class="evidence-link">[e2248.2]</a> <a href="../results/extraction-result-2248.html#e2248.3" class="evidence-link">[e2248.3]</a> <a href="../results/extraction-result-2248.html#e2248.5" class="evidence-link">[e2248.5]</a> <a href="../results/extraction-result-2250.html#e2250.5" class="evidence-link">[e2250.5]</a> </li>
    <li>LAOM's augmentations (random shifts, intensity changes) stabilize training and improve latent-action quality when combined with latent temporal consistency, suggesting that data augmentation is an important complementary mechanism to abstraction for achieving task-relevant representations, which should be incorporated into the theory <a href="../results/extraction-result-2250.html#e2250.0" class="evidence-link">[e2250.0]</a> </li>
    <li>Dissimilarity-based sampling in VITA (selecting diverse sub-trajectories) mitigates shortcut learning and improves discriminative performance (4/4 perfect vs worse for full-trajectory sampling), suggesting that training data selection strategies are important for learning task-relevant abstractions and should be considered alongside architectural choices <a href="../results/extraction-result-2249.html#e2249.0" class="evidence-link">[e2249.0]</a> <a href="../results/extraction-result-2249.html#e2249.4" class="evidence-link">[e2249.4]</a> </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>Add explicit statement that task-aligned abstractions typically require supervision, privileged information, or explicit task signals to achieve effective task-relevance, with minimal supervision (e.g., 2.5% labels in LAOM) often sufficient, rather than emerging automatically from unsupervised learning alone</li>
                <li>Incorporate temporal abstraction as a critical dimension alongside spatial/semantic abstraction, noting that semantic abstractions without temporal modeling fail on temporally-dependent tasks, and that temporal context can be encoded through sequential parameter updates (test-time adaptation) or architectural design (masked temporal contrastive learning)</li>
                <li>Expand theory to include dynamic/adaptive abstraction mechanisms that can adjust task-relevance at test time (e.g., test-time adaptation via meta-learned self-supervision, online intervention with VLM-based distractor detection) rather than only static training-time selection</li>
                <li>Modify theory to emphasize hybrid multi-level abstractions that combine different abstraction levels for different purposes (e.g., latent dynamics + pixel verification, geometric + semantic latents) rather than selecting a single optimal level, with explicit guidance on when each level is appropriate</li>
                <li>Add characterization of lower bounds on abstraction: specify that excessive compression (e.g., removing spatial relationships via Semantic Vector, extreme quantization via VQ-VAE, removing color information via Binary Masks) can harm performance when it discards task-relevant information, and provide principles for determining minimal sufficient abstraction (e.g., preserve information needed for task discrimination)</li>
                <li>Distinguish between learned task-aligned representations (e.g., DisWM's disentangled latents, OMC-RL's masked contrastive features) and explicit masking/filtering approaches (e.g., OCCAM's bounding-box masks), noting that simple explicit methods can sometimes match learned approaches with lower computational cost and better interpretability, though may require domain-specific design</li>
                <li>Clarify that transfer benefits of task-aligned abstractions are limited: supervised task-aligned pretraining does not always exceed training from scratch on target domain (LAOM cross-embodied results), especially under large domain shifts, and that transfer success depends on alignment between source and target task requirements</li>
                <li>Add special case that pixel-level appearance fidelity is necessary when fine-grained visual features encode task-relevant information (e.g., color-coded rewards in MsPacman, texture-based discrimination), not only for 'fine visual details directly impacting decisions', and that Object Masks preserving appearance can outperform more abstract representations in these cases</li>
                <li>Incorporate the role of data augmentation and training data selection (e.g., dissimilarity-based sampling) as complementary mechanisms to architectural abstraction for achieving task-relevant representations and avoiding shortcut learning</li>
                <li>Revise computational cost claims to acknowledge that some task-aligned abstractions (e.g., OCCAM Planes, multi-level hybrid models) can have higher computational costs than simpler approaches, and that cost benefits depend on specific implementation choices rather than being universal to all task-aligned abstractions</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-33",
    "theory_id": "theory-147",
    "fully_supporting_evidence": [
        {
            "text": "LAOM removes pixel reconstruction in favor of latent temporal consistency and achieves 2x downstream performance improvement and 8x better latent-action quality (via linear probe) compared to pixel-reconstruction baseline LAPO in distractor settings, with combined modifications yielding ~8x probe improvement, directly confirming that pixel reconstruction of task-irrelevant features wastes capacity and harms performance",
            "uuids": [
                "e2250.0",
                "e2250.1",
                "e2250.7"
            ]
        },
        {
            "text": "LaDi-WM's VFM-aligned latent diffusion world model outperforms pixel-space diffusion by 14.7% Avg.SR (71.7% vs 54.0%) on LIBERO-LONG manipulation tasks, demonstrating that geometric/semantic latent abstractions are superior to pixel-level prediction for robotic control",
            "uuids": [
                "e2252.0",
                "e2252.2"
            ]
        },
        {
            "text": "OCCAM object-centric masking variants (Object Masks, Planes) match or substantially exceed pixel-baseline (DQN-like) performance (e.g., MsPacman Object Masks 5880 vs DQN-like 3174, +85%) while showing better robustness to visual perturbations (higher GNS), confirming that removing background distractors via object-level abstraction improves both performance and generalization",
            "uuids": [
                "e2248.0",
                "e2248.1",
                "e2248.4",
                "e2248.6"
            ]
        },
        {
            "text": "OMC-RL's masked latent contrastive learning outperforms pixel-based CURL by +19.0pp success rate in furniture navigation (91.5% vs 72.5%) and shows superior transfer across environments and to real-world deployment, supporting that latent-level masked reconstruction focuses on task-relevant features better than pixel-level contrastive methods",
            "uuids": [
                "e2246.0",
                "e2246.1",
                "e2246.2"
            ]
        },
        {
            "text": "DisWM's disentangled latent world model with KL distillation achieves better sample efficiency and robustness to distractors than pixel-reconstruction baselines (DreamerV2, CURL, APV) across multiple continuous control tasks, with qualitative improvements in learning curves and cross-domain transfer (DMC→MuJoCo)",
            "uuids": [
                "e2247.0",
                "e2247.2",
                "e2247.4"
            ]
        },
        {
            "text": "Binary Masks and Planes abstractions in OCCAM remove within-object appearance details and often outperform pixel baselines (e.g., Planes in MsPacman 7187.50 vs DQN-like 3174.38), showing that aggressive abstraction away from pixel fidelity can improve learning when appearance is task-irrelevant, though at higher computational cost for Planes",
            "uuids": [
                "e2248.2",
                "e2248.4"
            ]
        },
        {
            "text": "LAOM with supervision achieves normalized score 0.44 using only 2.5% labeled transitions, recovering ~44% of BC full-data performance, and improves downstream performance by ~4.2-4.3x vs unsupervised LAOM in presence of distractors, demonstrating that task-aligned latent representations with minimal supervision are highly sample-efficient",
            "uuids": [
                "e2250.0"
            ]
        },
        {
            "text": "Multi-step IDM in LAOM (predicting latent actions from o_t and o_{t+k} for k∈{1..10}) doubled latent-action quality compared to single-step IDM, showing that temporal abstraction focusing on control-endogenous features improves task-relevance",
            "uuids": [
                "e2250.0",
                "e2250.8"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "LAOM achieves strong performance but requires supervision from ground-truth actions to avoid encoding action-correlated distractors; unsupervised LAOM still encodes exogenous noise into latent actions, suggesting task-alignment requires explicit task signals rather than emerging automatically from abstraction alone, though supervision requirements are minimal (2.5% labels)",
            "uuids": [
                "e2250.0",
                "e2250.2"
            ]
        },
        {
            "text": "LaDi-WM combines geometric (DINO) and semantic (SigLip) latent channels with interactive diffusion, showing that multiple abstraction levels working together (+3.4% Avg.SR for adding semantics) outperform single-level abstractions, suggesting optimal abstraction may be multi-level hybrid rather than selecting a single level",
            "uuids": [
                "e2252.0",
                "e2252.1",
                "e2252.3",
                "e2252.4"
            ]
        },
        {
            "text": "VITA achieves strong zero-shot value estimation (VOC 0.782 on tk_pnp, MT10 IQM 0.815) via test-time adaptation of CLIP features with meta-learned self-supervised objectives, demonstrating that task-relevance can be determined dynamically at test time through parameter adaptation rather than only during training, extending the theory's scope",
            "uuids": [
                "e2249.0",
                "e2249.3",
                "e2249.5"
            ]
        },
        {
            "text": "OMC-RL benefits substantially from oracle teacher guidance (privileged state information including depth and poses), with removal causing notable performance drops especially in complex scenes (Barrier/Tree), indicating that task-aligned representations are more effective when combined with privileged supervision during training",
            "uuids": [
                "e2246.0",
                "e2246.3"
            ]
        },
        {
            "text": "ReOI uses latent-space dynamics (DINO-WM) but reintroduces distractors via pixel-level compositing for downstream VLM-based visual verification, improving task success by up to 3× in presence of novel distractors, showing pixel fidelity has value for verification tasks even when dynamics operate in latent space, indicating hybrid approaches may be optimal",
            "uuids": [
                "e2251.0",
                "e2251.1",
                "e2251.7"
            ]
        },
        {
            "text": "DisWM's β-VAE pretraining on action-free distracting videos with KL-based latent distillation enables cross-domain transfer, but requires careful annealing of distillation weight (η: 0.1→0.01) to avoid overwriting pretrained structure, suggesting dynamic adjustment of abstraction influence during adaptation is important",
            "uuids": [
                "e2247.0",
                "e2247.1",
                "e2247.2"
            ]
        },
        {
            "text": "LAOM's computational efficiency improvements (removing image decoder reduces model size from ~211M to ~192M params and training time from ~7h38m to ~6h43m) support the theory's claim that task-aligned abstractions reduce computational cost, though gains are modest in this case",
            "uuids": [
                "e2250.0",
                "e2250.1"
            ]
        }
    ],
    "fully_contradicting_evidence": [
        {
            "text": "LAOM's cross-embodied transfer experiments show that supervised latent-action pretraining does not exceed BC trained only on target-domain labels, contradicting the theory's prediction that task-aligned abstractions should transfer better than pixel-level representations across domains",
            "uuids": [
                "e2250.0"
            ]
        },
        {
            "text": "VQ-VAE quantization (extreme information bottleneck intended for task-alignment) actively harms performance and causes codebook collapse, with removal of quantization improving latent-action probe quality substantially (Fig.5), suggesting that aggressive compression/abstraction via quantization can be counterproductive rather than beneficial",
            "uuids": [
                "e2250.1",
                "e2250.5",
                "e2250.6"
            ]
        }
    ],
    "partially_contradicting_evidence": [
        {
            "text": "Object Masks (preserving within-object appearance) outperform more abstract Binary Masks (4833.12 vs 5880.00 in MsPacman) and Class Masks (4549.38) in cases where ghost color encodes reward-relevant information, showing that pixel-level appearance fidelity is task-relevant in some cases and abstraction removes critical information",
            "uuids": [
                "e2248.1",
                "e2248.2",
                "e2248.3"
            ]
        },
        {
            "text": "Semantic Vector representation (extreme symbolic compression) performs poorly on spatially-demanding tasks by removing spatial relationships, with training time faster (~1h38m) but substantially worse performance, demonstrating that excessive abstraction beyond task requirements harms performance despite computational benefits",
            "uuids": [
                "e2248.5"
            ]
        },
        {
            "text": "CLIP-based methods (CLIP-FT, VLM-CL, VLM-RM) with semantic-level abstractions fail on temporally-dependent tasks (e.g., VLM-CL VOC 0.038 on tk_pnp vs VITA 0.782) despite semantic alignment, showing that semantic abstraction alone is insufficient without temporal modeling, indicating abstraction level must match multiple task dimensions simultaneously",
            "uuids": [
                "e2249.1",
                "e2249.6",
                "e2249.7"
            ]
        },
        {
            "text": "Pixel diffusion baseline achieves 54.0% Avg.SR on manipulation tasks, which while substantially lower than latent diffusion (71.7%, -14.7%), is not catastrophic failure, suggesting pixel-level models can achieve reasonable performance and the performance gap may be smaller than theory suggests in some domains",
            "uuids": [
                "e2252.2"
            ]
        },
        {
            "text": "GVL autoregressive VLM performs well on folding tasks (VOC 0.326-0.372) but poorly on pick-and-place (VOC 0.269) and stacking despite operating at semantic sequence level, showing that abstraction level alone doesn't determine performance and task-specific factors (temporal bias, execution variability) matter beyond abstraction choice",
            "uuids": [
                "e2249.2"
            ]
        },
        {
            "text": "OCCAM Planes representation increases computational cost (6-10 hours training vs 2-4 hours for simpler masks) despite often achieving best performance, showing that more structured task-aligned abstractions can have higher computational costs rather than always reducing cost as theory suggests",
            "uuids": [
                "e2248.4"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "Multiple methods (LAOM, OMC-RL, DisWM) show that supervision, privileged information, or explicit task signals are critical for achieving effective task-aligned representations, with LAOM requiring 2.5% labels for strong performance and OMC-RL benefiting from oracle guidance, suggesting the theory should explicitly incorporate the role of supervision in determining task-relevance rather than assuming it emerges from architecture alone",
            "uuids": [
                "e2250.0",
                "e2246.0",
                "e2246.3",
                "e2247.0"
            ]
        },
        {
            "text": "Temporal modeling is critical across multiple methods (VITA's test-time adaptation, OMC-RL's masked temporal contrastive learning, CLIP-FT failures on temporal tasks), indicating that task-aligned abstraction must consider temporal dimension explicitly as a separate axis from spatial/semantic abstraction level, suggesting theory should add temporal abstraction as a key dimension alongside semantic/geometric/pixel levels",
            "uuids": [
                "e2249.0",
                "e2249.3",
                "e2249.7",
                "e2246.0",
                "e2246.1"
            ]
        },
        {
            "text": "Test-time adaptation methods (VITA with meta-learned TTT, ReOI with VLM-based distractor detection and inpainting) show task-relevance can be determined dynamically at deployment rather than fixed at training time, suggesting theory should be expanded to include dynamic/adaptive abstraction mechanisms that adjust to distribution shifts and novel distractors beyond static training-time selection",
            "uuids": [
                "e2249.0",
                "e2249.3",
                "e2251.0"
            ]
        },
        {
            "text": "LaDi-WM and ReOI demonstrate that hybrid multi-level abstractions (combining geometric DINO, semantic SigLip, and pixel-level compositing for different purposes) outperform single-level abstractions, with LaDi-WM achieving 68.7% vs pixel 54.0% and ReOI improving success 3× via hybrid approach, suggesting theory should emphasize multi-level hybrid approaches rather than selecting a single optimal level",
            "uuids": [
                "e2252.0",
                "e2252.1",
                "e2251.0"
            ]
        },
        {
            "text": "OCCAM shows that simple explicit masking via bounding boxes can match or exceed learned representations (Object Masks 5880 vs DQN-like 3174 in MsPacman) while being more interpretable and requiring less pretraining overhead, suggesting theory should distinguish between learned vs explicit abstraction mechanisms and their relative merits in terms of performance, interpretability, and computational cost",
            "uuids": [
                "e2248.0",
                "e2248.1"
            ]
        },
        {
            "text": "Multiple failures of over-abstraction (Semantic Vector removing spatial relationships, VQ-VAE quantization causing collapse, Binary Masks losing color information in MsPacman) indicate there are lower bounds on information retention below which task-relevant information is lost, suggesting theory should explicitly characterize when abstraction becomes excessive and provide principles for determining minimal sufficient abstraction",
            "uuids": [
                "e2248.2",
                "e2248.3",
                "e2248.5",
                "e2250.5"
            ]
        },
        {
            "text": "LAOM's augmentations (random shifts, intensity changes) stabilize training and improve latent-action quality when combined with latent temporal consistency, suggesting that data augmentation is an important complementary mechanism to abstraction for achieving task-relevant representations, which should be incorporated into the theory",
            "uuids": [
                "e2250.0"
            ]
        },
        {
            "text": "Dissimilarity-based sampling in VITA (selecting diverse sub-trajectories) mitigates shortcut learning and improves discriminative performance (4/4 perfect vs worse for full-trajectory sampling), suggesting that training data selection strategies are important for learning task-relevant abstractions and should be considered alongside architectural choices",
            "uuids": [
                "e2249.0",
                "e2249.4"
            ]
        }
    ],
    "suggested_revisions": [
        "Add explicit statement that task-aligned abstractions typically require supervision, privileged information, or explicit task signals to achieve effective task-relevance, with minimal supervision (e.g., 2.5% labels in LAOM) often sufficient, rather than emerging automatically from unsupervised learning alone",
        "Incorporate temporal abstraction as a critical dimension alongside spatial/semantic abstraction, noting that semantic abstractions without temporal modeling fail on temporally-dependent tasks, and that temporal context can be encoded through sequential parameter updates (test-time adaptation) or architectural design (masked temporal contrastive learning)",
        "Expand theory to include dynamic/adaptive abstraction mechanisms that can adjust task-relevance at test time (e.g., test-time adaptation via meta-learned self-supervision, online intervention with VLM-based distractor detection) rather than only static training-time selection",
        "Modify theory to emphasize hybrid multi-level abstractions that combine different abstraction levels for different purposes (e.g., latent dynamics + pixel verification, geometric + semantic latents) rather than selecting a single optimal level, with explicit guidance on when each level is appropriate",
        "Add characterization of lower bounds on abstraction: specify that excessive compression (e.g., removing spatial relationships via Semantic Vector, extreme quantization via VQ-VAE, removing color information via Binary Masks) can harm performance when it discards task-relevant information, and provide principles for determining minimal sufficient abstraction (e.g., preserve information needed for task discrimination)",
        "Distinguish between learned task-aligned representations (e.g., DisWM's disentangled latents, OMC-RL's masked contrastive features) and explicit masking/filtering approaches (e.g., OCCAM's bounding-box masks), noting that simple explicit methods can sometimes match learned approaches with lower computational cost and better interpretability, though may require domain-specific design",
        "Clarify that transfer benefits of task-aligned abstractions are limited: supervised task-aligned pretraining does not always exceed training from scratch on target domain (LAOM cross-embodied results), especially under large domain shifts, and that transfer success depends on alignment between source and target task requirements",
        "Add special case that pixel-level appearance fidelity is necessary when fine-grained visual features encode task-relevant information (e.g., color-coded rewards in MsPacman, texture-based discrimination), not only for 'fine visual details directly impacting decisions', and that Object Masks preserving appearance can outperform more abstract representations in these cases",
        "Incorporate the role of data augmentation and training data selection (e.g., dissimilarity-based sampling) as complementary mechanisms to architectural abstraction for achieving task-relevant representations and avoiding shortcut learning",
        "Revise computational cost claims to acknowledge that some task-aligned abstractions (e.g., OCCAM Planes, multi-level hybrid models) can have higher computational costs than simpler approaches, and that cost benefits depend on specific implementation choices rather than being universal to all task-aligned abstractions"
    ],
    "overall_support_or_contradict": "support",
    "overall_support_or_contradict_explanation": "The new evidence strongly supports the core principle that task-aligned abstractions outperform pixel reconstruction for most control tasks, with multiple methods demonstrating substantial performance gains (LAOM 2× improvement, LaDi-WM +14.7%, OCCAM +85% in some games, OMC-RL +19pp). However, the evidence reveals important scope conditions: task-alignment often requires supervision, temporal modeling is critical, hybrid multi-level approaches may be superior to single-level abstractions, and there are lower bounds below which abstraction becomes excessive.",
    "revised_theory_ids": [
        "theory-395"
    ],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
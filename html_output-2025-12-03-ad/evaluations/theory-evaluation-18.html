<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Evaluation theory-evaluation-18 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Evaluation Details for theory-evaluation-18</h1>

        <div class="section">
            <h2>Theory Evaluation (General Information)</h2>
            <div class="info-section">
                <p><strong>Evaluation ID:</strong> theory-evaluation-18</p>
                <p><strong>This evaluation is for theory ID:</strong> <a href="../theories/theory-181.html">theory-181</a></p>
                <p><strong>This evaluation resulted in these revised theories:</strong> <a href="../theories/theory-378.html">theory-378</a></p>
                <p><strong>Overall Support or Contradict:</strong> support</p>
                <p><strong>Overall Support or Contradict Explanation:</strong> The evidence strongly supports the core theory claims about learned operators accessing richer hypothesis spaces, improving executability, and benefiting from hybrid approaches, with multiple systems demonstrating substantial improvements. The training bias limitation is explicitly confirmed with concrete failures. However, the evidence also reveals important nuances: representation matters as much as operators, traditional approaches remain competitive in many domains, and practical deployment requires careful cost management and validation infrastructure.</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Components)</h2>

            <h3>Fully Supporting Evidence</h3>
<ol>
    <li>AlphaEvolve demonstrates LLM-based operators discovering novel algorithms (4×4 matrix multiplication with 48 scalar multiplications improving on 49, 0.7% fleet-wide compute recovery, 23% kernel speedup) with ablations showing evolution, prompt context, meta-prompt adaptation, and whole-file evolution each materially contribute. Confirms hypothesis space expansion and sample efficiency claims. <a href="../results/extraction-result-1998.html#e1998.0" class="evidence-link">[e1998.0]</a> <a href="../results/extraction-result-1998.html#e1998.1" class="evidence-link">[e1998.1]</a> <a href="../results/extraction-result-1998.html#e1998.3" class="evidence-link">[e1998.3]</a> <a href="../results/extraction-result-1998.html#e1998.4" class="evidence-link">[e1998.4]</a> </li>
    <li>LLM-Assisted Crossover produces better variants (AOF 4.477s vs 4.598-5.169s traditional), accelerates search (25.6% fewer variants to milestones), yields more viable variants (~88.4% vs ~84.3%), and requires no task-specific pretraining, directly confirming that context-aware pretrained LLMs improve executability and search efficiency. <a href="../results/extraction-result-1992.html#e1992.0" class="evidence-link">[e1992.0]</a> </li>
    <li>LLM-Meta-SR's evolved Omni selection operator outperforms 9 expert-designed baselines on SRBench. Ablations show semantic feedback (per-instance score vectors) is critical, bloat control necessary, and domain knowledge in prompts essential. Successfully transfers to RAG-SR, confirming learned operators can encode domain-specific heuristics and transfer across systems. <a href="../results/extraction-result-2000.html#e2000.0" class="evidence-link">[e2000.0]</a> <a href="../results/extraction-result-2000.html#e2000.1" class="evidence-link">[e2000.1]</a> <a href="../results/extraction-result-2000.html#e2000.3" class="evidence-link">[e2000.3]</a> </li>
    <li>End2end LSPT model explicitly failed on dynamical systems due to absence of ODE/PDE examples in pretraining corpus, exhibiting highest predictive errors and performing worse than GP and linear baselines, directly confirming the theory's training bias limitation with concrete failure evidence. <a href="../results/extraction-result-2002.html#e2002.1" class="evidence-link">[e2002.1]</a> </li>
    <li>ODEFormer shows rapid performance degradation under moderate-to-high noise (SNR ≤20 dB) due to overfitting from large parameter count, providing additional concrete evidence of training distribution bias and sensitivity to data characteristics differing from pretraining. <a href="../results/extraction-result-2002.html#e2002.0" class="evidence-link">[e2002.0]</a> </li>
    <li>Multiple systems demonstrate effective cost management strategies confirming computational cost can be managed: AlphaEvolve ensemble (80% cheap Flash, 20% expensive Pro), stagnation-triggered LLM invocation reducing unnecessary calls, CoCoEvo early stopping (30-50% token reduction), supporting the theory's claim that cost-effectiveness trade-offs can be optimized. <a href="../results/extraction-result-1998.html#e1998.0" class="evidence-link">[e1998.0]</a> <a href="../results/extraction-result-1995.html#e1995.3" class="evidence-link">[e1995.3]</a> <a href="../results/extraction-result-1993.html#e1993.0" class="evidence-link">[e1993.0]</a> </li>
</ol>            <h3>Partially Supporting Evidence</h3>
<ol>
    <li>CODEEVOLVE outperforms AlphaEvolve on 5/6 benchmarks using meta-prompting and inspiration-based crossover, but ablations show problem-dependent operator utility (meta-prompting excels on analytic tasks, inspiration on geometric problems), indicating learned operators provide benefits but no single approach dominates across problem types. <a href="../results/extraction-result-1989.html#e1989.0" class="evidence-link">[e1989.0]</a> </li>
    <li>CoCoEvo's LLM-driven crossover/mutation with test co-evolution outperforms baselines (pass@1: 76.25% DeepSeek-V3), but token costs are high (crossover/mutation include programs in prompts) and performance depends on LLM capability, showing benefits come with significant computational overhead and model-dependent behavior. <a href="../results/extraction-result-1993.html#e1993.0" class="evidence-link">[e1993.0]</a> <a href="../results/extraction-result-1993.html#e1993.2" class="evidence-link">[e1993.2]</a> </li>
    <li>SEIDR matches/outperforms PushGP (19/25 vs 17/25 on PSB2-Python) with dramatically lower program executions (<1000 vs billions), but code-specialized Codex outperforms general GPT-3.5 and near-miss syndrome requires careful search strategies, showing learned operators improve efficiency but require specialization and careful design. <a href="../results/extraction-result-1999.html#e1999.0" class="evidence-link">[e1999.0]</a> <a href="../results/extraction-result-1999.html#e1999.3" class="evidence-link">[e1999.3]</a> </li>
    <li>TreEvo achieves 5×-100× evaluation reduction vs traditional methods, but ablations show representation (tree structure) provides larger gain (~35%) than semantic operators alone (~10-15%), suggesting structured representations may be more important than learned operators per se for search efficiency. <a href="../results/extraction-result-1996.html#e1996.0" class="evidence-link">[e1996.0]</a> <a href="../results/extraction-result-1996.html#e1996.3" class="evidence-link">[e1996.3]</a> </li>
    <li>MOTIF's multi-strategy LLM-based operators with self-play outperform baselines, with dynamic baseline and reasoning crucial, but computational cost (~$0.18/run, 1.5 hours) not directly compared to classical GP and no cross-domain transfer tested, limiting generalizability claims. <a href="../results/extraction-result-2001.html#e2001.0" class="evidence-link">[e2001.0]</a> </li>
    <li>REMoH's LLM-generated heuristics with reflection achieve competitive multi-objective trade-offs and reflection mechanism is crucial for diversity, but different LLMs show different training vs generalization dynamics (DeepSeek improves during training but generalizes worse than GPT), indicating model-specific behavior and training bias effects. <a href="../results/extraction-result-1997.html#e1997.0" class="evidence-link">[e1997.0]</a> </li>
    <li>VEO's MLLM-based operators show improvements over traditional EA on graph problems, but require careful validation checks (hallucination risk), higher per-call latency (2-5s), and are sensitive to visualization layout and prompt framing, indicating practical limitations and brittleness of learned operators. <a href="../results/extraction-result-1994.html#e1994.0" class="evidence-link">[e1994.0]</a> </li>
    <li>GA-LLM demonstrates LLM-guided variation produces semantically coherent offspring and accelerates convergence, but stochasticity in LLM scoring introduces variance and computational overhead from many LLM calls limits population size and generations, showing cost-quality trade-offs are significant. <a href="../results/extraction-result-1988.html#e1988.0" class="evidence-link">[e1988.0]</a> </li>
    <li>Hybrid approaches show benefits but with caveats: Lyria alternates LLM and external operators with problem-dependent gains (external operators helped some problems +6% but hurt others -6%), and Oracle evaluator dramatically outperforms LLM evaluator (84 vs ~50), indicating hybrid benefits are problem-dependent and LLM evaluation is unreliable. <a href="../results/extraction-result-1991.html#e1991.0" class="evidence-link">[e1991.0]</a> </li>
</ol>            <h3>Fully Contradicting Evidence</h3>
<p class="empty-note">No evidence provided.</p>            <h3>Partially Contradicting Evidence</h3>
<ol>
    <li>Traditional GP and mutation-only systems achieve strong results without learned operators: CGP with simple mutation produces large diverse libraries (16,833 non-dominated implementations), POET-GP using standard operators successfully rediscovered known potentials and discovered new ones, PySR/Operon remain competitive on clean data, and simple UMAD mutation performs strongly as baseline, contradicting the claim that learned operators are necessary for strong performance. <a href="../results/extraction-result-1740.html#e1740.0" class="evidence-link">[e1740.0]</a> <a href="../results/extraction-result-1740.html#e1740.1" class="evidence-link">[e1740.1]</a> <a href="../results/extraction-result-1740.html#e1740.2" class="evidence-link">[e1740.2]</a> <a href="../results/extraction-result-1740.html#e1740.3" class="evidence-link">[e1740.3]</a> <a href="../results/extraction-result-1571.html#e1571.0" class="evidence-link">[e1571.0]</a> <a href="../results/extraction-result-2002.html#e2002.3" class="evidence-link">[e2002.3]</a> <a href="../results/extraction-result-2002.html#e2002.4" class="evidence-link">[e2002.4]</a> <a href="../results/extraction-result-1606.html#e1606.1" class="evidence-link">[e1606.1]</a> <a href="../results/extraction-result-1603.html#e1603.1" class="evidence-link">[e1603.1]</a> </li>
    <li>AI Programmer using minimal 8-instruction language with simple mutation successfully generated functional programs, and grammar-guided GP with hand-designed constraints achieves high executability, suggesting constrained operator spaces and formal systems may be as effective as learned priors for some domains, contradicting the emphasis on learned operators. <a href="../results/extraction-result-1593.html#e1593.0" class="evidence-link">[e1593.0]</a> <a href="../results/extraction-result-1610.html#e1610.1" class="evidence-link">[e1610.1]</a> <a href="../results/extraction-result-1619.html#e1619.2" class="evidence-link">[e1619.2]</a> </li>
    <li>PIPE's learned probabilistic model (univariate) failed to outperform GP on problems with interacting components, demonstrating that learned operators must capture appropriate structure or they provide no benefit, contradicting the general claim that learned operators improve over traditional approaches. <a href="../results/extraction-result-1564.html#e1564.1" class="evidence-link">[e1564.1]</a> </li>
    <li>Domain-specific pretraining shows mixed results: while code-specialized Codex outperforms general GPT-3.5 on demanding benchmarks (PSB2), general LLMs can work without domain-specific fine-tuning when properly prompted (LLM-Assisted Crossover), and Omni evolved from general LLM with domain knowledge in prompts, suggesting the theory overstates the necessity of domain-specific pretraining. <a href="../results/extraction-result-1999.html#e1999.0" class="evidence-link">[e1999.0]</a> <a href="../results/extraction-result-1992.html#e1992.0" class="evidence-link">[e1992.0]</a> <a href="../results/extraction-result-2000.html#e2000.0" class="evidence-link">[e2000.0]</a> </li>
</ol>            <h3>Potentially Modifying Evidence</h3>
<ol>
    <li>Multiple systems show representation/structure is as important or more important than learned operators: TreEvo's tree structure provides 35% gain vs 10-15% from operators, constrained operator spaces (AI Programmer, grammar-guided GP) make search tractable, and CBGP's type system enables polymorphic generation. Theory should emphasize the interaction between representation and operators rather than focusing primarily on operators. <a href="../results/extraction-result-1996.html#e1996.0" class="evidence-link">[e1996.0]</a> <a href="../results/extraction-result-1593.html#e1593.0" class="evidence-link">[e1593.0]</a> <a href="../results/extraction-result-1610.html#e1610.1" class="evidence-link">[e1610.1]</a> <a href="../results/extraction-result-1619.html#e1619.2" class="evidence-link">[e1619.2]</a> <a href="../results/extraction-result-1617.html#e1617.0" class="evidence-link">[e1617.0]</a> </li>
    <li>Prompt-based adaptation (meta-prompting, dynamic baselines, reflection, stage-aware weighting) provides substantial benefits without online fine-tuning in most successful systems (CODEEVOLVE, MOTIF, REMoH, CoCoEvo, Omni), suggesting the theory should emphasize prompt engineering and in-context learning as primary adaptation mechanisms rather than treating fine-tuning and prompt adaptation as equally important. <a href="../results/extraction-result-1989.html#e1989.0" class="evidence-link">[e1989.0]</a> <a href="../results/extraction-result-2001.html#e2001.0" class="evidence-link">[e2001.0]</a> <a href="../results/extraction-result-1997.html#e1997.0" class="evidence-link">[e1997.0]</a> <a href="../results/extraction-result-1993.html#e1993.0" class="evidence-link">[e1993.0]</a> <a href="../results/extraction-result-2000.html#e2000.0" class="evidence-link">[e2000.0]</a> </li>
    <li>Cost management strategies are emerging as critical practical components: ensemble strategies (cheap/expensive model mixing), selective invocation (stagnation-triggered), early stopping, token reduction techniques, and cost-aware invocation policies. Theory should incorporate these as essential components of practical learned operator systems rather than just noting cost as a limitation. <a href="../results/extraction-result-1998.html#e1998.0" class="evidence-link">[e1998.0]</a> <a href="../results/extraction-result-1995.html#e1995.3" class="evidence-link">[e1995.3]</a> <a href="../results/extraction-result-1993.html#e1993.0" class="evidence-link">[e1993.0]</a> <a href="../results/extraction-result-1995.html#e1995.0" class="evidence-link">[e1995.0]</a> <a href="../results/extraction-result-1995.html#e1995.2" class="evidence-link">[e1995.2]</a> </li>
    <li>Validation and reliability mechanisms are necessary for learned operators: VEO requires validation checks for hallucination, Lyria shows LLM evaluators are weak vs Oracle (84 vs ~50), CoCoEvo needs multi-objective test selection, and multiple systems implement fallback mechanisms. Theory should emphasize that learned operators require robust validation infrastructure and cannot be trusted blindly, especially for evaluation/fitness functions. <a href="../results/extraction-result-1994.html#e1994.0" class="evidence-link">[e1994.0]</a> <a href="../results/extraction-result-1991.html#e1991.0" class="evidence-link">[e1991.0]</a> <a href="../results/extraction-result-1993.html#e1993.0" class="evidence-link">[e1993.0]</a> </li>
    <li>Problem-dependent operator utility is pervasive across systems: CODEEVOLVE shows meta-prompting works for analytic tasks while inspiration helps geometric problems, Lyria shows external operators help some problems but hurt others, Omni had small-dataset bug from meta-evolution on larger datasets, and different LLMs show different training vs generalization trade-offs. Theory should more strongly emphasize that no single operator approach dominates across all problem types and operator effectiveness depends on problem characteristics. <a href="../results/extraction-result-1989.html#e1989.0" class="evidence-link">[e1989.0]</a> <a href="../results/extraction-result-1991.html#e1991.0" class="evidence-link">[e1991.0]</a> <a href="../results/extraction-result-2000.html#e2000.0" class="evidence-link">[e2000.0]</a> <a href="../results/extraction-result-1997.html#e1997.0" class="evidence-link">[e1997.0]</a> </li>
    <li>Traditional operators remain competitive in many domains, especially with appropriate representations and constraints (CGP, POET-GP, PySR/Operon on clean data, grammar-guided GP, simple UMAD). Theory should clarify that learned operators' benefits are most pronounced when evaluation is expensive, search space is large and poorly structured, and domain has strong statistical regularities, rather than claiming general superiority. <a href="../results/extraction-result-1740.html#e1740.0" class="evidence-link">[e1740.0]</a> <a href="../results/extraction-result-1571.html#e1571.0" class="evidence-link">[e1571.0]</a> <a href="../results/extraction-result-2002.html#e2002.3" class="evidence-link">[e2002.3]</a> <a href="../results/extraction-result-2002.html#e2002.4" class="evidence-link">[e2002.4]</a> <a href="../results/extraction-result-1610.html#e1610.1" class="evidence-link">[e1610.1]</a> <a href="../results/extraction-result-1606.html#e1606.1" class="evidence-link">[e1606.1]</a> </li>
    <li>Limited evidence on truly novel out-of-distribution discovery: most successes (AlphaEvolve, CODEEVOLVE, LLM-Assisted Crossover) are on problems with patterns likely present in training data. End2end and ODEFormer failures on out-of-distribution problems suggest learned operators may be fundamentally limited to recombining known patterns. Theory's claim about discovering fundamentally novel solutions lacks strong supporting evidence. <a href="../results/extraction-result-1998.html#e1998.0" class="evidence-link">[e1998.0]</a> <a href="../results/extraction-result-1989.html#e1989.0" class="evidence-link">[e1989.0]</a> <a href="../results/extraction-result-1992.html#e1992.0" class="evidence-link">[e1992.0]</a> <a href="../results/extraction-result-2002.html#e2002.1" class="evidence-link">[e2002.1]</a> <a href="../results/extraction-result-2002.html#e2002.0" class="evidence-link">[e2002.0]</a> </li>
    <li>Hybrid approaches show benefits but optimal mix is problem-dependent: Lyria's external operators helped some problems but hurt others, different problems benefit from different operator combinations (CODEEVOLVE meta-prompting vs inspiration), and no universal hybrid strategy emerges. Theory should note that hybrid benefits require careful problem-specific design rather than being universally beneficial. <a href="../results/extraction-result-1991.html#e1991.0" class="evidence-link">[e1991.0]</a> <a href="../results/extraction-result-1989.html#e1989.0" class="evidence-link">[e1989.0]</a> </li>
</ol>            <h3>Suggested Revisions</h3>
            <ol>
                <li>Add explicit statement that representation/structure may be as important or more important than learned operators themselves, with structured representations (trees, types, grammars) providing substantial gains independent of operator sophistication.</li>
                <li>Strengthen the training bias limitation section with concrete failure cases (End2end, ODEFormer) and emphasize this is not just a theoretical concern but an observed failure mode that can cause catastrophic performance degradation on out-of-distribution problems.</li>
                <li>Revise the computational cost discussion to emphasize emerging cost management strategies (ensemble mixing, selective invocation, early stopping, token reduction) as essential components of practical systems rather than just noting cost as a trade-off.</li>
                <li>Clarify that prompt-based adaptation and in-context learning are the primary adaptation mechanisms in successful systems, with online fine-tuning being rare and less important than the theory currently suggests.</li>
                <li>Add statement that learned operators require robust validation infrastructure (hallucination checks, multi-objective test selection, oracle evaluators, fallback mechanisms) and cannot be trusted blindly, especially for evaluation/fitness functions.</li>
                <li>Strengthen the problem-dependent operator utility claim with evidence that different learned operators excel on different problem types (analytic vs geometric, small vs large datasets, different noise levels) and no single approach dominates universally.</li>
                <li>Modify the domain-specific pretraining claim to note that general LLMs with proper prompting and domain knowledge injection can sometimes match domain-specific models, though domain-specific pretraining helps on demanding benchmarks.</li>
                <li>Revise the necessity claim to clarify that traditional operators remain competitive in many domains (especially with appropriate representations and constraints), and learned operators' benefits are most pronounced when evaluation is expensive, search space is large and poorly structured, and domain has strong statistical regularities.</li>
                <li>Expand the hybrid approach discussion to note that the optimal mix of learned/traditional/formal operators is problem-dependent and requires careful design, with some problems benefiting from external operators while others are hurt by them.</li>
                <li>Add caveat to novelty/discovery claims that most evidence shows success on problems with training-distribution patterns, and truly novel out-of-distribution discovery remains an open question with limited supporting evidence.</li>
                <li>Revise the hypothesis space characterization to emphasize that learned operators may expand expressiveness while simultaneously narrowing effective search trajectories via training priors, creating both benefits and limitations.</li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory Evaluation (Debug)</h2>
            <pre><code>{
    "id": "theory-evaluation-18",
    "theory_id": "theory-181",
    "fully_supporting_evidence": [
        {
            "text": "AlphaEvolve demonstrates LLM-based operators discovering novel algorithms (4×4 matrix multiplication with 48 scalar multiplications improving on 49, 0.7% fleet-wide compute recovery, 23% kernel speedup) with ablations showing evolution, prompt context, meta-prompt adaptation, and whole-file evolution each materially contribute. Confirms hypothesis space expansion and sample efficiency claims.",
            "uuids": [
                "e1998.0",
                "e1998.1",
                "e1998.3",
                "e1998.4"
            ]
        },
        {
            "text": "LLM-Assisted Crossover produces better variants (AOF 4.477s vs 4.598-5.169s traditional), accelerates search (25.6% fewer variants to milestones), yields more viable variants (~88.4% vs ~84.3%), and requires no task-specific pretraining, directly confirming that context-aware pretrained LLMs improve executability and search efficiency.",
            "uuids": [
                "e1992.0"
            ]
        },
        {
            "text": "LLM-Meta-SR's evolved Omni selection operator outperforms 9 expert-designed baselines on SRBench. Ablations show semantic feedback (per-instance score vectors) is critical, bloat control necessary, and domain knowledge in prompts essential. Successfully transfers to RAG-SR, confirming learned operators can encode domain-specific heuristics and transfer across systems.",
            "uuids": [
                "e2000.0",
                "e2000.1",
                "e2000.3"
            ]
        },
        {
            "text": "End2end LSPT model explicitly failed on dynamical systems due to absence of ODE/PDE examples in pretraining corpus, exhibiting highest predictive errors and performing worse than GP and linear baselines, directly confirming the theory's training bias limitation with concrete failure evidence.",
            "uuids": [
                "e2002.1"
            ]
        },
        {
            "text": "ODEFormer shows rapid performance degradation under moderate-to-high noise (SNR ≤20 dB) due to overfitting from large parameter count, providing additional concrete evidence of training distribution bias and sensitivity to data characteristics differing from pretraining.",
            "uuids": [
                "e2002.0"
            ]
        },
        {
            "text": "Multiple systems demonstrate effective cost management strategies confirming computational cost can be managed: AlphaEvolve ensemble (80% cheap Flash, 20% expensive Pro), stagnation-triggered LLM invocation reducing unnecessary calls, CoCoEvo early stopping (30-50% token reduction), supporting the theory's claim that cost-effectiveness trade-offs can be optimized.",
            "uuids": [
                "e1998.0",
                "e1995.3",
                "e1993.0"
            ]
        }
    ],
    "partially_supporting_evidence": [
        {
            "text": "CODEEVOLVE outperforms AlphaEvolve on 5/6 benchmarks using meta-prompting and inspiration-based crossover, but ablations show problem-dependent operator utility (meta-prompting excels on analytic tasks, inspiration on geometric problems), indicating learned operators provide benefits but no single approach dominates across problem types.",
            "uuids": [
                "e1989.0"
            ]
        },
        {
            "text": "CoCoEvo's LLM-driven crossover/mutation with test co-evolution outperforms baselines (pass@1: 76.25% DeepSeek-V3), but token costs are high (crossover/mutation include programs in prompts) and performance depends on LLM capability, showing benefits come with significant computational overhead and model-dependent behavior.",
            "uuids": [
                "e1993.0",
                "e1993.2"
            ]
        },
        {
            "text": "SEIDR matches/outperforms PushGP (19/25 vs 17/25 on PSB2-Python) with dramatically lower program executions (&lt;1000 vs billions), but code-specialized Codex outperforms general GPT-3.5 and near-miss syndrome requires careful search strategies, showing learned operators improve efficiency but require specialization and careful design.",
            "uuids": [
                "e1999.0",
                "e1999.3"
            ]
        },
        {
            "text": "TreEvo achieves 5×-100× evaluation reduction vs traditional methods, but ablations show representation (tree structure) provides larger gain (~35%) than semantic operators alone (~10-15%), suggesting structured representations may be more important than learned operators per se for search efficiency.",
            "uuids": [
                "e1996.0",
                "e1996.3"
            ]
        },
        {
            "text": "MOTIF's multi-strategy LLM-based operators with self-play outperform baselines, with dynamic baseline and reasoning crucial, but computational cost (~$0.18/run, 1.5 hours) not directly compared to classical GP and no cross-domain transfer tested, limiting generalizability claims.",
            "uuids": [
                "e2001.0"
            ]
        },
        {
            "text": "REMoH's LLM-generated heuristics with reflection achieve competitive multi-objective trade-offs and reflection mechanism is crucial for diversity, but different LLMs show different training vs generalization dynamics (DeepSeek improves during training but generalizes worse than GPT), indicating model-specific behavior and training bias effects.",
            "uuids": [
                "e1997.0"
            ]
        },
        {
            "text": "VEO's MLLM-based operators show improvements over traditional EA on graph problems, but require careful validation checks (hallucination risk), higher per-call latency (2-5s), and are sensitive to visualization layout and prompt framing, indicating practical limitations and brittleness of learned operators.",
            "uuids": [
                "e1994.0"
            ]
        },
        {
            "text": "GA-LLM demonstrates LLM-guided variation produces semantically coherent offspring and accelerates convergence, but stochasticity in LLM scoring introduces variance and computational overhead from many LLM calls limits population size and generations, showing cost-quality trade-offs are significant.",
            "uuids": [
                "e1988.0"
            ]
        },
        {
            "text": "Hybrid approaches show benefits but with caveats: Lyria alternates LLM and external operators with problem-dependent gains (external operators helped some problems +6% but hurt others -6%), and Oracle evaluator dramatically outperforms LLM evaluator (84 vs ~50), indicating hybrid benefits are problem-dependent and LLM evaluation is unreliable.",
            "uuids": [
                "e1991.0"
            ]
        }
    ],
    "fully_contradicting_evidence": [],
    "partially_contradicting_evidence": [
        {
            "text": "Traditional GP and mutation-only systems achieve strong results without learned operators: CGP with simple mutation produces large diverse libraries (16,833 non-dominated implementations), POET-GP using standard operators successfully rediscovered known potentials and discovered new ones, PySR/Operon remain competitive on clean data, and simple UMAD mutation performs strongly as baseline, contradicting the claim that learned operators are necessary for strong performance.",
            "uuids": [
                "e1740.0",
                "e1740.1",
                "e1740.2",
                "e1740.3",
                "e1571.0",
                "e2002.3",
                "e2002.4",
                "e1606.1",
                "e1603.1"
            ]
        },
        {
            "text": "AI Programmer using minimal 8-instruction language with simple mutation successfully generated functional programs, and grammar-guided GP with hand-designed constraints achieves high executability, suggesting constrained operator spaces and formal systems may be as effective as learned priors for some domains, contradicting the emphasis on learned operators.",
            "uuids": [
                "e1593.0",
                "e1610.1",
                "e1619.2"
            ]
        },
        {
            "text": "PIPE's learned probabilistic model (univariate) failed to outperform GP on problems with interacting components, demonstrating that learned operators must capture appropriate structure or they provide no benefit, contradicting the general claim that learned operators improve over traditional approaches.",
            "uuids": [
                "e1564.1"
            ]
        },
        {
            "text": "Domain-specific pretraining shows mixed results: while code-specialized Codex outperforms general GPT-3.5 on demanding benchmarks (PSB2), general LLMs can work without domain-specific fine-tuning when properly prompted (LLM-Assisted Crossover), and Omni evolved from general LLM with domain knowledge in prompts, suggesting the theory overstates the necessity of domain-specific pretraining.",
            "uuids": [
                "e1999.0",
                "e1992.0",
                "e2000.0"
            ]
        }
    ],
    "potentially_modifying_evidence": [
        {
            "text": "Multiple systems show representation/structure is as important or more important than learned operators: TreEvo's tree structure provides 35% gain vs 10-15% from operators, constrained operator spaces (AI Programmer, grammar-guided GP) make search tractable, and CBGP's type system enables polymorphic generation. Theory should emphasize the interaction between representation and operators rather than focusing primarily on operators.",
            "uuids": [
                "e1996.0",
                "e1593.0",
                "e1610.1",
                "e1619.2",
                "e1617.0"
            ]
        },
        {
            "text": "Prompt-based adaptation (meta-prompting, dynamic baselines, reflection, stage-aware weighting) provides substantial benefits without online fine-tuning in most successful systems (CODEEVOLVE, MOTIF, REMoH, CoCoEvo, Omni), suggesting the theory should emphasize prompt engineering and in-context learning as primary adaptation mechanisms rather than treating fine-tuning and prompt adaptation as equally important.",
            "uuids": [
                "e1989.0",
                "e2001.0",
                "e1997.0",
                "e1993.0",
                "e2000.0"
            ]
        },
        {
            "text": "Cost management strategies are emerging as critical practical components: ensemble strategies (cheap/expensive model mixing), selective invocation (stagnation-triggered), early stopping, token reduction techniques, and cost-aware invocation policies. Theory should incorporate these as essential components of practical learned operator systems rather than just noting cost as a limitation.",
            "uuids": [
                "e1998.0",
                "e1995.3",
                "e1993.0",
                "e1995.0",
                "e1995.2"
            ]
        },
        {
            "text": "Validation and reliability mechanisms are necessary for learned operators: VEO requires validation checks for hallucination, Lyria shows LLM evaluators are weak vs Oracle (84 vs ~50), CoCoEvo needs multi-objective test selection, and multiple systems implement fallback mechanisms. Theory should emphasize that learned operators require robust validation infrastructure and cannot be trusted blindly, especially for evaluation/fitness functions.",
            "uuids": [
                "e1994.0",
                "e1991.0",
                "e1993.0"
            ]
        },
        {
            "text": "Problem-dependent operator utility is pervasive across systems: CODEEVOLVE shows meta-prompting works for analytic tasks while inspiration helps geometric problems, Lyria shows external operators help some problems but hurt others, Omni had small-dataset bug from meta-evolution on larger datasets, and different LLMs show different training vs generalization trade-offs. Theory should more strongly emphasize that no single operator approach dominates across all problem types and operator effectiveness depends on problem characteristics.",
            "uuids": [
                "e1989.0",
                "e1991.0",
                "e2000.0",
                "e1997.0"
            ]
        },
        {
            "text": "Traditional operators remain competitive in many domains, especially with appropriate representations and constraints (CGP, POET-GP, PySR/Operon on clean data, grammar-guided GP, simple UMAD). Theory should clarify that learned operators' benefits are most pronounced when evaluation is expensive, search space is large and poorly structured, and domain has strong statistical regularities, rather than claiming general superiority.",
            "uuids": [
                "e1740.0",
                "e1571.0",
                "e2002.3",
                "e2002.4",
                "e1610.1",
                "e1606.1"
            ]
        },
        {
            "text": "Limited evidence on truly novel out-of-distribution discovery: most successes (AlphaEvolve, CODEEVOLVE, LLM-Assisted Crossover) are on problems with patterns likely present in training data. End2end and ODEFormer failures on out-of-distribution problems suggest learned operators may be fundamentally limited to recombining known patterns. Theory's claim about discovering fundamentally novel solutions lacks strong supporting evidence.",
            "uuids": [
                "e1998.0",
                "e1989.0",
                "e1992.0",
                "e2002.1",
                "e2002.0"
            ]
        },
        {
            "text": "Hybrid approaches show benefits but optimal mix is problem-dependent: Lyria's external operators helped some problems but hurt others, different problems benefit from different operator combinations (CODEEVOLVE meta-prompting vs inspiration), and no universal hybrid strategy emerges. Theory should note that hybrid benefits require careful problem-specific design rather than being universally beneficial.",
            "uuids": [
                "e1991.0",
                "e1989.0"
            ]
        }
    ],
    "suggested_revisions": [
        "Add explicit statement that representation/structure may be as important or more important than learned operators themselves, with structured representations (trees, types, grammars) providing substantial gains independent of operator sophistication.",
        "Strengthen the training bias limitation section with concrete failure cases (End2end, ODEFormer) and emphasize this is not just a theoretical concern but an observed failure mode that can cause catastrophic performance degradation on out-of-distribution problems.",
        "Revise the computational cost discussion to emphasize emerging cost management strategies (ensemble mixing, selective invocation, early stopping, token reduction) as essential components of practical systems rather than just noting cost as a trade-off.",
        "Clarify that prompt-based adaptation and in-context learning are the primary adaptation mechanisms in successful systems, with online fine-tuning being rare and less important than the theory currently suggests.",
        "Add statement that learned operators require robust validation infrastructure (hallucination checks, multi-objective test selection, oracle evaluators, fallback mechanisms) and cannot be trusted blindly, especially for evaluation/fitness functions.",
        "Strengthen the problem-dependent operator utility claim with evidence that different learned operators excel on different problem types (analytic vs geometric, small vs large datasets, different noise levels) and no single approach dominates universally.",
        "Modify the domain-specific pretraining claim to note that general LLMs with proper prompting and domain knowledge injection can sometimes match domain-specific models, though domain-specific pretraining helps on demanding benchmarks.",
        "Revise the necessity claim to clarify that traditional operators remain competitive in many domains (especially with appropriate representations and constraints), and learned operators' benefits are most pronounced when evaluation is expensive, search space is large and poorly structured, and domain has strong statistical regularities.",
        "Expand the hybrid approach discussion to note that the optimal mix of learned/traditional/formal operators is problem-dependent and requires careful design, with some problems benefiting from external operators while others are hurt by them.",
        "Add caveat to novelty/discovery claims that most evidence shows success on problems with training-distribution patterns, and truly novel out-of-distribution discovery remains an open question with limited supporting evidence.",
        "Revise the hypothesis space characterization to emphasize that learned operators may expand expressiveness while simultaneously narrowing effective search trajectories via training priors, creating both benefits and limitations."
    ],
    "overall_support_or_contradict": "support",
    "overall_support_or_contradict_explanation": "The evidence strongly supports the core theory claims about learned operators accessing richer hypothesis spaces, improving executability, and benefiting from hybrid approaches, with multiple systems demonstrating substantial improvements. The training bias limitation is explicitly confirmed with concrete failures. However, the evidence also reveals important nuances: representation matters as much as operators, traditional approaches remain competitive in many domains, and practical deployment requires careful cost management and validation infrastructure.",
    "revised_theory_ids": [
        "theory-378"
    ],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
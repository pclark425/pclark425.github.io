<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-46 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-46</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-46</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>system_name</strong></td>
                        <td>str</td>
                        <td>The name of the system, method, or approach being described.</td>
                    </tr>
                    <tr>
                        <td><strong>operator_type</strong></td>
                        <td>str</td>
                        <td>What type of operator is used? (e.g. 'LLM-based', 'neural network', 'traditional GP', 'grammar-guided', 'hybrid', 'hand-designed', 'learned from data', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>operator_description</strong></td>
                        <td>str</td>
                        <td>Detailed description of how the operator works, including what it was trained on (if learned), what model architecture (if neural), or what rules (if hand-designed).</td>
                    </tr>
                    <tr>
                        <td><strong>training_data_description</strong></td>
                        <td>str</td>
                        <td>If the operator is learned, describe the training data: size, domain, source, diversity. (null if not applicable or not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>domain_or_benchmark</strong></td>
                        <td>str</td>
                        <td>What problem domain or benchmark is the system evaluated on? Be specific (e.g. 'program synthesis on HumanEval', 'symbolic regression', 'circuit design', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_baseline</strong></td>
                        <td>str</td>
                        <td>What baseline methods are compared? List all baselines mentioned (e.g. 'standard GP with subtree crossover', 'random search', 'grammar-guided GP', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_learned_operator</strong></td>
                        <td>str</td>
                        <td>Performance of the learned/neural operator approach. Include metric name, numerical value, and units. (null if not applicable)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_traditional_operator</strong></td>
                        <td>str</td>
                        <td>Performance of traditional/baseline operator approach on the same task. Include metric name, numerical value, and units. (null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_hybrid_operator</strong></td>
                        <td>str</td>
                        <td>If a hybrid approach combining learned and traditional operators is tested, report its performance. Include metric name, numerical value, and units. (null if not applicable)</td>
                    </tr>
                    <tr>
                        <td><strong>validity_or_executability_rate</strong></td>
                        <td>str</td>
                        <td>What percentage of generated solutions are valid/executable/compilable? Report separately for learned vs traditional if both are given. (null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_or_diversity_metric</strong></td>
                        <td>str</td>
                        <td>How is novelty or diversity measured? Include metric name and values for different operator types if compared. (null if not measured)</td>
                    </tr>
                    <tr>
                        <td><strong>out_of_distribution_performance</strong></td>
                        <td>str</td>
                        <td>Is performance on out-of-distribution or novel problems reported? If so, describe the results and how learned operators compare to traditional ones. (null if not tested)</td>
                    </tr>
                    <tr>
                        <td><strong>training_bias_evidence</strong></td>
                        <td>str</td>
                        <td>Is there evidence that learned operators are biased toward training distribution patterns? Describe any findings about limitations or failures on novel problems. (null if not discussed)</td>
                    </tr>
                    <tr>
                        <td><strong>computational_cost_comparison</strong></td>
                        <td>str</td>
                        <td>Are computational costs (time, memory, inference cost, etc.) compared between learned and traditional operators? Report quantitative comparisons. (null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>transfer_learning_results</strong></td>
                        <td>str</td>
                        <td>Is transfer of learned operators across domains tested? Describe what domains and the results. (null if not tested)</td>
                    </tr>
                    <tr>
                        <td><strong>domain_specific_vs_general_pretraining</strong></td>
                        <td>str</td>
                        <td>Are domain-specific and general pre-trained models compared? Report performance differences. (null if not compared)</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_study_results</strong></td>
                        <td>str</td>
                        <td>Are ablation studies performed to identify which components of learned operators matter? Summarize key findings. (null if not performed)</td>
                    </tr>
                    <tr>
                        <td><strong>hypothesis_space_characterization</strong></td>
                        <td>str</td>
                        <td>Is the hypothesis space or coverage of different operators characterized or measured? Describe the approach and findings. (null if not discussed)</td>
                    </tr>
                    <tr>
                        <td><strong>adaptation_during_evolution</strong></td>
                        <td>str</td>
                        <td>Can the learned operator adapt during evolution (online learning, fine-tuning, prompt adaptation)? Describe the mechanism and whether it improves performance. (null if not applicable)</td>
                    </tr>
                    <tr>
                        <td><strong>failure_modes</strong></td>
                        <td>str</td>
                        <td>What failure modes or limitations of learned operators are identified? Be specific about when and why they fail. (null if not discussed)</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings_for_theory</strong></td>
                        <td>str</td>
                        <td>Summarize the key findings most relevant to understanding when and why learned operators outperform or underperform traditional operators. Be specific and information-dense.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-46",
    "schema": [
        {
            "name": "system_name",
            "type": "str",
            "description": "The name of the system, method, or approach being described."
        },
        {
            "name": "operator_type",
            "type": "str",
            "description": "What type of operator is used? (e.g. 'LLM-based', 'neural network', 'traditional GP', 'grammar-guided', 'hybrid', 'hand-designed', 'learned from data', etc.)"
        },
        {
            "name": "operator_description",
            "type": "str",
            "description": "Detailed description of how the operator works, including what it was trained on (if learned), what model architecture (if neural), or what rules (if hand-designed)."
        },
        {
            "name": "training_data_description",
            "type": "str",
            "description": "If the operator is learned, describe the training data: size, domain, source, diversity. (null if not applicable or not reported)"
        },
        {
            "name": "domain_or_benchmark",
            "type": "str",
            "description": "What problem domain or benchmark is the system evaluated on? Be specific (e.g. 'program synthesis on HumanEval', 'symbolic regression', 'circuit design', etc.)"
        },
        {
            "name": "comparison_baseline",
            "type": "str",
            "description": "What baseline methods are compared? List all baselines mentioned (e.g. 'standard GP with subtree crossover', 'random search', 'grammar-guided GP', etc.)"
        },
        {
            "name": "performance_learned_operator",
            "type": "str",
            "description": "Performance of the learned/neural operator approach. Include metric name, numerical value, and units. (null if not applicable)"
        },
        {
            "name": "performance_traditional_operator",
            "type": "str",
            "description": "Performance of traditional/baseline operator approach on the same task. Include metric name, numerical value, and units. (null if not reported)"
        },
        {
            "name": "performance_hybrid_operator",
            "type": "str",
            "description": "If a hybrid approach combining learned and traditional operators is tested, report its performance. Include metric name, numerical value, and units. (null if not applicable)"
        },
        {
            "name": "validity_or_executability_rate",
            "type": "str",
            "description": "What percentage of generated solutions are valid/executable/compilable? Report separately for learned vs traditional if both are given. (null if not reported)"
        },
        {
            "name": "novelty_or_diversity_metric",
            "type": "str",
            "description": "How is novelty or diversity measured? Include metric name and values for different operator types if compared. (null if not measured)"
        },
        {
            "name": "out_of_distribution_performance",
            "type": "str",
            "description": "Is performance on out-of-distribution or novel problems reported? If so, describe the results and how learned operators compare to traditional ones. (null if not tested)"
        },
        {
            "name": "training_bias_evidence",
            "type": "str",
            "description": "Is there evidence that learned operators are biased toward training distribution patterns? Describe any findings about limitations or failures on novel problems. (null if not discussed)"
        },
        {
            "name": "computational_cost_comparison",
            "type": "str",
            "description": "Are computational costs (time, memory, inference cost, etc.) compared between learned and traditional operators? Report quantitative comparisons. (null if not reported)"
        },
        {
            "name": "transfer_learning_results",
            "type": "str",
            "description": "Is transfer of learned operators across domains tested? Describe what domains and the results. (null if not tested)"
        },
        {
            "name": "domain_specific_vs_general_pretraining",
            "type": "str",
            "description": "Are domain-specific and general pre-trained models compared? Report performance differences. (null if not compared)"
        },
        {
            "name": "ablation_study_results",
            "type": "str",
            "description": "Are ablation studies performed to identify which components of learned operators matter? Summarize key findings. (null if not performed)"
        },
        {
            "name": "hypothesis_space_characterization",
            "type": "str",
            "description": "Is the hypothesis space or coverage of different operators characterized or measured? Describe the approach and findings. (null if not discussed)"
        },
        {
            "name": "adaptation_during_evolution",
            "type": "str",
            "description": "Can the learned operator adapt during evolution (online learning, fine-tuning, prompt adaptation)? Describe the mechanism and whether it improves performance. (null if not applicable)"
        },
        {
            "name": "failure_modes",
            "type": "str",
            "description": "What failure modes or limitations of learned operators are identified? Be specific about when and why they fail. (null if not discussed)"
        },
        {
            "name": "key_findings_for_theory",
            "type": "str",
            "description": "Summarize the key findings most relevant to understanding when and why learned operators outperform or underperform traditional operators. Be specific and information-dense."
        }
    ],
    "extraction_query": "Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-121 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-121</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-121</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>The name or description of the method, framework, or approach used to evaluate LLM-generated scientific theories (e.g. human expert review, automated metrics, comparison to ground truth, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_criteria</strong></td>
                        <td>str</td>
                        <td>The specific criteria or metrics used for evaluation (e.g. accuracy, novelty, plausibility, consistency, explanatory power, falsifiability, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>llm_model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM or AI system generating the scientific theories (e.g. GPT-4, PaLM, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>theory_domain</strong></td>
                        <td>str</td>
                        <td>The scientific domain or field in which the LLM-generated theory is situated (e.g. physics, biology, chemistry, social sciences, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>theory_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM-generated scientific theory or hypothesis being evaluated.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_results</strong></td>
                        <td>str</td>
                        <td>A concise summary of the results or findings from the evaluation (e.g. scores, qualitative outcomes, human ratings, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>benchmarks_or_datasets</strong></td>
                        <td>str</td>
                        <td>Names or descriptions of any benchmarks, datasets, or test sets used in the evaluation process.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_human</strong></td>
                        <td>str</td>
                        <td>Any comparison between LLM-generated theories and human-generated theories, or between LLM and human evaluation methods.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_challenges</strong></td>
                        <td>str</td>
                        <td>Any limitations, challenges, or open questions noted in the evaluation of LLM-generated scientific theories.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-121",
    "schema": [
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "The name or description of the method, framework, or approach used to evaluate LLM-generated scientific theories (e.g. human expert review, automated metrics, comparison to ground truth, etc.)."
        },
        {
            "name": "evaluation_criteria",
            "type": "str",
            "description": "The specific criteria or metrics used for evaluation (e.g. accuracy, novelty, plausibility, consistency, explanatory power, falsifiability, etc.)."
        },
        {
            "name": "llm_model_name",
            "type": "str",
            "description": "The name of the LLM or AI system generating the scientific theories (e.g. GPT-4, PaLM, etc.)."
        },
        {
            "name": "theory_domain",
            "type": "str",
            "description": "The scientific domain or field in which the LLM-generated theory is situated (e.g. physics, biology, chemistry, social sciences, etc.)."
        },
        {
            "name": "theory_description",
            "type": "str",
            "description": "A brief description of the LLM-generated scientific theory or hypothesis being evaluated."
        },
        {
            "name": "evaluation_results",
            "type": "str",
            "description": "A concise summary of the results or findings from the evaluation (e.g. scores, qualitative outcomes, human ratings, etc.)."
        },
        {
            "name": "benchmarks_or_datasets",
            "type": "str",
            "description": "Names or descriptions of any benchmarks, datasets, or test sets used in the evaluation process."
        },
        {
            "name": "comparison_to_human",
            "type": "str",
            "description": "Any comparison between LLM-generated theories and human-generated theories, or between LLM and human evaluation methods."
        },
        {
            "name": "limitations_or_challenges",
            "type": "str",
            "description": "Any limitations, challenges, or open questions noted in the evaluation of LLM-generated scientific theories."
        }
    ],
    "extraction_query": "Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
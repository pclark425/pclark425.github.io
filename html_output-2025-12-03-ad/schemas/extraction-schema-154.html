<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-154 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-154</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-154</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being evaluated (e.g., GPT-3, Llama 2, PaLM, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the language model, including architecture, training data, or other relevant details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model, in parameters (e.g., 1B, 7B, 70B, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_task_name</strong></td>
                        <td>str</td>
                        <td>The name of the strict logical reasoning task or benchmark (e.g., ProofWriter, LogiQA, ReClor, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the logical reasoning task or benchmark, including what type of logic or reasoning is required.</td>
                    </tr>
                    <tr>
                        <td><strong>method_or_approach</strong></td>
                        <td>str</td>
                        <td>The method or approach used to improve or evaluate logical reasoning (e.g., chain-of-thought prompting, fine-tuning on logic data, use of external tools, neuro-symbolic methods, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>The performance of the model on the logical reasoning task (quantitative if possible, e.g., accuracy, F1, etc., with units; otherwise, a qualitative summary).</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_comparison</strong></td>
                        <td>str</td>
                        <td>A comparison to baseline models or ablations (e.g., performance without the method, or with a smaller model, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failures</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure cases, or types of logical reasoning where the model struggles.</td>
                    </tr>
                    <tr>
                        <td><strong>insights_or_conclusions</strong></td>
                        <td>str</td>
                        <td>Any insights, conclusions, or recommendations from the paper about what enables or limits strict logical reasoning in language models.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-154",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being evaluated (e.g., GPT-3, Llama 2, PaLM, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the language model, including architecture, training data, or other relevant details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model, in parameters (e.g., 1B, 7B, 70B, etc.)."
        },
        {
            "name": "reasoning_task_name",
            "type": "str",
            "description": "The name of the strict logical reasoning task or benchmark (e.g., ProofWriter, LogiQA, ReClor, etc.)."
        },
        {
            "name": "reasoning_task_description",
            "type": "str",
            "description": "A brief description of the logical reasoning task or benchmark, including what type of logic or reasoning is required."
        },
        {
            "name": "method_or_approach",
            "type": "str",
            "description": "The method or approach used to improve or evaluate logical reasoning (e.g., chain-of-thought prompting, fine-tuning on logic data, use of external tools, neuro-symbolic methods, etc.)."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "The performance of the model on the logical reasoning task (quantitative if possible, e.g., accuracy, F1, etc., with units; otherwise, a qualitative summary)."
        },
        {
            "name": "baseline_comparison",
            "type": "str",
            "description": "A comparison to baseline models or ablations (e.g., performance without the method, or with a smaller model, etc.)."
        },
        {
            "name": "limitations_or_failures",
            "type": "str",
            "description": "Any reported limitations, failure cases, or types of logical reasoning where the model struggles."
        },
        {
            "name": "insights_or_conclusions",
            "type": "str",
            "description": "Any insights, conclusions, or recommendations from the paper about what enables or limits strict logical reasoning in language models."
        }
    ],
    "extraction_query": "Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
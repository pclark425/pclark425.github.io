<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-49 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-49</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-49</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>curriculum_generation_method</strong></td>
                        <td>str</td>
                        <td>What method was used to generate or design the curriculum? (e.g., 'LLM-generated', 'hand-crafted by experts', 'random ordering', 'rule-based automated', 'reinforcement learning-based', 'difficulty-based ordering', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>curriculum_method_description</strong></td>
                        <td>str</td>
                        <td>A detailed description of how the curriculum was generated or designed, including any specific algorithms, prompting strategies, or design principles used.</td>
                    </tr>
                    <tr>
                        <td><strong>llm_model_used</strong></td>
                        <td>str</td>
                        <td>If an LLM was used for curriculum generation, what model was it? Include model name and size if available. (null if no LLM was used)</td>
                    </tr>
                    <tr>
                        <td><strong>domain_environment</strong></td>
                        <td>str</td>
                        <td>What domain or environment was the curriculum designed for? (e.g., 'text-based games', 'household tasks', 'science procedures', 'robotic manipulation', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>is_interactive_text_environment</strong></td>
                        <td>bool</td>
                        <td>Is the learning environment an interactive text-based environment? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>is_compositional</strong></td>
                        <td>bool</td>
                        <td>Does the task require compositional learning or combining multiple sub-skills? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>task_complexity_description</strong></td>
                        <td>str</td>
                        <td>Describe the complexity of the tasks in the curriculum, including number of steps, sub-skills required, or other complexity metrics.</td>
                    </tr>
                    <tr>
                        <td><strong>is_curriculum_adaptive</strong></td>
                        <td>bool</td>
                        <td>Does the curriculum adapt dynamically based on learner performance? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_comparisons</strong></td>
                        <td>str</td>
                        <td>What baseline curriculum methods were compared against? List all baselines mentioned (e.g., 'random ordering', 'hand-crafted', 'difficulty-based', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metrics</strong></td>
                        <td>str</td>
                        <td>What performance metrics were reported? Include metric names and values for each curriculum method compared, with units. Be specific about which method achieved which performance.</td>
                    </tr>
                    <tr>
                        <td><strong>learning_speed_comparison</strong></td>
                        <td>str</td>
                        <td>If reported, how did learning speed (convergence time, number of episodes, etc.) compare between different curriculum methods? Include specific numbers and percentages if available.</td>
                    </tr>
                    <tr>
                        <td><strong>generalization_performance</strong></td>
                        <td>str</td>
                        <td>If reported, how well did agents trained with different curricula generalize to held-out or novel tasks? Include specific metrics.</td>
                    </tr>
                    <tr>
                        <td><strong>task_diversity_analysis</strong></td>
                        <td>str</td>
                        <td>Was task diversity analyzed or measured? If so, describe how different curriculum methods compared in terms of task diversity.</td>
                    </tr>
                    <tr>
                        <td><strong>prerequisite_identification</strong></td>
                        <td>str</td>
                        <td>Did the paper discuss or evaluate how well different methods identified task prerequisites or dependencies? Describe findings.</td>
                    </tr>
                    <tr>
                        <td><strong>intermediate_task_generation</strong></td>
                        <td>str</td>
                        <td>Did the curriculum method generate intermediate or bridging tasks? If so, describe how and whether they were effective.</td>
                    </tr>
                    <tr>
                        <td><strong>llm_limitations_observed</strong></td>
                        <td>str</td>
                        <td>If LLMs were used, what limitations or failures were observed? (e.g., hallucinations, invalid tasks, poor sequencing, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>computational_cost</strong></td>
                        <td>str</td>
                        <td>Was the computational cost or time required for curriculum generation reported? Compare across methods if available.</td>
                    </tr>
                    <tr>
                        <td><strong>human_expert_evaluation</strong></td>
                        <td>str</td>
                        <td>Were curricula evaluated by human experts? If so, what were the results and how did different methods compare?</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings_summary</strong></td>
                        <td>str</td>
                        <td>Summarize the key findings of the paper in 2-3 sentences, focusing on what worked well and what didn't for curriculum generation.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-49",
    "schema": [
        {
            "name": "curriculum_generation_method",
            "type": "str",
            "description": "What method was used to generate or design the curriculum? (e.g., 'LLM-generated', 'hand-crafted by experts', 'random ordering', 'rule-based automated', 'reinforcement learning-based', 'difficulty-based ordering', etc.)"
        },
        {
            "name": "curriculum_method_description",
            "type": "str",
            "description": "A detailed description of how the curriculum was generated or designed, including any specific algorithms, prompting strategies, or design principles used."
        },
        {
            "name": "llm_model_used",
            "type": "str",
            "description": "If an LLM was used for curriculum generation, what model was it? Include model name and size if available. (null if no LLM was used)"
        },
        {
            "name": "domain_environment",
            "type": "str",
            "description": "What domain or environment was the curriculum designed for? (e.g., 'text-based games', 'household tasks', 'science procedures', 'robotic manipulation', etc.)"
        },
        {
            "name": "is_interactive_text_environment",
            "type": "bool",
            "description": "Is the learning environment an interactive text-based environment? (true, false, or null for no information)"
        },
        {
            "name": "is_compositional",
            "type": "bool",
            "description": "Does the task require compositional learning or combining multiple sub-skills? (true, false, or null for no information)"
        },
        {
            "name": "task_complexity_description",
            "type": "str",
            "description": "Describe the complexity of the tasks in the curriculum, including number of steps, sub-skills required, or other complexity metrics."
        },
        {
            "name": "is_curriculum_adaptive",
            "type": "bool",
            "description": "Does the curriculum adapt dynamically based on learner performance? (true, false, or null for no information)"
        },
        {
            "name": "baseline_comparisons",
            "type": "str",
            "description": "What baseline curriculum methods were compared against? List all baselines mentioned (e.g., 'random ordering', 'hand-crafted', 'difficulty-based', etc.)"
        },
        {
            "name": "performance_metrics",
            "type": "str",
            "description": "What performance metrics were reported? Include metric names and values for each curriculum method compared, with units. Be specific about which method achieved which performance."
        },
        {
            "name": "learning_speed_comparison",
            "type": "str",
            "description": "If reported, how did learning speed (convergence time, number of episodes, etc.) compare between different curriculum methods? Include specific numbers and percentages if available."
        },
        {
            "name": "generalization_performance",
            "type": "str",
            "description": "If reported, how well did agents trained with different curricula generalize to held-out or novel tasks? Include specific metrics."
        },
        {
            "name": "task_diversity_analysis",
            "type": "str",
            "description": "Was task diversity analyzed or measured? If so, describe how different curriculum methods compared in terms of task diversity."
        },
        {
            "name": "prerequisite_identification",
            "type": "str",
            "description": "Did the paper discuss or evaluate how well different methods identified task prerequisites or dependencies? Describe findings."
        },
        {
            "name": "intermediate_task_generation",
            "type": "str",
            "description": "Did the curriculum method generate intermediate or bridging tasks? If so, describe how and whether they were effective."
        },
        {
            "name": "llm_limitations_observed",
            "type": "str",
            "description": "If LLMs were used, what limitations or failures were observed? (e.g., hallucinations, invalid tasks, poor sequencing, etc.)"
        },
        {
            "name": "computational_cost",
            "type": "str",
            "description": "Was the computational cost or time required for curriculum generation reported? Compare across methods if available."
        },
        {
            "name": "human_expert_evaluation",
            "type": "str",
            "description": "Were curricula evaluated by human experts? If so, what were the results and how did different methods compare?"
        },
        {
            "name": "key_findings_summary",
            "type": "str",
            "description": "Summarize the key findings of the paper in 2-3 sentences, focusing on what worked well and what didn't for curriculum generation."
        }
    ],
    "extraction_query": "Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
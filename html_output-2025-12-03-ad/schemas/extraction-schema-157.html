<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-157 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-157</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-157</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being evaluated (e.g., GPT-3, GPT-4, PaLM, Llama-2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the language model, including size (e.g., 7B, 13B, 70B parameters) and any relevant training details.</td>
                    </tr>
                    <tr>
                        <td><strong>reflection_method_name</strong></td>
                        <td>str</td>
                        <td>The name or label of the self-reflection or iterative answer improvement method (e.g., generate-then-reflect, self-critique, reflexion, self-consistency, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>reflection_method_description</strong></td>
                        <td>str</td>
                        <td>A concise description of how the self-reflection or iterative answer improvement method works, including the number of iterations if specified.</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the task or benchmark used to evaluate the method (e.g., GSM8K, MMLU, BigBench, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the task or benchmark.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_with_reflection</strong></td>
                        <td>str</td>
                        <td>The performance of the model on the task when using self-reflection or iterative answer improvement (include numerical results and units if available).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_without_reflection</strong></td>
                        <td>str</td>
                        <td>The performance of the model on the task without using self-reflection or iterative answer improvement (include numerical results and units if available).</td>
                    </tr>
                    <tr>
                        <td><strong>has_performance_comparison</strong></td>
                        <td>bool</td>
                        <td>Does the paper report a direct comparison of performance with and without self-reflection or iterative answer improvement? (true, false, or null if no information)</td>
                    </tr>
                    <tr>
                        <td><strong>mechanism_of_reflection</strong></td>
                        <td>str</td>
                        <td>A description of the mechanism by which self-reflection is implemented (e.g., prompt engineering, special modules, external memory, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>number_of_iterations</strong></td>
                        <td>int</td>
                        <td>The number of generate-reflect cycles or iterations used in the method (null if not specified).</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_for_improvement</strong></td>
                        <td>str</td>
                        <td>A concise summary of the evidence (quantitative or qualitative) that self-reflection or iterative answer improvement leads to better answers.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_cases</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure cases, or situations where self-reflection did not improve or worsened performance.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_other_methods</strong></td>
                        <td>str</td>
                        <td>Any comparisons to other answer improvement methods (e.g., chain-of-thought, vanilla generation, etc.), including performance or qualitative differences.</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_study_results</strong></td>
                        <td>str</td>
                        <td>Results from any ablation studies that isolate the effect of self-reflection or iterative answer improvement (null if not reported).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-157",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being evaluated (e.g., GPT-3, GPT-4, PaLM, Llama-2, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the language model, including size (e.g., 7B, 13B, 70B parameters) and any relevant training details."
        },
        {
            "name": "reflection_method_name",
            "type": "str",
            "description": "The name or label of the self-reflection or iterative answer improvement method (e.g., generate-then-reflect, self-critique, reflexion, self-consistency, etc.)."
        },
        {
            "name": "reflection_method_description",
            "type": "str",
            "description": "A concise description of how the self-reflection or iterative answer improvement method works, including the number of iterations if specified."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the task or benchmark used to evaluate the method (e.g., GSM8K, MMLU, BigBench, etc.)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the task or benchmark."
        },
        {
            "name": "performance_with_reflection",
            "type": "str",
            "description": "The performance of the model on the task when using self-reflection or iterative answer improvement (include numerical results and units if available)."
        },
        {
            "name": "performance_without_reflection",
            "type": "str",
            "description": "The performance of the model on the task without using self-reflection or iterative answer improvement (include numerical results and units if available)."
        },
        {
            "name": "has_performance_comparison",
            "type": "bool",
            "description": "Does the paper report a direct comparison of performance with and without self-reflection or iterative answer improvement? (true, false, or null if no information)"
        },
        {
            "name": "mechanism_of_reflection",
            "type": "str",
            "description": "A description of the mechanism by which self-reflection is implemented (e.g., prompt engineering, special modules, external memory, etc.)."
        },
        {
            "name": "number_of_iterations",
            "type": "int",
            "description": "The number of generate-reflect cycles or iterations used in the method (null if not specified)."
        },
        {
            "name": "evidence_for_improvement",
            "type": "str",
            "description": "A concise summary of the evidence (quantitative or qualitative) that self-reflection or iterative answer improvement leads to better answers."
        },
        {
            "name": "limitations_or_failure_cases",
            "type": "str",
            "description": "Any reported limitations, failure cases, or situations where self-reflection did not improve or worsened performance."
        },
        {
            "name": "comparison_to_other_methods",
            "type": "str",
            "description": "Any comparisons to other answer improvement methods (e.g., chain-of-thought, vanilla generation, etc.), including performance or qualitative differences."
        },
        {
            "name": "ablation_study_results",
            "type": "str",
            "description": "Results from any ablation studies that isolate the effect of self-reflection or iterative answer improvement (null if not reported)."
        }
    ],
    "extraction_query": "Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
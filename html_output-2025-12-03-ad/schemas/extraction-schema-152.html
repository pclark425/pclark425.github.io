<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-152 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-152</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-152</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being evaluated (e.g., GPT-4, PaLM, Llama-2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the language model, including architecture, size, or other relevant details.</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_methods</strong></td>
                        <td>list[str]</td>
                        <td>A list of the reasoning methods used by the model (e.g., chain-of-thought, self-consistency, analogical reasoning, deductive reasoning, tool use, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_methods_description</strong></td>
                        <td>str</td>
                        <td>A brief description of each reasoning method used, including how it is implemented or prompted in the model.</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_diversity</strong></td>
                        <td>str</td>
                        <td>Does the model use diverse (multiple, varied) reasoning methods, similar (single, repeated) reasoning methods, or both? (e.g., 'diverse', 'similar', 'both', or a description if more nuanced)</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_diversity_experimental_setup</strong></td>
                        <td>str</td>
                        <td>Description of any experimental setup or ablation comparing diverse versus similar reasoning methods (e.g., 'compared chain-of-thought vs. analogical reasoning', 'ablation study with and without self-consistency', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_or_benchmark</strong></td>
                        <td>str</td>
                        <td>The name and brief description of the reasoning task or benchmark used (e.g., GSM8K, ARC, BigBench, custom logical reasoning tasks, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_results</strong></td>
                        <td>str</td>
                        <td>Performance of the model on the reasoning task(s), ideally broken down by reasoning method or diversity condition (include numerical results and units if available).</td>
                    </tr>
                    <tr>
                        <td><strong>qualitative_findings</strong></td>
                        <td>str</td>
                        <td>Any qualitative findings or observations about how diverse or similar reasoning methods affect the model's problem-solving (e.g., error types, reasoning chains, interpretability, robustness, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>explicit_comparison</strong></td>
                        <td>bool</td>
                        <td>Does the paper explicitly compare diverse versus similar reasoning methods in language models? (true, false, or null if no information)</td>
                    </tr>
                    <tr>
                        <td><strong>key_claims_or_conclusions</strong></td>
                        <td>str</td>
                        <td>Key claims or conclusions from the paper regarding the impact of reasoning method diversity or similarity on language model performance.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-152",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being evaluated (e.g., GPT-4, PaLM, Llama-2, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the language model, including architecture, size, or other relevant details."
        },
        {
            "name": "reasoning_methods",
            "type": "list[str]",
            "description": "A list of the reasoning methods used by the model (e.g., chain-of-thought, self-consistency, analogical reasoning, deductive reasoning, tool use, etc.)."
        },
        {
            "name": "reasoning_methods_description",
            "type": "str",
            "description": "A brief description of each reasoning method used, including how it is implemented or prompted in the model."
        },
        {
            "name": "reasoning_diversity",
            "type": "str",
            "description": "Does the model use diverse (multiple, varied) reasoning methods, similar (single, repeated) reasoning methods, or both? (e.g., 'diverse', 'similar', 'both', or a description if more nuanced)"
        },
        {
            "name": "reasoning_diversity_experimental_setup",
            "type": "str",
            "description": "Description of any experimental setup or ablation comparing diverse versus similar reasoning methods (e.g., 'compared chain-of-thought vs. analogical reasoning', 'ablation study with and without self-consistency', etc.)."
        },
        {
            "name": "task_or_benchmark",
            "type": "str",
            "description": "The name and brief description of the reasoning task or benchmark used (e.g., GSM8K, ARC, BigBench, custom logical reasoning tasks, etc.)."
        },
        {
            "name": "performance_results",
            "type": "str",
            "description": "Performance of the model on the reasoning task(s), ideally broken down by reasoning method or diversity condition (include numerical results and units if available)."
        },
        {
            "name": "qualitative_findings",
            "type": "str",
            "description": "Any qualitative findings or observations about how diverse or similar reasoning methods affect the model's problem-solving (e.g., error types, reasoning chains, interpretability, robustness, etc.)."
        },
        {
            "name": "explicit_comparison",
            "type": "bool",
            "description": "Does the paper explicitly compare diverse versus similar reasoning methods in language models? (true, false, or null if no information)"
        },
        {
            "name": "key_claims_or_conclusions",
            "type": "str",
            "description": "Key claims or conclusions from the paper regarding the impact of reasoning method diversity or similarity on language model performance."
        }
    ],
    "extraction_query": "Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
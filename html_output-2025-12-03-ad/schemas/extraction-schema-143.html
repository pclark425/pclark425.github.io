<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-143 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-143</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-143</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>paper_title</strong></td>
                        <td>str</td>
                        <td>Title of the paper.</td>
                    </tr>
                    <tr>
                        <td><strong>llm_name</strong></td>
                        <td>str</td>
                        <td>Name of the large language model used (e.g., GPT‑4, Claude, LLaMA, a fine‑tuned model).</td>
                    </tr>
                    <tr>
                        <td><strong>llm_type</strong></td>
                        <td>str</td>
                        <td>Brief description of how the LLM is employed (e.g., zero‑shot prompting, few‑shot prompting, chain‑of‑thought, fine‑tuned for extraction).</td>
                    </tr>
                    <tr>
                        <td><strong>input_corpus_description</strong></td>
                        <td>str</td>
                        <td>Description of the scholarly input corpus: domain, number of papers, total tokens, any preprocessing.</td>
                    </tr>
                    <tr>
                        <td><strong>extraction_method</strong></td>
                        <td>str</td>
                        <td>Method used to distill laws (e.g., prompting for equations, LLM‑guided symbolic regression, template‑based extraction, knowledge‑graph generation).</td>
                    </tr>
                    <tr>
                        <td><strong>law_type</strong></td>
                        <td>str</td>
                        <td>Category of quantitative law extracted (e.g., physical law, biological relationship, statistical correlation, engineering formula).</td>
                    </tr>
                    <tr>
                        <td><strong>law_representation</strong></td>
                        <td>str</td>
                        <td>Formal representation of the distilled law (e.g., algebraic equation, differential equation, inequality, functional form, rule).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_dataset</strong></td>
                        <td>str</td>
                        <td>Dataset or benchmark used to evaluate the extracted law (e.g., a held‑out set of experimental measurements, simulation data).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metrics</strong></td>
                        <td>str</td>
                        <td>Metrics reported for assessing the law (e.g., RMSE, MAE, R², precision/recall, predictive accuracy).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_results</strong></td>
                        <td>str</td>
                        <td>Numeric results for the reported metrics (include units where applicable).</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_comparison</strong></td>
                        <td>bool</td>
                        <td>Whether the paper compares the LLM‑based approach to baseline methods (true/false/null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_performance</strong></td>
                        <td>str</td>
                        <td>Performance of baseline methods on the same evaluation metrics (null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>validation_method</strong></td>
                        <td>str</td>
                        <td>How the extracted law was validated (e.g., experimental verification, simulation, cross‑validation, expert review).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations</strong></td>
                        <td>str</td>
                        <td>Any challenges, failure modes, or limitations noted by the authors regarding the LLM distillation process.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-143",
    "schema": [
        {
            "name": "paper_title",
            "type": "str",
            "description": "Title of the paper."
        },
        {
            "name": "llm_name",
            "type": "str",
            "description": "Name of the large language model used (e.g., GPT‑4, Claude, LLaMA, a fine‑tuned model)."
        },
        {
            "name": "llm_type",
            "type": "str",
            "description": "Brief description of how the LLM is employed (e.g., zero‑shot prompting, few‑shot prompting, chain‑of‑thought, fine‑tuned for extraction)."
        },
        {
            "name": "input_corpus_description",
            "type": "str",
            "description": "Description of the scholarly input corpus: domain, number of papers, total tokens, any preprocessing."
        },
        {
            "name": "extraction_method",
            "type": "str",
            "description": "Method used to distill laws (e.g., prompting for equations, LLM‑guided symbolic regression, template‑based extraction, knowledge‑graph generation)."
        },
        {
            "name": "law_type",
            "type": "str",
            "description": "Category of quantitative law extracted (e.g., physical law, biological relationship, statistical correlation, engineering formula)."
        },
        {
            "name": "law_representation",
            "type": "str",
            "description": "Formal representation of the distilled law (e.g., algebraic equation, differential equation, inequality, functional form, rule)."
        },
        {
            "name": "evaluation_dataset",
            "type": "str",
            "description": "Dataset or benchmark used to evaluate the extracted law (e.g., a held‑out set of experimental measurements, simulation data)."
        },
        {
            "name": "evaluation_metrics",
            "type": "str",
            "description": "Metrics reported for assessing the law (e.g., RMSE, MAE, R², precision/recall, predictive accuracy)."
        },
        {
            "name": "performance_results",
            "type": "str",
            "description": "Numeric results for the reported metrics (include units where applicable)."
        },
        {
            "name": "baseline_comparison",
            "type": "bool",
            "description": "Whether the paper compares the LLM‑based approach to baseline methods (true/false/null if not reported)."
        },
        {
            "name": "baseline_performance",
            "type": "str",
            "description": "Performance of baseline methods on the same evaluation metrics (null if not reported)."
        },
        {
            "name": "validation_method",
            "type": "str",
            "description": "How the extracted law was validated (e.g., experimental verification, simulation, cross‑validation, expert review)."
        },
        {
            "name": "limitations",
            "type": "str",
            "description": "Any challenges, failure modes, or limitations noted by the authors regarding the LLM distillation process."
        }
    ],
    "extraction_query": "Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
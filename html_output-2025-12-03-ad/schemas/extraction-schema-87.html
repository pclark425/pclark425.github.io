<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-87 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-87</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-87</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how problem presentation format (such as prompt wording, input structure, formatting, or context) affects the performance of large language models (LLMs), including details of the formats compared, tasks, models, and results.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM model being evaluated (e.g., GPT-3, GPT-4, Llama-2, PaLM, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM model, including size (e.g., 7B, 13B), training data, or other relevant properties.</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the task, dataset, or benchmark used to evaluate the model (e.g., MMLU, GSM8K, summarization, question answering, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the task or benchmark, including what the model is required to do.</td>
                    </tr>
                    <tr>
                        <td><strong>presentation_formats_compared</strong></td>
                        <td>list</td>
                        <td>A list of the different problem presentation formats (e.g., prompt templates, input structures, formatting styles) that were compared in the study. Each entry should briefly describe the format (e.g., 'question as a statement', 'bulleted list', 'with/without context', 'chain-of-thought', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_by_format</strong></td>
                        <td>list</td>
                        <td>A list of performance results for each presentation format, with each entry specifying the format, the performance metric(s) (e.g., accuracy, F1, BLEU), and the value(s) (include units).</td>
                    </tr>
                    <tr>
                        <td><strong>format_effect_summary</strong></td>
                        <td>str</td>
                        <td>A concise summary of the observed effect(s) of problem presentation format on LLM performance, as reported in the paper (e.g., 'chain-of-thought prompts improved accuracy by 20%', 'formatting had no significant effect', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>has_negative_or_null_results</strong></td>
                        <td>bool</td>
                        <td>Does the paper report any cases where changing the problem presentation format did NOT affect LLM performance, or had a negative effect? (true, false, or null if no information)</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_or_control_conditions</strong></td>
                        <td>str</td>
                        <td>Describe any ablation studies or control conditions related to problem presentation format (e.g., comparisons with/without certain prompt elements, randomization, etc.).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-87",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM model being evaluated (e.g., GPT-3, GPT-4, Llama-2, PaLM, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM model, including size (e.g., 7B, 13B), training data, or other relevant properties."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the task, dataset, or benchmark used to evaluate the model (e.g., MMLU, GSM8K, summarization, question answering, etc.)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the task or benchmark, including what the model is required to do."
        },
        {
            "name": "presentation_formats_compared",
            "type": "list",
            "description": "A list of the different problem presentation formats (e.g., prompt templates, input structures, formatting styles) that were compared in the study. Each entry should briefly describe the format (e.g., 'question as a statement', 'bulleted list', 'with/without context', 'chain-of-thought', etc.)."
        },
        {
            "name": "performance_by_format",
            "type": "list",
            "description": "A list of performance results for each presentation format, with each entry specifying the format, the performance metric(s) (e.g., accuracy, F1, BLEU), and the value(s) (include units)."
        },
        {
            "name": "format_effect_summary",
            "type": "str",
            "description": "A concise summary of the observed effect(s) of problem presentation format on LLM performance, as reported in the paper (e.g., 'chain-of-thought prompts improved accuracy by 20%', 'formatting had no significant effect', etc.)."
        },
        {
            "name": "has_negative_or_null_results",
            "type": "bool",
            "description": "Does the paper report any cases where changing the problem presentation format did NOT affect LLM performance, or had a negative effect? (true, false, or null if no information)"
        },
        {
            "name": "ablation_or_control_conditions",
            "type": "str",
            "description": "Describe any ablation studies or control conditions related to problem presentation format (e.g., comparisons with/without certain prompt elements, randomization, etc.)."
        }
    ],
    "extraction_query": "Extract any mentions of how problem presentation format (such as prompt wording, input structure, formatting, or context) affects the performance of large language models (LLMs), including details of the formats compared, tasks, models, and results.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-6 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-6</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-6</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of large language models' performance on first-order theory-of-mind tasks, focusing on model size, training data diversity, fine-tuning, multilingual training, and any reported limitations or counter-evidence.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model (LLM) being evaluated, e.g., GPT-3, PaLM, LLaMA.</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM, including architecture details, training data characteristics, or unique features relevant to theory-of-mind capabilities.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 1B, 7B, 13B, 70B), or other scale indicators.</td>
                    </tr>
                    <tr>
                        <td><strong>training_data_diversity</strong></td>
                        <td>str</td>
                        <td>Description of the diversity and richness of the training data, including multilinguality, social narrative content, and dataset variety.</td>
                    </tr>
                    <tr>
                        <td><strong>training_data_quality_vs_quantity</strong></td>
                        <td>str</td>
                        <td>Any reported distinctions or findings about the impact of training data quality versus quantity on first-order ToM performance.</td>
                    </tr>
                    <tr>
                        <td><strong>fine_tuning_description</strong></td>
                        <td>str</td>
                        <td>Details on any fine-tuning or instruction tuning performed specifically to improve theory-of-mind capabilities.</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the theory-of-mind task or benchmark used to evaluate the model, e.g., false belief task, Sally-Anne test.</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the theory-of-mind task, including what mental states or social reasoning it tests.</td>
                    </tr>
                    <tr>
                        <td><strong>task_type</strong></td>
                        <td>str</td>
                        <td>The category or type of theory-of-mind task, e.g., first-order belief reasoning, second-order belief reasoning, perspective taking.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metrics</strong></td>
                        <td>str</td>
                        <td>Quantitative or qualitative performance results of the model on the theory-of-mind task, including accuracy, success rate, or other relevant metrics.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>The method used to evaluate theory-of-mind capabilities, e.g., zero-shot prompting, few-shot prompting, fine-tuning, probing.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_human_performance</strong></td>
                        <td>str</td>
                        <td>If available, a comparison of the model's theory-of-mind task performance to human baseline or human-level performance.</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_model_size</strong></td>
                        <td>str</td>
                        <td>Any reported relationship between model size and theory-of-mind task performance, including thresholds or saturation effects.</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_training_data</strong></td>
                        <td>str</td>
                        <td>Any reported influence of training data composition, diversity, multilinguality, or social narrative content on theory-of-mind capabilities.</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_fine_tuning</strong></td>
                        <td>str</td>
                        <td>Reported effects of fine-tuning or instruction tuning on first-order ToM task performance.</td>
                    </tr>
                    <tr>
                        <td><strong>counter_evidence</strong></td>
                        <td>str</td>
                        <td>Any evidence or arguments presented that challenge or refute the presence of genuine theory-of-mind in LLMs or the size/data diversity correlations.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_reported</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure modes, or challenges the model exhibits in theory-of-mind tasks.</td>
                    </tr>
                    <tr>
                        <td><strong>unexplained_variability</strong></td>
                        <td>str</td>
                        <td>Reports of variability in ToM performance among models of similar size or training data, suggesting other influencing factors.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-6",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model (LLM) being evaluated, e.g., GPT-3, PaLM, LLaMA."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM, including architecture details, training data characteristics, or unique features relevant to theory-of-mind capabilities."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 1B, 7B, 13B, 70B), or other scale indicators."
        },
        {
            "name": "training_data_diversity",
            "type": "str",
            "description": "Description of the diversity and richness of the training data, including multilinguality, social narrative content, and dataset variety."
        },
        {
            "name": "training_data_quality_vs_quantity",
            "type": "str",
            "description": "Any reported distinctions or findings about the impact of training data quality versus quantity on first-order ToM performance."
        },
        {
            "name": "fine_tuning_description",
            "type": "str",
            "description": "Details on any fine-tuning or instruction tuning performed specifically to improve theory-of-mind capabilities."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the theory-of-mind task or benchmark used to evaluate the model, e.g., false belief task, Sally-Anne test."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the theory-of-mind task, including what mental states or social reasoning it tests."
        },
        {
            "name": "task_type",
            "type": "str",
            "description": "The category or type of theory-of-mind task, e.g., first-order belief reasoning, second-order belief reasoning, perspective taking."
        },
        {
            "name": "performance_metrics",
            "type": "str",
            "description": "Quantitative or qualitative performance results of the model on the theory-of-mind task, including accuracy, success rate, or other relevant metrics."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "The method used to evaluate theory-of-mind capabilities, e.g., zero-shot prompting, few-shot prompting, fine-tuning, probing."
        },
        {
            "name": "comparison_to_human_performance",
            "type": "str",
            "description": "If available, a comparison of the model's theory-of-mind task performance to human baseline or human-level performance."
        },
        {
            "name": "impact_of_model_size",
            "type": "str",
            "description": "Any reported relationship between model size and theory-of-mind task performance, including thresholds or saturation effects."
        },
        {
            "name": "impact_of_training_data",
            "type": "str",
            "description": "Any reported influence of training data composition, diversity, multilinguality, or social narrative content on theory-of-mind capabilities."
        },
        {
            "name": "impact_of_fine_tuning",
            "type": "str",
            "description": "Reported effects of fine-tuning or instruction tuning on first-order ToM task performance."
        },
        {
            "name": "counter_evidence",
            "type": "str",
            "description": "Any evidence or arguments presented that challenge or refute the presence of genuine theory-of-mind in LLMs or the size/data diversity correlations."
        },
        {
            "name": "limitations_reported",
            "type": "str",
            "description": "Any reported limitations, failure modes, or challenges the model exhibits in theory-of-mind tasks."
        },
        {
            "name": "unexplained_variability",
            "type": "str",
            "description": "Reports of variability in ToM performance among models of similar size or training data, suggesting other influencing factors."
        }
    ],
    "extraction_query": "Extract any mentions of large language models' performance on first-order theory-of-mind tasks, focusing on model size, training data diversity, fine-tuning, multilingual training, and any reported limitations or counter-evidence.",
    "supporting_theory_ids": [],
    "model_str": null
}</code></pre>
        </div>
    </div>
</body>
</html>
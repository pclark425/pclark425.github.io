<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-136 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-136</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-136</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model evaluated (e.g., GPT-4, LLaMA-2).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model, including architecture family if relevant.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Model scale expressed in parameters (e.g., 7B, 70B, 175B).</td>
                    </tr>
                    <tr>
                        <td><strong>test_name</strong></td>
                        <td>str</td>
                        <td>The specific cognitive psychology test or task (e.g., Stroop task, n‑back, Raven's Progressive Matrices).</td>
                    </tr>
                    <tr>
                        <td><strong>test_category</strong></td>
                        <td>str</td>
                        <td>Broad cognitive domain of the test (e.g., attention, working memory, reasoning, theory of mind).</td>
                    </tr>
                    <tr>
                        <td><strong>test_description</strong></td>
                        <td>str</td>
                        <td>A concise description of what the test measures and how it is administered.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metric</strong></td>
                        <td>str</td>
                        <td>Metric used to report performance (e.g., accuracy, mean reaction time, error rate, score).</td>
                    </tr>
                    <tr>
                        <td><strong>human_performance</strong></td>
                        <td>str</td>
                        <td>Reported human baseline performance on the test, including metric and value (e.g., "accuracy 94%", "mean RT 620 ms").</td>
                    </tr>
                    <tr>
                        <td><strong>llm_performance</strong></td>
                        <td>str</td>
                        <td>Reported LLM performance on the same metric (e.g., "accuracy 78%", "mean RT 850 ms").</td>
                    </tr>
                    <tr>
                        <td><strong>prompting_method</strong></td>
                        <td>str</td>
                        <td>How the model was queried (e.g., zero‑shot, few‑shot, chain‑of‑thought, instruction‑tuned).</td>
                    </tr>
                    <tr>
                        <td><strong>fine_tuned</strong></td>
                        <td>bool</td>
                        <td>Whether the model was fine‑tuned on the specific task or related data (true/false/null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>human_data_source</strong></td>
                        <td>str</td>
                        <td>Citation or dataset from which the human baseline was obtained (e.g., "PsyToolkit dataset", "Miller et al., 2020").</td>
                    </tr>
                    <tr>
                        <td><strong>statistical_significance</strong></td>
                        <td>str</td>
                        <td>Any reported statistical comparison between LLM and human performance (e.g., p‑value, confidence interval, effect size).</td>
                    </tr>
                    <tr>
                        <td><strong>notes</strong></td>
                        <td>str</td>
                        <td>Additional observations, error patterns, or limitations mentioned in the paper relevant to the comparison.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-136",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model evaluated (e.g., GPT-4, LLaMA-2)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model, including architecture family if relevant."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Model scale expressed in parameters (e.g., 7B, 70B, 175B)."
        },
        {
            "name": "test_name",
            "type": "str",
            "description": "The specific cognitive psychology test or task (e.g., Stroop task, n‑back, Raven's Progressive Matrices)."
        },
        {
            "name": "test_category",
            "type": "str",
            "description": "Broad cognitive domain of the test (e.g., attention, working memory, reasoning, theory of mind)."
        },
        {
            "name": "test_description",
            "type": "str",
            "description": "A concise description of what the test measures and how it is administered."
        },
        {
            "name": "evaluation_metric",
            "type": "str",
            "description": "Metric used to report performance (e.g., accuracy, mean reaction time, error rate, score)."
        },
        {
            "name": "human_performance",
            "type": "str",
            "description": "Reported human baseline performance on the test, including metric and value (e.g., \"accuracy 94%\", \"mean RT 620 ms\")."
        },
        {
            "name": "llm_performance",
            "type": "str",
            "description": "Reported LLM performance on the same metric (e.g., \"accuracy 78%\", \"mean RT 850 ms\")."
        },
        {
            "name": "prompting_method",
            "type": "str",
            "description": "How the model was queried (e.g., zero‑shot, few‑shot, chain‑of‑thought, instruction‑tuned)."
        },
        {
            "name": "fine_tuned",
            "type": "bool",
            "description": "Whether the model was fine‑tuned on the specific task or related data (true/false/null if not reported)."
        },
        {
            "name": "human_data_source",
            "type": "str",
            "description": "Citation or dataset from which the human baseline was obtained (e.g., \"PsyToolkit dataset\", \"Miller et al., 2020\")."
        },
        {
            "name": "statistical_significance",
            "type": "str",
            "description": "Any reported statistical comparison between LLM and human performance (e.g., p‑value, confidence interval, effect size)."
        },
        {
            "name": "notes",
            "type": "str",
            "description": "Additional observations, error patterns, or limitations mentioned in the paper relevant to the comparison."
        }
    ],
    "extraction_query": "Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
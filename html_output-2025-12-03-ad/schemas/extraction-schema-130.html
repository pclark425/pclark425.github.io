<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-130 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-130</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-130</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model evaluated (e.g., GPT-4, PaLM, LLaMA, Claude).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model architecture or training regime (e.g., decoder‑only transformer, instruction‑tuned).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Model scale expressed in parameters (e.g., 7B, 70B, 540B).</td>
                    </tr>
                    <tr>
                        <td><strong>puzzle_name</strong></td>
                        <td>str</td>
                        <td>The specific puzzle game evaluated (e.g., Sudoku, Nonogram, Minesweeper, Rubik's Cube).</td>
                    </tr>
                    <tr>
                        <td><strong>puzzle_type</strong></td>
                        <td>str</td>
                        <td>Category of the puzzle with respect to reasoning (e.g., spatial reasoning, combinatorial logic, constraint satisfaction).</td>
                    </tr>
                    <tr>
                        <td><strong>dataset_name</strong></td>
                        <td>str</td>
                        <td>Name of the dataset or benchmark used for evaluation (e.g., Sudoku81, Kaggle Sudoku, Puzzle10k).</td>
                    </tr>
                    <tr>
                        <td><strong>prompting_method</strong></td>
                        <td>str</td>
                        <td>The prompting strategy employed (e.g., zero‑shot, few‑shot, chain‑of‑thought, self‑consistency, tool‑use prompting).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_technique</strong></td>
                        <td>str</td>
                        <td>Explicit reasoning technique described (e.g., constraint propagation, backtracking search, Monte‑Carlo tree search, external SAT solver).</td>
                    </tr>
                    <tr>
                        <td><strong>internal_representation</strong></td>
                        <td>str</td>
                        <td>How the puzzle is encoded for the model (e.g., grid tokenization, positional encoding, row‑column string format).</td>
                    </tr>
                    <tr>
                        <td><strong>use_of_external_tool</strong></td>
                        <td>bool</td>
                        <td>Whether the model invokes an external computational tool (true/false).</td>
                    </tr>
                    <tr>
                        <td><strong>external_tool_description</strong></td>
                        <td>str</td>
                        <td>If an external tool is used, a brief description of the tool and its role (e.g., Python execution of backtracking algorithm, Z3 solver).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metric</strong></td>
                        <td>str</td>
                        <td>Metric reported for performance (e.g., solve rate, accuracy, average steps, time to solution).</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>Numeric result for the reported metric, including units if applicable (e.g., 92% solve rate, 15.3 steps per puzzle).</td>
                    </tr>
                    <tr>
                        <td><strong>analysis_findings</strong></td>
                        <td>str</td>
                        <td>Key observations about the model's reasoning behavior (e.g., attention focuses on rows/columns, common error patterns, evidence of spatial abstraction).</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_comparison</strong></td>
                        <td>str</td>
                        <td>Reported performance differences when a component is added or removed (e.g., with chain‑of‑thought vs without, with external tool vs without).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure modes, or open challenges for the model on the puzzle task.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-130",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model evaluated (e.g., GPT-4, PaLM, LLaMA, Claude)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model architecture or training regime (e.g., decoder‑only transformer, instruction‑tuned)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Model scale expressed in parameters (e.g., 7B, 70B, 540B)."
        },
        {
            "name": "puzzle_name",
            "type": "str",
            "description": "The specific puzzle game evaluated (e.g., Sudoku, Nonogram, Minesweeper, Rubik's Cube)."
        },
        {
            "name": "puzzle_type",
            "type": "str",
            "description": "Category of the puzzle with respect to reasoning (e.g., spatial reasoning, combinatorial logic, constraint satisfaction)."
        },
        {
            "name": "dataset_name",
            "type": "str",
            "description": "Name of the dataset or benchmark used for evaluation (e.g., Sudoku81, Kaggle Sudoku, Puzzle10k)."
        },
        {
            "name": "prompting_method",
            "type": "str",
            "description": "The prompting strategy employed (e.g., zero‑shot, few‑shot, chain‑of‑thought, self‑consistency, tool‑use prompting)."
        },
        {
            "name": "reasoning_technique",
            "type": "str",
            "description": "Explicit reasoning technique described (e.g., constraint propagation, backtracking search, Monte‑Carlo tree search, external SAT solver)."
        },
        {
            "name": "internal_representation",
            "type": "str",
            "description": "How the puzzle is encoded for the model (e.g., grid tokenization, positional encoding, row‑column string format)."
        },
        {
            "name": "use_of_external_tool",
            "type": "bool",
            "description": "Whether the model invokes an external computational tool (true/false)."
        },
        {
            "name": "external_tool_description",
            "type": "str",
            "description": "If an external tool is used, a brief description of the tool and its role (e.g., Python execution of backtracking algorithm, Z3 solver)."
        },
        {
            "name": "evaluation_metric",
            "type": "str",
            "description": "Metric reported for performance (e.g., solve rate, accuracy, average steps, time to solution)."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "Numeric result for the reported metric, including units if applicable (e.g., 92% solve rate, 15.3 steps per puzzle)."
        },
        {
            "name": "analysis_findings",
            "type": "str",
            "description": "Key observations about the model's reasoning behavior (e.g., attention focuses on rows/columns, common error patterns, evidence of spatial abstraction)."
        },
        {
            "name": "ablation_comparison",
            "type": "str",
            "description": "Reported performance differences when a component is added or removed (e.g., with chain‑of‑thought vs without, with external tool vs without)."
        },
        {
            "name": "limitations",
            "type": "str",
            "description": "Any reported limitations, failure modes, or open challenges for the model on the puzzle task."
        }
    ],
    "extraction_query": "Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
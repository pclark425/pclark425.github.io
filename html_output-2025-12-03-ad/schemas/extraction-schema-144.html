<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-144 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-144</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-144</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>paper_title</strong></td>
                        <td>str</td>
                        <td>Title of the paper describing the LLM‑based theory‑distillation method.</td>
                    </tr>
                    <tr>
                        <td><strong>authors</strong></td>
                        <td>str</td>
                        <td>List of authors (or first author) of the paper.</td>
                    </tr>
                    <tr>
                        <td><strong>year</strong></td>
                        <td>int</td>
                        <td>Publication year of the paper.</td>
                    </tr>
                    <tr>
                        <td><strong>method_name</strong></td>
                        <td>str</td>
                        <td>Name or identifier of the proposed method/approach (e.g., "SciFact", "MetaGPT", "LLM‑Synthesis", "Chain‑of‑Thought Review").</td>
                    </tr>
                    <tr>
                        <td><strong>method_description</strong></td>
                        <td>str</td>
                        <td>Brief description of how the method works, focusing on the role of the LLM.</td>
                    </tr>
                    <tr>
                        <td><strong>input_type</strong></td>
                        <td>str</td>
                        <td>Type of scholarly input used (e.g., full‑text articles, abstracts, titles, citation graph, metadata).</td>
                    </tr>
                    <tr>
                        <td><strong>output_type</strong></td>
                        <td>str</td>
                        <td>Form of the distilled output (e.g., textual theory, hypothesis list, concept map, knowledge graph, structured schema).</td>
                    </tr>
                    <tr>
                        <td><strong>prompting_technique</strong></td>
                        <td>str</td>
                        <td>Prompt engineering or interaction technique employed (e.g., few‑shot prompting, chain‑of‑thought, self‑consistency, retrieval‑augmented generation).</td>
                    </tr>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>Name of the LLM used (e.g., GPT‑4, Claude, LLaMA‑2, PaLM‑2).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Scale of the model (e.g., "7B parameters", "175B parameters").</td>
                    </tr>
                    <tr>
                        <td><strong>datasets_used</strong></td>
                        <td>str</td>
                        <td>Scientific corpora or benchmark datasets employed for training/evaluation (e.g., CORD‑19, PubMed, arXiv, Semantic Scholar Corpus).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metric</strong></td>
                        <td>str</td>
                        <td>Metrics reported to assess theory‑distillation quality (e.g., ROUGE, BLEU, human expert rating, precision/recall of extracted concepts, downstream task performance).</td>
                    </tr>
                    <tr>
                        <td><strong>reported_results</strong></td>
                        <td>str</td>
                        <td>Concise summary of the quantitative or qualitative results presented in the paper.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations</strong></td>
                        <td>str</td>
                        <td>Any noted challenges, failure modes, or limitations of the approach (e.g., hallucination, scalability, bias).</td>
                    </tr>
                    <tr>
                        <td><strong>counterpoint</strong></td>
                        <td>bool</td>
                        <td>Does the paper present evidence or arguments that question the feasibility or reliability of LLM‑based theory distillation? (true, false, or null if not addressed).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-144",
    "schema": [
        {
            "name": "paper_title",
            "type": "str",
            "description": "Title of the paper describing the LLM‑based theory‑distillation method."
        },
        {
            "name": "authors",
            "type": "str",
            "description": "List of authors (or first author) of the paper."
        },
        {
            "name": "year",
            "type": "int",
            "description": "Publication year of the paper."
        },
        {
            "name": "method_name",
            "type": "str",
            "description": "Name or identifier of the proposed method/approach (e.g., \"SciFact\", \"MetaGPT\", \"LLM‑Synthesis\", \"Chain‑of‑Thought Review\")."
        },
        {
            "name": "method_description",
            "type": "str",
            "description": "Brief description of how the method works, focusing on the role of the LLM."
        },
        {
            "name": "input_type",
            "type": "str",
            "description": "Type of scholarly input used (e.g., full‑text articles, abstracts, titles, citation graph, metadata)."
        },
        {
            "name": "output_type",
            "type": "str",
            "description": "Form of the distilled output (e.g., textual theory, hypothesis list, concept map, knowledge graph, structured schema)."
        },
        {
            "name": "prompting_technique",
            "type": "str",
            "description": "Prompt engineering or interaction technique employed (e.g., few‑shot prompting, chain‑of‑thought, self‑consistency, retrieval‑augmented generation)."
        },
        {
            "name": "model_name",
            "type": "str",
            "description": "Name of the LLM used (e.g., GPT‑4, Claude, LLaMA‑2, PaLM‑2)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Scale of the model (e.g., \"7B parameters\", \"175B parameters\")."
        },
        {
            "name": "datasets_used",
            "type": "str",
            "description": "Scientific corpora or benchmark datasets employed for training/evaluation (e.g., CORD‑19, PubMed, arXiv, Semantic Scholar Corpus)."
        },
        {
            "name": "evaluation_metric",
            "type": "str",
            "description": "Metrics reported to assess theory‑distillation quality (e.g., ROUGE, BLEU, human expert rating, precision/recall of extracted concepts, downstream task performance)."
        },
        {
            "name": "reported_results",
            "type": "str",
            "description": "Concise summary of the quantitative or qualitative results presented in the paper."
        },
        {
            "name": "limitations",
            "type": "str",
            "description": "Any noted challenges, failure modes, or limitations of the approach (e.g., hallucination, scalability, bias)."
        },
        {
            "name": "counterpoint",
            "type": "bool",
            "description": "Does the paper present evidence or arguments that question the feasibility or reliability of LLM‑based theory distillation? (true, false, or null if not addressed)."
        }
    ],
    "extraction_query": "Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
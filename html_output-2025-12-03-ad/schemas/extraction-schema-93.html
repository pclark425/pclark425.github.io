<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-93 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-93</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-93</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>evaluation_setting</strong></td>
                        <td>str</td>
                        <td>A brief description of the evaluation setting or protocol (e.g., 'LLM-as-a-judge for summarization', 'human evaluation of chatbot responses', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_or_domain</strong></td>
                        <td>str</td>
                        <td>The specific task, benchmark, or domain being evaluated (e.g., summarization, dialogue, code generation, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>llm_model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM used as a judge (e.g., GPT-4, Claude, etc.), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>agreement_rate</strong></td>
                        <td>str</td>
                        <td>Reported agreement rate or metric between LLM-as-a-judge and human evaluations (e.g., percentage agreement, Cohen's kappa, etc.), or null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>qualitative_differences</strong></td>
                        <td>str</td>
                        <td>A concise summary of any qualitative differences or aspects that are lost, missed, or degraded when using LLMs as judges compared to humans (e.g., 'LLMs miss subtle humor', 'LLMs are less sensitive to factual errors', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_cases</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure cases, or criticisms of LLM-as-a-judge evaluations (e.g., 'LLMs are biased towards certain styles', 'LLMs fail to detect offensive content', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>counterexamples_or_strengths</strong></td>
                        <td>str</td>
                        <td>Any reported cases where LLM-as-a-judge matches or exceeds human evaluation, or specific strengths of LLM-as-a-judge (e.g., 'LLMs are more consistent', 'LLMs are less fatigued', etc.), or null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>recommendations_or_best_practices</strong></td>
                        <td>str</td>
                        <td>Any recommendations, best practices, or mitigation strategies for using LLMs as judges (e.g., 'combine LLM and human evaluations', 'use LLMs only for certain tasks', etc.), or null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>citation</strong></td>
                        <td>str</td>
                        <td>The citation or reference for the paper, in a concise format.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-93",
    "schema": [
        {
            "name": "evaluation_setting",
            "type": "str",
            "description": "A brief description of the evaluation setting or protocol (e.g., 'LLM-as-a-judge for summarization', 'human evaluation of chatbot responses', etc.)."
        },
        {
            "name": "task_or_domain",
            "type": "str",
            "description": "The specific task, benchmark, or domain being evaluated (e.g., summarization, dialogue, code generation, etc.)."
        },
        {
            "name": "llm_model_name",
            "type": "str",
            "description": "The name of the LLM used as a judge (e.g., GPT-4, Claude, etc.), or null if not specified."
        },
        {
            "name": "agreement_rate",
            "type": "str",
            "description": "Reported agreement rate or metric between LLM-as-a-judge and human evaluations (e.g., percentage agreement, Cohen's kappa, etc.), or null if not reported."
        },
        {
            "name": "qualitative_differences",
            "type": "str",
            "description": "A concise summary of any qualitative differences or aspects that are lost, missed, or degraded when using LLMs as judges compared to humans (e.g., 'LLMs miss subtle humor', 'LLMs are less sensitive to factual errors', etc.)."
        },
        {
            "name": "limitations_or_failure_cases",
            "type": "str",
            "description": "Any reported limitations, failure cases, or criticisms of LLM-as-a-judge evaluations (e.g., 'LLMs are biased towards certain styles', 'LLMs fail to detect offensive content', etc.)."
        },
        {
            "name": "counterexamples_or_strengths",
            "type": "str",
            "description": "Any reported cases where LLM-as-a-judge matches or exceeds human evaluation, or specific strengths of LLM-as-a-judge (e.g., 'LLMs are more consistent', 'LLMs are less fatigued', etc.), or null if not reported."
        },
        {
            "name": "recommendations_or_best_practices",
            "type": "str",
            "description": "Any recommendations, best practices, or mitigation strategies for using LLMs as judges (e.g., 'combine LLM and human evaluations', 'use LLMs only for certain tasks', etc.), or null if not reported."
        },
        {
            "name": "citation",
            "type": "str",
            "description": "The citation or reference for the paper, in a concise format."
        }
    ],
    "extraction_query": "Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-64 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-64</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-64</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>system_name</strong></td>
                        <td>str</td>
                        <td>The name of the system, method, or model that automatically generates research hypotheses or ideas.</td>
                    </tr>
                    <tr>
                        <td><strong>system_description</strong></td>
                        <td>str</td>
                        <td>A detailed description of how the system generates research hypotheses, including the underlying approach (e.g., LLM-based, evolutionary algorithms, knowledge graphs, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>research_domain</strong></td>
                        <td>str</td>
                        <td>The research domain or field where hypotheses are being generated (e.g., 'machine learning', 'biology', 'chemistry', 'materials science', 'general scientific research', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>problem_type</strong></td>
                        <td>str</td>
                        <td>The type of research problem being addressed (e.g., 'open-ended exploration', 'targeted optimization', 'interdisciplinary synthesis', 'incremental improvement', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_metric</strong></td>
                        <td>str</td>
                        <td>How is novelty measured or quantified? Include specific metrics, formulas, or methods (e.g., 'semantic distance from existing literature', 'expert ratings on 1-5 scale', 'citation network analysis', etc.). null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_score</strong></td>
                        <td>str</td>
                        <td>What novelty scores or results were reported? Include numerical values with units/scales. null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>feasibility_metric</strong></td>
                        <td>str</td>
                        <td>How is feasibility measured or quantified? Include specific metrics, formulas, or methods (e.g., 'expert ratings', 'resource requirements', 'technical complexity scores', 'likelihood of success', etc.). null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>feasibility_score</strong></td>
                        <td>str</td>
                        <td>What feasibility scores or results were reported? Include numerical values with units/scales. null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>tradeoff_evidence</strong></td>
                        <td>str</td>
                        <td>What evidence is provided about the trade-off between novelty and feasibility? (e.g., 'negative correlation of -0.65', 'highly novel ideas rated as less feasible', 'Pareto frontier analysis', etc.). null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>optimization_strategy</strong></td>
                        <td>str</td>
                        <td>What strategy or method is used to optimize or balance the novelty-feasibility trade-off? (e.g., 'multi-objective optimization', 'weighted scoring', 'iterative refinement', 'constraint satisfaction', etc.). null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>human_evaluation</strong></td>
                        <td>bool</td>
                        <td>Were the generated hypotheses evaluated by human experts or researchers? (true, false, or null for no information)</td>
                    </tr>
                    <tr>
                        <td><strong>human_evaluation_results</strong></td>
                        <td>str</td>
                        <td>If human evaluation was performed, what were the key findings regarding novelty, feasibility, or their trade-off? Be specific and include numerical results if available. null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>comparative_baseline</strong></td>
                        <td>str</td>
                        <td>What baseline or comparison is used? (e.g., 'human-generated hypotheses', 'random sampling', 'simpler model', etc.). null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>comparative_results</strong></td>
                        <td>str</td>
                        <td>How does the system compare to baselines in terms of novelty, feasibility, or overall quality? Include specific numerical comparisons. null if not mentioned.</td>
                    </tr>
                    <tr>
                        <td><strong>domain_specific_findings</strong></td>
                        <td>str</td>
                        <td>Are there any findings specific to certain research domains or problem types regarding the novelty-feasibility trade-off? null if not mentioned.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-64",
    "schema": [
        {
            "name": "system_name",
            "type": "str",
            "description": "The name of the system, method, or model that automatically generates research hypotheses or ideas."
        },
        {
            "name": "system_description",
            "type": "str",
            "description": "A detailed description of how the system generates research hypotheses, including the underlying approach (e.g., LLM-based, evolutionary algorithms, knowledge graphs, etc.)."
        },
        {
            "name": "research_domain",
            "type": "str",
            "description": "The research domain or field where hypotheses are being generated (e.g., 'machine learning', 'biology', 'chemistry', 'materials science', 'general scientific research', etc.)."
        },
        {
            "name": "problem_type",
            "type": "str",
            "description": "The type of research problem being addressed (e.g., 'open-ended exploration', 'targeted optimization', 'interdisciplinary synthesis', 'incremental improvement', etc.)."
        },
        {
            "name": "novelty_metric",
            "type": "str",
            "description": "How is novelty measured or quantified? Include specific metrics, formulas, or methods (e.g., 'semantic distance from existing literature', 'expert ratings on 1-5 scale', 'citation network analysis', etc.). null if not mentioned."
        },
        {
            "name": "novelty_score",
            "type": "str",
            "description": "What novelty scores or results were reported? Include numerical values with units/scales. null if not mentioned."
        },
        {
            "name": "feasibility_metric",
            "type": "str",
            "description": "How is feasibility measured or quantified? Include specific metrics, formulas, or methods (e.g., 'expert ratings', 'resource requirements', 'technical complexity scores', 'likelihood of success', etc.). null if not mentioned."
        },
        {
            "name": "feasibility_score",
            "type": "str",
            "description": "What feasibility scores or results were reported? Include numerical values with units/scales. null if not mentioned."
        },
        {
            "name": "tradeoff_evidence",
            "type": "str",
            "description": "What evidence is provided about the trade-off between novelty and feasibility? (e.g., 'negative correlation of -0.65', 'highly novel ideas rated as less feasible', 'Pareto frontier analysis', etc.). null if not mentioned."
        },
        {
            "name": "optimization_strategy",
            "type": "str",
            "description": "What strategy or method is used to optimize or balance the novelty-feasibility trade-off? (e.g., 'multi-objective optimization', 'weighted scoring', 'iterative refinement', 'constraint satisfaction', etc.). null if not mentioned."
        },
        {
            "name": "human_evaluation",
            "type": "bool",
            "description": "Were the generated hypotheses evaluated by human experts or researchers? (true, false, or null for no information)"
        },
        {
            "name": "human_evaluation_results",
            "type": "str",
            "description": "If human evaluation was performed, what were the key findings regarding novelty, feasibility, or their trade-off? Be specific and include numerical results if available. null if not mentioned."
        },
        {
            "name": "comparative_baseline",
            "type": "str",
            "description": "What baseline or comparison is used? (e.g., 'human-generated hypotheses', 'random sampling', 'simpler model', etc.). null if not mentioned."
        },
        {
            "name": "comparative_results",
            "type": "str",
            "description": "How does the system compare to baselines in terms of novelty, feasibility, or overall quality? Include specific numerical comparisons. null if not mentioned."
        },
        {
            "name": "domain_specific_findings",
            "type": "str",
            "description": "Are there any findings specific to certain research domains or problem types regarding the novelty-feasibility trade-off? null if not mentioned."
        }
    ],
    "extraction_query": "Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
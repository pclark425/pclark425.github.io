<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-34 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-34</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-34</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>proxy_evaluation_method</strong></td>
                        <td>str</td>
                        <td>The type of proxy/automated evaluation method used (e.g., 'LLM-as-a-judge', 'Likert-style rating', 'automated metrics', 'rubric-based scoring', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>llm_judge_model</strong></td>
                        <td>str</td>
                        <td>If LLM-as-a-judge was used, what model(s) served as the judge? (e.g., 'GPT-4', 'Claude-3', 'GPT-3.5', etc. null if not applicable)</td>
                    </tr>
                    <tr>
                        <td><strong>llm_judge_prompt_approach</strong></td>
                        <td>str</td>
                        <td>If LLM-as-a-judge was used, describe the prompting approach (e.g., 'pairwise comparison', 'single rating with rubric', 'chain-of-thought reasoning', etc. null if not applicable)</td>
                    </tr>
                    <tr>
                        <td><strong>artifact_type</strong></td>
                        <td>str</td>
                        <td>What type of artifact was being evaluated? (e.g., 'source code', 'code documentation', 'API design', 'software architecture', 'bug reports', 'pull requests', 'test cases', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>artifact_domain</strong></td>
                        <td>str</td>
                        <td>What domain or programming language were the artifacts from? (e.g., 'Python code', 'Java applications', 'web development', 'general purpose', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_criteria</strong></td>
                        <td>str</td>
                        <td>What specific criteria or dimensions were being evaluated? (e.g., 'code quality', 'correctness', 'readability', 'maintainability', 'security', 'performance', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>human_evaluation_setup</strong></td>
                        <td>str</td>
                        <td>How was the human expert evaluation conducted? (e.g., 'three expert developers independently rated', 'consensus among senior engineers', 'single expert review', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>human_expert_count</strong></td>
                        <td>str</td>
                        <td>How many human experts were involved in the evaluation? (numerical, null if not specified)</td>
                    </tr>
                    <tr>
                        <td><strong>human_expert_expertise</strong></td>
                        <td>str</td>
                        <td>What was the expertise level or background of the human evaluators? (e.g., 'senior software engineers with 10+ years experience', 'domain experts', 'professional code reviewers', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>agreement_metric</strong></td>
                        <td>str</td>
                        <td>What metric(s) were used to measure agreement between proxy and human evaluation? (e.g., 'Pearson correlation', 'Spearman correlation', 'Cohen's kappa', 'accuracy', 'F1 score', 'Kendall's tau', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>agreement_score</strong></td>
                        <td>str</td>
                        <td>What was the quantitative agreement score between proxy and human evaluation? Include the metric and value (e.g., 'Pearson r=0.85', 'Cohen's kappa=0.72', 'accuracy=89%', etc. null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>high_agreement_conditions</strong></td>
                        <td>str</td>
                        <td>What conditions, factors, or characteristics were associated with HIGH agreement between proxy and human evaluation? (e.g., 'clear evaluation rubrics', 'objective criteria', 'simple artifacts', 'well-defined tasks', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>low_agreement_conditions</strong></td>
                        <td>str</td>
                        <td>What conditions, factors, or characteristics were associated with LOW agreement between proxy and human evaluation? (e.g., 'subjective criteria', 'complex artifacts', 'ambiguous requirements', 'domain-specific knowledge required', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>artifact_complexity_effect</strong></td>
                        <td>str</td>
                        <td>Did artifact complexity affect agreement? If so, how? (e.g., 'agreement decreased for complex codebases', 'no significant effect', 'higher agreement on simple functions', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>criteria_clarity_effect</strong></td>
                        <td>str</td>
                        <td>Did the clarity or specificity of evaluation criteria affect agreement? If so, how? (e.g., 'well-defined rubrics increased agreement by 20%', 'objective criteria showed higher correlation', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>sample_size</strong></td>
                        <td>str</td>
                        <td>How many artifacts were evaluated in the study? (numerical, null if not specified)</td>
                    </tr>
                    <tr>
                        <td><strong>inter_human_agreement</strong></td>
                        <td>str</td>
                        <td>If multiple human evaluators were used, what was the inter-rater agreement among humans? (e.g., 'Cohen's kappa=0.88 between human raters', null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_vs_human_comparison</strong></td>
                        <td>str</td>
                        <td>How did the proxy evaluation agreement with humans compare to inter-human agreement? (e.g., 'LLM-human agreement (r=0.82) approached inter-human agreement (r=0.89)', null if not compared)</td>
                    </tr>
                    <tr>
                        <td><strong>calibration_or_training</strong></td>
                        <td>str</td>
                        <td>Was the proxy evaluation method calibrated, fine-tuned, or trained on human judgments? If so, describe the approach. (null if not applicable)</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings</strong></td>
                        <td>str</td>
                        <td>What were the key findings or conclusions about alignment between proxy and human evaluation? Be specific and information-dense.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_noted</strong></td>
                        <td>str</td>
                        <td>What limitations or challenges in achieving alignment were noted by the authors? (e.g., 'LLM judges struggled with domain-specific conventions', 'Likert scales showed ceiling effects', etc.)</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-34",
    "schema": [
        {
            "name": "proxy_evaluation_method",
            "type": "str",
            "description": "The type of proxy/automated evaluation method used (e.g., 'LLM-as-a-judge', 'Likert-style rating', 'automated metrics', 'rubric-based scoring', etc.)"
        },
        {
            "name": "llm_judge_model",
            "type": "str",
            "description": "If LLM-as-a-judge was used, what model(s) served as the judge? (e.g., 'GPT-4', 'Claude-3', 'GPT-3.5', etc. null if not applicable)"
        },
        {
            "name": "llm_judge_prompt_approach",
            "type": "str",
            "description": "If LLM-as-a-judge was used, describe the prompting approach (e.g., 'pairwise comparison', 'single rating with rubric', 'chain-of-thought reasoning', etc. null if not applicable)"
        },
        {
            "name": "artifact_type",
            "type": "str",
            "description": "What type of artifact was being evaluated? (e.g., 'source code', 'code documentation', 'API design', 'software architecture', 'bug reports', 'pull requests', 'test cases', etc.)"
        },
        {
            "name": "artifact_domain",
            "type": "str",
            "description": "What domain or programming language were the artifacts from? (e.g., 'Python code', 'Java applications', 'web development', 'general purpose', etc.)"
        },
        {
            "name": "evaluation_criteria",
            "type": "str",
            "description": "What specific criteria or dimensions were being evaluated? (e.g., 'code quality', 'correctness', 'readability', 'maintainability', 'security', 'performance', etc.)"
        },
        {
            "name": "human_evaluation_setup",
            "type": "str",
            "description": "How was the human expert evaluation conducted? (e.g., 'three expert developers independently rated', 'consensus among senior engineers', 'single expert review', etc.)"
        },
        {
            "name": "human_expert_count",
            "type": "str",
            "description": "How many human experts were involved in the evaluation? (numerical, null if not specified)"
        },
        {
            "name": "human_expert_expertise",
            "type": "str",
            "description": "What was the expertise level or background of the human evaluators? (e.g., 'senior software engineers with 10+ years experience', 'domain experts', 'professional code reviewers', etc.)"
        },
        {
            "name": "agreement_metric",
            "type": "str",
            "description": "What metric(s) were used to measure agreement between proxy and human evaluation? (e.g., 'Pearson correlation', 'Spearman correlation', 'Cohen's kappa', 'accuracy', 'F1 score', 'Kendall's tau', etc.)"
        },
        {
            "name": "agreement_score",
            "type": "str",
            "description": "What was the quantitative agreement score between proxy and human evaluation? Include the metric and value (e.g., 'Pearson r=0.85', 'Cohen's kappa=0.72', 'accuracy=89%', etc. null if not reported)"
        },
        {
            "name": "high_agreement_conditions",
            "type": "str",
            "description": "What conditions, factors, or characteristics were associated with HIGH agreement between proxy and human evaluation? (e.g., 'clear evaluation rubrics', 'objective criteria', 'simple artifacts', 'well-defined tasks', etc.)"
        },
        {
            "name": "low_agreement_conditions",
            "type": "str",
            "description": "What conditions, factors, or characteristics were associated with LOW agreement between proxy and human evaluation? (e.g., 'subjective criteria', 'complex artifacts', 'ambiguous requirements', 'domain-specific knowledge required', etc.)"
        },
        {
            "name": "artifact_complexity_effect",
            "type": "str",
            "description": "Did artifact complexity affect agreement? If so, how? (e.g., 'agreement decreased for complex codebases', 'no significant effect', 'higher agreement on simple functions', etc.)"
        },
        {
            "name": "criteria_clarity_effect",
            "type": "str",
            "description": "Did the clarity or specificity of evaluation criteria affect agreement? If so, how? (e.g., 'well-defined rubrics increased agreement by 20%', 'objective criteria showed higher correlation', etc.)"
        },
        {
            "name": "sample_size",
            "type": "str",
            "description": "How many artifacts were evaluated in the study? (numerical, null if not specified)"
        },
        {
            "name": "inter_human_agreement",
            "type": "str",
            "description": "If multiple human evaluators were used, what was the inter-rater agreement among humans? (e.g., 'Cohen's kappa=0.88 between human raters', null if not reported)"
        },
        {
            "name": "proxy_vs_human_comparison",
            "type": "str",
            "description": "How did the proxy evaluation agreement with humans compare to inter-human agreement? (e.g., 'LLM-human agreement (r=0.82) approached inter-human agreement (r=0.89)', null if not compared)"
        },
        {
            "name": "calibration_or_training",
            "type": "str",
            "description": "Was the proxy evaluation method calibrated, fine-tuned, or trained on human judgments? If so, describe the approach. (null if not applicable)"
        },
        {
            "name": "key_findings",
            "type": "str",
            "description": "What were the key findings or conclusions about alignment between proxy and human evaluation? Be specific and information-dense."
        },
        {
            "name": "limitations_noted",
            "type": "str",
            "description": "What limitations or challenges in achieving alignment were noted by the authors? (e.g., 'LLM judges struggled with domain-specific conventions', 'Likert scales showed ceiling effects', etc.)"
        }
    ],
    "extraction_query": "Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
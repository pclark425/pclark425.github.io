<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-166 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-166</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-166</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM or model used for distilling quantitative laws (e.g., GPT-4, Llama-2, SciBERT, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM or model, including architecture, size (parameters), and any relevant modifications.</td>
                    </tr>
                    <tr>
                        <td><strong>scientific_domain</strong></td>
                        <td>str</td>
                        <td>The scientific field or domain from which quantitative laws are being distilled (e.g., physics, chemistry, biology, materials science).</td>
                    </tr>
                    <tr>
                        <td><strong>law_type</strong></td>
                        <td>str</td>
                        <td>The type of quantitative law or relationship being targeted (e.g., physical law, empirical equation, scaling law, mathematical formula).</td>
                    </tr>
                    <tr>
                        <td><strong>method_description</strong></td>
                        <td>str</td>
                        <td>A concise description of the method or approach used to distill laws (e.g., prompt engineering, fine-tuning, retrieval-augmented generation, chain-of-thought, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>input_corpus_description</strong></td>
                        <td>str</td>
                        <td>Description of the input corpus, including the number and type of scholarly papers, dataset name, and any preprocessing steps.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>How the success of law distillation is evaluated (e.g., accuracy, novelty, correctness, comparison to known laws, human expert evaluation).</td>
                    </tr>
                    <tr>
                        <td><strong>results_summary</strong></td>
                        <td>str</td>
                        <td>A brief summary of the results, including quantitative metrics if available (e.g., number of laws discovered, accuracy, precision, recall, F1, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>notable_examples</strong></td>
                        <td>str</td>
                        <td>Notable examples of quantitative laws or equations successfully distilled, or notable failures/missed laws.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_challenges</strong></td>
                        <td>str</td>
                        <td>Reported limitations, challenges, or failure cases in using LLMs for this task (e.g., hallucination, lack of novelty, inability to generalize, data quality issues).</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_comparison</strong></td>
                        <td>str</td>
                        <td>Comparison to baselines, such as human experts or traditional data mining methods, if available.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-166",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM or model used for distilling quantitative laws (e.g., GPT-4, Llama-2, SciBERT, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM or model, including architecture, size (parameters), and any relevant modifications."
        },
        {
            "name": "scientific_domain",
            "type": "str",
            "description": "The scientific field or domain from which quantitative laws are being distilled (e.g., physics, chemistry, biology, materials science)."
        },
        {
            "name": "law_type",
            "type": "str",
            "description": "The type of quantitative law or relationship being targeted (e.g., physical law, empirical equation, scaling law, mathematical formula)."
        },
        {
            "name": "method_description",
            "type": "str",
            "description": "A concise description of the method or approach used to distill laws (e.g., prompt engineering, fine-tuning, retrieval-augmented generation, chain-of-thought, etc.)."
        },
        {
            "name": "input_corpus_description",
            "type": "str",
            "description": "Description of the input corpus, including the number and type of scholarly papers, dataset name, and any preprocessing steps."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "How the success of law distillation is evaluated (e.g., accuracy, novelty, correctness, comparison to known laws, human expert evaluation)."
        },
        {
            "name": "results_summary",
            "type": "str",
            "description": "A brief summary of the results, including quantitative metrics if available (e.g., number of laws discovered, accuracy, precision, recall, F1, etc.)."
        },
        {
            "name": "notable_examples",
            "type": "str",
            "description": "Notable examples of quantitative laws or equations successfully distilled, or notable failures/missed laws."
        },
        {
            "name": "limitations_challenges",
            "type": "str",
            "description": "Reported limitations, challenges, or failure cases in using LLMs for this task (e.g., hallucination, lack of novelty, inability to generalize, data quality issues)."
        },
        {
            "name": "baseline_comparison",
            "type": "str",
            "description": "Comparison to baselines, such as human experts or traditional data mining methods, if available."
        }
    ],
    "extraction_query": "Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-169 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-169</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-169</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>task_domain</strong></td>
                        <td>str</td>
                        <td>The specific task or domain where LLM-as-a-judge and human evaluations are compared (e.g., summarization, question answering, dialogue, code generation, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>llm_judge_model</strong></td>
                        <td>str</td>
                        <td>The name and version of the LLM used as a judge (e.g., GPT-4, Claude 2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>llm_judge_setup</strong></td>
                        <td>str</td>
                        <td>A brief description of how the LLM-as-a-judge evaluation was conducted (e.g., prompt format, instructions, context given, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>human_evaluation_setup</strong></td>
                        <td>str</td>
                        <td>A brief description of how the human evaluation was conducted (e.g., number of annotators, expertise, instructions, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>agreement_metric</strong></td>
                        <td>str</td>
                        <td>The metric(s) used to compare LLM and human judgments (e.g., percent agreement, Cohen's kappa, correlation, etc.), including the reported value(s) if available.</td>
                    </tr>
                    <tr>
                        <td><strong>losses_identified</strong></td>
                        <td>str</td>
                        <td>A concise summary of what is lost, degraded, or different when using LLMs as judges instead of humans, as identified in the paper (e.g., inability to detect subtle errors, lack of domain knowledge, missing subjective nuance, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>examples_of_loss</strong></td>
                        <td>str</td>
                        <td>Specific examples or case studies where LLM-as-a-judge evaluations failed or diverged from human evaluations, if provided.</td>
                    </tr>
                    <tr>
                        <td><strong>counterexamples_or_caveats</strong></td>
                        <td>str</td>
                        <td>Any counterexamples, caveats, or cases where LLM-as-a-judge matched or outperformed human evaluation, or where losses were not observed.</td>
                    </tr>
                    <tr>
                        <td><strong>paper_reference</strong></td>
                        <td>str</td>
                        <td>A citation or reference to the paper or section where this information was found.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-169",
    "schema": [
        {
            "name": "task_domain",
            "type": "str",
            "description": "The specific task or domain where LLM-as-a-judge and human evaluations are compared (e.g., summarization, question answering, dialogue, code generation, etc.)."
        },
        {
            "name": "llm_judge_model",
            "type": "str",
            "description": "The name and version of the LLM used as a judge (e.g., GPT-4, Claude 2, etc.)."
        },
        {
            "name": "llm_judge_setup",
            "type": "str",
            "description": "A brief description of how the LLM-as-a-judge evaluation was conducted (e.g., prompt format, instructions, context given, etc.)."
        },
        {
            "name": "human_evaluation_setup",
            "type": "str",
            "description": "A brief description of how the human evaluation was conducted (e.g., number of annotators, expertise, instructions, etc.)."
        },
        {
            "name": "agreement_metric",
            "type": "str",
            "description": "The metric(s) used to compare LLM and human judgments (e.g., percent agreement, Cohen's kappa, correlation, etc.), including the reported value(s) if available."
        },
        {
            "name": "losses_identified",
            "type": "str",
            "description": "A concise summary of what is lost, degraded, or different when using LLMs as judges instead of humans, as identified in the paper (e.g., inability to detect subtle errors, lack of domain knowledge, missing subjective nuance, etc.)."
        },
        {
            "name": "examples_of_loss",
            "type": "str",
            "description": "Specific examples or case studies where LLM-as-a-judge evaluations failed or diverged from human evaluations, if provided."
        },
        {
            "name": "counterexamples_or_caveats",
            "type": "str",
            "description": "Any counterexamples, caveats, or cases where LLM-as-a-judge matched or outperformed human evaluation, or where losses were not observed."
        },
        {
            "name": "paper_reference",
            "type": "str",
            "description": "A citation or reference to the paper or section where this information was found."
        }
    ],
    "extraction_query": "Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
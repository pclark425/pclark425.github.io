<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-140 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-140</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-140</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM evaluated (e.g., GPT-4, LLaMA-2, PaLM).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model architecture or training regime.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Model scale in parameters (e.g., 7B, 70B, 540B).</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The benchmark or task on which the model is evaluated (e.g., MMLU, GSM8K, ARC, Codeforces).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A concise description of what the task requires.</td>
                    </tr>
                    <tr>
                        <td><strong>problem_format</strong></td>
                        <td>str</td>
                        <td>How the problem is presented to the model (e.g., multiple‑choice question, fill‑in‑the‑blank, chain‑of‑thought prompt, tabular data, code snippet, JSON input, bullet list, natural‑language description).</td>
                    </tr>
                    <tr>
                        <td><strong>format_category</strong></td>
                        <td>str</td>
                        <td>High‑level category of the format (e.g., 'question type', 'prompt style', 'input modality').</td>
                    </tr>
                    <tr>
                        <td><strong>format_details</strong></td>
                        <td>str</td>
                        <td>Specific characteristics of the format such as presence/number of examples, ordering, delimiters, use of headings, temperature settings, few‑shot vs zero‑shot, chain‑of‑thought prompting, etc.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metric</strong></td>
                        <td>str</td>
                        <td>Metric used to report results (e.g., accuracy, F1, BLEU, Exact Match, ROUGE).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_value</strong></td>
                        <td>str</td>
                        <td>Reported performance for the given format (include numeric value and unit, e.g., '78.4% accuracy').</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_performance</strong></td>
                        <td>str</td>
                        <td>Performance reported for a contrasting format or a default prompt, enabling direct comparison (null if not provided).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_change</strong></td>
                        <td>str</td>
                        <td>Relative change between the reported format and the baseline (e.g., '+5.2% absolute', '-1.3% relative'), if stated.</td>
                    </tr>
                    <tr>
                        <td><strong>experimental_setting</strong></td>
                        <td>str</td>
                        <td>Other relevant experimental conditions (e.g., temperature, max tokens, decoding strategy, few‑shot count).</td>
                    </tr>
                    <tr>
                        <td><strong>statistical_significance</strong></td>
                        <td>str</td>
                        <td>Any reported statistical significance or confidence interval for the performance difference (null if not reported).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-140",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM evaluated (e.g., GPT-4, LLaMA-2, PaLM)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model architecture or training regime."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Model scale in parameters (e.g., 7B, 70B, 540B)."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The benchmark or task on which the model is evaluated (e.g., MMLU, GSM8K, ARC, Codeforces)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A concise description of what the task requires."
        },
        {
            "name": "problem_format",
            "type": "str",
            "description": "How the problem is presented to the model (e.g., multiple‑choice question, fill‑in‑the‑blank, chain‑of‑thought prompt, tabular data, code snippet, JSON input, bullet list, natural‑language description)."
        },
        {
            "name": "format_category",
            "type": "str",
            "description": "High‑level category of the format (e.g., 'question type', 'prompt style', 'input modality')."
        },
        {
            "name": "format_details",
            "type": "str",
            "description": "Specific characteristics of the format such as presence/number of examples, ordering, delimiters, use of headings, temperature settings, few‑shot vs zero‑shot, chain‑of‑thought prompting, etc."
        },
        {
            "name": "performance_metric",
            "type": "str",
            "description": "Metric used to report results (e.g., accuracy, F1, BLEU, Exact Match, ROUGE)."
        },
        {
            "name": "performance_value",
            "type": "str",
            "description": "Reported performance for the given format (include numeric value and unit, e.g., '78.4% accuracy')."
        },
        {
            "name": "baseline_performance",
            "type": "str",
            "description": "Performance reported for a contrasting format or a default prompt, enabling direct comparison (null if not provided)."
        },
        {
            "name": "performance_change",
            "type": "str",
            "description": "Relative change between the reported format and the baseline (e.g., '+5.2% absolute', '-1.3% relative'), if stated."
        },
        {
            "name": "experimental_setting",
            "type": "str",
            "description": "Other relevant experimental conditions (e.g., temperature, max tokens, decoding strategy, few‑shot count)."
        },
        {
            "name": "statistical_significance",
            "type": "str",
            "description": "Any reported statistical significance or confidence interval for the performance difference (null if not reported)."
        }
    ],
    "extraction_query": "Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
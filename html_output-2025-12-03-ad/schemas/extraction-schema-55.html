<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-55 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-55</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-55</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>proxy_metric_type</strong></td>
                        <td>str</td>
                        <td>What proxy metric is being studied? (e.g., 'early citations', 'journal impact factor', 'peer review scores', 'author h-index', 'automated novelty detection', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_measure</strong></td>
                        <td>str</td>
                        <td>What ground truth measure of scientific value is used for comparison? (e.g., 'long-term citations', 'Nobel prizes', 'retrospective expert assessment', 'paradigm shift indicators', 'practical applications', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>novelty_transformation_measure</strong></td>
                        <td>str</td>
                        <td>How is novelty, transformation degree, or innovativeness measured in this study? (e.g., 'atypical combination score', 'interdisciplinarity index', 'paradigm departure score', 'incremental vs breakthrough classification', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>quantitative_relationship</strong></td>
                        <td>str</td>
                        <td>What is the quantitative relationship between novelty/transformation and the proxy-truth gap? Include specific numbers, percentages, correlations, or mathematical relationships if reported (e.g., 'highly novel papers receive 40% fewer citations in first 3 years but 200% more after 10 years', 'exponential relationship with R²=0.65', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>gap_magnitude</strong></td>
                        <td>str</td>
                        <td>What is the magnitude of divergence between proxy and ground truth for different levels of novelty? Include specific percentages or effect sizes if available (e.g., 'incremental work: 5% gap, transformational work: 75% gap', null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>temporal_pattern</strong></td>
                        <td>str</td>
                        <td>How does the proxy-truth gap change over time? Include specific time periods and patterns (e.g., 'gap closes exponentially with half-life of 5 years', 'novel work undervalued for 3-5 years then overvalued', null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>field_studied</strong></td>
                        <td>str</td>
                        <td>What scientific field(s) or discipline(s) were studied? (e.g., 'physics', 'biology', 'computer science', 'cross-disciplinary comparison', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>field_differences</strong></td>
                        <td>str</td>
                        <td>Are there differences between fields in how proxies relate to ground truth for novel work? Include specific comparisons if reported (e.g., 'physics shows 2x larger gap than biology', 'rigid paradigm fields show steeper relationships', null if not reported)</td>
                    </tr>
                    <tr>
                        <td><strong>multiplicative_vs_additive</strong></td>
                        <td>str</td>
                        <td>Does the study provide evidence about whether multiple proxy failures compound multiplicatively or additively? (e.g., 'violations across citation, prestige, and author metrics compound multiplicatively', 'independent additive effects', null if not addressed)</td>
                    </tr>
                    <tr>
                        <td><strong>automated_system_performance</strong></td>
                        <td>str</td>
                        <td>If automated evaluation systems are studied, how well do they identify transformational vs incremental work? Include accuracy, bias measures, or performance metrics (e.g., 'ML system achieves 65% accuracy on novel work vs 90% on incremental', null if not applicable)</td>
                    </tr>
                    <tr>
                        <td><strong>correction_mechanism</strong></td>
                        <td>str</td>
                        <td>Does the study test any mechanism to correct proxy-truth gaps? Describe the mechanism and its effectiveness (e.g., 'meta-learning approach reduced gap by 35%', 'explicit novelty bonus improved identification by 40%', null if not tested)</td>
                    </tr>
                    <tr>
                        <td><strong>training_distribution_bias</strong></td>
                        <td>str</td>
                        <td>Does the study provide evidence about whether evaluation systems are biased by their training on predominantly incremental work? (e.g., 'systems trained on typical papers show 50% worse performance on outliers', 'historical data bias confirmed', null if not addressed)</td>
                    </tr>
                    <tr>
                        <td><strong>counterexamples</strong></td>
                        <td>str</td>
                        <td>Are there cases where transformational work was NOT undervalued by proxies, or where the predicted pattern does not hold? Describe these counterexamples (e.g., 'work by prestigious authors showed no gap', '15% of highly novel papers received immediate recognition', null if none reported)</td>
                    </tr>
                    <tr>
                        <td><strong>study_design</strong></td>
                        <td>str</td>
                        <td>Brief description of the study design and sample (e.g., 'retrospective analysis of 10,000 papers over 20 years', 'controlled experiment with 500 peer reviewers', 'case study of Nobel Prize papers')</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-55",
    "schema": [
        {
            "name": "proxy_metric_type",
            "type": "str",
            "description": "What proxy metric is being studied? (e.g., 'early citations', 'journal impact factor', 'peer review scores', 'author h-index', 'automated novelty detection', etc.)"
        },
        {
            "name": "ground_truth_measure",
            "type": "str",
            "description": "What ground truth measure of scientific value is used for comparison? (e.g., 'long-term citations', 'Nobel prizes', 'retrospective expert assessment', 'paradigm shift indicators', 'practical applications', etc.)"
        },
        {
            "name": "novelty_transformation_measure",
            "type": "str",
            "description": "How is novelty, transformation degree, or innovativeness measured in this study? (e.g., 'atypical combination score', 'interdisciplinarity index', 'paradigm departure score', 'incremental vs breakthrough classification', etc.)"
        },
        {
            "name": "quantitative_relationship",
            "type": "str",
            "description": "What is the quantitative relationship between novelty/transformation and the proxy-truth gap? Include specific numbers, percentages, correlations, or mathematical relationships if reported (e.g., 'highly novel papers receive 40% fewer citations in first 3 years but 200% more after 10 years', 'exponential relationship with R²=0.65', etc.)"
        },
        {
            "name": "gap_magnitude",
            "type": "str",
            "description": "What is the magnitude of divergence between proxy and ground truth for different levels of novelty? Include specific percentages or effect sizes if available (e.g., 'incremental work: 5% gap, transformational work: 75% gap', null if not reported)"
        },
        {
            "name": "temporal_pattern",
            "type": "str",
            "description": "How does the proxy-truth gap change over time? Include specific time periods and patterns (e.g., 'gap closes exponentially with half-life of 5 years', 'novel work undervalued for 3-5 years then overvalued', null if not reported)"
        },
        {
            "name": "field_studied",
            "type": "str",
            "description": "What scientific field(s) or discipline(s) were studied? (e.g., 'physics', 'biology', 'computer science', 'cross-disciplinary comparison', etc.)"
        },
        {
            "name": "field_differences",
            "type": "str",
            "description": "Are there differences between fields in how proxies relate to ground truth for novel work? Include specific comparisons if reported (e.g., 'physics shows 2x larger gap than biology', 'rigid paradigm fields show steeper relationships', null if not reported)"
        },
        {
            "name": "multiplicative_vs_additive",
            "type": "str",
            "description": "Does the study provide evidence about whether multiple proxy failures compound multiplicatively or additively? (e.g., 'violations across citation, prestige, and author metrics compound multiplicatively', 'independent additive effects', null if not addressed)"
        },
        {
            "name": "automated_system_performance",
            "type": "str",
            "description": "If automated evaluation systems are studied, how well do they identify transformational vs incremental work? Include accuracy, bias measures, or performance metrics (e.g., 'ML system achieves 65% accuracy on novel work vs 90% on incremental', null if not applicable)"
        },
        {
            "name": "correction_mechanism",
            "type": "str",
            "description": "Does the study test any mechanism to correct proxy-truth gaps? Describe the mechanism and its effectiveness (e.g., 'meta-learning approach reduced gap by 35%', 'explicit novelty bonus improved identification by 40%', null if not tested)"
        },
        {
            "name": "training_distribution_bias",
            "type": "str",
            "description": "Does the study provide evidence about whether evaluation systems are biased by their training on predominantly incremental work? (e.g., 'systems trained on typical papers show 50% worse performance on outliers', 'historical data bias confirmed', null if not addressed)"
        },
        {
            "name": "counterexamples",
            "type": "str",
            "description": "Are there cases where transformational work was NOT undervalued by proxies, or where the predicted pattern does not hold? Describe these counterexamples (e.g., 'work by prestigious authors showed no gap', '15% of highly novel papers received immediate recognition', null if none reported)"
        },
        {
            "name": "study_design",
            "type": "str",
            "description": "Brief description of the study design and sample (e.g., 'retrospective analysis of 10,000 papers over 20 years', 'controlled experiment with 500 peer reviewers', 'case study of Nobel Prize papers')"
        }
    ],
    "extraction_query": "Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
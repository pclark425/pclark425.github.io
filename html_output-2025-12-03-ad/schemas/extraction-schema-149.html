<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-149 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-149</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-149</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being studied (e.g., GPT-3, Llama-2, PaLM, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model, including architecture, size (e.g., 7B, 13B), and any relevant training details.</td>
                    </tr>
                    <tr>
                        <td><strong>arithmetic_task_type</strong></td>
                        <td>str</td>
                        <td>The type(s) of arithmetic tasks evaluated (e.g., single-digit addition, multi-digit multiplication, mixed operations, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>mechanism_or_representation</strong></td>
                        <td>str</td>
                        <td>Any identified internal mechanism, representation, or process by which the model performs arithmetic (e.g., attention patterns, neuron activations, digit-by-digit computation, memorization, algorithmic reasoning, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>probing_or_intervention_method</strong></td>
                        <td>str</td>
                        <td>Any probing or intervention method used to study the model's arithmetic (e.g., linear probes, activation patching, prompt engineering, fine-tuning, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metrics</strong></td>
                        <td>str</td>
                        <td>Performance of the model on the arithmetic tasks, ideally broken down by task type and including numerical results (e.g., accuracy, error rate, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>error_types_or_failure_modes</strong></td>
                        <td>str</td>
                        <td>Descriptions of common error types, failure modes, or limitations observed in the model's arithmetic performance.</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_for_mechanism</strong></td>
                        <td>str</td>
                        <td>Concise summary of evidence supporting a particular mechanism or process for arithmetic in the model (e.g., ablation results, attention analysis, neuron activation studies).</td>
                    </tr>
                    <tr>
                        <td><strong>counterexamples_or_challenges</strong></td>
                        <td>str</td>
                        <td>Any counterexamples, negative results, or challenges to proposed mechanisms (e.g., cases where the model fails despite the mechanism, or where the mechanism does not generalize).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-149",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being studied (e.g., GPT-3, Llama-2, PaLM, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model, including architecture, size (e.g., 7B, 13B), and any relevant training details."
        },
        {
            "name": "arithmetic_task_type",
            "type": "str",
            "description": "The type(s) of arithmetic tasks evaluated (e.g., single-digit addition, multi-digit multiplication, mixed operations, etc.)."
        },
        {
            "name": "mechanism_or_representation",
            "type": "str",
            "description": "Any identified internal mechanism, representation, or process by which the model performs arithmetic (e.g., attention patterns, neuron activations, digit-by-digit computation, memorization, algorithmic reasoning, etc.)."
        },
        {
            "name": "probing_or_intervention_method",
            "type": "str",
            "description": "Any probing or intervention method used to study the model's arithmetic (e.g., linear probes, activation patching, prompt engineering, fine-tuning, etc.)."
        },
        {
            "name": "performance_metrics",
            "type": "str",
            "description": "Performance of the model on the arithmetic tasks, ideally broken down by task type and including numerical results (e.g., accuracy, error rate, etc.)."
        },
        {
            "name": "error_types_or_failure_modes",
            "type": "str",
            "description": "Descriptions of common error types, failure modes, or limitations observed in the model's arithmetic performance."
        },
        {
            "name": "evidence_for_mechanism",
            "type": "str",
            "description": "Concise summary of evidence supporting a particular mechanism or process for arithmetic in the model (e.g., ablation results, attention analysis, neuron activation studies)."
        },
        {
            "name": "counterexamples_or_challenges",
            "type": "str",
            "description": "Any counterexamples, negative results, or challenges to proposed mechanisms (e.g., cases where the model fails despite the mechanism, or where the mechanism does not generalize)."
        }
    ],
    "extraction_query": "Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
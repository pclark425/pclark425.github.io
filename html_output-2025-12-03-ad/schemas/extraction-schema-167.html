<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-167 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-167</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-167</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM or system used for theory distillation (e.g., GPT-4, Llama 2, custom pipeline, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM or system, including any relevant architectural or training details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model, in parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>input_corpus_description</strong></td>
                        <td>str</td>
                        <td>A description of the scholarly input corpus used (e.g., number of papers, domains, sources, selection criteria).</td>
                    </tr>
                    <tr>
                        <td><strong>input_corpus_size</strong></td>
                        <td>int</td>
                        <td>The number of scholarly papers or documents used as input, or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>topic_query_description</strong></td>
                        <td>str</td>
                        <td>A description of the specific topic or query used to guide the theory distillation process.</td>
                    </tr>
                    <tr>
                        <td><strong>distillation_method</strong></td>
                        <td>str</td>
                        <td>A detailed description of the method used for theory distillation (e.g., prompting strategy, retrieval-augmented generation, chain-of-thought, summarization pipeline, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>output_type</strong></td>
                        <td>str</td>
                        <td>The type of output produced (e.g., theory summary, knowledge graph, structured report, narrative synthesis, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>output_example</strong></td>
                        <td>str</td>
                        <td>A brief example or excerpt of the output produced by the system, if available.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>How the output was evaluated (e.g., human expert review, comparison to gold-standard theories, automated metrics, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_results</strong></td>
                        <td>str</td>
                        <td>A concise summary of the evaluation results, including any quantitative or qualitative findings.</td>
                    </tr>
                    <tr>
                        <td><strong>strengths</strong></td>
                        <td>str</td>
                        <td>Any reported strengths or advantages of the approach (e.g., scalability, accuracy, novelty, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations</strong></td>
                        <td>str</td>
                        <td>Any reported limitations or weaknesses of the approach (e.g., hallucination, lack of interpretability, domain specificity, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>failure_cases</strong></td>
                        <td>str</td>
                        <td>Any reported failure cases, counter-examples, or challenges encountered in using LLMs for theory distillation.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-167",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM or system used for theory distillation (e.g., GPT-4, Llama 2, custom pipeline, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM or system, including any relevant architectural or training details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model, in parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified."
        },
        {
            "name": "input_corpus_description",
            "type": "str",
            "description": "A description of the scholarly input corpus used (e.g., number of papers, domains, sources, selection criteria)."
        },
        {
            "name": "input_corpus_size",
            "type": "int",
            "description": "The number of scholarly papers or documents used as input, or null if not specified."
        },
        {
            "name": "topic_query_description",
            "type": "str",
            "description": "A description of the specific topic or query used to guide the theory distillation process."
        },
        {
            "name": "distillation_method",
            "type": "str",
            "description": "A detailed description of the method used for theory distillation (e.g., prompting strategy, retrieval-augmented generation, chain-of-thought, summarization pipeline, etc.)."
        },
        {
            "name": "output_type",
            "type": "str",
            "description": "The type of output produced (e.g., theory summary, knowledge graph, structured report, narrative synthesis, etc.)."
        },
        {
            "name": "output_example",
            "type": "str",
            "description": "A brief example or excerpt of the output produced by the system, if available."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "How the output was evaluated (e.g., human expert review, comparison to gold-standard theories, automated metrics, etc.)."
        },
        {
            "name": "evaluation_results",
            "type": "str",
            "description": "A concise summary of the evaluation results, including any quantitative or qualitative findings."
        },
        {
            "name": "strengths",
            "type": "str",
            "description": "Any reported strengths or advantages of the approach (e.g., scalability, accuracy, novelty, etc.)."
        },
        {
            "name": "limitations",
            "type": "str",
            "description": "Any reported limitations or weaknesses of the approach (e.g., hallucination, lack of interpretability, domain specificity, etc.)."
        },
        {
            "name": "failure_cases",
            "type": "str",
            "description": "Any reported failure cases, counter-examples, or challenges encountered in using LLMs for theory distillation."
        }
    ],
    "extraction_query": "Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
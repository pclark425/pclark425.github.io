<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-4 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-4</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-4</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how large language models perform arithmetic tasks using algorithmic or step-by-step prompting, including model details, prompting methods, performance metrics, error types, generalization ability, and comparisons to other prompting strategies.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model evaluated (e.g., GPT-3, Codex, PaLM, LLaMA).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model architecture, training data, or unique features relevant to arithmetic performance.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 1B, 7B, 70B, 540B).</td>
                    </tr>
                    <tr>
                        <td><strong>arithmetic_task_type</strong></td>
                        <td>str</td>
                        <td>The specific arithmetic or math problem type evaluated (e.g., addition, subtraction, multiplication, parity, multi-step arithmetic, algorithmic tasks).</td>
                    </tr>
                    <tr>
                        <td><strong>prompting_strategy</strong></td>
                        <td>str</td>
                        <td>The prompting method used to elicit arithmetic performance (e.g., algorithmic prompting, chain-of-thought prompting, few-shot prompting, zero-shot, hybrid approaches).</td>
                    </tr>
                    <tr>
                        <td><strong>prompt_description</strong></td>
                        <td>str</td>
                        <td>A detailed description of the prompt content and structure, especially how algorithmic or step-by-step instructions are provided.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metrics</strong></td>
                        <td>str</td>
                        <td>Quantitative or qualitative performance results on arithmetic tasks, including accuracy, error rates, or other relevant metrics, ideally broken down by task type and prompting strategy.</td>
                    </tr>
                    <tr>
                        <td><strong>error_analysis</strong></td>
                        <td>str</td>
                        <td>Descriptions of common or systematic errors observed under different prompting strategies, including error types and their frequency.</td>
                    </tr>
                    <tr>
                        <td><strong>generalization_ability</strong></td>
                        <td>str</td>
                        <td>Evidence or analysis of the model's ability to generalize to out-of-distribution or longer/more complex arithmetic problems under algorithmic prompting.</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_baselines</strong></td>
                        <td>str</td>
                        <td>Comparisons of arithmetic performance between algorithmic prompting and other prompting methods (e.g., chain-of-thought, few-shot), or against external tools or symbolic solvers.</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_model_size</strong></td>
                        <td>str</td>
                        <td>Information on how model size affects arithmetic performance under algorithmic prompting, if reported.</td>
                    </tr>
                    <tr>
                        <td><strong>impact_of_training_data</strong></td>
                        <td>str</td>
                        <td>Information on how training data composition (e.g., inclusion of code, math textbooks, symbolic data) influences arithmetic performance with algorithmic prompting.</td>
                    </tr>
                    <tr>
                        <td><strong>use_of_external_tools</strong></td>
                        <td>bool</td>
                        <td>Whether the model uses external tools (e.g., calculators, symbolic math engines) during arithmetic problem solving.</td>
                    </tr>
                    <tr>
                        <td><strong>tool_description</strong></td>
                        <td>str</td>
                        <td>If external tools are used, describe the type of tool and how it is integrated.</td>
                    </tr>
                    <tr>
                        <td><strong>prompt_sensitivity</strong></td>
                        <td>str</td>
                        <td>Evidence on how sensitive the model's arithmetic performance is to prompt correctness, prompt errors, or prompt variations.</td>
                    </tr>
                    <tr>
                        <td><strong>mechanistic_insights</strong></td>
                        <td>str</td>
                        <td>Any proposed explanations or mechanistic insights into how algorithmic prompting enables the model to simulate arithmetic algorithms internally.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_and_challenges</strong></td>
                        <td>str</td>
                        <td>Reported limitations or challenges in using algorithmic prompting for arithmetic tasks, including failure cases or negative results.</td>
                    </tr>
                    <tr>
                        <td><strong>automation_of_prompting</strong></td>
                        <td>str</td>
                        <td>Any evidence or discussion on whether algorithmic prompting can be automated, learned, or generated by the model itself.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-4",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model evaluated (e.g., GPT-3, Codex, PaLM, LLaMA)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model architecture, training data, or unique features relevant to arithmetic performance."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 1B, 7B, 70B, 540B)."
        },
        {
            "name": "arithmetic_task_type",
            "type": "str",
            "description": "The specific arithmetic or math problem type evaluated (e.g., addition, subtraction, multiplication, parity, multi-step arithmetic, algorithmic tasks)."
        },
        {
            "name": "prompting_strategy",
            "type": "str",
            "description": "The prompting method used to elicit arithmetic performance (e.g., algorithmic prompting, chain-of-thought prompting, few-shot prompting, zero-shot, hybrid approaches)."
        },
        {
            "name": "prompt_description",
            "type": "str",
            "description": "A detailed description of the prompt content and structure, especially how algorithmic or step-by-step instructions are provided."
        },
        {
            "name": "performance_metrics",
            "type": "str",
            "description": "Quantitative or qualitative performance results on arithmetic tasks, including accuracy, error rates, or other relevant metrics, ideally broken down by task type and prompting strategy."
        },
        {
            "name": "error_analysis",
            "type": "str",
            "description": "Descriptions of common or systematic errors observed under different prompting strategies, including error types and their frequency."
        },
        {
            "name": "generalization_ability",
            "type": "str",
            "description": "Evidence or analysis of the model's ability to generalize to out-of-distribution or longer/more complex arithmetic problems under algorithmic prompting."
        },
        {
            "name": "comparison_to_baselines",
            "type": "str",
            "description": "Comparisons of arithmetic performance between algorithmic prompting and other prompting methods (e.g., chain-of-thought, few-shot), or against external tools or symbolic solvers."
        },
        {
            "name": "impact_of_model_size",
            "type": "str",
            "description": "Information on how model size affects arithmetic performance under algorithmic prompting, if reported."
        },
        {
            "name": "impact_of_training_data",
            "type": "str",
            "description": "Information on how training data composition (e.g., inclusion of code, math textbooks, symbolic data) influences arithmetic performance with algorithmic prompting."
        },
        {
            "name": "use_of_external_tools",
            "type": "bool",
            "description": "Whether the model uses external tools (e.g., calculators, symbolic math engines) during arithmetic problem solving."
        },
        {
            "name": "tool_description",
            "type": "str",
            "description": "If external tools are used, describe the type of tool and how it is integrated."
        },
        {
            "name": "prompt_sensitivity",
            "type": "str",
            "description": "Evidence on how sensitive the model's arithmetic performance is to prompt correctness, prompt errors, or prompt variations."
        },
        {
            "name": "mechanistic_insights",
            "type": "str",
            "description": "Any proposed explanations or mechanistic insights into how algorithmic prompting enables the model to simulate arithmetic algorithms internally."
        },
        {
            "name": "limitations_and_challenges",
            "type": "str",
            "description": "Reported limitations or challenges in using algorithmic prompting for arithmetic tasks, including failure cases or negative results."
        },
        {
            "name": "automation_of_prompting",
            "type": "str",
            "description": "Any evidence or discussion on whether algorithmic prompting can be automated, learned, or generated by the model itself."
        }
    ],
    "extraction_query": "Extract any mentions of how large language models perform arithmetic tasks using algorithmic or step-by-step prompting, including model details, prompting methods, performance metrics, error types, generalization ability, and comparisons to other prompting strategies.",
    "supporting_theory_ids": [],
    "model_str": null
}</code></pre>
        </div>
    </div>
</body>
</html>
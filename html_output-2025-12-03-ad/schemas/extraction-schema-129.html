<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-129 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-129</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-129</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name or identifier of the language model evaluated (e.g., GPT-4, PaLM 2, LLaMA-13B).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Model scale, typically expressed in parameters (e.g., 7B, 70B, 540B).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_method_name</strong></td>
                        <td>str</td>
                        <td>The specific reasoning technique or prompting style used (e.g., Chain‑of‑Thought, Tree‑of‑Thought, Self‑Consistency, Scratchpad, Decomposition, Retrieval‑Augmented Generation, Tool‑Use).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_method_type</strong></td>
                        <td>str</td>
                        <td>A high‑level categorization of the method (e.g., "sequential", "tree‑search", "ensemble", "retrieval‑based", "tool‑augmented").</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_style_diversity</strong></td>
                        <td>str</td>
                        <td>Whether the method is described as diverse or similar relative to other methods in the same study (e.g., "diverse", "homogeneous", "mixed", "single style").</td>
                    </tr>
                    <tr>
                        <td><strong>benchmark_name</strong></td>
                        <td>str</td>
                        <td>The name of the reasoning benchmark or dataset used (e.g., GSM8K, MATH, BIG‑BENCH Reasoning, ARC‑E, CommonsenseQA, LogicalDeduction).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>Brief description of the reasoning task (e.g., "grade‑school arithmetic", "symbolic logic puzzles", "commonsense inference").</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metric</strong></td>
                        <td>str</td>
                        <td>Metric reported for the task (e.g., accuracy, exact match, F1, solve rate).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_value</strong></td>
                        <td>float</td>
                        <td>Numeric value of the reported performance (include percentage if applicable, e.g., 78.5).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_target_method</strong></td>
                        <td>str</td>
                        <td>Name of the other reasoning method(s) the paper directly compares against (if any).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_difference</strong></td>
                        <td>float</td>
                        <td>Reported difference in performance between the primary method and the comparison target (positive if primary is better). Null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>statistical_significance</strong></td>
                        <td>bool</td>
                        <td>Whether the paper reports statistical significance for the performance difference (true/false/null).</td>
                    </tr>
                    <tr>
                        <td><strong>analysis_notes</strong></td>
                        <td>str</td>
                        <td>Key qualitative observations or explanations the authors provide about why diversity or similarity of reasoning styles impacts performance.</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_study_present</strong></td>
                        <td>bool</td>
                        <td>Does the paper include an ablation study isolating components of the reasoning method? (true/false/null).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-129",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name or identifier of the language model evaluated (e.g., GPT-4, PaLM 2, LLaMA-13B)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Model scale, typically expressed in parameters (e.g., 7B, 70B, 540B)."
        },
        {
            "name": "reasoning_method_name",
            "type": "str",
            "description": "The specific reasoning technique or prompting style used (e.g., Chain‑of‑Thought, Tree‑of‑Thought, Self‑Consistency, Scratchpad, Decomposition, Retrieval‑Augmented Generation, Tool‑Use)."
        },
        {
            "name": "reasoning_method_type",
            "type": "str",
            "description": "A high‑level categorization of the method (e.g., \"sequential\", \"tree‑search\", \"ensemble\", \"retrieval‑based\", \"tool‑augmented\")."
        },
        {
            "name": "reasoning_style_diversity",
            "type": "str",
            "description": "Whether the method is described as diverse or similar relative to other methods in the same study (e.g., \"diverse\", \"homogeneous\", \"mixed\", \"single style\")."
        },
        {
            "name": "benchmark_name",
            "type": "str",
            "description": "The name of the reasoning benchmark or dataset used (e.g., GSM8K, MATH, BIG‑BENCH Reasoning, ARC‑E, CommonsenseQA, LogicalDeduction)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "Brief description of the reasoning task (e.g., \"grade‑school arithmetic\", \"symbolic logic puzzles\", \"commonsense inference\")."
        },
        {
            "name": "performance_metric",
            "type": "str",
            "description": "Metric reported for the task (e.g., accuracy, exact match, F1, solve rate)."
        },
        {
            "name": "performance_value",
            "type": "float",
            "description": "Numeric value of the reported performance (include percentage if applicable, e.g., 78.5)."
        },
        {
            "name": "comparison_target_method",
            "type": "str",
            "description": "Name of the other reasoning method(s) the paper directly compares against (if any)."
        },
        {
            "name": "performance_difference",
            "type": "float",
            "description": "Reported difference in performance between the primary method and the comparison target (positive if primary is better). Null if not reported."
        },
        {
            "name": "statistical_significance",
            "type": "bool",
            "description": "Whether the paper reports statistical significance for the performance difference (true/false/null)."
        },
        {
            "name": "analysis_notes",
            "type": "str",
            "description": "Key qualitative observations or explanations the authors provide about why diversity or similarity of reasoning styles impacts performance."
        },
        {
            "name": "ablation_study_present",
            "type": "bool",
            "description": "Does the paper include an ablation study isolating components of the reasoning method? (true/false/null)."
        }
    ],
    "extraction_query": "Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-159 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-159</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-159</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM model being evaluated (e.g., GPT-4, Llama-2, PaLM, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM model, including architecture, training data, or other relevant details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model, typically in number of parameters (e.g., 7B, 13B, 70B, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>test_battery_name</strong></td>
                        <td>str</td>
                        <td>The name of the cognitive psychology test battery or individual test (e.g., Raven's Progressive Matrices, Stroop Test, Digit Span, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>test_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the cognitive test or battery, including the cognitive domain assessed (e.g., working memory, reasoning, attention, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>llm_performance</strong></td>
                        <td>str</td>
                        <td>The performance of the LLM on the test, ideally as a numerical value with units (e.g., accuracy %, score, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>human_baseline_performance</strong></td>
                        <td>str</td>
                        <td>The reported performance of normal human participants on the same test, ideally as a numerical value with units.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_comparison</strong></td>
                        <td>str</td>
                        <td>A summary of how LLM performance compares to human baseline (e.g., 'LLM matches average human', 'LLM outperforms humans', 'LLM below human baseline', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>experimental_details</strong></td>
                        <td>str</td>
                        <td>Any relevant experimental details, such as prompt engineering, context length, test adaptation, or other methodological notes.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_caveats</strong></td>
                        <td>str</td>
                        <td>Any limitations, caveats, or counter-examples noted in the paper regarding LLM performance or the validity of the comparison to humans.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-159",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM model being evaluated (e.g., GPT-4, Llama-2, PaLM, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM model, including architecture, training data, or other relevant details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model, typically in number of parameters (e.g., 7B, 13B, 70B, etc.)."
        },
        {
            "name": "test_battery_name",
            "type": "str",
            "description": "The name of the cognitive psychology test battery or individual test (e.g., Raven's Progressive Matrices, Stroop Test, Digit Span, etc.)."
        },
        {
            "name": "test_description",
            "type": "str",
            "description": "A brief description of the cognitive test or battery, including the cognitive domain assessed (e.g., working memory, reasoning, attention, etc.)."
        },
        {
            "name": "llm_performance",
            "type": "str",
            "description": "The performance of the LLM on the test, ideally as a numerical value with units (e.g., accuracy %, score, etc.)."
        },
        {
            "name": "human_baseline_performance",
            "type": "str",
            "description": "The reported performance of normal human participants on the same test, ideally as a numerical value with units."
        },
        {
            "name": "performance_comparison",
            "type": "str",
            "description": "A summary of how LLM performance compares to human baseline (e.g., 'LLM matches average human', 'LLM outperforms humans', 'LLM below human baseline', etc.)."
        },
        {
            "name": "experimental_details",
            "type": "str",
            "description": "Any relevant experimental details, such as prompt engineering, context length, test adaptation, or other methodological notes."
        },
        {
            "name": "limitations_or_caveats",
            "type": "str",
            "description": "Any limitations, caveats, or counter-examples noted in the paper regarding LLM performance or the validity of the comparison to humans."
        }
    ],
    "extraction_query": "Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-128 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-128</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-128</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>paper_title</strong></td>
                        <td>str</td>
                        <td>Title of the paper.</td>
                    </tr>
                    <tr>
                        <td><strong>agent_name</strong></td>
                        <td>str</td>
                        <td>Name or identifier of the LLM‑based agent (e.g., "ReAct", "RetroReader", "Memory‑Augmented Agent").</td>
                    </tr>
                    <tr>
                        <td><strong>agent_description</strong></td>
                        <td>str</td>
                        <td>Brief description of the agent architecture and its main components.</td>
                    </tr>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>Name of the underlying LLM (e.g., GPT‑3, LLaMA‑13B, PaLM‑2).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Scale of the model in parameters (e.g., "7B", "13B", "70B").</td>
                    </tr>
                    <tr>
                        <td><strong>benchmark_name</strong></td>
                        <td>str</td>
                        <td>Name of the text‑game benchmark or specific game used for evaluation (e.g., "TextWorld", "Jericho/Zork", "CoinCollector").</td>
                    </tr>
                    <tr>
                        <td><strong>memory_used</strong></td>
                        <td>bool</td>
                        <td>Whether the agent incorporates an explicit memory component (true/false).</td>
                    </tr>
                    <tr>
                        <td><strong>memory_type</strong></td>
                        <td>str</td>
                        <td>Type of memory employed (e.g., "external key‑value store", "episodic buffer", "recurrent hidden state", "retrieval‑augmented generation", "scratchpad", "long‑context window").</td>
                    </tr>
                    <tr>
                        <td><strong>memory_representation</strong></td>
                        <td>str</td>
                        <td>What information is stored in memory (e.g., "raw observation text", "state embeddings", "action‑observation pairs", "symbolic game state").</td>
                    </tr>
                    <tr>
                        <td><strong>memory_update_mechanism</strong></td>
                        <td>str</td>
                        <td>How the memory is updated (e.g., "learned write network", "FIFO queue", "attention‑based overwrite", "rule‑based summarization").</td>
                    </tr>
                    <tr>
                        <td><strong>memory_retrieval_method</strong></td>
                        <td>str</td>
                        <td>Method used to retrieve information from memory (e.g., "nearest‑neighbor search", "learned attention", "TF‑IDF similarity", "prompt‑based retrieval").</td>
                    </tr>
                    <tr>
                        <td><strong>training_method</strong></td>
                        <td>str</td>
                        <td>Learning paradigm used for the agent (e.g., "reinforcement learning", "imitation learning", "supervised fine‑tuning", "few‑shot prompting").</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metric</strong></td>
                        <td>str</td>
                        <td>Metric(s) reported for game performance (e.g., "final score", "success rate", "steps to solve", "normalized reward").</td>
                    </tr>
                    <tr>
                        <td><strong>performance_with_memory</strong></td>
                        <td>str</td>
                        <td>Reported performance of the agent when the memory component is enabled (include numeric value and metric).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_without_memory</strong></td>
                        <td>str</td>
                        <td>Reported performance of the same agent without the memory component (if available).</td>
                    </tr>
                    <tr>
                        <td><strong>has_comparative_results</strong></td>
                        <td>bool</td>
                        <td>Does the paper provide a direct comparison between memory‑enabled and memory‑disabled versions of the agent? (true/false/null).</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_findings</strong></td>
                        <td>str</td>
                        <td>Key findings from any ablation studies on memory size, horizon, retrieval strategy, or representation.</td>
                    </tr>
                    <tr>
                        <td><strong>reported_limitations</strong></td>
                        <td>str</td>
                        <td>Any challenges, drawbacks, or failure modes of the memory approach mentioned by the authors (e.g., "context window overflow", "irrelevant retrieval", "catastrophic forgetting").</td>
                    </tr>
                    <tr>
                        <td><strong>best_practices_recommendations</strong></td>
                        <td>str</td>
                        <td>Explicit recommendations or design guidelines the authors propose for using memory in text‑game agents.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-128",
    "schema": [
        {
            "name": "paper_title",
            "type": "str",
            "description": "Title of the paper."
        },
        {
            "name": "agent_name",
            "type": "str",
            "description": "Name or identifier of the LLM‑based agent (e.g., \"ReAct\", \"RetroReader\", \"Memory‑Augmented Agent\")."
        },
        {
            "name": "agent_description",
            "type": "str",
            "description": "Brief description of the agent architecture and its main components."
        },
        {
            "name": "model_name",
            "type": "str",
            "description": "Name of the underlying LLM (e.g., GPT‑3, LLaMA‑13B, PaLM‑2)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Scale of the model in parameters (e.g., \"7B\", \"13B\", \"70B\")."
        },
        {
            "name": "benchmark_name",
            "type": "str",
            "description": "Name of the text‑game benchmark or specific game used for evaluation (e.g., \"TextWorld\", \"Jericho/Zork\", \"CoinCollector\")."
        },
        {
            "name": "memory_used",
            "type": "bool",
            "description": "Whether the agent incorporates an explicit memory component (true/false)."
        },
        {
            "name": "memory_type",
            "type": "str",
            "description": "Type of memory employed (e.g., \"external key‑value store\", \"episodic buffer\", \"recurrent hidden state\", \"retrieval‑augmented generation\", \"scratchpad\", \"long‑context window\")."
        },
        {
            "name": "memory_representation",
            "type": "str",
            "description": "What information is stored in memory (e.g., \"raw observation text\", \"state embeddings\", \"action‑observation pairs\", \"symbolic game state\")."
        },
        {
            "name": "memory_update_mechanism",
            "type": "str",
            "description": "How the memory is updated (e.g., \"learned write network\", \"FIFO queue\", \"attention‑based overwrite\", \"rule‑based summarization\")."
        },
        {
            "name": "memory_retrieval_method",
            "type": "str",
            "description": "Method used to retrieve information from memory (e.g., \"nearest‑neighbor search\", \"learned attention\", \"TF‑IDF similarity\", \"prompt‑based retrieval\")."
        },
        {
            "name": "training_method",
            "type": "str",
            "description": "Learning paradigm used for the agent (e.g., \"reinforcement learning\", \"imitation learning\", \"supervised fine‑tuning\", \"few‑shot prompting\")."
        },
        {
            "name": "evaluation_metric",
            "type": "str",
            "description": "Metric(s) reported for game performance (e.g., \"final score\", \"success rate\", \"steps to solve\", \"normalized reward\")."
        },
        {
            "name": "performance_with_memory",
            "type": "str",
            "description": "Reported performance of the agent when the memory component is enabled (include numeric value and metric)."
        },
        {
            "name": "performance_without_memory",
            "type": "str",
            "description": "Reported performance of the same agent without the memory component (if available)."
        },
        {
            "name": "has_comparative_results",
            "type": "bool",
            "description": "Does the paper provide a direct comparison between memory‑enabled and memory‑disabled versions of the agent? (true/false/null)."
        },
        {
            "name": "ablation_findings",
            "type": "str",
            "description": "Key findings from any ablation studies on memory size, horizon, retrieval strategy, or representation."
        },
        {
            "name": "reported_limitations",
            "type": "str",
            "description": "Any challenges, drawbacks, or failure modes of the memory approach mentioned by the authors (e.g., \"context window overflow\", \"irrelevant retrieval\", \"catastrophic forgetting\")."
        },
        {
            "name": "best_practices_recommendations",
            "type": "str",
            "description": "Explicit recommendations or design guidelines the authors propose for using memory in text‑game agents."
        }
    ],
    "extraction_query": "Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
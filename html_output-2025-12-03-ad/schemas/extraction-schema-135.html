<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-135 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-135</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-135</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>representation_name</strong></td>
                        <td>str</td>
                        <td>The name or label of the graph-to-text representation (e.g., "DFS linearization", "edge list", "JSON‑LD", "RDF triple sequence").</td>
                    </tr>
                    <tr>
                        <td><strong>representation_description</strong></td>
                        <td>str</td>
                        <td>A concise description of how the representation encodes graph structure into text, including any special tokens or formatting.</td>
                    </tr>
                    <tr>
                        <td><strong>representation_type</strong></td>
                        <td>str</td>
                        <td>Classification of the representation: "lossless" vs "lossy", "sequential", "hierarchical", "token‑based", etc.</td>
                    </tr>
                    <tr>
                        <td><strong>encoding_method</strong></td>
                        <td>str</td>
                        <td>The algorithm or traversal used to produce the text (e.g., depth‑first search, breadth‑first search, edge‑list ordering, path‑based encoding, attribute‑first serialization).</td>
                    </tr>
                    <tr>
                        <td><strong>canonicalization</strong></td>
                        <td>bool</td>
                        <td>Whether the representation includes a canonical ordering of nodes/edges to ensure deterministic output (true/false/null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>average_token_length</strong></td>
                        <td>int</td>
                        <td>Typical number of tokens produced per graph instance for this representation (null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>dataset_name</strong></td>
                        <td>str</td>
                        <td>Name of the dataset on which the representation was evaluated (e.g., "WebNLG", "AMR", "OGB", "ConceptNet").</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The specific task associated with the representation (e.g., "graph‑to‑text generation", "knowledge‑graph verbalization", "pre‑training data generation").</td>
                    </tr>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>Name of the language model or encoder‑decoder architecture used with this representation (e.g., "BART", "T5", "GPT‑3", "Graph‑BERT").</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>Brief description of the model (size, architecture highlights) relevant to the study.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metric</strong></td>
                        <td>str</td>
                        <td>Metric(s) reported to evaluate the representation (e.g., BLEU, ROUGE, METEOR, CIDEr, graph reconstruction accuracy, downstream task F1).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_value</strong></td>
                        <td>str</td>
                        <td>Reported numeric result(s) for the metric(s), including units or percentages (null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>impact_on_training</strong></td>
                        <td>str</td>
                        <td>Observed effect of using this representation on language model training (e.g., faster convergence, higher downstream accuracy, reduced over‑fitting).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations</strong></td>
                        <td>str</td>
                        <td>Any drawbacks or challenges noted for the representation (e.g., high token cost, loss of structural information, difficulty scaling).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_with_other</strong></td>
                        <td>str</td>
                        <td>Statements comparing this representation to alternative approaches, highlighting relative strengths or weaknesses.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-135",
    "schema": [
        {
            "name": "representation_name",
            "type": "str",
            "description": "The name or label of the graph-to-text representation (e.g., \"DFS linearization\", \"edge list\", \"JSON‑LD\", \"RDF triple sequence\")."
        },
        {
            "name": "representation_description",
            "type": "str",
            "description": "A concise description of how the representation encodes graph structure into text, including any special tokens or formatting."
        },
        {
            "name": "representation_type",
            "type": "str",
            "description": "Classification of the representation: \"lossless\" vs \"lossy\", \"sequential\", \"hierarchical\", \"token‑based\", etc."
        },
        {
            "name": "encoding_method",
            "type": "str",
            "description": "The algorithm or traversal used to produce the text (e.g., depth‑first search, breadth‑first search, edge‑list ordering, path‑based encoding, attribute‑first serialization)."
        },
        {
            "name": "canonicalization",
            "type": "bool",
            "description": "Whether the representation includes a canonical ordering of nodes/edges to ensure deterministic output (true/false/null if not reported)."
        },
        {
            "name": "average_token_length",
            "type": "int",
            "description": "Typical number of tokens produced per graph instance for this representation (null if not reported)."
        },
        {
            "name": "dataset_name",
            "type": "str",
            "description": "Name of the dataset on which the representation was evaluated (e.g., \"WebNLG\", \"AMR\", \"OGB\", \"ConceptNet\")."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The specific task associated with the representation (e.g., \"graph‑to‑text generation\", \"knowledge‑graph verbalization\", \"pre‑training data generation\")."
        },
        {
            "name": "model_name",
            "type": "str",
            "description": "Name of the language model or encoder‑decoder architecture used with this representation (e.g., \"BART\", \"T5\", \"GPT‑3\", \"Graph‑BERT\")."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "Brief description of the model (size, architecture highlights) relevant to the study."
        },
        {
            "name": "performance_metric",
            "type": "str",
            "description": "Metric(s) reported to evaluate the representation (e.g., BLEU, ROUGE, METEOR, CIDEr, graph reconstruction accuracy, downstream task F1)."
        },
        {
            "name": "performance_value",
            "type": "str",
            "description": "Reported numeric result(s) for the metric(s), including units or percentages (null if not reported)."
        },
        {
            "name": "impact_on_training",
            "type": "str",
            "description": "Observed effect of using this representation on language model training (e.g., faster convergence, higher downstream accuracy, reduced over‑fitting)."
        },
        {
            "name": "limitations",
            "type": "str",
            "description": "Any drawbacks or challenges noted for the representation (e.g., high token cost, loss of structural information, difficulty scaling)."
        },
        {
            "name": "comparison_with_other",
            "type": "str",
            "description": "Statements comparing this representation to alternative approaches, highlighting relative strengths or weaknesses."
        }
    ],
    "extraction_query": "Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
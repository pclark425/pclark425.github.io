<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-122 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-122</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-122</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>task_domain</strong></td>
                        <td>str</td>
                        <td>The specific task or domain where LLM-as-a-judge and human evaluations are compared (e.g., summarization, question answering, dialogue, code generation, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>llm_judge_model</strong></td>
                        <td>str</td>
                        <td>The name and version of the LLM used as a judge (e.g., GPT-4, Claude 2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>human_evaluation_setup</strong></td>
                        <td>str</td>
                        <td>A brief description of how human evaluation was conducted (e.g., number of annotators, expertise, instructions, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>metrics_compared</strong></td>
                        <td>str</td>
                        <td>The evaluation metrics or criteria used to compare LLM and human judgments (e.g., agreement rate, accuracy, consistency, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>reported_differences</strong></td>
                        <td>str</td>
                        <td>A concise summary of the main differences found between LLM-as-a-judge and human evaluations, both quantitative and qualitative.</td>
                    </tr>
                    <tr>
                        <td><strong>llm_specific_limitations</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, weaknesses, or types of errors unique to LLM-as-a-judge evaluations (e.g., missing nuance, bias, lack of world knowledge, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>notable_failure_cases</strong></td>
                        <td>str</td>
                        <td>Descriptions of specific failure cases or counterexamples where LLM-as-a-judge diverged from human judgment in a significant way.</td>
                    </tr>
                    <tr>
                        <td><strong>mitigation_strategies</strong></td>
                        <td>str</td>
                        <td>Any strategies, methods, or recommendations proposed to mitigate the losses or limitations of LLM-as-a-judge evaluations.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-122",
    "schema": [
        {
            "name": "task_domain",
            "type": "str",
            "description": "The specific task or domain where LLM-as-a-judge and human evaluations are compared (e.g., summarization, question answering, dialogue, code generation, etc.)."
        },
        {
            "name": "llm_judge_model",
            "type": "str",
            "description": "The name and version of the LLM used as a judge (e.g., GPT-4, Claude 2, etc.)."
        },
        {
            "name": "human_evaluation_setup",
            "type": "str",
            "description": "A brief description of how human evaluation was conducted (e.g., number of annotators, expertise, instructions, etc.)."
        },
        {
            "name": "metrics_compared",
            "type": "str",
            "description": "The evaluation metrics or criteria used to compare LLM and human judgments (e.g., agreement rate, accuracy, consistency, etc.)."
        },
        {
            "name": "reported_differences",
            "type": "str",
            "description": "A concise summary of the main differences found between LLM-as-a-judge and human evaluations, both quantitative and qualitative."
        },
        {
            "name": "llm_specific_limitations",
            "type": "str",
            "description": "Any reported limitations, weaknesses, or types of errors unique to LLM-as-a-judge evaluations (e.g., missing nuance, bias, lack of world knowledge, etc.)."
        },
        {
            "name": "notable_failure_cases",
            "type": "str",
            "description": "Descriptions of specific failure cases or counterexamples where LLM-as-a-judge diverged from human judgment in a significant way."
        },
        {
            "name": "mitigation_strategies",
            "type": "str",
            "description": "Any strategies, methods, or recommendations proposed to mitigate the losses or limitations of LLM-as-a-judge evaluations."
        }
    ],
    "extraction_query": "Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
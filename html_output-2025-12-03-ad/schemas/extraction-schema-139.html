<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-139 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-139</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-139</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model used (e.g., GPT-3, PaLM, LLaMA, T5).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model architecture or variant (e.g., decoder-only, encoder-decoder, instruction-tuned).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Size of the model in parameters (e.g., 6B, 13B, 175B).</td>
                    </tr>
                    <tr>
                        <td><strong>anomaly_detection_approach</strong></td>
                        <td>str</td>
                        <td>The overall approach used (e.g., zero‑shot prompting, few‑shot prompting, fine‑tuning, embedding similarity, score‑based, generative reconstruction).</td>
                    </tr>
                    <tr>
                        <td><strong>prompt_template</strong></td>
                        <td>str</td>
                        <td>If prompting is used, the exact prompt or a concise description of the template (e.g., "Identify any outliers in the following list: ...").</td>
                    </tr>
                    <tr>
                        <td><strong>training_data</strong></td>
                        <td>str</td>
                        <td>Details of any fine‑tuning or supervised data used (e.g., synthetic anomalies, labeled tabular datasets). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>data_type</strong></td>
                        <td>str</td>
                        <td>The type of list data evaluated (e.g., numeric vector, categorical list, time‑series, log entries, code tokens).</td>
                    </tr>
                    <tr>
                        <td><strong>dataset_name</strong></td>
                        <td>str</td>
                        <td>Name of the dataset or benchmark used for evaluation (e.g., KDDCup99, NAB, UCI Anomaly, synthetic list dataset).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metric</strong></td>
                        <td>str</td>
                        <td>Metric(s) reported for anomaly detection performance (e.g., AUROC, AUPRC, F1, precision@k).</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>Reported performance numbers for the model on the given metric(s), including any confidence intervals if provided.</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_comparison</strong></td>
                        <td>str</td>
                        <td>Performance of baseline methods reported in the same study (e.g., Isolation Forest, One‑Class SVM, autoencoder).</td>
                    </tr>
                    <tr>
                        <td><strong>zero_shot_or_few_shot</strong></td>
                        <td>str</td>
                        <td>Indicates whether the method is zero‑shot, few‑shot (specify number of examples), or fully supervised.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_cases</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure modes, or cases where the LLM performed poorly (e.g., high false‑positive rate on certain data types).</td>
                    </tr>
                    <tr>
                        <td><strong>computational_cost</strong></td>
                        <td>str</td>
                        <td>Information about inference cost, latency, or resource usage (e.g., GPU hours, tokens per example). Null if not reported.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-139",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model used (e.g., GPT-3, PaLM, LLaMA, T5)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model architecture or variant (e.g., decoder-only, encoder-decoder, instruction-tuned)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Size of the model in parameters (e.g., 6B, 13B, 175B)."
        },
        {
            "name": "anomaly_detection_approach",
            "type": "str",
            "description": "The overall approach used (e.g., zero‑shot prompting, few‑shot prompting, fine‑tuning, embedding similarity, score‑based, generative reconstruction)."
        },
        {
            "name": "prompt_template",
            "type": "str",
            "description": "If prompting is used, the exact prompt or a concise description of the template (e.g., \"Identify any outliers in the following list: ...\")."
        },
        {
            "name": "training_data",
            "type": "str",
            "description": "Details of any fine‑tuning or supervised data used (e.g., synthetic anomalies, labeled tabular datasets). Null if not applicable."
        },
        {
            "name": "data_type",
            "type": "str",
            "description": "The type of list data evaluated (e.g., numeric vector, categorical list, time‑series, log entries, code tokens)."
        },
        {
            "name": "dataset_name",
            "type": "str",
            "description": "Name of the dataset or benchmark used for evaluation (e.g., KDDCup99, NAB, UCI Anomaly, synthetic list dataset)."
        },
        {
            "name": "evaluation_metric",
            "type": "str",
            "description": "Metric(s) reported for anomaly detection performance (e.g., AUROC, AUPRC, F1, precision@k)."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "Reported performance numbers for the model on the given metric(s), including any confidence intervals if provided."
        },
        {
            "name": "baseline_comparison",
            "type": "str",
            "description": "Performance of baseline methods reported in the same study (e.g., Isolation Forest, One‑Class SVM, autoencoder)."
        },
        {
            "name": "zero_shot_or_few_shot",
            "type": "str",
            "description": "Indicates whether the method is zero‑shot, few‑shot (specify number of examples), or fully supervised."
        },
        {
            "name": "limitations_or_failure_cases",
            "type": "str",
            "description": "Any reported limitations, failure modes, or cases where the LLM performed poorly (e.g., high false‑positive rate on certain data types)."
        },
        {
            "name": "computational_cost",
            "type": "str",
            "description": "Information about inference cost, latency, or resource usage (e.g., GPU hours, tokens per example). Null if not reported."
        }
    ],
    "extraction_query": "Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
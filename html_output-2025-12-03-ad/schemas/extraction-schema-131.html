<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-131 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-131</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-131</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model evaluated (e.g., GPT-4, PaLM, LLaMA).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model, including architecture family and any special components.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Size of the model in parameters (e.g., 7B, 70B, 540B).</td>
                    </tr>
                    <tr>
                        <td><strong>architecture_type</strong></td>
                        <td>str</td>
                        <td>Specific architecture or augmentation used for reasoning (e.g., transformer, neuro‑symbolic, transformer+SAT solver).</td>
                    </tr>
                    <tr>
                        <td><strong>training_data</strong></td>
                        <td>str</td>
                        <td>Key datasets or fine‑tuning data used to improve logical reasoning (e.g., ProofWriter, LogicalEntailment, formal theorem corpora).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_method</strong></td>
                        <td>str</td>
                        <td>The technique employed to achieve logical reasoning (e.g., chain‑of‑thought prompting, self‑consistency, program synthesis, external tool integration, formal proof generation).</td>
                    </tr>
                    <tr>
                        <td><strong>external_tool_used</strong></td>
                        <td>bool</td>
                        <td>Whether the approach incorporates an external reasoning tool such as a theorem prover or SAT solver (true/false/null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>external_tool_description</strong></td>
                        <td>str</td>
                        <td>If an external tool is used, a brief description of the tool and how it is integrated.</td>
                    </tr>
                    <tr>
                        <td><strong>benchmark_name</strong></td>
                        <td>str</td>
                        <td>Name of the benchmark or dataset used to evaluate strict logical reasoning (e.g., ProofWriter, LogicalEntailment, MATH, MiniF2F).</td>
                    </tr>
                    <tr>
                        <td><strong>benchmark_description</strong></td>
                        <td>str</td>
                        <td>Short description of the benchmark, including the type of logical tasks it contains.</td>
                    </tr>
                    <tr>
                        <td><strong>task_type</strong></td>
                        <td>str</td>
                        <td>Specific logical task evaluated (e.g., propositional entailment, first‑order theorem proving, proof generation, logical deduction).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metric</strong></td>
                        <td>str</td>
                        <td>Metric reported for the task (e.g., accuracy, exact proof match, pass@k, logical entailment score).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_value</strong></td>
                        <td>str</td>
                        <td>Numeric result for the metric, including units or percentages (e.g., 78.5% accuracy).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_with_baseline</strong></td>
                        <td>str</td>
                        <td>Reported performance difference when the reasoning method is added/removed or compared to a baseline model (e.g., +12% over vanilla model).</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings</strong></td>
                        <td>str</td>
                        <td>Concise summary of the main conclusion regarding logical reasoning performance (e.g., "Chain‑of‑thought improves proof correctness by 15% when combined with self‑consistency").</td>
                    </tr>
                    <tr>
                        <td><strong>limitations</strong></td>
                        <td>str</td>
                        <td>Any noted limitations or failure modes of the approach (e.g., struggles with deeper quantifier reasoning, high computational cost).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-131",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model evaluated (e.g., GPT-4, PaLM, LLaMA)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model, including architecture family and any special components."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Size of the model in parameters (e.g., 7B, 70B, 540B)."
        },
        {
            "name": "architecture_type",
            "type": "str",
            "description": "Specific architecture or augmentation used for reasoning (e.g., transformer, neuro‑symbolic, transformer+SAT solver)."
        },
        {
            "name": "training_data",
            "type": "str",
            "description": "Key datasets or fine‑tuning data used to improve logical reasoning (e.g., ProofWriter, LogicalEntailment, formal theorem corpora)."
        },
        {
            "name": "reasoning_method",
            "type": "str",
            "description": "The technique employed to achieve logical reasoning (e.g., chain‑of‑thought prompting, self‑consistency, program synthesis, external tool integration, formal proof generation)."
        },
        {
            "name": "external_tool_used",
            "type": "bool",
            "description": "Whether the approach incorporates an external reasoning tool such as a theorem prover or SAT solver (true/false/null if not reported)."
        },
        {
            "name": "external_tool_description",
            "type": "str",
            "description": "If an external tool is used, a brief description of the tool and how it is integrated."
        },
        {
            "name": "benchmark_name",
            "type": "str",
            "description": "Name of the benchmark or dataset used to evaluate strict logical reasoning (e.g., ProofWriter, LogicalEntailment, MATH, MiniF2F)."
        },
        {
            "name": "benchmark_description",
            "type": "str",
            "description": "Short description of the benchmark, including the type of logical tasks it contains."
        },
        {
            "name": "task_type",
            "type": "str",
            "description": "Specific logical task evaluated (e.g., propositional entailment, first‑order theorem proving, proof generation, logical deduction)."
        },
        {
            "name": "performance_metric",
            "type": "str",
            "description": "Metric reported for the task (e.g., accuracy, exact proof match, pass@k, logical entailment score)."
        },
        {
            "name": "performance_value",
            "type": "str",
            "description": "Numeric result for the metric, including units or percentages (e.g., 78.5% accuracy)."
        },
        {
            "name": "comparison_with_baseline",
            "type": "str",
            "description": "Reported performance difference when the reasoning method is added/removed or compared to a baseline model (e.g., +12% over vanilla model)."
        },
        {
            "name": "key_findings",
            "type": "str",
            "description": "Concise summary of the main conclusion regarding logical reasoning performance (e.g., \"Chain‑of‑thought improves proof correctness by 15% when combined with self‑consistency\")."
        },
        {
            "name": "limitations",
            "type": "str",
            "description": "Any noted limitations or failure modes of the approach (e.g., struggles with deeper quantifier reasoning, high computational cost)."
        }
    ],
    "extraction_query": "Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-134 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-134</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-134</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model evaluated (e.g., GPT‑4, LLaMA‑2, PaLM‑2).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model architecture or training regime.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Model scale in parameters (e.g., 7B, 70B, 540B).</td>
                    </tr>
                    <tr>
                        <td><strong>reflection_method_name</strong></td>
                        <td>str</td>
                        <td>The name of the self‑reflection technique used (e.g., Self‑Consistency, Self‑Refine, Self‑Critique, Self‑Feedback, ReAct).</td>
                    </tr>
                    <tr>
                        <td><strong>reflection_method_description</strong></td>
                        <td>str</td>
                        <td>A concise description of how the method works (e.g., generate answer, generate critique, revise answer).</td>
                    </tr>
                    <tr>
                        <td><strong>iteration_type</strong></td>
                        <td>str</td>
                        <td>The type of iterative process (e.g., generate‑then‑reflect, voting over multiple samples, recursive self‑critique).</td>
                    </tr>
                    <tr>
                        <td><strong>num_iterations</strong></td>
                        <td>int</td>
                        <td>Number of generate‑then‑reflect cycles reported (null if not specified).</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The benchmark or task on which the model was evaluated (e.g., GSM8K, MATH, TruthfulQA, HotpotQA).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the task (e.g., grade‑school math reasoning, factual QA).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metric</strong></td>
                        <td>str</td>
                        <td>Metric used to assess performance (e.g., accuracy, F1, EM, BLEU, ROUGE).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_before_reflection</strong></td>
                        <td>str</td>
                        <td>Reported performance of the model without any self‑reflection step (include numeric value and metric).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_after_reflection</strong></td>
                        <td>str</td>
                        <td>Reported performance after applying the self‑reflection/iterative process (include numeric value and metric).</td>
                    </tr>
                    <tr>
                        <td><strong>improvement_observed</strong></td>
                        <td>bool</td>
                        <td>Does the paper report an improvement after reflection? (true, false, or null if not stated).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_cases</strong></td>
                        <td>str</td>
                        <td>Any noted limitations, failure modes, or cases where reflection did not help or hurt performance.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-134",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model evaluated (e.g., GPT‑4, LLaMA‑2, PaLM‑2)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model architecture or training regime."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Model scale in parameters (e.g., 7B, 70B, 540B)."
        },
        {
            "name": "reflection_method_name",
            "type": "str",
            "description": "The name of the self‑reflection technique used (e.g., Self‑Consistency, Self‑Refine, Self‑Critique, Self‑Feedback, ReAct)."
        },
        {
            "name": "reflection_method_description",
            "type": "str",
            "description": "A concise description of how the method works (e.g., generate answer, generate critique, revise answer)."
        },
        {
            "name": "iteration_type",
            "type": "str",
            "description": "The type of iterative process (e.g., generate‑then‑reflect, voting over multiple samples, recursive self‑critique)."
        },
        {
            "name": "num_iterations",
            "type": "int",
            "description": "Number of generate‑then‑reflect cycles reported (null if not specified)."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The benchmark or task on which the model was evaluated (e.g., GSM8K, MATH, TruthfulQA, HotpotQA)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the task (e.g., grade‑school math reasoning, factual QA)."
        },
        {
            "name": "evaluation_metric",
            "type": "str",
            "description": "Metric used to assess performance (e.g., accuracy, F1, EM, BLEU, ROUGE)."
        },
        {
            "name": "performance_before_reflection",
            "type": "str",
            "description": "Reported performance of the model without any self‑reflection step (include numeric value and metric)."
        },
        {
            "name": "performance_after_reflection",
            "type": "str",
            "description": "Reported performance after applying the self‑reflection/iterative process (include numeric value and metric)."
        },
        {
            "name": "improvement_observed",
            "type": "bool",
            "description": "Does the paper report an improvement after reflection? (true, false, or null if not stated)."
        },
        {
            "name": "limitations_or_failure_cases",
            "type": "str",
            "description": "Any noted limitations, failure modes, or cases where reflection did not help or hurt performance."
        }
    ],
    "extraction_query": "Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-38 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-38</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-38</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>study_type</strong></td>
                        <td>str</td>
                        <td>What type of study is this? (e.g., 'empirical analysis of citation patterns', 'peer review experiment', 'historical case study', 'automated system evaluation', 'meta-analysis', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_metrics_studied</strong></td>
                        <td>str</td>
                        <td>What proxy metrics or evaluation methods were studied? (e.g., 'early citations', 'journal impact factor', 'peer review scores', 'author h-index', 'automated ML-based assessment', etc.). List all that apply.</td>
                    </tr>
                    <tr>
                        <td><strong>ground_truth_measure</strong></td>
                        <td>str</td>
                        <td>What was used as the ground truth or long-term impact measure? (e.g., 'citations after 10 years', 'Nobel Prize', 'field-changing impact', 'practical applications', etc.). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>discovery_type_classification</strong></td>
                        <td>str</td>
                        <td>How were discoveries classified in terms of novelty/transformation? (e.g., 'incremental vs. transformational', 'conventional vs. novel', 'low/medium/high novelty score', etc.). Include the specific classification scheme used.</td>
                    </tr>
                    <tr>
                        <td><strong>sample_characteristics</strong></td>
                        <td>str</td>
                        <td>What papers/discoveries were studied? Include field(s), time period, sample size, and any other relevant characteristics.</td>
                    </tr>
                    <tr>
                        <td><strong>key_quantitative_findings</strong></td>
                        <td>str</td>
                        <td>What are the key quantitative results? Be specific with numbers, percentages, correlations, or statistical relationships. (e.g., 'highly novel papers received 40% fewer citations in first 3 years but 200% more after 10 years', 'correlation between novelty score and initial peer review rating was r=-0.45', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>proxy_truth_gap_magnitude</strong></td>
                        <td>str</td>
                        <td>If the study quantifies a gap between proxy metrics and ground truth, what is the magnitude? Specify for different levels of novelty/transformation if available. Null if not quantified.</td>
                    </tr>
                    <tr>
                        <td><strong>temporal_pattern</strong></td>
                        <td>str</td>
                        <td>Does the study describe how the evaluation/recognition changes over time? If so, describe the temporal pattern (e.g., 'delayed recognition with crossover at 5 years', 'exponential growth after initial lag', etc.). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>field_specific_findings</strong></td>
                        <td>str</td>
                        <td>Are there differences across scientific fields or disciplines? If so, describe them (e.g., 'physics shows stronger bias against novelty than biology', 'paradigm rigidity higher in mathematics', etc.). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>relationship_shape</strong></td>
                        <td>str</td>
                        <td>What is the shape of the relationship between novelty/transformation and the proxy-truth gap? (e.g., 'linear', 'exponential', 'U-shaped', 'threshold effect', etc.). Include any mathematical relationships if specified. Null if not characterized.</td>
                    </tr>
                    <tr>
                        <td><strong>automated_system_performance</strong></td>
                        <td>str</td>
                        <td>If the study involves automated/AI evaluation systems, how do they perform on novel vs. conventional work? Include specific performance metrics. Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>mechanism_identified</strong></td>
                        <td>str</td>
                        <td>Does the study identify specific mechanisms for why proxy metrics fail or succeed? (e.g., 'reviewers penalize unfamiliar methods', 'citation networks take time to form across disciplines', 'training data bias in ML systems', etc.). Null if not discussed.</td>
                    </tr>
                    <tr>
                        <td><strong>correction_approaches</strong></td>
                        <td>str</td>
                        <td>Does the study propose or test any approaches to correct proxy-truth gaps? If so, describe them and their effectiveness. Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>counterexamples_or_exceptions</strong></td>
                        <td>str</td>
                        <td>Does the study identify cases where transformational work was quickly recognized, or where proxy metrics worked well for novel work? Describe these exceptions. Null if none identified.</td>
                    </tr>
                    <tr>
                        <td><strong>supports_or_challenges_theory</strong></td>
                        <td>str</td>
                        <td>Based on the findings, does this evidence support or challenge the Proxy-to-Ground-Truth Gap Theory? Explain briefly how it relates to the theory's predictions (exponential gap, systematic undervaluation, field differences, etc.).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-38",
    "schema": [
        {
            "name": "study_type",
            "type": "str",
            "description": "What type of study is this? (e.g., 'empirical analysis of citation patterns', 'peer review experiment', 'historical case study', 'automated system evaluation', 'meta-analysis', etc.)"
        },
        {
            "name": "proxy_metrics_studied",
            "type": "str",
            "description": "What proxy metrics or evaluation methods were studied? (e.g., 'early citations', 'journal impact factor', 'peer review scores', 'author h-index', 'automated ML-based assessment', etc.). List all that apply."
        },
        {
            "name": "ground_truth_measure",
            "type": "str",
            "description": "What was used as the ground truth or long-term impact measure? (e.g., 'citations after 10 years', 'Nobel Prize', 'field-changing impact', 'practical applications', etc.). Null if not applicable."
        },
        {
            "name": "discovery_type_classification",
            "type": "str",
            "description": "How were discoveries classified in terms of novelty/transformation? (e.g., 'incremental vs. transformational', 'conventional vs. novel', 'low/medium/high novelty score', etc.). Include the specific classification scheme used."
        },
        {
            "name": "sample_characteristics",
            "type": "str",
            "description": "What papers/discoveries were studied? Include field(s), time period, sample size, and any other relevant characteristics."
        },
        {
            "name": "key_quantitative_findings",
            "type": "str",
            "description": "What are the key quantitative results? Be specific with numbers, percentages, correlations, or statistical relationships. (e.g., 'highly novel papers received 40% fewer citations in first 3 years but 200% more after 10 years', 'correlation between novelty score and initial peer review rating was r=-0.45', etc.)"
        },
        {
            "name": "proxy_truth_gap_magnitude",
            "type": "str",
            "description": "If the study quantifies a gap between proxy metrics and ground truth, what is the magnitude? Specify for different levels of novelty/transformation if available. Null if not quantified."
        },
        {
            "name": "temporal_pattern",
            "type": "str",
            "description": "Does the study describe how the evaluation/recognition changes over time? If so, describe the temporal pattern (e.g., 'delayed recognition with crossover at 5 years', 'exponential growth after initial lag', etc.). Null if not applicable."
        },
        {
            "name": "field_specific_findings",
            "type": "str",
            "description": "Are there differences across scientific fields or disciplines? If so, describe them (e.g., 'physics shows stronger bias against novelty than biology', 'paradigm rigidity higher in mathematics', etc.). Null if not applicable."
        },
        {
            "name": "relationship_shape",
            "type": "str",
            "description": "What is the shape of the relationship between novelty/transformation and the proxy-truth gap? (e.g., 'linear', 'exponential', 'U-shaped', 'threshold effect', etc.). Include any mathematical relationships if specified. Null if not characterized."
        },
        {
            "name": "automated_system_performance",
            "type": "str",
            "description": "If the study involves automated/AI evaluation systems, how do they perform on novel vs. conventional work? Include specific performance metrics. Null if not applicable."
        },
        {
            "name": "mechanism_identified",
            "type": "str",
            "description": "Does the study identify specific mechanisms for why proxy metrics fail or succeed? (e.g., 'reviewers penalize unfamiliar methods', 'citation networks take time to form across disciplines', 'training data bias in ML systems', etc.). Null if not discussed."
        },
        {
            "name": "correction_approaches",
            "type": "str",
            "description": "Does the study propose or test any approaches to correct proxy-truth gaps? If so, describe them and their effectiveness. Null if not applicable."
        },
        {
            "name": "counterexamples_or_exceptions",
            "type": "str",
            "description": "Does the study identify cases where transformational work was quickly recognized, or where proxy metrics worked well for novel work? Describe these exceptions. Null if none identified."
        },
        {
            "name": "supports_or_challenges_theory",
            "type": "str",
            "description": "Based on the findings, does this evidence support or challenge the Proxy-to-Ground-Truth Gap Theory? Explain briefly how it relates to the theory's predictions (exponential gap, systematic undervaluation, field differences, etc.)."
        }
    ],
    "extraction_query": "Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-164 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-164</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-164</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM or language model used for prediction (e.g., GPT-4, PaLM, Llama-2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM, including architecture, size, and any relevant training details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>prediction_target</strong></td>
                        <td>str</td>
                        <td>A description of the specific future scientific discovery or event that the LLM is predicting (e.g., 'discovery of a room-temperature superconductor', 'successful synthesis of a new antibiotic', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>prediction_method</strong></td>
                        <td>str</td>
                        <td>How the LLM is used to generate the probability or likelihood (e.g., direct probability output, logit extraction, chain-of-thought prompting, fine-tuning, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>probability_format</strong></td>
                        <td>str</td>
                        <td>The format in which the probability or likelihood is elicited from the LLM (e.g., percentage, log-odds, binary classification, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>How the accuracy or calibration of the LLM's probability estimates is evaluated (e.g., Brier score, log-likelihood, real-world outcome comparison, calibration plots, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>results</strong></td>
                        <td>str</td>
                        <td>A concise summary of the results, including how accurate or well-calibrated the LLM's predictions were, with numerical metrics if available.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_challenges</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, challenges, or failure cases in using LLMs to predict scientific discoveries (e.g., overconfidence, lack of domain knowledge, poor calibration, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_baselines</strong></td>
                        <td>str</td>
                        <td>Any comparisons to other methods or baselines (e.g., human experts, other models, random guessing), including relative performance.</td>
                    </tr>
                    <tr>
                        <td><strong>methods_for_improvement</strong></td>
                        <td>str</td>
                        <td>Any methods or techniques reported for improving the accuracy or calibration of LLM probability estimates (e.g., prompt engineering, post-hoc calibration, ensembling, etc.).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-164",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM or language model used for prediction (e.g., GPT-4, PaLM, Llama-2, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM, including architecture, size, and any relevant training details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified."
        },
        {
            "name": "prediction_target",
            "type": "str",
            "description": "A description of the specific future scientific discovery or event that the LLM is predicting (e.g., 'discovery of a room-temperature superconductor', 'successful synthesis of a new antibiotic', etc.)."
        },
        {
            "name": "prediction_method",
            "type": "str",
            "description": "How the LLM is used to generate the probability or likelihood (e.g., direct probability output, logit extraction, chain-of-thought prompting, fine-tuning, etc.)."
        },
        {
            "name": "probability_format",
            "type": "str",
            "description": "The format in which the probability or likelihood is elicited from the LLM (e.g., percentage, log-odds, binary classification, etc.)."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "How the accuracy or calibration of the LLM's probability estimates is evaluated (e.g., Brier score, log-likelihood, real-world outcome comparison, calibration plots, etc.)."
        },
        {
            "name": "results",
            "type": "str",
            "description": "A concise summary of the results, including how accurate or well-calibrated the LLM's predictions were, with numerical metrics if available."
        },
        {
            "name": "limitations_or_challenges",
            "type": "str",
            "description": "Any reported limitations, challenges, or failure cases in using LLMs to predict scientific discoveries (e.g., overconfidence, lack of domain knowledge, poor calibration, etc.)."
        },
        {
            "name": "comparison_to_baselines",
            "type": "str",
            "description": "Any comparisons to other methods or baselines (e.g., human experts, other models, random guessing), including relative performance."
        },
        {
            "name": "methods_for_improvement",
            "type": "str",
            "description": "Any methods or techniques reported for improving the accuracy or calibration of LLM probability estimates (e.g., prompt engineering, post-hoc calibration, ensembling, etc.)."
        }
    ],
    "extraction_query": "Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-15 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-15</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-15</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being evaluated (e.g., GPT-3, GPT-4, LLaMA, T5, BERT, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 1B, 7B, 175B, etc.). Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model, including any special training procedures, architectures, or modifications relevant to embodied tasks.</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the embodied planning task or benchmark (e.g., VirtualHome, ALFWorld, BEHAVIOR, Room-to-Room navigation, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A detailed description of the embodied planning task, including what the agent needs to accomplish and the environment it operates in.</td>
                    </tr>
                    <tr>
                        <td><strong>task_type</strong></td>
                        <td>str</td>
                        <td>The category of embodied task (e.g., 'navigation', 'object manipulation', 'household tasks', 'instruction following', 'multi-step planning', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>knowledge_type</strong></td>
                        <td>str</td>
                        <td>What type of knowledge is being tested or utilized? Options: 'spatial' (locations, layouts, distances, orientations), 'procedural' (action sequences, steps, procedures), 'object-relational' (object properties, affordances, relationships), or combinations like 'spatial+procedural'. Be specific.</td>
                    </tr>
                    <tr>
                        <td><strong>knowledge_source</strong></td>
                        <td>str</td>
                        <td>Where does the model's knowledge come from? (e.g., 'pre-training on text corpora', 'fine-tuning on embodied task data', 'in-context examples', 'explicit knowledge base', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>has_direct_sensory_input</strong></td>
                        <td>bool</td>
                        <td>Does the model receive direct sensory input (vision, proprioception, etc.) during the task? True if yes, False if operating purely from text/memory, null if unclear.</td>
                    </tr>
                    <tr>
                        <td><strong>elicitation_method</strong></td>
                        <td>str</td>
                        <td>How is the knowledge elicited or utilized from the model? (e.g., 'zero-shot prompting', 'few-shot prompting', 'chain-of-thought', 'fine-tuning', 'probing', 'code generation', 'planning algorithms', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>knowledge_representation</strong></td>
                        <td>str</td>
                        <td>How is the knowledge represented or encoded in the model? (e.g., 'implicit in weights', 'explicit symbolic representations', 'spatial maps', 'action sequences', 'scene graphs', 'natural language descriptions', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metric</strong></td>
                        <td>str</td>
                        <td>What metric is used to evaluate performance? (e.g., 'success rate', 'task completion', 'path efficiency', 'accuracy', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_result</strong></td>
                        <td>str</td>
                        <td>What was the quantitative performance of the model on this task? Include numbers and units. Be specific about conditions (e.g., 'zero-shot: 45%, few-shot: 67%').</td>
                    </tr>
                    <tr>
                        <td><strong>success_patterns</strong></td>
                        <td>str</td>
                        <td>What types of spatial, procedural, or object-relational reasoning did the model succeed at? Be specific about what knowledge was successfully utilized.</td>
                    </tr>
                    <tr>
                        <td><strong>failure_patterns</strong></td>
                        <td>str</td>
                        <td>What types of spatial, procedural, or object-relational reasoning did the model fail at? Be specific about what knowledge was missing or incorrectly utilized.</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_comparison</strong></td>
                        <td>str</td>
                        <td>If comparisons are made, what are the baseline methods and their performance? (e.g., 'random agent: 5%', 'rule-based: 30%', 'smaller model: 25%', etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_results</strong></td>
                        <td>str</td>
                        <td>If ablation studies are performed, what components were removed and what was the impact on performance? This helps understand what knowledge sources are critical.</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings</strong></td>
                        <td>str</td>
                        <td>What are the key findings or insights about how the language model encodes or utilizes spatial, procedural, or object-relational knowledge? Be concise but information-dense.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-15",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being evaluated (e.g., GPT-3, GPT-4, LLaMA, T5, BERT, etc.)"
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 1B, 7B, 175B, etc.). Null if not specified."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model, including any special training procedures, architectures, or modifications relevant to embodied tasks."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the embodied planning task or benchmark (e.g., VirtualHome, ALFWorld, BEHAVIOR, Room-to-Room navigation, etc.)"
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A detailed description of the embodied planning task, including what the agent needs to accomplish and the environment it operates in."
        },
        {
            "name": "task_type",
            "type": "str",
            "description": "The category of embodied task (e.g., 'navigation', 'object manipulation', 'household tasks', 'instruction following', 'multi-step planning', etc.)"
        },
        {
            "name": "knowledge_type",
            "type": "str",
            "description": "What type of knowledge is being tested or utilized? Options: 'spatial' (locations, layouts, distances, orientations), 'procedural' (action sequences, steps, procedures), 'object-relational' (object properties, affordances, relationships), or combinations like 'spatial+procedural'. Be specific."
        },
        {
            "name": "knowledge_source",
            "type": "str",
            "description": "Where does the model's knowledge come from? (e.g., 'pre-training on text corpora', 'fine-tuning on embodied task data', 'in-context examples', 'explicit knowledge base', etc.)"
        },
        {
            "name": "has_direct_sensory_input",
            "type": "bool",
            "description": "Does the model receive direct sensory input (vision, proprioception, etc.) during the task? True if yes, False if operating purely from text/memory, null if unclear."
        },
        {
            "name": "elicitation_method",
            "type": "str",
            "description": "How is the knowledge elicited or utilized from the model? (e.g., 'zero-shot prompting', 'few-shot prompting', 'chain-of-thought', 'fine-tuning', 'probing', 'code generation', 'planning algorithms', etc.)"
        },
        {
            "name": "knowledge_representation",
            "type": "str",
            "description": "How is the knowledge represented or encoded in the model? (e.g., 'implicit in weights', 'explicit symbolic representations', 'spatial maps', 'action sequences', 'scene graphs', 'natural language descriptions', etc.)"
        },
        {
            "name": "performance_metric",
            "type": "str",
            "description": "What metric is used to evaluate performance? (e.g., 'success rate', 'task completion', 'path efficiency', 'accuracy', etc.)"
        },
        {
            "name": "performance_result",
            "type": "str",
            "description": "What was the quantitative performance of the model on this task? Include numbers and units. Be specific about conditions (e.g., 'zero-shot: 45%, few-shot: 67%')."
        },
        {
            "name": "success_patterns",
            "type": "str",
            "description": "What types of spatial, procedural, or object-relational reasoning did the model succeed at? Be specific about what knowledge was successfully utilized."
        },
        {
            "name": "failure_patterns",
            "type": "str",
            "description": "What types of spatial, procedural, or object-relational reasoning did the model fail at? Be specific about what knowledge was missing or incorrectly utilized."
        },
        {
            "name": "baseline_comparison",
            "type": "str",
            "description": "If comparisons are made, what are the baseline methods and their performance? (e.g., 'random agent: 5%', 'rule-based: 30%', 'smaller model: 25%', etc.)"
        },
        {
            "name": "ablation_results",
            "type": "str",
            "description": "If ablation studies are performed, what components were removed and what was the impact on performance? This helps understand what knowledge sources are critical."
        },
        {
            "name": "key_findings",
            "type": "str",
            "description": "What are the key findings or insights about how the language model encodes or utilizes spatial, procedural, or object-relational knowledge? Be concise but information-dense."
        }
    ],
    "extraction_query": "Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.",
    "supporting_theory_ids": [],
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
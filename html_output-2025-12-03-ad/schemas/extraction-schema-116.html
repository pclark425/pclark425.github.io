<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-116 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-116</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-116</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model (LLM) being evaluated (e.g., GPT-3, PaLM, Llama-2).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 7B, 13B, 70B), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the task or benchmark used to evaluate the model (e.g., arithmetic, commonsense reasoning, MMLU, GSM8K).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the task or benchmark.</td>
                    </tr>
                    <tr>
                        <td><strong>problem_format</strong></td>
                        <td>str</td>
                        <td>A detailed description of the problem presentation format or prompt format used (e.g., zero-shot, few-shot, chain-of-thought, rephrased question, added context, formatting changes).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_format</strong></td>
                        <td>str</td>
                        <td>If the paper compares multiple formats, describe the alternative format(s) used for comparison (e.g., standard prompt vs. chain-of-thought). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>The performance of the model on the task with the specified problem format (include metric and value, e.g., 'accuracy: 78%').</td>
                    </tr>
                    <tr>
                        <td><strong>performance_comparison</strong></td>
                        <td>str</td>
                        <td>If available, the performance of the model on the same task with the comparison format(s) (include metric and value). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>format_effect_size</strong></td>
                        <td>str</td>
                        <td>If available, the quantitative effect size or difference in performance between formats (e.g., '+12% accuracy with chain-of-thought'). Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>format_effect_direction</strong></td>
                        <td>str</td>
                        <td>A brief statement of whether the format improved, reduced, or had no effect on performance (e.g., 'improved', 'reduced', 'no effect').</td>
                    </tr>
                    <tr>
                        <td><strong>explanation_or_hypothesis</strong></td>
                        <td>str</td>
                        <td>Any explanations, hypotheses, or findings about why the problem format affected (or did not affect) LLM performance, as stated in the paper.</td>
                    </tr>
                    <tr>
                        <td><strong>counterexample_or_null_result</strong></td>
                        <td>str</td>
                        <td>Any mention of cases where problem format had no effect, or where an expected effect did not occur. Null if not applicable.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-116",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model (LLM) being evaluated (e.g., GPT-3, PaLM, Llama-2)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 7B, 13B, 70B), or null if not specified."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the task or benchmark used to evaluate the model (e.g., arithmetic, commonsense reasoning, MMLU, GSM8K)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the task or benchmark."
        },
        {
            "name": "problem_format",
            "type": "str",
            "description": "A detailed description of the problem presentation format or prompt format used (e.g., zero-shot, few-shot, chain-of-thought, rephrased question, added context, formatting changes)."
        },
        {
            "name": "comparison_format",
            "type": "str",
            "description": "If the paper compares multiple formats, describe the alternative format(s) used for comparison (e.g., standard prompt vs. chain-of-thought). Null if not applicable."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "The performance of the model on the task with the specified problem format (include metric and value, e.g., 'accuracy: 78%')."
        },
        {
            "name": "performance_comparison",
            "type": "str",
            "description": "If available, the performance of the model on the same task with the comparison format(s) (include metric and value). Null if not applicable."
        },
        {
            "name": "format_effect_size",
            "type": "str",
            "description": "If available, the quantitative effect size or difference in performance between formats (e.g., '+12% accuracy with chain-of-thought'). Null if not specified."
        },
        {
            "name": "format_effect_direction",
            "type": "str",
            "description": "A brief statement of whether the format improved, reduced, or had no effect on performance (e.g., 'improved', 'reduced', 'no effect')."
        },
        {
            "name": "explanation_or_hypothesis",
            "type": "str",
            "description": "Any explanations, hypotheses, or findings about why the problem format affected (or did not affect) LLM performance, as stated in the paper."
        },
        {
            "name": "counterexample_or_null_result",
            "type": "str",
            "description": "Any mention of cases where problem format had no effect, or where an expected effect did not occur. Null if not applicable."
        }
    ],
    "extraction_query": "Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
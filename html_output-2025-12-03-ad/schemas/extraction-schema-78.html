<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-78 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-78</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-78</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being evaluated (e.g., GPT-4, Llama-2, PaLM, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the language model, including architecture, size, and any relevant training details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_task_name</strong></td>
                        <td>str</td>
                        <td>The name of the logical reasoning task or benchmark (e.g., ProofWriter, LogiQA, ReClor, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>reasoning_task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the logical reasoning task or benchmark, including the type of logic or reasoning required.</td>
                    </tr>
                    <tr>
                        <td><strong>method_or_intervention</strong></td>
                        <td>str</td>
                        <td>Description of any method, intervention, or technique used to improve logical reasoning (e.g., chain-of-thought prompting, fine-tuning on logic data, neuro-symbolic methods, tool use, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>Performance of the model on the logical reasoning task (include numerical results and units if available, e.g., accuracy %, or qualitative results if not).</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_performance</strong></td>
                        <td>str</td>
                        <td>Performance of the baseline model (e.g., vanilla LLM, or symbolic system) on the same task, for comparison. Include numerical results and units if available, or null if not reported.</td>
                    </tr>
                    <tr>
                        <td><strong>improvement_over_baseline</strong></td>
                        <td>str</td>
                        <td>Description of the improvement (or lack thereof) over the baseline, including quantitative or qualitative differences.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failures</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure cases, or types of logical reasoning where the model struggled.</td>
                    </tr>
                    <tr>
                        <td><strong>ablation_or_analysis</strong></td>
                        <td>str</td>
                        <td>Any ablation studies or analysis results that help explain why the method works or fails, or what factors are important for strict logical reasoning.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-78",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being evaluated (e.g., GPT-4, Llama-2, PaLM, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the language model, including architecture, size, and any relevant training details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified."
        },
        {
            "name": "reasoning_task_name",
            "type": "str",
            "description": "The name of the logical reasoning task or benchmark (e.g., ProofWriter, LogiQA, ReClor, etc.)."
        },
        {
            "name": "reasoning_task_description",
            "type": "str",
            "description": "A brief description of the logical reasoning task or benchmark, including the type of logic or reasoning required."
        },
        {
            "name": "method_or_intervention",
            "type": "str",
            "description": "Description of any method, intervention, or technique used to improve logical reasoning (e.g., chain-of-thought prompting, fine-tuning on logic data, neuro-symbolic methods, tool use, etc.)."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "Performance of the model on the logical reasoning task (include numerical results and units if available, e.g., accuracy %, or qualitative results if not)."
        },
        {
            "name": "baseline_performance",
            "type": "str",
            "description": "Performance of the baseline model (e.g., vanilla LLM, or symbolic system) on the same task, for comparison. Include numerical results and units if available, or null if not reported."
        },
        {
            "name": "improvement_over_baseline",
            "type": "str",
            "description": "Description of the improvement (or lack thereof) over the baseline, including quantitative or qualitative differences."
        },
        {
            "name": "limitations_or_failures",
            "type": "str",
            "description": "Any reported limitations, failure cases, or types of logical reasoning where the model struggled."
        },
        {
            "name": "ablation_or_analysis",
            "type": "str",
            "description": "Any ablation studies or analysis results that help explain why the method works or fails, or what factors are important for strict logical reasoning."
        }
    ],
    "extraction_query": "Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
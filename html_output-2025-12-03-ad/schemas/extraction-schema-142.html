<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-142 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-142</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-142</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name or identifier of the LLM used (e.g., GPT-4, Claude, LLaMA, T5).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the model architecture or training regime.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Size of the model in parameters or a comparable metric (e.g., 13B, 70B, "base", "large").</td>
                    </tr>
                    <tr>
                        <td><strong>corpus_domain</strong></td>
                        <td>str</td>
                        <td>Scientific domain(s) of the input papers (e.g., physics, biology, social sciences).</td>
                    </tr>
                    <tr>
                        <td><strong>corpus_size</strong></td>
                        <td>int</td>
                        <td>Number of scholarly papers or documents used as input (approximate if exact number not given).</td>
                    </tr>
                    <tr>
                        <td><strong>method_name</strong></td>
                        <td>str</td>
                        <td>Name of the specific prompting or fine‑tuning method employed to extract laws (e.g., chain‑of‑thought prompting, few‑shot prompting, instruction tuning).</td>
                    </tr>
                    <tr>
                        <td><strong>method_description</strong></td>
                        <td>str</td>
                        <td>A concise description of how the method works (e.g., "iterative hypothesis generation with self‑critique").</td>
                    </tr>
                    <tr>
                        <td><strong>law_extraction_approach</strong></td>
                        <td>str</td>
                        <td>The technical approach used to turn model outputs into qualitative laws (e.g., rule induction, pattern mining, causal inference, knowledge‑graph synthesis).</td>
                    </tr>
                    <tr>
                        <td><strong>extracted_law</strong></td>
                        <td>str</td>
                        <td>The qualitative law, principle, or rule that the paper reports the LLM distilled (e.g., "increasing temperature reduces enzyme activity linearly within 20‑40°C").</td>
                    </tr>
                    <tr>
                        <td><strong>law_type</strong></td>
                        <td>str</td>
                        <td>Category of the law (e.g., causal rule, empirical regularity, theoretical principle, heuristic).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>How the extracted law was evaluated or validated (e.g., expert assessment, comparison to known theory, downstream prediction task).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_result</strong></td>
                        <td>str</td>
                        <td>Summary of the evaluation outcome (e.g., "80% of expert judges rated the law as accurate", "precision 0.73, recall 0.68").</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_comparison</strong></td>
                        <td>bool</td>
                        <td>Does the paper compare the LLM‑based distillation to a baseline method (true/false/null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, challenges, or failure modes of the approach.</td>
                    </tr>
                    <tr>
                        <td><strong>future_directions</strong></td>
                        <td>str</td>
                        <td>Suggested future work or open questions related to LLM‑driven law distillation.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-142",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name or identifier of the LLM used (e.g., GPT-4, Claude, LLaMA, T5)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the model architecture or training regime."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Size of the model in parameters or a comparable metric (e.g., 13B, 70B, \"base\", \"large\")."
        },
        {
            "name": "corpus_domain",
            "type": "str",
            "description": "Scientific domain(s) of the input papers (e.g., physics, biology, social sciences)."
        },
        {
            "name": "corpus_size",
            "type": "int",
            "description": "Number of scholarly papers or documents used as input (approximate if exact number not given)."
        },
        {
            "name": "method_name",
            "type": "str",
            "description": "Name of the specific prompting or fine‑tuning method employed to extract laws (e.g., chain‑of‑thought prompting, few‑shot prompting, instruction tuning)."
        },
        {
            "name": "method_description",
            "type": "str",
            "description": "A concise description of how the method works (e.g., \"iterative hypothesis generation with self‑critique\")."
        },
        {
            "name": "law_extraction_approach",
            "type": "str",
            "description": "The technical approach used to turn model outputs into qualitative laws (e.g., rule induction, pattern mining, causal inference, knowledge‑graph synthesis)."
        },
        {
            "name": "extracted_law",
            "type": "str",
            "description": "The qualitative law, principle, or rule that the paper reports the LLM distilled (e.g., \"increasing temperature reduces enzyme activity linearly within 20‑40°C\")."
        },
        {
            "name": "law_type",
            "type": "str",
            "description": "Category of the law (e.g., causal rule, empirical regularity, theoretical principle, heuristic)."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "How the extracted law was evaluated or validated (e.g., expert assessment, comparison to known theory, downstream prediction task)."
        },
        {
            "name": "evaluation_result",
            "type": "str",
            "description": "Summary of the evaluation outcome (e.g., \"80% of expert judges rated the law as accurate\", \"precision 0.73, recall 0.68\")."
        },
        {
            "name": "baseline_comparison",
            "type": "bool",
            "description": "Does the paper compare the LLM‑based distillation to a baseline method (true/false/null if not reported)."
        },
        {
            "name": "limitations",
            "type": "str",
            "description": "Any reported limitations, challenges, or failure modes of the approach."
        },
        {
            "name": "future_directions",
            "type": "str",
            "description": "Suggested future work or open questions related to LLM‑driven law distillation."
        }
    ],
    "extraction_query": "Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-163 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-163</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-163</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the LLM model being evaluated (e.g., GPT-3, PaLM, Llama-2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>task_name</strong></td>
                        <td>str</td>
                        <td>The name of the task or benchmark used to evaluate the model (e.g., MMLU, GSM8K, commonsense reasoning, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the task or benchmark.</td>
                    </tr>
                    <tr>
                        <td><strong>presentation_format</strong></td>
                        <td>str</td>
                        <td>A detailed description of the problem or prompt presentation format used (e.g., zero-shot, few-shot, chain-of-thought, specific prompt wording, formatting, context length, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_format</strong></td>
                        <td>str</td>
                        <td>If the paper compares multiple formats, describe the alternative format(s) used for comparison. Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>performance</strong></td>
                        <td>str</td>
                        <td>The performance of the model on the task with the specified presentation format (include metric and value, e.g., 'accuracy: 78%').</td>
                    </tr>
                    <tr>
                        <td><strong>performance_comparison</strong></td>
                        <td>str</td>
                        <td>If available, the performance of the model on the same task with the comparison format(s) (include metric and value). Null if not applicable.</td>
                    </tr>
                    <tr>
                        <td><strong>format_effect_size</strong></td>
                        <td>str</td>
                        <td>If available, the effect size or difference in performance between formats (e.g., '+12% accuracy with chain-of-thought vs. standard prompt'). Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>explanation_or_hypothesis</strong></td>
                        <td>str</td>
                        <td>Any explanations, hypotheses, or findings about why or how the presentation format affects LLM performance, as stated in the paper. Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>null_or_negative_result</strong></td>
                        <td>bool</td>
                        <td>Does the paper report that presentation format had no effect, or a negative effect, on LLM performance for any task/model? (true, false, or null if no information)</td>
                    </tr>
                    <tr>
                        <td><strong>experimental_details</strong></td>
                        <td>str</td>
                        <td>Any relevant details about the experimental setup (e.g., number of examples, prompt length, context window, etc.).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-163",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the LLM model being evaluated (e.g., GPT-3, PaLM, Llama-2, etc.)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified."
        },
        {
            "name": "task_name",
            "type": "str",
            "description": "The name of the task or benchmark used to evaluate the model (e.g., MMLU, GSM8K, commonsense reasoning, etc.)."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the task or benchmark."
        },
        {
            "name": "presentation_format",
            "type": "str",
            "description": "A detailed description of the problem or prompt presentation format used (e.g., zero-shot, few-shot, chain-of-thought, specific prompt wording, formatting, context length, etc.)."
        },
        {
            "name": "comparison_format",
            "type": "str",
            "description": "If the paper compares multiple formats, describe the alternative format(s) used for comparison. Null if not applicable."
        },
        {
            "name": "performance",
            "type": "str",
            "description": "The performance of the model on the task with the specified presentation format (include metric and value, e.g., 'accuracy: 78%')."
        },
        {
            "name": "performance_comparison",
            "type": "str",
            "description": "If available, the performance of the model on the same task with the comparison format(s) (include metric and value). Null if not applicable."
        },
        {
            "name": "format_effect_size",
            "type": "str",
            "description": "If available, the effect size or difference in performance between formats (e.g., '+12% accuracy with chain-of-thought vs. standard prompt'). Null if not specified."
        },
        {
            "name": "explanation_or_hypothesis",
            "type": "str",
            "description": "Any explanations, hypotheses, or findings about why or how the presentation format affects LLM performance, as stated in the paper. Null if not specified."
        },
        {
            "name": "null_or_negative_result",
            "type": "bool",
            "description": "Does the paper report that presentation format had no effect, or a negative effect, on LLM performance for any task/model? (true, false, or null if no information)"
        },
        {
            "name": "experimental_details",
            "type": "str",
            "description": "Any relevant details about the experimental setup (e.g., number of examples, prompt length, context window, etc.)."
        }
    ],
    "extraction_query": "Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
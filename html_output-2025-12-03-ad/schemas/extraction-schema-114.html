<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-114 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-114</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-114</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the large language model (LLM) used as a simulator (e.g., GPT-4, PaLM, Llama-2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the LLM, including architecture, training data, or other relevant details if available.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model, typically in number of parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>scientific_subdomain</strong></td>
                        <td>str</td>
                        <td>The specific scientific subdomain(s) in which the LLM is used as a simulator (e.g., organic chemistry, quantum physics, molecular biology, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>simulation_task</strong></td>
                        <td>str</td>
                        <td>A concise description of the simulation task(s) performed by the LLM (e.g., simulating chemical reactions, predicting physical phenomena, modeling biological processes, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>accuracy_metric</strong></td>
                        <td>str</td>
                        <td>The metric(s) used to evaluate the accuracy of the LLM's simulation (e.g., exact match, F1, RMSE, human evaluation, etc.), or null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>reported_accuracy</strong></td>
                        <td>str</td>
                        <td>The reported accuracy or performance of the LLM on the simulation task, including numerical values and units if available.</td>
                    </tr>
                    <tr>
                        <td><strong>factors_affecting_accuracy</strong></td>
                        <td>str</td>
                        <td>A list or description of factors identified as affecting the accuracy of the LLM as a simulator (e.g., model size, prompt design, training data domain coverage, subdomain complexity, reasoning requirements, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_for_factors</strong></td>
                        <td>str</td>
                        <td>Brief summary of evidence, experiments, or analysis supporting the identified factors (e.g., ablation studies, comparisons, error analysis, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method</strong></td>
                        <td>str</td>
                        <td>Description of how the accuracy was evaluated (e.g., automated metrics, human expert review, comparison to ground truth, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_cases</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure cases, or counterexamples where the LLM's simulation was inaccurate or unreliable.</td>
                    </tr>
                    <tr>
                        <td><strong>comparisons</strong></td>
                        <td>str</td>
                        <td>Any comparisons made between different models, prompts, subdomains, or methods, especially regarding accuracy.</td>
                    </tr>
                    <tr>
                        <td><strong>recommendations_or_best_practices</strong></td>
                        <td>str</td>
                        <td>Any recommendations, best practices, or guidelines suggested by the paper for improving the accuracy of LLM-based scientific simulators.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-114",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the large language model (LLM) used as a simulator (e.g., GPT-4, PaLM, Llama-2, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the LLM, including architecture, training data, or other relevant details if available."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model, typically in number of parameters (e.g., 7B, 13B, 70B, etc.), or null if not specified."
        },
        {
            "name": "scientific_subdomain",
            "type": "str",
            "description": "The specific scientific subdomain(s) in which the LLM is used as a simulator (e.g., organic chemistry, quantum physics, molecular biology, etc.)."
        },
        {
            "name": "simulation_task",
            "type": "str",
            "description": "A concise description of the simulation task(s) performed by the LLM (e.g., simulating chemical reactions, predicting physical phenomena, modeling biological processes, etc.)."
        },
        {
            "name": "accuracy_metric",
            "type": "str",
            "description": "The metric(s) used to evaluate the accuracy of the LLM's simulation (e.g., exact match, F1, RMSE, human evaluation, etc.), or null if not specified."
        },
        {
            "name": "reported_accuracy",
            "type": "str",
            "description": "The reported accuracy or performance of the LLM on the simulation task, including numerical values and units if available."
        },
        {
            "name": "factors_affecting_accuracy",
            "type": "str",
            "description": "A list or description of factors identified as affecting the accuracy of the LLM as a simulator (e.g., model size, prompt design, training data domain coverage, subdomain complexity, reasoning requirements, etc.)."
        },
        {
            "name": "evidence_for_factors",
            "type": "str",
            "description": "Brief summary of evidence, experiments, or analysis supporting the identified factors (e.g., ablation studies, comparisons, error analysis, etc.)."
        },
        {
            "name": "evaluation_method",
            "type": "str",
            "description": "Description of how the accuracy was evaluated (e.g., automated metrics, human expert review, comparison to ground truth, etc.)."
        },
        {
            "name": "limitations_or_failure_cases",
            "type": "str",
            "description": "Any reported limitations, failure cases, or counterexamples where the LLM's simulation was inaccurate or unreliable."
        },
        {
            "name": "comparisons",
            "type": "str",
            "description": "Any comparisons made between different models, prompts, subdomains, or methods, especially regarding accuracy."
        },
        {
            "name": "recommendations_or_best_practices",
            "type": "str",
            "description": "Any recommendations, best practices, or guidelines suggested by the paper for improving the accuracy of LLM-based scientific simulators."
        }
    ],
    "extraction_query": "Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
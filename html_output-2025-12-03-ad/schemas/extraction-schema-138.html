<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-138 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-138</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-138</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name or identifier of the LLM evaluated (e.g., GPT-4, Claude, LLaMA-2-70B).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Size of the model in parameters or a comparable descriptor (e.g., 13B, 70B, 175B).</td>
                    </tr>
                    <tr>
                        <td><strong>model_type</strong></td>
                        <td>str</td>
                        <td>Whether the model is base, instruction‑tuned, fine‑tuned on domain data, or uses retrieval/plug‑in tools.</td>
                    </tr>
                    <tr>
                        <td><strong>scientific_domain</strong></td>
                        <td>str</td>
                        <td>Specific subdomain of science addressed (e.g., chemistry, physics, biology, earth science, astronomy, materials science).</td>
                    </tr>
                    <tr>
                        <td><strong>simulation_task_description</strong></td>
                        <td>str</td>
                        <td>A concise description of the text‑based simulation task (e.g., predict reaction outcome, solve a differential equation, generate climate projection, explain a biological pathway).</td>
                    </tr>
                    <tr>
                        <td><strong>prompting_strategy</strong></td>
                        <td>str</td>
                        <td>Details of the prompting approach used (e.g., zero‑shot, few‑shot with examples, chain‑of‑thought, self‑consistency, tool‑use, retrieval‑augmented prompting).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metric</strong></td>
                        <td>str</td>
                        <td>Metric(s) reported for assessing simulation accuracy (e.g., exact match, BLEU, ROUGE, MAE, RMSE, Pearson correlation, F1).</td>
                    </tr>
                    <tr>
                        <td><strong>reported_accuracy</strong></td>
                        <td>str</td>
                        <td>The quantitative result reported for the model on the task, including units if applicable (e.g., 85% exact match, RMSE=0.12).</td>
                    </tr>
                    <tr>
                        <td><strong>baseline_accuracy</strong></td>
                        <td>str</td>
                        <td>Performance of a baseline for comparison (e.g., smaller model, no prompting, random guess).</td>
                    </tr>
                    <tr>
                        <td><strong>factors_reported</strong></td>
                        <td>list</td>
                        <td>List of factors the paper identifies as influencing accuracy (e.g., model size, domain‑specific fine‑tuning, temperature, number of few‑shot examples, chain‑of‑thought, tool use, external knowledge retrieval).</td>
                    </tr>
                    <tr>
                        <td><strong>experimental_conditions</strong></td>
                        <td>str</td>
                        <td>Any additional experimental settings that may affect results (e.g., temperature=0.7, top‑k=40, number of demonstration examples, hardware constraints).</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_modes</strong></td>
                        <td>str</td>
                        <td>Reported limitations, failure cases, or domains where accuracy drops significantly.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-138",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name or identifier of the LLM evaluated (e.g., GPT-4, Claude, LLaMA-2-70B)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Size of the model in parameters or a comparable descriptor (e.g., 13B, 70B, 175B)."
        },
        {
            "name": "model_type",
            "type": "str",
            "description": "Whether the model is base, instruction‑tuned, fine‑tuned on domain data, or uses retrieval/plug‑in tools."
        },
        {
            "name": "scientific_domain",
            "type": "str",
            "description": "Specific subdomain of science addressed (e.g., chemistry, physics, biology, earth science, astronomy, materials science)."
        },
        {
            "name": "simulation_task_description",
            "type": "str",
            "description": "A concise description of the text‑based simulation task (e.g., predict reaction outcome, solve a differential equation, generate climate projection, explain a biological pathway)."
        },
        {
            "name": "prompting_strategy",
            "type": "str",
            "description": "Details of the prompting approach used (e.g., zero‑shot, few‑shot with examples, chain‑of‑thought, self‑consistency, tool‑use, retrieval‑augmented prompting)."
        },
        {
            "name": "evaluation_metric",
            "type": "str",
            "description": "Metric(s) reported for assessing simulation accuracy (e.g., exact match, BLEU, ROUGE, MAE, RMSE, Pearson correlation, F1)."
        },
        {
            "name": "reported_accuracy",
            "type": "str",
            "description": "The quantitative result reported for the model on the task, including units if applicable (e.g., 85% exact match, RMSE=0.12)."
        },
        {
            "name": "baseline_accuracy",
            "type": "str",
            "description": "Performance of a baseline for comparison (e.g., smaller model, no prompting, random guess)."
        },
        {
            "name": "factors_reported",
            "type": "list",
            "description": "List of factors the paper identifies as influencing accuracy (e.g., model size, domain‑specific fine‑tuning, temperature, number of few‑shot examples, chain‑of‑thought, tool use, external knowledge retrieval)."
        },
        {
            "name": "experimental_conditions",
            "type": "str",
            "description": "Any additional experimental settings that may affect results (e.g., temperature=0.7, top‑k=40, number of demonstration examples, hardware constraints)."
        },
        {
            "name": "limitations_or_failure_modes",
            "type": "str",
            "description": "Reported limitations, failure cases, or domains where accuracy drops significantly."
        }
    ],
    "extraction_query": "Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
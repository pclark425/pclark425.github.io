<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-146 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-146</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-146</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>paper_title</strong></td>
                        <td>str</td>
                        <td>Title of the paper reporting the comparison.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_task</strong></td>
                        <td>str</td>
                        <td>The downstream task for which the LLM judge was used (e.g., summarization, code generation, dialogue, reasoning).</td>
                    </tr>
                    <tr>
                        <td><strong>dataset_name</strong></td>
                        <td>str</td>
                        <td>Name of the dataset or benchmark on which the evaluation was performed.</td>
                    </tr>
                    <tr>
                        <td><strong>judge_model_name</strong></td>
                        <td>str</td>
                        <td>Name of the language model used as the judge (e.g., GPT-4, Claude, LLaMA-2).</td>
                    </tr>
                    <tr>
                        <td><strong>judge_model_details</strong></td>
                        <td>str</td>
                        <td>Brief details about the judge model such as size, training data, or version.</td>
                    </tr>
                    <tr>
                        <td><strong>human_evaluator_type</strong></td>
                        <td>str</td>
                        <td>Description of the human evaluators (e.g., crowdworkers, domain experts, authors).</td>
                    </tr>
                    <tr>
                        <td><strong>agreement_metric</strong></td>
                        <td>str</td>
                        <td>Metric used to quantify agreement between LLM judge and humans (e.g., Pearson r, Spearman rho, Kendall tau, accuracy, Cohen's kappa).</td>
                    </tr>
                    <tr>
                        <td><strong>agreement_score</strong></td>
                        <td>float</td>
                        <td>Numeric value of the agreement metric reported in the paper (null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>reported_loss_aspects</strong></td>
                        <td>str</td>
                        <td>Specific aspects where LLM judges underperform relative to humans, listed as a semicolon-separated string (e.g., 'lack of nuance; reduced sensitivity to factual errors; bias toward fluent but incorrect answers').</td>
                    </tr>
                    <tr>
                        <td><strong>qualitative_findings</strong></td>
                        <td>str</td>
                        <td>Brief summary of qualitative observations about differences between LLM and human judgments (e.g., consistency, interpretability, domain expertise).</td>
                    </tr>
                    <tr>
                        <td><strong>advantages_of_llm_judge</strong></td>
                        <td>str</td>
                        <td>Any reported benefits of using LLM judges compared to humans (e.g., speed, cost, scalability).</td>
                    </tr>
                    <tr>
                        <td><strong>experimental_setting</strong></td>
                        <td>str</td>
                        <td>Key details of the experimental setup such as prompt format, number of judges per sample, and evaluation protocol.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-146",
    "schema": [
        {
            "name": "paper_title",
            "type": "str",
            "description": "Title of the paper reporting the comparison."
        },
        {
            "name": "evaluation_task",
            "type": "str",
            "description": "The downstream task for which the LLM judge was used (e.g., summarization, code generation, dialogue, reasoning)."
        },
        {
            "name": "dataset_name",
            "type": "str",
            "description": "Name of the dataset or benchmark on which the evaluation was performed."
        },
        {
            "name": "judge_model_name",
            "type": "str",
            "description": "Name of the language model used as the judge (e.g., GPT-4, Claude, LLaMA-2)."
        },
        {
            "name": "judge_model_details",
            "type": "str",
            "description": "Brief details about the judge model such as size, training data, or version."
        },
        {
            "name": "human_evaluator_type",
            "type": "str",
            "description": "Description of the human evaluators (e.g., crowdworkers, domain experts, authors)."
        },
        {
            "name": "agreement_metric",
            "type": "str",
            "description": "Metric used to quantify agreement between LLM judge and humans (e.g., Pearson r, Spearman rho, Kendall tau, accuracy, Cohen's kappa)."
        },
        {
            "name": "agreement_score",
            "type": "float",
            "description": "Numeric value of the agreement metric reported in the paper (null if not reported)."
        },
        {
            "name": "reported_loss_aspects",
            "type": "str",
            "description": "Specific aspects where LLM judges underperform relative to humans, listed as a semicolon-separated string (e.g., 'lack of nuance; reduced sensitivity to factual errors; bias toward fluent but incorrect answers')."
        },
        {
            "name": "qualitative_findings",
            "type": "str",
            "description": "Brief summary of qualitative observations about differences between LLM and human judgments (e.g., consistency, interpretability, domain expertise)."
        },
        {
            "name": "advantages_of_llm_judge",
            "type": "str",
            "description": "Any reported benefits of using LLM judges compared to humans (e.g., speed, cost, scalability)."
        },
        {
            "name": "experimental_setting",
            "type": "str",
            "description": "Key details of the experimental setup such as prompt format, number of judges per sample, and evaluation protocol."
        }
    ],
    "extraction_query": "Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
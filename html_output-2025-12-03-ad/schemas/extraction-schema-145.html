<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-145 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-145</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-145</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>Name of the large language model that generated the scientific theory (e.g., GPT-4, Claude, PaLM).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Size or version of the model (e.g., 175B parameters, 13B, v1.5).</td>
                    </tr>
                    <tr>
                        <td><strong>scientific_domain</strong></td>
                        <td>str</td>
                        <td>Domain of the generated theory or hypothesis (e.g., physics, biology, chemistry, computer science).</td>
                    </tr>
                    <tr>
                        <td><strong>theory_type</strong></td>
                        <td>str</td>
                        <td>Type of scientific output produced (e.g., hypothesis, model, explanation, experimental design).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method_name</strong></td>
                        <td>str</td>
                        <td>Name of the evaluation method or framework used (e.g., Human expert rating, Falsifiability test, Citation prediction benchmark).</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_method_description</strong></td>
                        <td>str</td>
                        <td>Brief description of how the evaluation method works.</td>
                    </tr>
                    <tr>
                        <td><strong>evaluation_metric</strong></td>
                        <td>str</td>
                        <td>Specific metric(s) reported for the evaluation (e.g., plausibility score, accuracy, novelty score, citation count, reproducibility rate).</td>
                    </tr>
                    <tr>
                        <td><strong>metric_definition</strong></td>
                        <td>str</td>
                        <td>Definition of the metric, including units or scale (e.g., 0–100 plausibility rating, % of reproducible results).</td>
                    </tr>
                    <tr>
                        <td><strong>dataset_or_benchmark</strong></td>
                        <td>str</td>
                        <td>Name of the dataset, benchmark, or corpus used for evaluation (e.g., SciBench, PubMedQA, arXiv benchmark).</td>
                    </tr>
                    <tr>
                        <td><strong>human_evaluation_details</strong></td>
                        <td>str</td>
                        <td>Details of any human assessment (e.g., number of expert reviewers, rating scale, inter‑rater agreement).</td>
                    </tr>
                    <tr>
                        <td><strong>automated_falsifiability_check</strong></td>
                        <td>bool</td>
                        <td>Whether the evaluation includes an automated test of falsifiability (true, false, or null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>reproducibility_assessment</strong></td>
                        <td>bool</td>
                        <td>Whether reproducibility of the generated theory is assessed (true, false, or null if not reported).</td>
                    </tr>
                    <tr>
                        <td><strong>reported_results</strong></td>
                        <td>str</td>
                        <td>Quantitative results reported for the evaluation (e.g., "plausibility 78% vs. human baseline 85%").</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_to_human_generated</strong></td>
                        <td>bool</td>
                        <td>Whether the paper compares LLM‑generated theories to human‑generated ones (true, false, or null).</td>
                    </tr>
                    <tr>
                        <td><strong>comparison_results</strong></td>
                        <td>str</td>
                        <td>Summary of comparative outcomes if a comparison is made (e.g., "LLM hypotheses were 10% less novel than human hypotheses").</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_noted</strong></td>
                        <td>str</td>
                        <td>Any limitations, challenges, or caveats reported regarding the evaluation approach.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-145",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "Name of the large language model that generated the scientific theory (e.g., GPT-4, Claude, PaLM)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Size or version of the model (e.g., 175B parameters, 13B, v1.5)."
        },
        {
            "name": "scientific_domain",
            "type": "str",
            "description": "Domain of the generated theory or hypothesis (e.g., physics, biology, chemistry, computer science)."
        },
        {
            "name": "theory_type",
            "type": "str",
            "description": "Type of scientific output produced (e.g., hypothesis, model, explanation, experimental design)."
        },
        {
            "name": "evaluation_method_name",
            "type": "str",
            "description": "Name of the evaluation method or framework used (e.g., Human expert rating, Falsifiability test, Citation prediction benchmark)."
        },
        {
            "name": "evaluation_method_description",
            "type": "str",
            "description": "Brief description of how the evaluation method works."
        },
        {
            "name": "evaluation_metric",
            "type": "str",
            "description": "Specific metric(s) reported for the evaluation (e.g., plausibility score, accuracy, novelty score, citation count, reproducibility rate)."
        },
        {
            "name": "metric_definition",
            "type": "str",
            "description": "Definition of the metric, including units or scale (e.g., 0–100 plausibility rating, % of reproducible results)."
        },
        {
            "name": "dataset_or_benchmark",
            "type": "str",
            "description": "Name of the dataset, benchmark, or corpus used for evaluation (e.g., SciBench, PubMedQA, arXiv benchmark)."
        },
        {
            "name": "human_evaluation_details",
            "type": "str",
            "description": "Details of any human assessment (e.g., number of expert reviewers, rating scale, inter‑rater agreement)."
        },
        {
            "name": "automated_falsifiability_check",
            "type": "bool",
            "description": "Whether the evaluation includes an automated test of falsifiability (true, false, or null if not reported)."
        },
        {
            "name": "reproducibility_assessment",
            "type": "bool",
            "description": "Whether reproducibility of the generated theory is assessed (true, false, or null if not reported)."
        },
        {
            "name": "reported_results",
            "type": "str",
            "description": "Quantitative results reported for the evaluation (e.g., \"plausibility 78% vs. human baseline 85%\")."
        },
        {
            "name": "comparison_to_human_generated",
            "type": "bool",
            "description": "Whether the paper compares LLM‑generated theories to human‑generated ones (true, false, or null)."
        },
        {
            "name": "comparison_results",
            "type": "str",
            "description": "Summary of comparative outcomes if a comparison is made (e.g., \"LLM hypotheses were 10% less novel than human hypotheses\")."
        },
        {
            "name": "limitations_noted",
            "type": "str",
            "description": "Any limitations, challenges, or caveats reported regarding the evaluation approach."
        }
    ],
    "extraction_query": "Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
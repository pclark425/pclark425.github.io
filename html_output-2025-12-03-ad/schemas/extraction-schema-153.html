<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-153 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-153</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-153</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model (LLM) being evaluated (e.g., GPT-4, PaLM, Llama-2, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>model_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the language model, including architecture, training data, or other relevant details.</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model, in parameters (e.g., 7B, 13B, 70B, etc.), if available.</td>
                    </tr>
                    <tr>
                        <td><strong>puzzle_name</strong></td>
                        <td>str</td>
                        <td>The name of the puzzle game being used (e.g., Sudoku, Rubik's Cube, Minesweeper, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>puzzle_type</strong></td>
                        <td>str</td>
                        <td>A brief description of the type of puzzle and the spatial knowledge required (e.g., 'grid-based logic puzzle', '3D spatial manipulation', etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>task_setup</strong></td>
                        <td>str</td>
                        <td>A concise description of how the puzzle task is presented to the model (e.g., input/output format, prompt engineering, use of chain-of-thought, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>mechanisms_or_strategies</strong></td>
                        <td>str</td>
                        <td>Any described mechanisms, strategies, or reasoning approaches the model uses to solve the puzzle (e.g., chain-of-thought, symbolic manipulation, spatial representation, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metrics</strong></td>
                        <td>str</td>
                        <td>Reported performance of the model on the puzzle task (e.g., accuracy, success rate, number of puzzles solved, etc., with numerical values and units if available).</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_of_spatial_reasoning</strong></td>
                        <td>str</td>
                        <td>Any evidence or analysis indicating whether the model is using spatial reasoning (e.g., ablation studies, probing, qualitative analysis, etc.).</td>
                    </tr>
                    <tr>
                        <td><strong>comparisons</strong></td>
                        <td>str</td>
                        <td>Any comparisons to other models, ablations, or human performance on the same puzzle tasks.</td>
                    </tr>
                    <tr>
                        <td><strong>limitations_or_failure_cases</strong></td>
                        <td>str</td>
                        <td>Any reported limitations, failure cases, or counter-examples where the model fails to solve the puzzle or does not exhibit spatial reasoning.</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-153",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model (LLM) being evaluated (e.g., GPT-4, PaLM, Llama-2, etc.)."
        },
        {
            "name": "model_description",
            "type": "str",
            "description": "A brief description of the language model, including architecture, training data, or other relevant details."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model, in parameters (e.g., 7B, 13B, 70B, etc.), if available."
        },
        {
            "name": "puzzle_name",
            "type": "str",
            "description": "The name of the puzzle game being used (e.g., Sudoku, Rubik's Cube, Minesweeper, etc.)."
        },
        {
            "name": "puzzle_type",
            "type": "str",
            "description": "A brief description of the type of puzzle and the spatial knowledge required (e.g., 'grid-based logic puzzle', '3D spatial manipulation', etc.)."
        },
        {
            "name": "task_setup",
            "type": "str",
            "description": "A concise description of how the puzzle task is presented to the model (e.g., input/output format, prompt engineering, use of chain-of-thought, etc.)."
        },
        {
            "name": "mechanisms_or_strategies",
            "type": "str",
            "description": "Any described mechanisms, strategies, or reasoning approaches the model uses to solve the puzzle (e.g., chain-of-thought, symbolic manipulation, spatial representation, etc.)."
        },
        {
            "name": "performance_metrics",
            "type": "str",
            "description": "Reported performance of the model on the puzzle task (e.g., accuracy, success rate, number of puzzles solved, etc., with numerical values and units if available)."
        },
        {
            "name": "evidence_of_spatial_reasoning",
            "type": "str",
            "description": "Any evidence or analysis indicating whether the model is using spatial reasoning (e.g., ablation studies, probing, qualitative analysis, etc.)."
        },
        {
            "name": "comparisons",
            "type": "str",
            "description": "Any comparisons to other models, ablations, or human performance on the same puzzle tasks."
        },
        {
            "name": "limitations_or_failure_cases",
            "type": "str",
            "description": "Any reported limitations, failure cases, or counter-examples where the model fails to solve the puzzle or does not exhibit spatial reasoning."
        }
    ],
    "extraction_query": "Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.",
    "supporting_theory_ids": [],
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-126 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-126</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-126</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model evaluated (e.g., GPT-4, PaLM 2, LLaMA-13B).</td>
                    </tr>
                    <tr>
                        <td><strong>model_family</strong></td>
                        <td>str</td>
                        <td>The broader family or architecture of the model (e.g., transformer, decoder‑only, encoder‑decoder).</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>Parameter count or size descriptor (e.g., 7B, 70B, 540B).</td>
                    </tr>
                    <tr>
                        <td><strong>training_data_description</strong></td>
                        <td>str</td>
                        <td>Brief description of the training data relevant to mathematics or numeric reasoning (e.g., includes code, math textbooks, web data).</td>
                    </tr>
                    <tr>
                        <td><strong>benchmark_name</strong></td>
                        <td>str</td>
                        <td>The name of the arithmetic benchmark or dataset used for evaluation (e.g., GSM8K, MATH, AQUA, BIG-Bench Arithmetic).</td>
                    </tr>
                    <tr>
                        <td><strong>task_type</strong></td>
                        <td>str</td>
                        <td>Specific arithmetic task evaluated (e.g., addition, subtraction, multiplication, division, multi‑step word problems, algebra, geometry).</td>
                    </tr>
                    <tr>
                        <td><strong>problem_format</strong></td>
                        <td>str</td>
                        <td>How the problem is presented to the model (e.g., natural‑language question, symbolic expression, multiple‑choice).</td>
                    </tr>
                    <tr>
                        <td><strong>difficulty_level</strong></td>
                        <td>str</td>
                        <td>Reported difficulty or grade level of the problems (e.g., 2‑digit addition, grade‑8 algebra, hard).</td>
                    </tr>
                    <tr>
                        <td><strong>prompting_method</strong></td>
                        <td>str</td>
                        <td>Prompting strategy used (e.g., zero‑shot, few‑shot, chain‑of‑thought, self‑consistency, tool‑use, program synthesis).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_metric</strong></td>
                        <td>str</td>
                        <td>Metric reported for the task (e.g., accuracy, exact match, error rate, mean absolute error).</td>
                    </tr>
                    <tr>
                        <td><strong>performance_value</strong></td>
                        <td>str</td>
                        <td>Numeric result for the metric, including units if applicable (e.g., 78% accuracy, 0.12 MAE).</td>
                    </tr>
                    <tr>
                        <td><strong>internal_analysis</strong></td>
                        <td>str</td>
                        <td>Summary of any interpretability findings about how the model processes numbers (e.g., attention to digit tokens, activation probing of numeric concepts, logit‑lens observations).</td>
                    </tr>
                    <tr>
                        <td><strong>failure_modes</strong></td>
                        <td>str</td>
                        <td>Common error patterns observed (e.g., off‑by‑one, digit swapping, overflow, mis‑ordering of operations).</td>
                    </tr>
                    <tr>
                        <td><strong>scaling_trend</strong></td>
                        <td>str</td>
                        <td>Description of how performance changes with model size, data scale, or finetuning (e.g., emergent jump at 30B parameters, linear improvement with data).</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-126",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model evaluated (e.g., GPT-4, PaLM 2, LLaMA-13B)."
        },
        {
            "name": "model_family",
            "type": "str",
            "description": "The broader family or architecture of the model (e.g., transformer, decoder‑only, encoder‑decoder)."
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "Parameter count or size descriptor (e.g., 7B, 70B, 540B)."
        },
        {
            "name": "training_data_description",
            "type": "str",
            "description": "Brief description of the training data relevant to mathematics or numeric reasoning (e.g., includes code, math textbooks, web data)."
        },
        {
            "name": "benchmark_name",
            "type": "str",
            "description": "The name of the arithmetic benchmark or dataset used for evaluation (e.g., GSM8K, MATH, AQUA, BIG-Bench Arithmetic)."
        },
        {
            "name": "task_type",
            "type": "str",
            "description": "Specific arithmetic task evaluated (e.g., addition, subtraction, multiplication, division, multi‑step word problems, algebra, geometry)."
        },
        {
            "name": "problem_format",
            "type": "str",
            "description": "How the problem is presented to the model (e.g., natural‑language question, symbolic expression, multiple‑choice)."
        },
        {
            "name": "difficulty_level",
            "type": "str",
            "description": "Reported difficulty or grade level of the problems (e.g., 2‑digit addition, grade‑8 algebra, hard)."
        },
        {
            "name": "prompting_method",
            "type": "str",
            "description": "Prompting strategy used (e.g., zero‑shot, few‑shot, chain‑of‑thought, self‑consistency, tool‑use, program synthesis)."
        },
        {
            "name": "performance_metric",
            "type": "str",
            "description": "Metric reported for the task (e.g., accuracy, exact match, error rate, mean absolute error)."
        },
        {
            "name": "performance_value",
            "type": "str",
            "description": "Numeric result for the metric, including units if applicable (e.g., 78% accuracy, 0.12 MAE)."
        },
        {
            "name": "internal_analysis",
            "type": "str",
            "description": "Summary of any interpretability findings about how the model processes numbers (e.g., attention to digit tokens, activation probing of numeric concepts, logit‑lens observations)."
        },
        {
            "name": "failure_modes",
            "type": "str",
            "description": "Common error patterns observed (e.g., off‑by‑one, digit swapping, overflow, mis‑ordering of operations)."
        },
        {
            "name": "scaling_trend",
            "type": "str",
            "description": "Description of how performance changes with model size, data scale, or finetuning (e.g., emergent jump at 30B parameters, linear improvement with data)."
        }
    ],
    "extraction_query": "Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.",
    "supporting_theory_ids": [],
    "model_str": "openrouter/openai/gpt-oss-120b"
}</code></pre>
        </div>
    </div>
</body>
</html>
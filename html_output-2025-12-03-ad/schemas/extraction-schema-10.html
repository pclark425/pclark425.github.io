<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Schema extraction-schema-10 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extraction Schema Details for extraction-schema-10</h1>

        <div class="section">
            <h2>Extraction Schema (General Information)</h2>
            <div class="info-section">
                <p><strong>Schema ID:</strong> extraction-schema-10</p>
                <p><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</p>
            </div>
        </div>

        <div class="section">
            <h2>Extraction Schema (Details)</h2>
            <table>
                <thead>
                    <tr>
                        <th style="width: 20%;">Field Name</th>
                        <th style="width: 15%;">Type</th>
                        <th style="width: 65%;">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>model_name</strong></td>
                        <td>str</td>
                        <td>The name of the language model being evaluated (e.g., GPT-3, GPT-4, LLaMA, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>model_size</strong></td>
                        <td>str</td>
                        <td>The size of the model in parameters (e.g., 1B, 7B, 13B, 175B, etc.). Null if not specified.</td>
                    </tr>
                    <tr>
                        <td><strong>task_description</strong></td>
                        <td>str</td>
                        <td>A brief description of the task or question type being evaluated (e.g., factual QA, reasoning, classification, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_type</strong></td>
                        <td>str</td>
                        <td>What type of evidence was provided in the prompt? (e.g., factual statements, reasoning chains, examples, retrieved documents, contradictory information, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_source</strong></td>
                        <td>str</td>
                        <td>Where did the evidence come from? (e.g., ground truth facts, retrieved documents, synthetic/generated, human-provided, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>parametric_knowledge_alignment</strong></td>
                        <td>str</td>
                        <td>Does the provided evidence align with, contradict, or is neutral relative to the model's parametric knowledge? (aligned/contradictory/neutral/mixed/unknown)</td>
                    </tr>
                    <tr>
                        <td><strong>performance_without_evidence</strong></td>
                        <td>str</td>
                        <td>Model performance or confidence/probability when NO evidence is provided in the prompt (baseline using only parametric knowledge). Include numerical values and units if available.</td>
                    </tr>
                    <tr>
                        <td><strong>performance_with_evidence</strong></td>
                        <td>str</td>
                        <td>Model performance or confidence/probability when evidence IS provided in the prompt. Include numerical values and units if available.</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_effect</strong></td>
                        <td>str</td>
                        <td>What was the effect of adding evidence? (positive/negative/neutral/mixed) Be specific about whether accuracy, confidence, or probability increased, decreased, or stayed the same.</td>
                    </tr>
                    <tr>
                        <td><strong>evidence_decreased_confidence</strong></td>
                        <td>bool</td>
                        <td>Did adding evidence specifically DECREASE the model's confidence or probability assessment compared to baseline? (true/false/null)</td>
                    </tr>
                    <tr>
                        <td><strong>proposed_mechanism</strong></td>
                        <td>str</td>
                        <td>What mechanism or explanation does the paper propose for why evidence had the observed effect? (e.g., hallucination detection, contradiction detection, sycophancy, prompt sensitivity, etc.)</td>
                    </tr>
                    <tr>
                        <td><strong>key_findings</strong></td>
                        <td>str</td>
                        <td>Summarize the key findings about how the model integrated (or failed to integrate) the provided evidence with its parametric knowledge. Be concise but information-dense.</td>
                    </tr>
                    <tr>
                        <td><strong>counterintuitive_behavior</strong></td>
                        <td>bool</td>
                        <td>Does the paper report counterintuitive behavior where evidence had unexpected negative effects? (true/false/null)</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Extraction Schema (Debug)</h2>
            <pre><code>{
    "id": "extraction-schema-10",
    "schema": [
        {
            "name": "model_name",
            "type": "str",
            "description": "The name of the language model being evaluated (e.g., GPT-3, GPT-4, LLaMA, etc.)"
        },
        {
            "name": "model_size",
            "type": "str",
            "description": "The size of the model in parameters (e.g., 1B, 7B, 13B, 175B, etc.). Null if not specified."
        },
        {
            "name": "task_description",
            "type": "str",
            "description": "A brief description of the task or question type being evaluated (e.g., factual QA, reasoning, classification, etc.)"
        },
        {
            "name": "evidence_type",
            "type": "str",
            "description": "What type of evidence was provided in the prompt? (e.g., factual statements, reasoning chains, examples, retrieved documents, contradictory information, etc.)"
        },
        {
            "name": "evidence_source",
            "type": "str",
            "description": "Where did the evidence come from? (e.g., ground truth facts, retrieved documents, synthetic/generated, human-provided, etc.)"
        },
        {
            "name": "parametric_knowledge_alignment",
            "type": "str",
            "description": "Does the provided evidence align with, contradict, or is neutral relative to the model's parametric knowledge? (aligned/contradictory/neutral/mixed/unknown)"
        },
        {
            "name": "performance_without_evidence",
            "type": "str",
            "description": "Model performance or confidence/probability when NO evidence is provided in the prompt (baseline using only parametric knowledge). Include numerical values and units if available."
        },
        {
            "name": "performance_with_evidence",
            "type": "str",
            "description": "Model performance or confidence/probability when evidence IS provided in the prompt. Include numerical values and units if available."
        },
        {
            "name": "evidence_effect",
            "type": "str",
            "description": "What was the effect of adding evidence? (positive/negative/neutral/mixed) Be specific about whether accuracy, confidence, or probability increased, decreased, or stayed the same."
        },
        {
            "name": "evidence_decreased_confidence",
            "type": "bool",
            "description": "Did adding evidence specifically DECREASE the model's confidence or probability assessment compared to baseline? (true/false/null)"
        },
        {
            "name": "proposed_mechanism",
            "type": "str",
            "description": "What mechanism or explanation does the paper propose for why evidence had the observed effect? (e.g., hallucination detection, contradiction detection, sycophancy, prompt sensitivity, etc.)"
        },
        {
            "name": "key_findings",
            "type": "str",
            "description": "Summarize the key findings about how the model integrated (or failed to integrate) the provided evidence with its parametric knowledge. Be concise but information-dense."
        },
        {
            "name": "counterintuitive_behavior",
            "type": "bool",
            "description": "Does the paper report counterintuitive behavior where evidence had unexpected negative effects? (true/false/null)"
        }
    ],
    "extraction_query": "Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.",
    "supporting_theory_ids": [],
    "model_str": null
}</code></pre>
        </div>
    </div>
</body>
</html>
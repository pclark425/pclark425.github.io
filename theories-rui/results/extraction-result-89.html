<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-89 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-89</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-89</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-6aff3d48e80e4780b232c7c2ec25be2e004fe842</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6aff3d48e80e4780b232c7c2ec25be2e004fe842" target="_blank">A framework for studying synaptic plasticity with neural spike train data</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work treats synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM) and provides an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules.</p>
                <p><strong>Paper Abstract:</strong> Learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. The computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. Until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. However, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. Here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. We treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM). In addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules. Using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a substantial role. On synthetic data generated from the biophysical simulator NEURON, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e89.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e89.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STDP (additive, double-exponential)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-Timing-Dependent Plasticity (canonical additive double-exponential form)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A timing-based Hebbian learning rule in which weight updates occur only at pre- or post-synaptic spike times and the magnitude of each update depends nonlinearly on the pre-post spike interval via double-exponential kernels parameterized by A_+, A_-, tau_+, and tau_-.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Competitive Hebbian learning through spike-timing-dependent synaptic plasticitye.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>STDP (additive)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Weight changes occur at spike times: at a postsynaptic spike t the weight is increased by ell_+ = sum_{presyn spikes < t} A_+ exp((t - s_pres)/tau_+); at a presynaptic spike t the weight is decreased by ell_- = sum_{post spikes < t} A_- exp((t - s_post)/tau_-). The additive canonical rule updates are independent of current weight (except when bounds are enforced externally); parameters are A_+, A_-, tau_+, tau_-.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Fast updates at individual spike times (milliseconds for inter-spike timing effects), with learning-rule time constants tau_+, tau_- (treated in this paper with units of seconds in priors but capturing subsecond-to-second decay of eligibility); weight trajectories tracked over seconds to minutes (simulations run for tens of seconds). The paper also notes plasticity occurs over a variety of time scales (milliseconds to days) in general.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Not specified to a particular biological region; evaluated on simulated, sparsely connected populations (GLM and NEURON simulations) of generic excitatory/inhibitory neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Applied to sparse, directed synaptic networks (Erdős–Rényi prior for adjacency), including chains of excitatory synapses and self-inhibitory (refractory) connections in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative/timing-dependent synaptic learning (Hebbian-type; supports strengthening or weakening of monosynaptic connections based on spike timing).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational model: canonical STDP was implemented as a parametric learning rule in the time-varying GLM and used to generate synthetic spike data and to fit inferred weight trajectories; comparisons made with NEURON biophysical simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast (ms) pairwise spike-timing updates accumulate to produce weight trajectories observable over seconds to tens of seconds; time constants control decay of eligibility contributions (tau_+, tau_-). The paper shows that rapid spike-driven updates can lead to slower saturation of weights (over the recording duration), which then limits ability to distinguish different STDP variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The time-varying GLM with an additive STDP learning rule can recover weight trajectories and learning-rule parameters when that rule generated the data; when weights saturate (after substantial strengthening) the model loses the ability to distinguish additive from alternative rules; additive STDP captures qualitative trends in simulated data and yields improved predictive likelihood relative to static models when weights vary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Used predictive log likelihood on held-out data as model-comparison metric; in NEURON simulations the additive time-varying GLM detected synapses with high ROC AUC (AUC = 0.99 for detecting 28 excitatory synapses). Simulation specifics: 60 s recordings, 1 ms temporal resolution; in GLM simulations weights updated every 1 s.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit memory consolidation mechanism described; paper discusses accumulation of fast spike-timing updates into slower weight trajectories and eventual saturation but does not describe a separate consolidation process transferring fast to slow memory stores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A framework for studying synaptic plasticity with neural spike train data', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e89.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e89.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multiplicative STDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiplicative Spike-Timing-Dependent Plasticity (bounded multiplicative rule)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A timing-based STDP variant that bounds weights between W_min and W_max and scales the magnitude of potentiation/depression by distance to those bounds, producing multiplicative dependence on current weight.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Phenomenological models of synaptic plasticity based on spike timing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>STDP (multiplicative, bounded)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>At a postsynaptic spike the potentiation term is scaled by (W_max - W(t)) and at a presynaptic spike the depression term is scaled by (W(t) - W_min); the timing-dependent kernels are the same form as additive STDP but the effective update amplitude multiplies a factor that depends on current weight. Parameters include A_+, A_-, tau_+, tau_-, W_max, W_min and clipped/truncated noise to respect bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Instantaneous updates at spike times (ms-scale inter-spike timing dependence); weight evolution and approach to saturation occur over seconds to tens of seconds in the simulations; bounds introduce slower approach-to-bound dynamics (longer timescale effects observable across the recording).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Simulated generic neural populations (NEURON and GLM-generated data), not tied to a specific brain region.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Applied on sparse directed networks; used to model strengthening/weakening of individual synapses while preserving global sparsity priors (Erdős–Rényi).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative, timing-dependent synaptic learning with normalization via multiplicative scaling toward bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model implemented in the time-varying GLM and evaluated on synthetic GLM data; compared against additive STDP and static models.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast spike-timing updates are modulated by the current weight via multiplicative factors, which slows or speeds changes depending on proximity to bounds; thus fast timing effects (ms) interact with slower saturation dynamics (seconds to recording length) governed by W_max/W_min.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multiplicative STDP was implemented and inferred; when true dynamics followed multiplicative STDP the multiplicative model provided better fits than static models; however, when weights saturate the multiplicative and additive STDP models can become indistinguishable in predictive likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Predictive log likelihood comparisons across models; running-time characteristics reported (e.g., ~2 iterations/s for two-neuron examples, ~20 s/iteration for ten neurons), but no single scalar learning-rate metric; inability to distinguish multiplicative vs additive when weights saturate is emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation described; the multiplicative scaling toward hard bounds functions as a slower constraint on fast updates but no multi-stage consolidation process is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A framework for studying synaptic plasticity with neural spike train data', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e89.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e89.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Additive bounded STDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Additive STDP with hard bounds (bounded additive rule)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An additive STDP variant that is identical to the canonical additive rule except updates are thresholded so weights are kept within specified W_min and W_max bounds; update magnitude does not depend on current weight except at the boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>STDP (additive, bounded)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Same timing-dependent double-exponential update kernels as canonical additive STDP, but after a proposed additive update the weight is clipped to remain within [W_min, W_max]; noise model typically uses truncated Gaussian to enforce bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Fast spike-time-driven updates (ms scale) with system-level weight evolution observed over seconds to tens of seconds; bounds produce slower changes once weights approach limits.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic simulated neural populations (GLM and NEURON simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Sparse directed synaptic networks; used in simulations including 10-neuron networks with 28 excitatory synapses.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative/timing-dependent synaptic learning with explicit bounds limiting long-term growth.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational model used in simulations and inference experiments (GLM and NEURON simulated data).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast timing-dependent additive updates accumulate over the recording epoch; clipping at bounds introduces a slower regime where further fast potentiation/depression has no effect (saturation), reducing identifiability of underlying rule.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Additive bounded STDP was used both to generate data and as a candidate inference model; the time-varying GLM recovered weight trajectories when the additive bounded rule generated the data, and detected synapses that static models missed (notably synapses that decay over the recording).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Predictive log likelihood for held-out data used to compare models; additive models often yield higher predictive likelihood when weights truly vary. NEURON synapse detection AUC = 0.99 (additive time-varying GLM).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation; bounding acts as a saturation mechanism that halts further net change beyond longer-term thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A framework for studying synaptic plasticity with neural spike train data', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e89.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e89.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oja rule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oja's rule (rate-based Hebbian learning / principal component analyzer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rate-based synaptic normalization rule in which synaptic strengths are adjusted according to a Hebbian term with an additional normalization term that scales with total incoming synaptic strength, effectively performing principal component extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simplified neuron model as a principal component analyzer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Oja rule</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Rate-based update that depends on pre- and post-synaptic firing rates: dW ∝ y x - y^2 W (where x is presynaptic activity and y is postsynaptic activity), implementing a normalization of incoming weights so that competition among inputs is enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates on slower, rate-averaged timescales (seconds to minutes or longer) compared to spike-timing rules; paper treats it conceptually as a slower, normalization-based learning rule distinct from spike-timing-driven STDP.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Mentioned generically (not applied to a specific brain region) as an example of rate-based plasticity in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Imposes interactions among incoming weights to a postsynaptic neuron (normalization across a column of weights); relevant to recurrent and feedforward circuits where incoming-synapse competition shapes connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Rate-based associative learning / principal component extraction; enforces competition and normalization among inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mentioned as a conceptual alternative learning rule; not used in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Implicit: Oja-like normalization would interact with faster spike-timing updates by slowly scaling and normalizing weights over longer epochs, but the paper does not model or quantify such interactions experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Only mentioned as an example of rate-based learning rules that the general dynamical-system learning-rule framework can encompass; no empirical results in this paper concerning Oja dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A framework for studying synaptic plasticity with neural spike train data', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e89.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e89.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse time-varying GLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Time-Varying Generalized Linear Model for spike trains</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully-Bayesian extension of the GLM that factors impulse responses into adjacency (A), time-varying weights W(t), and normalized impulse-shape r(Δt), and embeds parametric synaptic plasticity dynamical systems as priors for W(t); inference performed via particle MCMC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Model factorization: h_{n'→n}(Δt,t) = A_{n'→n} · W_{n'→n}(t) · r_{n'→n}(Δt). W(t) evolves according to a chosen parametric learning rule ℓ(W(t), past spikes) plus noise ε; r are normalized convex combinations of basis impulse shapes; A has a spike-and-slab (Bernoulli) prior (Erdős–Rényi).
Inference uses particle MCMC to jointly sample weight trajectories, adjacency, GLM parameters, and learning-rule parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Model explicitly represents three temporal scales: (1) fast spike timing within impulse response r(Δt) (ms to 100 ms; Δt_max in simulations = 100 ms), (2) learning-rule-driven weight evolution W(t) updated discretely during simulation (GLM sims updated weights every 1 s; NEURON recorded weights every 1 ms), and (3) longer recording epochs (tens of seconds) over which saturation and slower changes occur. The paper also notes general biological plasticity across ms to days.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic simulated circuits (GLM and NEURON-simulated networks); motivated by application to optically recorded cortical microcircuits but not applied to specific brain region data here.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Encodes sparsity explicitly via adjacency matrix A with Erdős–Rényi prior; supports chain motifs, sparse excitatory-inhibitory networks, and self-inhibitory refractoriness.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Framework designed to study timing-based and rate-based synaptic learning rules (e.g., STDP variants, Oja), enabling inference about associative/timing-dependent learning from spike trains.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/modeling framework with evaluations on synthetic GLM-generated spike trains and biophysical NEURON simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Framework couples fast impulse responses (r(Δt)) that convert spike timing into instantaneous effects with slower weight dynamics W(t) governed by a chosen learning rule ℓ; particle-MCMC inference recovers how fast spike correlations (ms-scale) drive gradual weight changes (seconds to minutes) and eventual saturation (longer timescales), enabling analysis of interactions across these scales.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The sparse time-varying GLM can (i) recover weight trajectories and learning-rule parameters from synthetic and NEURON-simulated data, (ii) detect synapses that static GLMs miss (notably decaying synapses), and (iii) show that when weights saturate predictive likelihood may not distinguish different STDP variants—motivating optimal experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>ROC AUC for synapse detection (Add. STDP AUC = 0.99 on NEURON data); predictive log likelihood on held-out 10 s used for model comparison; computational performance numbers (e.g., ~2 iterations/s for 2-neuron case, ~20 s/iteration for 10 neurons) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation mechanism implemented; framework can represent both fast spike-timing driven updates and slower accumulation/saturation of weights, but transfer/consolidation between distinct memory systems is not modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A framework for studying synaptic plasticity with neural spike train data', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e89.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e89.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Connectivity patterns (sparse/Erdős–Rényi & motifs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse connectivity with Erdős–Rényi prior, chain motifs, and self-inhibitory refractoriness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper models network wiring as a sparse directed adjacency matrix A (spike-and-slab), using an Erdős–Rényi prior for edge existence, and considers motifs such as chains of excitatory synapses and self-inhibitory connections to mimic refractoriness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Connectivity is structural (static or slowly varying via W(t) modulation) while functional impact manifests on fast timescales via impulse responses r(Δt) (ms–100 ms); structural inference integrates spike data over seconds to tens of seconds to infer presence/absence and time-varying strength.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic simulated circuits (GLM and NEURON networks); intended for application to optically recorded cortical microcircuits but not tied to a particular brain region in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Explicit sparse directed graph with independent Bernoulli edges (Erdős–Rényi sparsity parameter ρ); motifs studied include excitatory chains (1→2→3→4) and self-inhibitory (refractory) connections.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports study of associative/timing-dependent learning at the synapse level and network-level effects where connectivity motifs and sparse wiring modulate propagation and reinforcement of activity patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Modeled computationally within the time-varying GLM and validated on simulations (GLM and NEURON); adjacency inferred jointly with weights via particle MCMC.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Structural sparsity (A) is inferred from data accumulated over long epochs while instantaneous functional interactions (r(Δt)) produce ms-scale spike correlations that drive learning; decaying or strengthening weights over seconds to minutes change effective connectivity over time.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Inferring a sparse, time-varying connectivity combined with plasticity rules improves detection of real synapses compared to static GLMs and cross-correlation, particularly for synapses that weaken over the recording; marginalizing out weight trajectories improves adjacency sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Synapse detection ROC AUC = 0.99 (additive STDP time-varying GLM on NEURON data); posterior mean of A used to rank edges.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A framework for studying synaptic plasticity with neural spike train data', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e89.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e89.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Refractory self-connections</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-inhibitory (refractoriness) GLM weights</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Self-connection weights in the GLM implement neuronal refractoriness via negative impulse responses; these were modeled as static with Gaussian priors and not typically allowed to change over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Implements fast post-spike refractory effects via impulse responses r(Δt) on the order of ms to 100 ms (Δt_max set to 100 ms in simulations); treated as static across the recording in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic simulated neurons in GLM and NEURON simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Self-edges (n→n) with negative weight to model refractoriness; these were held static and sampled under Gaussian priors rather than via W(t).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Not a learning mechanism; rather a short-timescale biophysical constraint (refractoriness) shaping immediate spike probability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Model component within the GLM; handled analytically in inference (Gaussian prior, ARS/HMC sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Acts on the fastest timescale (immediate post-spike suppression) interacting with longer-timescale synaptic weight changes by shaping spike statistics that drive plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Modeling self-inhibitory weights as static with appropriate priors yields tractable inference and captures refractory effects which are important for accurate spike modeling and downstream plasticity inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A framework for studying synaptic plasticity with neural spike train data', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Competitive Hebbian learning through spike-timing-dependent synaptic plasticitye. <em>(Rating: 2)</em></li>
                <li>Phenomenological models of synaptic plasticity based on spike timing. <em>(Rating: 2)</em></li>
                <li>Spike timing-dependent plasticity: a Hebbian learning rule. <em>(Rating: 2)</em></li>
                <li>The spike-timing dependence of plasticity. <em>(Rating: 2)</em></li>
                <li>Inferring spike-timing-dependent plasticity from spike train data. <em>(Rating: 2)</em></li>
                <li>Simplified neuron model as a principal component analyzer. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
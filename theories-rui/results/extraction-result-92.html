<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-92 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-92</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-92</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-c4fe3d035ba350576f16282c805cba8b91fb5dd8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c4fe3d035ba350576f16282c805cba8b91fb5dd8" target="_blank">From statistical inference to a differential learning rule for stochastic neural networks</a></p>
                <p><strong>Paper Venue:</strong> Interface Focus</p>
                <p><strong>Paper TL;DR:</strong> A synaptic plasticity rule that relies only on delayed activity correlations, and that shows a number of remarkable features that can deal with correlated patterns, a broad range of architectures, one-shot learning with the palimpsest property, and the proliferation of spurious attractors.</p>
                <p><strong>Paper Abstract:</strong> Stochastic neural networks are a prototypical computational device able to build a probabilistic representation of an ensemble of external stimuli. Building on the relationship between inference and learning, we derive a synaptic plasticity rule that relies only on delayed activity correlations, and that shows a number of remarkable features. Our delayed-correlations matching (DCM) rule satisfies some basic requirements for biological feasibility: finite and noisy afferent signals, Dale’s principle and asymmetry of synaptic connections, locality of the weight update computations. Nevertheless, the DCM rule is capable of storing a large, extensive number of patterns as attractors in a stochastic recurrent neural network, under general scenarios without requiring any modification: it can deal with correlated patterns, a broad range of architectures (with or without hidden neuronal states), one-shot learning with the palimpsest property, all the while avoiding the proliferation of spurious attractors. When hidden units are present, our learning rule can be employed to construct Boltzmann machine-like generative models, exploiting the addition of hidden neurons in feature extraction and classification tasks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e92.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e92.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delayed-Correlations Matching learning rule</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biologically-motivated, local synaptic plasticity rule that updates weights by matching time-delayed (t+1 vs t) activity correlations measured under two nearby external field intensities, driving the autonomous network to reproduce stimulus-evoked dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Delayed-Correlations Matching (DCM)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Weight updates are proportional to the difference between empirical time-delayed correlations recorded during two consecutive windows with slightly different external field intensities: ΔJ_ij ∝ <s_i^{t+1} s_j^{t}>_{λ} - <s_i^{t+1} s_j^{t}>_{λ-Δλ>. Thresholds are updated similarly with single-site delayed activations. The rule is local to each synapse, works with asymmetric J_ij, tolerates noisy finite fields, and includes differential/homeostatic behavior (no change when driven and autonomous correlations already match). The rule reduces to pseudo-likelihood optimization and in the β→∞ limit recovers the perceptron rule.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Two interacting model timescales: (1) fast step-to-step stochastic dynamics where time-delayed correlations s^{t+1}s^{t} are measured (model time-step; in simulations T=20 steps per recording window, updates every 2T steps); (2) slower synaptic update timescale (updates applied after each 2T window and accumulated across cycles/presentations). Absolute biological times are not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Not tied to a specific brain region; studied as a generic stochastic recurrent network / attractor network (fully visible recurrent networks and extensions with hidden units, RBM-like visible↦hidden architectures).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Allows asymmetric recurrent couplings (J_{ij} ≠ J_{ji}), can be applied with sparse connectivity, excitatory/inhibitory segregation satisfying Dale's principle, and to visible-to-hidden directed synapses (restricted architectures).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory / attractor embedding, generative model learning (with hidden units), one-shot (online) learning with palimpsest properties, feature extraction and classification in generative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model: analytic derivation (KL divergence → update), mean-field approximations (TAP), and numerical simulations (discrete-time Glauber dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Explicit: fast stochastic activity (step transitions) is monitored via time-delayed correlations; slower synaptic plasticity updates aim to make autonomous dynamics reproduce stimulus-driven dynamics, thus transferring information from transient input-driven activity into persistent recurrent connectivity; inhibitory feedback and thresholds adapt on yet longer timescales to stabilize activity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DCM is local and biologically plausible (finite noisy fields, asymmetry allowed, Dale's principle compatible); it stores an extensive number of patterns as attractors across many scenarios (correlated patterns, sparse coding, hidden units) while strongly suppressing spurious attractors; it supports one-shot online learning with palimpsest capacity (extensive, ≈0.05N reported with regularization); it performs near or above Hopfield-like baselines depending on basin width and field intensity and remains effective even with relatively small external stimuli.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Reported metrics and parameters from simulations: network sizes N up to 800; retrieval noise-level χ (e.g., χ=0.3 in many tests); inverse temperature β=2 used in many experiments; recording window T=20 steps; Δλ=λ_max/3 in one experiment; maximum Hopfield capacity cited ~0.14N for comparison; DCM palimpsest capacity ≈0.05N (one-shot with regularization); pseudo-likelihood / perceptron limits referenced (α_c up to 2 at zero temperature in perceptron limit).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Iterative minimization of KL divergence between conditional transition distributions at two field strengths: matching delayed correlations causes recurrent weights to adapt so that the network's free (no-field) dynamics reproduces the stimulus-driven dynamics, effectively consolidating a transient stimulus into a persistent attractor; robustness can be increased by choosing λ_min (negative or positive) and by weight regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From statistical inference to a differential learning rule for stochastic neural networks', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e92.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e92.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hebbian</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hebbian plasticity (correlational LTP/LTD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic correlation-based synaptic modification where pre-post co-activation potentiates synapses and anti-correlation depresses them; used as baseline comparator in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Hebbian learning (and generalized Hebb adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Standard Hebb: ΔJ_{ij} ∝ s_i s_j (for clamped presentations). Generalized Hebbian variants for biased or sparse patterns include explicit normalization/homeostatic terms (cited adaptations). In the Hopfield framing the Hebb rule produces symmetric weight matrices and can embed patterns as attractors but is prone to catastrophic forgetting and spurious mixture states.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates as weight updates tied to stimulus presentations / clamped phases (in the paper this is contrasted with DCM's gradual finite-field protocol). The paper does not specify absolute biological timescales for Hebbian updates.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Used as a conceptual baseline for recurrent attractor networks (Hopfield-type fully recurrent networks).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Usually symmetric recurrent connectivity in classical Hopfield; variants applied to sparse and biased patterns; in constrained models excitatory-only plastic synapses combined with inhibitory feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory / attractor formation; developmental shaping of selectivity cited in introduction.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical (used as a comparative algorithm in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper highlights that classical Hebbian learning lacks the differential/homeostatic comparator present in DCM and can lead to uncontrolled positive feedback; no explicit multi-timescale interaction mechanism is proposed for Hebbian in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hebbian learning is capable of embedding attractors but is prone to catastrophic forgetting and producing spurious attractors; generalized Hebbian adjustments are needed for biased or sparse patterns and require knowledge of stimulus statistics, while DCM handles these without modification and with better performance in tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Hopfield (Hebbian) maximum storage load ~0.14N cited; generalized Hebbian adaptations required for biased patterns; in several figures DCM outperforms naive Hebb in capacity vs bias and in spurious attractor counts.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From statistical inference to a differential learning rule for stochastic neural networks', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e92.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e92.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Storkey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Storkey local learning rule</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A local update rule that adds corrective terms to naive Hebbian updates to limit weight growth and improve basin structure; tested by the authors in online settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Storkey learning rule</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>ΔJ_{ij} = ΔJ_{ji} ∝ (ξ_i ξ_j - h_i ξ_j - h_j ξ_i) where h_i = Σ_k J_{ik} ξ_k; the extra terms penalize weights when a memory is already stored, embedding a form of regularization in the local rule.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Online learning rule applied at presentation time; paper assesses it in finite-temperature retrieval setting but does not provide absolute biological timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Compared in the same recurrent stochastic attractor network context.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Symmetric or made symmetric in Storkey's original formulation; used as a baseline comparator against DCM in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory / attractor embedding, online learning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical comparison (simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>No explicit multi-timescale mechanism discussed in this paper for Storkey; tested in an online/one-shot context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>While Storkey includes a built-in regularization to curb weight growth, in the paper it fails under the strict finite-temperature retrieval criterion used (β=2), whereas DCM achieves robust embedding and palimpsest capacity in the same setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Reported failure on the paper's finite-temperature retrieval tests (β=2); no quantitative palimpsest capacity reported for Storkey within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From statistical inference to a differential learning rule for stochastic neural networks', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e92.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e92.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perceptron-limit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perceptron rule (β → ∞ limit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The deterministic zero-temperature limit of DCM/pseudo-likelihood recovers the perceptron update: only misclassified neuron incoming weights are updated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Perceptron learning rule</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>In the noise-free limit (β→∞), the DCM/pseudo-likelihood update reduces to perceptron steps: if ξ_i h_i < 0, update ΔJ_{ij} ∝ ξ_i ξ_j; variants include a robustness margin when λ_min<0.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Implicitly instantaneous deterministic update per presentation in the zero-noise limit; the paper uses it to connect theory rather than as a time-resolved biological mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Used as an analytical limiting case for fully-visible recurrent networks.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Fully connected incoming weights to each neuron (as in classical perceptron framing).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory encoding; theoretical capacity analysis (Gardner bound).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Analytic / theoretical limiting case within the model.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Not discussed beyond being the β→∞ instantaneous limit of the DCM multi-timescale derivation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Perceptron limit clarifies that DCM generalizes perceptron-like updates to stochastic finite-noise regimes and justifies capacity bounds (α_c=2 in zero-noise perceptron limit).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Gardner bound α_c = 2 cited for zero-temperature perceptron limit.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From statistical inference to a differential learning rule for stochastic neural networks', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e92.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e92.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dale & E/I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dale's principle and excitatory/inhibitory constraint</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Enforcing Dale's principle by partitioning neurons into excitatory and inhibitory populations with fixed sign outgoing synapses and plastic excitatory synapses, combined with inhibitory feedback to control activity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Paper enforces Dale's law by designating neurons excitatory or inhibitory; only excitatory outgoing synapses are plastic while inhibition is provided by specialized inhibitory mechanisms (global inhibitory unit, soft WTA, or adaptive thresholds). This reduces capacity but preserves functionality; thresholds and inhibitory feedback are tuned to maintain target sparsity f_v.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Inhibitory feedback and adaptive thresholds act on a slower regulatory timescale relative to fast step dynamics; synaptic plasticity remains on the DCM update timescale. Exact biological times are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic excitatory-inhibitory recurrent circuits (model-level), intended to be biologically plausible analogues of cortical E/I networks.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Two sub-populations (E/I), plastic excitatory-to-excitatory synapses, inhibitory network provides global or population-specific feedback; sparse connectivity also considered.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory and stable activity regulation; prevents runaway (epileptic) states and maintains desired activity sparsity during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical simulations implementing constrained synapses plus inhibitory schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Homeostatic inhibitory control and adaptive thresholds modulate activity over longer timescales relative to fast activity fluctuations and intermediate synaptic updates, stabilizing learned attractors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Introducing Dale constraints reduces theoretical capacity modestly but DCM combined with appropriate inhibitory feedback still achieves good storage performance; the paper demonstrates three inhibitory schemes that yield comparable retrieval capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Comparisons show required learning cycles and maximum storage load under Dale-constrained synapses; example simulation sizes N=200 and N=400 reported, with curves interrupted where algorithm fails.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From statistical inference to a differential learning rule for stochastic neural networks', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e92.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e92.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InhibitorySchemes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inhibitory feedback schemes (global unit, soft WTA, adaptive thresholds)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three implemented schemes that provide inhibitory regulation to maintain target activity levels: (i) a global inhibitory unit with elastic feedback; (ii) a soft winner-takes-all mechanism; (iii) locally adaptive thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Global inhibitory unit computes a feedback term I depending on population activities and drives mean input to achieve desired sparsity; soft WTA sorts local activities and applies an inhibitory threshold to keep the top fN active; adaptive thresholds shift neuron thresholds individually based on recent activity. These mechanisms are used to keep activity near f_v and prevent all-on/all-off states while excitatory synapses undergo plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>The schemes model regulatory processes acting on slower timescales than single-step activations (described as continuous-time phenomena for WTA and as adaptive over longer intervals for thresholds), but precise temporal units are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Model-level inhibitory regulation for excitatory recurrent networks; inspired by cortical E/I motifs and canonical microcircuits.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Global population-level inhibitory feedback (connectivity to all excitatory neurons), or implicit inhibitory effect implemented by sorted activations (soft WTA), or per-neuron threshold adjustments (local regulation).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports stable associative memory learning by controlling gain and sparsity during training and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical; analytic derivations provided for global inhibitory parameters and simulation implementations for all three schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>These inhibitory mechanisms act as longer-timescale stabilizers interacting with fast activity and intermediate DCM weight updates; authors derive parameter formulas assuming synaptic changes are adiabatic (slow) relative to immediate inhibitory corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>All three inhibitory schemes enabled DCM to function under Dale constraints and maintain performance comparable to unconstrained networks; global inhibition parameters and adaptive terms were derived analytically under Gaussian approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Simulations report comparable results across the three inhibitory schemes; example parameters and analytic expressions given (e.g., H_0, ν^{αβ}) but no absolute biological-time metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From statistical inference to a differential learning rule for stochastic neural networks', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e92.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e92.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RBM/CD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Restricted Boltzmann Machine / Contrastive Divergence (CD-k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hidden-unit extension: DCM generalizes to networks with hidden neurons and the resulting updates are closely related to RBM learning and to the CD-k positive/negative phase approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>DCM applied to visible↦hidden architectures; relation to Contrastive Divergence (CD-k)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>In the infinite-field limit DCM yields a pseudo-likelihood-like update for visible-to-hidden weights that requires sampling hidden states conditioned on clamped visible data (positive phase) and sampling under no-field dynamics (negative phase). The derived update resembles CD-k: first term samples P(s_hidden | s_visible=ξ) (positive), second term approximates free-phase correlations via short Gibbs chains (negative). Because DCM uses delayed correlations it applies to asymmetric kinetic models as well.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Two timescales: (1) fast Glauber dynamics for visible and hidden units (single-step transitions recorded as delayed correlations); (2) slower weight updates performed after windows; CD-like negative phase can be run for k Gibbs steps (k small in practice), so learning depends on the number of Gibbs steps as an intermediate timescale.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Model-level visible↦hidden layered architecture (RBM-like), not mapped to a specific brain area but used to study generative feature extraction and classification.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Restricted connectivity: directed visible-to-hidden synapses (no lateral visible-visible or hidden-hidden in RBM-like tests); asymmetric/directed synapses allowed in generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Generative modeling, feature extraction, classification; learning statistical structure of datasets (e.g., MNIST experiments in Appendix F).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical: derivation of update, TAP mean-field approximations for steady states, and simulation experiments on image datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast stochastic sampling of hidden states during clamped positive phase vs slower negative-phase sampling (k-step chains) and weight updates; DCM's delayed-correlation matching replaces equilibrium assumptions and enables learning in asymmetric kinetic settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DCM-derived updates closely match CD intuition: positive clamped hidden statistics vs negative free dynamics statistics; DCM allows generative learning even with biological constraints, and TAP mean-field approximations were used to accelerate inference in sparse asymmetric networks; decent feature extraction and classification performance reported though degraded relative to unconstrained algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Empirical tests on real image data (MNIST) and feature extraction reported in Appendix F; details include comparisons to TAP-based learning and notes on degradation under biological constraints (no single summary number in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Positive-phase clamped hidden statistics encode immediate stimulus structure; negative-phase free dynamics plus weight updates iteratively consolidate generative structure into weights, analogous to CD's positive/negative phase consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From statistical inference to a differential learning rule for stochastic neural networks', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A learning algorithm for boltzmann machines <em>(Rating: 2)</em></li>
                <li>On contrastive divergence learning <em>(Rating: 2)</em></li>
                <li>A three-threshold learning rule approaches the maximal capacity of recurrent neural networks <em>(Rating: 2)</em></li>
                <li>The basins of attraction of a new hopfield learning rule <em>(Rating: 2)</em></li>
                <li>Neural networks and physical systems with emergent collective computational abilities <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
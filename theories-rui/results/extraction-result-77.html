<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-77 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-77</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-77</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-ab8f916353e9672dc607d6118b2f26f0c60c1437</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ab8f916353e9672dc607d6118b2f26f0c60c1437" target="_blank">Learning spatiotemporal signals using a recurrent spiking network that discretizes time</a></p>
                <p><strong>Paper Venue:</strong> bioRxiv</p>
                <p><strong>Paper TL;DR:</strong> A model using biological-plausible plasticity rules for a specific computational task: spatiotemporal sequence learning is investigated where a spiking recurrent network of excitatory and inhibitory biophysical neurons drives a read-out layer: the dynamics of the recurrent network is constrained to encode time while the read- out neurons encode space.</p>
                <p><strong>Paper Abstract:</strong> Learning to produce spatiotemporal sequences is a common task the brain has to solve. The same neural substrate may be used by the brain to produce different sequential behaviours. The way the brain learns and encodes such tasks remains unknown as current computational models do not typically use realistic biologically-plausible learning. Here, we propose a model where a spiking recurrent network of excitatory and inhibitory biophysical neurons drives a read-out layer: the dynamics of the recurrent network is constrained to encode time while the read-out neurons encode space. Space is then linked with time through plastic synapses that follow common Hebbian learning rules. We demonstrate that the model is able to learn spatiotemporal dynamics on a timescale that is behaviourally relevant. Learned sequences are robustly replayed during a regime of spontaneous activity. Author summary The brain has the ability to learn flexible behaviours on a wide range of time scales. Previous studies have successfully build spiking network models that learn a variety of computational tasks. However, often the learning involved is not local. Here, we investigate a model using biological-plausible plasticity rules for a specific computational task: spatiotemporal sequence learning. The architecture separates time and space into two different parts and this allows learning to bind space to time. Importantly, the time component is encoded into a recurrent network which exhibits sequential dynamics on a behavioural time scale. This network is then used as an engine to drive spatial read-out neurons. We demonstrate that the model can learn complicated spatiotemporal spiking dynamics, such as the song of a bird, and replay the song robustly.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e77.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e77.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>vSTDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voltage-based spike-timing-dependent plasticity (voltage-based STDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A local Hebbian plasticity rule used for excitatory-to-excitatory and excitatory-to-readout synapses where potentiation and depression depend on pre-synaptic spikes and filtered postsynaptic membrane potential thresholds; implemented with explicit time constants, thresholds, and weight bounds so that temporally-correlated spiking drives synaptic strengthening.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Voltage-based STDP (Hebbian, Clopath-style)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Synaptic changes follow dW_ij/dt = -A_LTD * s_j(t) * R(u_i(t)-θ_LTD) + A_LTP * x_j(t) * R(V_i(t)-θ_LTP) * R(v_i(t)-θ_LTD). Here s_j is presynaptic spike train, x_j is a low-pass of s_j (τ_x), u_i and v_i are low-pass filtered versions of the postsynaptic membrane potential (time constants τ_u, τ_v). R(.) is linear-rectifier. LTP amplitude is made weight-dependent for read-out synapses (A_LTP decreases linearly with W) to create a soft upper bound; recurrent E→E synapses are L1-normalized periodically to enforce competition. Hard lower/upper bounds W_min/W_max are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates on millisecond timescales for the STDP windows and low-pass filters (τ_x = 3.5 ms for recurrent E→E, τ_x = 5 ms for E→R; τ_u = 10 ms; τ_v = 7 ms). Expression of learned connectivity occurs over minutes-to-hours during training (recurrent network: tens of minutes to hours of repeated stimulation) and read-out potentiation can occur over seconds (examples: read-out learning reported after 6–24 s).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Computational model of a generic cortical-like recurrent excitatory–inhibitory network (no specific biological brain region; model motivated by cortex/songbird circuitry), used on a recurrent network (RNN) driving separate read-out neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Applied to a sparse recurrent E–I network with fixed topology (connection probability p); results in strong intra-cluster recurrent E→E weights and unidirectional feedforward E→E weights from cluster i→i+1. Read-out receives all-to-all E→R projections (plastic). Recurrent E→E plasticity paired with L1 normalization sculpts clustered + feedforward structure.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supervised associative/Hebbian sequence learning: the RNN learns a temporal backbone (unsupervised/semi-supervised sequential stimulation) and read-out synapses learn supervised mappings from time bins to output (spatiotemporal mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical spiking-network model (adaptive exponential IF excitatory neurons, LIF inhibitory neurons) with simulated plasticity and training protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast STDP operations (ms filters and spike timing) accumulate over many repeated presentations (seconds → minutes → hours) to shape slower mesoscale connectivity (clusters and feedforward chains) which generate cluster activation times (τ_c ≈ tens of ms) and full-sequence periods (τ_p ≈ hundreds of ms). Weight-dependent LTP and L1 normalization operate on every plasticity update (ms-level) but produce network-level structure over long training durations; read-out weights can be learned rapidly (seconds) once the RNN backbone is established.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying voltage-based STDP during sequential stimulation of clusters produces a stable clustered + feedforward connectivity (strong intra-cluster and i→i+1 weights). This yields spontaneous sequential reactivations that discretize time into ~15 ms ticks and support behavioral timescale sequences (period ≈ 450 ms). The discretized time enables simple Hebbian formation of read-out synapses that map time bins to spatial/feature outputs, allowing the learning and replay of non-Markovian sequences and complex time-varying signals (e.g., bird song). Spontaneous activity reinforces the learned recurrent connectivity when dynamics remain sequential.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Reported numeric values: cluster activation duration ≈ 15 ms; overall sequence period ≈ 450 ms (example); read-out training durations reported as short examples (6 s, 12 s, 24 s) for various tasks; recurrent-training protocol used 60 minutes of sequential stimulation + 60 minutes of spontaneous stabilization (80-cluster scaled example used 3 hours stimulation + 3 hours spontaneous for stability). STDP parameters given: A_LTD = 0.0014 pA·mV^-2, A (max LTP) = 0.0008 pA·mV^-1, θ_LTD = -70 mV, θ_LTP = -49 mV, τ_x(EE)=3.5 ms, τ_x(RE)=5 ms, τ_u=10 ms, τ_v=7 ms.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Spontaneous sequential reactivation after training reinforces the recurrent feedforward and intra-cluster connectivity (i.e., a form of offline stabilization); read-out plasticity is frozen after supervised learning to avoid runaway potentiation (no explicit multi-area consolidation protocol, but spontaneous replays act to maintain/reinforce slow network structure).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning spatiotemporal signals using a recurrent spiking network that discretizes time', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e77.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e77.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inhibitory homeostatic plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local inhibitory-to-excitatory homeostatic plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A local I→E plasticity rule that adjusts inhibitory synaptic strengths as a function of pre- and postsynaptic activity to maintain a target excitatory firing rate and to prevent runaway excitation during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Inhibitory plasticity (homeostatic I→E rule)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>I→E weights change according to dW_ij/dt = A_inh [ y_i^E(t) - 2 r_0 τ_y ] s_j^I(t) + A_inh y_j^I(t) s_i^E(t), where s^E and s^I are spike trains, y^E and y^I are low-pass filtered spike trains (time constant τ_y). The rule tries to push postsynaptic excitatory rates toward a target r_0 by potentiating inhibition when excitatory firing is above target and depressing otherwise. I→E synapses have hard bounds W_min and W_max.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates on fast spike-timing interactions (ms-level spikes) with low-pass filtering τ_y = 20 ms; functional homeostasis emerges over the same minutes-to-hours training timescales as excitatory plasticity (used during recurrent training and spontaneous stabilization).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Model recurrent excitatory–inhibitory network (generic cortical-like circuit) used to stabilize sequence-generating dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Plastic I→E connections (from inhibitory population to each excitatory neuron) in a recurrent E–I network; used together with E→E plasticity to maintain balanced dynamics and sculpt cluster activity.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Homeostatic stabilization during associative sequence learning; prevents runaway potentiation and helps maintain desired excitatory rates during formation of sequential structure.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model implementing the plasticity rule in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>I→E plasticity acts at similar fast filtering timescales (tens of ms) to detect deviations in firing rate and adapt inhibitory weights; over many repeated stimulations (minutes to hours), it shapes network excitability and cooperates with excitatory STDP to maintain stable sequential dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Including inhibitory plasticity allowed the model to automatically find parameters that prevent runaway dynamics and aided in stabilizing sequential clustered activity during and after training; it contributes to stable spontaneous sequence replays that reinforce learned structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Parameters reported: A_inh = 1e-5 (units A·Hz in the paper), target rate r_0 = 3 Hz, τ_y = 20 ms; bounds W_EI_min = 48.7 pF and W_EI_max = 243 pF. No single scalar 'accuracy' metric, but inclusion improved stability of learned sequences across simulations (qualitative and via examples in S5 Fig).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Acts as an online homeostatic mechanism rather than a consolidation process; by adjusting inhibition during and after training it helps stabilize synaptic configurations shaped by slower accumulation of excitatory potentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning spatiotemporal signals using a recurrent spiking network that discretizes time', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e77.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e77.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clustered-feedforward RNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clustered recurrent network with feedforward i→i+1 embedding (temporal backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A connectivity motif where excitatory neurons are grouped into clusters with strong intra-cluster recurrence and asymmetric feedforward connections from cluster i to i+1, creating sequential activations that discretize time into 'ticks' used as a temporal backbone to drive downstream read-outs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Voltage-based STDP + L1 normalization + weight-dependent potentiation (composite mechanisms used to sculpt connectivity)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Connectivity emerges when repeated sequential stimulation within STDP time windows potentiates intra-cluster synapses bidirectionally and potentiates inter-cluster synapses in the forward direction (pre-before-post). Periodic L1 normalization of all incoming excitatory weights (every 20 ms) enforces competition and preserves total incoming excitatory weight per neuron; read-out synapses include weight-dependent LTP (linear decrease of A_LTP with current weight) to impose soft upper bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Network operates across multiple scales: single spikes at millisecond timescale; cluster activation 'tick' τ_c ≈ 15 ms (set by intra-cluster recurrent reverberation and adaptation); full sequence period τ_p ≈ hundreds of ms (example ≈ 450 ms); formation of the structural connectivity occurs over minutes-to-hours of repeated stimulation (training protocols: e.g., 60 min sequential stimulation + 60 min spontaneous; larger networks used 3 h + 3 h).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Abstract model of a recurrent excitatory–inhibitory circuit (inspired by cortical and songbird sequential circuits), not mapped to a specific anatomical region but analogous to sequence-generating networks like HVC→RA in songbirds or cortical sequence activity.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Sparse initial topology with connection probability p; after learning, strong intra-cluster E→E weights, strong unidirectional feedforward E→E weights from cluster i→i+1, central inhibitory cluster connected to all excitatory clusters, fixed topology zeros remain zero; read-out projection is all-to-all from RNN excitatory neurons to read-out neurons which themselves are not recurrently connected.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Temporal coding and sequence learning — the recurrent network learns to encode time (temporal discretization) which enables associative Hebbian mapping to read-out neurons (spatiotemporal mapping); used to learn non-Markovian sequences and complex time-varying signals.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical spiking-network model; connectivity is learned in-silico via sequential stimulation and plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast local plasticity (ms STDP) driven by repeated short (~10 ms) cluster stimulations produces mesoscale connectivity (clusters and feedforward links) whose dynamics manifests at intermediate time scales (cluster ticks tens of ms) and at the behavioral sequence period (~hundreds ms). Read-out learning leverages the discretized time bins and can proceed quickly (seconds) once backbone is formed. Spontaneous reactivation (ongoing activity) operates on similar timescales and serves to maintain or reinforce the slow structural connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The clustered + feedforward connectivity converts millisecond spike timing information into behavioral-timescale sequential dynamics: clusters produce long-lasting reverberation (~15 ms per cluster) and the feedforward links chain these activations into sequences with periods on the order of hundreds of ms. This decomposition allows simple Hebbian read-out learning to encode complex spatiotemporal patterns (including non-Markovian sequences and bird song spectrograms) and is robust to synapse deletion depending on cluster size. Spectral analysis of the learned weight matrix shows outlier eigenvalues whose imaginary parts scale with feedforward strength, linking connectivity to oscillation/frequency (i.e., time scale) of sequence reactivation. Temporal variability of sequence durations scales diffusively (σ ∝ √μ), limiting long-sequence precision.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Quantitative values: example RNN of 2400 E + 600 I with 30 clusters of 80 neurons produced period ≈ 450 ms and cluster activation ≈ 15 ms; variability relation σ_n = 0.213 √μ_n (root mean squared error 0.223 ms); robustness tests show larger clusters improve recall under synapse deletion (Fig 5C); scaling examples: network with N_E = 6400 and N_C = 200 recommended for ~400 ms target sequences. Training protocols: cluster stimulation: 10 ms stimulation per cluster with 5 ms gap, repeated for 60 min (small network) or 3 h (large network), then spontaneous stabilization of comparable duration.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit multi-area consolidation scheme described; instead, repeated supervised presentations produce synaptic changes over minutes-to-hours in RNN connectivity, while spontaneous sequential reactivations after training reinforce and stabilize the recurrent structure (an offline maintenance mechanism). Read-out synapses are frozen after supervised learning to prevent continuous potentiation during spontaneous replays (i.e., the model suppresses further consolidation in read-outs to avoid saturation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning spatiotemporal signals using a recurrent spiking network that discretizes time', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        </div>

    </div>
</body>
</html>
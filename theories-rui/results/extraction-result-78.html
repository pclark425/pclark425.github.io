<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-78 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-78</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-78</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-3a76603f03b45903bb030b2efd79984693625dc2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3a76603f03b45903bb030b2efd79984693625dc2" target="_blank">Meta-Learning through Hebbian Plasticity in Random Networks</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent.</p>
                <p><strong>Paper Abstract:</strong> Lifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to different morphological damage in the absence of any explicit reward or error signal.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e78.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e78.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABCD Hebbian</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalized Hebbian ABCD plasticity rule (with per-synapse learning rate η)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalized, per-synapse Hebbian plasticity rule parameterized by coefficients A (pre-post correlation), B (presynaptic term), C (postsynaptic term), D (bias/inhibitory-excitatory offset) and an intra-life learning rate η; applied to continuously update synaptic weights during an agent's lifetime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Hebbian (generalized ABCD with η)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Δw_ij = η_w · (A_w o_i o_j + B_w o_i + C_w o_j + D_w). Each synapse has its own A,B,C,D and η parameters which determine updates based only on local pre- and post-synaptic activations (o_i, o_j). A multiplies the product (correlation), B and C implement one-sided dependence on pre/post firing rates, and D acts as a constant bias term that can push weights up or down; η scales per-timestep update magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates at the agent's lifetime timesteps in simulations (fast): weights typically converge within tens to hundreds of timesteps (observed 30–80 timesteps to reach high performance; recovery ≈50 timesteps after perturbation). Conceptually maps to 'within-life' (seconds-to-minutes in simulation units) while the rule parameters themselves are optimized over evolutionary generations (slow: evolution across many episodes / generations).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Not a biological region in experiments — applied to artificial feedforward policy networks (fully-connected layers); paper relates conceptually to cortical/spiking systems in discussion but experimental model is an artificial feedforward policy network.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Applied to randomly-initialized fully-connected feedforward layers (plastic); convolutional layers were kept static. Each connection has its own rule (heterogeneous per-synapse plasticity).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Fast lifetime adaptation / motor skill self-organization and meta-learning (unsupervised local-rule-driven weight self-organization that supports reinforcement-task performance without lifetime reward input).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational model/simulation: evolutionary optimization (ES) of per-synapse ABCD+η coefficients, with weight updates applied online during task rollouts (CarRacing-v0 and 3D quadruped).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Explicit multi-scale setup: slow evolutionary timescale optimizes the per-synapse ABCD+η coefficients across many episodes/generations; those evolved local rules then drive fast within-lifetime weight updates (tens to hundreds of timesteps) that self-organize weights to an attractor in weight space. The paper frames evolution as providing an innate bias (slow) and Hebbian updates as fast adaptation (within-life); no explicit intermediate consolidation process is implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Evolved ABCD rules enable agents starting from random weights to self-organize weights quickly to high-performing configurations (emergent attractor in weight space). For the quadruped, weights converge in ~30–80 timesteps; the Hebbian agents generalize to an unseen damaged morphology (left-front leg) while static-weight agents largely fail. Hebbian agents are robust to perturbations (actuator freeze, partial weight zeroing) and rapidly reconverge to good weights. A restricted Hebbian variant with only A (pure correlation term, single coefficient per synapse) failed to solve the pixel-based task, indicating benefit of richer terms (B,C,D,η).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Quantitative outcomes tied to systems using ABCD rules: CarRacing reward = 872 ± 11 (Hebbian) vs 711 ± 16 (static) averaged over 100 rollouts; quadruped distances (100 rollouts) for unseen damaged morphology: Hebbian = 452 ± 95 units vs static = 68 ± 56 units. Convergence measured at 30–80 timesteps; recovery from actuator freeze ≈50 timesteps. Model sizes: e.g., quadruped network 12,288 synapses → 61,440 Hebbian coefficients (5 per synapse).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit biological-style consolidation is implemented; the paper describes a two-timescale interaction (slow evolution of learning rules vs fast within-life weight updates) but does not implement a separate consolidation process transferring fast changes into a longer-lasting memory beyond the immediate lifetime-weight state.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Learning through Hebbian Plasticity in Random Networks', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e78.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e78.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STDP (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-Timing-Dependent Plasticity (STDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Biological synaptic plasticity rule wherein the sign and magnitude of synaptic change depend on the precise millisecond-scale relative timing of pre- and post-synaptic spikes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Spike-Timing-Dependent Plasticity (STDP)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>In STDP, if a presynaptic spike precedes a postsynaptic spike within a narrow time window (typically milliseconds), potentiation occurs; the reverse timing produces depression. The rule is timing-dependent and sensitive to sub-100 ms / millisecond-scale differences in spike times.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Millisecond-scale dependence on relative spike timing (biological STDP operates on ~1–100 ms windows).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>General biological spiking networks (mentioned as the most well-known form of plasticity in spiking networks); no specific experimental region in this paper's simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Typically studied in recurrent and feedforward spiking circuits; mentioned as a contrasting biological mechanism to rate-based Hebbian updates used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative synaptic modification, temporal credit assignment at fine timescales; implicated in precise spike-timing based learning and synaptic refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mentioned in background (experimental/neuroscience literature), not used in the paper's computational experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper contrasts STDP (fast, millisecond timing) with rate-based Hebbian mechanisms conceptually, but does not model multi-timescale interaction involving STDP.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as established biological mechanism; the authors state that in continuous-output artificial networks where spike timing is abstracted away, relative timing plays a smaller role, motivating use of spike-rate-dependent/rate-based Hebbian rules instead.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>No quantitative measures in this paper (STDP only discussed conceptually).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not discussed for STDP in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Learning through Hebbian Plasticity in Random Networks', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e78.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e78.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SRDP / rate-based Hebb</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-Rate-Dependent Plasticity / rate-based Hebbian learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rate-based plasticity where synaptic changes depend on average firing rates (pre- and post-synaptic), appropriate for non-spiking or rate-coded neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Spike-Rate-Dependent Plasticity (SRDP) / rate-based Hebbian</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Plasticity depends on the average activity (firing rate) of pre- and post-synaptic neurons over a time window rather than precise spike timing; in the paper this motivates using o_i and o_j (continuous activations) in the ABCD rule to update weights based on correlations and single-sided rate terms.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates over longer windows than spike-timing rules — described as 'long time window' rate averages (seconds to many milliseconds in biological interpretation); in simulations updates are per timestep (timestep scale not mapped to real time).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Mentioned in general neuroscience context; used as conceptual motivation for applying Hebbian ABCD to continuous-rate artificial neurons rather than modeling biological spiking circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Applicable to feedforward and recurrent networks where activations are continuous; paper applies it to fully-connected feedforward policy networks.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative / correlation-based adaptation supporting motor behavior self-organization and learning in rate-coded models.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Conceptual / literature mention and motivation for the rate-based Hebbian update used in simulations; not separately experimentally tested.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper implies SRDP-like updates act at within-life timescales (fast) and are shaped by evolutionary optimization (slow); no explicit intermediate consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors justify using rate-based Hebbian rules for continuous-output networks and demonstrate that such local rate-based plasticity (ABCD rule) can support rapid within-life adaptation in complex RL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>No direct numerical measures specific to SRDP distinct from the ABCD-rule experiment statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not implemented; rate-based plasticity produces within-life weight changes that persist for the episode but no separate consolidation mechanism is described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Learning through Hebbian Plasticity in Random Networks', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e78.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e78.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random initial connectivity / random networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of randomly-initialized connectivity patterns (weights) as the network wiring prior, with learning rules (Hebbian ABCD) enabling self-organization from those random starts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Random connectivity (wiring prior) coupled with Hebbian plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Networks are initialized with random synaptic weights (e.g., U[-0.1,0.1] or N(0,0.1)); the paper uses per-synapse Hebbian ABCD+η rules to modify these connections during life so that useful structure emerges from initially random connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Initialization is a zero-time (start of episode) condition; subsequent self-organization occurs over fast within-life timesteps (30–80 timesteps to converge in locomotion experiments). Evolution operates on a much slower timescale to discover effective plasticity rules.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Paper links the concept to the prefrontal cortex in discussion (citing literature that random connectivity in prefrontal cortex increases representational dimensionality), but the experiments use artificial feedforward policy networks.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Fully-connected feedforward layers initialized randomly; convolutional layers kept static (not plastic). The paper highlights heterogenous per-connection plasticity rather than uniform plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Meta-learning / rapid within-life adaptation: the random wiring provides a rich substrate that, together with local learning rules, enables learning of motor policies and adaptation to morphological changes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational model/simulation: experiments demonstrate that random initial networks with evolved local Hebbian rules can reach competitive performance and generalize to unseen perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Random connectivity is the initial condition; slow evolution tunes learning rules; fast Hebbian updates transform random connectivity into task-appropriate structure within the lifetime. The paper emphasizes emergent attractors in weight-space driven by this interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Randomly-initialized networks, when paired with evolved, connection-specific Hebbian rules, can self-organize to solve complex RL tasks starting from random weights; they show robustness and generalization (e.g., unseen morphologies), and weight trajectories show structured attractors (PCA) under evolved rules but not under random rules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Same task-level metrics as ABCD experiments: CarRacing reward and quadruped distances reported; observed convergence and resilience metrics (30–80 timesteps to converge, reconvergence after perturbations in a few tens of timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation; random connectivity is refined by fast Hebbian updates during lifetime, while evolution slowly encodes prior learning biases into the plasticity rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Learning through Hebbian Plasticity in Random Networks', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e78.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e78.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuromodulated plasticity (future)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuromodulated plasticity (proposed extension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Plasticity in which neuromodulatory signals dynamically gate or scale synaptic plasticity (i.e., certain neurons or signals modulate the amount or sign of weight change), suggested as a promising extension to improve goal-directed adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Neuromodulated plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Plasticity rules are modulated by additional signals (neuromodulators) that can up-/down-regulate the plasticity of specific connections or neurons; can be implemented as neurons whose activity multiplies or gates local weight updates, enabling context-dependent or reward-gated plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Neuromodulatory influences can operate at multiple scales: phasic fast signals (milliseconds–seconds) and slower tonic changes (minutes–hours); the paper suggests such mechanisms would allow more elaborate learning but does not implement timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>General (discussion relates to animal brains broadly); neuromodulation cited as important for more elaborate forms of learning across brain systems.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Would typically overlay on existing feedforward/recurrent architectures, enabling selective gating of plasticity on particular connections or subcircuits.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Goal-directed adaptation, reward-modulated learning, potentially consolidation-linked modulation; proposed to improve plastic networks especially when explicit reward signals are provided during lifetime.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mentioned as future work / literature-backed idea; not implemented in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper suggests neuromodulation would add an additional control dimension enabling plasticity to be gated dynamically (fast) while long-term rule structure remains encoded by evolution (slow), but does not provide an implemented mechanism or results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Proposed as likely to improve evolving plastic networks; authors cite prior work showing benefits of neuromodulated plasticity. No new empirical findings in this paper regarding neuromodulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>No measures in this paper (future work suggestion).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not implemented; neuromodulation is suggested as a mechanism that could enable better coordination across timescales but no concrete consolidation mechanism is provided in the present work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Learning through Hebbian Plasticity in Random Networks', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Differentiable plasticity: training plastic neural networks with backpropagation <em>(Rating: 2)</em></li>
                <li>Hebbian learning in a random network captures selectivity properties of the prefrontal cortex <em>(Rating: 2)</em></li>
                <li>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity <em>(Rating: 2)</em></li>
                <li>Evolutionary advantages of neuromodulated plasticity in dynamic, reward-based scenarios <em>(Rating: 2)</em></li>
                <li>Evolving neuromodulatory topologies for reinforcement learning-like problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Induced Calibration Distortion Theory: General Law of Contextual Anchoring - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1862</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1862</p>
                <p><strong>Name:</strong> Prompt-Induced Calibration Distortion Theory: General Law of Contextual Anchoring</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory asserts that LLMs' probability estimates for future scientific discoveries are anchored by contextual cues present in the prompt, such as reference points, examples, or explicit probability ranges. These anchors bias the model's output, leading to systematic over- or under-estimation depending on the context provided, regardless of the true likelihood of the event.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contextual Anchoring Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt &#8594; contains_contextual_anchor &#8594; reference_point or example or explicit_probability<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_queried_with &#8594; prompt</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_biased_toward &#8594; contextual_anchor_value</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs' outputs are influenced by explicit examples or reference values in the prompt, a phenomenon analogous to anchoring bias in humans. </li>
    <li>Empirical studies show that including a high or low probability in the prompt shifts the LLM's output toward that value. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends the concept of anchoring bias to LLMs in the context of scientific discovery forecasting.</p>            <p><strong>What Already Exists:</strong> Anchoring bias is well-known in human cognition and has been observed in LLMs, but not formalized as a law for scientific discovery probability estimation.</p>            <p><strong>What is Novel:</strong> The explicit law that contextual anchors in prompts systematically bias LLM probability estimates for scientific forecasting is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Tversky & Kahneman (1974) Judgment under Uncertainty: Heuristics and Biases [Anchoring bias in humans]</li>
    <li>Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Prompt context effects in LLMs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Providing a high probability example in the prompt will increase the LLM's probability estimate for a scientific event, even if the event is unlikely.</li>
                <li>Removing all contextual anchors from prompts will reduce systematic bias in LLM probability estimates.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There may exist optimal anchor values that minimize overall calibration error for a given LLM.</li>
                <li>Repeated exposure to misleading anchors during training may cause persistent calibration errors in LLMs.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs' probability estimates are unaffected by contextual anchors in prompts, the theory would be falsified.</li>
                <li>If removing anchors does not reduce bias, the theory would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs with explicit de-biasing mechanisms may not exhibit anchoring effects. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This is a new application of anchoring bias theory to LLM-based scientific forecasting.</p>
            <p><strong>References:</strong> <ul>
    <li>Tversky & Kahneman (1974) Judgment under Uncertainty: Heuristics and Biases [Anchoring bias in humans]</li>
    <li>Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Prompt context effects in LLMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Induced Calibration Distortion Theory: General Law of Contextual Anchoring",
    "theory_description": "This theory asserts that LLMs' probability estimates for future scientific discoveries are anchored by contextual cues present in the prompt, such as reference points, examples, or explicit probability ranges. These anchors bias the model's output, leading to systematic over- or under-estimation depending on the context provided, regardless of the true likelihood of the event.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contextual Anchoring Law",
                "if": [
                    {
                        "subject": "prompt",
                        "relation": "contains_contextual_anchor",
                        "object": "reference_point or example or explicit_probability"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_queried_with",
                        "object": "prompt"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_biased_toward",
                        "object": "contextual_anchor_value"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs' outputs are influenced by explicit examples or reference values in the prompt, a phenomenon analogous to anchoring bias in humans.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that including a high or low probability in the prompt shifts the LLM's output toward that value.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Anchoring bias is well-known in human cognition and has been observed in LLMs, but not formalized as a law for scientific discovery probability estimation.",
                    "what_is_novel": "The explicit law that contextual anchors in prompts systematically bias LLM probability estimates for scientific forecasting is new.",
                    "classification_explanation": "This law extends the concept of anchoring bias to LLMs in the context of scientific discovery forecasting.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tversky & Kahneman (1974) Judgment under Uncertainty: Heuristics and Biases [Anchoring bias in humans]",
                        "Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Prompt context effects in LLMs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Providing a high probability example in the prompt will increase the LLM's probability estimate for a scientific event, even if the event is unlikely.",
        "Removing all contextual anchors from prompts will reduce systematic bias in LLM probability estimates."
    ],
    "new_predictions_unknown": [
        "There may exist optimal anchor values that minimize overall calibration error for a given LLM.",
        "Repeated exposure to misleading anchors during training may cause persistent calibration errors in LLMs."
    ],
    "negative_experiments": [
        "If LLMs' probability estimates are unaffected by contextual anchors in prompts, the theory would be falsified.",
        "If removing anchors does not reduce bias, the theory would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs with explicit de-biasing mechanisms may not exhibit anchoring effects.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs trained with adversarial prompts show reduced sensitivity to contextual anchors.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For prompts with multiple conflicting anchors, the LLM's output may reflect an average or weighted combination.",
        "For prompts with no anchors, other biases (e.g., prior frequency) may dominate."
    ],
    "existing_theory": {
        "what_already_exists": "Anchoring bias is known in humans and observed in LLMs, but not formalized for scientific discovery probability estimation.",
        "what_is_novel": "The explicit law of contextual anchoring in LLM probability estimates for scientific forecasting is new.",
        "classification_explanation": "This is a new application of anchoring bias theory to LLM-based scientific forecasting.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tversky & Kahneman (1974) Judgment under Uncertainty: Heuristics and Biases [Anchoring bias in humans]",
            "Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Prompt context effects in LLMs]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-650",
    "original_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
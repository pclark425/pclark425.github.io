<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Distributed Fourier-Feature Representation and Modular Arithmetic Computation in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-579</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-579</p>
                <p><strong>Name:</strong> Distributed Fourier-Feature Representation and Modular Arithmetic Computation in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> Large language models (LLMs) perform arithmetic, especially integer addition, by encoding numbers as distributed representations in a sparse Fourier basis within token embeddings and intermediate activations. Arithmetic operations are implemented by combining low-frequency (magnitude) and high-frequency (modular residue) components, with MLPs and attention heads specializing in different frequency bands. The final output is produced by constructive interference of these components, enabling precise localization of the correct answer. This mechanism is a product of pretraining on natural language corpora, which induces Fourier structure in number token embeddings, and is further refined by fine-tuning or in-context learning.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Fourier-Feature Encoding of Numbers (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is pre-trained on &#8594; natural language corpora containing numbers<span style="color: #888888;">, and</span></div>
        <div>&#8226; number token &#8594; is embedded by &#8594; LLM's token embedding matrix</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; number token embedding &#8594; contains &#8594; sparse large-magnitude components in the discrete Fourier basis (specific periods/frequencies)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Token embeddings of pre-trained models (e.g., GPT-2-XL, GPT-J, Phi-2) contain matching Fourier peaks; DFT of token embeddings reveals sparse large-magnitude components at periods 2, 2.5, 5, 10. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Transferring pretrained embeddings to scratch-trained models rescues arithmetic performance, indicating the necessity of Fourier structure. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While Fourier analysis is established, its centrality to LLM arithmetic computation is a novel mechanistic insight.</p>            <p><strong>What Already Exists:</strong> Fourier analysis of neural representations is known in signal processing and some neural network interpretability work.</p>            <p><strong>What is Novel:</strong> The identification of sparse, interpretable Fourier components as the basis for number representation and arithmetic in LLMs is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits in modular addition]</li>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [FFN sub-updates as interpretable, but not Fourier-specific]</li>
</ul>
            <h3>Statement 1: Modular Arithmetic via Superposition and Phase Alignment (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has &#8594; Fourier-feature number representations<span style="color: #888888;">, and</span></div>
        <div>&#8226; arithmetic operation &#8594; is performed by &#8594; LLM (e.g., addition)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; MLPs &#8594; contribute &#8594; low-frequency (magnitude) components<span style="color: #888888;">, and</span></div>
        <div>&#8226; attention heads &#8594; contribute &#8594; high-frequency (modular residue) components<span style="color: #888888;">, and</span></div>
        <div>&#8226; final logits &#8594; are computed by &#8594; linear superposition and phase alignment of these components, producing a peaked distribution at the correct answer</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>DFT of intermediate logits (W^U路MLP and W^U路Attn) shows sparse outlier Fourier components; MLPs mainly contribute low-frequency, attention heads high-frequency; reconstructing logits with top Fourier components yields a peaked distribution at the correct answer. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Causal ablations via projection-based low/high-pass filters on module outputs produce failure modes consistent with the hypothesized roles of low/high frequencies. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> </li>
    <li>Behavioral error analysis in closed-source models (GPT-3.5, GPT-4, PaLM-2) shows predominance of multiples-of-10 errors, consistent with missing/incorrect unit-digit (high-frequency) resolution. <a href="../results/extraction-result-4629.html#e4629.4" class="evidence-link">[e4629.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law generalizes Fourier-based arithmetic from toy models to real LLMs and links it to observed error patterns.</p>            <p><strong>What Already Exists:</strong> Fourier-based mechanisms for modular arithmetic have been described in small models and theoretical work.</p>            <p><strong>What is Novel:</strong> The demonstration that large, pre-trained LLMs use a distributed, frequency-specialized mechanism for arithmetic is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits in modular addition]</li>
    <li>Barak et al. (2022) Towards Understanding Grokking: An Effective Theory of Representation Learning [Fourier analysis in toy models]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If high-frequency Fourier components are selectively ablated in attention outputs, models will make systematic modular (unit-digit) errors in arithmetic tasks.</li>
                <li>If a model is trained from scratch without pre-existing Fourier structure in embeddings, it will achieve lower arithmetic accuracy and exhibit more magnitude (off-by-10, 100) errors.</li>
                <li>If a new arithmetic operation (e.g., modular multiplication) is introduced via fine-tuning, new sparse Fourier components corresponding to relevant moduli will emerge in intermediate activations.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained on a language with a different numeric base (e.g., base-12), the dominant Fourier components will shift to match the new base's periodicities.</li>
                <li>If a model is exposed to adversarially constructed number tokens with no periodic structure, it may fail to develop any effective arithmetic mechanism.</li>
                <li>If a model is fine-tuned on arithmetic tasks with non-standard tokenization (e.g., subword or character-level), the frequency specialization of MLPs and attention heads may reorganize or become less distinct.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If ablation of all high-frequency components does not increase modular (unit-digit) errors, the modular decomposition claim would be undermined.</li>
                <li>If models with no Fourier structure in embeddings can achieve perfect arithmetic accuracy, the necessity of Fourier features would be challenged.</li>
                <li>If attention heads do not contribute high-frequency components, or MLPs do not contribute low-frequency components, the frequency specialization claim would be questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The role of explicit symbolic or program-synthesis mechanisms (e.g., program-of-thought, external calculators) in arithmetic is not explained by this theory. <a href="../results/extraction-result-4707.html#e4707.2" class="evidence-link">[e4707.2]</a> <a href="../results/extraction-result-4730.html#e4730.1" class="evidence-link">[e4730.1]</a> <a href="../results/extraction-result-4707.html#e4707.1" class="evidence-link">[e4707.1]</a> </li>
    <li>The emergence and causal role of interpretable feed-forward (FF) neurons encoding arithmetic concepts is not addressed. <a href="../results/extraction-result-4626.html#e4626.0" class="evidence-link">[e4626.0]</a> <a href="../results/extraction-result-4626.html#e4626.3" class="evidence-link">[e4626.3]</a> </li>
    <li>The effect of chain-of-thought prompting and stepwise reasoning on arithmetic performance is not directly explained. <a href="../results/extraction-result-4741.html#e4741.0" class="evidence-link">[e4741.0]</a> <a href="../results/extraction-result-4730.html#e4730.0" class="evidence-link">[e4730.0]</a> <a href="../results/extraction-result-4721.html#e4721.0" class="evidence-link">[e4721.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory extends Fourier-based arithmetic from toy models to real LLMs and provides a mechanistic account of arithmetic computation in modern language models.</p>
            <p><strong>References:</strong> <ul>
    <li>Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits in modular addition]</li>
    <li>Barak et al. (2022) Towards Understanding Grokking: An Effective Theory of Representation Learning [Fourier analysis in toy models]</li>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [FFN sub-updates as interpretable, but not Fourier-specific]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Distributed Fourier-Feature Representation and Modular Arithmetic Computation in Language Models",
    "theory_description": "Large language models (LLMs) perform arithmetic, especially integer addition, by encoding numbers as distributed representations in a sparse Fourier basis within token embeddings and intermediate activations. Arithmetic operations are implemented by combining low-frequency (magnitude) and high-frequency (modular residue) components, with MLPs and attention heads specializing in different frequency bands. The final output is produced by constructive interference of these components, enabling precise localization of the correct answer. This mechanism is a product of pretraining on natural language corpora, which induces Fourier structure in number token embeddings, and is further refined by fine-tuning or in-context learning.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Fourier-Feature Encoding of Numbers",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is pre-trained on",
                        "object": "natural language corpora containing numbers"
                    },
                    {
                        "subject": "number token",
                        "relation": "is embedded by",
                        "object": "LLM's token embedding matrix"
                    }
                ],
                "then": [
                    {
                        "subject": "number token embedding",
                        "relation": "contains",
                        "object": "sparse large-magnitude components in the discrete Fourier basis (specific periods/frequencies)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Token embeddings of pre-trained models (e.g., GPT-2-XL, GPT-J, Phi-2) contain matching Fourier peaks; DFT of token embeddings reveals sparse large-magnitude components at periods 2, 2.5, 5, 10.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Transferring pretrained embeddings to scratch-trained models rescues arithmetic performance, indicating the necessity of Fourier structure.",
                        "uuids": [
                            "e4629.5"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Fourier analysis of neural representations is known in signal processing and some neural network interpretability work.",
                    "what_is_novel": "The identification of sparse, interpretable Fourier components as the basis for number representation and arithmetic in LLMs is new.",
                    "classification_explanation": "While Fourier analysis is established, its centrality to LLM arithmetic computation is a novel mechanistic insight.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits in modular addition]",
                        "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [FFN sub-updates as interpretable, but not Fourier-specific]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Modular Arithmetic via Superposition and Phase Alignment",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has",
                        "object": "Fourier-feature number representations"
                    },
                    {
                        "subject": "arithmetic operation",
                        "relation": "is performed by",
                        "object": "LLM (e.g., addition)"
                    }
                ],
                "then": [
                    {
                        "subject": "MLPs",
                        "relation": "contribute",
                        "object": "low-frequency (magnitude) components"
                    },
                    {
                        "subject": "attention heads",
                        "relation": "contribute",
                        "object": "high-frequency (modular residue) components"
                    },
                    {
                        "subject": "final logits",
                        "relation": "are computed by",
                        "object": "linear superposition and phase alignment of these components, producing a peaked distribution at the correct answer"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "DFT of intermediate logits (W^U路MLP and W^U路Attn) shows sparse outlier Fourier components; MLPs mainly contribute low-frequency, attention heads high-frequency; reconstructing logits with top Fourier components yields a peaked distribution at the correct answer.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Causal ablations via projection-based low/high-pass filters on module outputs produce failure modes consistent with the hypothesized roles of low/high frequencies.",
                        "uuids": [
                            "e4629.5",
                            "e4629.0"
                        ]
                    },
                    {
                        "text": "Behavioral error analysis in closed-source models (GPT-3.5, GPT-4, PaLM-2) shows predominance of multiples-of-10 errors, consistent with missing/incorrect unit-digit (high-frequency) resolution.",
                        "uuids": [
                            "e4629.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Fourier-based mechanisms for modular arithmetic have been described in small models and theoretical work.",
                    "what_is_novel": "The demonstration that large, pre-trained LLMs use a distributed, frequency-specialized mechanism for arithmetic is new.",
                    "classification_explanation": "This law generalizes Fourier-based arithmetic from toy models to real LLMs and links it to observed error patterns.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits in modular addition]",
                        "Barak et al. (2022) Towards Understanding Grokking: An Effective Theory of Representation Learning [Fourier analysis in toy models]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If high-frequency Fourier components are selectively ablated in attention outputs, models will make systematic modular (unit-digit) errors in arithmetic tasks.",
        "If a model is trained from scratch without pre-existing Fourier structure in embeddings, it will achieve lower arithmetic accuracy and exhibit more magnitude (off-by-10, 100) errors.",
        "If a new arithmetic operation (e.g., modular multiplication) is introduced via fine-tuning, new sparse Fourier components corresponding to relevant moduli will emerge in intermediate activations."
    ],
    "new_predictions_unknown": [
        "If a model is trained on a language with a different numeric base (e.g., base-12), the dominant Fourier components will shift to match the new base's periodicities.",
        "If a model is exposed to adversarially constructed number tokens with no periodic structure, it may fail to develop any effective arithmetic mechanism.",
        "If a model is fine-tuned on arithmetic tasks with non-standard tokenization (e.g., subword or character-level), the frequency specialization of MLPs and attention heads may reorganize or become less distinct."
    ],
    "negative_experiments": [
        "If ablation of all high-frequency components does not increase modular (unit-digit) errors, the modular decomposition claim would be undermined.",
        "If models with no Fourier structure in embeddings can achieve perfect arithmetic accuracy, the necessity of Fourier features would be challenged.",
        "If attention heads do not contribute high-frequency components, or MLPs do not contribute low-frequency components, the frequency specialization claim would be questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The role of explicit symbolic or program-synthesis mechanisms (e.g., program-of-thought, external calculators) in arithmetic is not explained by this theory.",
            "uuids": [
                "e4707.2",
                "e4730.1",
                "e4707.1"
            ]
        },
        {
            "text": "The emergence and causal role of interpretable feed-forward (FF) neurons encoding arithmetic concepts is not addressed.",
            "uuids": [
                "e4626.0",
                "e4626.3"
            ]
        },
        {
            "text": "The effect of chain-of-thought prompting and stepwise reasoning on arithmetic performance is not directly explained.",
            "uuids": [
                "e4741.0",
                "e4730.0",
                "e4721.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Models trained from scratch can reach non-trivial accuracy using primarily low-frequency approximation, indicating alternative (less precise) solutions exist.",
            "uuids": [
                "e4629.5"
            ]
        },
        {
            "text": "Some models (e.g., T5 with explicit position tokens) can learn arithmetic via position-based pattern learning rather than Fourier features.",
            "uuids": [
                "e4712.0"
            ]
        }
    ],
    "special_cases": [
        "For arithmetic tasks involving very large numbers or non-integer formats (e.g., floats, negatives), the Fourier mechanism may be insufficient or require additional components.",
        "In models with highly redundant or hydra-like circuits, ablation of frequency components may be compensated by backup pathways."
    ],
    "existing_theory": {
        "what_already_exists": "Fourier-based mechanisms for modular arithmetic have been described in small models and theoretical work; distributed representations in neural networks are well-established.",
        "what_is_novel": "The identification of a distributed, frequency-specialized mechanism for arithmetic in large, pre-trained LLMs, and its link to observed error patterns and module specialization, is new.",
        "classification_explanation": "This theory extends Fourier-based arithmetic from toy models to real LLMs and provides a mechanistic account of arithmetic computation in modern language models.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Nanda et al. (2023) Progress measures for grokking via mechanistic interpretability [Fourier circuits in modular addition]",
            "Barak et al. (2022) Towards Understanding Grokking: An Effective Theory of Representation Learning [Fourier analysis in toy models]",
            "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [FFN sub-updates as interpretable, but not Fourier-specific]"
        ]
    },
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Driven Emergence Threshold Law - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-494</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-494</p>
                <p><strong>Name:</strong> Prompt-Driven Emergence Threshold Law</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that the effectiveness of prompt-based interventions (such as chain-of-thought, least-to-most, and related prompting strategies) for strict logical reasoning in language models is governed by a threshold effect: only when the model's scale, prompt structure, and task complexity are jointly aligned does emergent logical reasoning ability appear. Below this threshold, prompting has little effect; above it, performance increases sharply. The theory further asserts that prompt template choice and exemplar-task alignment are critical, and that self-consistency decoding amplifies gains by marginalizing over diverse reasoning chains. The theory also recognizes that for shallow or highly regular tasks, prompt-driven emergence may not be necessary, and that logic-driven objectives or architectural changes can lower the threshold.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Emergence Threshold for Prompt-Driven Reasoning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; model size &#8594; is &#8594; above a critical threshold (e.g., >100B parameters)<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt structure &#8594; is &#8594; aligned with task logic (e.g., chain-of-thought, least-to-most, instructive trigger)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; emergent logical reasoning ability &#8594; is &#8594; observed (sharp increase in accuracy on multi-step reasoning tasks)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Chain-of-thought prompting yields dramatic accuracy gains for large models (>100B), but little effect for small models. <a href="../results/extraction-result-3438.html#e3438.0" class="evidence-link">[e3438.0]</a> <a href="../results/extraction-result-3537.html#e3537.7" class="evidence-link">[e3537.7]</a> <a href="../results/extraction-result-3513.html#e3513.0" class="evidence-link">[e3513.0]</a> <a href="../results/extraction-result-3537.html#e3537.3" class="evidence-link">[e3537.3]</a> <a href="../results/extraction-result-3526.html#e3526.5" class="evidence-link">[e3526.5]</a> <a href="../results/extraction-result-3513.html#e3513.3" class="evidence-link">[e3513.3]</a> <a href="../results/extraction-result-3438.html#e3438.1" class="evidence-link">[e3438.1]</a> <a href="../results/extraction-result-3537.html#e3537.5" class="evidence-link">[e3537.5]</a> <a href="../results/extraction-result-3490.html#e3490.0" class="evidence-link">[e3490.0]</a> </li>
    <li>Prompt template ablations show instructive triggers (e.g., 'Let's think step by step') are critical for zero-shot-CoT gains. <a href="../results/extraction-result-3537.html#e3537.7" class="evidence-link">[e3537.7]</a> <a href="../results/extraction-result-3438.html#e3438.1" class="evidence-link">[e3438.1]</a> </li>
    <li>Scaling law analysis shows that logical reasoning performance scales much more slowly with model size than other NLP tasks, and that prompt-based gains only appear above a certain scale. <a href="../results/extraction-result-3503.html#e3503.5" class="evidence-link">[e3503.5]</a> <a href="../results/extraction-result-3503.html#e3503.3" class="evidence-link">[e3503.3]</a> <a href="../results/extraction-result-3430.html#e3430.3" class="evidence-link">[e3430.3]</a> <a href="../results/extraction-result-3430.html#e3430.5" class="evidence-link">[e3430.5]</a> <a href="../results/extraction-result-3430.html#e3430.4" class="evidence-link">[e3430.4]</a> <a href="../results/extraction-result-3412.html#e3412.5" class="evidence-link">[e3412.5]</a> </li>
    <li>Zero-shot-CoT and least-to-most prompting only yield substantial improvements for large models; small models show little to no improvement. <a href="../results/extraction-result-3537.html#e3537.7" class="evidence-link">[e3537.7]</a> <a href="../results/extraction-result-3438.html#e3438.1" class="evidence-link">[e3438.1]</a> <a href="../results/extraction-result-3490.html#e3490.0" class="evidence-link">[e3490.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Self-Consistency Decoding Amplifies Prompt-Driven Gains (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; self-consistency decoding &#8594; is_applied_to &#8594; chain-of-thought prompted models</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; accuracy &#8594; increases &#8594; by up to 18 percentage points on arithmetic and symbolic reasoning tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Self-consistency improves PaLM-540B accuracy on GSM8K from 56.5% to 74.4%; similar gains observed for other large models and tasks. <a href="../results/extraction-result-3513.html#e3513.0" class="evidence-link">[e3513.0]</a> <a href="../results/extraction-result-3537.html#e3537.3" class="evidence-link">[e3537.3]</a> <a href="../results/extraction-result-3513.html#e3513.3" class="evidence-link">[e3513.3]</a> <a href="../results/extraction-result-3513.html#e3513.4" class="evidence-link">[e3513.4]</a> <a href="../results/extraction-result-3438.html#e3438.2" class="evidence-link">[e3438.2]</a> </li>
    <li>Self-consistency decoding aggregates over diverse reasoning chains, reducing sensitivity to individual chain errors. <a href="../results/extraction-result-3438.html#e3438.2" class="evidence-link">[e3438.2]</a> <a href="../results/extraction-result-3537.html#e3537.3" class="evidence-link">[e3537.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Prompt-Task Alignment is Critical for Reasoning Gains (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt exemplars &#8594; are &#8594; aligned with the target task domain and structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; chain-of-thought prompting &#8594; yields &#8594; maximal gains</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Few-shot-CoT is sensitive to exemplar-task mismatch; domain-mismatched exemplars degrade performance. <a href="../results/extraction-result-3537.html#e3537.7" class="evidence-link">[e3537.7]</a> <a href="../results/extraction-result-3438.html#e3438.0" class="evidence-link">[e3438.0]</a> <a href="../results/extraction-result-3537.html#e3537.5" class="evidence-link">[e3537.5]</a> </li>
    <li>Prompt design remains brittle: some reasonable-seeming templates perform much worse; prompt sensitivity remains (annotator style, exemplar content can change outcomes notably for some tasks). <a href="../results/extraction-result-3537.html#e3537.7" class="evidence-link">[e3537.7]</a> <a href="../results/extraction-result-3523.html#e3523.6" class="evidence-link">[e3523.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Shallow or Highly Regular Tasks Do Not Require Prompt-Driven Emergence (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; is &#8594; shallow reasoning or has strong statistical regularities</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; small models or standard prompting &#8594; can &#8594; achieve high accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Some tasks (e.g., simple NLI, shallow reasoning) can be solved by small models or without prompt-driven emergence. <a href="../results/extraction-result-3544.html#e3544.0" class="evidence-link">[e3544.0]</a> <a href="../results/extraction-result-3544.html#e3544.9" class="evidence-link">[e3544.9]</a> <a href="../results/extraction-result-3544.html#e3544.12" class="evidence-link">[e3544.12]</a> </li>
    <li>RuleTakers and RoBERTa-based models achieve high accuracy on synthetic rulebase tasks without prompt-driven emergence, though these may be less challenging. <a href="../results/extraction-result-3525.html#e3525.1" class="evidence-link">[e3525.1]</a> <a href="../results/extraction-result-3525.html#e3525.2" class="evidence-link">[e3525.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If chain-of-thought or least-to-most prompting is applied to a new multi-step reasoning task with a large model (>100B), and prompt templates are carefully aligned, sharp accuracy gains will be observed.</li>
                <li>If self-consistency decoding is added to chain-of-thought prompting for a new arithmetic or symbolic task, accuracy will increase by 10-20 percentage points.</li>
                <li>If prompt templates are poorly aligned or exemplars are mismatched to the task, even large models will show reduced gains from chain-of-thought prompting.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new model is trained to just below the emergence threshold (e.g., 80B parameters), it is unclear whether prompt-driven reasoning will appear or if a sharp threshold will be observed.</li>
                <li>If prompt templates are automatically generated or optimized, it is unknown whether they will match or exceed hand-crafted instructive triggers in eliciting reasoning.</li>
                <li>If a new class of tasks with complex, non-arithmetic multi-step reasoning is introduced, it is unknown whether the same threshold and prompt effects will hold.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If small models (<10B) show large gains from chain-of-thought or least-to-most prompting, the threshold law would be challenged.</li>
                <li>If self-consistency decoding fails to improve or reduces accuracy on multi-step reasoning tasks, the theory's claims would be weakened.</li>
                <li>If prompt template choice and exemplar-task alignment are found to have no effect on reasoning gains, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some logic-specific or neurosymbolic interventions (e.g., solver-augmented models, logic-driven fine-tuning) can yield high logical reasoning performance even for smaller models, suggesting alternative routes to reasoning beyond prompt-driven emergence. <a href="../results/extraction-result-3522.html#e3522.0" class="evidence-link">[e3522.0]</a> <a href="../results/extraction-result-3522.html#e3522.4" class="evidence-link">[e3522.4]</a> <a href="../results/extraction-result-3454.html#e3454.8" class="evidence-link">[e3454.8]</a> <a href="../results/extraction-result-3454.html#e3454.10" class="evidence-link">[e3454.10]</a> <a href="../results/extraction-result-3439.html#e3439.0" class="evidence-link">[e3439.0]</a> <a href="../results/extraction-result-3432.html#e3432.0" class="evidence-link">[e3432.0]</a> <a href="../results/extraction-result-3521.html#e3521.2" class="evidence-link">[e3521.2]</a> <a href="../results/extraction-result-3521.html#e3521.1" class="evidence-link">[e3521.1]</a> </li>
    <li>Some models (e.g., Mistral-7B) outperform larger open-source models on deep logic tasks, suggesting that training data and architecture can modulate the threshold. <a href="../results/extraction-result-3430.html#e3430.3" class="evidence-link">[e3430.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompting and emergence, but explicit threshold law and integration of template/exemplar alignment and self-consistency as a unified threshold theory is novel here]</li>
    <li>Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [Zero-shot-CoT and template ablations]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [Self-consistency decoding]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Least-to-most prompting and prompt structure effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Driven Emergence Threshold Law",
    "theory_description": "This theory posits that the effectiveness of prompt-based interventions (such as chain-of-thought, least-to-most, and related prompting strategies) for strict logical reasoning in language models is governed by a threshold effect: only when the model's scale, prompt structure, and task complexity are jointly aligned does emergent logical reasoning ability appear. Below this threshold, prompting has little effect; above it, performance increases sharply. The theory further asserts that prompt template choice and exemplar-task alignment are critical, and that self-consistency decoding amplifies gains by marginalizing over diverse reasoning chains. The theory also recognizes that for shallow or highly regular tasks, prompt-driven emergence may not be necessary, and that logic-driven objectives or architectural changes can lower the threshold.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Emergence Threshold for Prompt-Driven Reasoning",
                "if": [
                    {
                        "subject": "model size",
                        "relation": "is",
                        "object": "above a critical threshold (e.g., &gt;100B parameters)"
                    },
                    {
                        "subject": "prompt structure",
                        "relation": "is",
                        "object": "aligned with task logic (e.g., chain-of-thought, least-to-most, instructive trigger)"
                    }
                ],
                "then": [
                    {
                        "subject": "emergent logical reasoning ability",
                        "relation": "is",
                        "object": "observed (sharp increase in accuracy on multi-step reasoning tasks)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Chain-of-thought prompting yields dramatic accuracy gains for large models (&gt;100B), but little effect for small models.",
                        "uuids": [
                            "e3438.0",
                            "e3537.7",
                            "e3513.0",
                            "e3537.3",
                            "e3526.5",
                            "e3513.3",
                            "e3438.1",
                            "e3537.5",
                            "e3490.0"
                        ]
                    },
                    {
                        "text": "Prompt template ablations show instructive triggers (e.g., 'Let's think step by step') are critical for zero-shot-CoT gains.",
                        "uuids": [
                            "e3537.7",
                            "e3438.1"
                        ]
                    },
                    {
                        "text": "Scaling law analysis shows that logical reasoning performance scales much more slowly with model size than other NLP tasks, and that prompt-based gains only appear above a certain scale.",
                        "uuids": [
                            "e3503.5",
                            "e3503.3",
                            "e3430.3",
                            "e3430.5",
                            "e3430.4",
                            "e3412.5"
                        ]
                    },
                    {
                        "text": "Zero-shot-CoT and least-to-most prompting only yield substantial improvements for large models; small models show little to no improvement.",
                        "uuids": [
                            "e3537.7",
                            "e3438.1",
                            "e3490.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Self-Consistency Decoding Amplifies Prompt-Driven Gains",
                "if": [
                    {
                        "subject": "self-consistency decoding",
                        "relation": "is_applied_to",
                        "object": "chain-of-thought prompted models"
                    }
                ],
                "then": [
                    {
                        "subject": "accuracy",
                        "relation": "increases",
                        "object": "by up to 18 percentage points on arithmetic and symbolic reasoning tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Self-consistency improves PaLM-540B accuracy on GSM8K from 56.5% to 74.4%; similar gains observed for other large models and tasks.",
                        "uuids": [
                            "e3513.0",
                            "e3537.3",
                            "e3513.3",
                            "e3513.4",
                            "e3438.2"
                        ]
                    },
                    {
                        "text": "Self-consistency decoding aggregates over diverse reasoning chains, reducing sensitivity to individual chain errors.",
                        "uuids": [
                            "e3438.2",
                            "e3537.3"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        },
        {
            "law": {
                "law_name": "Prompt-Task Alignment is Critical for Reasoning Gains",
                "if": [
                    {
                        "subject": "prompt exemplars",
                        "relation": "are",
                        "object": "aligned with the target task domain and structure"
                    }
                ],
                "then": [
                    {
                        "subject": "chain-of-thought prompting",
                        "relation": "yields",
                        "object": "maximal gains"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Few-shot-CoT is sensitive to exemplar-task mismatch; domain-mismatched exemplars degrade performance.",
                        "uuids": [
                            "e3537.7",
                            "e3438.0",
                            "e3537.5"
                        ]
                    },
                    {
                        "text": "Prompt design remains brittle: some reasonable-seeming templates perform much worse; prompt sensitivity remains (annotator style, exemplar content can change outcomes notably for some tasks).",
                        "uuids": [
                            "e3537.7",
                            "e3523.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Shallow or Highly Regular Tasks Do Not Require Prompt-Driven Emergence",
                "if": [
                    {
                        "subject": "task",
                        "relation": "is",
                        "object": "shallow reasoning or has strong statistical regularities"
                    }
                ],
                "then": [
                    {
                        "subject": "small models or standard prompting",
                        "relation": "can",
                        "object": "achieve high accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Some tasks (e.g., simple NLI, shallow reasoning) can be solved by small models or without prompt-driven emergence.",
                        "uuids": [
                            "e3544.0",
                            "e3544.9",
                            "e3544.12"
                        ]
                    },
                    {
                        "text": "RuleTakers and RoBERTa-based models achieve high accuracy on synthetic rulebase tasks without prompt-driven emergence, though these may be less challenging.",
                        "uuids": [
                            "e3525.1",
                            "e3525.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If chain-of-thought or least-to-most prompting is applied to a new multi-step reasoning task with a large model (&gt;100B), and prompt templates are carefully aligned, sharp accuracy gains will be observed.",
        "If self-consistency decoding is added to chain-of-thought prompting for a new arithmetic or symbolic task, accuracy will increase by 10-20 percentage points.",
        "If prompt templates are poorly aligned or exemplars are mismatched to the task, even large models will show reduced gains from chain-of-thought prompting."
    ],
    "new_predictions_unknown": [
        "If a new model is trained to just below the emergence threshold (e.g., 80B parameters), it is unclear whether prompt-driven reasoning will appear or if a sharp threshold will be observed.",
        "If prompt templates are automatically generated or optimized, it is unknown whether they will match or exceed hand-crafted instructive triggers in eliciting reasoning.",
        "If a new class of tasks with complex, non-arithmetic multi-step reasoning is introduced, it is unknown whether the same threshold and prompt effects will hold."
    ],
    "negative_experiments": [
        "If small models (&lt;10B) show large gains from chain-of-thought or least-to-most prompting, the threshold law would be challenged.",
        "If self-consistency decoding fails to improve or reduces accuracy on multi-step reasoning tasks, the theory's claims would be weakened.",
        "If prompt template choice and exemplar-task alignment are found to have no effect on reasoning gains, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Some logic-specific or neurosymbolic interventions (e.g., solver-augmented models, logic-driven fine-tuning) can yield high logical reasoning performance even for smaller models, suggesting alternative routes to reasoning beyond prompt-driven emergence.",
            "uuids": [
                "e3522.0",
                "e3522.4",
                "e3454.8",
                "e3454.10",
                "e3439.0",
                "e3432.0",
                "e3521.2",
                "e3521.1"
            ]
        },
        {
            "text": "Some models (e.g., Mistral-7B) outperform larger open-source models on deep logic tasks, suggesting that training data and architecture can modulate the threshold.",
            "uuids": [
                "e3430.3"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "RuleTakers and RoBERTa-based models achieve high accuracy on synthetic rulebase tasks without prompt-driven emergence, though these may be less challenging.",
            "uuids": [
                "e3525.1",
                "e3525.2"
            ]
        },
        {
            "text": "Some neurosymbolic and solver-augmented approaches (e.g., LOGIPT, Logic-LM) achieve high logical reasoning accuracy without relying on prompt-driven emergence.",
            "uuids": [
                "e3439.0",
                "e3454.8"
            ]
        }
    ],
    "special_cases": [
        "For tasks with shallow reasoning or strong statistical regularities, prompt-driven emergence may not be necessary.",
        "If the model is trained with logic-driven objectives, the threshold for prompt-driven emergence may be lowered.",
        "Architectural or training interventions (e.g., logic-specific fine-tuning, neurosymbolic augmentation) can bypass or lower the emergence threshold."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompting and emergence, but explicit threshold law and integration of template/exemplar alignment and self-consistency as a unified threshold theory is novel here]",
            "Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [Zero-shot-CoT and template ablations]",
            "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [Self-consistency decoding]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Least-to-most prompting and prompt structure effects]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multidimensional Evaluation Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-676</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-676</p>
                <p><strong>Name:</strong> Multidimensional Evaluation Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that robust evaluation of LLM-generated scientific theories requires a multidimensional approach that integrates human expert judgment, LLM-based evaluators, and task-specific automated metrics. Each evaluation axis (e.g., factuality, novelty, helpfulness, clarity, calibration, and robustness) captures distinct aspects of scientific theory quality, and no single metric or evaluator is sufficient. The theory further asserts that alignment between LLM-based evaluators and human experts is highest when evaluation rubrics are explicit, multidimensional, and when LLM evaluators are calibrated and provided with reference answers or chain-of-thought prompts. However, systematic biases and domain/task dependencies persist, necessitating ensemble and hybrid evaluation protocols.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Multidimensionality Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_of_LLM-generated_scientific_theories &#8594; is_performed_with &#8594; single-metric or single-judge approach</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation &#8594; fails_to_capture &#8594; full spectrum of theory quality (e.g., factuality, novelty, helpfulness, clarity, calibration, robustness)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Evaluation rubrics such as Validness/Novelty/Helpfulness (e6116.2), LA/OCQ/TQ/PC/H (e6153.3), and 10-skill taxonomy (e6118.2) show that multiple axes are needed to capture theory quality; single metrics like BLEU/ROUGE or factuality alone miss key aspects. <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6153.html#e6153.3" class="evidence-link">[e6153.3]</a> <a href="../results/extraction-result-6118.html#e6118.2" class="evidence-link">[e6118.2]</a> <a href="../results/extraction-result-6018.html#e6018.1" class="evidence-link">[e6018.1]</a> <a href="../results/extraction-result-6025.html#e6025.6" class="evidence-link">[e6025.6]</a> <a href="../results/extraction-result-6129.html#e6129.1" class="evidence-link">[e6129.1]</a> <a href="../results/extraction-result-6001.html#e6001.6" class="evidence-link">[e6001.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While multidimensional rubrics are used, the explicit law that single-metric evaluation is insufficient for scientific theory evaluation is novel.</p>            <p><strong>What Already Exists:</strong> Multidimensional rubrics are used in human evaluation and some LLM evaluation studies.</p>            <p><strong>What is Novel:</strong> The formalization that no single metric or judge suffices for scientific theory evaluation, and that multidimensionality is a necessary property.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [multidimensional evaluation]</li>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [validness/novelty/helpfulness rubric]</li>
</ul>
            <h3>Statement 1: Alignment-Through-Explicit-Rubric Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-based_evaluator &#8594; is_provided_with &#8594; explicit multidimensional evaluation rubric and reference answers or chain-of-thought prompts</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-based_evaluator &#8594; achieves &#8594; higher alignment with human expert judgments</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM evaluators (GPT-4, ChatGPT) align more closely with human experts when given explicit rubrics (e6116.2, e6153.3), reference answers (e6147.4), and chain-of-thought prompts (e6149.1, e6155.4, e6011.3). Inclusion of few-shot examples and reference-guided grading improves correlation (e6166.5, e6011.3). <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6153.html#e6153.3" class="evidence-link">[e6153.3]</a> <a href="../results/extraction-result-6147.html#e6147.4" class="evidence-link">[e6147.4]</a> <a href="../results/extraction-result-6149.html#e6149.1" class="evidence-link">[e6149.1]</a> <a href="../results/extraction-result-6155.html#e6155.4" class="evidence-link">[e6155.4]</a> <a href="../results/extraction-result-6011.html#e6011.3" class="evidence-link">[e6011.3]</a> <a href="../results/extraction-result-6166.html#e6166.5" class="evidence-link">[e6166.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Empirical findings exist, but the law's generalization to scientific theory evaluation is novel.</p>            <p><strong>What Already Exists:</strong> Empirical studies show improved LLM-human alignment with explicit rubrics and reference answers.</p>            <p><strong>What is Novel:</strong> The law that explicit multidimensional rubrics and reference-guided evaluation are necessary for high alignment in scientific theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [rubric and reference effects]</li>
    <li>Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [reference-guided grading]</li>
</ul>
            <h3>Statement 2: Systematic Bias and Domain Dependence Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-based_evaluator &#8594; is_applied_to &#8594; diverse domains or tasks</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-based_evaluator &#8594; exhibits &#8594; systematic biases (e.g., high-score bias, position bias, domain/task dependence)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM evaluators show high-score bias (e6165.14, e6153.0), position bias (e6165.5), and domain/task dependence (e6165.14, e6131.8, e6011.2). Alignment varies by language, metric, and task (e6153.1, e6165.14). <a href="../results/extraction-result-6165.html#e6165.14" class="evidence-link">[e6165.14]</a> <a href="../results/extraction-result-6153.html#e6153.0" class="evidence-link">[e6153.0]</a> <a href="../results/extraction-result-6165.html#e6165.5" class="evidence-link">[e6165.5]</a> <a href="../results/extraction-result-6131.html#e6131.8" class="evidence-link">[e6131.8]</a> <a href="../results/extraction-result-6011.html#e6011.2" class="evidence-link">[e6011.2]</a> <a href="../results/extraction-result-6153.html#e6153.1" class="evidence-link">[e6153.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Biases are known, but their formalization as a law for scientific theory evaluation is novel.</p>            <p><strong>What Already Exists:</strong> Empirical observations of bias and domain dependence in LLM evaluators.</p>            <p><strong>What is Novel:</strong> The law that these biases are systematic and must be accounted for in scientific theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [systematic bias]</li>
    <li>Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [domain dependence]</li>
</ul>
            <h3>Statement 3: Ensemble and Hybrid Evaluation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_protocol &#8594; combines &#8594; human expert judgment, LLM-based evaluators, and task-specific automated metrics</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation &#8594; achieves &#8594; greater robustness and reliability than any single method</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Ensemble approaches (e.g., Retrieve→LM + NP, e6127.6), hybrid protocols (e.g., human+LLM, e6114.0, e6011.2), and multi-criteria evaluation (e6116.2, e6153.3) yield more robust and reliable evaluation than any single method. <a href="../results/extraction-result-6127.html#e6127.6" class="evidence-link">[e6127.6]</a> <a href="../results/extraction-result-6114.html#e6114.0" class="evidence-link">[e6114.0]</a> <a href="../results/extraction-result-6011.html#e6011.2" class="evidence-link">[e6011.2]</a> <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6153.html#e6153.3" class="evidence-link">[e6153.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Ensemble methods are used, but the law's generalization to scientific theory evaluation is novel.</p>            <p><strong>What Already Exists:</strong> Ensemble and hybrid evaluation protocols are used in some studies.</p>            <p><strong>What is Novel:</strong> The law that ensemble/hybrid evaluation is necessary for robust scientific theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2023) FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation [ensemble evaluation]</li>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [hybrid evaluation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new LLM-generated scientific theory is evaluated using only a single metric (e.g., factuality), important aspects such as novelty or helpfulness will be missed, leading to incomplete or misleading evaluation.</li>
                <li>If an LLM-based evaluator is provided with explicit multidimensional rubrics and reference answers, its alignment with human expert judgments will increase compared to when such guidance is absent.</li>
                <li>If ensemble evaluation protocols are used, the reliability and robustness of scientific theory evaluation will be higher than with any single method.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLM-based evaluators are further fine-tuned on multidimensional, expert-annotated scientific theory evaluations, their alignment with human experts may approach or surpass current human inter-annotator agreement.</li>
                <li>If new automated metrics are developed that integrate multidimensional evaluation axes, they may reduce the need for human expert involvement in routine scientific theory evaluation.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a single-metric or single-judge evaluation protocol is shown to capture all relevant aspects of scientific theory quality as reliably as multidimensional or ensemble protocols, the multidimensionality and ensemble laws would be falsified.</li>
                <li>If LLM-based evaluators provided with explicit rubrics and reference answers do not improve alignment with human experts, the alignment-through-explicit-rubric law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some domains with highly objective, automated metrics (e.g., code generation strict accuracy, e6108.2) may not require multidimensional or ensemble evaluation. <a href="../results/extraction-result-6108.html#e6108.1" class="evidence-link">[e6108.1]</a> <a href="../results/extraction-result-6108.html#e6108.2" class="evidence-link">[e6108.2]</a> </li>
    <li>In certain tasks, LLM-based evaluators may already achieve near-human agreement without explicit rubrics (e6011.2, e6166.2). <a href="../results/extraction-result-6011.html#e6011.2" class="evidence-link">[e6011.2]</a> <a href="../results/extraction-result-6166.html#e6166.2" class="evidence-link">[e6166.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While elements exist, the comprehensive, formalized theory is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [multidimensional evaluation, meta-correlation]</li>
    <li>Min et al. (2023) FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation [ensemble evaluation]</li>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [multidimensional rubrics]</li>
    <li>Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [systematic bias, reference-guided grading]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multidimensional Evaluation Alignment Theory",
    "theory_description": "This theory posits that robust evaluation of LLM-generated scientific theories requires a multidimensional approach that integrates human expert judgment, LLM-based evaluators, and task-specific automated metrics. Each evaluation axis (e.g., factuality, novelty, helpfulness, clarity, calibration, and robustness) captures distinct aspects of scientific theory quality, and no single metric or evaluator is sufficient. The theory further asserts that alignment between LLM-based evaluators and human experts is highest when evaluation rubrics are explicit, multidimensional, and when LLM evaluators are calibrated and provided with reference answers or chain-of-thought prompts. However, systematic biases and domain/task dependencies persist, necessitating ensemble and hybrid evaluation protocols.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Multidimensionality Law",
                "if": [
                    {
                        "subject": "evaluation_of_LLM-generated_scientific_theories",
                        "relation": "is_performed_with",
                        "object": "single-metric or single-judge approach"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation",
                        "relation": "fails_to_capture",
                        "object": "full spectrum of theory quality (e.g., factuality, novelty, helpfulness, clarity, calibration, robustness)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Evaluation rubrics such as Validness/Novelty/Helpfulness (e6116.2), LA/OCQ/TQ/PC/H (e6153.3), and 10-skill taxonomy (e6118.2) show that multiple axes are needed to capture theory quality; single metrics like BLEU/ROUGE or factuality alone miss key aspects.",
                        "uuids": [
                            "e6116.2",
                            "e6153.3",
                            "e6118.2",
                            "e6018.1",
                            "e6025.6",
                            "e6129.1",
                            "e6001.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Multidimensional rubrics are used in human evaluation and some LLM evaluation studies.",
                    "what_is_novel": "The formalization that no single metric or judge suffices for scientific theory evaluation, and that multidimensionality is a necessary property.",
                    "classification_explanation": "While multidimensional rubrics are used, the explicit law that single-metric evaluation is insufficient for scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [multidimensional evaluation]",
                        "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [validness/novelty/helpfulness rubric]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Alignment-Through-Explicit-Rubric Law",
                "if": [
                    {
                        "subject": "LLM-based_evaluator",
                        "relation": "is_provided_with",
                        "object": "explicit multidimensional evaluation rubric and reference answers or chain-of-thought prompts"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-based_evaluator",
                        "relation": "achieves",
                        "object": "higher alignment with human expert judgments"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM evaluators (GPT-4, ChatGPT) align more closely with human experts when given explicit rubrics (e6116.2, e6153.3), reference answers (e6147.4), and chain-of-thought prompts (e6149.1, e6155.4, e6011.3). Inclusion of few-shot examples and reference-guided grading improves correlation (e6166.5, e6011.3).",
                        "uuids": [
                            "e6116.2",
                            "e6153.3",
                            "e6147.4",
                            "e6149.1",
                            "e6155.4",
                            "e6011.3",
                            "e6166.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical studies show improved LLM-human alignment with explicit rubrics and reference answers.",
                    "what_is_novel": "The law that explicit multidimensional rubrics and reference-guided evaluation are necessary for high alignment in scientific theory evaluation.",
                    "classification_explanation": "Empirical findings exist, but the law's generalization to scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [rubric and reference effects]",
                        "Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [reference-guided grading]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Systematic Bias and Domain Dependence Law",
                "if": [
                    {
                        "subject": "LLM-based_evaluator",
                        "relation": "is_applied_to",
                        "object": "diverse domains or tasks"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-based_evaluator",
                        "relation": "exhibits",
                        "object": "systematic biases (e.g., high-score bias, position bias, domain/task dependence)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM evaluators show high-score bias (e6165.14, e6153.0), position bias (e6165.5), and domain/task dependence (e6165.14, e6131.8, e6011.2). Alignment varies by language, metric, and task (e6153.1, e6165.14).",
                        "uuids": [
                            "e6165.14",
                            "e6153.0",
                            "e6165.5",
                            "e6131.8",
                            "e6011.2",
                            "e6153.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical observations of bias and domain dependence in LLM evaluators.",
                    "what_is_novel": "The law that these biases are systematic and must be accounted for in scientific theory evaluation.",
                    "classification_explanation": "Biases are known, but their formalization as a law for scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [systematic bias]",
                        "Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [domain dependence]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Ensemble and Hybrid Evaluation Law",
                "if": [
                    {
                        "subject": "evaluation_protocol",
                        "relation": "combines",
                        "object": "human expert judgment, LLM-based evaluators, and task-specific automated metrics"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation",
                        "relation": "achieves",
                        "object": "greater robustness and reliability than any single method"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Ensemble approaches (e.g., Retrieve→LM + NP, e6127.6), hybrid protocols (e.g., human+LLM, e6114.0, e6011.2), and multi-criteria evaluation (e6116.2, e6153.3) yield more robust and reliable evaluation than any single method.",
                        "uuids": [
                            "e6127.6",
                            "e6114.0",
                            "e6011.2",
                            "e6116.2",
                            "e6153.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Ensemble and hybrid evaluation protocols are used in some studies.",
                    "what_is_novel": "The law that ensemble/hybrid evaluation is necessary for robust scientific theory evaluation.",
                    "classification_explanation": "Ensemble methods are used, but the law's generalization to scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Min et al. (2023) FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation [ensemble evaluation]",
                        "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [hybrid evaluation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new LLM-generated scientific theory is evaluated using only a single metric (e.g., factuality), important aspects such as novelty or helpfulness will be missed, leading to incomplete or misleading evaluation.",
        "If an LLM-based evaluator is provided with explicit multidimensional rubrics and reference answers, its alignment with human expert judgments will increase compared to when such guidance is absent.",
        "If ensemble evaluation protocols are used, the reliability and robustness of scientific theory evaluation will be higher than with any single method."
    ],
    "new_predictions_unknown": [
        "If LLM-based evaluators are further fine-tuned on multidimensional, expert-annotated scientific theory evaluations, their alignment with human experts may approach or surpass current human inter-annotator agreement.",
        "If new automated metrics are developed that integrate multidimensional evaluation axes, they may reduce the need for human expert involvement in routine scientific theory evaluation."
    ],
    "negative_experiments": [
        "If a single-metric or single-judge evaluation protocol is shown to capture all relevant aspects of scientific theory quality as reliably as multidimensional or ensemble protocols, the multidimensionality and ensemble laws would be falsified.",
        "If LLM-based evaluators provided with explicit rubrics and reference answers do not improve alignment with human experts, the alignment-through-explicit-rubric law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some domains with highly objective, automated metrics (e.g., code generation strict accuracy, e6108.2) may not require multidimensional or ensemble evaluation.",
            "uuids": [
                "e6108.1",
                "e6108.2"
            ]
        },
        {
            "text": "In certain tasks, LLM-based evaluators may already achieve near-human agreement without explicit rubrics (e6011.2, e6166.2).",
            "uuids": [
                "e6011.2",
                "e6166.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some studies, LLM-based evaluators align well with human experts even at high quality levels or in specific domains (e6011.2, e6166.2).",
            "uuids": [
                "e6011.2",
                "e6166.2"
            ]
        }
    ],
    "special_cases": [
        "Domains with clear, objective ground truth (e.g., code correctness, math) may not require multidimensional or ensemble evaluation.",
        "Tasks with highly granular or continuous scoring rubrics may reduce the need for explicit multidimensionality."
    ],
    "existing_theory": {
        "what_already_exists": "Multidimensional rubrics, ensemble evaluation, and explicit rubric use are present in some evaluation studies.",
        "what_is_novel": "The formalization of these as necessary laws for robust scientific theory evaluation, and the explicit integration of human, LLM, and automated metrics.",
        "classification_explanation": "While elements exist, the comprehensive, formalized theory is new.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [multidimensional evaluation, meta-correlation]",
            "Min et al. (2023) FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation [ensemble evaluation]",
            "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [multidimensional rubrics]",
            "Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [systematic bias, reference-guided grading]"
        ]
    },
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
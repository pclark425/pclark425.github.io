<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositional Generalization Gap Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-335</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-335</p>
                <p><strong>Name:</strong> Compositional Generalization Gap Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that the gap between an agent's performance on trained compositional tasks versus novel compositional tasks in interactive text environments is primarily determined by three factors: (1) the degree of primitive skill automaticity achieved before composition training, (2) the diversity of compositional contexts encountered during training, and (3) the presence of spurious correlations that create context-dependent rather than truly compositional representations. The theory suggests that optimal curricula must balance primitive skill mastery, systematic exposure to diverse compositions, and explicit training on compositionally-equivalent but contextually-varied scenarios to minimize this gap. The theory further proposes that the generalization gap exhibits non-linear scaling with composition depth and that curriculum pacing should be adaptive to the agent's current generalization performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <ol>
                <li><a href="../evaluations/theory-evaluation-19.html">theory-evaluation-19</a></li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The compositional generalization gap G can be operationalized as G = P_train - P_novel, where P_train is performance (accuracy or success rate) on trained compositions and P_novel is performance on novel compositions of known primitives.</li>
                <li>The gap G is inversely related to primitive skill automaticity A: as A approaches mastery level (operationalized as ≥95% accuracy with consistent response times), G decreases. This relationship can be approximated as G ∝ 1/(1 + α·A) where α is a domain-specific scaling factor.</li>
                <li>The gap G is inversely related to compositional diversity D in training, where D is measured as the number of distinct compositional patterns and contextual variations encountered. The relationship follows G ∝ exp(-β·D) where β reflects the efficiency of diversity in reducing the gap.</li>
                <li>The gap G is directly proportional to spurious correlation strength S in the training data: when training data contains strong context-specific patterns that are not truly compositional, both S and G increase. This can be expressed as G ∝ γ·S where γ depends on the agent's susceptibility to spurious patterns.</li>
                <li>Optimal curricula follow a three-phase structure: (1) primitive mastery phase where individual skills reach an automaticity threshold (≥95% accuracy), (2) systematic composition phase where diverse combinations are introduced with controlled complexity progression, (3) robustness phase where compositionally-equivalent but contextually-varied scenarios are presented to reduce spurious correlations.</li>
                <li>The rate of compositional complexity increase in curricula should be adaptive to the agent's current generalization gap: when G exceeds a threshold (e.g., >30%), complexity should increase slowly or pause; when G is below threshold (e.g., <15%), complexity can increase more rapidly.</li>
                <li>Negative transfer occurs when compositional training begins before primitive automaticity is achieved (A < 90%), leading to entangled representations that increase G and slow overall learning.</li>
                <li>The compositional generalization gap exhibits a super-linear (potentially exponential) relationship with composition depth d: G ∝ d^k where k > 1, or G ∝ exp(δ·d), meaning that compositions requiring 4+ primitives show disproportionately larger gaps than 2-3 primitive compositions.</li>
                <li>Compositional diversity D must include both structural diversity (different orderings and combinations of primitives) and contextual diversity (different surface forms and scenarios for the same underlying composition) to effectively reduce G.</li>
                <li>The automaticity threshold for primitives is not absolute but context-dependent: primitives that will be used in more complex compositions require higher automaticity (closer to 100%) than those used in simpler compositions.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Agents trained on limited compositional variations show significant performance drops when tested on novel combinations of known primitives in text-based environments, demonstrating the existence of a compositional generalization gap. </li>
    <li>Curriculum learning approaches that introduce compositional complexity gradually show improved generalization compared to random sampling in instruction-following tasks, supporting the importance of structured curriculum design. </li>
    <li>Agents that achieve high accuracy on primitive skills in isolation still fail when those skills must be composed in novel ways, indicating a composition-specific learning challenge distinct from primitive skill acquisition. </li>
    <li>Training data that includes systematic variations of compositional structures leads to better generalization than training on a single canonical form of each composition, supporting the role of compositional diversity. </li>
    <li>Spurious correlations in training data can lead to context-dependent rather than truly compositional learning, increasing the generalization gap. </li>
    <li>The difficulty of compositional generalization increases with the depth of composition (number of primitives that must be combined), suggesting non-linear scaling. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An agent trained with a curriculum that ensures ≥95% accuracy on all primitive skills before introducing any compositions will show a smaller generalization gap compared to an agent that begins composition training at 70-80% primitive accuracy, with the difference being more pronounced for deeper compositions (4+ primitives).</li>
                <li>Introducing compositional tasks in order of increasing number of required primitives (2-way compositions, then 3-way, then 4-way) will result in better generalization to novel compositions than introducing them in random order, with the benefit increasing as composition depth increases.</li>
                <li>Agents trained on multiple contextual variations of each composition type (varying surface forms while maintaining compositional structure) will generalize better to novel compositions than agents trained on fewer variations, even when total training examples are held constant.</li>
                <li>Measuring the generalization gap at regular intervals during training and using it to adjust curriculum pacing (slowing down when gap is large, speeding up when gap is small) will result in faster overall learning and better final generalization than fixed-pace curricula.</li>
                <li>Explicitly training agents on compositionally-equivalent scenarios that differ in spurious features will reduce the generalization gap more than simply increasing the total number of training examples without controlling for spurious correlations.</li>
                <li>The benefit of primitive mastery before composition training will be most pronounced in domains where primitives can be clearly isolated and practiced independently, and less pronounced in domains with highly interdependent primitives.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an agent is trained exclusively on compositions of depth 2 (two primitives) with perfect diversity, it remains unknown whether it can generalize to compositions of depth 4+ without explicit training, or if there exists a fundamental limit requiring training at each depth level. If generalization across depth levels is possible, this would suggest compositional learning is more abstract than the theory currently proposes.</li>
                <li>It is unclear whether there exists an optimal ratio of primitive-only training to compositional training (e.g., 30:70, 50:50, 70:30) that minimizes the generalization gap across all domains, or if this ratio is fundamentally task-dependent and cannot be generalized. Finding a universal ratio would have major implications for curriculum design efficiency.</li>
                <li>The theory predicts that spurious correlations increase the gap, but it is unknown whether explicitly training agents to recognize and ignore spurious correlations (through adversarial examples, causal intervention training, or counterfactual reasoning) can reduce the gap more effectively than simply increasing compositional diversity. If explicit spurious correlation training is more effective, this would suggest a need for a fourth curriculum phase.</li>
                <li>If the compositional generalization gap could be reduced to near-zero for a specific domain through optimal curriculum design, it is unknown whether the learned compositional abilities (the meta-skill of composition itself) would transfer to entirely new domains with different primitives, or if compositional learning is fundamentally domain-specific. Transfer of compositional ability would suggest a more general cognitive mechanism.</li>
                <li>It is unknown whether the super-linear scaling of the generalization gap with composition depth continues indefinitely or reaches an asymptote at some depth level (e.g., 5-6 primitives), and whether this asymptote (if it exists) is architecture-dependent or fundamental to the learning problem.</li>
                <li>The theory does not specify whether there is an interaction effect between primitive automaticity and compositional diversity: it is unknown whether high diversity can compensate for lower automaticity, or whether automaticity is a necessary prerequisite regardless of diversity levels.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents trained with high primitive automaticity (≥95% accuracy) before composition training show the same or larger generalization gaps as agents trained with low primitive automaticity (60-70%), this would challenge the theory's core emphasis on primitive mastery as a prerequisite for compositional learning.</li>
                <li>If curricula with low compositional diversity (few variations of each composition type) produce equal or better generalization than high-diversity curricula when controlling for total training time, this would contradict the theory's prediction about the importance of diverse compositional contexts.</li>
                <li>If the generalization gap does not decrease when spurious correlations are systematically removed from training data (through careful data curation or augmentation), this would challenge the theory's claim about the causal role of spurious correlations in creating the gap.</li>
                <li>If agents show equal or better generalization when compositional complexity is introduced all at once rather than gradually, this would undermine the theory's prescription for systematic complexity progression and adaptive pacing.</li>
                <li>If the relationship between composition depth and generalization gap is linear or sub-linear rather than super-linear, this would contradict the theory's prediction about the exponential scaling of compositional difficulty.</li>
                <li>If negative transfer does not occur when compositional training begins before primitive automaticity is achieved, or if early compositional training actually improves primitive learning, this would challenge the theory's three-phase curriculum structure.</li>
                <li>If measuring and adapting to the generalization gap during training does not improve outcomes compared to fixed curricula, this would suggest that the gap is not a useful signal for curriculum pacing.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not fully account for the role of agent architecture (e.g., modular networks vs. monolithic networks, attention mechanisms, memory systems) in determining the compositional generalization gap, though architecture likely interacts significantly with curriculum design effectiveness. </li>
    <li>The theory does not specify how to handle situations where primitives themselves have hierarchical structure (sub-primitives or primitive compositions), and whether these require separate curriculum phases or recursive application of the three-phase structure. </li>
    <li>The role of language grounding, semantic understanding, and pre-existing world knowledge in reducing the compositional gap for text-based environments is not fully addressed by the theory, though these factors may significantly affect both primitive learning and compositional generalization. </li>
    <li>The theory does not address the role of forgetting or interference during the transition from primitive training to compositional training, or how to maintain primitive performance while learning compositions. </li>
    <li>The theory does not specify how to quantitatively measure compositional diversity (D) in practice, particularly how to weight structural versus contextual diversity, or how to determine when sufficient diversity has been achieved. </li>
    <li>The theory does not fully account for the role of exploration strategies and interactive learning in text environments, where agents may need to discover compositional structures through trial and error rather than supervised learning. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Narvekar et al. (2020) Curriculum learning for reinforcement learning domains: A framework and survey [Related work on curriculum learning in RL but does not specifically theorize about compositional generalization gaps or propose the three-phase structure with automaticity thresholds]</li>
    <li>Lake and Baroni (2018) Generalization without systematicity [Identifies and characterizes compositional generalization challenges but does not propose a curriculum-based theory or specify how to minimize the gap]</li>
    <li>Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data [Provides measurement methods and benchmarks for compositional generalization but does not propose a theory of how curricula affect the gap]</li>
    <li>Andreas (2019) Good-Enough Compositional Data Augmentation [Proposes data augmentation approaches for compositional generalization but not a comprehensive curriculum theory with phases and adaptive pacing]</li>
    <li>Bengio et al. (2009) Curriculum learning [Foundational work on curriculum learning but does not address compositional generalization specifically or propose the automaticity-diversity-spurious correlation framework]</li>
    <li>Eppe et al. (2022) Intelligent problem-solving as integrated hierarchical reinforcement learning [Discusses hierarchical learning but does not propose the specific compositional generalization gap theory with its three factors and three-phase curriculum]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositional Generalization Gap Theory",
    "theory_description": "This theory posits that the gap between an agent's performance on trained compositional tasks versus novel compositional tasks in interactive text environments is primarily determined by three factors: (1) the degree of primitive skill automaticity achieved before composition training, (2) the diversity of compositional contexts encountered during training, and (3) the presence of spurious correlations that create context-dependent rather than truly compositional representations. The theory suggests that optimal curricula must balance primitive skill mastery, systematic exposure to diverse compositions, and explicit training on compositionally-equivalent but contextually-varied scenarios to minimize this gap. The theory further proposes that the generalization gap exhibits non-linear scaling with composition depth and that curriculum pacing should be adaptive to the agent's current generalization performance.",
    "supporting_evidence": [
        {
            "text": "Agents trained on limited compositional variations show significant performance drops when tested on novel combinations of known primitives in text-based environments, demonstrating the existence of a compositional generalization gap.",
            "citations": [
                "Jiang et al. (2020) Compositional generalization in semantic parsing",
                "Lake and Baroni (2018) Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
                "Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"
            ]
        },
        {
            "text": "Curriculum learning approaches that introduce compositional complexity gradually show improved generalization compared to random sampling in instruction-following tasks, supporting the importance of structured curriculum design.",
            "citations": [
                "Narvekar et al. (2020) Curriculum learning for reinforcement learning domains: A framework and survey",
                "Eppe et al. (2022) Intelligent problem-solving as integrated hierarchical reinforcement learning"
            ]
        },
        {
            "text": "Agents that achieve high accuracy on primitive skills in isolation still fail when those skills must be composed in novel ways, indicating a composition-specific learning challenge distinct from primitive skill acquisition.",
            "citations": [
                "Andreas (2019) Good-Enough Compositional Data Augmentation",
                "Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"
            ]
        },
        {
            "text": "Training data that includes systematic variations of compositional structures leads to better generalization than training on a single canonical form of each composition, supporting the role of compositional diversity.",
            "citations": [
                "Bahdanau et al. (2019) Systematic generalization: What is required and can it be learned?",
                "Akyürek et al. (2021) Learning to Recombine and Resample Data for Compositional Generalization"
            ]
        },
        {
            "text": "Spurious correlations in training data can lead to context-dependent rather than truly compositional learning, increasing the generalization gap.",
            "citations": [
                "Bahdanau et al. (2019) Systematic generalization: What is required and can it be learned?",
                "Hupkes et al. (2020) Compositionality decomposed: How do neural networks generalise?"
            ]
        },
        {
            "text": "The difficulty of compositional generalization increases with the depth of composition (number of primitives that must be combined), suggesting non-linear scaling.",
            "citations": [
                "Lake and Baroni (2018) Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
                "Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"
            ]
        }
    ],
    "theory_statements": [
        "The compositional generalization gap G can be operationalized as G = P_train - P_novel, where P_train is performance (accuracy or success rate) on trained compositions and P_novel is performance on novel compositions of known primitives.",
        "The gap G is inversely related to primitive skill automaticity A: as A approaches mastery level (operationalized as ≥95% accuracy with consistent response times), G decreases. This relationship can be approximated as G ∝ 1/(1 + α·A) where α is a domain-specific scaling factor.",
        "The gap G is inversely related to compositional diversity D in training, where D is measured as the number of distinct compositional patterns and contextual variations encountered. The relationship follows G ∝ exp(-β·D) where β reflects the efficiency of diversity in reducing the gap.",
        "The gap G is directly proportional to spurious correlation strength S in the training data: when training data contains strong context-specific patterns that are not truly compositional, both S and G increase. This can be expressed as G ∝ γ·S where γ depends on the agent's susceptibility to spurious patterns.",
        "Optimal curricula follow a three-phase structure: (1) primitive mastery phase where individual skills reach an automaticity threshold (≥95% accuracy), (2) systematic composition phase where diverse combinations are introduced with controlled complexity progression, (3) robustness phase where compositionally-equivalent but contextually-varied scenarios are presented to reduce spurious correlations.",
        "The rate of compositional complexity increase in curricula should be adaptive to the agent's current generalization gap: when G exceeds a threshold (e.g., &gt;30%), complexity should increase slowly or pause; when G is below threshold (e.g., &lt;15%), complexity can increase more rapidly.",
        "Negative transfer occurs when compositional training begins before primitive automaticity is achieved (A &lt; 90%), leading to entangled representations that increase G and slow overall learning.",
        "The compositional generalization gap exhibits a super-linear (potentially exponential) relationship with composition depth d: G ∝ d^k where k &gt; 1, or G ∝ exp(δ·d), meaning that compositions requiring 4+ primitives show disproportionately larger gaps than 2-3 primitive compositions.",
        "Compositional diversity D must include both structural diversity (different orderings and combinations of primitives) and contextual diversity (different surface forms and scenarios for the same underlying composition) to effectively reduce G.",
        "The automaticity threshold for primitives is not absolute but context-dependent: primitives that will be used in more complex compositions require higher automaticity (closer to 100%) than those used in simpler compositions."
    ],
    "new_predictions_likely": [
        "An agent trained with a curriculum that ensures ≥95% accuracy on all primitive skills before introducing any compositions will show a smaller generalization gap compared to an agent that begins composition training at 70-80% primitive accuracy, with the difference being more pronounced for deeper compositions (4+ primitives).",
        "Introducing compositional tasks in order of increasing number of required primitives (2-way compositions, then 3-way, then 4-way) will result in better generalization to novel compositions than introducing them in random order, with the benefit increasing as composition depth increases.",
        "Agents trained on multiple contextual variations of each composition type (varying surface forms while maintaining compositional structure) will generalize better to novel compositions than agents trained on fewer variations, even when total training examples are held constant.",
        "Measuring the generalization gap at regular intervals during training and using it to adjust curriculum pacing (slowing down when gap is large, speeding up when gap is small) will result in faster overall learning and better final generalization than fixed-pace curricula.",
        "Explicitly training agents on compositionally-equivalent scenarios that differ in spurious features will reduce the generalization gap more than simply increasing the total number of training examples without controlling for spurious correlations.",
        "The benefit of primitive mastery before composition training will be most pronounced in domains where primitives can be clearly isolated and practiced independently, and less pronounced in domains with highly interdependent primitives."
    ],
    "new_predictions_unknown": [
        "If an agent is trained exclusively on compositions of depth 2 (two primitives) with perfect diversity, it remains unknown whether it can generalize to compositions of depth 4+ without explicit training, or if there exists a fundamental limit requiring training at each depth level. If generalization across depth levels is possible, this would suggest compositional learning is more abstract than the theory currently proposes.",
        "It is unclear whether there exists an optimal ratio of primitive-only training to compositional training (e.g., 30:70, 50:50, 70:30) that minimizes the generalization gap across all domains, or if this ratio is fundamentally task-dependent and cannot be generalized. Finding a universal ratio would have major implications for curriculum design efficiency.",
        "The theory predicts that spurious correlations increase the gap, but it is unknown whether explicitly training agents to recognize and ignore spurious correlations (through adversarial examples, causal intervention training, or counterfactual reasoning) can reduce the gap more effectively than simply increasing compositional diversity. If explicit spurious correlation training is more effective, this would suggest a need for a fourth curriculum phase.",
        "If the compositional generalization gap could be reduced to near-zero for a specific domain through optimal curriculum design, it is unknown whether the learned compositional abilities (the meta-skill of composition itself) would transfer to entirely new domains with different primitives, or if compositional learning is fundamentally domain-specific. Transfer of compositional ability would suggest a more general cognitive mechanism.",
        "It is unknown whether the super-linear scaling of the generalization gap with composition depth continues indefinitely or reaches an asymptote at some depth level (e.g., 5-6 primitives), and whether this asymptote (if it exists) is architecture-dependent or fundamental to the learning problem.",
        "The theory does not specify whether there is an interaction effect between primitive automaticity and compositional diversity: it is unknown whether high diversity can compensate for lower automaticity, or whether automaticity is a necessary prerequisite regardless of diversity levels."
    ],
    "negative_experiments": [
        "If agents trained with high primitive automaticity (≥95% accuracy) before composition training show the same or larger generalization gaps as agents trained with low primitive automaticity (60-70%), this would challenge the theory's core emphasis on primitive mastery as a prerequisite for compositional learning.",
        "If curricula with low compositional diversity (few variations of each composition type) produce equal or better generalization than high-diversity curricula when controlling for total training time, this would contradict the theory's prediction about the importance of diverse compositional contexts.",
        "If the generalization gap does not decrease when spurious correlations are systematically removed from training data (through careful data curation or augmentation), this would challenge the theory's claim about the causal role of spurious correlations in creating the gap.",
        "If agents show equal or better generalization when compositional complexity is introduced all at once rather than gradually, this would undermine the theory's prescription for systematic complexity progression and adaptive pacing.",
        "If the relationship between composition depth and generalization gap is linear or sub-linear rather than super-linear, this would contradict the theory's prediction about the exponential scaling of compositional difficulty.",
        "If negative transfer does not occur when compositional training begins before primitive automaticity is achieved, or if early compositional training actually improves primitive learning, this would challenge the theory's three-phase curriculum structure.",
        "If measuring and adapting to the generalization gap during training does not improve outcomes compared to fixed curricula, this would suggest that the gap is not a useful signal for curriculum pacing."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not fully account for the role of agent architecture (e.g., modular networks vs. monolithic networks, attention mechanisms, memory systems) in determining the compositional generalization gap, though architecture likely interacts significantly with curriculum design effectiveness.",
            "citations": [
                "Goyal et al. (2021) Coordination Among Neural Modules Through a Shared Global Workspace",
                "Mittal et al. (2022) Is a Modular Architecture Enough?",
                "Andreas et al. (2016) Neural Module Networks"
            ]
        },
        {
            "text": "The theory does not specify how to handle situations where primitives themselves have hierarchical structure (sub-primitives or primitive compositions), and whether these require separate curriculum phases or recursive application of the three-phase structure.",
            "citations": [
                "Devin et al. (2017) Learning modular neural network policies for multi-task and multi-robot transfer",
                "Eppe et al. (2022) Intelligent problem-solving as integrated hierarchical reinforcement learning"
            ]
        },
        {
            "text": "The role of language grounding, semantic understanding, and pre-existing world knowledge in reducing the compositional gap for text-based environments is not fully addressed by the theory, though these factors may significantly affect both primitive learning and compositional generalization.",
            "citations": [
                "Hill et al. (2020) Environmental drivers of systematicity and generalization in a situated agent",
                "Lake et al. (2017) Building machines that learn and think like people"
            ]
        },
        {
            "text": "The theory does not address the role of forgetting or interference during the transition from primitive training to compositional training, or how to maintain primitive performance while learning compositions.",
            "citations": [
                "Kirkpatrick et al. (2017) Overcoming catastrophic forgetting in neural networks",
                "Parisi et al. (2019) Continual lifelong learning with neural networks: A review"
            ]
        },
        {
            "text": "The theory does not specify how to quantitatively measure compositional diversity (D) in practice, particularly how to weight structural versus contextual diversity, or how to determine when sufficient diversity has been achieved.",
            "citations": [
                "Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"
            ]
        },
        {
            "text": "The theory does not fully account for the role of exploration strategies and interactive learning in text environments, where agents may need to discover compositional structures through trial and error rather than supervised learning.",
            "citations": [
                "Côté et al. (2018) TextWorld: A Learning Environment for Text-based Games",
                "Ammanabrolu and Riedl (2019) Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that end-to-end training without explicit curriculum structure or primitive mastery phases can achieve good compositional generalization in certain domains, which seems to conflict with the theory's emphasis on structured three-phase curricula. This may suggest that the theory's prescriptions are domain-dependent or that certain architectures can implicitly learn compositional structure.",
            "citations": [
                "Hupkes et al. (2020) Compositionality decomposed: How do neural networks generalise?",
                "Russin et al. (2019) Compositional generalization in a deep seq2seq model by separating syntax and semantics"
            ]
        },
        {
            "text": "Meta-learning approaches that do not follow the three-phase curriculum structure proposed by the theory have shown strong compositional generalization in some settings, suggesting that learning-to-learn mechanisms may provide an alternative path to compositional generalization that does not require explicit primitive mastery.",
            "citations": [
                "Lake et al. (2019) Compositional generalization through meta sequence-to-sequence learning",
                "Finn et al. (2017) Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"
            ]
        },
        {
            "text": "Some research suggests that certain forms of data augmentation or regularization can improve compositional generalization without requiring the structured curriculum phases proposed by the theory, potentially offering more efficient alternatives.",
            "citations": [
                "Andreas (2019) Good-Enough Compositional Data Augmentation",
                "Akyürek et al. (2021) Learning to Recombine and Resample Data for Compositional Generalization"
            ]
        }
    ],
    "special_cases": [
        "For domains where primitives are highly interdependent and cannot be meaningfully practiced in isolation (e.g., dialogue where context is essential), the theory's primitive mastery phase may need to be modified to include minimal necessary compositions or context, rather than pure primitive isolation.",
        "In cases where the number of possible compositions is exponentially large relative to training resources, the diversity requirement may need to be satisfied through systematic sampling strategies (e.g., covering different compositional patterns) rather than exhaustive coverage of all variations.",
        "For agents with strong pre-training on language, related tasks, or world knowledge, the primitive mastery phase may be shortened or partially skipped if transfer learning provides sufficient automaticity for some or all primitives, though this should be verified through assessment.",
        "When primitives have varying difficulty levels, the theory suggests that easier primitives should reach automaticity before harder ones are introduced, creating a sub-structure within the primitive mastery phase with its own internal curriculum.",
        "In domains with very shallow composition depth (maximum 2-3 primitives), the super-linear scaling prediction may not be observable, and simpler curriculum structures may be sufficient.",
        "For real-time or resource-constrained applications, the theory's adaptive pacing mechanism may need to be approximated or simplified, potentially using periodic rather than continuous gap assessment.",
        "In cases where primitives evolve or need refinement during compositional training (e.g., when compositional practice reveals primitive deficiencies), the curriculum may need to cycle back to primitive training, suggesting an iterative rather than strictly sequential three-phase structure."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Narvekar et al. (2020) Curriculum learning for reinforcement learning domains: A framework and survey [Related work on curriculum learning in RL but does not specifically theorize about compositional generalization gaps or propose the three-phase structure with automaticity thresholds]",
            "Lake and Baroni (2018) Generalization without systematicity [Identifies and characterizes compositional generalization challenges but does not propose a curriculum-based theory or specify how to minimize the gap]",
            "Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data [Provides measurement methods and benchmarks for compositional generalization but does not propose a theory of how curricula affect the gap]",
            "Andreas (2019) Good-Enough Compositional Data Augmentation [Proposes data augmentation approaches for compositional generalization but not a comprehensive curriculum theory with phases and adaptive pacing]",
            "Bengio et al. (2009) Curriculum learning [Foundational work on curriculum learning but does not address compositional generalization specifically or propose the automaticity-diversity-spurious correlation framework]",
            "Eppe et al. (2022) Intelligent problem-solving as integrated hierarchical reinforcement learning [Discusses hierarchical learning but does not propose the specific compositional generalization gap theory with its three factors and three-phase curriculum]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-177",
    "original_theory_name": "Compositional Generalization Gap Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
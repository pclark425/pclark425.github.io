<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1902</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1902</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format in which a problem is presented acts as an information bottleneck, filtering and structuring the information that reaches the LLM's reasoning process. Formats that minimize irrelevant information and maximize the salience of task-relevant cues enable more efficient and accurate LLM processing, while formats that introduce noise or ambiguity degrade performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Salience Maximization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; task_relevant_salience</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_improved_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Highlighting key information in prompts (e.g., bolding, explicit cues) improves LLM accuracy. </li>
    <li>Ambiguous or cluttered formats lead to more errors and hallucinations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law reframes prompt design as an information-theoretic bottleneck problem.</p>            <p><strong>What Already Exists:</strong> Prompt clarity and explicitness are known to improve LLM performance.</p>            <p><strong>What is Novel:</strong> The explicit analogy to an information bottleneck and the focus on salience maximization is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise cues]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience and relevance in prompts]</li>
</ul>
            <h3>Statement 1: Noise Amplification Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; irrelevant_or_ambiguous_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_degraded_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are more likely to hallucinate or make errors when prompts include distractors or ambiguous phrasing. </li>
    <li>Removing irrelevant details from prompts improves accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes prompt clarity findings into an information-theoretic framework.</p>            <p><strong>What Already Exists:</strong> Prompt ambiguity and distractors are known to harm LLM performance.</p>            <p><strong>What is Novel:</strong> The law formalizes this as a general principle of noise amplification through the information bottleneck.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise cues]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience and relevance in prompts]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Explicitly marking relevant information in a prompt will increase LLM accuracy compared to unmarked or cluttered formats.</li>
                <li>Removing irrelevant details from a prompt will reduce hallucination rates.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained on highly noisy data, they may develop robustness to noise, reducing the impact of the bottleneck.</li>
                <li>Future LLMs with advanced attention mechanisms may be less sensitive to noise in problem presentation.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If adding irrelevant information does not degrade performance, the theory is falsified.</li>
                <li>If maximizing salience does not improve performance, the theory's mechanism is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs use irrelevant information as useful context due to spurious correlations in training data. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory reframes prompt design as an information-theoretic bottleneck, synthesizing existing findings.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise cues]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience and relevance in prompts]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of LLM Problem Presentation",
    "theory_description": "This theory proposes that the format in which a problem is presented acts as an information bottleneck, filtering and structuring the information that reaches the LLM's reasoning process. Formats that minimize irrelevant information and maximize the salience of task-relevant cues enable more efficient and accurate LLM processing, while formats that introduce noise or ambiguity degrade performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Salience Maximization Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "task_relevant_salience"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_improved_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Highlighting key information in prompts (e.g., bolding, explicit cues) improves LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or cluttered formats lead to more errors and hallucinations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt clarity and explicitness are known to improve LLM performance.",
                    "what_is_novel": "The explicit analogy to an information bottleneck and the focus on salience maximization is new.",
                    "classification_explanation": "The law reframes prompt design as an information-theoretic bottleneck problem.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise cues]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience and relevance in prompts]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Noise Amplification Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "irrelevant_or_ambiguous_information"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_degraded_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are more likely to hallucinate or make errors when prompts include distractors or ambiguous phrasing.",
                        "uuids": []
                    },
                    {
                        "text": "Removing irrelevant details from prompts improves accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt ambiguity and distractors are known to harm LLM performance.",
                    "what_is_novel": "The law formalizes this as a general principle of noise amplification through the information bottleneck.",
                    "classification_explanation": "The law synthesizes prompt clarity findings into an information-theoretic framework.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise cues]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience and relevance in prompts]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Explicitly marking relevant information in a prompt will increase LLM accuracy compared to unmarked or cluttered formats.",
        "Removing irrelevant details from a prompt will reduce hallucination rates."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained on highly noisy data, they may develop robustness to noise, reducing the impact of the bottleneck.",
        "Future LLMs with advanced attention mechanisms may be less sensitive to noise in problem presentation."
    ],
    "negative_experiments": [
        "If adding irrelevant information does not degrade performance, the theory is falsified.",
        "If maximizing salience does not improve performance, the theory's mechanism is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs use irrelevant information as useful context due to spurious correlations in training data.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can ignore distractors if they have been instruction-tuned for robustness.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For tasks requiring integration of multiple, seemingly irrelevant cues, removing information may harm performance.",
        "Highly redundant prompts may be robust to noise."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt clarity and explicitness are known to improve LLM performance.",
        "what_is_novel": "The explicit information bottleneck framing and the generalization to all problem formats is new.",
        "classification_explanation": "The theory reframes prompt design as an information-theoretic bottleneck, synthesizing existing findings.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise cues]",
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Salience and relevance in prompts]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-653",
    "original_theory_name": "Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Commonsense Knowledge Integration Specificity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-101</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-101</p>
                <p><strong>Name:</strong> Commonsense Knowledge Integration Specificity Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how agents perform planning with external tools in partially observable text environments, including belief-state updates that incorporate tool outputs and guide shortest-path actions, based on the following results.</p>
                <p><strong>Description:</strong> When integrating external commonsense knowledge (from sources like ConceptNet, Visual Genome, or COMET) into text-based RL agents operating in partially observable environments, the effectiveness depends critically on three factors: (1) the specificity and grounding of knowledge to current task context, (2) the representation method that preserves object-level correspondences between observed state and commonsense expectations, and (3) the incremental exposure strategy that prevents information overload. Agents that construct focused, object-centric difference representations (explicitly comparing observed state to commonsense expectations for each entity) consistently outperform agents that use aggregated or undifferentiated commonsense knowledge. Furthermore, incremental exposure (evolving graphs that grow with observations) is more effective than providing complete knowledge upfront, and grounded commonsense sources (e.g., Visual Genome scene graphs) can outperform abstract knowledge (e.g., ConceptNet) for spatial/physical reasoning tasks. The integration method matters as much as the knowledge source: filtering mechanisms (Extract-by-Meaning, Narrow-by-Circumstance, Transform-into-Grounded-Representation) and encoding architectures (GAT with co-attention) that preserve entity-level structure are essential for making commonsense actionable.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Object-centric commonsense retrieval (focused on entities in current observation) is more effective than global or undifferentiated commonsense knowledge for action selection in partially observable environments.</li>
                <li>Difference representations that explicitly compare observed state to commonsense expectations for each entity enable agents to identify discrepancies and guide exploration more effectively than merged or aggregated representations.</li>
                <li>Incremental exposure to commonsense (evolving with observations) prevents information overload and focuses learning on relevant knowledge, outperforming complete upfront knowledge provision.</li>
                <li>The source of commonsense knowledge matters: grounded knowledge (Visual Genome scene graphs) can be more effective than abstract knowledge (ConceptNet) for spatial/physical reasoning tasks, though abstract knowledge may be better for other domains.</li>
                <li>Filtering and grounding mechanisms (Extract-by-Meaning, Narrow-by-Circumstance, Transform-into-Grounded-Representation) are essential to make commonsense knowledge actionable in RL agents.</li>
                <li>The encoding architecture matters: Graph attention networks (GAT) with co-attention mechanisms that preserve entity-level structure are more effective than simple aggregation for commonsense integration.</li>
                <li>Pretrained knowledge graph embeddings (e.g., Numberbatch) provide better initialization than random embeddings for commonsense node features.</li>
                <li>Commonsense knowledge is most beneficial when it fills gaps in partial observability (e.g., inferring unmentioned objects) rather than when full state information is available.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>DiffG-RL using Difference Graphs (object-specific state vs commonsense) achieves 0.35±0.02 on TWC OUT Hard vs 0.33±0.01 for KG-A2C (aggregated commonsense) <a href="../results/extraction-result-778.html#e778.0" class="evidence-link">[e778.0]</a> <a href="../results/extraction-result-778.html#e778.1" class="evidence-link">[e778.1]</a> </li>
    <li>KG_Evolve (incremental commonsense exposure) outperforms KG_Full (complete upfront knowledge) in Kitchen Cleanup, showing focused KB prevents information overload <a href="../results/extraction-result-782.html#e782.1" class="evidence-link">[e782.1]</a> <a href="../results/extraction-result-782.html#e782.2" class="evidence-link">[e782.2]</a> </li>
    <li>Text+Commonsense with CDC (Contextual Direct Connections) filtering outperforms naive DC retrieval by focusing on object-to-container relations <a href="../results/extraction-result-782.html#e782.0" class="evidence-link">[e782.0]</a> </li>
    <li>TWC agent-VG using Visual Genome (grounded scene graphs) achieves 0.25±0.01 vs TWC agent-CN (ConceptNet) 0.29±0.02, showing grounded knowledge can be more effective for spatial tasks <a href="../results/extraction-result-778.html#e778.2" class="evidence-link">[e778.2]</a> <a href="../results/extraction-result-778.html#e778.3" class="evidence-link">[e778.3]</a> </li>
    <li>Manual graph curation (focused, pruned subgraphs) reduces steps by 2-5 compared to automated retrieval, demonstrating value of precision <a href="../results/extraction-result-782.html#e782.3" class="evidence-link">[e782.3]</a> </li>
    <li>COMET-A2C using focused HasA inferences enables task completion when objects aren't explicitly mentioned, showing targeted commonsense fills observability gaps <a href="../results/extraction-result-772.html#e772.2" class="evidence-link">[e772.2]</a> </li>
    <li>Swift with affordance injection achieves 30.18 avg vs 27.86 baseline, showing targeted knowledge helps instruction-tuned models <a href="../results/extraction-result-789.html#e789.3" class="evidence-link">[e789.3]</a> </li>
    <li>Ammanabrolu&Riedl KG-based agent using OpenIE with game-specific rules improves performance but requires domain knowledge, showing need for grounding mechanisms <a href="../results/extraction-result-769.html#e769.1" class="evidence-link">[e769.1]</a> </li>
    <li>Object-centric retrieval augmentation (Guo et al. 2020) using past observations improves multi-step reasoning by retrieving relevant context <a href="../results/extraction-result-783.html#e783.5" class="evidence-link">[e783.5]</a> </li>
    <li>Belief+KG agents using GAT encoding and Numberbatch embeddings show that encoding architecture matters for commonsense integration <a href="../results/extraction-result-761.html#e761.0" class="evidence-link">[e761.0]</a> <a href="../results/extraction-result-782.html#e782.0" class="evidence-link">[e782.0]</a> </li>
    <li>DiffG-RL's three-step filtering (EbM, NbC, TGR) is necessary to make Visual Genome triples actionable, showing grounding mechanisms are essential <a href="../results/extraction-result-778.html#e778.0" class="evidence-link">[e778.0]</a> </li>
    <li>Text+Commonsense using co-attention between context and graph nodes enables action-specific knowledge encoding <a href="../results/extraction-result-782.html#e782.0" class="evidence-link">[e782.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An agent that dynamically adjusts the scope of commonsense retrieval based on task difficulty (narrow for simple tasks, broader for complex) will be more sample-efficient than fixed-scope retrieval.</li>
                <li>Agents that use multiple commonsense sources and learn to weight them based on task context (e.g., Visual Genome for spatial tasks, ConceptNet for abstract reasoning) will outperform single-source agents.</li>
                <li>In tasks requiring physical reasoning (e.g., container placement, object manipulation), grounded commonsense from vision will consistently outperform text-only commonsense.</li>
                <li>Combining object-centric retrieval with commonsense knowledge will be more effective than either technique alone for long-horizon tasks.</li>
                <li>Agents that explicitly model the confidence or reliability of commonsense knowledge will be more robust than agents that treat all commonsense equally.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether learned commonsense extraction (end-to-end from raw text using modern LLMs) can match or exceed the performance of curated knowledge bases like ConceptNet for RL agents.</li>
                <li>Whether there exists a universal optimal filtering strategy for commonsense knowledge, or whether filtering must be task-specific and learned.</li>
                <li>Whether agents can learn to detect when commonsense knowledge is misleading or incorrect (e.g., in fantasy worlds) and should be ignored or downweighted.</li>
                <li>Whether the benefits of commonsense knowledge scale with the size of the knowledge base, or whether there are diminishing returns beyond a certain size.</li>
                <li>Whether commonsense knowledge integration provides similar benefits in continuous control tasks as it does in discrete text-based tasks.</li>
                <li>Whether the computational overhead of commonsense retrieval and encoding can be reduced through learned approximations without sacrificing performance.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Demonstrating that undifferentiated commonsense (full graph, no filtering) performs as well as focused difference representations would challenge the specificity claim.</li>
                <li>Showing that random commonsense triples perform as well as carefully-selected relevant triples would question the importance of grounding and filtering.</li>
                <li>Finding that commonsense knowledge provides no benefit in tasks where it should be highly relevant (e.g., object placement tasks) would challenge the theory's scope.</li>
                <li>Showing that simple aggregation of commonsense (e.g., mean pooling) performs as well as structured difference representations would challenge the need for object-centric approaches.</li>
                <li>Demonstrating that providing complete commonsense upfront (KG_Full) consistently outperforms incremental exposure (KG_Evolve) would challenge the information overload hypothesis.</li>
                <li>Finding that abstract commonsense (ConceptNet) consistently outperforms grounded commonsense (Visual Genome) even in spatial tasks would challenge the grounding hypothesis.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically determine the optimal scope of commonsense retrieval for different task types without manual tuning </li>
    <li>The trade-offs between commonsense precision and recall in different environments and how to optimize this trade-off </li>
    <li>How to handle conflicting commonsense knowledge from multiple sources (e.g., when ConceptNet and Visual Genome disagree) </li>
    <li>The computational cost-benefit analysis of commonsense integration: when does the overhead outweigh the benefits </li>
    <li>How commonsense knowledge interacts with other techniques like retrieval-based memory or QA-based extraction </li>
    <li>Whether commonsense knowledge helps more in early training (exploration) or late training (exploitation) </li>
    <li>How to transfer commonsense knowledge learned in one domain to another domain </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Murugesan et al. (2021) Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines [TWC benchmark and initial commonsense integration approaches, but doesn't emphasize difference representations]</li>
    <li>Xu et al. (2022) DiffG-RL: Leveraging Difference between State and Common Sense [Introduces difference graph approach, which is a key component of this theory]</li>
    <li>Speer et al. (2017) ConceptNet 5.5: An Open Multilingual Graph of General Knowledge [ConceptNet knowledge base, foundational resource]</li>
    <li>Krishna et al. (2017) Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations [Visual Genome, alternative grounded commonsense source]</li>
    <li>Bosselut et al. (2019) COMET: Commonsense Transformers for Automatic Knowledge Graph Construction [COMET for generating commonsense inferences]</li>
    <li>Ammanabrolu & Riedl (2019) Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning [Early work on KG-based RL, but uses OpenIE rather than external commonsense]</li>
    <li>Adhikari et al. (2020) Learning Dynamic Knowledge Graphs to Generalize on Text-Based Games [GATA, which uses belief graphs but not external commonsense in the same way]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Commonsense Knowledge Integration Specificity Theory",
    "theory_description": "When integrating external commonsense knowledge (from sources like ConceptNet, Visual Genome, or COMET) into text-based RL agents operating in partially observable environments, the effectiveness depends critically on three factors: (1) the specificity and grounding of knowledge to current task context, (2) the representation method that preserves object-level correspondences between observed state and commonsense expectations, and (3) the incremental exposure strategy that prevents information overload. Agents that construct focused, object-centric difference representations (explicitly comparing observed state to commonsense expectations for each entity) consistently outperform agents that use aggregated or undifferentiated commonsense knowledge. Furthermore, incremental exposure (evolving graphs that grow with observations) is more effective than providing complete knowledge upfront, and grounded commonsense sources (e.g., Visual Genome scene graphs) can outperform abstract knowledge (e.g., ConceptNet) for spatial/physical reasoning tasks. The integration method matters as much as the knowledge source: filtering mechanisms (Extract-by-Meaning, Narrow-by-Circumstance, Transform-into-Grounded-Representation) and encoding architectures (GAT with co-attention) that preserve entity-level structure are essential for making commonsense actionable.",
    "supporting_evidence": [
        {
            "text": "DiffG-RL using Difference Graphs (object-specific state vs commonsense) achieves 0.35±0.02 on TWC OUT Hard vs 0.33±0.01 for KG-A2C (aggregated commonsense)",
            "uuids": [
                "e778.0",
                "e778.1"
            ]
        },
        {
            "text": "KG_Evolve (incremental commonsense exposure) outperforms KG_Full (complete upfront knowledge) in Kitchen Cleanup, showing focused KB prevents information overload",
            "uuids": [
                "e782.1",
                "e782.2"
            ]
        },
        {
            "text": "Text+Commonsense with CDC (Contextual Direct Connections) filtering outperforms naive DC retrieval by focusing on object-to-container relations",
            "uuids": [
                "e782.0"
            ]
        },
        {
            "text": "TWC agent-VG using Visual Genome (grounded scene graphs) achieves 0.25±0.01 vs TWC agent-CN (ConceptNet) 0.29±0.02, showing grounded knowledge can be more effective for spatial tasks",
            "uuids": [
                "e778.2",
                "e778.3"
            ]
        },
        {
            "text": "Manual graph curation (focused, pruned subgraphs) reduces steps by 2-5 compared to automated retrieval, demonstrating value of precision",
            "uuids": [
                "e782.3"
            ]
        },
        {
            "text": "COMET-A2C using focused HasA inferences enables task completion when objects aren't explicitly mentioned, showing targeted commonsense fills observability gaps",
            "uuids": [
                "e772.2"
            ]
        },
        {
            "text": "Swift with affordance injection achieves 30.18 avg vs 27.86 baseline, showing targeted knowledge helps instruction-tuned models",
            "uuids": [
                "e789.3"
            ]
        },
        {
            "text": "Ammanabrolu&Riedl KG-based agent using OpenIE with game-specific rules improves performance but requires domain knowledge, showing need for grounding mechanisms",
            "uuids": [
                "e769.1"
            ]
        },
        {
            "text": "Object-centric retrieval augmentation (Guo et al. 2020) using past observations improves multi-step reasoning by retrieving relevant context",
            "uuids": [
                "e783.5"
            ]
        },
        {
            "text": "Belief+KG agents using GAT encoding and Numberbatch embeddings show that encoding architecture matters for commonsense integration",
            "uuids": [
                "e761.0",
                "e782.0"
            ]
        },
        {
            "text": "DiffG-RL's three-step filtering (EbM, NbC, TGR) is necessary to make Visual Genome triples actionable, showing grounding mechanisms are essential",
            "uuids": [
                "e778.0"
            ]
        },
        {
            "text": "Text+Commonsense using co-attention between context and graph nodes enables action-specific knowledge encoding",
            "uuids": [
                "e782.0"
            ]
        }
    ],
    "theory_statements": [
        "Object-centric commonsense retrieval (focused on entities in current observation) is more effective than global or undifferentiated commonsense knowledge for action selection in partially observable environments.",
        "Difference representations that explicitly compare observed state to commonsense expectations for each entity enable agents to identify discrepancies and guide exploration more effectively than merged or aggregated representations.",
        "Incremental exposure to commonsense (evolving with observations) prevents information overload and focuses learning on relevant knowledge, outperforming complete upfront knowledge provision.",
        "The source of commonsense knowledge matters: grounded knowledge (Visual Genome scene graphs) can be more effective than abstract knowledge (ConceptNet) for spatial/physical reasoning tasks, though abstract knowledge may be better for other domains.",
        "Filtering and grounding mechanisms (Extract-by-Meaning, Narrow-by-Circumstance, Transform-into-Grounded-Representation) are essential to make commonsense knowledge actionable in RL agents.",
        "The encoding architecture matters: Graph attention networks (GAT) with co-attention mechanisms that preserve entity-level structure are more effective than simple aggregation for commonsense integration.",
        "Pretrained knowledge graph embeddings (e.g., Numberbatch) provide better initialization than random embeddings for commonsense node features.",
        "Commonsense knowledge is most beneficial when it fills gaps in partial observability (e.g., inferring unmentioned objects) rather than when full state information is available."
    ],
    "new_predictions_likely": [
        "An agent that dynamically adjusts the scope of commonsense retrieval based on task difficulty (narrow for simple tasks, broader for complex) will be more sample-efficient than fixed-scope retrieval.",
        "Agents that use multiple commonsense sources and learn to weight them based on task context (e.g., Visual Genome for spatial tasks, ConceptNet for abstract reasoning) will outperform single-source agents.",
        "In tasks requiring physical reasoning (e.g., container placement, object manipulation), grounded commonsense from vision will consistently outperform text-only commonsense.",
        "Combining object-centric retrieval with commonsense knowledge will be more effective than either technique alone for long-horizon tasks.",
        "Agents that explicitly model the confidence or reliability of commonsense knowledge will be more robust than agents that treat all commonsense equally."
    ],
    "new_predictions_unknown": [
        "Whether learned commonsense extraction (end-to-end from raw text using modern LLMs) can match or exceed the performance of curated knowledge bases like ConceptNet for RL agents.",
        "Whether there exists a universal optimal filtering strategy for commonsense knowledge, or whether filtering must be task-specific and learned.",
        "Whether agents can learn to detect when commonsense knowledge is misleading or incorrect (e.g., in fantasy worlds) and should be ignored or downweighted.",
        "Whether the benefits of commonsense knowledge scale with the size of the knowledge base, or whether there are diminishing returns beyond a certain size.",
        "Whether commonsense knowledge integration provides similar benefits in continuous control tasks as it does in discrete text-based tasks.",
        "Whether the computational overhead of commonsense retrieval and encoding can be reduced through learned approximations without sacrificing performance."
    ],
    "negative_experiments": [
        "Demonstrating that undifferentiated commonsense (full graph, no filtering) performs as well as focused difference representations would challenge the specificity claim.",
        "Showing that random commonsense triples perform as well as carefully-selected relevant triples would question the importance of grounding and filtering.",
        "Finding that commonsense knowledge provides no benefit in tasks where it should be highly relevant (e.g., object placement tasks) would challenge the theory's scope.",
        "Showing that simple aggregation of commonsense (e.g., mean pooling) performs as well as structured difference representations would challenge the need for object-centric approaches.",
        "Demonstrating that providing complete commonsense upfront (KG_Full) consistently outperforms incremental exposure (KG_Evolve) would challenge the information overload hypothesis.",
        "Finding that abstract commonsense (ConceptNet) consistently outperforms grounded commonsense (Visual Genome) even in spatial tasks would challenge the grounding hypothesis."
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically determine the optimal scope of commonsense retrieval for different task types without manual tuning",
            "uuids": []
        },
        {
            "text": "The trade-offs between commonsense precision and recall in different environments and how to optimize this trade-off",
            "uuids": []
        },
        {
            "text": "How to handle conflicting commonsense knowledge from multiple sources (e.g., when ConceptNet and Visual Genome disagree)",
            "uuids": []
        },
        {
            "text": "The computational cost-benefit analysis of commonsense integration: when does the overhead outweigh the benefits",
            "uuids": []
        },
        {
            "text": "How commonsense knowledge interacts with other techniques like retrieval-based memory or QA-based extraction",
            "uuids": []
        },
        {
            "text": "Whether commonsense knowledge helps more in early training (exploration) or late training (exploitation)",
            "uuids": []
        },
        {
            "text": "How to transfer commonsense knowledge learned in one domain to another domain",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some agents achieve strong performance without any external commonsense knowledge (e.g., LSTM-DQN achieves ~100% in Home world, DRRN achieves 13.0% on Jericho)",
            "uuids": [
                "e872.0",
                "e894.0"
            ]
        },
        {
            "text": "In some simple tasks (e.g., single-room recipe retrieval), full belief graphs (GATA Full) outperform commonsense-augmented agents, suggesting commonsense may not always be necessary",
            "uuids": [
                "e761.1",
                "e777.2"
            ]
        },
        {
            "text": "Manual graph curation is not always perfect and can still miss useful connections, suggesting even expert-curated commonsense has limitations",
            "uuids": [
                "e782.3"
            ]
        },
        {
            "text": "Q*BERT using QA-based extraction (without explicit commonsense KB) can outperform some commonsense-augmented methods, suggesting alternative approaches may be viable",
            "uuids": [
                "e797.0"
            ]
        },
        {
            "text": "COMET-A2C requires more iterations than Q*BERT to converge in some cases, suggesting focused commonsense may be less diverse than QA-based inference",
            "uuids": [
                "e772.2"
            ]
        },
        {
            "text": "Some high-performing agents (e.g., MPRC-DQN with 64% winning percentage) use object-centric retrieval without external commonsense KBs",
            "uuids": [
                "e871.2"
            ]
        }
    ],
    "special_cases": [
        "In tasks where commonsense knowledge is not relevant (e.g., purely logical puzzles, mathematical reasoning), commonsense integration may provide no benefit or even harm performance by adding noise.",
        "When the environment's dynamics violate commonsense expectations (e.g., fantasy worlds with magic, games with non-physical rules), commonsense knowledge may be misleading and should be downweighted or ignored.",
        "For tasks with very small state spaces or short horizons, the computational overhead of commonsense retrieval and encoding may outweigh benefits.",
        "In fully observable environments where the agent has complete state information, commonsense knowledge provides less benefit than in partially observable settings.",
        "When high-quality demonstrations or expert trajectories are available, imitation learning may be more effective than commonsense-augmented RL.",
        "For tasks requiring social reasoning or theory of mind, abstract commonsense (ConceptNet) may be more valuable than grounded visual commonsense (Visual Genome).",
        "In domains where the agent can learn sufficient world knowledge through exploration (e.g., simple gridworlds), external commonsense may be redundant.",
        "When commonsense knowledge is outdated or domain-mismatched, it can hurt performance more than help."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Murugesan et al. (2021) Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines [TWC benchmark and initial commonsense integration approaches, but doesn't emphasize difference representations]",
            "Xu et al. (2022) DiffG-RL: Leveraging Difference between State and Common Sense [Introduces difference graph approach, which is a key component of this theory]",
            "Speer et al. (2017) ConceptNet 5.5: An Open Multilingual Graph of General Knowledge [ConceptNet knowledge base, foundational resource]",
            "Krishna et al. (2017) Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations [Visual Genome, alternative grounded commonsense source]",
            "Bosselut et al. (2019) COMET: Commonsense Transformers for Automatic Knowledge Graph Construction [COMET for generating commonsense inferences]",
            "Ammanabrolu & Riedl (2019) Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning [Early work on KG-based RL, but uses OpenIE rather than external commonsense]",
            "Adhikari et al. (2020) Learning Dynamic Knowledge Graphs to Generalize on Text-Based Games [GATA, which uses belief graphs but not external commonsense in the same way]"
        ]
    },
    "reflected_from_theory_index": 4,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Instruction Template Dominance Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1921</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1921</p>
                <p><strong>Name:</strong> Instruction Template Dominance Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the format in which a problem is presented to an instruction-tuned LLM exerts a dominant influence on the model's performance, often overriding the underlying task semantics. The model's output is shaped more by the syntactic and structural cues of the instruction template than by the content of the problem itself, due to the model's reliance on learned instruction-response mappings from its fine-tuning data.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Template-Driven Response Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; input_prompt &#8594; matches_instruction_template &#8594; template_X<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_instruction_tuned_on &#8594; template_X</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_output &#8594; is_determined_by &#8594; template_X<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_output &#8594; is_less_sensitive_to &#8594; problem_semantics</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Instruction-tuned LLMs show higher accuracy when prompts match their fine-tuning templates, even when task content is held constant. </li>
    <li>Performance drops when the same task is presented in a novel or mismatched template, regardless of semantic similarity. </li>
    <li>Empirical studies (e.g., prompt injection, adversarial rephrasing) show that LLMs can be misled or confused by template changes, even when the underlying task is unchanged. </li>
    <li>Instruction tuning datasets (e.g., FLAN, Super-NaturalInstructions) show that models learn strong associations between template structure and expected output format. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to prompt sensitivity and instruction tuning, the explicit dominance of template over semantics is a novel, stronger claim.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that prompt format affects LLM performance, and that instruction tuning improves generalization to similar instructions.</p>            <p><strong>What is Novel:</strong> This law asserts that the template can dominate over task semantics, making the model's output more a function of template than of underlying problem.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [shows instruction tuning improves generalization, but does not claim template dominance]</li>
    <li>Zhou et al. (2023) LLM Prompt Engineering: A Survey [reviews prompt format effects, but not dominance]</li>
    <li>Longpre et al. (2023) FLAN Collection: Designing Data and Methods for Effective Instruction Tuning [shows template variety impacts performance]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [shows prompt format sensitivity]</li>
</ul>
            <h3>Statement 1: Instruction Template Generalization Boundary Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; instruction_template &#8594; is_novel &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_instruction_tuned &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; decreases &#8594; relative_to_familiar_templates</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show LLMs perform worse on tasks presented in unfamiliar templates, even if the underlying task is unchanged. </li>
    <li>Cross-task generalization studies (e.g., Super-NaturalInstructions) show performance drops at the boundary of template familiarity. </li>
    <li>Prompt rephrasing and adversarial prompt studies demonstrate that even minor template changes can reduce accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This is a more formal and explicit boundary than prior work, which focused on general prompt sensitivity.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs are sensitive to prompt phrasing and format.</p>            <p><strong>What is Novel:</strong> This law formalizes a boundary: performance drops specifically at the boundary of template familiarity, not just with minor rewordings.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [shows prompt format matters, but not explicit boundary]</li>
    <li>Mishra et al. (2022) Cross-Task Generalization via Natural Language Crowdsourcing Instructions [studies generalization, but not template boundaries]</li>
    <li>Wang et al. (2022) Super-NaturalInstructions: Generalization via Instruction Tuning [shows template boundaries]</li>
    <li>Webson & Pavlick (2022) Do Prompt Formats Matter? Measuring Robustness to Variations in Prompting [shows performance drops with prompt format changes]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a model is instruction-tuned on a specific template, its performance will be highest when the test prompt matches that template, even if the task is trivial.</li>
                <li>If two semantically identical tasks are presented in different templates, the model will perform better on the template it was tuned on.</li>
                <li>If a model is presented with a template it has never seen, its performance will drop relative to familiar templates.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is exposed to a hybrid template (combining elements of two known templates), its performance may be unpredictable, potentially showing interference or confusion.</li>
                <li>If a model is instruction-tuned on a large, diverse set of templates, there may be a threshold beyond which template dominance diminishes and semantic understanding increases.</li>
                <li>If a model is trained with meta-learning to recognize and adapt to new templates, template dominance may be reduced or eliminated.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a model performs equally well on all templates, regardless of tuning, this would contradict the theory.</li>
                <li>If a model's performance is determined solely by task semantics and not by template, this would refute template dominance.</li>
                <li>If models generalize perfectly to unseen templates after limited instruction tuning, the theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where models generalize well to unseen templates after extensive multi-template instruction tuning. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes and extends prior work, making a stronger, more formal claim about template dominance.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [instruction tuning and prompt format effects]</li>
    <li>Zhou et al. (2023) LLM Prompt Engineering: A Survey [prompt format effects]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations [prompt format and generalization]</li>
    <li>Wang et al. (2022) Super-NaturalInstructions [template diversity and generalization]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Instruction Template Dominance Theory",
    "theory_description": "This theory posits that the format in which a problem is presented to an instruction-tuned LLM exerts a dominant influence on the model's performance, often overriding the underlying task semantics. The model's output is shaped more by the syntactic and structural cues of the instruction template than by the content of the problem itself, due to the model's reliance on learned instruction-response mappings from its fine-tuning data.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Template-Driven Response Law",
                "if": [
                    {
                        "subject": "input_prompt",
                        "relation": "matches_instruction_template",
                        "object": "template_X"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_instruction_tuned_on",
                        "object": "template_X"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_output",
                        "relation": "is_determined_by",
                        "object": "template_X"
                    },
                    {
                        "subject": "LLM_output",
                        "relation": "is_less_sensitive_to",
                        "object": "problem_semantics"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Instruction-tuned LLMs show higher accuracy when prompts match their fine-tuning templates, even when task content is held constant.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops when the same task is presented in a novel or mismatched template, regardless of semantic similarity.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies (e.g., prompt injection, adversarial rephrasing) show that LLMs can be misled or confused by template changes, even when the underlying task is unchanged.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction tuning datasets (e.g., FLAN, Super-NaturalInstructions) show that models learn strong associations between template structure and expected output format.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that prompt format affects LLM performance, and that instruction tuning improves generalization to similar instructions.",
                    "what_is_novel": "This law asserts that the template can dominate over task semantics, making the model's output more a function of template than of underlying problem.",
                    "classification_explanation": "While related to prompt sensitivity and instruction tuning, the explicit dominance of template over semantics is a novel, stronger claim.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [shows instruction tuning improves generalization, but does not claim template dominance]",
                        "Zhou et al. (2023) LLM Prompt Engineering: A Survey [reviews prompt format effects, but not dominance]",
                        "Longpre et al. (2023) FLAN Collection: Designing Data and Methods for Effective Instruction Tuning [shows template variety impacts performance]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [shows prompt format sensitivity]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Instruction Template Generalization Boundary Law",
                "if": [
                    {
                        "subject": "instruction_template",
                        "relation": "is_novel",
                        "object": "True"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_instruction_tuned",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "decreases",
                        "object": "relative_to_familiar_templates"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show LLMs perform worse on tasks presented in unfamiliar templates, even if the underlying task is unchanged.",
                        "uuids": []
                    },
                    {
                        "text": "Cross-task generalization studies (e.g., Super-NaturalInstructions) show performance drops at the boundary of template familiarity.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt rephrasing and adversarial prompt studies demonstrate that even minor template changes can reduce accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs are sensitive to prompt phrasing and format.",
                    "what_is_novel": "This law formalizes a boundary: performance drops specifically at the boundary of template familiarity, not just with minor rewordings.",
                    "classification_explanation": "This is a more formal and explicit boundary than prior work, which focused on general prompt sensitivity.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [shows prompt format matters, but not explicit boundary]",
                        "Mishra et al. (2022) Cross-Task Generalization via Natural Language Crowdsourcing Instructions [studies generalization, but not template boundaries]",
                        "Wang et al. (2022) Super-NaturalInstructions: Generalization via Instruction Tuning [shows template boundaries]",
                        "Webson & Pavlick (2022) Do Prompt Formats Matter? Measuring Robustness to Variations in Prompting [shows performance drops with prompt format changes]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a model is instruction-tuned on a specific template, its performance will be highest when the test prompt matches that template, even if the task is trivial.",
        "If two semantically identical tasks are presented in different templates, the model will perform better on the template it was tuned on.",
        "If a model is presented with a template it has never seen, its performance will drop relative to familiar templates."
    ],
    "new_predictions_unknown": [
        "If a model is exposed to a hybrid template (combining elements of two known templates), its performance may be unpredictable, potentially showing interference or confusion.",
        "If a model is instruction-tuned on a large, diverse set of templates, there may be a threshold beyond which template dominance diminishes and semantic understanding increases.",
        "If a model is trained with meta-learning to recognize and adapt to new templates, template dominance may be reduced or eliminated."
    ],
    "negative_experiments": [
        "If a model performs equally well on all templates, regardless of tuning, this would contradict the theory.",
        "If a model's performance is determined solely by task semantics and not by template, this would refute template dominance.",
        "If models generalize perfectly to unseen templates after limited instruction tuning, the theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where models generalize well to unseen templates after extensive multi-template instruction tuning.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some recent LLMs (e.g., GPT-4) show strong robustness to template variation, suggesting diminishing dominance in larger models.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very large models with extensive instruction tuning may exhibit reduced template dominance.",
        "Tasks with extremely strong semantic cues (e.g., math) may override template effects.",
        "Meta-learning or explicit template adaptation may reduce or eliminate template dominance."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt format sensitivity and instruction tuning are well-studied.",
        "what_is_novel": "The explicit claim that template can dominate over semantics, and the formalization of a generalization boundary.",
        "classification_explanation": "This theory synthesizes and extends prior work, making a stronger, more formal claim about template dominance.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [instruction tuning and prompt format effects]",
            "Zhou et al. (2023) LLM Prompt Engineering: A Survey [prompt format effects]",
            "Min et al. (2022) Rethinking the Role of Demonstrations [prompt format and generalization]",
            "Wang et al. (2022) Super-NaturalInstructions [template diversity and generalization]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Grounding-by-Proxy Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-63</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-63</p>
                <p><strong>Name:</strong> Grounding-by-Proxy Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models encode and utilize spatial, procedural, and object-relational knowledge for embodied planning tasks without direct sensory input, based on the following results.</p>
                <p><strong>Description:</strong> Language models without direct sensory input achieve embodied reasoning through 'grounding-by-proxy' - leveraging intermediate representations that bridge the gap between abstract linguistic knowledge and concrete physical states. These proxies include: (1) Textual scene descriptions derived from perception systems, (2) Symbolic object lists and attributes, (3) Coordinate systems and numeric spatial parameters, (4) Affordance signals from learned value functions, (5) Simulation outputs, (6) Multimodal embeddings from vision-language models, (7) Generated code/programs as executable representations, (8) Natural language action sequences, (9) Memory modules tracking state, and (10) Text-form visualizations (e.g., ASCII grids). The effectiveness of grounding-by-proxy depends on: (a) the fidelity and completeness of proxy representations, (b) structural alignment between proxy format and model capabilities (e.g., qualitative relations vs raw coordinates), (c) the model's ability to map between linguistic and proxy representations, and (d) mechanisms for iterative refinement through proxy feedback. Models can reason about embodied tasks by operating on these proxies rather than raw sensory data, with performance bounded by both proxy generation quality and proxy utilization quality. Critically, different proxy types enable different reasoning capabilities: symbolic proxies enable compositional reasoning, numeric proxies enable precise computation, affordance proxies enable feasibility assessment, and code proxies enable executable procedural knowledge.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models achieve embodied reasoning without direct sensory input by operating on intermediate proxy representations that bridge linguistic and physical domains</li>
                <li>Effective proxies must be: (1) structured in formats the LM can process, (2) faithful to physical reality, (3) aligned with the LM's internal representations, and (4) sufficiently complete for the task</li>
                <li>Proxy quality has two components: generation quality (fidelity to physical state) and utilization quality (model's ability to reason over the proxy), both of which bound overall performance</li>
                <li>Different proxy types enable different reasoning capabilities: symbolic proxies enable compositional reasoning, numeric proxies enable precise spatial computation, affordance proxies enable feasibility assessment, code proxies enable executable procedural knowledge, and memory proxies enable temporal reasoning</li>
                <li>Models can learn to map between proxy representations and natural language through fine-tuning or prompting, but this mapping quality varies with proxy structure and model capacity</li>
                <li>Multi-proxy systems that combine complementary representations (e.g., semantic + geometric + affordance) outperform single-proxy systems by providing multiple views of the same physical state</li>
                <li>The grounding-by-proxy approach trades direct sensory grounding for modularity and interpretability, but inherits limitations from proxy generation systems</li>
                <li>Proxy structure must match model capabilities: qualitative/relational proxies (left/right, on/in) are more effective for LLM reasoning than raw numeric coordinates</li>
                <li>Iterative refinement through proxy feedback (verification, error detection, replanning) can overcome limitations of single-pass proxy-based reasoning</li>
                <li>Code generation as a proxy mechanism enables LLMs to express complex procedural and spatial knowledge in executable form, delegating computation to external systems</li>
                <li>Text-form spatial visualizations (ASCII grids) emerge as effective proxies when models are exposed to such representations during pretraining</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Systems that provide LLMs with textual object lists (rather than raw images) enable successful planning, as demonstrated by TaPA and context-aware SayCan <a href="../results/extraction-result-350.html#e350.0" class="evidence-link">[e350.0]</a> <a href="../results/extraction-result-394.html#e394.1" class="evidence-link">[e394.1]</a> <a href="../results/extraction-result-395.html#e395.7" class="evidence-link">[e395.7]</a> </li>
    <li>Explicit coordinate representations (bounding boxes, 3D positions, spatial predicates) allow text-only models to perform spatial reasoning when provided as structured input <a href="../results/extraction-result-342.html#e342.0" class="evidence-link">[e342.0]</a> <a href="../results/extraction-result-520.html#e520.3" class="evidence-link">[e520.3]</a> <a href="../results/extraction-result-385.html#e385.1" class="evidence-link">[e385.1]</a> <a href="../results/extraction-result-357.html#e357.3" class="evidence-link">[e357.3]</a> <a href="../results/extraction-result-518.html#e518.2" class="evidence-link">[e518.2]</a> </li>
    <li>Affordance value functions serve as grounding proxies by converting visual states into language-conditioned feasibility scores that LLMs can use for planning <a href="../results/extraction-result-549.html#e549.1" class="evidence-link">[e549.1]</a> <a href="../results/extraction-result-351.html#e351.0" class="evidence-link">[e351.0]</a> </li>
    <li>Simulation outputs (MuJoCo trajectories converted to text) dramatically improve LLM physical reasoning compared to relying on implicit knowledge alone <a href="../results/extraction-result-545.html#e545.0" class="evidence-link">[e545.0]</a> <a href="../results/extraction-result-545.html#e545.1" class="evidence-link">[e545.1]</a> <a href="../results/extraction-result-545.html#e545.2" class="evidence-link">[e545.2]</a> </li>
    <li>Multimodal embeddings (CLIP, vision-language models) enable grounding by mapping between visual and linguistic spaces, allowing language models to reason about visual content through aligned representations <a href="../results/extraction-result-376.html#e376.3" class="evidence-link">[e376.3]</a> <a href="../results/extraction-result-527.html#e527.0" class="evidence-link">[e527.0]</a> <a href="../results/extraction-result-547.html#e547.0" class="evidence-link">[e547.0]</a> <a href="../results/extraction-result-524.html#e524.0" class="evidence-link">[e524.0]</a> <a href="../results/extraction-result-512.html#e512.2" class="evidence-link">[e512.2]</a> </li>
    <li>Quality of proxy representations directly determines task success: systems with ground-truth perception substantially outperform those with noisy proxies <a href="../results/extraction-result-392.html#e392.3" class="evidence-link">[e392.3]</a> <a href="../results/extraction-result-354.html#e354.4" class="evidence-link">[e354.4]</a> <a href="../results/extraction-result-365.html#e365.3" class="evidence-link">[e365.3]</a> <a href="../results/extraction-result-530.html#e530.3" class="evidence-link">[e530.3]</a> <a href="../results/extraction-result-357.html#e357.5" class="evidence-link">[e357.5]</a> </li>
    <li>Models can operate on multiple proxy types simultaneously and integrate them: combining object lists with affordances, or coordinates with semantic labels <a href="../results/extraction-result-351.html#e351.0" class="evidence-link">[e351.0]</a> <a href="../results/extraction-result-394.html#e394.1" class="evidence-link">[e394.1]</a> <a href="../results/extraction-result-519.html#e519.2" class="evidence-link">[e519.2]</a> <a href="../results/extraction-result-340.html#e340.1" class="evidence-link">[e340.1]</a> <a href="../results/extraction-result-365.html#e365.0" class="evidence-link">[e365.0]</a> </li>
    <li>Proxy representations must be structured and aligned with model capabilities: qualitative spatial relations (left/right) work better than raw coordinates for LLM reasoning <a href="../results/extraction-result-385.html#e385.6" class="evidence-link">[e385.6]</a> <a href="../results/extraction-result-344.html#e344.0" class="evidence-link">[e344.0]</a> <a href="../results/extraction-result-518.html#e518.2" class="evidence-link">[e518.2]</a> <a href="../results/extraction-result-385.html#e385.1" class="evidence-link">[e385.1]</a> </li>
    <li>Code generation serves as an executable proxy mechanism, allowing LLMs to express procedural and spatial knowledge as programs that can be executed by external systems <a href="../results/extraction-result-367.html#e367.0" class="evidence-link">[e367.0]</a> <a href="../results/extraction-result-367.html#e367.1" class="evidence-link">[e367.1]</a> <a href="../results/extraction-result-367.html#e367.4" class="evidence-link">[e367.4]</a> <a href="../results/extraction-result-392.html#e392.0" class="evidence-link">[e392.0]</a> <a href="../results/extraction-result-392.html#e392.3" class="evidence-link">[e392.3]</a> <a href="../results/extraction-result-532.html#e532.1" class="evidence-link">[e532.1]</a> </li>
    <li>Natural language action sequences serve as procedural proxies, with explicit skill libraries reducing hallucination and improving executability <a href="../results/extraction-result-395.html#e395.0" class="evidence-link">[e395.0]</a> <a href="../results/extraction-result-395.html#e395.1" class="evidence-link">[e395.1]</a> <a href="../results/extraction-result-351.html#e351.0" class="evidence-link">[e351.0]</a> <a href="../results/extraction-result-365.html#e365.0" class="evidence-link">[e365.0]</a> </li>
    <li>Memory modules serve as state-tracking proxies, enabling models to maintain context across multi-step tasks without direct sensory access <a href="../results/extraction-result-395.html#e395.2" class="evidence-link">[e395.2]</a> <a href="../results/extraction-result-365.html#e365.3" class="evidence-link">[e365.3]</a> <a href="../results/extraction-result-352.html#e352.0" class="evidence-link">[e352.0]</a> <a href="../results/extraction-result-368.html#e368.4" class="evidence-link">[e368.4]</a> </li>
    <li>Text-form visualizations (ASCII grids, emoji maps) serve as spatial proxies that LLMs can generate and manipulate to track spatial state <a href="../results/extraction-result-358.html#e358.0" class="evidence-link">[e358.0]</a> <a href="../results/extraction-result-358.html#e358.2" class="evidence-link">[e358.2]</a> </li>
    <li>Iterative refinement through proxy feedback loops (verification, error detection, replanning) improves proxy-based system performance <a href="../results/extraction-result-354.html#e354.2" class="evidence-link">[e354.2]</a> <a href="../results/extraction-result-354.html#e354.3" class="evidence-link">[e354.3]</a> <a href="../results/extraction-result-352.html#e352.0" class="evidence-link">[e352.0]</a> <a href="../results/extraction-result-368.html#e368.4" class="evidence-link">[e368.4]</a> </li>
    <li>Flattened tabular representations of environments enable better grounding than raw coordinate lists, showing structure matters <a href="../results/extraction-result-342.html#e342.0" class="evidence-link">[e342.0]</a> <a href="../results/extraction-result-342.html#e342.2" class="evidence-link">[e342.2]</a> </li>
    <li>Sentence embeddings can serve as proxies for selecting geometric priors or matching instructions to skills <a href="../results/extraction-result-548.html#e548.2" class="evidence-link">[e548.2]</a> <a href="../results/extraction-result-351.html#e351.3" class="evidence-link">[e351.3]</a> </li>
    <li>Symbolic ASP representations serve as explicit logical proxies that enable multi-hop reasoning when paired with LLM semantic parsing <a href="../results/extraction-result-518.html#e518.0" class="evidence-link">[e518.0]</a> <a href="../results/extraction-result-518.html#e518.2" class="evidence-link">[e518.2]</a> <a href="../results/extraction-result-535.html#e535.2" class="evidence-link">[e535.2]</a> <a href="../results/extraction-result-520.html#e520.3" class="evidence-link">[e520.3]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Providing LLMs with multiple complementary proxy representations (e.g., object lists + coordinates + affordances + code generation capability) will improve performance more than providing any single proxy type alone</li>
                <li>Training LLMs to generate their own proxy representations from partial information (e.g., generating ASCII spatial maps from object lists) will improve their ability to reason about underspecified embodied scenarios</li>
                <li>Proxy representations that match the structure of natural language (qualitative relations, hierarchical descriptions) will be more effective than those requiring numeric computation for most LLM architectures</li>
                <li>Fine-tuning LLMs on proxy-to-action mappings will improve their ability to utilize novel proxy types at test time</li>
                <li>Systems that combine code generation proxies with affordance proxies will outperform those using either alone, as code enables precise specification while affordances enable feasibility checking</li>
                <li>Iterative proxy refinement (generate proxy, verify, regenerate) will consistently outperform single-pass proxy generation across diverse embodied tasks</li>
                <li>LLMs trained to generate text-form spatial visualizations will show improved performance on navigation and spatial reasoning tasks compared to those without such training</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a 'universal proxy' representation that works optimally across all embodied domains, or whether domain-specific proxies are fundamentally necessary</li>
                <li>Whether LLMs can learn to actively query for specific proxy information they need (active perception through proxy requests) rather than passively receiving fixed proxies</li>
                <li>Whether proxy-based grounding can ever achieve the same performance ceiling as direct sensory grounding, or whether fundamental limitations exist due to information loss in proxy generation</li>
                <li>Whether the optimal proxy representation changes with model scale, or whether certain proxy types remain superior regardless of model capacity</li>
                <li>Whether learned proxy generation (e.g., neural networks generating object lists) can match or exceed hand-crafted proxy generation rules</li>
                <li>Whether multi-modal models that process both raw sensory input and proxy representations will converge to primarily using one or the other, or maintain complementary usage</li>
                <li>Whether proxy-based systems can develop genuine spatial understanding or are fundamentally limited to pattern matching over proxy structures</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Demonstrating that LLMs perform equally well with random/corrupted proxies as with faithful proxies would challenge the grounding assumption</li>
                <li>Finding that direct sensory input to LLMs (without proxy conversion) consistently outperforms proxy-based approaches across diverse tasks would question the necessity of the proxy layer</li>
                <li>Showing that LLMs cannot learn to utilize new proxy types even with extensive training would challenge the flexibility assumption</li>
                <li>Demonstrating that proxy-based systems cannot generalize to novel object categories or spatial configurations would reveal fundamental limitations</li>
                <li>Finding that increasing proxy fidelity beyond a certain threshold does not improve performance would suggest proxy utilization, not generation, is the bottleneck</li>
                <li>Showing that models trained on one proxy type cannot transfer to other proxy types would challenge the assumption of shared underlying representations</li>
                <li>Demonstrating that iterative proxy refinement does not improve performance would question the value of feedback loops in proxy-based systems</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to optimally design proxy representations for novel domains without extensive empirical testing </li>
    <li>Interactions between different proxy types (e.g., when symbolic and numeric proxies conflict) are not fully characterized </li>
    <li>The computational and cognitive costs of maintaining and reasoning over multiple proxy representations are not addressed </li>
    <li>How models handle missing or incomplete proxy information is not fully explained, though some systems show robustness <a href="../results/extraction-result-352.html#e352.0" class="evidence-link">[e352.0]</a> <a href="../results/extraction-result-365.html#e365.0" class="evidence-link">[e365.0]</a> </li>
    <li>The theory does not explain why certain proxy structures (qualitative vs numeric) work better for LLMs - is this due to pretraining data distribution or fundamental architectural constraints? <a href="../results/extraction-result-385.html#e385.6" class="evidence-link">[e385.6]</a> <a href="../results/extraction-result-344.html#e344.0" class="evidence-link">[e344.0]</a> </li>
    <li>The relationship between proxy complexity and model scale is not characterized - do larger models benefit more from complex proxies? </li>
    <li>How proxy-based systems handle dynamic environments where proxies become stale is not fully addressed </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Harnad (1990) The symbol grounding problem [Classic formulation of grounding challenge, but doesn't propose proxy-based solution]</li>
    <li>Barsalou (1999) Perceptual symbol systems [Proposes grounding through perceptual simulation, related but different mechanism - focuses on internal simulation rather than external proxies]</li>
    <li>Tellex et al. (2011) Understanding Natural Language Commands for Robotic Navigation and Manipulation [Early work on grounding language in robotics, uses some proxy concepts but not systematically theorized]</li>
    <li>Roy (2005) Semiotic schemas: A framework for grounding language in action and perception [Discusses grounding through intermediate representations but focuses on learned schemas rather than explicit proxies]</li>
    <li>Winograd (1972) Understanding natural language [SHRDLU system used symbolic proxies but in a limited blocks world, not theorized as general principle]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Grounding-by-Proxy Theory",
    "theory_description": "Language models without direct sensory input achieve embodied reasoning through 'grounding-by-proxy' - leveraging intermediate representations that bridge the gap between abstract linguistic knowledge and concrete physical states. These proxies include: (1) Textual scene descriptions derived from perception systems, (2) Symbolic object lists and attributes, (3) Coordinate systems and numeric spatial parameters, (4) Affordance signals from learned value functions, (5) Simulation outputs, (6) Multimodal embeddings from vision-language models, (7) Generated code/programs as executable representations, (8) Natural language action sequences, (9) Memory modules tracking state, and (10) Text-form visualizations (e.g., ASCII grids). The effectiveness of grounding-by-proxy depends on: (a) the fidelity and completeness of proxy representations, (b) structural alignment between proxy format and model capabilities (e.g., qualitative relations vs raw coordinates), (c) the model's ability to map between linguistic and proxy representations, and (d) mechanisms for iterative refinement through proxy feedback. Models can reason about embodied tasks by operating on these proxies rather than raw sensory data, with performance bounded by both proxy generation quality and proxy utilization quality. Critically, different proxy types enable different reasoning capabilities: symbolic proxies enable compositional reasoning, numeric proxies enable precise computation, affordance proxies enable feasibility assessment, and code proxies enable executable procedural knowledge.",
    "supporting_evidence": [
        {
            "text": "Systems that provide LLMs with textual object lists (rather than raw images) enable successful planning, as demonstrated by TaPA and context-aware SayCan",
            "uuids": [
                "e350.0",
                "e394.1",
                "e395.7"
            ]
        },
        {
            "text": "Explicit coordinate representations (bounding boxes, 3D positions, spatial predicates) allow text-only models to perform spatial reasoning when provided as structured input",
            "uuids": [
                "e342.0",
                "e520.3",
                "e385.1",
                "e357.3",
                "e518.2"
            ]
        },
        {
            "text": "Affordance value functions serve as grounding proxies by converting visual states into language-conditioned feasibility scores that LLMs can use for planning",
            "uuids": [
                "e549.1",
                "e351.0"
            ]
        },
        {
            "text": "Simulation outputs (MuJoCo trajectories converted to text) dramatically improve LLM physical reasoning compared to relying on implicit knowledge alone",
            "uuids": [
                "e545.0",
                "e545.1",
                "e545.2"
            ]
        },
        {
            "text": "Multimodal embeddings (CLIP, vision-language models) enable grounding by mapping between visual and linguistic spaces, allowing language models to reason about visual content through aligned representations",
            "uuids": [
                "e376.3",
                "e527.0",
                "e547.0",
                "e524.0",
                "e512.2"
            ]
        },
        {
            "text": "Quality of proxy representations directly determines task success: systems with ground-truth perception substantially outperform those with noisy proxies",
            "uuids": [
                "e392.3",
                "e354.4",
                "e365.3",
                "e530.3",
                "e357.5"
            ]
        },
        {
            "text": "Models can operate on multiple proxy types simultaneously and integrate them: combining object lists with affordances, or coordinates with semantic labels",
            "uuids": [
                "e351.0",
                "e394.1",
                "e519.2",
                "e340.1",
                "e365.0"
            ]
        },
        {
            "text": "Proxy representations must be structured and aligned with model capabilities: qualitative spatial relations (left/right) work better than raw coordinates for LLM reasoning",
            "uuids": [
                "e385.6",
                "e344.0",
                "e518.2",
                "e385.1"
            ]
        },
        {
            "text": "Code generation serves as an executable proxy mechanism, allowing LLMs to express procedural and spatial knowledge as programs that can be executed by external systems",
            "uuids": [
                "e367.0",
                "e367.1",
                "e367.4",
                "e392.0",
                "e392.3",
                "e532.1"
            ]
        },
        {
            "text": "Natural language action sequences serve as procedural proxies, with explicit skill libraries reducing hallucination and improving executability",
            "uuids": [
                "e395.0",
                "e395.1",
                "e351.0",
                "e365.0"
            ]
        },
        {
            "text": "Memory modules serve as state-tracking proxies, enabling models to maintain context across multi-step tasks without direct sensory access",
            "uuids": [
                "e395.2",
                "e365.3",
                "e352.0",
                "e368.4"
            ]
        },
        {
            "text": "Text-form visualizations (ASCII grids, emoji maps) serve as spatial proxies that LLMs can generate and manipulate to track spatial state",
            "uuids": [
                "e358.0",
                "e358.2"
            ]
        },
        {
            "text": "Iterative refinement through proxy feedback loops (verification, error detection, replanning) improves proxy-based system performance",
            "uuids": [
                "e354.2",
                "e354.3",
                "e352.0",
                "e368.4"
            ]
        },
        {
            "text": "Flattened tabular representations of environments enable better grounding than raw coordinate lists, showing structure matters",
            "uuids": [
                "e342.0",
                "e342.2"
            ]
        },
        {
            "text": "Sentence embeddings can serve as proxies for selecting geometric priors or matching instructions to skills",
            "uuids": [
                "e548.2",
                "e351.3"
            ]
        },
        {
            "text": "Symbolic ASP representations serve as explicit logical proxies that enable multi-hop reasoning when paired with LLM semantic parsing",
            "uuids": [
                "e518.0",
                "e518.2",
                "e535.2",
                "e520.3"
            ]
        }
    ],
    "theory_statements": [
        "Language models achieve embodied reasoning without direct sensory input by operating on intermediate proxy representations that bridge linguistic and physical domains",
        "Effective proxies must be: (1) structured in formats the LM can process, (2) faithful to physical reality, (3) aligned with the LM's internal representations, and (4) sufficiently complete for the task",
        "Proxy quality has two components: generation quality (fidelity to physical state) and utilization quality (model's ability to reason over the proxy), both of which bound overall performance",
        "Different proxy types enable different reasoning capabilities: symbolic proxies enable compositional reasoning, numeric proxies enable precise spatial computation, affordance proxies enable feasibility assessment, code proxies enable executable procedural knowledge, and memory proxies enable temporal reasoning",
        "Models can learn to map between proxy representations and natural language through fine-tuning or prompting, but this mapping quality varies with proxy structure and model capacity",
        "Multi-proxy systems that combine complementary representations (e.g., semantic + geometric + affordance) outperform single-proxy systems by providing multiple views of the same physical state",
        "The grounding-by-proxy approach trades direct sensory grounding for modularity and interpretability, but inherits limitations from proxy generation systems",
        "Proxy structure must match model capabilities: qualitative/relational proxies (left/right, on/in) are more effective for LLM reasoning than raw numeric coordinates",
        "Iterative refinement through proxy feedback (verification, error detection, replanning) can overcome limitations of single-pass proxy-based reasoning",
        "Code generation as a proxy mechanism enables LLMs to express complex procedural and spatial knowledge in executable form, delegating computation to external systems",
        "Text-form spatial visualizations (ASCII grids) emerge as effective proxies when models are exposed to such representations during pretraining"
    ],
    "new_predictions_likely": [
        "Providing LLMs with multiple complementary proxy representations (e.g., object lists + coordinates + affordances + code generation capability) will improve performance more than providing any single proxy type alone",
        "Training LLMs to generate their own proxy representations from partial information (e.g., generating ASCII spatial maps from object lists) will improve their ability to reason about underspecified embodied scenarios",
        "Proxy representations that match the structure of natural language (qualitative relations, hierarchical descriptions) will be more effective than those requiring numeric computation for most LLM architectures",
        "Fine-tuning LLMs on proxy-to-action mappings will improve their ability to utilize novel proxy types at test time",
        "Systems that combine code generation proxies with affordance proxies will outperform those using either alone, as code enables precise specification while affordances enable feasibility checking",
        "Iterative proxy refinement (generate proxy, verify, regenerate) will consistently outperform single-pass proxy generation across diverse embodied tasks",
        "LLMs trained to generate text-form spatial visualizations will show improved performance on navigation and spatial reasoning tasks compared to those without such training"
    ],
    "new_predictions_unknown": [
        "Whether there exists a 'universal proxy' representation that works optimally across all embodied domains, or whether domain-specific proxies are fundamentally necessary",
        "Whether LLMs can learn to actively query for specific proxy information they need (active perception through proxy requests) rather than passively receiving fixed proxies",
        "Whether proxy-based grounding can ever achieve the same performance ceiling as direct sensory grounding, or whether fundamental limitations exist due to information loss in proxy generation",
        "Whether the optimal proxy representation changes with model scale, or whether certain proxy types remain superior regardless of model capacity",
        "Whether learned proxy generation (e.g., neural networks generating object lists) can match or exceed hand-crafted proxy generation rules",
        "Whether multi-modal models that process both raw sensory input and proxy representations will converge to primarily using one or the other, or maintain complementary usage",
        "Whether proxy-based systems can develop genuine spatial understanding or are fundamentally limited to pattern matching over proxy structures"
    ],
    "negative_experiments": [
        "Demonstrating that LLMs perform equally well with random/corrupted proxies as with faithful proxies would challenge the grounding assumption",
        "Finding that direct sensory input to LLMs (without proxy conversion) consistently outperforms proxy-based approaches across diverse tasks would question the necessity of the proxy layer",
        "Showing that LLMs cannot learn to utilize new proxy types even with extensive training would challenge the flexibility assumption",
        "Demonstrating that proxy-based systems cannot generalize to novel object categories or spatial configurations would reveal fundamental limitations",
        "Finding that increasing proxy fidelity beyond a certain threshold does not improve performance would suggest proxy utilization, not generation, is the bottleneck",
        "Showing that models trained on one proxy type cannot transfer to other proxy types would challenge the assumption of shared underlying representations",
        "Demonstrating that iterative proxy refinement does not improve performance would question the value of feedback loops in proxy-based systems"
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to optimally design proxy representations for novel domains without extensive empirical testing",
            "uuids": []
        },
        {
            "text": "Interactions between different proxy types (e.g., when symbolic and numeric proxies conflict) are not fully characterized",
            "uuids": []
        },
        {
            "text": "The computational and cognitive costs of maintaining and reasoning over multiple proxy representations are not addressed",
            "uuids": []
        },
        {
            "text": "How models handle missing or incomplete proxy information is not fully explained, though some systems show robustness",
            "uuids": [
                "e352.0",
                "e365.0"
            ]
        },
        {
            "text": "The theory does not explain why certain proxy structures (qualitative vs numeric) work better for LLMs - is this due to pretraining data distribution or fundamental architectural constraints?",
            "uuids": [
                "e385.6",
                "e344.0"
            ]
        },
        {
            "text": "The relationship between proxy complexity and model scale is not characterized - do larger models benefit more from complex proxies?",
            "uuids": []
        },
        {
            "text": "How proxy-based systems handle dynamic environments where proxies become stale is not fully addressed",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some end-to-end vision-language models that process raw images directly outperform proxy-based systems on certain tasks, suggesting proxies may introduce information loss or that direct sensory processing can be more effective",
            "uuids": [
                "e355.0",
                "e425.0",
                "e395.4"
            ]
        },
        {
            "text": "Text-only models sometimes match multimodal models on certain benchmarks by exploiting dataset biases, questioning whether proxies provide genuine grounding or just statistical shortcuts",
            "uuids": [
                "e525.2",
                "e544.2"
            ]
        },
        {
            "text": "Some systems show that vision-only or action-only baselines can match or exceed language-conditioned systems, suggesting proxies may not always add value",
            "uuids": [
                "e525.2"
            ]
        },
        {
            "text": "Fine-tuned end-to-end models sometimes outperform modular proxy-based systems, suggesting the proxy layer may introduce optimization challenges",
            "uuids": [
                "e533.0",
                "e533.1"
            ]
        }
    ],
    "special_cases": [
        "For tasks where perception is highly reliable (e.g., simulation with perfect state), proxy-based approaches may match or exceed end-to-end learned systems due to interpretability and modularity advantages",
        "In domains with well-defined symbolic structures (e.g., board games, formal planning, logic puzzles), symbolic proxies may be sufficient without numeric or affordance proxies",
        "For real-time applications, the latency of proxy generation may make direct sensory processing more practical despite theoretical advantages of proxies",
        "In safety-critical applications, interpretable proxy representations may be preferred even if they sacrifice some performance",
        "For tasks requiring precise numeric computation (e.g., coordinate-based manipulation), code generation proxies may be superior to natural language proxies",
        "When visual changes are subtle or absent, memory-based proxies become critical for tracking state across time",
        "For long-horizon tasks, iterative proxy refinement becomes necessary as single-pass proxy generation accumulates errors",
        "In low-data regimes, proxy-based systems may outperform end-to-end systems due to better sample efficiency from modular design",
        "For tasks requiring multi-hop reasoning, symbolic proxies (ASP, logic programs) may be necessary to avoid reasoning errors in pure neural approaches"
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Harnad (1990) The symbol grounding problem [Classic formulation of grounding challenge, but doesn't propose proxy-based solution]",
            "Barsalou (1999) Perceptual symbol systems [Proposes grounding through perceptual simulation, related but different mechanism - focuses on internal simulation rather than external proxies]",
            "Tellex et al. (2011) Understanding Natural Language Commands for Robotic Navigation and Manipulation [Early work on grounding language in robotics, uses some proxy concepts but not systematically theorized]",
            "Roy (2005) Semiotic schemas: A framework for grounding language in action and perception [Discusses grounding through intermediate representations but focuses on learned schemas rather than explicit proxies]",
            "Winograd (1972) Understanding natural language [SHRDLU system used symbolic proxies but in a limited blocks world, not theorized as general principle]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
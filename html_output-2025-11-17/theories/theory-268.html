<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Grounding Gap Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-268</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-268</p>
                <p><strong>Name:</strong> Interactive Grounding Gap Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory explaining the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents, and the architectural/training interventions required to close the gap.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents arises from a fundamental mismatch between the static, context-complete nature of QA tasks and the dynamic, context-evolving nature of interactive procedural tasks. LLMs are trained primarily on static text corpora where 'grounding' refers to linguistic co-reference and semantic coherence, not to maintaining correspondence between linguistic representations and evolving environmental states. In interactive settings, agents must continuously update their internal representations to reflect environmental changes, track causal dependencies across action sequences, and maintain bidirectional grounding between language and world state. The gap emerges because: (1) QA tasks provide all necessary context upfront and require single-step reasoning, while interactive tasks require iterative context construction through action-observation loops; (2) Training on static text does not teach models to distinguish between 'knowing about' procedures (declarative knowledge) and 'executing' procedures (procedural knowledge with grounding); (3) The temporal credit assignment problem in multi-step interactions is not addressed by next-token prediction objectives; (4) Interactive tasks require maintaining multiple simultaneous groundings (goal states, current states, action effects, preconditions) that must be dynamically updated, whereas QA requires only understanding the question context; and (5) The gap can be partially mitigated through inference-time interventions (prompting strategies) or training-time interventions (interactive trajectory fine-tuning), but complete closure requires architectural changes that separate world-state tracking from linguistic processing. The theory further distinguishes between four types of grounding deficits: perceptual grounding (mapping observations to state), causal grounding (understanding action effects), temporal grounding (maintaining state across time), and goal grounding (tracking progress toward objectives). The magnitude of the gap varies systematically with task properties: it is smaller in deterministic, fully-observable, short-horizon tasks and in domains with strong pre-training coverage or formal feedback structures (code, mathematics), and larger in stochastic, partially-observable, long-horizon tasks in novel domains with natural language feedback.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The performance gap between QA and interactive tasks increases monotonically with task horizon length, as longer horizons require more sustained grounding maintenance across multiple types (perceptual, causal, temporal, goal).</li>
                <li>LLMs trained solely on static text will exhibit a systematic dissociation between their ability to describe procedures (declarative knowledge) and their ability to execute procedures (procedural knowledge with grounding), with the dissociation magnitude inversely proportional to the formality and determinism of the task domain.</li>
                <li>Interactive grounding requires four distinct capabilities not fully trained by standard language modeling: (1) perceptual grounding - mapping observations to state representations; (2) causal grounding - understanding and predicting action effects; (3) temporal grounding - maintaining state correspondence across time steps; and (4) goal grounding - tracking progress toward objectives and subgoals.</li>
                <li>The grounding gap can be quantified as the ratio of performance degradation per interaction step in procedural tasks versus performance stability in multi-turn QA dialogues with equivalent context lengths, with this ratio being larger for tasks requiring all four grounding types simultaneously.</li>
                <li>Architectural interventions that explicitly separate world-state representations from linguistic representations (e.g., external memory modules, state trackers), and training interventions that include action-observation trajectories with explicit grounding supervision, are necessary to fully close the interactive grounding gap, though partial improvements can be achieved through inference-time prompting strategies or scale alone.</li>
                <li>The gap manifests most severely at decision points where the agent must: (a) choose actions based on partially observable state, (b) integrate feedback from previous actions, (c) recover from errors, or (d) perform temporal credit assignment - all scenarios requiring active maintenance of multiple grounding types.</li>
                <li>LLMs will show asymmetric performance: high accuracy in predicting what action should be taken next when given complete state descriptions (QA-style), but lower accuracy in actually selecting and executing correct actions when they must infer state from interaction history (interactive-style), with the asymmetry decreasing as model scale increases and as prompting sophistication increases.</li>
                <li>The grounding gap is smaller in domains where: (a) the environment provides structured, formal feedback (code interpreters, mathematical verification); (b) the task is deterministic and fully observable; (c) the LLM has extensive pre-training coverage; or (d) the task can be decomposed into independent subtasks, because these properties reduce the complexity of grounding maintenance.</li>
                <li>Training on interactive trajectories creates a qualitatively different internal representation compared to training on static descriptions of the same procedures, with the interactive training producing representations that bind actions to state changes rather than merely encoding action sequences.</li>
                <li>The gap persists even with very large context windows because the issue is not memory capacity but rather the lack of mechanisms for active state tracking, causal modeling, and temporal credit assignment that are orthogonal to context length.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>LLMs demonstrate strong performance on static question-answering benchmarks but struggle with embodied AI tasks requiring sequential decision-making and environmental interaction. </li>
    <li>Agents based on LLMs show degraded performance as task horizons increase and as the number of required interaction steps grows, suggesting issues with temporal reasoning and state tracking. </li>
    <li>LLMs can describe procedures accurately when asked but fail to execute them correctly in interactive settings, indicating a declarative-procedural knowledge gap. </li>
    <li>Fine-tuning on interactive trajectories significantly improves agent performance compared to base LLMs, suggesting that interactive grounding requires specific training signals not present in standard language modeling. </li>
    <li>LLM agents struggle particularly with error recovery and replanning when initial actions fail, indicating difficulty with dynamic context updating and causal reasoning about action effects. </li>
    <li>Carefully engineered prompting strategies like chain-of-thought and ReAct can substantially improve interactive performance without fine-tuning, demonstrating that part of the gap is due to inference-time factors and can be partially bridged through better elicitation of existing capabilities. </li>
    <li>LLMs show relatively smaller performance gaps on interactive tasks in formal domains like code execution and mathematical problem-solving compared to embodied or web navigation tasks, suggesting that structured feedback and deterministic environments reduce grounding requirements. </li>
    <li>Very large language models (100B+ parameters) show improved interactive task performance with minimal fine-tuning, suggesting that scale partially addresses the grounding gap but does not eliminate it entirely. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Augmenting LLM agents with explicit external memory modules that track environmental state separately from the language context (e.g., key-value memory stores, state graphs) will reduce the interactive grounding gap more effectively than simply increasing context window size, with the improvement being proportional to the degree of partial observability in the task.</li>
                <li>Training LLMs with contrastive objectives that explicitly distinguish between valid and invalid state transitions (e.g., contrasting successful vs. failed action sequences) will improve interactive task performance more than equivalent amounts of additional pre-training on static text describing the same domain.</li>
                <li>LLM agents will show better interactive performance on tasks where environmental feedback is provided in structured formats (e.g., JSON state representations, formal verification results) compared to natural language descriptions, even when the information content is identical, with the performance difference being largest for tasks requiring precise state tracking.</li>
                <li>The performance gap will be smaller for tasks that can be decomposed into independent subtasks versus tasks requiring tight coupling between sequential actions, as independent subtasks reduce grounding maintenance requirements, with the effect being measurable as a function of the causal dependency graph density.</li>
                <li>Fine-tuning on synthetic interactive trajectories generated by rule-based agents or simulators will improve real-world interactive performance, demonstrating that the gap is partly due to lack of training signal rather than fundamental model limitations, with transfer effectiveness depending on the fidelity of the simulation.</li>
                <li>Providing LLM agents with explicit state summaries at each step (even if generated by the LLM itself through self-prompting) will improve performance more than providing equivalent amounts of raw observational history, demonstrating the importance of explicit state representation.</li>
                <li>LLM agents trained with curriculum learning that gradually increases task horizon length will show better long-horizon performance than agents trained on mixed-length tasks, as this allows progressive development of grounding maintenance capabilities.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Training LLMs with a dual objective that simultaneously optimizes for next-token prediction and next-state prediction might completely eliminate the grounding gap by forcing the model to maintain explicit state representations, or might create interference between objectives that worsens both capabilities due to competing optimization pressures.</li>
                <li>Scaling model size alone (without architectural or training changes) may eventually close the grounding gap through emergent abilities at sufficient scale (e.g., 1T+ parameters), or the gap may persist regardless of scale due to fundamental training objective misalignment that cannot be overcome by capacity alone.</li>
                <li>Incorporating explicit causal reasoning modules (e.g., learned causal graphs, counterfactual reasoning components) that model action effects might enable zero-shot transfer of interactive capabilities across domains, or might overfit to specific causal structures and reduce generalization to novel environments.</li>
                <li>The grounding gap might be fundamentally related to the autoregressive nature of transformer architectures, such that non-autoregressive or bidirectional architectures could show dramatically different QA-to-interactive performance ratios, or the gap might persist across architectures because it reflects training data properties rather than architectural constraints.</li>
                <li>Training on interactive tasks in simulation environments might fully transfer to real-world interactive performance if the simulation is sufficiently high-fidelity, or might create a new 'sim-to-real grounding gap' that compounds the original problem due to distribution shift in observation and action spaces.</li>
                <li>Providing LLMs with the ability to request specific types of information from the environment (active perception) might eliminate the partial observability component of the grounding gap, or might introduce new failure modes related to inefficient information gathering strategies.</li>
                <li>Multi-modal training that grounds language in vision and action from the start (rather than adding it post-hoc) might prevent the grounding gap from emerging, or might simply shift the gap to other modalities without fundamentally addressing the underlying issue.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs trained only on static text (no interactive data) can achieve parity with specialized interactive agents on long-horizon procedural tasks through prompting alone, this would contradict the theory that interactive grounding requires specific training signals beyond what can be elicited from static text training.</li>
                <li>If increasing context window size alone (without architectural changes for state tracking) closes the performance gap between QA and interactive tasks, this would suggest the gap is due to memory limitations rather than grounding deficits, contradicting the theory's emphasis on active state maintenance.</li>
                <li>If LLM agents show equal performance degradation in multi-turn QA dialogues as in interactive procedural tasks (when controlling for context length and complexity), this would indicate the gap is not specific to grounding but rather a general sequential reasoning limitation, undermining the theory's focus on environment interaction.</li>
                <li>If removing environmental feedback and converting interactive tasks to pure planning problems (where the agent generates complete action sequences upfront without intermediate observations) does not improve performance, this would suggest the issue is not with dynamic grounding but with planning capabilities, contradicting the theory's emphasis on action-observation loops.</li>
                <li>If LLMs perform equally poorly on describing procedures as on executing them (when tested rigorously with detailed procedural questions), this would contradict the declarative-procedural knowledge dissociation central to the theory.</li>
                <li>If the performance gap does not increase with task horizon length (i.e., if it remains constant or decreases), this would contradict the theory's prediction about grounding maintenance requirements scaling with horizon.</li>
                <li>If structured feedback (JSON, formal specifications) does not improve performance compared to natural language feedback with identical information content, this would contradict the theory's claims about the role of feedback formality in reducing grounding complexity.</li>
                <li>If external memory modules or state trackers do not improve performance beyond context window expansion, this would suggest that explicit state representation is not the key missing component, contradicting a core architectural prediction of the theory.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not fully explain why certain prompting strategies (like ReAct or chain-of-thought) can substantially improve interactive performance without any architectural changes or fine-tuning, suggesting that some grounding capabilities may already be latent in pre-trained models but poorly elicited by standard prompting. </li>
    <li>The theory does not fully account for individual differences in the magnitude of the grounding gap across different model families (e.g., why some models show larger gaps than others even at similar scales), which might relate to pre-training data composition, architecture details, or training procedures not captured by the theory. </li>
    <li>The theory does not address the role of instruction tuning and RLHF in affecting the grounding gap, which may have effects independent of interactive trajectory training. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Related work on grounding language in robotic affordances, but focuses on affordance learning rather than the systematic dissociation between QA and interactive performance]</li>
    <li>Valmeekam et al. (2023) On the Planning Abilities of Large Language Models [Documents the planning limitations but doesn't provide a unified theory of the grounding gap or distinguish between different types of grounding]</li>
    <li>Bisk et al. (2020) Experience Grounds Language [Discusses grounding in general but predates LLM agents and doesn't address the QA-interactive dissociation specifically]</li>
    <li>Harnad (1990) The Symbol Grounding Problem [Classic work on symbol grounding but focused on semantic grounding rather than interactive procedural grounding in LLMs]</li>
    <li>Shridhar et al. (2020) ALFWorld: Aligning Text and Embodied Environments for Interactive Learning [Addresses embodied task performance but doesn't provide a comprehensive theory of the grounding gap across task types]</li>
    <li>Ahn et al. (2022) Can Wikipedia Help Offline Reinforcement Learning? [Explores using language for RL but doesn't theorize about the QA-interactive dissociation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Interactive Grounding Gap Theory",
    "theory_description": "This theory posits that the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents arises from a fundamental mismatch between the static, context-complete nature of QA tasks and the dynamic, context-evolving nature of interactive procedural tasks. LLMs are trained primarily on static text corpora where 'grounding' refers to linguistic co-reference and semantic coherence, not to maintaining correspondence between linguistic representations and evolving environmental states. In interactive settings, agents must continuously update their internal representations to reflect environmental changes, track causal dependencies across action sequences, and maintain bidirectional grounding between language and world state. The gap emerges because: (1) QA tasks provide all necessary context upfront and require single-step reasoning, while interactive tasks require iterative context construction through action-observation loops; (2) Training on static text does not teach models to distinguish between 'knowing about' procedures (declarative knowledge) and 'executing' procedures (procedural knowledge with grounding); (3) The temporal credit assignment problem in multi-step interactions is not addressed by next-token prediction objectives; (4) Interactive tasks require maintaining multiple simultaneous groundings (goal states, current states, action effects, preconditions) that must be dynamically updated, whereas QA requires only understanding the question context; and (5) The gap can be partially mitigated through inference-time interventions (prompting strategies) or training-time interventions (interactive trajectory fine-tuning), but complete closure requires architectural changes that separate world-state tracking from linguistic processing. The theory further distinguishes between four types of grounding deficits: perceptual grounding (mapping observations to state), causal grounding (understanding action effects), temporal grounding (maintaining state across time), and goal grounding (tracking progress toward objectives). The magnitude of the gap varies systematically with task properties: it is smaller in deterministic, fully-observable, short-horizon tasks and in domains with strong pre-training coverage or formal feedback structures (code, mathematics), and larger in stochastic, partially-observable, long-horizon tasks in novel domains with natural language feedback.",
    "supporting_evidence": [
        {
            "text": "LLMs demonstrate strong performance on static question-answering benchmarks but struggle with embodied AI tasks requiring sequential decision-making and environmental interaction.",
            "citations": [
                "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "Huang et al. (2022) Language Models as Zero-Shot Planners",
                "Driess et al. (2023) PaLM-E: An Embodied Multimodal Language Model"
            ]
        },
        {
            "text": "Agents based on LLMs show degraded performance as task horizons increase and as the number of required interaction steps grows, suggesting issues with temporal reasoning and state tracking.",
            "citations": [
                "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models",
                "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning"
            ]
        },
        {
            "text": "LLMs can describe procedures accurately when asked but fail to execute them correctly in interactive settings, indicating a declarative-procedural knowledge gap.",
            "citations": [
                "Valmeekam et al. (2023) On the Planning Abilities of Large Language Models",
                "Liu et al. (2023) Mind2Web: Towards a Generalist Agent for the Web"
            ]
        },
        {
            "text": "Fine-tuning on interactive trajectories significantly improves agent performance compared to base LLMs, suggesting that interactive grounding requires specific training signals not present in standard language modeling.",
            "citations": [
                "Brohan et al. (2023) RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                "Reed et al. (2022) A Generalist Agent (Gato)"
            ]
        },
        {
            "text": "LLM agents struggle particularly with error recovery and replanning when initial actions fail, indicating difficulty with dynamic context updating and causal reasoning about action effects.",
            "citations": [
                "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning",
                "Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
            ]
        },
        {
            "text": "Carefully engineered prompting strategies like chain-of-thought and ReAct can substantially improve interactive performance without fine-tuning, demonstrating that part of the gap is due to inference-time factors and can be partially bridged through better elicitation of existing capabilities.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models"
            ]
        },
        {
            "text": "LLMs show relatively smaller performance gaps on interactive tasks in formal domains like code execution and mathematical problem-solving compared to embodied or web navigation tasks, suggesting that structured feedback and deterministic environments reduce grounding requirements.",
            "citations": [
                "Chen et al. (2021) Evaluating Large Language Models Trained on Code",
                "Hendrycks et al. (2021) Measuring Mathematical Problem Solving With the MATH Dataset"
            ]
        },
        {
            "text": "Very large language models (100B+ parameters) show improved interactive task performance with minimal fine-tuning, suggesting that scale partially addresses the grounding gap but does not eliminate it entirely.",
            "citations": [
                "Chowdhery et al. (2022) PaLM: Scaling Language Modeling with Pathways",
                "OpenAI (2023) GPT-4 Technical Report"
            ]
        }
    ],
    "theory_statements": [
        "The performance gap between QA and interactive tasks increases monotonically with task horizon length, as longer horizons require more sustained grounding maintenance across multiple types (perceptual, causal, temporal, goal).",
        "LLMs trained solely on static text will exhibit a systematic dissociation between their ability to describe procedures (declarative knowledge) and their ability to execute procedures (procedural knowledge with grounding), with the dissociation magnitude inversely proportional to the formality and determinism of the task domain.",
        "Interactive grounding requires four distinct capabilities not fully trained by standard language modeling: (1) perceptual grounding - mapping observations to state representations; (2) causal grounding - understanding and predicting action effects; (3) temporal grounding - maintaining state correspondence across time steps; and (4) goal grounding - tracking progress toward objectives and subgoals.",
        "The grounding gap can be quantified as the ratio of performance degradation per interaction step in procedural tasks versus performance stability in multi-turn QA dialogues with equivalent context lengths, with this ratio being larger for tasks requiring all four grounding types simultaneously.",
        "Architectural interventions that explicitly separate world-state representations from linguistic representations (e.g., external memory modules, state trackers), and training interventions that include action-observation trajectories with explicit grounding supervision, are necessary to fully close the interactive grounding gap, though partial improvements can be achieved through inference-time prompting strategies or scale alone.",
        "The gap manifests most severely at decision points where the agent must: (a) choose actions based on partially observable state, (b) integrate feedback from previous actions, (c) recover from errors, or (d) perform temporal credit assignment - all scenarios requiring active maintenance of multiple grounding types.",
        "LLMs will show asymmetric performance: high accuracy in predicting what action should be taken next when given complete state descriptions (QA-style), but lower accuracy in actually selecting and executing correct actions when they must infer state from interaction history (interactive-style), with the asymmetry decreasing as model scale increases and as prompting sophistication increases.",
        "The grounding gap is smaller in domains where: (a) the environment provides structured, formal feedback (code interpreters, mathematical verification); (b) the task is deterministic and fully observable; (c) the LLM has extensive pre-training coverage; or (d) the task can be decomposed into independent subtasks, because these properties reduce the complexity of grounding maintenance.",
        "Training on interactive trajectories creates a qualitatively different internal representation compared to training on static descriptions of the same procedures, with the interactive training producing representations that bind actions to state changes rather than merely encoding action sequences.",
        "The gap persists even with very large context windows because the issue is not memory capacity but rather the lack of mechanisms for active state tracking, causal modeling, and temporal credit assignment that are orthogonal to context length."
    ],
    "new_predictions_likely": [
        "Augmenting LLM agents with explicit external memory modules that track environmental state separately from the language context (e.g., key-value memory stores, state graphs) will reduce the interactive grounding gap more effectively than simply increasing context window size, with the improvement being proportional to the degree of partial observability in the task.",
        "Training LLMs with contrastive objectives that explicitly distinguish between valid and invalid state transitions (e.g., contrasting successful vs. failed action sequences) will improve interactive task performance more than equivalent amounts of additional pre-training on static text describing the same domain.",
        "LLM agents will show better interactive performance on tasks where environmental feedback is provided in structured formats (e.g., JSON state representations, formal verification results) compared to natural language descriptions, even when the information content is identical, with the performance difference being largest for tasks requiring precise state tracking.",
        "The performance gap will be smaller for tasks that can be decomposed into independent subtasks versus tasks requiring tight coupling between sequential actions, as independent subtasks reduce grounding maintenance requirements, with the effect being measurable as a function of the causal dependency graph density.",
        "Fine-tuning on synthetic interactive trajectories generated by rule-based agents or simulators will improve real-world interactive performance, demonstrating that the gap is partly due to lack of training signal rather than fundamental model limitations, with transfer effectiveness depending on the fidelity of the simulation.",
        "Providing LLM agents with explicit state summaries at each step (even if generated by the LLM itself through self-prompting) will improve performance more than providing equivalent amounts of raw observational history, demonstrating the importance of explicit state representation.",
        "LLM agents trained with curriculum learning that gradually increases task horizon length will show better long-horizon performance than agents trained on mixed-length tasks, as this allows progressive development of grounding maintenance capabilities."
    ],
    "new_predictions_unknown": [
        "Training LLMs with a dual objective that simultaneously optimizes for next-token prediction and next-state prediction might completely eliminate the grounding gap by forcing the model to maintain explicit state representations, or might create interference between objectives that worsens both capabilities due to competing optimization pressures.",
        "Scaling model size alone (without architectural or training changes) may eventually close the grounding gap through emergent abilities at sufficient scale (e.g., 1T+ parameters), or the gap may persist regardless of scale due to fundamental training objective misalignment that cannot be overcome by capacity alone.",
        "Incorporating explicit causal reasoning modules (e.g., learned causal graphs, counterfactual reasoning components) that model action effects might enable zero-shot transfer of interactive capabilities across domains, or might overfit to specific causal structures and reduce generalization to novel environments.",
        "The grounding gap might be fundamentally related to the autoregressive nature of transformer architectures, such that non-autoregressive or bidirectional architectures could show dramatically different QA-to-interactive performance ratios, or the gap might persist across architectures because it reflects training data properties rather than architectural constraints.",
        "Training on interactive tasks in simulation environments might fully transfer to real-world interactive performance if the simulation is sufficiently high-fidelity, or might create a new 'sim-to-real grounding gap' that compounds the original problem due to distribution shift in observation and action spaces.",
        "Providing LLMs with the ability to request specific types of information from the environment (active perception) might eliminate the partial observability component of the grounding gap, or might introduce new failure modes related to inefficient information gathering strategies.",
        "Multi-modal training that grounds language in vision and action from the start (rather than adding it post-hoc) might prevent the grounding gap from emerging, or might simply shift the gap to other modalities without fundamentally addressing the underlying issue."
    ],
    "negative_experiments": [
        "If LLMs trained only on static text (no interactive data) can achieve parity with specialized interactive agents on long-horizon procedural tasks through prompting alone, this would contradict the theory that interactive grounding requires specific training signals beyond what can be elicited from static text training.",
        "If increasing context window size alone (without architectural changes for state tracking) closes the performance gap between QA and interactive tasks, this would suggest the gap is due to memory limitations rather than grounding deficits, contradicting the theory's emphasis on active state maintenance.",
        "If LLM agents show equal performance degradation in multi-turn QA dialogues as in interactive procedural tasks (when controlling for context length and complexity), this would indicate the gap is not specific to grounding but rather a general sequential reasoning limitation, undermining the theory's focus on environment interaction.",
        "If removing environmental feedback and converting interactive tasks to pure planning problems (where the agent generates complete action sequences upfront without intermediate observations) does not improve performance, this would suggest the issue is not with dynamic grounding but with planning capabilities, contradicting the theory's emphasis on action-observation loops.",
        "If LLMs perform equally poorly on describing procedures as on executing them (when tested rigorously with detailed procedural questions), this would contradict the declarative-procedural knowledge dissociation central to the theory.",
        "If the performance gap does not increase with task horizon length (i.e., if it remains constant or decreases), this would contradict the theory's prediction about grounding maintenance requirements scaling with horizon.",
        "If structured feedback (JSON, formal specifications) does not improve performance compared to natural language feedback with identical information content, this would contradict the theory's claims about the role of feedback formality in reducing grounding complexity.",
        "If external memory modules or state trackers do not improve performance beyond context window expansion, this would suggest that explicit state representation is not the key missing component, contradicting a core architectural prediction of the theory."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not fully explain why certain prompting strategies (like ReAct or chain-of-thought) can substantially improve interactive performance without any architectural changes or fine-tuning, suggesting that some grounding capabilities may already be latent in pre-trained models but poorly elicited by standard prompting.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models"
            ]
        },
        {
            "text": "The theory does not fully account for individual differences in the magnitude of the grounding gap across different model families (e.g., why some models show larger gaps than others even at similar scales), which might relate to pre-training data composition, architecture details, or training procedures not captured by the theory.",
            "citations": [
                "Chowdhery et al. (2022) PaLM: Scaling Language Modeling with Pathways",
                "OpenAI (2023) GPT-4 Technical Report"
            ]
        },
        {
            "text": "The theory does not address the role of instruction tuning and RLHF in affecting the grounding gap, which may have effects independent of interactive trajectory training.",
            "citations": [
                "Ouyang et al. (2022) Training language models to follow instructions with human feedback"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that very large language models (100B+ parameters) can achieve reasonable interactive task performance with minimal fine-tuning, suggesting the gap may narrow substantially with scale alone, which somewhat conflicts with the theory's emphasis on the necessity of architectural and training interventions (though the theory now acknowledges scale as a partial solution).",
            "citations": [
                "Chowdhery et al. (2022) PaLM: Scaling Language Modeling with Pathways",
                "OpenAI (2023) GPT-4 Technical Report"
            ]
        },
        {
            "text": "The success of prompting-only methods like ReAct in achieving substantial improvements on interactive tasks suggests that the grounding gap may be partly an elicitation problem rather than a fundamental capability gap, which somewhat conflicts with the theory's emphasis on missing training signals (though the theory now incorporates this as partial evidence).",
            "citations": [
                "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models",
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
            ]
        }
    ],
    "special_cases": [
        "Tasks with deterministic, fully observable environments show smaller grounding gaps because perceptual and temporal grounding requirements are simplified, with the gap potentially approaching zero for very simple deterministic tasks.",
        "Interactive tasks that closely resemble QA in structure (e.g., multi-turn information retrieval, conversational QA) may not exhibit the full grounding gap because they lack the causal grounding component (actions don't change environment state).",
        "Domains where the LLM has extensive pre-training coverage (e.g., common household tasks, popular programming languages) show reduced gaps compared to novel domains because relevant procedural patterns may be implicitly learned during pre-training.",
        "Very short-horizon tasks (1-3 steps) may not reveal the full grounding gap because temporal dependencies and credit assignment problems are minimal, making the task more similar to QA.",
        "Tasks in formal domains with structured feedback (code execution, mathematical verification, game environments with explicit rules) show smaller gaps because the feedback directly indicates state and action validity, reducing perceptual and causal grounding requirements.",
        "Tasks that can be decomposed into independent subtasks show smaller gaps than tasks with tight sequential dependencies because each subtask can be treated more like an independent QA problem.",
        "Environments with dense, immediate feedback show smaller gaps than environments with sparse, delayed feedback because temporal credit assignment is simplified.",
        "Tasks where the optimal policy is highly stochastic or requires exploration show larger gaps because LLMs trained on static text lack mechanisms for exploration and uncertainty-driven action selection."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Related work on grounding language in robotic affordances, but focuses on affordance learning rather than the systematic dissociation between QA and interactive performance]",
            "Valmeekam et al. (2023) On the Planning Abilities of Large Language Models [Documents the planning limitations but doesn't provide a unified theory of the grounding gap or distinguish between different types of grounding]",
            "Bisk et al. (2020) Experience Grounds Language [Discusses grounding in general but predates LLM agents and doesn't address the QA-interactive dissociation specifically]",
            "Harnad (1990) The Symbol Grounding Problem [Classic work on symbol grounding but focused on semantic grounding rather than interactive procedural grounding in LLMs]",
            "Shridhar et al. (2020) ALFWorld: Aligning Text and Embodied Environments for Interactive Learning [Addresses embodied task performance but doesn't provide a comprehensive theory of the grounding gap across task types]",
            "Ahn et al. (2022) Can Wikipedia Help Offline Reinforcement Learning? [Explores using language for RL but doesn't theorize about the QA-interactive dissociation]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory explaining the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents, and the architectural/training interventions required to close the gap.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-105",
    "original_theory_name": "Interactive Grounding Gap Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
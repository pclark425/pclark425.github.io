<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Iterative Self-Reflection as a Bounded Rationality Process in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1444</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1444</p>
                <p><strong>Name:</strong> Iterative Self-Reflection as a Bounded Rationality Process in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.</p>
                <p><strong>Description:</strong> This theory conceptualizes iterative self-reflection in LLMs as a process of bounded rationality, where each reflection step is a local search in the space of possible answers, constrained by the model's internal representations and the information available in the prompt. The process is subject to diminishing returns, error propagation, and local minima, with the outcome determined by the interplay of task complexity, model capacity, and the structure of the reflection prompt.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Diminishing Returns of Iterative Reflection (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reflection_steps &#8594; greater_than &#8594; N_optimal</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; marginal_gain_in_answer_quality &#8594; approaches &#8594; zero_or_negative</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that after a small number of reflection steps, further iterations yield little or no improvement, and can even degrade answer quality. </li>
    <li>Reflection-induced overcorrection and oscillation are observed with excessive iterations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law generalizes empirical findings into a quantitative, process-level law.</p>            <p><strong>What Already Exists:</strong> Diminishing returns with repeated self-refinement are empirically observed.</p>            <p><strong>What is Novel:</strong> The formalization as a bounded rationality process with an optimal number of steps is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [diminishing returns]</li>
    <li>Uesato et al. (2022) Solving Math Word Problems with Process Supervision [reflection and local search]</li>
</ul>
            <h3>Statement 1: Local Minima and Error Propagation in Reflection (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; initial_answer &#8594; contains_error &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; reflection &#8594; is_applied &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model_output &#8594; may_converge_to &#8594; local_minimum_with_persistent_error</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Models sometimes get stuck repeating or justifying an initial error through multiple reflection steps, indicating local minima in the answer space. </li>
    <li>Error propagation is observed when reflection fails to identify or correct initial mistakes. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law synthesizes empirical observations into a process-theoretic framework.</p>            <p><strong>What Already Exists:</strong> Reflection sometimes fails to correct initial errors, leading to persistent mistakes.</p>            <p><strong>What is Novel:</strong> The explicit analogy to local minima and error propagation in bounded rationality is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [error propagation]</li>
    <li>Uesato et al. (2022) Solving Math Word Problems with Process Supervision [local search and error correction]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>There exists an optimal number of reflection steps for each task-model pair, beyond which answer quality plateaus or declines.</li>
                <li>Tasks with high initial error rates are more likely to result in persistent mistakes after reflection.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If models are trained with explicit mechanisms to escape local minima, reflection may yield sustained improvements over more iterations.</li>
                <li>Reflection with external feedback (e.g., human-in-the-loop) may overcome error propagation and local minima.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If answer quality continues to improve indefinitely with more reflection steps, the theory would be challenged.</li>
                <li>If initial errors are always corrected by reflection, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where reflection unexpectedly escapes local minima without external intervention. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory synthesizes and extends empirical findings into a new, process-level theoretical framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [diminishing returns, error propagation]</li>
    <li>Uesato et al. (2022) Solving Math Word Problems with Process Supervision [local search, process supervision]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Iterative Self-Reflection as a Bounded Rationality Process in LLMs",
    "theory_description": "This theory conceptualizes iterative self-reflection in LLMs as a process of bounded rationality, where each reflection step is a local search in the space of possible answers, constrained by the model's internal representations and the information available in the prompt. The process is subject to diminishing returns, error propagation, and local minima, with the outcome determined by the interplay of task complexity, model capacity, and the structure of the reflection prompt.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Diminishing Returns of Iterative Reflection",
                "if": [
                    {
                        "subject": "reflection_steps",
                        "relation": "greater_than",
                        "object": "N_optimal"
                    }
                ],
                "then": [
                    {
                        "subject": "marginal_gain_in_answer_quality",
                        "relation": "approaches",
                        "object": "zero_or_negative"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that after a small number of reflection steps, further iterations yield little or no improvement, and can even degrade answer quality.",
                        "uuids": []
                    },
                    {
                        "text": "Reflection-induced overcorrection and oscillation are observed with excessive iterations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Diminishing returns with repeated self-refinement are empirically observed.",
                    "what_is_novel": "The formalization as a bounded rationality process with an optimal number of steps is novel.",
                    "classification_explanation": "The law generalizes empirical findings into a quantitative, process-level law.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [diminishing returns]",
                        "Uesato et al. (2022) Solving Math Word Problems with Process Supervision [reflection and local search]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Local Minima and Error Propagation in Reflection",
                "if": [
                    {
                        "subject": "initial_answer",
                        "relation": "contains_error",
                        "object": "True"
                    },
                    {
                        "subject": "reflection",
                        "relation": "is_applied",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "model_output",
                        "relation": "may_converge_to",
                        "object": "local_minimum_with_persistent_error"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Models sometimes get stuck repeating or justifying an initial error through multiple reflection steps, indicating local minima in the answer space.",
                        "uuids": []
                    },
                    {
                        "text": "Error propagation is observed when reflection fails to identify or correct initial mistakes.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Reflection sometimes fails to correct initial errors, leading to persistent mistakes.",
                    "what_is_novel": "The explicit analogy to local minima and error propagation in bounded rationality is novel.",
                    "classification_explanation": "The law synthesizes empirical observations into a process-theoretic framework.",
                    "likely_classification": "new",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [error propagation]",
                        "Uesato et al. (2022) Solving Math Word Problems with Process Supervision [local search and error correction]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "There exists an optimal number of reflection steps for each task-model pair, beyond which answer quality plateaus or declines.",
        "Tasks with high initial error rates are more likely to result in persistent mistakes after reflection."
    ],
    "new_predictions_unknown": [
        "If models are trained with explicit mechanisms to escape local minima, reflection may yield sustained improvements over more iterations.",
        "Reflection with external feedback (e.g., human-in-the-loop) may overcome error propagation and local minima."
    ],
    "negative_experiments": [
        "If answer quality continues to improve indefinitely with more reflection steps, the theory would be challenged.",
        "If initial errors are always corrected by reflection, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where reflection unexpectedly escapes local minima without external intervention.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks show non-monotonic improvement, with answer quality improving after many reflection steps, contrary to the law of diminishing returns.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with external verification or feedback may not be subject to the same local minima.",
        "Models with explicit error-detection modules may be less prone to error propagation."
    ],
    "existing_theory": {
        "what_already_exists": "Diminishing returns and error propagation in reflection are empirically observed.",
        "what_is_novel": "The process-theoretic, bounded rationality framing and formalization of local minima are novel.",
        "classification_explanation": "The theory synthesizes and extends empirical findings into a new, process-level theoretical framework.",
        "likely_classification": "new",
        "references": [
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [diminishing returns, error propagation]",
            "Uesato et al. (2022) Solving Math Word Problems with Process Supervision [local search, process supervision]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.",
    "original_theory_id": "theory-623",
    "original_theory_name": "Task- and Model-Dependence of Self-Reflection Efficacy in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Task- and Model-Dependence of Self-Reflection Efficacy in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dynamic Episodic-Semantic Memory Integration for LLM Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-856</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-856</p>
                <p><strong>Name:</strong> Dynamic Episodic-Semantic Memory Integration for LLM Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language model agents can best use memory to solve tasks.</p>
                <p><strong>Description:</strong> This theory posits that language model agents achieve optimal task performance by dynamically integrating episodic (event-based) and semantic (fact-based) memories. The agent maintains both types of memory, leveraging episodic memory for context-specific reasoning and semantic memory for generalization. The integration is guided by task demands, with the agent adaptively weighting the contribution of each memory type based on the novelty and specificity of the current task.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Adaptive Memory Integration Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; is_solving &#8594; task<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; has_novelty_level &#8594; N<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; has_specificity_level &#8594; S</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; weights_memory_integration &#8594; f(N, S)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human cognition flexibly integrates episodic and semantic memory depending on task demands. </li>
    <li>LLM agents with both episodic and semantic memory modules outperform those with only one type on complex tasks. </li>
    <li>Novel tasks require more episodic retrieval, while familiar tasks benefit from semantic generalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While episodic and semantic memory are known, their adaptive integration in LLM agents based on task features is a new formalization.</p>            <p><strong>What Already Exists:</strong> Episodic and semantic memory distinction is well-established in cognitive science; some LLMs use retrieval-augmented memory.</p>            <p><strong>What is Novel:</strong> The dynamic, task-driven weighting of episodic and semantic memory in LLM agents is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [distinction in human memory]</li>
    <li>Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [retrieval-augmented LLMs]</li>
    <li>Madotto et al. (2020) Continual Learning in Task-Oriented Dialogue Systems [episodic/semantic memory in agents]</li>
</ul>
            <h3>Statement 1: Context-Driven Memory Selection Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; is_solving &#8594; task<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; has_context_vector &#8594; C<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory_trace &#8594; has_context_vector &#8594; C_m</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; retrieves &#8594; memory_traces_with_max_similarity(C, C_m)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Contextual similarity is a strong predictor of relevant memory retrieval in both humans and LLMs. </li>
    <li>LLM agents using context-aware retrieval outperform those using random or exhaustive retrieval. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Contextual retrieval is known, but the vector-based formalization and its application to LLM agent memory is new.</p>            <p><strong>What Already Exists:</strong> Contextual retrieval is used in information retrieval and some LLM memory systems.</p>            <p><strong>What is Novel:</strong> The explicit formalization of context vectors for both tasks and memory traces in LLM agents is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [contextual retrieval in LLMs]</li>
    <li>Kahana (2012) Foundations of Human Memory [contextual retrieval in humans]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with dynamic episodic-semantic integration will outperform those with static or single-type memory on tasks requiring both generalization and adaptation.</li>
                <li>Context-driven memory selection will reduce irrelevant retrievals and improve task accuracy, especially in multi-step reasoning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Emergent meta-memory strategies may arise, where agents learn to predict which memory type to prioritize for novel task types.</li>
                <li>Dynamic integration may enable agents to transfer learning across domains more effectively than static memory architectures.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If static memory weighting outperforms dynamic integration, the theory would be challenged.</li>
                <li>If context-driven retrieval does not improve over random retrieval, the theory's mechanism would be questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address catastrophic forgetting in continual learning scenarios. </li>
    <li>The theory does not specify how to construct or update context vectors in non-textual or multimodal tasks. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known memory types with new adaptive integration and formal context representation for LLM agents.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [human memory theory]</li>
    <li>Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [retrieval-augmented LLMs]</li>
    <li>Madotto et al. (2020) Continual Learning in Task-Oriented Dialogue Systems [episodic/semantic memory in agents]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dynamic Episodic-Semantic Memory Integration for LLM Agents",
    "theory_description": "This theory posits that language model agents achieve optimal task performance by dynamically integrating episodic (event-based) and semantic (fact-based) memories. The agent maintains both types of memory, leveraging episodic memory for context-specific reasoning and semantic memory for generalization. The integration is guided by task demands, with the agent adaptively weighting the contribution of each memory type based on the novelty and specificity of the current task.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Adaptive Memory Integration Law",
                "if": [
                    {
                        "subject": "agent",
                        "relation": "is_solving",
                        "object": "task"
                    },
                    {
                        "subject": "task",
                        "relation": "has_novelty_level",
                        "object": "N"
                    },
                    {
                        "subject": "task",
                        "relation": "has_specificity_level",
                        "object": "S"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "weights_memory_integration",
                        "object": "f(N, S)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human cognition flexibly integrates episodic and semantic memory depending on task demands.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents with both episodic and semantic memory modules outperform those with only one type on complex tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Novel tasks require more episodic retrieval, while familiar tasks benefit from semantic generalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Episodic and semantic memory distinction is well-established in cognitive science; some LLMs use retrieval-augmented memory.",
                    "what_is_novel": "The dynamic, task-driven weighting of episodic and semantic memory in LLM agents is novel.",
                    "classification_explanation": "While episodic and semantic memory are known, their adaptive integration in LLM agents based on task features is a new formalization.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [distinction in human memory]",
                        "Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [retrieval-augmented LLMs]",
                        "Madotto et al. (2020) Continual Learning in Task-Oriented Dialogue Systems [episodic/semantic memory in agents]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Context-Driven Memory Selection Law",
                "if": [
                    {
                        "subject": "agent",
                        "relation": "is_solving",
                        "object": "task"
                    },
                    {
                        "subject": "task",
                        "relation": "has_context_vector",
                        "object": "C"
                    },
                    {
                        "subject": "memory_trace",
                        "relation": "has_context_vector",
                        "object": "C_m"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "retrieves",
                        "object": "memory_traces_with_max_similarity(C, C_m)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Contextual similarity is a strong predictor of relevant memory retrieval in both humans and LLMs.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents using context-aware retrieval outperform those using random or exhaustive retrieval.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Contextual retrieval is used in information retrieval and some LLM memory systems.",
                    "what_is_novel": "The explicit formalization of context vectors for both tasks and memory traces in LLM agents is novel.",
                    "classification_explanation": "Contextual retrieval is known, but the vector-based formalization and its application to LLM agent memory is new.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [contextual retrieval in LLMs]",
                        "Kahana (2012) Foundations of Human Memory [contextual retrieval in humans]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with dynamic episodic-semantic integration will outperform those with static or single-type memory on tasks requiring both generalization and adaptation.",
        "Context-driven memory selection will reduce irrelevant retrievals and improve task accuracy, especially in multi-step reasoning."
    ],
    "new_predictions_unknown": [
        "Emergent meta-memory strategies may arise, where agents learn to predict which memory type to prioritize for novel task types.",
        "Dynamic integration may enable agents to transfer learning across domains more effectively than static memory architectures."
    ],
    "negative_experiments": [
        "If static memory weighting outperforms dynamic integration, the theory would be challenged.",
        "If context-driven retrieval does not improve over random retrieval, the theory's mechanism would be questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address catastrophic forgetting in continual learning scenarios.",
            "uuids": []
        },
        {
            "text": "The theory does not specify how to construct or update context vectors in non-textual or multimodal tasks.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks benefit from exhaustive memory search rather than context-driven selection.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with highly ambiguous or underspecified context may not benefit from context-driven retrieval.",
        "Agents with limited memory capacity may need to prune memories, affecting integration."
    ],
    "existing_theory": {
        "what_already_exists": "Episodic/semantic memory distinction and context-based retrieval are established in cognitive science and some LLMs.",
        "what_is_novel": "Dynamic, task-driven integration and vector-based context formalization for LLM agents is novel.",
        "classification_explanation": "The theory synthesizes known memory types with new adaptive integration and formal context representation for LLM agents.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [human memory theory]",
            "Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [retrieval-augmented LLMs]",
            "Madotto et al. (2020) Continual Learning in Task-Oriented Dialogue Systems [episodic/semantic memory in agents]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language model agents can best use memory to solve tasks.",
    "original_theory_id": "theory-586",
    "original_theory_name": "Hybrid Memory Coordination Theory for LLM Agents",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
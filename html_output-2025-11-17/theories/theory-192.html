<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Online Adaptation and Residual Learning Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-192</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-192</p>
                <p><strong>Name:</strong> Online Adaptation and Residual Learning Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about sim-to-real transfer for scientific discovery agents that specifies environment fidelity requirements and skill-transfer conditions from virtual labs to real robotics labs, based on the following results.</p>
                <p><strong>Description:</strong> Online adaptation—adjusting policy or simulator parameters during deployment using real-world feedback—enables transfer when: (1) the sim-to-real gap is systematic and learnable from limited data, (2) safe exploration is possible during adaptation, and (3) adaptation can occur faster than environment changes. Effective approaches include: residual policy learning (learning corrections to sim-trained policies), online parameter adaptation (updating simulator or controller parameters), iterative simulator tuning (updating simulator parameters offline between deployment cycles), and memory-augmented policies (implicit online adaptation). The sample efficiency of adaptation depends on the adaptation mechanism, the magnitude and structure of the gap, and the quality of the sim-trained initialization. Adaptation is most effective for parametric gaps (wrong parameter values) rather than structural gaps (missing physics).</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Residual learning is effective when the sim-to-real gap is systematic and can be learned from 10-500 real examples, with sample efficiency depending on gap dimensionality and structure</li>
                <li>Online parameter adaptation is effective when 1-10 key parameters can be adjusted based on real-time feedback, with diminishing returns beyond 10 parameters due to identifiability issues</li>
                <li>Memory-augmented policies (LSTM, Transformers) enable implicit online adaptation by learning to identify and compensate for dynamics differences, achieving 10-50% better performance than feedforward policies on tasks with systematic gaps</li>
                <li>Safe online adaptation requires either: (1) a protective policy that ensures safety during exploration, (2) a safe exploration region, (3) human supervision, or (4) off-policy learning from safe demonstrations</li>
                <li>Iterative simulator tuning (offline adaptation between deployment cycles) is more sample-efficient than pure online adaptation, requiring 3-20 real rollouts per iteration vs 50-1000 for online methods</li>
                <li>Online adaptation is most effective when combined with good sim-trained initialization: pure online learning from scratch requires 10-100× more samples</li>
                <li>The adaptation rate must exceed the rate of environment changes for online adaptation to be effective; non-stationary environments require continuous adaptation</li>
                <li>Adaptation is most effective for parametric gaps (wrong parameter values) and less effective for structural gaps (missing physics or wrong model class)</li>
                <li>One-shot adaptation methods can work when the gap is low-dimensional (1-3 parameters) and the adaptation model is pre-trained on diverse source variations</li>
                <li>Gated or selective adaptation (applying corrections only when needed) outperforms always-on adaptation by 10-30% in tasks with partial sim-to-real gaps</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Online admittance residual learning adapted compliance parameters during deployment, achieving 10/10 assembly success and 9/10 pivoting success by optimizing residuals from recent force measurements <a href="../results/extraction-result-1650.html#e1650.0" class="evidence-link">[e1650.0]</a> </li>
    <li>TRANSIC learned gated residual policies from human corrections (20-100 trajectories with 58-489 corrections per task), achieving 45-100% success on contact-rich manipulation tasks <a href="../results/extraction-result-1672.html#e1672.0" class="evidence-link">[e1672.0]</a> </li>
    <li>Dynamics adaptor (DA) learned residual dynamics from 100-5,000 target-domain samples, matching in-target fine-tuning performance with 117× fewer samples <a href="../results/extraction-result-1662.html#e1662.2" class="evidence-link">[e1662.2]</a> </li>
    <li>Protective policy transfer used safety-aware switching and OSSE to enable safe online learning, achieving successful transfer across substantial simulation gaps with minimal unsafe trials <a href="../results/extraction-result-1660.html#e1660.0" class="evidence-link">[e1660.0]</a> <a href="../results/extraction-result-1660.html#e1660.2" class="evidence-link">[e1660.2]</a> </li>
    <li>Memory-augmented policies (LSTM) enabled online implicit adaptation for dexterous manipulation, achieving median 13 consecutive successes vs 0 for non-randomized policies <a href="../results/extraction-result-1651.html#e1651.0" class="evidence-link">[e1651.0]</a> </li>
    <li>Model-predictive control using simulator as foresight enabled online replanning for skill composition, achieving zero-shot transfer for drawing and box-pushing tasks <a href="../results/extraction-result-1631.html#e1631.0" class="evidence-link">[e1631.0]</a> </li>
    <li>Grounded simulation learning (GSL) iteratively adapted simulator using real robot data, achieving 43.27% improvement in walk velocity through iterative parameter updates <a href="../results/extraction-result-1795.html#e1795.1" class="evidence-link">[e1795.1]</a> <a href="../results/extraction-result-1795.html#e1795.0" class="evidence-link">[e1795.0]</a> </li>
    <li>Online RL fine-tuning in semi-virtual environment improved coverage path planning, with higher-order policies improving after ~60k real steps <a href="../results/extraction-result-1641.html#e1641.0" class="evidence-link">[e1641.0]</a> </li>
    <li>SimOpt adapted simulation parameter distributions using 3 real rollouts per iteration, achieving 100% drawer-opening success after iterative updates <a href="../results/extraction-result-1657.html#e1657.2" class="evidence-link">[e1657.2]</a> </li>
    <li>Reward-based re-identification using 10 failed-policy episodes enabled recovery and successful transfer for bipedal locomotion <a href="../results/extraction-result-1643.html#e1643.2" class="evidence-link">[e1643.2]</a> </li>
    <li>TuneNet one-shot residual tuning estimated parameter differences from a single target observation plus simulated rollouts, achieving 62-87% bounce-shot success <a href="../results/extraction-result-1648.html#e1648.0" class="evidence-link">[e1648.0]</a> </li>
    <li>Auto-tuned sim-to-real with SPM adjusted simulator mean using unlabeled real RGB sequences, achieving 90% cabinet-slide success vs 0% for baseline <a href="../results/extraction-result-1647.html#e1647.1" class="evidence-link">[e1647.1]</a> </li>
    <li>Simulator parameter tuning via Differential Evolution reduced trajectory error by 14-91% across experiments <a href="../results/extraction-result-1814.html#e1814.0" class="evidence-link">[e1814.0]</a> </li>
    <li>Sim2Real2Sim iterative workflow used real-world measurements to identify stiffness and damping, reducing joint-angle errors to <4.2% <a href="../results/extraction-result-1764.html#e1764.0" class="evidence-link">[e1764.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A manipulation task with systematic contact dynamics mismatch will achieve >80% success after learning residuals from 50-100 real corrections, provided the gap is parametric</li>
                <li>A locomotion task will adapt to new terrain within 5-10 episodes using meta-learned adaptation policies trained on 20+ source terrains</li>
                <li>Online admittance adaptation will reduce contact forces by 30-50% within 10-20 task executions for contact-rich tasks with force feedback</li>
                <li>Memory-augmented policies will outperform feedforward policies by 20-40% on tasks with systematic sim-to-real gaps that can be inferred from observation history</li>
                <li>Iterative simulator tuning with 5-10 real rollouts per iteration will match the performance of policies trained with 1000+ real samples for tasks with 3-5 key parameters</li>
                <li>Gated residual policies will achieve 10-20% higher success than always-on residuals on tasks where the sim-to-real gap is spatially or temporally localized</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether online adaptation can handle non-stationary environments (changing dynamics, wear) or only static sim-to-real gaps, and what adaptation rate is required</li>
                <li>Whether learned adaptation strategies generalize across task types or must be task-specific, and what determines transferability</li>
                <li>Whether there exists a minimum adaptation rate required for stability, or if arbitrarily slow adaptation can eventually succeed without divergence</li>
                <li>Whether online adaptation can correct for structural model errors (wrong model class) or only parametric errors (wrong parameter values)</li>
                <li>Whether adaptation can handle multi-modal parameter spaces where multiple parameter settings produce similar behavior</li>
                <li>Whether there is a fundamental limit to the dimensionality of gaps that can be adapted online (e.g., can 50+ parameters be adapted simultaneously?)</li>
                <li>Whether adaptation interferes with task performance during the adaptation phase, and how to minimize this interference</li>
                <li>Whether human-in-the-loop adaptation scales to complex tasks or is limited to simple corrections</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding a task where online adaptation without sim-trained initialization outperforms sim-to-real transfer would challenge the initialization principle</li>
                <li>Demonstrating successful adaptation with <10 real examples for high-dimensional gaps (>10 parameters) would challenge the sample-efficiency limits</li>
                <li>Showing that feedforward policies adapt as well as memory-augmented policies on tasks with systematic temporal gaps would challenge the memory principle</li>
                <li>Finding that unsafe exploration during adaptation does not degrade performance would challenge the safety requirement</li>
                <li>Demonstrating that always-on residuals outperform gated residuals would challenge the selective adaptation principle</li>
                <li>Finding that adaptation works equally well for structural and parametric gaps would challenge the gap-type distinction</li>
                <li>Showing that adaptation rate can be arbitrarily slow without stability issues would challenge the rate requirement</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How adaptation requirements change with the magnitude and type of sim-to-real gap (parametric vs structural, low-dimensional vs high-dimensional) <a href="../results/extraction-result-1650.html#e1650.0" class="evidence-link">[e1650.0]</a> <a href="../results/extraction-result-1672.html#e1672.0" class="evidence-link">[e1672.0]</a> <a href="../results/extraction-result-1648.html#e1648.0" class="evidence-link">[e1648.0]</a> </li>
    <li>Whether adaptation effectiveness depends on policy architecture (feedforward vs recurrent vs transformer) or learning algorithm (on-policy vs off-policy) <a href="../results/extraction-result-1651.html#e1651.0" class="evidence-link">[e1651.0]</a> <a href="../results/extraction-result-1662.html#e1662.2" class="evidence-link">[e1662.2]</a> <a href="../results/extraction-result-1641.html#e1641.0" class="evidence-link">[e1641.0]</a> </li>
    <li>How to determine when adaptation has converged and no further real-world data is needed, and whether convergence guarantees exist <a href="../results/extraction-result-1660.html#e1660.0" class="evidence-link">[e1660.0]</a> <a href="../results/extraction-result-1641.html#e1641.0" class="evidence-link">[e1641.0]</a> <a href="../results/extraction-result-1795.html#e1795.1" class="evidence-link">[e1795.1]</a> </li>
    <li>How to choose what to adapt (policy parameters, simulator parameters, controller parameters, or combinations) for a given task <a href="../results/extraction-result-1650.html#e1650.0" class="evidence-link">[e1650.0]</a> <a href="../results/extraction-result-1647.html#e1647.1" class="evidence-link">[e1647.1]</a> <a href="../results/extraction-result-1795.html#e1795.0" class="evidence-link">[e1795.0]</a> </li>
    <li>Computational requirements for online adaptation and whether real-time adaptation is feasible for high-frequency control tasks <a href="../results/extraction-result-1641.html#e1641.0" class="evidence-link">[e1641.0]</a> <a href="../results/extraction-result-1631.html#e1631.0" class="evidence-link">[e1631.0]</a> </li>
    <li>Whether adaptation can handle multi-modal parameter spaces where multiple parameter settings produce similar behavior <a href="../results/extraction-result-1785.html#e1785.1" class="evidence-link">[e1785.1]</a> <a href="../results/extraction-result-1785.html#e1785.2" class="evidence-link">[e1785.2]</a> <a href="../results/extraction-result-1678.html#e1678.0" class="evidence-link">[e1678.0]</a> </li>
    <li>How adaptation interacts with domain randomization: whether they are complementary or redundant <a href="../results/extraction-result-1650.html#e1650.0" class="evidence-link">[e1650.0]</a> <a href="../results/extraction-result-1651.html#e1651.0" class="evidence-link">[e1651.0]</a> <a href="../results/extraction-result-1791.html#e1791.0" class="evidence-link">[e1791.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Finn et al. (2017) Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [MAML meta-learning for fast adaptation]</li>
    <li>Nagabandi et al. (2019) Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning [Meta-RL for online adaptation]</li>
    <li>Clavera et al. (2018) Model-Based Reinforcement Learning via Meta-Policy Optimization [Model-based meta-learning for adaptation]</li>
    <li>Yu et al. (2020) Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning [Meta-learning benchmark for adaptation]</li>
    <li>Chebotar et al. (2019) Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience [SimOpt iterative adaptation]</li>
    <li>Muratore et al. (2022) Robot Learning from Randomized Simulations: A Review [Review of adaptation and domain randomization methods]</li>
    <li>Pinto et al. (2017) Robust Adversarial Reinforcement Learning [Adversarial training for robustness, related to adaptation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Online Adaptation and Residual Learning Theory",
    "theory_description": "Online adaptation—adjusting policy or simulator parameters during deployment using real-world feedback—enables transfer when: (1) the sim-to-real gap is systematic and learnable from limited data, (2) safe exploration is possible during adaptation, and (3) adaptation can occur faster than environment changes. Effective approaches include: residual policy learning (learning corrections to sim-trained policies), online parameter adaptation (updating simulator or controller parameters), iterative simulator tuning (updating simulator parameters offline between deployment cycles), and memory-augmented policies (implicit online adaptation). The sample efficiency of adaptation depends on the adaptation mechanism, the magnitude and structure of the gap, and the quality of the sim-trained initialization. Adaptation is most effective for parametric gaps (wrong parameter values) rather than structural gaps (missing physics).",
    "supporting_evidence": [
        {
            "text": "Online admittance residual learning adapted compliance parameters during deployment, achieving 10/10 assembly success and 9/10 pivoting success by optimizing residuals from recent force measurements",
            "uuids": [
                "e1650.0"
            ]
        },
        {
            "text": "TRANSIC learned gated residual policies from human corrections (20-100 trajectories with 58-489 corrections per task), achieving 45-100% success on contact-rich manipulation tasks",
            "uuids": [
                "e1672.0"
            ]
        },
        {
            "text": "Dynamics adaptor (DA) learned residual dynamics from 100-5,000 target-domain samples, matching in-target fine-tuning performance with 117× fewer samples",
            "uuids": [
                "e1662.2"
            ]
        },
        {
            "text": "Protective policy transfer used safety-aware switching and OSSE to enable safe online learning, achieving successful transfer across substantial simulation gaps with minimal unsafe trials",
            "uuids": [
                "e1660.0",
                "e1660.2"
            ]
        },
        {
            "text": "Memory-augmented policies (LSTM) enabled online implicit adaptation for dexterous manipulation, achieving median 13 consecutive successes vs 0 for non-randomized policies",
            "uuids": [
                "e1651.0"
            ]
        },
        {
            "text": "Model-predictive control using simulator as foresight enabled online replanning for skill composition, achieving zero-shot transfer for drawing and box-pushing tasks",
            "uuids": [
                "e1631.0"
            ]
        },
        {
            "text": "Grounded simulation learning (GSL) iteratively adapted simulator using real robot data, achieving 43.27% improvement in walk velocity through iterative parameter updates",
            "uuids": [
                "e1795.1",
                "e1795.0"
            ]
        },
        {
            "text": "Online RL fine-tuning in semi-virtual environment improved coverage path planning, with higher-order policies improving after ~60k real steps",
            "uuids": [
                "e1641.0"
            ]
        },
        {
            "text": "SimOpt adapted simulation parameter distributions using 3 real rollouts per iteration, achieving 100% drawer-opening success after iterative updates",
            "uuids": [
                "e1657.2"
            ]
        },
        {
            "text": "Reward-based re-identification using 10 failed-policy episodes enabled recovery and successful transfer for bipedal locomotion",
            "uuids": [
                "e1643.2"
            ]
        },
        {
            "text": "TuneNet one-shot residual tuning estimated parameter differences from a single target observation plus simulated rollouts, achieving 62-87% bounce-shot success",
            "uuids": [
                "e1648.0"
            ]
        },
        {
            "text": "Auto-tuned sim-to-real with SPM adjusted simulator mean using unlabeled real RGB sequences, achieving 90% cabinet-slide success vs 0% for baseline",
            "uuids": [
                "e1647.1"
            ]
        },
        {
            "text": "Simulator parameter tuning via Differential Evolution reduced trajectory error by 14-91% across experiments",
            "uuids": [
                "e1814.0"
            ]
        },
        {
            "text": "Sim2Real2Sim iterative workflow used real-world measurements to identify stiffness and damping, reducing joint-angle errors to &lt;4.2%",
            "uuids": [
                "e1764.0"
            ]
        }
    ],
    "theory_statements": [
        "Residual learning is effective when the sim-to-real gap is systematic and can be learned from 10-500 real examples, with sample efficiency depending on gap dimensionality and structure",
        "Online parameter adaptation is effective when 1-10 key parameters can be adjusted based on real-time feedback, with diminishing returns beyond 10 parameters due to identifiability issues",
        "Memory-augmented policies (LSTM, Transformers) enable implicit online adaptation by learning to identify and compensate for dynamics differences, achieving 10-50% better performance than feedforward policies on tasks with systematic gaps",
        "Safe online adaptation requires either: (1) a protective policy that ensures safety during exploration, (2) a safe exploration region, (3) human supervision, or (4) off-policy learning from safe demonstrations",
        "Iterative simulator tuning (offline adaptation between deployment cycles) is more sample-efficient than pure online adaptation, requiring 3-20 real rollouts per iteration vs 50-1000 for online methods",
        "Online adaptation is most effective when combined with good sim-trained initialization: pure online learning from scratch requires 10-100× more samples",
        "The adaptation rate must exceed the rate of environment changes for online adaptation to be effective; non-stationary environments require continuous adaptation",
        "Adaptation is most effective for parametric gaps (wrong parameter values) and less effective for structural gaps (missing physics or wrong model class)",
        "One-shot adaptation methods can work when the gap is low-dimensional (1-3 parameters) and the adaptation model is pre-trained on diverse source variations",
        "Gated or selective adaptation (applying corrections only when needed) outperforms always-on adaptation by 10-30% in tasks with partial sim-to-real gaps"
    ],
    "new_predictions_likely": [
        "A manipulation task with systematic contact dynamics mismatch will achieve &gt;80% success after learning residuals from 50-100 real corrections, provided the gap is parametric",
        "A locomotion task will adapt to new terrain within 5-10 episodes using meta-learned adaptation policies trained on 20+ source terrains",
        "Online admittance adaptation will reduce contact forces by 30-50% within 10-20 task executions for contact-rich tasks with force feedback",
        "Memory-augmented policies will outperform feedforward policies by 20-40% on tasks with systematic sim-to-real gaps that can be inferred from observation history",
        "Iterative simulator tuning with 5-10 real rollouts per iteration will match the performance of policies trained with 1000+ real samples for tasks with 3-5 key parameters",
        "Gated residual policies will achieve 10-20% higher success than always-on residuals on tasks where the sim-to-real gap is spatially or temporally localized"
    ],
    "new_predictions_unknown": [
        "Whether online adaptation can handle non-stationary environments (changing dynamics, wear) or only static sim-to-real gaps, and what adaptation rate is required",
        "Whether learned adaptation strategies generalize across task types or must be task-specific, and what determines transferability",
        "Whether there exists a minimum adaptation rate required for stability, or if arbitrarily slow adaptation can eventually succeed without divergence",
        "Whether online adaptation can correct for structural model errors (wrong model class) or only parametric errors (wrong parameter values)",
        "Whether adaptation can handle multi-modal parameter spaces where multiple parameter settings produce similar behavior",
        "Whether there is a fundamental limit to the dimensionality of gaps that can be adapted online (e.g., can 50+ parameters be adapted simultaneously?)",
        "Whether adaptation interferes with task performance during the adaptation phase, and how to minimize this interference",
        "Whether human-in-the-loop adaptation scales to complex tasks or is limited to simple corrections"
    ],
    "negative_experiments": [
        "Finding a task where online adaptation without sim-trained initialization outperforms sim-to-real transfer would challenge the initialization principle",
        "Demonstrating successful adaptation with &lt;10 real examples for high-dimensional gaps (&gt;10 parameters) would challenge the sample-efficiency limits",
        "Showing that feedforward policies adapt as well as memory-augmented policies on tasks with systematic temporal gaps would challenge the memory principle",
        "Finding that unsafe exploration during adaptation does not degrade performance would challenge the safety requirement",
        "Demonstrating that always-on residuals outperform gated residuals would challenge the selective adaptation principle",
        "Finding that adaptation works equally well for structural and parametric gaps would challenge the gap-type distinction",
        "Showing that adaptation rate can be arbitrarily slow without stability issues would challenge the rate requirement"
    ],
    "unaccounted_for": [
        {
            "text": "How adaptation requirements change with the magnitude and type of sim-to-real gap (parametric vs structural, low-dimensional vs high-dimensional)",
            "uuids": [
                "e1650.0",
                "e1672.0",
                "e1648.0"
            ]
        },
        {
            "text": "Whether adaptation effectiveness depends on policy architecture (feedforward vs recurrent vs transformer) or learning algorithm (on-policy vs off-policy)",
            "uuids": [
                "e1651.0",
                "e1662.2",
                "e1641.0"
            ]
        },
        {
            "text": "How to determine when adaptation has converged and no further real-world data is needed, and whether convergence guarantees exist",
            "uuids": [
                "e1660.0",
                "e1641.0",
                "e1795.1"
            ]
        },
        {
            "text": "How to choose what to adapt (policy parameters, simulator parameters, controller parameters, or combinations) for a given task",
            "uuids": [
                "e1650.0",
                "e1647.1",
                "e1795.0"
            ]
        },
        {
            "text": "Computational requirements for online adaptation and whether real-time adaptation is feasible for high-frequency control tasks",
            "uuids": [
                "e1641.0",
                "e1631.0"
            ]
        },
        {
            "text": "Whether adaptation can handle multi-modal parameter spaces where multiple parameter settings produce similar behavior",
            "uuids": [
                "e1785.1",
                "e1785.2",
                "e1678.0"
            ]
        },
        {
            "text": "How adaptation interacts with domain randomization: whether they are complementary or redundant",
            "uuids": [
                "e1650.0",
                "e1651.0",
                "e1791.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some papers achieve transfer without online adaptation using system identification or domain randomization, suggesting adaptation is not always necessary",
            "uuids": [
                "e1654.0",
                "e1814.0",
                "e1780.0"
            ]
        },
        {
            "text": "Online fine-tuning sometimes degraded performance for first-order policies, suggesting adaptation can be harmful when the policy is not designed for adaptation",
            "uuids": [
                "e1641.0"
            ]
        },
        {
            "text": "Some methods required re-identification after initial failure rather than continuous adaptation, suggesting iterative offline tuning may be more reliable than online adaptation",
            "uuids": [
                "e1643.2",
                "e1657.2"
            ]
        },
        {
            "text": "System identification without adaptation achieved comparable or better results than adaptive methods in some cases, suggesting accurate modeling may be preferable to adaptation",
            "uuids": [
                "e1654.0",
                "e1654.2",
                "e1764.0"
            ]
        },
        {
            "text": "Some successful transfers used zero-shot deployment without any adaptation, suggesting good sim-trained policies may not need adaptation",
            "uuids": [
                "e1790.0",
                "e1768.0",
                "e1802.0"
            ]
        }
    ],
    "special_cases": [
        "For safety-critical tasks, adaptation must be performed with safety guarantees (protective policies, safe exploration regions) or human supervision to prevent catastrophic failures",
        "For tasks with catastrophic failures, adaptation must use safe exploration, off-policy learning, or iterative offline tuning rather than online exploration",
        "For tasks with long horizons, adaptation may need to be hierarchical (adapt high-level strategy, then low-level control) to avoid credit assignment issues",
        "For multi-task scenarios, adaptation may need to be task-specific or use meta-learning to generalize across tasks, with task-specific adaptation requiring 10-100× less data",
        "For high-frequency control tasks (&gt;100 Hz), online adaptation may not be computationally feasible and iterative offline tuning may be required",
        "For tasks with multi-modal parameter spaces, adaptation may converge to local optima that differ from ground truth but still achieve good performance",
        "For tasks with structural model errors (wrong model class), adaptation may fail and model redesign may be required",
        "For tasks with very high-dimensional gaps (&gt;20 parameters), adaptation may require prohibitive amounts of data and domain randomization may be more effective",
        "For contact-rich tasks, adaptation of compliance parameters (admittance, impedance) is often more effective than adapting the policy directly",
        "For tasks with systematic temporal patterns, memory-augmented policies enable implicit adaptation without explicit parameter updates"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Finn et al. (2017) Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [MAML meta-learning for fast adaptation]",
            "Nagabandi et al. (2019) Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning [Meta-RL for online adaptation]",
            "Clavera et al. (2018) Model-Based Reinforcement Learning via Meta-Policy Optimization [Model-based meta-learning for adaptation]",
            "Yu et al. (2020) Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning [Meta-learning benchmark for adaptation]",
            "Chebotar et al. (2019) Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience [SimOpt iterative adaptation]",
            "Muratore et al. (2022) Robot Learning from Randomized Simulations: A Review [Review of adaptation and domain randomization methods]",
            "Pinto et al. (2017) Robust Adversarial Reinforcement Learning [Adversarial training for robustness, related to adaptation]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 6,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
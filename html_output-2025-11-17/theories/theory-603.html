<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emergent Reasoning Thresholds and Modularization Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-603</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-603</p>
                <p><strong>Name:</strong> Emergent Reasoning Thresholds and Modularization Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that strict logical reasoning in language models is an emergent property that appears only above certain model scale and/or with explicit modularization of reasoning processes. Below these thresholds, LMs fail to perform multi-step or compositional reasoning, regardless of prompting. Modularization—decomposing reasoning into explicit steps (e.g., chain-of-thought, least-to-most, selection-inference, program synthesis, or external tool use)—enables models to surpass the limitations of monolithic, left-to-right generation. The interaction between model scale, modularization, and external symbolic augmentation determines the upper bound of logical reasoning performance. The theory is supported by extensive evidence across arithmetic, symbolic, and formal logic tasks, and is challenged by a small number of cases where scale or modularization alone appear sufficient.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Emergent Reasoning Threshold Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; has_parameter_count &#8594; N<span style="color: #888888;">, and</span></div>
        <div>&#8226; N &#8594; is_below &#8594; critical threshold N_c for task T</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; cannot_perform &#8594; multi-step logical reasoning on task T, regardless of prompting</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>CoT and self-consistency only improve performance above certain model sizes (e.g., 68B for LaMDA, 13B for GPT-3 on arithmetic, 62B for PaLM on StrategyQA). <a href="../results/extraction-result-5118.html#e5118.6" class="evidence-link">[e5118.6]</a> <a href="../results/extraction-result-5118.html#e5118.3" class="evidence-link">[e5118.3]</a> <a href="../results/extraction-result-5118.html#e5118.7" class="evidence-link">[e5118.7]</a> <a href="../results/extraction-result-5121.html#e5121.3" class="evidence-link">[e5121.3]</a> </li>
    <li>UL2-20B and smaller models show negligible CoT benefit on arithmetic and logic tasks. <a href="../results/extraction-result-4992.html#e4992.5" class="evidence-link">[e4992.5]</a> </li>
    <li>Smaller/mid-scale models (GPT-2, GPT-Neo, GPT-J, T0, OPT) show flat or minimal gains from CoT prompting; only large models show emergent reasoning. <a href="../results/extraction-result-5121.html#e5121.3" class="evidence-link">[e5121.3]</a> </li>
    <li>Scratchpad finetuning enables intermediate computation at smaller scales, but only for certain tasks (e.g., 8-digit addition at 40M params), not for general logical reasoning. <a href="../results/extraction-result-5118.html#e5118.4" class="evidence-link">[e5118.4]</a> </li>
    <li>Chain-of-thought prompting is ineffective or marginal on smaller models; gains require sufficient model capacity. <a href="../results/extraction-result-5121.html#e5121.3" class="evidence-link">[e5121.3]</a> <a href="../results/extraction-result-4992.html#e4992.5" class="evidence-link">[e4992.5]</a> </li>
    <li>On FOLIO and RobustLR, few-shot prompting with large LMs only slightly outperforms random, while fine-tuned medium-sized models can perform well, indicating that scale alone is not always sufficient without task-specific adaptation. <a href="../results/extraction-result-5004.html#e5004.2" class="evidence-link">[e5004.2]</a> <a href="../results/extraction-result-5095.html#e5095.4" class="evidence-link">[e5095.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law synthesizes scaling and modularization as joint determinants of reasoning emergence, extending prior work on emergent abilities.</p>            <p><strong>What Already Exists:</strong> Emergent abilities and scaling laws are established in LLM literature.</p>            <p><strong>What is Novel:</strong> This law formalizes the existence of task-specific reasoning thresholds and their dependence on both scale and modularization, and ties them to the inability of prompting alone to elicit reasoning below threshold.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Emergent Abilities of Large Language Models [emergence thresholds]</li>
    <li>Srivastava et al. (2022) Beyond the Imitation Game [scaling and emergence]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads [emergence of intermediate computation]</li>
</ul>
            <h3>Statement 1: Modularization Enables Reasoning Beyond Scale (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning process &#8594; is_decomposed_into &#8594; explicit modular steps (e.g., CoT, ToT, selection-inference, program synthesis, least-to-most, multi-agent debate, verification, external tool use)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; achieves &#8594; higher logical reasoning accuracy than monolithic generation at same scale</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Selection-Inference, ToT, PAL, PoT, program synthesis, and multi-agent debate methods outperform standard CoT and monolithic generation, even at smaller scales. <a href="../results/extraction-result-4947.html#e4947.1" class="evidence-link">[e4947.1]</a> <a href="../results/extraction-result-5088.html#e5088.0" class="evidence-link">[e5088.0]</a> <a href="../results/extraction-result-5115.html#e5115.0" class="evidence-link">[e5115.0]</a> <a href="../results/extraction-result-5085.html#e5085.1" class="evidence-link">[e5085.1]</a> <a href="../results/extraction-result-4933.html#e4933.0" class="evidence-link">[e4933.0]</a> </li>
    <li>Least-to-most prompting enables length generalization and compositional reasoning beyond what is possible with standard CoT. <a href="../results/extraction-result-5110.html#e5110.1" class="evidence-link">[e5110.1]</a> <a href="../results/extraction-result-5110.html#e5110.3" class="evidence-link">[e5110.3]</a> <a href="../results/extraction-result-5110.html#e5110.4" class="evidence-link">[e5110.4]</a> </li>
    <li>Tree-of-Thought (ToT) search enables lookahead, backtracking, and outperforms CoT and best-of-k sampling on combinatorial and planning tasks. <a href="../results/extraction-result-5088.html#e5088.0" class="evidence-link">[e5088.0]</a> <a href="../results/extraction-result-5088.html#e5088.1" class="evidence-link">[e5088.1]</a> <a href="../results/extraction-result-5088.html#e5088.4" class="evidence-link">[e5088.4]</a> </li>
    <li>Program-aided approaches (PAL, PoT) and external tool use (Toolformer, calculator API) enable models to achieve near-perfect or superhuman accuracy on arithmetic and logic tasks. <a href="../results/extraction-result-5115.html#e5115.0" class="evidence-link">[e5115.0]</a> <a href="../results/extraction-result-5085.html#e5085.1" class="evidence-link">[e5085.1]</a> <a href="../results/extraction-result-5089.html#e5089.1" class="evidence-link">[e5089.1]</a> </li>
    <li>Multi-agent debate and reflection pipelines yield higher accuracy than single-agent or single-path reasoning. <a href="../results/extraction-result-4933.html#e4933.0" class="evidence-link">[e4933.0]</a> <a href="../results/extraction-result-4933.html#e4933.3" class="evidence-link">[e4933.3]</a> </li>
    <li>Verifier-guided search (NLProofs, verification) and contrastive learning (ConDec) improve stepwise proof generation and reduce hallucinations compared to monolithic generation. <a href="../results/extraction-result-5016.html#e5016.0" class="evidence-link">[e5016.0]</a> <a href="../results/extraction-result-5016.html#e5016.9" class="evidence-link">[e5016.9]</a> <a href="../results/extraction-result-5097.html#e5097.2" class="evidence-link">[e5097.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is a synthesis and generalization of modular reasoning findings, extending them to a broad class of reasoning tasks.</p>            <p><strong>What Already Exists:</strong> Modular and stepwise reasoning methods are known to improve performance.</p>            <p><strong>What is Novel:</strong> The law asserts that modularization can compensate for lack of scale and is necessary for strict logical reasoning, and that modularization is a general principle across architectures and tasks.</p>
            <p><strong>References:</strong> <ul>
    <li>Yao et al. (2023) Tree of Thoughts [modular search]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads [intermediate computation]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [program synthesis]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning [ensemble modularization]</li>
</ul>
            <h3>Statement 2: Self-Consistency and Diversity Amplify Modularization Gains (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; modular reasoning method &#8594; is_combined_with &#8594; self-consistency or diverse sampling (multiple CoT samples, diverse rationales, multi-agent debate)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; final answer accuracy &#8594; increases &#8594; relative to single-path modularization</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Self-consistency and diverse reasoning (multiple CoT samples, diverse rationales, multi-agent debate) yield large gains over single-path CoT. <a href="../results/extraction-result-5114.html#e5114.2" class="evidence-link">[e5114.2]</a> <a href="../results/extraction-result-5094.html#e5094.4" class="evidence-link">[e5094.4]</a> <a href="../results/extraction-result-5118.html#e5118.3" class="evidence-link">[e5118.3]</a> <a href="../results/extraction-result-4933.html#e4933.0" class="evidence-link">[e4933.0]</a> </li>
    <li>Self-consistency improves chain-of-thought accuracy, especially at large scale (LaMDA 68B, code-davinci-002). <a href="../results/extraction-result-5118.html#e5118.3" class="evidence-link">[e5118.3]</a> <a href="../results/extraction-result-5114.html#e5114.2" class="evidence-link">[e5114.2]</a> </li>
    <li>Diverse reasoning (teacher-generated multiple CoT rationales) enables small models to surpass larger models without diversity. <a href="../results/extraction-result-5094.html#e5094.4" class="evidence-link">[e5094.4]</a> </li>
    <li>Multi-agent debate and reflection pipelines outperform single-agent and majority-vote baselines. <a href="../results/extraction-result-4933.html#e4933.0" class="evidence-link">[e4933.0]</a> <a href="../results/extraction-result-4933.html#e4933.3" class="evidence-link">[e4933.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is a generalization of known ensemble effects in the context of modular reasoning, extending to multi-agent and diverse rationale settings.</p>            <p><strong>What Already Exists:</strong> Self-consistency and ensemble methods are established.</p>            <p><strong>What is Novel:</strong> The law formalizes their role as amplifiers of modularization, especially for complex reasoning, and generalizes to multi-agent and diverse sampling settings.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning [self-consistency]</li>
    <li>Huang et al. (2022) Large Language Models are Reasoning Teachers [diverse reasoning]</li>
    <li>Du et al. (2023) Improving Factuality and Reasoning in Language Models through Multiagent Debate [multi-agent debate]</li>
</ul>
            <h3>Statement 3: External Tool Use and Program Synthesis Enable Superhuman Reasoning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_augmented_with &#8594; external tools (e.g., calculators, code execution, symbolic solvers, program synthesis, SAT solvers, symbolic chain-of-thought, logic solvers)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; system &#8594; achieves &#8594; higher accuracy than pure LMs or humans on strict logical tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>PAL, PoT, Toolformer, program synthesis, SATLM, and solver-augmented LMs achieve near-perfect or superhuman accuracy on arithmetic and logic tasks, surpassing human averages and pure LMs. <a href="../results/extraction-result-5115.html#e5115.0" class="evidence-link">[e5115.0]</a> <a href="../results/extraction-result-5085.html#e5085.1" class="evidence-link">[e5085.1]</a> <a href="../results/extraction-result-5089.html#e5089.1" class="evidence-link">[e5089.1]</a> <a href="../results/extraction-result-5103.html#e5103.1" class="evidence-link">[e5103.1]</a> <a href="../results/extraction-result-4974.html#e4974.6" class="evidence-link">[e4974.6]</a> <a href="../results/extraction-result-4988.html#e4988.1" class="evidence-link">[e4988.1]</a> <a href="../results/extraction-result-4988.html#e4988.3" class="evidence-link">[e4988.3]</a> <a href="../results/extraction-result-4953.html#e4953.0" class="evidence-link">[e4953.0]</a> </li>
    <li>Toolformer with calculator API outperforms much larger pure-LM baselines (OPT 66B, GPT-3 175B) on math benchmarks. <a href="../results/extraction-result-5089.html#e5089.1" class="evidence-link">[e5089.1]</a> </li>
    <li>Program synthesis (Codex, PoT, PAL) enables models to solve university-level math and MATH benchmarks at or above human level. <a href="../results/extraction-result-5082.html#e5082.1" class="evidence-link">[e5082.1]</a> <a href="../results/extraction-result-5115.html#e5115.0" class="evidence-link">[e5115.0]</a> <a href="../results/extraction-result-5085.html#e5085.1" class="evidence-link">[e5085.1]</a> </li>
    <li>Solver-augmented LMs (LogicLM, SymbCoT, LOGIC-LM, Prover9, Z3) achieve higher accuracy and faithfulness on FOL and proof tasks than pure LMs. <a href="../results/extraction-result-4974.html#e4974.6" class="evidence-link">[e4974.6]</a> <a href="../results/extraction-result-4988.html#e4988.1" class="evidence-link">[e4988.1]</a> <a href="../results/extraction-result-4988.html#e4988.3" class="evidence-link">[e4988.3]</a> <a href="../results/extraction-result-4953.html#e4953.0" class="evidence-link">[e4953.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is a synthesis and extension of tool-augmentation findings, generalizing across arithmetic, logic, and proof tasks.</p>            <p><strong>What Already Exists:</strong> Tool-augmented LMs and program synthesis are established.</p>            <p><strong>What is Novel:</strong> The law asserts that such augmentation is necessary for superhuman performance on strict logical tasks, and that tool use is a general principle for surpassing both human and pure-LM performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Schick et al. (2023) Toolformer [tool-augmented LMs]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [program synthesis]</li>
    <li>Yao et al. (2023) Tree of Thoughts [modular search with tool use]</li>
    <li>Zhou et al. (2023) Satisfiability-Aided Language Models [SATLM, declarative prompting]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Increasing model size beyond the emergent threshold for a given logical task will result in a sharp, non-linear increase in reasoning accuracy, especially when combined with modularization.</li>
                <li>Applying modularization (e.g., ToT, selection-inference, program synthesis) to smaller models will yield larger relative gains than further scaling without modularization.</li>
                <li>Combining modularization with self-consistency or diverse sampling will further improve accuracy, especially on tasks with multiple valid reasoning paths.</li>
                <li>Tool-augmented or program-synthesis methods will outperform pure LMs on new, harder logical reasoning benchmarks, even as model scale increases.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained with explicit modularization (e.g., via curriculum, architectural bias, or stepwise supervision), it may achieve emergent reasoning at smaller scales than current LMs.</li>
                <li>There may exist tasks for which no amount of scaling or modularization enables perfect reasoning without explicit symbolic augmentation (e.g., certain FOL or multi-hop proofs).</li>
                <li>If a model is trained end-to-end to both generate and verify reasoning steps (e.g., with a learned verifier or self-refinement), it may surpass current tool-augmented systems.</li>
                <li>Hybrid approaches that combine modularization, self-consistency, and tool use may yield new emergent abilities not present in any single method.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a small model (below the emergent threshold) achieves high accuracy on strict logical tasks with only modularization, this would challenge the necessity of scale.</li>
                <li>If a large model (above the threshold) fails to improve with modularization, this would challenge the theory.</li>
                <li>If tool-augmented or program-synthesis methods do not outperform pure LMs on strict logical tasks, this would call the theory into question.</li>
                <li>If self-consistency or diverse sampling fails to improve modularized reasoning accuracy, this would challenge the amplifying role of diversity.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks (e.g., simple arithmetic, shallow logic, or highly templated problems) may not exhibit clear emergent thresholds and can be solved by small models or with simple prompt engineering. <a href="../results/extraction-result-5118.html#e5118.4" class="evidence-link">[e5118.4]</a> <a href="../results/extraction-result-5110.html#e5110.1" class="evidence-link">[e5110.1]</a> <a href="../results/extraction-result-5110.html#e5110.5" class="evidence-link">[e5110.5]</a> </li>
    <li>Certain prompt engineering techniques (e.g., least-to-most, manual decomposition) can yield gains even below the expected threshold, suggesting a more nuanced interaction between prompt design and model capacity. <a href="../results/extraction-result-5110.html#e5110.1" class="evidence-link">[e5110.1]</a> <a href="../results/extraction-result-5110.html#e5110.3" class="evidence-link">[e5110.3]</a> <a href="../results/extraction-result-5110.html#e5110.4" class="evidence-link">[e5110.4]</a> </li>
    <li>Fine-tuned medium-sized models can outperform few-shot large LMs on some formal logic tasks (e.g., FOLIO), indicating that scale is not the only path to reasoning competence. <a href="../results/extraction-result-5004.html#e5004.2" class="evidence-link">[e5004.2]</a> <a href="../results/extraction-result-5095.html#e5095.4" class="evidence-link">[e5095.4]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and extends existing findings on emergence, modularization, and tool use, providing a unified framework for understanding strict logical reasoning in LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Emergent Abilities of Large Language Models [emergence thresholds]</li>
    <li>Yao et al. (2023) Tree of Thoughts [modular search]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning [self-consistency]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [program synthesis]</li>
    <li>Schick et al. (2023) Toolformer [tool-augmented LMs]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads [intermediate computation]</li>
    <li>Du et al. (2023) Improving Factuality and Reasoning in Language Models through Multiagent Debate [multi-agent debate]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Emergent Reasoning Thresholds and Modularization Theory",
    "theory_description": "This theory posits that strict logical reasoning in language models is an emergent property that appears only above certain model scale and/or with explicit modularization of reasoning processes. Below these thresholds, LMs fail to perform multi-step or compositional reasoning, regardless of prompting. Modularization—decomposing reasoning into explicit steps (e.g., chain-of-thought, least-to-most, selection-inference, program synthesis, or external tool use)—enables models to surpass the limitations of monolithic, left-to-right generation. The interaction between model scale, modularization, and external symbolic augmentation determines the upper bound of logical reasoning performance. The theory is supported by extensive evidence across arithmetic, symbolic, and formal logic tasks, and is challenged by a small number of cases where scale or modularization alone appear sufficient.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Emergent Reasoning Threshold Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "has_parameter_count",
                        "object": "N"
                    },
                    {
                        "subject": "N",
                        "relation": "is_below",
                        "object": "critical threshold N_c for task T"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "cannot_perform",
                        "object": "multi-step logical reasoning on task T, regardless of prompting"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "CoT and self-consistency only improve performance above certain model sizes (e.g., 68B for LaMDA, 13B for GPT-3 on arithmetic, 62B for PaLM on StrategyQA).",
                        "uuids": [
                            "e5118.6",
                            "e5118.3",
                            "e5118.7",
                            "e5121.3"
                        ]
                    },
                    {
                        "text": "UL2-20B and smaller models show negligible CoT benefit on arithmetic and logic tasks.",
                        "uuids": [
                            "e4992.5"
                        ]
                    },
                    {
                        "text": "Smaller/mid-scale models (GPT-2, GPT-Neo, GPT-J, T0, OPT) show flat or minimal gains from CoT prompting; only large models show emergent reasoning.",
                        "uuids": [
                            "e5121.3"
                        ]
                    },
                    {
                        "text": "Scratchpad finetuning enables intermediate computation at smaller scales, but only for certain tasks (e.g., 8-digit addition at 40M params), not for general logical reasoning.",
                        "uuids": [
                            "e5118.4"
                        ]
                    },
                    {
                        "text": "Chain-of-thought prompting is ineffective or marginal on smaller models; gains require sufficient model capacity.",
                        "uuids": [
                            "e5121.3",
                            "e4992.5"
                        ]
                    },
                    {
                        "text": "On FOLIO and RobustLR, few-shot prompting with large LMs only slightly outperforms random, while fine-tuned medium-sized models can perform well, indicating that scale alone is not always sufficient without task-specific adaptation.",
                        "uuids": [
                            "e5004.2",
                            "e5095.4"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Emergent abilities and scaling laws are established in LLM literature.",
                    "what_is_novel": "This law formalizes the existence of task-specific reasoning thresholds and their dependence on both scale and modularization, and ties them to the inability of prompting alone to elicit reasoning below threshold.",
                    "classification_explanation": "The law synthesizes scaling and modularization as joint determinants of reasoning emergence, extending prior work on emergent abilities.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Emergent Abilities of Large Language Models [emergence thresholds]",
                        "Srivastava et al. (2022) Beyond the Imitation Game [scaling and emergence]",
                        "Nye et al. (2021) Show Your Work: Scratchpads [emergence of intermediate computation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Modularization Enables Reasoning Beyond Scale",
                "if": [
                    {
                        "subject": "reasoning process",
                        "relation": "is_decomposed_into",
                        "object": "explicit modular steps (e.g., CoT, ToT, selection-inference, program synthesis, least-to-most, multi-agent debate, verification, external tool use)"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "achieves",
                        "object": "higher logical reasoning accuracy than monolithic generation at same scale"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Selection-Inference, ToT, PAL, PoT, program synthesis, and multi-agent debate methods outperform standard CoT and monolithic generation, even at smaller scales.",
                        "uuids": [
                            "e4947.1",
                            "e5088.0",
                            "e5115.0",
                            "e5085.1",
                            "e4933.0"
                        ]
                    },
                    {
                        "text": "Least-to-most prompting enables length generalization and compositional reasoning beyond what is possible with standard CoT.",
                        "uuids": [
                            "e5110.1",
                            "e5110.3",
                            "e5110.4"
                        ]
                    },
                    {
                        "text": "Tree-of-Thought (ToT) search enables lookahead, backtracking, and outperforms CoT and best-of-k sampling on combinatorial and planning tasks.",
                        "uuids": [
                            "e5088.0",
                            "e5088.1",
                            "e5088.4"
                        ]
                    },
                    {
                        "text": "Program-aided approaches (PAL, PoT) and external tool use (Toolformer, calculator API) enable models to achieve near-perfect or superhuman accuracy on arithmetic and logic tasks.",
                        "uuids": [
                            "e5115.0",
                            "e5085.1",
                            "e5089.1"
                        ]
                    },
                    {
                        "text": "Multi-agent debate and reflection pipelines yield higher accuracy than single-agent or single-path reasoning.",
                        "uuids": [
                            "e4933.0",
                            "e4933.3"
                        ]
                    },
                    {
                        "text": "Verifier-guided search (NLProofs, verification) and contrastive learning (ConDec) improve stepwise proof generation and reduce hallucinations compared to monolithic generation.",
                        "uuids": [
                            "e5016.0",
                            "e5016.9",
                            "e5097.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Modular and stepwise reasoning methods are known to improve performance.",
                    "what_is_novel": "The law asserts that modularization can compensate for lack of scale and is necessary for strict logical reasoning, and that modularization is a general principle across architectures and tasks.",
                    "classification_explanation": "The law is a synthesis and generalization of modular reasoning findings, extending them to a broad class of reasoning tasks.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Yao et al. (2023) Tree of Thoughts [modular search]",
                        "Nye et al. (2021) Show Your Work: Scratchpads [intermediate computation]",
                        "Gao et al. (2022) PAL: Program-aided Language Models [program synthesis]",
                        "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning [ensemble modularization]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Self-Consistency and Diversity Amplify Modularization Gains",
                "if": [
                    {
                        "subject": "modular reasoning method",
                        "relation": "is_combined_with",
                        "object": "self-consistency or diverse sampling (multiple CoT samples, diverse rationales, multi-agent debate)"
                    }
                ],
                "then": [
                    {
                        "subject": "final answer accuracy",
                        "relation": "increases",
                        "object": "relative to single-path modularization"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Self-consistency and diverse reasoning (multiple CoT samples, diverse rationales, multi-agent debate) yield large gains over single-path CoT.",
                        "uuids": [
                            "e5114.2",
                            "e5094.4",
                            "e5118.3",
                            "e4933.0"
                        ]
                    },
                    {
                        "text": "Self-consistency improves chain-of-thought accuracy, especially at large scale (LaMDA 68B, code-davinci-002).",
                        "uuids": [
                            "e5118.3",
                            "e5114.2"
                        ]
                    },
                    {
                        "text": "Diverse reasoning (teacher-generated multiple CoT rationales) enables small models to surpass larger models without diversity.",
                        "uuids": [
                            "e5094.4"
                        ]
                    },
                    {
                        "text": "Multi-agent debate and reflection pipelines outperform single-agent and majority-vote baselines.",
                        "uuids": [
                            "e4933.0",
                            "e4933.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Self-consistency and ensemble methods are established.",
                    "what_is_novel": "The law formalizes their role as amplifiers of modularization, especially for complex reasoning, and generalizes to multi-agent and diverse sampling settings.",
                    "classification_explanation": "The law is a generalization of known ensemble effects in the context of modular reasoning, extending to multi-agent and diverse rationale settings.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning [self-consistency]",
                        "Huang et al. (2022) Large Language Models are Reasoning Teachers [diverse reasoning]",
                        "Du et al. (2023) Improving Factuality and Reasoning in Language Models through Multiagent Debate [multi-agent debate]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "External Tool Use and Program Synthesis Enable Superhuman Reasoning",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_augmented_with",
                        "object": "external tools (e.g., calculators, code execution, symbolic solvers, program synthesis, SAT solvers, symbolic chain-of-thought, logic solvers)"
                    }
                ],
                "then": [
                    {
                        "subject": "system",
                        "relation": "achieves",
                        "object": "higher accuracy than pure LMs or humans on strict logical tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "PAL, PoT, Toolformer, program synthesis, SATLM, and solver-augmented LMs achieve near-perfect or superhuman accuracy on arithmetic and logic tasks, surpassing human averages and pure LMs.",
                        "uuids": [
                            "e5115.0",
                            "e5085.1",
                            "e5089.1",
                            "e5103.1",
                            "e4974.6",
                            "e4988.1",
                            "e4988.3",
                            "e4953.0"
                        ]
                    },
                    {
                        "text": "Toolformer with calculator API outperforms much larger pure-LM baselines (OPT 66B, GPT-3 175B) on math benchmarks.",
                        "uuids": [
                            "e5089.1"
                        ]
                    },
                    {
                        "text": "Program synthesis (Codex, PoT, PAL) enables models to solve university-level math and MATH benchmarks at or above human level.",
                        "uuids": [
                            "e5082.1",
                            "e5115.0",
                            "e5085.1"
                        ]
                    },
                    {
                        "text": "Solver-augmented LMs (LogicLM, SymbCoT, LOGIC-LM, Prover9, Z3) achieve higher accuracy and faithfulness on FOL and proof tasks than pure LMs.",
                        "uuids": [
                            "e4974.6",
                            "e4988.1",
                            "e4988.3",
                            "e4953.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Tool-augmented LMs and program synthesis are established.",
                    "what_is_novel": "The law asserts that such augmentation is necessary for superhuman performance on strict logical tasks, and that tool use is a general principle for surpassing both human and pure-LM performance.",
                    "classification_explanation": "The law is a synthesis and extension of tool-augmentation findings, generalizing across arithmetic, logic, and proof tasks.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Schick et al. (2023) Toolformer [tool-augmented LMs]",
                        "Gao et al. (2022) PAL: Program-aided Language Models [program synthesis]",
                        "Yao et al. (2023) Tree of Thoughts [modular search with tool use]",
                        "Zhou et al. (2023) Satisfiability-Aided Language Models [SATLM, declarative prompting]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Increasing model size beyond the emergent threshold for a given logical task will result in a sharp, non-linear increase in reasoning accuracy, especially when combined with modularization.",
        "Applying modularization (e.g., ToT, selection-inference, program synthesis) to smaller models will yield larger relative gains than further scaling without modularization.",
        "Combining modularization with self-consistency or diverse sampling will further improve accuracy, especially on tasks with multiple valid reasoning paths.",
        "Tool-augmented or program-synthesis methods will outperform pure LMs on new, harder logical reasoning benchmarks, even as model scale increases."
    ],
    "new_predictions_unknown": [
        "If a model is trained with explicit modularization (e.g., via curriculum, architectural bias, or stepwise supervision), it may achieve emergent reasoning at smaller scales than current LMs.",
        "There may exist tasks for which no amount of scaling or modularization enables perfect reasoning without explicit symbolic augmentation (e.g., certain FOL or multi-hop proofs).",
        "If a model is trained end-to-end to both generate and verify reasoning steps (e.g., with a learned verifier or self-refinement), it may surpass current tool-augmented systems.",
        "Hybrid approaches that combine modularization, self-consistency, and tool use may yield new emergent abilities not present in any single method."
    ],
    "negative_experiments": [
        "If a small model (below the emergent threshold) achieves high accuracy on strict logical tasks with only modularization, this would challenge the necessity of scale.",
        "If a large model (above the threshold) fails to improve with modularization, this would challenge the theory.",
        "If tool-augmented or program-synthesis methods do not outperform pure LMs on strict logical tasks, this would call the theory into question.",
        "If self-consistency or diverse sampling fails to improve modularized reasoning accuracy, this would challenge the amplifying role of diversity."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks (e.g., simple arithmetic, shallow logic, or highly templated problems) may not exhibit clear emergent thresholds and can be solved by small models or with simple prompt engineering.",
            "uuids": [
                "e5118.4",
                "e5110.1",
                "e5110.5"
            ]
        },
        {
            "text": "Certain prompt engineering techniques (e.g., least-to-most, manual decomposition) can yield gains even below the expected threshold, suggesting a more nuanced interaction between prompt design and model capacity.",
            "uuids": [
                "e5110.1",
                "e5110.3",
                "e5110.4"
            ]
        },
        {
            "text": "Fine-tuned medium-sized models can outperform few-shot large LMs on some formal logic tasks (e.g., FOLIO), indicating that scale is not the only path to reasoning competence.",
            "uuids": [
                "e5004.2",
                "e5095.4"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "GPT-4 and Gemini-Pro achieve high accuracy on some logic tasks with only NL input, suggesting that scale alone may suffice in some cases, especially for tasks with less compositional depth or more regular structure.",
            "uuids": [
                "e4993.2",
                "e5001.2",
                "e5090.1"
            ]
        },
        {
            "text": "Some open-source models (e.g., Mistral-7B, Llama3-70B) achieve strong performance on certain logic benchmarks with only few-shot or zero-shot prompting, though still below top proprietary models.",
            "uuids": [
                "e4993.3",
                "e5001.5"
            ]
        }
    ],
    "special_cases": [
        "Tasks with highly regular or templated NL may not require modularization or scale for high performance.",
        "For tasks with a single valid reasoning path, self-consistency may not yield additional gains.",
        "Fine-tuning on task-specific data can enable smaller models to perform well on narrow domains, even without modularization.",
        "Certain symbolic manipulation tasks (e.g., last-letter concatenation) benefit more from explicit decomposition (least-to-most) than from scale or CoT alone."
    ],
    "existing_theory": {
        "what_already_exists": "Emergent abilities, scaling laws, and modular reasoning methods are established in the literature. Tool-augmented and program-synthesis approaches are also well-studied.",
        "what_is_novel": "The explicit joint dependence on scale and modularization, and the prediction of sharp thresholds for reasoning emergence, is novel. The theory unifies modularization, scale, and tool use as interacting determinants of strict logical reasoning.",
        "classification_explanation": "The theory synthesizes and extends existing findings on emergence, modularization, and tool use, providing a unified framework for understanding strict logical reasoning in LMs.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Emergent Abilities of Large Language Models [emergence thresholds]",
            "Yao et al. (2023) Tree of Thoughts [modular search]",
            "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning [self-consistency]",
            "Gao et al. (2022) PAL: Program-aided Language Models [program synthesis]",
            "Schick et al. (2023) Toolformer [tool-augmented LMs]",
            "Nye et al. (2021) Show Your Work: Scratchpads [intermediate computation]",
            "Du et al. (2023) Improving Factuality and Reasoning in Language Models through Multiagent Debate [multi-agent debate]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
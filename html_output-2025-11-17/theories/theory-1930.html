<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1930</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1930</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the format in which a problem is presented to a large language model (LLM) acts as an information bottleneck, modulating the amount and accessibility of solution-relevant information. Formats that maximize the mutual information between the input and the solution-relevant features enable the LLM to perform optimally, while formats that obscure or compress key information reduce effective context and degrade performance. The theory predicts that optimal formats are those that structure, highlight, or otherwise foreground the information most relevant to the solution.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contextual Information Maximization Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; mutual_information_with_solution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_maximized &#8594; on_given_task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better when relevant information is foregrounded (e.g., via bullet points, tables, or explicit cues). </li>
    <li>Performance drops when key information is buried or presented in a convoluted manner. </li>
    <li>Prompt engineering studies show that explicit structuring of input (e.g., separating instructions and context) improves LLM accuracy. </li>
    <li>Information bottleneck theory in neural networks demonstrates that maximizing relevant information flow improves downstream task performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While inspired by information theory, the law is novel in its application to LLM prompt design and the explicit link between mutual information in presentation and LLM task performance.</p>            <p><strong>What Already Exists:</strong> Information bottleneck principles are established in information theory and have been applied to neural network internals.</p>            <p><strong>What is Novel:</strong> This law applies the information bottleneck concept specifically to the interface between problem format and LLM inference, rather than to model internals.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby et al. (2000) The Information Bottleneck Method [General information bottleneck principle]</li>
    <li>Sanh et al. (2022) Multitask Prompted Training Enables Zero-Shot Task Generalization [Prompt structure and information accessibility]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and performance]</li>
</ul>
            <h3>Statement 1: Obfuscation Penalty Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; obscures &#8594; solution-relevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_decreased &#8594; on_given_task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs make more errors when key details are hidden in long or complex text. </li>
    <li>Performance is lower on tasks with distractors or irrelevant information interleaved with relevant content. </li>
    <li>Studies show that LLMs are sensitive to the placement of relevant information within the context window, with information 'lost in the middle' leading to performance drops. </li>
    <li>Experiments with adversarial prompts demonstrate that obfuscation or misleading context reduces LLM accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law extends known distractor/context effects into a formal, predictive relationship between obfuscation and performance.</p>            <p><strong>What Already Exists:</strong> Distractor effects and context window limitations are known in LLMs.</p>            <p><strong>What is Novel:</strong> This law formalizes the effect as a penalty proportional to the degree of obfuscation, not just as a qualitative observation.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Context window and information loss]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Distractor and information accessibility effects]</li>
    <li>Zhou et al. (2023) LLMs are Easily Distracted by Irrelevant Context [Distractor effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Presenting a problem as a structured table with all relevant variables highlighted will improve LLM accuracy compared to a narrative format with the same information.</li>
                <li>Inserting irrelevant information between key details will reduce LLM performance, even if the total context length is unchanged.</li>
                <li>Explicitly separating instructions from context in the prompt will increase LLM task accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a format is designed to maximize mutual information but is highly unnatural (e.g., using artificial symbols), will LLMs still perform better?</li>
                <li>Can LLMs learn to ignore obfuscation penalties with sufficient fine-tuning, or is the bottleneck effect intrinsic?</li>
                <li>Does the optimal format for maximizing mutual information differ across LLM architectures or training regimes?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on obfuscated and non-obfuscated formats, the theory is falsified.</li>
                <li>If maximizing mutual information does not improve performance, the theory is challenged.</li>
                <li>If LLMs show no performance drop when key information is buried or interleaved with distractors, the theory is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs use world knowledge to recover obscured information and still perform well. </li>
    <li>Instances where LLMs perform well on highly compressed or symbolic formats due to pretraining exposure. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory is inspired by information theory but is novel in its application to LLM prompt design and performance, with explicit predictive laws.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby et al. (2000) The Information Bottleneck Method [General information bottleneck principle]</li>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Context window and information loss]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Distractor and information accessibility effects]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that the format in which a problem is presented to a large language model (LLM) acts as an information bottleneck, modulating the amount and accessibility of solution-relevant information. Formats that maximize the mutual information between the input and the solution-relevant features enable the LLM to perform optimally, while formats that obscure or compress key information reduce effective context and degrade performance. The theory predicts that optimal formats are those that structure, highlight, or otherwise foreground the information most relevant to the solution.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contextual Information Maximization Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "mutual_information_with_solution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_maximized",
                        "object": "on_given_task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better when relevant information is foregrounded (e.g., via bullet points, tables, or explicit cues).",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops when key information is buried or presented in a convoluted manner.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering studies show that explicit structuring of input (e.g., separating instructions and context) improves LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Information bottleneck theory in neural networks demonstrates that maximizing relevant information flow improves downstream task performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Information bottleneck principles are established in information theory and have been applied to neural network internals.",
                    "what_is_novel": "This law applies the information bottleneck concept specifically to the interface between problem format and LLM inference, rather than to model internals.",
                    "classification_explanation": "While inspired by information theory, the law is novel in its application to LLM prompt design and the explicit link between mutual information in presentation and LLM task performance.",
                    "likely_classification": "new",
                    "references": [
                        "Tishby et al. (2000) The Information Bottleneck Method [General information bottleneck principle]",
                        "Sanh et al. (2022) Multitask Prompted Training Enables Zero-Shot Task Generalization [Prompt structure and information accessibility]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Obfuscation Penalty Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "obscures",
                        "object": "solution-relevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_decreased",
                        "object": "on_given_task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs make more errors when key details are hidden in long or complex text.",
                        "uuids": []
                    },
                    {
                        "text": "Performance is lower on tasks with distractors or irrelevant information interleaved with relevant content.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that LLMs are sensitive to the placement of relevant information within the context window, with information 'lost in the middle' leading to performance drops.",
                        "uuids": []
                    },
                    {
                        "text": "Experiments with adversarial prompts demonstrate that obfuscation or misleading context reduces LLM accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Distractor effects and context window limitations are known in LLMs.",
                    "what_is_novel": "This law formalizes the effect as a penalty proportional to the degree of obfuscation, not just as a qualitative observation.",
                    "classification_explanation": "The law extends known distractor/context effects into a formal, predictive relationship between obfuscation and performance.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Context window and information loss]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Distractor and information accessibility effects]",
                        "Zhou et al. (2023) LLMs are Easily Distracted by Irrelevant Context [Distractor effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Presenting a problem as a structured table with all relevant variables highlighted will improve LLM accuracy compared to a narrative format with the same information.",
        "Inserting irrelevant information between key details will reduce LLM performance, even if the total context length is unchanged.",
        "Explicitly separating instructions from context in the prompt will increase LLM task accuracy."
    ],
    "new_predictions_unknown": [
        "If a format is designed to maximize mutual information but is highly unnatural (e.g., using artificial symbols), will LLMs still perform better?",
        "Can LLMs learn to ignore obfuscation penalties with sufficient fine-tuning, or is the bottleneck effect intrinsic?",
        "Does the optimal format for maximizing mutual information differ across LLM architectures or training regimes?"
    ],
    "negative_experiments": [
        "If LLMs perform equally well on obfuscated and non-obfuscated formats, the theory is falsified.",
        "If maximizing mutual information does not improve performance, the theory is challenged.",
        "If LLMs show no performance drop when key information is buried or interleaved with distractors, the theory is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs use world knowledge to recover obscured information and still perform well.",
            "uuids": []
        },
        {
            "text": "Instances where LLMs perform well on highly compressed or symbolic formats due to pretraining exposure.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robustness to distractors in certain tasks, suggesting partial immunity to obfuscation.",
            "uuids": []
        },
        {
            "text": "In some cases, LLMs can infer missing information from context or prior knowledge, mitigating the bottleneck effect.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with extremely short context may not exhibit bottleneck effects.",
        "LLMs with very large context windows may be less sensitive to obfuscation.",
        "Highly familiar or formulaic tasks may be less affected by presentation format."
    ],
    "existing_theory": {
        "what_already_exists": "Information bottleneck and context window effects are known in neural networks and LLMs.",
        "what_is_novel": "The explicit application of information bottleneck theory to problem presentation format and prompt design is new.",
        "classification_explanation": "The theory is inspired by information theory but is novel in its application to LLM prompt design and performance, with explicit predictive laws.",
        "likely_classification": "new",
        "references": [
            "Tishby et al. (2000) The Information Bottleneck Method [General information bottleneck principle]",
            "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Context window and information loss]",
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Distractor and information accessibility effects]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and performance]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-655",
    "original_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
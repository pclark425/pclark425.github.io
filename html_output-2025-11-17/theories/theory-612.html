<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Inductive Bias and Modality Adaptation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-612</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-612</p>
                <p><strong>Name:</strong> Structural Inductive Bias and Modality Adaptation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that the effectiveness of graph-to-text representations for LLM training is governed by the degree to which the representation preserves and exposes structural inductive biases (e.g., explicit connectivity, motifs, positional encodings) and adapts the modality (text, image, motif, soft-token) to the task and graph properties. It claims that representations which encode explicit structure (e.g., adjacency, Levi, motif, or path-label encodings) and adapt the modality to the information density and reasoning requirements of the task (e.g., using image for large graphs, motif for high-motif graphs, text for attribute-rich graphs) will yield superior performance and generalization. The theory further posits that the optimal representation is not static but should be dynamically selected or composed based on graph size, density, and task type.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structural Inductive Bias Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; preserves &#8594; explicit structural inductive biases (e.g., adjacency, motif, Levi, path-label, positional encodings)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves &#8594; higher accuracy and robustness on structure-sensitive tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GraphToken (learned soft-token encoding) and GNP (graph neural prompt) approaches that encode explicit structure outperform text-only or attribute-only baselines on graph reasoning tasks. <a href="../results/extraction-result-5237.html#e5237.3" class="evidence-link">[e5237.3]</a> <a href="../results/extraction-result-5349.html#e5349.0" class="evidence-link">[e5349.0]</a> </li>
    <li>Adjacency list, motif, and Levi graph encodings preserve explicit connectivity and outperform plain text or edge-list encodings on node classification and multi-hop reasoning. <a href="../results/extraction-result-5365.html#e5365.1" class="evidence-link">[e5365.1]</a> <a href="../results/extraction-result-5357.html#e5357.1" class="evidence-link">[e5357.1]</a> <a href="../results/extraction-result-5239.html#e5239.1" class="evidence-link">[e5239.1]</a> </li>
    <li>Path-label encodings and structure-aware self-attention (r_ij) in AMR-to-text generation improve BLEU and semantic fidelity over vanilla Transformer baselines. <a href="../results/extraction-result-5354.html#e5354.3" class="evidence-link">[e5354.3]</a> <a href="../results/extraction-result-5354.html#e5354.4" class="evidence-link">[e5354.4]</a> </li>
    <li>Relative position encoding (Shaw/Raffel) and attention matrices that encode graph distances or connectivity improve performance in graph-to-text generation. <a href="../results/extraction-result-5368.html#e5368.5" class="evidence-link">[e5368.5]</a> <a href="../results/extraction-result-5380.html#e5380.6" class="evidence-link">[e5380.6]</a> </li>
    <li>Graph Conv (Deep GCN encoder), GAT, and GIN-based encoders that preserve local and global structure outperform sequence-only baselines on graph-to-text and node classification. <a href="../results/extraction-result-5371.html#e5371.1" class="evidence-link">[e5371.1]</a> <a href="../results/extraction-result-5360.html#e5360.3" class="evidence-link">[e5360.3]</a> <a href="../results/extraction-result-5376.html#e5376.5" class="evidence-link">[e5376.5]</a> </li>
    <li>GCN-based planners and dual-encoder architectures that explicitly model graph structure generalize better to unseen domains than sequence planners. <a href="../results/extraction-result-5372.html#e5372.2" class="evidence-link">[e5372.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While structural bias is known in GNNs, its explicit necessity in graph-to-text LLM representations is a novel generalization.</p>            <p><strong>What Already Exists:</strong> The importance of structural inductive bias is well-established in GNNs and graph learning.</p>            <p><strong>What is Novel:</strong> This law extends the principle to graph-to-text representations for LLMs, asserting that explicit preservation of structure in the text or embedding space is necessary for robust LLM graph reasoning.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [GIN, structural bias]</li>
    <li>Guo et al. (2019) Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning [Levi, GCN]</li>
    <li>Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [motif, adjacency, structure]</li>
</ul>
            <h3>Statement 1: Modality Adaptation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; adapts &#8594; modality (text, image, motif, soft-token) to graph size, density, and task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; achieves &#8594; higher performance and scalability across diverse graph tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GraphTMI shows that image modality outperforms text and motif encodings for large graphs, while motif and text are better for smaller or motif-rich graphs. <a href="../results/extraction-result-5252.html#e5252.8" class="evidence-link">[e5252.8]</a> </li>
    <li>Motif encodings are most effective on 'hard' graphs (high motif count/low homophily), while text modality is best on easy/medium tasks. <a href="../results/extraction-result-5357.html#e5357.1" class="evidence-link">[e5357.1]</a> </li>
    <li>Textualized Graph (flattening) is only practical for small subgraphs or when paired with retrieval; otherwise, token limits are exceeded. <a href="../results/extraction-result-5257.html#e5257.0" class="evidence-link">[e5257.0]</a> </li>
    <li>Prefix-tuning and soft-prompt methods are effective for small graphs but do not scale to large or complex graphs. <a href="../results/extraction-result-5237.html#e5237.2" class="evidence-link">[e5237.2]</a> <a href="../results/extraction-result-5230.html#e5230.3" class="evidence-link">[e5230.3]</a> </li>
    <li>Ego-graph summarization and k-hop subgraph sampling are used to adapt the representation to fit within LLM context windows for large graphs. <a href="../results/extraction-result-5367.html#e5367.2" class="evidence-link">[e5367.2]</a> <a href="../results/extraction-result-5243.html#e5243.5" class="evidence-link">[e5243.5]</a> </li>
    <li>GLaM-Combined encoding (node descriptors + adjacency + summarization) adapts the representation to the domain and outperforms single-modality encodings. <a href="../results/extraction-result-5356.html#e5356.5" class="evidence-link">[e5356.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While modality adaptation is known in multimodal learning, its explicit application to graph-to-text LLM representations and the need for dynamic selection is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Modality adaptation is discussed in multimodal learning and some graph-LLM work.</p>            <p><strong>What is Novel:</strong> This law formalizes the need for dynamic modality selection or composition based on graph/task properties for optimal LLM performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [modality adaptation]</li>
    <li>Zhang et al. (2024) GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment [composite encodings]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For large, dense graphs, image-based or motif-based encodings will outperform text-only or edge-list encodings in LLM-based graph reasoning tasks.</li>
                <li>For attribute-rich graphs with small subgraphs, text-based or node-descriptor-based encodings will yield the highest accuracy.</li>
                <li>Dynamic selection of encoding modality (e.g., switching from text to image as graph size increases) will improve LLM performance over any static encoding.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>A hybrid system that automatically selects or composes modalities per instance (e.g., image+motif+text) will outperform all current static or single-modality approaches.</li>
                <li>For extremely large graphs (>10,000 nodes), motif+retrieval+image encodings will allow LLMs to perform reasoning tasks that are currently infeasible.</li>
                <li>If LLMs are trained with explicit structure-preserving encodings (e.g., Levi, path-label, motif) and dynamic modality adaptation, they will generalize to unseen graph types and tasks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a static, single-modality encoding (e.g., text-only) consistently outperforms dynamic or hybrid modality approaches across all graph sizes and tasks, the modality adaptation law would be challenged.</li>
                <li>If removing explicit structural inductive bias (e.g., using only attribute text) does not degrade performance on structure-sensitive tasks, the structural inductive bias law would be weakened.</li>
                <li>If image-based encodings do not outperform text or motif encodings on large graphs, the modality adaptation law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Retrieval-augmented approaches (e.g., G-Retriever) that combine retrieval and textualization achieve large gains, which may not fit neatly into the modality adaptation framework. <a href="../results/extraction-result-5257.html#e5257.0" class="evidence-link">[e5257.0]</a> </li>
    <li>Some soft-prompt and prefix-tuning methods achieve competitive results on small graphs without explicit structure or modality adaptation. <a href="../results/extraction-result-5237.html#e5237.2" class="evidence-link">[e5237.2]</a> <a href="../results/extraction-result-5230.html#e5230.3" class="evidence-link">[e5230.3]</a> </li>
    <li>Attribute-only or text-only encodings (e.g., LM baselines) can outperform structure-aware encodings for attribute-rich node classification. <a href="../results/extraction-result-5253.html#e5253.3" class="evidence-link">[e5253.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes and extends known principles from GNNs and multimodal learning to the specific context of graph-to-text LLM representations.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [GIN, structural bias]</li>
    <li>Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [modality adaptation]</li>
    <li>Zhang et al. (2024) GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment [composite encodings]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structural Inductive Bias and Modality Adaptation Theory",
    "theory_description": "This theory asserts that the effectiveness of graph-to-text representations for LLM training is governed by the degree to which the representation preserves and exposes structural inductive biases (e.g., explicit connectivity, motifs, positional encodings) and adapts the modality (text, image, motif, soft-token) to the task and graph properties. It claims that representations which encode explicit structure (e.g., adjacency, Levi, motif, or path-label encodings) and adapt the modality to the information density and reasoning requirements of the task (e.g., using image for large graphs, motif for high-motif graphs, text for attribute-rich graphs) will yield superior performance and generalization. The theory further posits that the optimal representation is not static but should be dynamically selected or composed based on graph size, density, and task type.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structural Inductive Bias Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "preserves",
                        "object": "explicit structural inductive biases (e.g., adjacency, motif, Levi, path-label, positional encodings)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves",
                        "object": "higher accuracy and robustness on structure-sensitive tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GraphToken (learned soft-token encoding) and GNP (graph neural prompt) approaches that encode explicit structure outperform text-only or attribute-only baselines on graph reasoning tasks.",
                        "uuids": [
                            "e5237.3",
                            "e5349.0"
                        ]
                    },
                    {
                        "text": "Adjacency list, motif, and Levi graph encodings preserve explicit connectivity and outperform plain text or edge-list encodings on node classification and multi-hop reasoning.",
                        "uuids": [
                            "e5365.1",
                            "e5357.1",
                            "e5239.1"
                        ]
                    },
                    {
                        "text": "Path-label encodings and structure-aware self-attention (r_ij) in AMR-to-text generation improve BLEU and semantic fidelity over vanilla Transformer baselines.",
                        "uuids": [
                            "e5354.3",
                            "e5354.4"
                        ]
                    },
                    {
                        "text": "Relative position encoding (Shaw/Raffel) and attention matrices that encode graph distances or connectivity improve performance in graph-to-text generation.",
                        "uuids": [
                            "e5368.5",
                            "e5380.6"
                        ]
                    },
                    {
                        "text": "Graph Conv (Deep GCN encoder), GAT, and GIN-based encoders that preserve local and global structure outperform sequence-only baselines on graph-to-text and node classification.",
                        "uuids": [
                            "e5371.1",
                            "e5360.3",
                            "e5376.5"
                        ]
                    },
                    {
                        "text": "GCN-based planners and dual-encoder architectures that explicitly model graph structure generalize better to unseen domains than sequence planners.",
                        "uuids": [
                            "e5372.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The importance of structural inductive bias is well-established in GNNs and graph learning.",
                    "what_is_novel": "This law extends the principle to graph-to-text representations for LLMs, asserting that explicit preservation of structure in the text or embedding space is necessary for robust LLM graph reasoning.",
                    "classification_explanation": "While structural bias is known in GNNs, its explicit necessity in graph-to-text LLM representations is a novel generalization.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [GIN, structural bias]",
                        "Guo et al. (2019) Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning [Levi, GCN]",
                        "Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [motif, adjacency, structure]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Modality Adaptation Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "adapts",
                        "object": "modality (text, image, motif, soft-token) to graph size, density, and task"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "achieves",
                        "object": "higher performance and scalability across diverse graph tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GraphTMI shows that image modality outperforms text and motif encodings for large graphs, while motif and text are better for smaller or motif-rich graphs.",
                        "uuids": [
                            "e5252.8"
                        ]
                    },
                    {
                        "text": "Motif encodings are most effective on 'hard' graphs (high motif count/low homophily), while text modality is best on easy/medium tasks.",
                        "uuids": [
                            "e5357.1"
                        ]
                    },
                    {
                        "text": "Textualized Graph (flattening) is only practical for small subgraphs or when paired with retrieval; otherwise, token limits are exceeded.",
                        "uuids": [
                            "e5257.0"
                        ]
                    },
                    {
                        "text": "Prefix-tuning and soft-prompt methods are effective for small graphs but do not scale to large or complex graphs.",
                        "uuids": [
                            "e5237.2",
                            "e5230.3"
                        ]
                    },
                    {
                        "text": "Ego-graph summarization and k-hop subgraph sampling are used to adapt the representation to fit within LLM context windows for large graphs.",
                        "uuids": [
                            "e5367.2",
                            "e5243.5"
                        ]
                    },
                    {
                        "text": "GLaM-Combined encoding (node descriptors + adjacency + summarization) adapts the representation to the domain and outperforms single-modality encodings.",
                        "uuids": [
                            "e5356.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Modality adaptation is discussed in multimodal learning and some graph-LLM work.",
                    "what_is_novel": "This law formalizes the need for dynamic modality selection or composition based on graph/task properties for optimal LLM performance.",
                    "classification_explanation": "While modality adaptation is known in multimodal learning, its explicit application to graph-to-text LLM representations and the need for dynamic selection is a novel synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [modality adaptation]",
                        "Zhang et al. (2024) GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment [composite encodings]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "For large, dense graphs, image-based or motif-based encodings will outperform text-only or edge-list encodings in LLM-based graph reasoning tasks.",
        "For attribute-rich graphs with small subgraphs, text-based or node-descriptor-based encodings will yield the highest accuracy.",
        "Dynamic selection of encoding modality (e.g., switching from text to image as graph size increases) will improve LLM performance over any static encoding."
    ],
    "new_predictions_unknown": [
        "A hybrid system that automatically selects or composes modalities per instance (e.g., image+motif+text) will outperform all current static or single-modality approaches.",
        "For extremely large graphs (&gt;10,000 nodes), motif+retrieval+image encodings will allow LLMs to perform reasoning tasks that are currently infeasible.",
        "If LLMs are trained with explicit structure-preserving encodings (e.g., Levi, path-label, motif) and dynamic modality adaptation, they will generalize to unseen graph types and tasks."
    ],
    "negative_experiments": [
        "If a static, single-modality encoding (e.g., text-only) consistently outperforms dynamic or hybrid modality approaches across all graph sizes and tasks, the modality adaptation law would be challenged.",
        "If removing explicit structural inductive bias (e.g., using only attribute text) does not degrade performance on structure-sensitive tasks, the structural inductive bias law would be weakened.",
        "If image-based encodings do not outperform text or motif encodings on large graphs, the modality adaptation law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Retrieval-augmented approaches (e.g., G-Retriever) that combine retrieval and textualization achieve large gains, which may not fit neatly into the modality adaptation framework.",
            "uuids": [
                "e5257.0"
            ]
        },
        {
            "text": "Some soft-prompt and prefix-tuning methods achieve competitive results on small graphs without explicit structure or modality adaptation.",
            "uuids": [
                "e5237.2",
                "e5230.3"
            ]
        },
        {
            "text": "Attribute-only or text-only encodings (e.g., LM baselines) can outperform structure-aware encodings for attribute-rich node classification.",
            "uuids": [
                "e5253.3"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In PubMed, adding 2-hop neighbor summaries (ego-graph summarization) decreased zero-shot node classification accuracy, showing that more structure is not always better.",
            "uuids": [
                "e5367.2"
            ]
        },
        {
            "text": "Motif encodings have higher mismatch rates in some datasets, indicating that explicit structure does not always guarantee improved performance.",
            "uuids": [
                "e5357.1"
            ]
        }
    ],
    "special_cases": [
        "For graphs with little or no structure (e.g., attribute-only graphs), structural inductive bias is unnecessary and may be detrimental.",
        "In highly attribute-rich graphs, text-only or attribute-only encodings may suffice and outperform structure-aware methods.",
        "For very small graphs, the overhead of modality adaptation or explicit structure may not be justified."
    ],
    "existing_theory": {
        "what_already_exists": "Structural inductive bias is well-established in GNNs and graph learning; modality adaptation is discussed in multimodal learning.",
        "what_is_novel": "The explicit extension of these principles to graph-to-text LLM representations and the need for dynamic, task- and graph-adaptive modality selection is novel.",
        "classification_explanation": "This theory synthesizes and extends known principles from GNNs and multimodal learning to the specific context of graph-to-text LLM representations.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [GIN, structural bias]",
            "Wang et al. (2023) Which Modality should I use - Text, Motif, or Image? [modality adaptation]",
            "Zhang et al. (2024) GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment [composite encodings]"
        ]
    },
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
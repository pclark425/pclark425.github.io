<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain and Prompt Sensitivity Theory (General Formulation) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1809</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1809</p>
                <p><strong>Name:</strong> Domain and Prompt Sensitivity Theory (General Formulation)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory posits that the ability of large language models (LLMs) to accurately estimate the probability of future real-world scientific discoveries is fundamentally determined by the alignment between the domain of the prompt (i.e., the scientific field, specificity, and novelty of the question) and the density, recency, and diversity of relevant information present in the LLM's training data. The theory further asserts that LLMs' probabilistic outputs are modulated by their internal representations of scientific consensus, novelty detection, and the prompt's semantic structure.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Domain-Data Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_domain &#8594; is_highly_represented_in &#8594; LLM_training_data<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; is_structured_similarly_to &#8594; training_prompts</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_highly_accurate_for &#8594; future_scientific_discovery</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best on tasks and questions that closely match the distribution and style of their training data, as shown in benchmarks for scientific QA and forecasting. </li>
    <li>Empirical studies show LLMs' accuracy drops for domains with sparse or outdated training data. </li>
    <li>Prompt engineering research demonstrates that prompt phrasing and structure can significantly affect LLM output accuracy and calibration. </li>
    <li>LLMs trained on recent and diverse scientific literature outperform those trained on older or less diverse corpora in forecasting tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing work on domain adaptation and prompt engineering, this law formalizes the relationship for the specific case of scientific discovery forecasting.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs perform better on in-domain tasks and when prompts are similar to training data.</p>            <p><strong>What is Novel:</strong> This law explicitly links the accuracy of probabilistic forecasting for scientific discoveries to the density and recency of domain-specific data, and to prompt structure.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Discusses domain adaptation and generalization in LLMs]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning abilities]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity and in-domain performance]</li>
</ul>
            <h3>Statement 1: Consensus and Novelty Modulation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt &#8594; concerns &#8594; scientific topic with high consensus</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_biased_toward &#8594; consensus_view</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs tend to reflect the majority or consensus view in their training data, as seen in outputs for established scientific facts. </li>
    <li>LLMs underpredict rare or novel discoveries unless such events are well-represented in the data. </li>
    <li>Studies on LLMs' outputs show that they are less likely to generate or assign high probability to paradigm-shifting or highly novel scientific claims. </li>
    <li>Bias amplification in LLMs is well-documented, especially for topics with strong consensus or majority representation in the training corpus. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is a novel application of known LLM properties to the context of scientific discovery probability estimation.</p>            <p><strong>What Already Exists:</strong> LLMs are known to reflect the distributional properties of their training data, including consensus and bias.</p>            <p><strong>What is Novel:</strong> This law extends the concept to the domain of scientific forecasting, predicting systematic underestimation of novel discoveries.</p>
            <p><strong>References:</strong> <ul>
    <li>Bender et al. (2021) On the Dangers of Stochastic Parrots [LLMs reflect and amplify consensus and bias]</li>
    <li>Koehler (1991) Explanation, Imagination, and Confidence in Judgment [Consensus and confidence in probabilistic reasoning]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Calibration and bias in LLMs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt about a well-studied field (e.g., CRISPR gene editing) is given to an LLM, its probability estimate for a near-future discovery will closely match expert consensus.</li>
                <li>If a prompt is phrased in a style or format common in the LLM's training data, the model's probability estimates will be more calibrated.</li>
                <li>If the LLM is prompted about a scientific field with abundant, recent literature in its training data, its probability estimates for future discoveries will be more accurate than for underrepresented fields.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a prompt concerns a highly novel or interdisciplinary scientific area with little precedent, the LLM's probability estimate may systematically underestimate the likelihood of breakthrough discoveries.</li>
                <li>If the LLM is exposed to a prompt about a future discovery that is plausible but not yet discussed in the literature, its probability estimate may be less accurate than that of a domain expert.</li>
                <li>If LLMs are fine-tuned on simulated or synthetic data representing hypothetical discoveries, their probability estimates for real-world novel discoveries may improve or degrade in unpredictable ways.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs are able to accurately predict the probability of discoveries in domains with little or no representation in their training data, this would contradict the theory.</li>
                <li>If LLMs provide well-calibrated probability estimates for highly novel or paradigm-shifting discoveries, this would challenge the consensus and novelty modulation law.</li>
                <li>If prompt structure has no measurable effect on LLM probability calibration, the domain-data alignment law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of LLM fine-tuning on real-time data or expert feedback is not addressed. </li>
    <li>The role of explicit uncertainty quantification or Bayesian calibration in LLM outputs is not considered. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes and extends existing knowledge to a new, high-impact application domain.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Domain adaptation and generalization]</li>
    <li>Bender et al. (2021) On the Dangers of Stochastic Parrots [Consensus and bias in LLMs]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity and in-domain performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain and Prompt Sensitivity Theory (General Formulation)",
    "theory_description": "This theory posits that the ability of large language models (LLMs) to accurately estimate the probability of future real-world scientific discoveries is fundamentally determined by the alignment between the domain of the prompt (i.e., the scientific field, specificity, and novelty of the question) and the density, recency, and diversity of relevant information present in the LLM's training data. The theory further asserts that LLMs' probabilistic outputs are modulated by their internal representations of scientific consensus, novelty detection, and the prompt's semantic structure.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Domain-Data Alignment Law",
                "if": [
                    {
                        "subject": "prompt_domain",
                        "relation": "is_highly_represented_in",
                        "object": "LLM_training_data"
                    },
                    {
                        "subject": "prompt",
                        "relation": "is_structured_similarly_to",
                        "object": "training_prompts"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_highly_accurate_for",
                        "object": "future_scientific_discovery"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best on tasks and questions that closely match the distribution and style of their training data, as shown in benchmarks for scientific QA and forecasting.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LLMs' accuracy drops for domains with sparse or outdated training data.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering research demonstrates that prompt phrasing and structure can significantly affect LLM output accuracy and calibration.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs trained on recent and diverse scientific literature outperform those trained on older or less diverse corpora in forecasting tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs perform better on in-domain tasks and when prompts are similar to training data.",
                    "what_is_novel": "This law explicitly links the accuracy of probabilistic forecasting for scientific discoveries to the density and recency of domain-specific data, and to prompt structure.",
                    "classification_explanation": "While related to existing work on domain adaptation and prompt engineering, this law formalizes the relationship for the specific case of scientific discovery forecasting.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Discusses domain adaptation and generalization in LLMs]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning abilities]",
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity and in-domain performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Consensus and Novelty Modulation Law",
                "if": [
                    {
                        "subject": "prompt",
                        "relation": "concerns",
                        "object": "scientific topic with high consensus"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_biased_toward",
                        "object": "consensus_view"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs tend to reflect the majority or consensus view in their training data, as seen in outputs for established scientific facts.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs underpredict rare or novel discoveries unless such events are well-represented in the data.",
                        "uuids": []
                    },
                    {
                        "text": "Studies on LLMs' outputs show that they are less likely to generate or assign high probability to paradigm-shifting or highly novel scientific claims.",
                        "uuids": []
                    },
                    {
                        "text": "Bias amplification in LLMs is well-documented, especially for topics with strong consensus or majority representation in the training corpus.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to reflect the distributional properties of their training data, including consensus and bias.",
                    "what_is_novel": "This law extends the concept to the domain of scientific forecasting, predicting systematic underestimation of novel discoveries.",
                    "classification_explanation": "The law is a novel application of known LLM properties to the context of scientific discovery probability estimation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bender et al. (2021) On the Dangers of Stochastic Parrots [LLMs reflect and amplify consensus and bias]",
                        "Koehler (1991) Explanation, Imagination, and Confidence in Judgment [Consensus and confidence in probabilistic reasoning]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Calibration and bias in LLMs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt about a well-studied field (e.g., CRISPR gene editing) is given to an LLM, its probability estimate for a near-future discovery will closely match expert consensus.",
        "If a prompt is phrased in a style or format common in the LLM's training data, the model's probability estimates will be more calibrated.",
        "If the LLM is prompted about a scientific field with abundant, recent literature in its training data, its probability estimates for future discoveries will be more accurate than for underrepresented fields."
    ],
    "new_predictions_unknown": [
        "If a prompt concerns a highly novel or interdisciplinary scientific area with little precedent, the LLM's probability estimate may systematically underestimate the likelihood of breakthrough discoveries.",
        "If the LLM is exposed to a prompt about a future discovery that is plausible but not yet discussed in the literature, its probability estimate may be less accurate than that of a domain expert.",
        "If LLMs are fine-tuned on simulated or synthetic data representing hypothetical discoveries, their probability estimates for real-world novel discoveries may improve or degrade in unpredictable ways."
    ],
    "negative_experiments": [
        "If LLMs are able to accurately predict the probability of discoveries in domains with little or no representation in their training data, this would contradict the theory.",
        "If LLMs provide well-calibrated probability estimates for highly novel or paradigm-shifting discoveries, this would challenge the consensus and novelty modulation law.",
        "If prompt structure has no measurable effect on LLM probability calibration, the domain-data alignment law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of LLM fine-tuning on real-time data or expert feedback is not addressed.",
            "uuids": []
        },
        {
            "text": "The role of explicit uncertainty quantification or Bayesian calibration in LLM outputs is not considered.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show LLMs can extrapolate to novel domains with few-shot prompting, which may contradict the strict domain-data alignment law.",
            "uuids": []
        },
        {
            "text": "LLMs with retrieval-augmented generation can sometimes provide accurate estimates even for underrepresented domains.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs trained with active retrieval or real-time updates may partially overcome domain-data misalignment.",
        "Prompts that explicitly ask for counterfactual or speculative reasoning may elicit less consensus-biased responses.",
        "LLMs with explicit uncertainty modeling or Bayesian heads may behave differently from standard LLMs."
    ],
    "existing_theory": {
        "what_already_exists": "The general idea that LLMs' performance depends on training data coverage and prompt structure is established.",
        "what_is_novel": "The explicit connection to scientific discovery probability estimation and the formalization of domain and prompt sensitivity as governing laws is novel.",
        "classification_explanation": "This theory synthesizes and extends existing knowledge to a new, high-impact application domain.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Domain adaptation and generalization]",
            "Bender et al. (2021) On the Dangers of Stochastic Parrots [Consensus and bias in LLMs]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity and in-domain performance]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-647",
    "original_theory_name": "Domain and Prompt Sensitivity Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Domain and Prompt Sensitivity Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
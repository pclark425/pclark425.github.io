<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositionality and Locality Theory of Graph-to-Text Representation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1291</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1291</p>
                <p><strong>Name:</strong> Compositionality and Locality Theory of Graph-to-Text Representation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that ideal graph-to-text representations for language model training should maximize compositionality (the ability to represent complex structures as compositions of simpler parts) and locality (the grouping of related graph elements together in text). The theory predicts that such representations will facilitate more efficient learning, better generalization, and improved reasoning about both local and global graph structure.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Compositional Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; is_compositional &#8594; with_respect_to_graph_substructures</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; learns &#8594; complex_graph_patterns_from_simple_components</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Compositional representations (e.g., triple-based, subgraph-based) enable LMs to generalize to unseen graph patterns by reusing learned components. </li>
    <li>Non-compositional, monolithic representations hinder transfer and generalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is a novel application of a well-known principle.</p>            <p><strong>What Already Exists:</strong> Compositionality is a core principle in linguistics and formal semantics.</p>            <p><strong>What is Novel:</strong> The law applies compositionality as a formal requirement for graph-to-text LM training.</p>
            <p><strong>References:</strong> <ul>
    <li>Montague (1970) Universal Grammar [compositionality in semantics]</li>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [compositional graph representations]</li>
</ul>
            <h3>Statement 1: Locality Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; preserves &#8594; locality_of_graph_elements_in_text</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; achieves &#8594; improved_learning_efficiency_and_local_reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Grouping related nodes and edges together in text (e.g., by subgraph or neighborhood) improves LM performance on local reasoning tasks. </li>
    <li>Scattered or non-local representations lead to higher perplexity and lower accuracy on local graph queries. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends a known principle to a new modality.</p>            <p><strong>What Already Exists:</strong> Locality is a principle in graph neural networks and some serialization schemes.</p>            <p><strong>What is Novel:</strong> The law formalizes locality preservation as a requirement for graph-to-text LM training.</p>
            <p><strong>References:</strong> <ul>
    <li>Wu et al. (2021) A Comprehensive Survey on Graph Neural Networks [locality in GNNs]</li>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [locality in AMR graphs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Compositional, locality-preserving graph-to-text representations will enable LMs to generalize to larger or more complex graphs than seen during training.</li>
                <li>Grouping related graph elements together in text will improve LM performance on subgraph matching and local reasoning tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There may be an optimal granularity for compositional encoding (e.g., triples vs. larger subgraphs) that maximizes LM performance.</li>
                <li>Locality-preserving representations may interact with model architecture (e.g., attention span) in non-obvious ways.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If non-compositional or non-local representations yield equal or better LM performance, the theory is challenged.</li>
                <li>If compositionality or locality preservation leads to degraded performance on global reasoning tasks, the theory is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address how to balance locality with the need to encode long-range dependencies in large graphs. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory applies known principles to a new context.</p>
            <p><strong>References:</strong> <ul>
    <li>Montague (1970) Universal Grammar [compositionality]</li>
    <li>Wu et al. (2021) A Comprehensive Survey on Graph Neural Networks [locality in GNNs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositionality and Locality Theory of Graph-to-Text Representation",
    "theory_description": "This theory asserts that ideal graph-to-text representations for language model training should maximize compositionality (the ability to represent complex structures as compositions of simpler parts) and locality (the grouping of related graph elements together in text). The theory predicts that such representations will facilitate more efficient learning, better generalization, and improved reasoning about both local and global graph structure.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Compositional Encoding Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "is_compositional",
                        "object": "with_respect_to_graph_substructures"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "learns",
                        "object": "complex_graph_patterns_from_simple_components"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Compositional representations (e.g., triple-based, subgraph-based) enable LMs to generalize to unseen graph patterns by reusing learned components.",
                        "uuids": []
                    },
                    {
                        "text": "Non-compositional, monolithic representations hinder transfer and generalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositionality is a core principle in linguistics and formal semantics.",
                    "what_is_novel": "The law applies compositionality as a formal requirement for graph-to-text LM training.",
                    "classification_explanation": "The law is a novel application of a well-known principle.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Montague (1970) Universal Grammar [compositionality in semantics]",
                        "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [compositional graph representations]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Locality Preservation Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "preserves",
                        "object": "locality_of_graph_elements_in_text"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "achieves",
                        "object": "improved_learning_efficiency_and_local_reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Grouping related nodes and edges together in text (e.g., by subgraph or neighborhood) improves LM performance on local reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Scattered or non-local representations lead to higher perplexity and lower accuracy on local graph queries.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Locality is a principle in graph neural networks and some serialization schemes.",
                    "what_is_novel": "The law formalizes locality preservation as a requirement for graph-to-text LM training.",
                    "classification_explanation": "The law extends a known principle to a new modality.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wu et al. (2021) A Comprehensive Survey on Graph Neural Networks [locality in GNNs]",
                        "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [locality in AMR graphs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Compositional, locality-preserving graph-to-text representations will enable LMs to generalize to larger or more complex graphs than seen during training.",
        "Grouping related graph elements together in text will improve LM performance on subgraph matching and local reasoning tasks."
    ],
    "new_predictions_unknown": [
        "There may be an optimal granularity for compositional encoding (e.g., triples vs. larger subgraphs) that maximizes LM performance.",
        "Locality-preserving representations may interact with model architecture (e.g., attention span) in non-obvious ways."
    ],
    "negative_experiments": [
        "If non-compositional or non-local representations yield equal or better LM performance, the theory is challenged.",
        "If compositionality or locality preservation leads to degraded performance on global reasoning tasks, the theory is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address how to balance locality with the need to encode long-range dependencies in large graphs.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LMs can learn global graph structure even from non-local, shuffled representations.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For very small graphs, compositionality and locality may be less important.",
        "In highly regular graphs, non-compositional representations may suffice."
    ],
    "existing_theory": {
        "what_already_exists": "Compositionality and locality are established in linguistics and GNNs.",
        "what_is_novel": "Their explicit formalization as laws for graph-to-text LM training is new.",
        "classification_explanation": "The theory applies known principles to a new context.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Montague (1970) Universal Grammar [compositionality]",
            "Wu et al. (2021) A Comprehensive Survey on Graph Neural Networks [locality in GNNs]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-614",
    "original_theory_name": "Motif-Driven Locality Enhancement Theory for Hard Graph Problems",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
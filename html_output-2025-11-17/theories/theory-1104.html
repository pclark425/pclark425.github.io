<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Theory of Logical Reasoning in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1104</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1104</p>
                <p><strong>Name:</strong> Dual-Process Theory of Logical Reasoning in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models (LMs) can best perform strict logical reasoning when they are architecturally or procedurally augmented to support two distinct but interacting processes: (1) a fast, pattern-matching, context-driven process (analogous to System 1 in human cognition) and (2) a slow, explicit, symbolic manipulation process (analogous to System 2). The theory asserts that optimal logical reasoning emerges when LMs can dynamically allocate reasoning tasks between these processes, using the fast process for routine or familiar logical forms and the slow process for novel, complex, or multi-step logical inferences.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Separation of Pattern-Based and Symbolic Reasoning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_augmented_with &#8594; distinct pattern-matching and symbolic modules</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_perform &#8594; both rapid context-based inference and explicit logical manipulation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human reasoning is often modeled as a dual-process system, with fast intuitive and slow deliberative components (Kahneman, 2011). LMs excel at pattern-matching but struggle with strict logic unless augmented with symbolic tools (e.g., external calculators, logic engines). </li>
    <li>Recent work shows LMs improve at arithmetic and logic when given access to explicit symbolic computation modules (e.g., Chen et al., 2022; Cobbe et al., 2021). </li>
    <li>LMs' performance on logic tasks improves when they are allowed to call external verifiers or symbolic engines, as in math word problem solvers. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While dual-process models exist in cognitive science, their direct, dynamic implementation in LMs for strict logical reasoning is not standard; most LMs use either pattern-matching or ad hoc tool use, not a principled dual-process allocation.</p>            <p><strong>What Already Exists:</strong> Dual-process theories are well-established in cognitive science, and LMs have been shown to benefit from external symbolic tools.</p>            <p><strong>What is Novel:</strong> The explicit architectural or procedural separation and dynamic allocation of logical reasoning tasks between these two processes within LMs is a novel proposal.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory in human cognition]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [LMs with external verifiers]</li>
    <li>Chen et al. (2022) Program of Thoughts Prompting [LMs using symbolic computation modules]</li>
</ul>
            <h3>Statement 1: Dynamic Task Allocation for Logical Reasoning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning task &#8594; is_familiar_or_simple &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; allocates_to &#8594; pattern-matching process</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs can solve simple logic tasks via pattern completion, but fail on novel or multi-step logic without explicit symbolic reasoning (Zhou et al., 2023). </li>
    <li>Empirical results show LMs perform well on logic tasks that closely resemble their training data, indicating reliance on pattern-matching. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While LMs can use pattern-matching, the theory's novelty is in the dynamic, context-sensitive allocation between processes.</p>            <p><strong>What Already Exists:</strong> Pattern-matching is the default mode for LMs, and they perform well on familiar logic forms.</p>            <p><strong>What is Novel:</strong> The explicit, dynamic allocation of tasks to different reasoning modules based on task familiarity is not standard in LM architectures.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2023) LLMs as Zero-Shot Reasoners [LMs' pattern-matching vs. reasoning abilities]</li>
</ul>
            <h3>Statement 2: Dynamic Task Allocation for Complex or Novel Reasoning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning task &#8594; is_novel_or_complex &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; allocates_to &#8594; symbolic manipulation process</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs augmented with explicit symbolic reasoning modules outperform standard LMs on complex logic tasks (e.g., theorem proving, multi-step deduction). </li>
    <li>Program-of-thoughts prompting and tool-augmented LMs show improved performance on tasks requiring multi-step logical inference. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Existing work uses symbolic modules, but not in a dynamically allocated, dual-process system.</p>            <p><strong>What Already Exists:</strong> Symbolic reasoning modules have been used to improve LM performance on complex logic tasks.</p>            <p><strong>What is Novel:</strong> The dynamic, context-sensitive allocation of tasks to symbolic modules within a dual-process framework is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Chen et al. (2022) Program of Thoughts Prompting [symbolic modules for LMs]</li>
    <li>Polu & Sutskever (2020) Generative Language Modeling for Automated Theorem Proving [LMs with symbolic reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a language model is architecturally or procedurally augmented with both pattern-matching and symbolic reasoning modules, and can dynamically allocate tasks, it will outperform standard LMs on a suite of logic benchmarks that mix familiar and novel logical forms.</li>
                <li>When presented with logic puzzles that are similar to training data, the LM will use pattern-matching; when presented with novel logic puzzles, it will invoke symbolic reasoning, as evidenced by internal module activation patterns.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If the dual-process allocation is made fully differentiable and end-to-end trainable, the LM may develop emergent meta-reasoning strategies that surpass both pure pattern-matching and pure symbolic approaches.</li>
                <li>In adversarial settings where the distinction between 'familiar' and 'novel' is blurred, the LM may develop hybrid reasoning strategies not seen in either process alone.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an LM with dual-process architecture does not outperform a standard LM on mixed logic tasks, the theory's core claim is challenged.</li>
                <li>If the LM fails to dynamically allocate tasks (e.g., always uses one process regardless of task type), the theory's mechanism is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs show improvement on logic tasks with scale alone, without explicit symbolic modules, suggesting scale may partially substitute for dual-process separation. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is inspired by cognitive science but proposes a novel, principled implementation in LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [LMs with external verifiers]</li>
    <li>Chen et al. (2022) Program of Thoughts Prompting [symbolic modules for LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Theory of Logical Reasoning in Language Models",
    "theory_description": "This theory posits that language models (LMs) can best perform strict logical reasoning when they are architecturally or procedurally augmented to support two distinct but interacting processes: (1) a fast, pattern-matching, context-driven process (analogous to System 1 in human cognition) and (2) a slow, explicit, symbolic manipulation process (analogous to System 2). The theory asserts that optimal logical reasoning emerges when LMs can dynamically allocate reasoning tasks between these processes, using the fast process for routine or familiar logical forms and the slow process for novel, complex, or multi-step logical inferences.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Separation of Pattern-Based and Symbolic Reasoning",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_augmented_with",
                        "object": "distinct pattern-matching and symbolic modules"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_perform",
                        "object": "both rapid context-based inference and explicit logical manipulation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human reasoning is often modeled as a dual-process system, with fast intuitive and slow deliberative components (Kahneman, 2011). LMs excel at pattern-matching but struggle with strict logic unless augmented with symbolic tools (e.g., external calculators, logic engines).",
                        "uuids": []
                    },
                    {
                        "text": "Recent work shows LMs improve at arithmetic and logic when given access to explicit symbolic computation modules (e.g., Chen et al., 2022; Cobbe et al., 2021).",
                        "uuids": []
                    },
                    {
                        "text": "LMs' performance on logic tasks improves when they are allowed to call external verifiers or symbolic engines, as in math word problem solvers.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Dual-process theories are well-established in cognitive science, and LMs have been shown to benefit from external symbolic tools.",
                    "what_is_novel": "The explicit architectural or procedural separation and dynamic allocation of logical reasoning tasks between these two processes within LMs is a novel proposal.",
                    "classification_explanation": "While dual-process models exist in cognitive science, their direct, dynamic implementation in LMs for strict logical reasoning is not standard; most LMs use either pattern-matching or ad hoc tool use, not a principled dual-process allocation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kahneman (2011) Thinking, Fast and Slow [dual-process theory in human cognition]",
                        "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [LMs with external verifiers]",
                        "Chen et al. (2022) Program of Thoughts Prompting [LMs using symbolic computation modules]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dynamic Task Allocation for Logical Reasoning",
                "if": [
                    {
                        "subject": "reasoning task",
                        "relation": "is_familiar_or_simple",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "allocates_to",
                        "object": "pattern-matching process"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs can solve simple logic tasks via pattern completion, but fail on novel or multi-step logic without explicit symbolic reasoning (Zhou et al., 2023).",
                        "uuids": []
                    },
                    {
                        "text": "Empirical results show LMs perform well on logic tasks that closely resemble their training data, indicating reliance on pattern-matching.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Pattern-matching is the default mode for LMs, and they perform well on familiar logic forms.",
                    "what_is_novel": "The explicit, dynamic allocation of tasks to different reasoning modules based on task familiarity is not standard in LM architectures.",
                    "classification_explanation": "While LMs can use pattern-matching, the theory's novelty is in the dynamic, context-sensitive allocation between processes.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2023) LLMs as Zero-Shot Reasoners [LMs' pattern-matching vs. reasoning abilities]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dynamic Task Allocation for Complex or Novel Reasoning",
                "if": [
                    {
                        "subject": "reasoning task",
                        "relation": "is_novel_or_complex",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "allocates_to",
                        "object": "symbolic manipulation process"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs augmented with explicit symbolic reasoning modules outperform standard LMs on complex logic tasks (e.g., theorem proving, multi-step deduction).",
                        "uuids": []
                    },
                    {
                        "text": "Program-of-thoughts prompting and tool-augmented LMs show improved performance on tasks requiring multi-step logical inference.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Symbolic reasoning modules have been used to improve LM performance on complex logic tasks.",
                    "what_is_novel": "The dynamic, context-sensitive allocation of tasks to symbolic modules within a dual-process framework is novel.",
                    "classification_explanation": "Existing work uses symbolic modules, but not in a dynamically allocated, dual-process system.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Chen et al. (2022) Program of Thoughts Prompting [symbolic modules for LMs]",
                        "Polu & Sutskever (2020) Generative Language Modeling for Automated Theorem Proving [LMs with symbolic reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a language model is architecturally or procedurally augmented with both pattern-matching and symbolic reasoning modules, and can dynamically allocate tasks, it will outperform standard LMs on a suite of logic benchmarks that mix familiar and novel logical forms.",
        "When presented with logic puzzles that are similar to training data, the LM will use pattern-matching; when presented with novel logic puzzles, it will invoke symbolic reasoning, as evidenced by internal module activation patterns."
    ],
    "new_predictions_unknown": [
        "If the dual-process allocation is made fully differentiable and end-to-end trainable, the LM may develop emergent meta-reasoning strategies that surpass both pure pattern-matching and pure symbolic approaches.",
        "In adversarial settings where the distinction between 'familiar' and 'novel' is blurred, the LM may develop hybrid reasoning strategies not seen in either process alone."
    ],
    "negative_experiments": [
        "If an LM with dual-process architecture does not outperform a standard LM on mixed logic tasks, the theory's core claim is challenged.",
        "If the LM fails to dynamically allocate tasks (e.g., always uses one process regardless of task type), the theory's mechanism is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs show improvement on logic tasks with scale alone, without explicit symbolic modules, suggesting scale may partially substitute for dual-process separation.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Recent very large LMs (e.g., GPT-4) can sometimes perform multi-step logic without explicit symbolic modules, challenging the necessity of dual-process separation.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks that are both novel and require world knowledge may not benefit from symbolic reasoning alone.",
        "If the LM's symbolic module is poorly integrated, performance may degrade due to interface bottlenecks."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process models in cognitive science and some LM tool-use approaches.",
        "what_is_novel": "Explicit, dynamic, architectural/procedural dual-process separation and allocation in LMs for strict logical reasoning.",
        "classification_explanation": "The theory is inspired by cognitive science but proposes a novel, principled implementation in LMs.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kahneman (2011) Thinking, Fast and Slow [dual-process theory]",
            "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [LMs with external verifiers]",
            "Chen et al. (2022) Program of Thoughts Prompting [symbolic modules for LMs]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-602",
    "original_theory_name": "Emergent Reasoning Threshold Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
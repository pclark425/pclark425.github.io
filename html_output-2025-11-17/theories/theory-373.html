<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Level Perception-Language-Action Grounding Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-373</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-373</p>
                <p><strong>Name:</strong> Multi-Level Perception-Language-Action Grounding Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about when and how pretraining on text worlds transfers to 3D embodied tasks, including mappings between high-level action semantics and low-level perception and predicted sample complexity gains, based on the following results.</p>
                <p><strong>Description:</strong> Transfer from text-world pretraining to 3D embodied tasks requires multi-level grounding across four interconnected dimensions: (1) Perception-Language Grounding—mapping linguistic concepts to visual features through vision-language pretraining, object-centric representations, and handling distribution shift; (2) Action-Space Grounding—aligning language-conditioned perceptual representations with executable motor commands; (3) Temporal Grounding—extending single-step localization to sequential trajectory prediction and long-horizon planning; and (4) Spatial-Semantic Memory—maintaining grounded state representations across time. Each grounding level has distinct requirements and failure modes. Grounding quality is necessary but not sufficient for task success—execution precision, system coordination, and real-time constraints impose additional bottlenecks. Grounding quality scales with pretraining data scale and domain-relevance, is sensitive to representation precision and adversarial perturbations, and can be enhanced through explicit reasoning scaffolds, synthetic data generation, and partially-frozen adaptation strategies.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-196.html">[theory-196]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Renamed theory to 'Multi-Level Perception-Language-Action Grounding Theory' to emphasize the four interconnected grounding dimensions</li>
                <li>Restructured theory description to clearly delineate four grounding levels: (1) Perception-Language, (2) Action-Space, (3) Temporal, and (4) Spatial-Semantic Memory</li>
                <li>Emphasized that grounding is necessary but not sufficient for task success, with execution precision and system coordination as additional bottlenecks</li>
                <li>Reorganized supporting evidence into 16 more focused clusters with clearer themes and more precise quantitative results</li>
                <li>Added explicit evidence for action-space grounding as distinct bottleneck (string tokenizer, VLM-VLA misalignment, FAST compression)</li>
                <li>Added explicit evidence for temporal grounding requiring different mechanisms (PIO S1/S2 vs S3 results, model-specific strengths)</li>
                <li>Strengthened evidence for grounding being necessary but not sufficient (ROBOGROUND contact-success gap, VLM-VLA misalignment, dual-system coordination issues)</li>
                <li>Split theory statements into 19 more focused statements, separating previously combined concepts</li>
                <li>Added statement distinguishing temporal grounding from single-step localization as requiring different mechanisms</li>
                <li>Added statement about multi-level grounding requiring careful coordination to avoid latency and consistency issues</li>
                <li>Refined predictions to be more specific and testable, with clearer expected outcomes</li>
                <li>Added predictions about contact-to-success gap and whether it can be closed through improved grounding alone</li>
                <li>Added negative experiments testing synthetic data generation, memory systems, and reasoning modules</li>
                <li>Expanded unaccounted_for items to include contact-success gap mechanisms, scene-task contradiction handling, and multi-level coordination challenges</li>
                <li>Added unaccounted_for item about proprioceptive and force feedback integration</li>
                <li>Improved precision of quantitative results throughout (e.g., specific percentage improvements, exact model comparisons)</li>
                <li>Clarified that partially-frozen approaches outperform both full freezing and naive full fine-tuning</li>
                <li>Added evidence for quantization effects being disproportionate on high-level vs simple grounding</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Perception-language grounding quality is a primary bottleneck for embodied AI transfer, often more limiting than language understanding capabilities alone, with perception errors accounting for >50% of failures even when language models are strong</li>
                <li>Visual encoders pretrained with language supervision through large-scale contrastive learning provide better grounding than purely visual pretraining, with grounding quality scaling with both pretraining data scale and domain-relevance</li>
                <li>Object-centric or region-level visual representations enable substantially better grounding than global image features by providing explicit associations between language tokens and visual entities</li>
                <li>Distribution shift between pretraining visual data (web images, third-person views) and embodied observations (egocentric views, partial observability, occlusion) severely degrades grounding quality</li>
                <li>Multi-view and diverse pretraining data (including wrist-camera, egocentric videos) improves grounding robustness to viewpoint changes, occlusion, and adversarial perturbations</li>
                <li>Explicit spatial representations (3D coordinates, depth, spatial relations, multi-scale features) improve grounding for embodied tasks compared to purely appearance-based 2D features</li>
                <li>Cross-modal fusion mechanisms (cross-attention, FiLM conditioning, hierarchical semantic enhancement) are necessary to effectively combine language and visual information</li>
                <li>Partially-frozen encoder approaches that preserve one frozen copy while adapting another effectively balance retention of pretrained grounding with task-specific adaptation, outperforming both full freezing and naive full fine-tuning</li>
                <li>Action-space grounding—the mapping from grounded perceptions to executable motor commands—is a distinct bottleneck requiring explicit alignment between language tokens and action representations</li>
                <li>Temporal grounding for sequential trajectory prediction requires different mechanisms than single-step localization; models can excel at one while failing at the other</li>
                <li>Explicit reasoning and chain-of-thought scaffolds enhance grounding for complex spatial reasoning, long-horizon tasks, and handling scene-task contradictions</li>
                <li>Memory systems (spatial memory banks, episodic memory, implicit scene representations) are critical for maintaining grounding across time in sequential tasks, but depend on input quality</li>
                <li>Grounding quality is necessary but not sufficient for task success—execution precision, system coordination, real-time constraints, and action-space alignment impose additional bottlenecks</li>
                <li>Grounding representation format (heatmaps vs points vs boxes vs masks) affects both accuracy and computational efficiency, with adaptive representations achieving better trade-offs</li>
                <li>Grounding quality is sensitive to representation precision: quantization disproportionately harms high-level grounding (relational, implicit) compared to simple attribute recognition</li>
                <li>Adversarial robustness is a dimension of grounding quality; embedding-space attacks exploit alignment vulnerabilities, and diverse pretraining plus adversarial fine-tuning improve robustness</li>
                <li>Synthetic and procedural data generation with domain-matching augmentations can effectively scale grounding supervision and improve open-world generalization</li>
                <li>Grounding quality degrades more severely for novel objects, rare viewpoints, complex spatial configurations, and out-of-distribution appearances not well-represented in pretraining data</li>
                <li>Multi-level grounding (perception-language, action-space, temporal, memory) provides more robust transfer than single-level approaches, but requires careful coordination to avoid latency and consistency issues</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Perception/grounding identified as dominant failure mode (>50% of errors) across multiple studies: EmbodiedVSR shows perception errors are major bottleneck despite strong reasoning; OPEx analysis found perception and low-level execution >50% of errors; OpenVLA adversarial attacks exploit embedding-space grounding causing near-100% failure rates; DenseGrounding identifies semantic loss from sparse sampling as primary bottleneck; modular VLM pipelines fail on implicit affordance despite GPT-4-class LLMs <a href="../results/extraction-result-1941.html#e1941.0" class="evidence-link">[e1941.0]</a> <a href="../results/extraction-result-1696.html#e1696.0" class="evidence-link">[e1696.0]</a> <a href="../results/extraction-result-1975.html#e1975.0" class="evidence-link">[e1975.0]</a> <a href="../results/extraction-result-1975.html#e1975.3" class="evidence-link">[e1975.3]</a> <a href="../results/extraction-result-1968.html#e1968.0" class="evidence-link">[e1968.0]</a> <a href="../results/extraction-result-1981.html#e1981.1" class="evidence-link">[e1981.1]</a> </li>
    <li>Vision-language pretraining scale and quality directly improve grounding: Vlaser finetuning of InternVL3 on embodied data yields +30 point gains on reasoning benchmarks; RoboVLMs study shows stronger VLM alignment correlates with better control performance; WMNav comparison shows larger VLMs (Gemini vs Qwen 3B→7B) yield 29.7%→58.1% SR improvements; richer embeddings (SigLIP 1152-d) outperform smaller ones (CLIP 512-d: 74% vs 62% SR; ViLT 40% SR) <a href="../results/extraction-result-1967.html#e1967.4" class="evidence-link">[e1967.4]</a> <a href="../results/extraction-result-1948.html#e1948.8" class="evidence-link">[e1948.8]</a> <a href="../results/extraction-result-1980.html#e1980.0" class="evidence-link">[e1980.0]</a> <a href="../results/extraction-result-1973.html#e1973.0" class="evidence-link">[e1973.0]</a> <a href="../results/extraction-result-1973.html#e1973.2" class="evidence-link">[e1973.2]</a> </li>
    <li>Object-centric and region-level representations substantially improve grounding over global features: ProxyTransformation's cluster-level multimodal enhancement yields +7.49% AP (easy) and +4.60% AP (hard) on EmbodiedScan; MTU3D's object-centric spatial memory enables +13.7% SR gains on HM3D-OVON; ROBOGROUND's pixel-level masks improve contact rates by +18-30% over no-grounding baseline; RoboMAP's adaptive heatmaps outperform point-based methods by +9-14% on grounding benchmarks <a href="../results/extraction-result-1936.html#e1936.0" class="evidence-link">[e1936.0]</a> <a href="../results/extraction-result-1977.html#e1977.0" class="evidence-link">[e1977.0]</a> <a href="../results/extraction-result-1943.html#e1943.0" class="evidence-link">[e1943.0]</a> <a href="../results/extraction-result-1978.html#e1978.1" class="evidence-link">[e1978.1]</a> <a href="../results/extraction-result-1978.html#e1978.2" class="evidence-link">[e1978.2]</a> <a href="../results/extraction-result-1978.html#e1978.3" class="evidence-link">[e1978.3]</a> </li>
    <li>Domain shift between pretraining and embodied observations is critical bottleneck: OneTwoVLA without synthetic VL co-training achieves only 8% open-world grounding vs 73% with co-training (+65% absolute improvement); OpenVLA shows catastrophic representation degradation under fine-tuning on robot data requiring dual-encoder preservation; π0's wrist-camera pretraining reduces adversarial vulnerability compared to third-person-only training; RecBert pretrained on ALFRED shows negative transfer to R2R due to synthetic rendering domain gap <a href="../results/extraction-result-1965.html#e1965.0" class="evidence-link">[e1965.0]</a> <a href="../results/extraction-result-1947.html#e1947.2" class="evidence-link">[e1947.2]</a> <a href="../results/extraction-result-1975.html#e1975.2" class="evidence-link">[e1975.2]</a> <a href="../results/extraction-result-1729.html#e1729.2" class="evidence-link">[e1729.2]</a> </li>
    <li>Explicit spatial representations improve grounding: DenseGrounding's hierarchical scene semantic enhancer yields +5.57% ACC@25; EmbodiedVSR's dynamic scene graphs with physics-constrained CoT reduce geometric hallucinations; ProxyTransformation's 3D cluster transforms outperform 2D-only by +4.2 AP; MTU3D's 3D spatial memory bank enables lifelong navigation gains; VEME's geometry-aware features (VGGT) essential for spatio-temporal understanding (removal causes ~21 point drop) <a href="../results/extraction-result-1968.html#e1968.0" class="evidence-link">[e1968.0]</a> <a href="../results/extraction-result-1941.html#e1941.0" class="evidence-link">[e1941.0]</a> <a href="../results/extraction-result-1936.html#e1936.0" class="evidence-link">[e1936.0]</a> <a href="../results/extraction-result-1977.html#e1977.0" class="evidence-link">[e1977.0]</a> <a href="../results/extraction-result-1956.html#e1956.3" class="evidence-link">[e1956.3]</a> </li>
    <li>Multi-view and diverse visual pretraining improves grounding robustness: π0 with wrist-camera pretraining shows lower adversarial vulnerability (FR increases ~31.4% under EDPA vs OpenVLA ~100%); OpenVLA-OFT with multi-camera fine-tuning reduces EDPA-induced failures; Gondola's multi-view inputs substantially reduce occlusion-driven grounding errors; ProxyTransformation using multi-view features outperforms single-view approaches <a href="../results/extraction-result-1975.html#e1975.2" class="evidence-link">[e1975.2]</a> <a href="../results/extraction-result-1975.html#e1975.1" class="evidence-link">[e1975.1]</a> <a href="../results/extraction-result-1958.html#e1958.0" class="evidence-link">[e1958.0]</a> <a href="../results/extraction-result-1936.html#e1936.0" class="evidence-link">[e1936.0]</a> </li>
    <li>Partially-frozen dual encoder approaches effectively preserve pretrained grounding while enabling task adaptation: OpenVLA+ partially-frozen dual encoder (one frozen anchor, one trainable) improves from 35.03% to 55.55% avg SR, outperforming naive full fine-tuning; combined with string tokenizer and co-training yields 78.46% (+43.43 points); EF-VLA using frozen CLIP encoders achieves +20% on compositional tasks and 85% success on unseen goal descriptions <a href="../results/extraction-result-1947.html#e1947.2" class="evidence-link">[e1947.2]</a> <a href="../results/extraction-result-1982.html#e1982.3" class="evidence-link">[e1982.3]</a> </li>
    <li>Action-space grounding is distinct bottleneck requiring alignment between language and executable commands: OpenVLA+ string tokenizer alone yields +15.22 points (35.03%→50.25%) by aligning actions with language tokens; combined with co-training yields +43.14 points; VLM-VLA misalignment causes execution failures despite correct high-level grounding (e.g., 'Oreo' vs 'biscuit box'); FAST tokenization enables 15× faster inference and 200 Hz control by compressing action sequences <a href="../results/extraction-result-1947.html#e1947.3" class="evidence-link">[e1947.3]</a> <a href="../results/extraction-result-1962.html#e1962.3" class="evidence-link">[e1962.3]</a> <a href="../results/extraction-result-1982.html#e1982.11" class="evidence-link">[e1982.11]</a> </li>
    <li>Temporal/trajectory grounding requires different mechanisms than single-step localization: PIO benchmark shows models excel at S1/S2 single-target grounding but fail at S3 trajectory prediction; grounding-finetuned models (MoLMO, Qwen-2.5-VL, RoboRefer) strong on S1/S2 but weak on S3; OneTwoVLA's explicit textual reasoning improves long-horizon tasks (+30% over π0 baseline: 87% vs 57%); Gemini-2.5-Pro succeeds at S3 planning despite weaker S1/S2 precision <a href="../results/extraction-result-1972.html#e1972.0" class="evidence-link">[e1972.0]</a> <a href="../results/extraction-result-1972.html#e1972.3" class="evidence-link">[e1972.3]</a> <a href="../results/extraction-result-1972.html#e1972.4" class="evidence-link">[e1972.4]</a> <a href="../results/extraction-result-1972.html#e1972.5" class="evidence-link">[e1972.5]</a> <a href="../results/extraction-result-1965.html#e1965.0" class="evidence-link">[e1965.0]</a> <a href="../results/extraction-result-1972.html#e1972.7" class="evidence-link">[e1972.7]</a> </li>
    <li>Memory systems critical for sequential grounding but depend on input quality: MTU3D's spatial memory bank contributes substantially to lifelong navigation (+13.7% SR on HM3D-OVON, +23.0% on GOAT-Bench); VEME's spatial semantic memory yields ~+9.8 SPL when included; ISR (implicit scene representation) reduces redundant geometric details that impede grounding; however, memory systems depend on clean point clouds and accurate trajectories <a href="../results/extraction-result-1977.html#e1977.2" class="evidence-link">[e1977.2]</a> <a href="../results/extraction-result-1956.html#e1956.1" class="evidence-link">[e1956.1]</a> <a href="../results/extraction-result-1933.html#e1933.1" class="evidence-link">[e1933.1]</a> <a href="../results/extraction-result-1977.html#e1977.0" class="evidence-link">[e1977.0]</a> </li>
    <li>Explicit reasoning and chain-of-thought enhance grounding for complex spatial reasoning: EmbodiedVSR's physics-constrained CoT reduces geometric hallucinations and improves spatial reasoning on MMBench (+10.9 EN, +10.6 CN V11 when adding scene graph); Robix's thought-action consistency training via RL improves grounding and reduces irrational steps (+3.3 to +8.3 points on Internal OOD); RFT with free-form CoT yields +28.65 points over SFT on Where2Place grounding benchmark <a href="../results/extraction-result-1941.html#e1941.0" class="evidence-link">[e1941.0]</a> <a href="../results/extraction-result-1962.html#e1962.0" class="evidence-link">[e1962.0]</a> <a href="../results/extraction-result-1979.html#e1979.7" class="evidence-link">[e1979.7]</a> </li>
    <li>Grounding is necessary but not sufficient for task success: ROBOGROUND achieves 89% contact rate but only 43.3% success on pick-and-place (46-point gap), showing localization doesn't guarantee manipulation; VLM-VLA misalignment causes execution failures despite correct visual grounding; dual-system approaches face latency (Groot N1: ~800ms LLM vs ~10ms control) and coordination issues despite +17% SR and -28% collisions <a href="../results/extraction-result-1943.html#e1943.0" class="evidence-link">[e1943.0]</a> <a href="../results/extraction-result-1962.html#e1962.3" class="evidence-link">[e1962.3]</a> <a href="../results/extraction-result-1965.html#e1965.3" class="evidence-link">[e1965.3]</a> <a href="../results/extraction-result-1982.html#e1982.4" class="evidence-link">[e1982.4]</a> </li>
    <li>Synthetic and procedural data generation with domain-matching augmentations effectively scales grounding supervision: OneTwoVLA's synthetic VL pipeline (16K images with fisheye distortion and gripper compositing) yields +65% open-world grounding improvement (8%→73%); OXE-MAP procedural conversion of trajectories to heatmaps enables effective training; DenseGrounding's LLM-grounded text augmentation with scene relationships and locations yields +3.48% ACC@25 <a href="../results/extraction-result-1965.html#e1965.4" class="evidence-link">[e1965.4]</a> <a href="../results/extraction-result-1978.html#e1978.4" class="evidence-link">[e1978.4]</a> <a href="../results/extraction-result-1968.html#e1968.4" class="evidence-link">[e1968.4]</a> </li>
    <li>Adversarial robustness reveals systematic grounding vulnerabilities: EDPA embedding-space attacks cause near-100% failure by disrupting image-instruction alignment; attacks transfer across models (OpenVLA ~49.8% avg FR, OpenVLA-OFT ~26.98%, π0 ~9.3%) and datasets (~74.7% cross-dataset transfer); adversarial fine-tuning of visual encoder reduces vulnerability by ~34.2% on average; diverse pretraining (wrist-camera) improves robustness <a href="../results/extraction-result-1975.html#e1975.3" class="evidence-link">[e1975.3]</a> <a href="../results/extraction-result-1975.html#e1975.0" class="evidence-link">[e1975.0]</a> </li>
    <li>Grounding representation format affects accuracy-efficiency trade-offs: RoboMAP's adaptive heatmaps outperform fixed-shape heatmaps and point methods (+9-14% on benchmarks) with 15× faster inference than iterative methods like Embodied-R1; Gondola's end-to-end mask generation outperforms box-based grounding by +20-25 IoU points; normalized IoU metric needed to fairly compare boxes vs points vs masks <a href="../results/extraction-result-1978.html#e1978.1" class="evidence-link">[e1978.1]</a> <a href="../results/extraction-result-1958.html#e1958.0" class="evidence-link">[e1958.0]</a> <a href="../results/extraction-result-1978.html#e1978.10" class="evidence-link">[e1978.10]</a> <a href="../results/extraction-result-1972.html#e1972.9" class="evidence-link">[e1972.9]</a> </li>
    <li>Quantization and numerical precision disproportionately affect high-level grounding: INT4 quantization of Llama 3.2 Vision 11B causes 14-17% accuracy drops on implicit and relational grounding but only ~4% loss on attribute recognition; string-based action tokenization preserves fine-grained spatial values while enabling language alignment <a href="../results/extraction-result-1981.html#e1981.6" class="evidence-link">[e1981.6]</a> <a href="../results/extraction-result-1947.html#e1947.3" class="evidence-link">[e1947.3]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Pretraining visual encoders on large-scale egocentric video datasets (e.g., Ego4D) with multi-view consistency objectives should improve robustness to viewpoint changes and occlusion compared to third-person web images, especially for manipulation tasks</li>
                <li>Incorporating depth information or 3D structure during vision-language pretraining should improve transfer to 3D embodied tasks, particularly for spatial reasoning and navigation tasks requiring metric understanding</li>
                <li>Object-centric pretraining with explicit object annotations should improve grounding for manipulation tasks compared to scene-level pretraining, especially for fine-grained object discrimination in clutter</li>
                <li>Domain-adaptive pretraining using synthetic data with domain-matching augmentations (fisheye distortion, gripper compositing, lighting variation) should reduce perception domain gap and improve sim-to-real transfer</li>
                <li>Video-language pretraining should provide better temporal grounding and trajectory prediction compared to static image-text pretraining for dynamic embodied tasks</li>
                <li>Adversarial training targeting embedding-space alignment should improve robustness to both adversarial attacks and natural distribution shifts in embodied settings</li>
                <li>Combining adaptive heatmap representations with action chunking should enable both efficient grounding and high-frequency control (>100 Hz) for real-time manipulation</li>
                <li>Memory systems that explicitly track object permanence and spatial relations across occlusions should improve long-horizon task performance in partially observable environments</li>
                <li>Partially-frozen dual-encoder approaches combined with string-based action tokenization and vision-language co-training should enable effective transfer with minimal catastrophic forgetting across diverse robot platforms</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether synthetic data with perfect annotations but limited photorealism can provide sufficient visual diversity for grounding despite domain gap, or if photorealism is necessary for robust transfer to real-world embodied tasks</li>
                <li>Whether grounding can be learned purely from language-annotated videos without explicit object/region annotations, or if structured supervision is necessary for object-centric grounding quality</li>
                <li>Whether there is a fundamental limit to grounding quality achievable from static pretraining vs. interactive embodied experience with closed-loop feedback and online adaptation</li>
                <li>Whether grounding quality continues to improve with scale beyond current largest models (100B+ parameters) or if there are diminishing returns, and whether architectural improvements matter more than scale</li>
                <li>Whether grounding learned in one embodiment (robotic arm) transfers effectively to different embodiments (quadruped, humanoid) or if embodiment-specific grounding is required</li>
                <li>Whether world models and latent planning can fully compensate for single-frame grounding limitations, or if explicit multi-frame memory is necessary for long-horizon tasks</li>
                <li>Whether adversarial training on embedding-space attacks improves robustness to natural distribution shifts, or if these are fundamentally different challenges requiring separate solutions</li>
                <li>Whether unified architectures that jointly perform reasoning and acting can scale to very long-horizon tasks (100+ steps) without facing memory or latency constraints that degrade performance</li>
                <li>Whether temporal grounding for trajectory prediction can be learned from static image-text pairs with motion-related language, or requires video-based pretraining with temporal supervision</li>
                <li>Whether the contact-to-success gap in manipulation (e.g., 89% contact but 43% success) can be closed through improved grounding alone, or requires fundamentally different approaches to grasp pose estimation and force control</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Demonstrating successful transfer using random visual features (no pretraining) would challenge the importance of learned visual representations for grounding</li>
                <li>Showing that language-only models (no visual input) can achieve comparable performance on embodied manipulation tasks would challenge the necessity of perception-language grounding</li>
                <li>Finding that increasing visual encoder capacity or pretraining scale does not improve grounding quality beyond a certain point would challenge the scalability of current approaches</li>
                <li>Demonstrating that explicit object annotations during pretraining do not improve grounding compared to image-level captions would challenge the importance of object-centric representations</li>
                <li>Showing that domain-specific pretraining does not improve transfer compared to general web pretraining would challenge the importance of domain alignment</li>
                <li>Finding that frozen pretrained encoders perform as well as partially-frozen or fine-tuned encoders would challenge the need for task-specific adaptation</li>
                <li>Demonstrating that 2D visual features are sufficient for 3D embodied tasks would challenge the importance of explicit 3D representations</li>
                <li>Showing that single-step grounding mechanisms work equally well for temporal/trajectory prediction would challenge the need for specialized temporal grounding mechanisms</li>
                <li>Finding that grounding improvements do not translate to execution success improvements would challenge grounding as a primary bottleneck (though evidence suggests grounding is necessary but not sufficient)</li>
                <li>Demonstrating that adversarial training does not improve robustness to natural distribution shifts would challenge the connection between adversarial and natural robustness</li>
                <li>Showing that memory systems do not improve performance on long-horizon tasks would challenge their necessity for temporal grounding</li>
                <li>Finding that explicit reasoning modules do not reduce hallucinations or improve spatial reasoning would challenge their role in grounding enhancement</li>
                <li>Demonstrating that synthetic data generation does not improve open-world grounding would challenge its effectiveness for scaling supervision</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to effectively handle the long-tail distribution of objects and scenes in embodied environments that are rare or absent in both pretraining data and synthetic data generation </li>
    <li>The precise mechanisms by which explicit reasoning and chain-of-thought improve grounding quality, and whether these can be learned end-to-end or require structured supervision and physics constraints </li>
    <li>How grounding quality interacts with partial observability and the need to maintain object permanence across time steps, beyond current memory system approaches </li>
    <li>The optimal balance between grounding quality and computational efficiency for real-time embodied control, and whether this trade-off is fundamental or can be overcome through architectural innovations </li>
    <li>How to balance the trade-off between single-step grounding precision and temporal/trajectory grounding in unified architectures without sacrificing either capability </li>
    <li>The extent to which grounding can be improved through online learning or adaptation during deployment vs. requiring offline pretraining, and optimal strategies for continual learning without catastrophic forgetting </li>
    <li>How action-space grounding interacts with perception grounding, and whether they can be optimized jointly or require separate mechanisms and training stages </li>
    <li>The relationship between adversarial robustness and robustness to natural distribution shifts, and whether improving one necessarily improves the other or requires different approaches </li>
    <li>How grounding degrades with increasing task complexity and longer time horizons, and whether there are fundamental limits to grounding-based approaches for very long-horizon tasks (100+ steps) </li>
    <li>The role of proprioceptive and force feedback in grounding for manipulation tasks, beyond visual and language grounding, and how to integrate these modalities </li>
    <li>How to effectively coordinate multiple levels of grounding (perception, action, temporal, memory) in unified architectures without latency or coordination failures that degrade performance </li>
    <li>The mechanisms underlying the large contact-to-success gap in manipulation (e.g., 89% contact but 43% success), and whether this is fundamentally a grounding problem or requires different approaches </li>
    <li>How to handle scene-task contradictions and infeasible task specifications, as models struggle to refuse or replan even with explicit feedback about missing objects </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Level Perception-Language-Action Grounding Theory",
    "type": "general",
    "theory_description": "Transfer from text-world pretraining to 3D embodied tasks requires multi-level grounding across four interconnected dimensions: (1) Perception-Language Grounding—mapping linguistic concepts to visual features through vision-language pretraining, object-centric representations, and handling distribution shift; (2) Action-Space Grounding—aligning language-conditioned perceptual representations with executable motor commands; (3) Temporal Grounding—extending single-step localization to sequential trajectory prediction and long-horizon planning; and (4) Spatial-Semantic Memory—maintaining grounded state representations across time. Each grounding level has distinct requirements and failure modes. Grounding quality is necessary but not sufficient for task success—execution precision, system coordination, and real-time constraints impose additional bottlenecks. Grounding quality scales with pretraining data scale and domain-relevance, is sensitive to representation precision and adversarial perturbations, and can be enhanced through explicit reasoning scaffolds, synthetic data generation, and partially-frozen adaptation strategies.",
    "supporting_evidence": [
        {
            "text": "Perception/grounding identified as dominant failure mode (&gt;50% of errors) across multiple studies: EmbodiedVSR shows perception errors are major bottleneck despite strong reasoning; OPEx analysis found perception and low-level execution &gt;50% of errors; OpenVLA adversarial attacks exploit embedding-space grounding causing near-100% failure rates; DenseGrounding identifies semantic loss from sparse sampling as primary bottleneck; modular VLM pipelines fail on implicit affordance despite GPT-4-class LLMs",
            "uuids": [
                "e1941.0",
                "e1696.0",
                "e1975.0",
                "e1975.3",
                "e1968.0",
                "e1981.1"
            ]
        },
        {
            "text": "Vision-language pretraining scale and quality directly improve grounding: Vlaser finetuning of InternVL3 on embodied data yields +30 point gains on reasoning benchmarks; RoboVLMs study shows stronger VLM alignment correlates with better control performance; WMNav comparison shows larger VLMs (Gemini vs Qwen 3B→7B) yield 29.7%→58.1% SR improvements; richer embeddings (SigLIP 1152-d) outperform smaller ones (CLIP 512-d: 74% vs 62% SR; ViLT 40% SR)",
            "uuids": [
                "e1967.4",
                "e1948.8",
                "e1980.0",
                "e1973.0",
                "e1973.2"
            ]
        },
        {
            "text": "Object-centric and region-level representations substantially improve grounding over global features: ProxyTransformation's cluster-level multimodal enhancement yields +7.49% AP (easy) and +4.60% AP (hard) on EmbodiedScan; MTU3D's object-centric spatial memory enables +13.7% SR gains on HM3D-OVON; ROBOGROUND's pixel-level masks improve contact rates by +18-30% over no-grounding baseline; RoboMAP's adaptive heatmaps outperform point-based methods by +9-14% on grounding benchmarks",
            "uuids": [
                "e1936.0",
                "e1977.0",
                "e1943.0",
                "e1978.1",
                "e1978.2",
                "e1978.3"
            ]
        },
        {
            "text": "Domain shift between pretraining and embodied observations is critical bottleneck: OneTwoVLA without synthetic VL co-training achieves only 8% open-world grounding vs 73% with co-training (+65% absolute improvement); OpenVLA shows catastrophic representation degradation under fine-tuning on robot data requiring dual-encoder preservation; π0's wrist-camera pretraining reduces adversarial vulnerability compared to third-person-only training; RecBert pretrained on ALFRED shows negative transfer to R2R due to synthetic rendering domain gap",
            "uuids": [
                "e1965.0",
                "e1947.2",
                "e1975.2",
                "e1729.2"
            ]
        },
        {
            "text": "Explicit spatial representations improve grounding: DenseGrounding's hierarchical scene semantic enhancer yields +5.57% ACC@25; EmbodiedVSR's dynamic scene graphs with physics-constrained CoT reduce geometric hallucinations; ProxyTransformation's 3D cluster transforms outperform 2D-only by +4.2 AP; MTU3D's 3D spatial memory bank enables lifelong navigation gains; VEME's geometry-aware features (VGGT) essential for spatio-temporal understanding (removal causes ~21 point drop)",
            "uuids": [
                "e1968.0",
                "e1941.0",
                "e1936.0",
                "e1977.0",
                "e1956.3"
            ]
        },
        {
            "text": "Multi-view and diverse visual pretraining improves grounding robustness: π0 with wrist-camera pretraining shows lower adversarial vulnerability (FR increases ~31.4% under EDPA vs OpenVLA ~100%); OpenVLA-OFT with multi-camera fine-tuning reduces EDPA-induced failures; Gondola's multi-view inputs substantially reduce occlusion-driven grounding errors; ProxyTransformation using multi-view features outperforms single-view approaches",
            "uuids": [
                "e1975.2",
                "e1975.1",
                "e1958.0",
                "e1936.0"
            ]
        },
        {
            "text": "Partially-frozen dual encoder approaches effectively preserve pretrained grounding while enabling task adaptation: OpenVLA+ partially-frozen dual encoder (one frozen anchor, one trainable) improves from 35.03% to 55.55% avg SR, outperforming naive full fine-tuning; combined with string tokenizer and co-training yields 78.46% (+43.43 points); EF-VLA using frozen CLIP encoders achieves +20% on compositional tasks and 85% success on unseen goal descriptions",
            "uuids": [
                "e1947.2",
                "e1982.3"
            ]
        },
        {
            "text": "Action-space grounding is distinct bottleneck requiring alignment between language and executable commands: OpenVLA+ string tokenizer alone yields +15.22 points (35.03%→50.25%) by aligning actions with language tokens; combined with co-training yields +43.14 points; VLM-VLA misalignment causes execution failures despite correct high-level grounding (e.g., 'Oreo' vs 'biscuit box'); FAST tokenization enables 15× faster inference and 200 Hz control by compressing action sequences",
            "uuids": [
                "e1947.3",
                "e1962.3",
                "e1982.11"
            ]
        },
        {
            "text": "Temporal/trajectory grounding requires different mechanisms than single-step localization: PIO benchmark shows models excel at S1/S2 single-target grounding but fail at S3 trajectory prediction; grounding-finetuned models (MoLMO, Qwen-2.5-VL, RoboRefer) strong on S1/S2 but weak on S3; OneTwoVLA's explicit textual reasoning improves long-horizon tasks (+30% over π0 baseline: 87% vs 57%); Gemini-2.5-Pro succeeds at S3 planning despite weaker S1/S2 precision",
            "uuids": [
                "e1972.0",
                "e1972.3",
                "e1972.4",
                "e1972.5",
                "e1965.0",
                "e1972.7"
            ]
        },
        {
            "text": "Memory systems critical for sequential grounding but depend on input quality: MTU3D's spatial memory bank contributes substantially to lifelong navigation (+13.7% SR on HM3D-OVON, +23.0% on GOAT-Bench); VEME's spatial semantic memory yields ~+9.8 SPL when included; ISR (implicit scene representation) reduces redundant geometric details that impede grounding; however, memory systems depend on clean point clouds and accurate trajectories",
            "uuids": [
                "e1977.2",
                "e1956.1",
                "e1933.1",
                "e1977.0"
            ]
        },
        {
            "text": "Explicit reasoning and chain-of-thought enhance grounding for complex spatial reasoning: EmbodiedVSR's physics-constrained CoT reduces geometric hallucinations and improves spatial reasoning on MMBench (+10.9 EN, +10.6 CN V11 when adding scene graph); Robix's thought-action consistency training via RL improves grounding and reduces irrational steps (+3.3 to +8.3 points on Internal OOD); RFT with free-form CoT yields +28.65 points over SFT on Where2Place grounding benchmark",
            "uuids": [
                "e1941.0",
                "e1962.0",
                "e1979.7"
            ]
        },
        {
            "text": "Grounding is necessary but not sufficient for task success: ROBOGROUND achieves 89% contact rate but only 43.3% success on pick-and-place (46-point gap), showing localization doesn't guarantee manipulation; VLM-VLA misalignment causes execution failures despite correct visual grounding; dual-system approaches face latency (Groot N1: ~800ms LLM vs ~10ms control) and coordination issues despite +17% SR and -28% collisions",
            "uuids": [
                "e1943.0",
                "e1962.3",
                "e1965.3",
                "e1982.4"
            ]
        },
        {
            "text": "Synthetic and procedural data generation with domain-matching augmentations effectively scales grounding supervision: OneTwoVLA's synthetic VL pipeline (16K images with fisheye distortion and gripper compositing) yields +65% open-world grounding improvement (8%→73%); OXE-MAP procedural conversion of trajectories to heatmaps enables effective training; DenseGrounding's LLM-grounded text augmentation with scene relationships and locations yields +3.48% ACC@25",
            "uuids": [
                "e1965.4",
                "e1978.4",
                "e1968.4"
            ]
        },
        {
            "text": "Adversarial robustness reveals systematic grounding vulnerabilities: EDPA embedding-space attacks cause near-100% failure by disrupting image-instruction alignment; attacks transfer across models (OpenVLA ~49.8% avg FR, OpenVLA-OFT ~26.98%, π0 ~9.3%) and datasets (~74.7% cross-dataset transfer); adversarial fine-tuning of visual encoder reduces vulnerability by ~34.2% on average; diverse pretraining (wrist-camera) improves robustness",
            "uuids": [
                "e1975.3",
                "e1975.0"
            ]
        },
        {
            "text": "Grounding representation format affects accuracy-efficiency trade-offs: RoboMAP's adaptive heatmaps outperform fixed-shape heatmaps and point methods (+9-14% on benchmarks) with 15× faster inference than iterative methods like Embodied-R1; Gondola's end-to-end mask generation outperforms box-based grounding by +20-25 IoU points; normalized IoU metric needed to fairly compare boxes vs points vs masks",
            "uuids": [
                "e1978.1",
                "e1958.0",
                "e1978.10",
                "e1972.9"
            ]
        },
        {
            "text": "Quantization and numerical precision disproportionately affect high-level grounding: INT4 quantization of Llama 3.2 Vision 11B causes 14-17% accuracy drops on implicit and relational grounding but only ~4% loss on attribute recognition; string-based action tokenization preserves fine-grained spatial values while enabling language alignment",
            "uuids": [
                "e1981.6",
                "e1947.3"
            ]
        }
    ],
    "theory_statements": [
        "Perception-language grounding quality is a primary bottleneck for embodied AI transfer, often more limiting than language understanding capabilities alone, with perception errors accounting for &gt;50% of failures even when language models are strong",
        "Visual encoders pretrained with language supervision through large-scale contrastive learning provide better grounding than purely visual pretraining, with grounding quality scaling with both pretraining data scale and domain-relevance",
        "Object-centric or region-level visual representations enable substantially better grounding than global image features by providing explicit associations between language tokens and visual entities",
        "Distribution shift between pretraining visual data (web images, third-person views) and embodied observations (egocentric views, partial observability, occlusion) severely degrades grounding quality",
        "Multi-view and diverse pretraining data (including wrist-camera, egocentric videos) improves grounding robustness to viewpoint changes, occlusion, and adversarial perturbations",
        "Explicit spatial representations (3D coordinates, depth, spatial relations, multi-scale features) improve grounding for embodied tasks compared to purely appearance-based 2D features",
        "Cross-modal fusion mechanisms (cross-attention, FiLM conditioning, hierarchical semantic enhancement) are necessary to effectively combine language and visual information",
        "Partially-frozen encoder approaches that preserve one frozen copy while adapting another effectively balance retention of pretrained grounding with task-specific adaptation, outperforming both full freezing and naive full fine-tuning",
        "Action-space grounding—the mapping from grounded perceptions to executable motor commands—is a distinct bottleneck requiring explicit alignment between language tokens and action representations",
        "Temporal grounding for sequential trajectory prediction requires different mechanisms than single-step localization; models can excel at one while failing at the other",
        "Explicit reasoning and chain-of-thought scaffolds enhance grounding for complex spatial reasoning, long-horizon tasks, and handling scene-task contradictions",
        "Memory systems (spatial memory banks, episodic memory, implicit scene representations) are critical for maintaining grounding across time in sequential tasks, but depend on input quality",
        "Grounding quality is necessary but not sufficient for task success—execution precision, system coordination, real-time constraints, and action-space alignment impose additional bottlenecks",
        "Grounding representation format (heatmaps vs points vs boxes vs masks) affects both accuracy and computational efficiency, with adaptive representations achieving better trade-offs",
        "Grounding quality is sensitive to representation precision: quantization disproportionately harms high-level grounding (relational, implicit) compared to simple attribute recognition",
        "Adversarial robustness is a dimension of grounding quality; embedding-space attacks exploit alignment vulnerabilities, and diverse pretraining plus adversarial fine-tuning improve robustness",
        "Synthetic and procedural data generation with domain-matching augmentations can effectively scale grounding supervision and improve open-world generalization",
        "Grounding quality degrades more severely for novel objects, rare viewpoints, complex spatial configurations, and out-of-distribution appearances not well-represented in pretraining data",
        "Multi-level grounding (perception-language, action-space, temporal, memory) provides more robust transfer than single-level approaches, but requires careful coordination to avoid latency and consistency issues"
    ],
    "new_predictions_likely": [
        "Pretraining visual encoders on large-scale egocentric video datasets (e.g., Ego4D) with multi-view consistency objectives should improve robustness to viewpoint changes and occlusion compared to third-person web images, especially for manipulation tasks",
        "Incorporating depth information or 3D structure during vision-language pretraining should improve transfer to 3D embodied tasks, particularly for spatial reasoning and navigation tasks requiring metric understanding",
        "Object-centric pretraining with explicit object annotations should improve grounding for manipulation tasks compared to scene-level pretraining, especially for fine-grained object discrimination in clutter",
        "Domain-adaptive pretraining using synthetic data with domain-matching augmentations (fisheye distortion, gripper compositing, lighting variation) should reduce perception domain gap and improve sim-to-real transfer",
        "Video-language pretraining should provide better temporal grounding and trajectory prediction compared to static image-text pretraining for dynamic embodied tasks",
        "Adversarial training targeting embedding-space alignment should improve robustness to both adversarial attacks and natural distribution shifts in embodied settings",
        "Combining adaptive heatmap representations with action chunking should enable both efficient grounding and high-frequency control (&gt;100 Hz) for real-time manipulation",
        "Memory systems that explicitly track object permanence and spatial relations across occlusions should improve long-horizon task performance in partially observable environments",
        "Partially-frozen dual-encoder approaches combined with string-based action tokenization and vision-language co-training should enable effective transfer with minimal catastrophic forgetting across diverse robot platforms"
    ],
    "new_predictions_unknown": [
        "Whether synthetic data with perfect annotations but limited photorealism can provide sufficient visual diversity for grounding despite domain gap, or if photorealism is necessary for robust transfer to real-world embodied tasks",
        "Whether grounding can be learned purely from language-annotated videos without explicit object/region annotations, or if structured supervision is necessary for object-centric grounding quality",
        "Whether there is a fundamental limit to grounding quality achievable from static pretraining vs. interactive embodied experience with closed-loop feedback and online adaptation",
        "Whether grounding quality continues to improve with scale beyond current largest models (100B+ parameters) or if there are diminishing returns, and whether architectural improvements matter more than scale",
        "Whether grounding learned in one embodiment (robotic arm) transfers effectively to different embodiments (quadruped, humanoid) or if embodiment-specific grounding is required",
        "Whether world models and latent planning can fully compensate for single-frame grounding limitations, or if explicit multi-frame memory is necessary for long-horizon tasks",
        "Whether adversarial training on embedding-space attacks improves robustness to natural distribution shifts, or if these are fundamentally different challenges requiring separate solutions",
        "Whether unified architectures that jointly perform reasoning and acting can scale to very long-horizon tasks (100+ steps) without facing memory or latency constraints that degrade performance",
        "Whether temporal grounding for trajectory prediction can be learned from static image-text pairs with motion-related language, or requires video-based pretraining with temporal supervision",
        "Whether the contact-to-success gap in manipulation (e.g., 89% contact but 43% success) can be closed through improved grounding alone, or requires fundamentally different approaches to grasp pose estimation and force control"
    ],
    "negative_experiments": [
        "Demonstrating successful transfer using random visual features (no pretraining) would challenge the importance of learned visual representations for grounding",
        "Showing that language-only models (no visual input) can achieve comparable performance on embodied manipulation tasks would challenge the necessity of perception-language grounding",
        "Finding that increasing visual encoder capacity or pretraining scale does not improve grounding quality beyond a certain point would challenge the scalability of current approaches",
        "Demonstrating that explicit object annotations during pretraining do not improve grounding compared to image-level captions would challenge the importance of object-centric representations",
        "Showing that domain-specific pretraining does not improve transfer compared to general web pretraining would challenge the importance of domain alignment",
        "Finding that frozen pretrained encoders perform as well as partially-frozen or fine-tuned encoders would challenge the need for task-specific adaptation",
        "Demonstrating that 2D visual features are sufficient for 3D embodied tasks would challenge the importance of explicit 3D representations",
        "Showing that single-step grounding mechanisms work equally well for temporal/trajectory prediction would challenge the need for specialized temporal grounding mechanisms",
        "Finding that grounding improvements do not translate to execution success improvements would challenge grounding as a primary bottleneck (though evidence suggests grounding is necessary but not sufficient)",
        "Demonstrating that adversarial training does not improve robustness to natural distribution shifts would challenge the connection between adversarial and natural robustness",
        "Showing that memory systems do not improve performance on long-horizon tasks would challenge their necessity for temporal grounding",
        "Finding that explicit reasoning modules do not reduce hallucinations or improve spatial reasoning would challenge their role in grounding enhancement",
        "Demonstrating that synthetic data generation does not improve open-world grounding would challenge its effectiveness for scaling supervision"
    ],
    "unaccounted_for": [
        {
            "text": "How to effectively handle the long-tail distribution of objects and scenes in embodied environments that are rare or absent in both pretraining data and synthetic data generation",
            "uuids": []
        },
        {
            "text": "The precise mechanisms by which explicit reasoning and chain-of-thought improve grounding quality, and whether these can be learned end-to-end or require structured supervision and physics constraints",
            "uuids": []
        },
        {
            "text": "How grounding quality interacts with partial observability and the need to maintain object permanence across time steps, beyond current memory system approaches",
            "uuids": []
        },
        {
            "text": "The optimal balance between grounding quality and computational efficiency for real-time embodied control, and whether this trade-off is fundamental or can be overcome through architectural innovations",
            "uuids": []
        },
        {
            "text": "How to balance the trade-off between single-step grounding precision and temporal/trajectory grounding in unified architectures without sacrificing either capability",
            "uuids": []
        },
        {
            "text": "The extent to which grounding can be improved through online learning or adaptation during deployment vs. requiring offline pretraining, and optimal strategies for continual learning without catastrophic forgetting",
            "uuids": []
        },
        {
            "text": "How action-space grounding interacts with perception grounding, and whether they can be optimized jointly or require separate mechanisms and training stages",
            "uuids": []
        },
        {
            "text": "The relationship between adversarial robustness and robustness to natural distribution shifts, and whether improving one necessarily improves the other or requires different approaches",
            "uuids": []
        },
        {
            "text": "How grounding degrades with increasing task complexity and longer time horizons, and whether there are fundamental limits to grounding-based approaches for very long-horizon tasks (100+ steps)",
            "uuids": []
        },
        {
            "text": "The role of proprioceptive and force feedback in grounding for manipulation tasks, beyond visual and language grounding, and how to integrate these modalities",
            "uuids": []
        },
        {
            "text": "How to effectively coordinate multiple levels of grounding (perception, action, temporal, memory) in unified architectures without latency or coordination failures that degrade performance",
            "uuids": []
        },
        {
            "text": "The mechanisms underlying the large contact-to-success gap in manipulation (e.g., 89% contact but 43% success), and whether this is fundamentally a grounding problem or requires different approaches",
            "uuids": []
        },
        {
            "text": "How to handle scene-task contradictions and infeasible task specifications, as models struggle to refuse or replan even with explicit feedback about missing objects",
            "uuids": []
        }
    ],
    "change_log": [
        "Renamed theory to 'Multi-Level Perception-Language-Action Grounding Theory' to emphasize the four interconnected grounding dimensions",
        "Restructured theory description to clearly delineate four grounding levels: (1) Perception-Language, (2) Action-Space, (3) Temporal, and (4) Spatial-Semantic Memory",
        "Emphasized that grounding is necessary but not sufficient for task success, with execution precision and system coordination as additional bottlenecks",
        "Reorganized supporting evidence into 16 more focused clusters with clearer themes and more precise quantitative results",
        "Added explicit evidence for action-space grounding as distinct bottleneck (string tokenizer, VLM-VLA misalignment, FAST compression)",
        "Added explicit evidence for temporal grounding requiring different mechanisms (PIO S1/S2 vs S3 results, model-specific strengths)",
        "Strengthened evidence for grounding being necessary but not sufficient (ROBOGROUND contact-success gap, VLM-VLA misalignment, dual-system coordination issues)",
        "Split theory statements into 19 more focused statements, separating previously combined concepts",
        "Added statement distinguishing temporal grounding from single-step localization as requiring different mechanisms",
        "Added statement about multi-level grounding requiring careful coordination to avoid latency and consistency issues",
        "Refined predictions to be more specific and testable, with clearer expected outcomes",
        "Added predictions about contact-to-success gap and whether it can be closed through improved grounding alone",
        "Added negative experiments testing synthetic data generation, memory systems, and reasoning modules",
        "Expanded unaccounted_for items to include contact-success gap mechanisms, scene-task contradiction handling, and multi-level coordination challenges",
        "Added unaccounted_for item about proprioceptive and force feedback integration",
        "Improved precision of quantitative results throughout (e.g., specific percentage improvements, exact model comparisons)",
        "Clarified that partially-frozen approaches outperform both full freezing and naive full fine-tuning",
        "Added evidence for quantization effects being disproportionate on high-level vs simple grounding"
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1884</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1884</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that LLM performance is maximized when the problem presentation format aligns with the model's pretraining distribution and internal cognitive priors. Formats that match the linguistic, structural, and logical patterns the LLM has seen during training facilitate better comprehension and reasoning, while unfamiliar or misaligned formats reduce performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Pretraining Distribution Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; matches &#8594; LLM_pretraining_distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_maximized &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best on tasks and formats similar to those seen during pretraining (e.g., question-answering, Wikipedia-style text). </li>
    <li>Performance drops when prompts are presented in unfamiliar or nonstandard formats. </li>
    <li>Instruction tuning improves LLM performance on tasks with formats similar to those used in instruction datasets. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect is known, but the explicit alignment framing and predictive law are novel.</p>            <p><strong>What Already Exists:</strong> The importance of pretraining data distribution for downstream performance is well established.</p>            <p><strong>What is Novel:</strong> This law formalizes the effect as a direct alignment between presentation format and cognitive priors, and predicts performance as a function of this alignment.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [Instruction tuning and format alignment]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Pretraining distribution and prompt format effects]</li>
</ul>
            <h3>Statement 1: Format Familiarity Gradient Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; has_familiarity_score &#8594; f<span style="color: #888888;">, and</span></div>
        <div>&#8226; f &#8594; greater_than &#8594; g</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance_on_format_f &#8594; greater_than &#8594; LLM_performance_on_format_g</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs show graded performance based on how closely the prompt matches familiar patterns. </li>
    <li>Rewriting prompts to use more common or canonical phrasing improves LLM accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect is observed, but the law's formalization and predictive structure are novel.</p>            <p><strong>What Already Exists:</strong> Prompt format and familiarity effects are observed in LLMs.</p>            <p><strong>What is Novel:</strong> The explicit formulation of a familiarity gradient and its predictive relationship to performance is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and performance]</li>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [Instruction tuning and format effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a problem is reformatted to match the style of the LLM's pretraining data, performance will improve.</li>
                <li>LLMs will perform worse on tasks presented in novel or synthetic formats not seen during pretraining.</li>
                <li>Instruction-tuned LLMs will outperform base LLMs on instruction-style prompts but not on unfamiliar formats.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are exposed to adversarially unfamiliar formats during finetuning, can they generalize to new formats, or does performance remain format-dependent?</li>
                <li>Does the effect of format familiarity persist for very large LLMs, or do they develop format-invariant reasoning?</li>
                <li>Can LLMs be trained to explicitly recognize and adapt to unfamiliar formats in real time?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on familiar and unfamiliar formats, the theory would be challenged.</li>
                <li>If instruction tuning does not improve performance on instruction-style prompts, the alignment law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs generalize well to new formats despite lack of pretraining exposure. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known effects into a formal alignment framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and performance]</li>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [Instruction tuning and format effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that LLM performance is maximized when the problem presentation format aligns with the model's pretraining distribution and internal cognitive priors. Formats that match the linguistic, structural, and logical patterns the LLM has seen during training facilitate better comprehension and reasoning, while unfamiliar or misaligned formats reduce performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Pretraining Distribution Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "matches",
                        "object": "LLM_pretraining_distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_maximized",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best on tasks and formats similar to those seen during pretraining (e.g., question-answering, Wikipedia-style text).",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops when prompts are presented in unfamiliar or nonstandard formats.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction tuning improves LLM performance on tasks with formats similar to those used in instruction datasets.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The importance of pretraining data distribution for downstream performance is well established.",
                    "what_is_novel": "This law formalizes the effect as a direct alignment between presentation format and cognitive priors, and predicts performance as a function of this alignment.",
                    "classification_explanation": "The effect is known, but the explicit alignment framing and predictive law are novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [Instruction tuning and format alignment]",
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Pretraining distribution and prompt format effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Format Familiarity Gradient Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "has_familiarity_score",
                        "object": "f"
                    },
                    {
                        "subject": "f",
                        "relation": "greater_than",
                        "object": "g"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance_on_format_f",
                        "relation": "greater_than",
                        "object": "LLM_performance_on_format_g"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs show graded performance based on how closely the prompt matches familiar patterns.",
                        "uuids": []
                    },
                    {
                        "text": "Rewriting prompts to use more common or canonical phrasing improves LLM accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt format and familiarity effects are observed in LLMs.",
                    "what_is_novel": "The explicit formulation of a familiarity gradient and its predictive relationship to performance is new.",
                    "classification_explanation": "The effect is observed, but the law's formalization and predictive structure are novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and performance]",
                        "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [Instruction tuning and format effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a problem is reformatted to match the style of the LLM's pretraining data, performance will improve.",
        "LLMs will perform worse on tasks presented in novel or synthetic formats not seen during pretraining.",
        "Instruction-tuned LLMs will outperform base LLMs on instruction-style prompts but not on unfamiliar formats."
    ],
    "new_predictions_unknown": [
        "If LLMs are exposed to adversarially unfamiliar formats during finetuning, can they generalize to new formats, or does performance remain format-dependent?",
        "Does the effect of format familiarity persist for very large LLMs, or do they develop format-invariant reasoning?",
        "Can LLMs be trained to explicitly recognize and adapt to unfamiliar formats in real time?"
    ],
    "negative_experiments": [
        "If LLMs perform equally well on familiar and unfamiliar formats, the theory would be challenged.",
        "If instruction tuning does not improve performance on instruction-style prompts, the alignment law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs generalize well to new formats despite lack of pretraining exposure.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show surprising robustness to format changes, especially with chain-of-thought prompting.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very large LLMs may develop partial format invariance.",
        "Explicit meta-prompting (e.g., 'You will see a new format...') can sometimes mitigate format unfamiliarity."
    ],
    "existing_theory": {
        "what_already_exists": "The importance of pretraining data and prompt format is established.",
        "what_is_novel": "The explicit cognitive alignment framing and the familiarity gradient law are new.",
        "classification_explanation": "The theory synthesizes known effects into a formal alignment framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and performance]",
            "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [Instruction tuning and format effects]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-652",
    "original_theory_name": "Prompt Format as a High-Dimensional Control Signal for LLM Computation",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
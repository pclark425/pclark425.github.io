<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Competency Tool-Use Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-109</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-109</p>
                <p><strong>Name:</strong> Multi-Competency Tool-Use Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory explaining the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents, and the architectural/training interventions required to close the gap, based on the following results.</p>
                <p><strong>Description:</strong> Effective tool use in LLM agents requires five distinct but interrelated competencies: (1) Tool awareness - recognizing when parametric knowledge is insufficient and external tools are needed, (2) Tool selection - choosing appropriate tool(s) from available options based on task requirements and constraints, (3) Tool invocation - correctly formatting arguments, handling outputs, and managing execution, (4) Tool composition - chaining multiple tools and managing dependencies in multi-step workflows, and (5) Error recovery - detecting failures, diagnosing errors, and adapting tool usage accordingly. The QA-interactive gap manifests differently across these competencies: models may excel at knowledge retrieval (QA) but fail at tool awareness (overconfidence in parametric knowledge), tool selection (choosing wrong tools or missing relevant ones), tool invocation (argument formatting errors, hallucinated parameters), tool composition (incorrect sequencing, missing dependencies), or error recovery (inability to adapt after failures). The theory predicts that: (a) interventions targeting specific competencies will show domain-specific improvements, (b) all five competencies must be addressed for robust tool-augmented performance, (c) execution feedback is more effective than static training for invocation and error recovery, (d) tool documentation quality primarily affects selection and invocation but not awareness, and (e) tool-use skills show limited transfer across different tool interfaces and domains without explicit training on diverse tool sets.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Tool-use competence requires five distinct capabilities: awareness (knowing when tools are needed), selection (choosing appropriate tools), invocation (correct argument formatting and output handling), composition (chaining multiple tools), and error recovery (detecting and adapting to failures)</li>
                <li>Tool awareness involves recognizing when parametric knowledge is insufficient and external tools are needed, often manifesting as overconfidence in models with strong QA performance</li>
                <li>Tool selection requires understanding tool capabilities, constraints, applicability, and documentation, with performance depending heavily on tool description quality</li>
                <li>Tool invocation requires precise argument formatting, output handling, and execution management, with code interfaces generally outperforming natural language interfaces</li>
                <li>Tool composition requires managing dependencies, sequencing, and state across multiple tool calls, with failures increasing non-linearly with the number of tools</li>
                <li>Error recovery requires detecting failures, diagnosing root causes, and adapting tool usage, with execution feedback being more effective than static training</li>
                <li>The QA-interactive gap manifests differently across these five competencies, with models potentially excelling at some while failing at others</li>
                <li>Models can have strong parametric knowledge (QA) but poor tool awareness due to overconfidence in their own knowledge</li>
                <li>Execution feedback is more effective than static training for improving tool invocation and error recovery</li>
                <li>Tool documentation quality primarily affects tool selection and invocation but has limited impact on tool awareness</li>
                <li>All five competencies must be addressed for robust tool-augmented performance, with weakness in any competency limiting overall effectiveness</li>
                <li>Training interventions can target specific competencies, with different training methods showing different competency profiles</li>
                <li>Tool-use skills show limited transfer across different tool interfaces (e.g., API vs. code) and domains without explicit training on diverse tool sets</li>
                <li>Tool composition difficulty increases non-linearly with the number of tools and complexity of dependencies</li>
                <li>Interactive learning with execution feedback is more effective than static training for tool invocation and error recovery</li>
                <li>Tool-making (creating new tools) can improve task-specific tool use but shows domain-dependent effectiveness</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>MetaTool evaluation reveals systematic failures in tool awareness (knowing when to use tools) and tool selection (choosing appropriate tools), with models showing high variance across these competencies <a href="../results/extraction-result-902.html#e902.0" class="evidence-link">[e902.0]</a> </li>
    <li>ToolTalk demonstrates failures across multiple competencies: premature tool calls (awareness), faulty planning (selection), and incorrect argument invocations (invocation), with documentation ablations showing importance of tool descriptions for selection and invocation <a href="../results/extraction-result-813.html#e813.0" class="evidence-link">[e813.0]</a> <a href="../results/extraction-result-813.html#e813.1" class="evidence-link">[e813.1]</a> <a href="../results/extraction-result-813.html#e813.2" class="evidence-link">[e813.2]</a> </li>
    <li>ToolQA shows failures in tool composition (multi-tool sequencing) and argument handling (invocation), with ReAct achieving best performance through iterative tool use and feedback <a href="../results/extraction-result-839.html#e839.2" class="evidence-link">[e839.2]</a> <a href="../results/extraction-result-839.html#e839.5" class="evidence-link">[e839.5]</a> </li>
    <li>AVATAR demonstrates that batch-wise contrastive reasoning improves tool selection by identifying systematic flaws in decomposition and tool choice across multiple examples <a href="../results/extraction-result-819.html#e819.0" class="evidence-link">[e819.0]</a> <a href="../results/extraction-result-819.html#e819.1" class="evidence-link">[e819.1]</a> <a href="../results/extraction-result-819.html#e819.3" class="evidence-link">[e819.3]</a> </li>
    <li>ConAgents shows that specialized execution agent improves tool invocation accuracy through dedicated code generation and review mechanisms <a href="../results/extraction-result-832.html#e832.0" class="evidence-link">[e832.0]</a> <a href="../results/extraction-result-832.html#e832.1" class="evidence-link">[e832.1]</a> <a href="../results/extraction-result-832.html#e832.5" class="evidence-link">[e832.5]</a> </li>
    <li>ToolLLaMA demonstrates that training on diverse APIs (16k+ tools) with DFSDT search improves tool generalization and composition through exposure to varied tool interfaces <a href="../results/extraction-result-850.html#e850.6" class="evidence-link">[e850.6]</a> <a href="../results/extraction-result-850.html#e850.9" class="evidence-link">[e850.9]</a> </li>
    <li>ToolAlpaca shows that simulated tool-use training on 3.9k instances improves all competencies, with massive gains in both simulated and real-world tool use <a href="../results/extraction-result-918.html#e918.2" class="evidence-link">[e918.2]</a> <a href="../results/extraction-result-918.html#e918.3" class="evidence-link">[e918.3]</a> </li>
    <li>Toolformer demonstrates that self-supervised learning can teach tool invocation by generating and filtering tool-call examples based on perplexity reduction <a href="../results/extraction-result-905.html#e905.2" class="evidence-link">[e905.2]</a> <a href="../results/extraction-result-939.html#e939.1" class="evidence-link">[e939.1]</a> <a href="../results/extraction-result-950.html#e950.9" class="evidence-link">[e950.9]</a> </li>
    <li>ART shows that tool library organization and program-format decomposition improve tool selection and composition through structured reasoning <a href="../results/extraction-result-946.html#e946.3" class="evidence-link">[e946.3]</a> <a href="../results/extraction-result-946.html#e946.5" class="evidence-link">[e946.5]</a> </li>
    <li>PAL demonstrates that code interpreter interface improves computational tool invocation by offloading precise operations to external executors <a href="../results/extraction-result-938.html#e938.0" class="evidence-link">[e938.0]</a> <a href="../results/extraction-result-938.html#e938.4" class="evidence-link">[e938.4]</a> <a href="../results/extraction-result-938.html#e938.5" class="evidence-link">[e938.5]</a> <a href="../results/extraction-result-950.html#e950.3" class="evidence-link">[e950.3]</a> </li>
    <li>ReAct shows that interleaving reasoning and actions improves tool awareness and selection by providing explicit reasoning traces about when and which tools to use <a href="../results/extraction-result-848.html#e848.1" class="evidence-link">[e848.1]</a> <a href="../results/extraction-result-848.html#e848.2" class="evidence-link">[e848.2]</a> <a href="../results/extraction-result-823.html#e823.1" class="evidence-link">[e823.1]</a> <a href="../results/extraction-result-910.html#e910.3" class="evidence-link">[e910.3]</a> <a href="../results/extraction-result-948.html#e948.3" class="evidence-link">[e948.3]</a> </li>
    <li>EHRAgent demonstrates that code interface with rubber duck debugging improves both tool invocation accuracy and error recovery through execution feedback and error tracing <a href="../results/extraction-result-844.html#e844.0" class="evidence-link">[e844.0]</a> <a href="../results/extraction-result-844.html#e844.5" class="evidence-link">[e844.5]</a> <a href="../results/extraction-result-844.html#e844.8" class="evidence-link">[e844.8]</a> </li>
    <li>PDoctor reveals systematic tool-call errors (Action Lost, hallucinated parameters) and planning failures that increase with task complexity, showing failures across awareness, selection, and invocation <a href="../results/extraction-result-836.html#e836.1" class="evidence-link">[e836.1]</a> </li>
    <li>Mobile-Bench shows hallucinated API calls as a key failure mode in tool invocation, with ablations demonstrating importance of API access for multi-app tasks <a href="../results/extraction-result-901.html#e901.0" class="evidence-link">[e901.0]</a> <a href="../results/extraction-result-901.html#e901.1" class="evidence-link">[e901.1]</a> </li>
    <li>TravelPlanner demonstrates tool argument errors and incomplete information collection, showing failures in tool composition and awareness of information needs <a href="../results/extraction-result-916.html#e916.2" class="evidence-link">[e916.2]</a> <a href="../results/extraction-result-916.html#e916.5" class="evidence-link">[e916.5]</a> <a href="../results/extraction-result-916.html#e916.7" class="evidence-link">[e916.7]</a> </li>
    <li>WebGPT shows that browser-assisted QA with tool interface (search, click, quote) and training via BC, RM, and PPO improves tool use, with rejection sampling providing largest gains <a href="../results/extraction-result-925.html#e925.0" class="evidence-link">[e925.0]</a> <a href="../results/extraction-result-925.html#e925.1" class="evidence-link">[e925.1]</a> <a href="../results/extraction-result-925.html#e925.3" class="evidence-link">[e925.3]</a> <a href="../results/extraction-result-921.html#e921.0" class="evidence-link">[e921.0]</a> </li>
    <li>ReHAC demonstrates learned collaboration policy for deciding when to use agent tools vs. request human intervention, showing tool awareness can be learned via RL <a href="../results/extraction-result-847.html#e847.0" class="evidence-link">[e847.0]</a> </li>
    <li>FlowMind shows that user feedback loop improves tool composition and error recovery by allowing iterative refinement of generated workflows <a href="../results/extraction-result-904.html#e904.2" class="evidence-link">[e904.2]</a> </li>
    <li>Coscientist demonstrates that documentation retrieval (docs searcher) is crucial for correct API usage and tool invocation, enabling correct hardware control <a href="../results/extraction-result-945.html#e945.1" class="evidence-link">[e945.1]</a> <a href="../results/extraction-result-945.html#e945.2" class="evidence-link">[e945.2]</a> <a href="../results/extraction-result-945.html#e945.3" class="evidence-link">[e945.3]</a> <a href="../results/extraction-result-945.html#e945.4" class="evidence-link">[e945.4]</a> <a href="../results/extraction-result-945.html#e945.5" class="evidence-link">[e945.5]</a> </li>
    <li>LATM shows that tool-making (creating task-specific tools) can improve tool use, with +29.1pp improvement on BigBench tasks <a href="../results/extraction-result-935.html#e935.4" class="evidence-link">[e935.4]</a> </li>
    <li>TroVE demonstrates efficient tool induction with execution feedback, achieving +21.0pp on MATH through execution-driven tool creation <a href="../results/extraction-result-935.html#e935.7" class="evidence-link">[e935.7]</a> </li>
    <li>CREATOR shows mixed results from per-example tool creation, with small gains on MATH but no improvement on table tasks, suggesting tool-making effectiveness varies by domain <a href="../results/extraction-result-935.html#e935.5" class="evidence-link">[e935.5]</a> </li>
    <li>Tool learning surveys identify awareness, selection, and invocation as key challenges, with additional emphasis on tool composition and error handling <a href="../results/extraction-result-907.html#e907.6" class="evidence-link">[e907.6]</a> <a href="../results/extraction-result-931.html#e931.0" class="evidence-link">[e931.0]</a> <a href="../results/extraction-result-931.html#e931.10" class="evidence-link">[e931.10]</a> <a href="../results/extraction-result-935.html#e935.4" class="evidence-link">[e935.4]</a> <a href="../results/extraction-result-935.html#e935.5" class="evidence-link">[e935.5]</a> <a href="../results/extraction-result-935.html#e935.7" class="evidence-link">[e935.7]</a> </li>
    <li>InterCode shows that interactive coding with execution feedback substantially improves tool invocation and error recovery compared to single-turn generation <a href="../results/extraction-result-947.html#e947.2" class="evidence-link">[e947.2]</a> <a href="../results/extraction-result-947.html#e947.6" class="evidence-link">[e947.6]</a> <a href="../results/extraction-result-947.html#e947.10" class="evidence-link">[e947.10]</a> </li>
    <li>Inner Monologue demonstrates that closed-loop feedback (success detection, scene descriptions) improves tool awareness and error recovery in embodied tasks <a href="../results/extraction-result-932.html#e932.0" class="evidence-link">[e932.0]</a> <a href="../results/extraction-result-932.html#e932.5" class="evidence-link">[e932.5]</a> </li>
    <li>ChatDB shows that symbolic memory (database) as tool interface improves multi-step reasoning and tool composition through structured state management <a href="../results/extraction-result-913.html#e913.0" class="evidence-link">[e913.0]</a> </li>
    <li>FISHNET demonstrates neural conditioning (priming with examples) and HalluciBot filtering improve tool awareness and reduce hallucination in tool invocation <a href="../results/extraction-result-824.html#e824.4" class="evidence-link">[e824.4]</a> </li>
    <li>StreamBench shows that continuous learning with memory (MemPrompt, Self-StreamICL) improves tool use over time through accumulation of successful examples <a href="../results/extraction-result-903.html#e903.2" class="evidence-link">[e903.2]</a> <a href="../results/extraction-result-903.html#e903.3" class="evidence-link">[e903.3]</a> </li>
    <li>LASER demonstrates that state-space exploration with backtracking improves tool selection and error recovery in web navigation <a href="../results/extraction-result-823.html#e823.1" class="evidence-link">[e823.1]</a> <a href="../results/extraction-result-823.html#e823.2" class="evidence-link">[e823.2]</a> <a href="../results/extraction-result-823.html#e823.5" class="evidence-link">[e823.5]</a> </li>
    <li>AgentTuning shows that hybrid instruction tuning (agent trajectories + general data) improves tool use while preserving general capabilities, addressing training trade-offs <a href="../results/extraction-result-820.html#e820.0" class="evidence-link">[e820.0]</a> <a href="../results/extraction-result-820.html#e820.1" class="evidence-link">[e820.1]</a> <a href="../results/extraction-result-820.html#e820.4" class="evidence-link">[e820.4]</a> </li>
    <li>DARA demonstrates that hierarchical decomposition improves tool composition in structured domains (KGQA) <a href="../results/extraction-result-841.html#e841.3" class="evidence-link">[e841.3]</a> </li>
    <li>Triad shows that multi-role LLM agents with KB memory improve tool selection and invocation in KBQA tasks <a href="../results/extraction-result-810.html#e810.1" class="evidence-link">[e810.1]</a> </li>
    <li>AGILE demonstrates that RL training of tool-use policy improves tool awareness (when to seek advice) and tool selection (when to use search/SQL) <a href="../results/extraction-result-816.html#e816.0" class="evidence-link">[e816.0]</a> <a href="../results/extraction-result-816.html#e816.3" class="evidence-link">[e816.3]</a> </li>
    <li>Retroformer shows that learned retrospective feedback improves error recovery and tool adaptation through policy gradient optimization <a href="../results/extraction-result-941.html#e941.0" class="evidence-link">[e941.0]</a> <a href="../results/extraction-result-941.html#e941.2" class="evidence-link">[e941.2]</a> </li>
    <li>Reflexion demonstrates that verbal self-reflection improves error recovery through episodic memory of failures <a href="../results/extraction-result-821.html#e821.1" class="evidence-link">[e821.1]</a> <a href="../results/extraction-result-910.html#e910.3" class="evidence-link">[e910.3]</a> <a href="../results/extraction-result-912.html#e912.4" class="evidence-link">[e912.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Training specifically on tool-awareness examples (when to use tools vs. rely on parametric knowledge) will improve tool-use initiation without necessarily improving selection, invocation, composition, or error recovery</li>
                <li>Providing better tool documentation will improve tool selection and invocation more than tool awareness, composition, or error recovery</li>
                <li>Models with stronger parametric knowledge will show worse tool awareness (more overconfident) than weaker models, leading to fewer tool invocations even when tools would help</li>
                <li>Tool invocation accuracy will improve more from execution feedback than from static training examples, with the gap widening for complex APIs</li>
                <li>Multi-tool tasks will show greater failures in tool composition and selection than single-tool tasks, with failure rates increasing non-linearly with tool count</li>
                <li>Code-based tool interfaces will show better invocation accuracy than natural language API interfaces due to more precise formatting requirements</li>
                <li>Tool-use training on diverse tool sets will improve generalization to unseen tools more than training on narrow tool sets</li>
                <li>Error recovery competency will improve more from interactive debugging (like rubber duck debugging) than from static error examples</li>
                <li>Tool composition will benefit more from hierarchical decomposition methods than from flat sequential planning</li>
                <li>Models trained with execution feedback will show better error recovery than models trained only on successful trajectories</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether tool awareness can be fully learned through training or requires architectural changes (e.g., explicit confidence estimation modules) to reliably detect knowledge boundaries</li>
                <li>The extent to which tool selection skills transfer across fundamentally different tool domains (e.g., from web APIs to database queries to code libraries)</li>
                <li>Whether tool invocation skills learned for one API format (e.g., REST) transfer effectively to other formats (e.g., GraphQL, gRPC) without additional training</li>
                <li>The optimal balance between tool-specific training and general tool-use training for maximizing both performance and generalization</li>
                <li>Whether future models can learn to create and use novel tools on-the-fly without explicit tool-making training</li>
                <li>The extent to which tool composition skills learned in one domain (e.g., web navigation) transfer to other domains (e.g., scientific workflows)</li>
                <li>Whether error recovery can be learned purely from successful trajectories or requires exposure to failure cases</li>
                <li>The degree to which tool-use competencies can be learned through self-supervised methods (like Toolformer) versus requiring human demonstrations or RL</li>
                <li>Whether tool awareness can be improved through calibration training or requires fundamental changes to model confidence estimation</li>
                <li>The extent to which tool-use skills degrade general language capabilities and whether this trade-off is fundamental or can be eliminated</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Demonstrating that tool awareness does not improve with training on diverse tool-use examples would challenge the learnability of this competency and suggest architectural changes are needed</li>
                <li>Showing that tool selection accuracy does not depend on tool documentation quality would question the importance of tool descriptions and suggest selection relies on other factors</li>
                <li>Finding that tool invocation errors persist despite extensive execution feedback would challenge the value of interactive learning for this competency</li>
                <li>Demonstrating that multi-tool tasks show no additional failures beyond single-tool tasks would question the complexity of tool composition as a distinct competency</li>
                <li>Showing that tool-use training does not improve generalization to unseen tools would challenge the transferability of tool-use skills and suggest tool-specific training is necessary</li>
                <li>Finding that error recovery does not improve with exposure to failure cases would challenge the importance of negative examples in learning</li>
                <li>Demonstrating that code interfaces show no advantage over natural language interfaces for tool invocation would question the importance of interface design</li>
                <li>Showing that tool composition difficulty does not increase with the number of tools would challenge the non-linear complexity hypothesis</li>
                <li>Finding that tool-use training consistently degrades general capabilities would suggest a fundamental trade-off that cannot be resolved</li>
                <li>Demonstrating that execution feedback provides no advantage over static training for any competency would challenge the importance of interactive learning</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some models (e.g., GPT-4) show good tool use without explicit tool-use training, suggesting strong base capabilities or implicit learning during pretraining <a href="../results/extraction-result-944.html#e944.1" class="evidence-link">[e944.1]</a> <a href="../results/extraction-result-944.html#e944.2" class="evidence-link">[e944.2]</a> <a href="../results/extraction-result-837.html#e837.4" class="evidence-link">[e837.4]</a> </li>
    <li>The relative importance of the five competencies varies significantly across tasks and domains, with some tasks primarily requiring awareness while others require composition <a href="../results/extraction-result-902.html#e902.0" class="evidence-link">[e902.0]</a> <a href="../results/extraction-result-839.html#e839.5" class="evidence-link">[e839.5]</a> </li>
    <li>Some tool-use failures are due to factors beyond the five competencies, such as context length limitations, computational constraints, or fundamental model limitations <a href="../results/extraction-result-836.html#e836.1" class="evidence-link">[e836.1]</a> <a href="../results/extraction-result-901.html#e901.0" class="evidence-link">[e901.0]</a> </li>
    <li>Tool-use performance can be affected by factors like prompt engineering, temperature settings, and sampling strategies that are orthogonal to the five competencies <a href="../results/extraction-result-840.html#e840.0" class="evidence-link">[e840.0]</a> <a href="../results/extraction-result-840.html#e840.1" class="evidence-link">[e840.1]</a> </li>
    <li>Some interventions (e.g., rejection sampling in WebGPT) improve tool use through inference-time compute rather than improving any specific competency <a href="../results/extraction-result-925.html#e925.0" class="evidence-link">[e925.0]</a> </li>
    <li>Human-in-the-loop approaches can substitute for or augment tool-use competencies in ways not captured by the five-competency model <a href="../results/extraction-result-847.html#e847.0" class="evidence-link">[e847.0]</a> <a href="../results/extraction-result-904.html#e904.2" class="evidence-link">[e904.2]</a> </li>
    <li>Tool interface design (e.g., function-calling APIs vs. natural language) affects tool use in ways that interact with but are distinct from the five competencies <a href="../results/extraction-result-813.html#e813.0" class="evidence-link">[e813.0]</a> <a href="../results/extraction-result-813.html#e813.1" class="evidence-link">[e813.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Qin et al. (2023) Tool Learning with Foundation Models [Survey identifying tool-use challenges including awareness, selection, and invocation, but not explicitly proposing five-competency model]</li>
    <li>Xu et al. (2023) MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use [Evaluates tool awareness and selection but does not propose comprehensive competency model]</li>
    <li>Qin et al. (2023) ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs [Demonstrates importance of diverse tool training but does not explicitly decompose competencies]</li>
    <li>Tang et al. (2023) ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases [Shows simulated training improves tool use but does not propose competency framework]</li>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [Demonstrates self-supervised tool learning but focuses on invocation rather than full competency model]</li>
    <li>Paranjape et al. (2023) ART: Automatic multi-step reasoning and tool-use for large language models [Shows importance of tool library and decomposition but does not explicitly model competencies]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [Demonstrates code interface benefits but does not propose general competency framework]</li>
    <li>Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [Shows interleaving reasoning and actions helps but does not decompose into competencies]</li>
    <li>Zhuang et al. (2024) ToolQA: A Dataset for LLM Question Answering with External Tools [Evaluates tool use but does not propose competency model]</li>
    <li>Nakano et al. (2021) WebGPT: Browser-assisted question-answering with human feedback [Demonstrates tool-use training methods but does not explicitly model competencies]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Competency Tool-Use Theory",
    "theory_description": "Effective tool use in LLM agents requires five distinct but interrelated competencies: (1) Tool awareness - recognizing when parametric knowledge is insufficient and external tools are needed, (2) Tool selection - choosing appropriate tool(s) from available options based on task requirements and constraints, (3) Tool invocation - correctly formatting arguments, handling outputs, and managing execution, (4) Tool composition - chaining multiple tools and managing dependencies in multi-step workflows, and (5) Error recovery - detecting failures, diagnosing errors, and adapting tool usage accordingly. The QA-interactive gap manifests differently across these competencies: models may excel at knowledge retrieval (QA) but fail at tool awareness (overconfidence in parametric knowledge), tool selection (choosing wrong tools or missing relevant ones), tool invocation (argument formatting errors, hallucinated parameters), tool composition (incorrect sequencing, missing dependencies), or error recovery (inability to adapt after failures). The theory predicts that: (a) interventions targeting specific competencies will show domain-specific improvements, (b) all five competencies must be addressed for robust tool-augmented performance, (c) execution feedback is more effective than static training for invocation and error recovery, (d) tool documentation quality primarily affects selection and invocation but not awareness, and (e) tool-use skills show limited transfer across different tool interfaces and domains without explicit training on diverse tool sets.",
    "supporting_evidence": [
        {
            "text": "MetaTool evaluation reveals systematic failures in tool awareness (knowing when to use tools) and tool selection (choosing appropriate tools), with models showing high variance across these competencies",
            "uuids": [
                "e902.0"
            ]
        },
        {
            "text": "ToolTalk demonstrates failures across multiple competencies: premature tool calls (awareness), faulty planning (selection), and incorrect argument invocations (invocation), with documentation ablations showing importance of tool descriptions for selection and invocation",
            "uuids": [
                "e813.0",
                "e813.1",
                "e813.2"
            ]
        },
        {
            "text": "ToolQA shows failures in tool composition (multi-tool sequencing) and argument handling (invocation), with ReAct achieving best performance through iterative tool use and feedback",
            "uuids": [
                "e839.2",
                "e839.5"
            ]
        },
        {
            "text": "AVATAR demonstrates that batch-wise contrastive reasoning improves tool selection by identifying systematic flaws in decomposition and tool choice across multiple examples",
            "uuids": [
                "e819.0",
                "e819.1",
                "e819.3"
            ]
        },
        {
            "text": "ConAgents shows that specialized execution agent improves tool invocation accuracy through dedicated code generation and review mechanisms",
            "uuids": [
                "e832.0",
                "e832.1",
                "e832.5"
            ]
        },
        {
            "text": "ToolLLaMA demonstrates that training on diverse APIs (16k+ tools) with DFSDT search improves tool generalization and composition through exposure to varied tool interfaces",
            "uuids": [
                "e850.6",
                "e850.9"
            ]
        },
        {
            "text": "ToolAlpaca shows that simulated tool-use training on 3.9k instances improves all competencies, with massive gains in both simulated and real-world tool use",
            "uuids": [
                "e918.2",
                "e918.3"
            ]
        },
        {
            "text": "Toolformer demonstrates that self-supervised learning can teach tool invocation by generating and filtering tool-call examples based on perplexity reduction",
            "uuids": [
                "e905.2",
                "e939.1",
                "e950.9"
            ]
        },
        {
            "text": "ART shows that tool library organization and program-format decomposition improve tool selection and composition through structured reasoning",
            "uuids": [
                "e946.3",
                "e946.5"
            ]
        },
        {
            "text": "PAL demonstrates that code interpreter interface improves computational tool invocation by offloading precise operations to external executors",
            "uuids": [
                "e938.0",
                "e938.4",
                "e938.5",
                "e950.3"
            ]
        },
        {
            "text": "ReAct shows that interleaving reasoning and actions improves tool awareness and selection by providing explicit reasoning traces about when and which tools to use",
            "uuids": [
                "e848.1",
                "e848.2",
                "e823.1",
                "e910.3",
                "e948.3"
            ]
        },
        {
            "text": "EHRAgent demonstrates that code interface with rubber duck debugging improves both tool invocation accuracy and error recovery through execution feedback and error tracing",
            "uuids": [
                "e844.0",
                "e844.5",
                "e844.8"
            ]
        },
        {
            "text": "PDoctor reveals systematic tool-call errors (Action Lost, hallucinated parameters) and planning failures that increase with task complexity, showing failures across awareness, selection, and invocation",
            "uuids": [
                "e836.1"
            ]
        },
        {
            "text": "Mobile-Bench shows hallucinated API calls as a key failure mode in tool invocation, with ablations demonstrating importance of API access for multi-app tasks",
            "uuids": [
                "e901.0",
                "e901.1"
            ]
        },
        {
            "text": "TravelPlanner demonstrates tool argument errors and incomplete information collection, showing failures in tool composition and awareness of information needs",
            "uuids": [
                "e916.2",
                "e916.5",
                "e916.7"
            ]
        },
        {
            "text": "WebGPT shows that browser-assisted QA with tool interface (search, click, quote) and training via BC, RM, and PPO improves tool use, with rejection sampling providing largest gains",
            "uuids": [
                "e925.0",
                "e925.1",
                "e925.3",
                "e921.0"
            ]
        },
        {
            "text": "ReHAC demonstrates learned collaboration policy for deciding when to use agent tools vs. request human intervention, showing tool awareness can be learned via RL",
            "uuids": [
                "e847.0"
            ]
        },
        {
            "text": "FlowMind shows that user feedback loop improves tool composition and error recovery by allowing iterative refinement of generated workflows",
            "uuids": [
                "e904.2"
            ]
        },
        {
            "text": "Coscientist demonstrates that documentation retrieval (docs searcher) is crucial for correct API usage and tool invocation, enabling correct hardware control",
            "uuids": [
                "e945.1",
                "e945.2",
                "e945.3",
                "e945.4",
                "e945.5"
            ]
        },
        {
            "text": "LATM shows that tool-making (creating task-specific tools) can improve tool use, with +29.1pp improvement on BigBench tasks",
            "uuids": [
                "e935.4"
            ]
        },
        {
            "text": "TroVE demonstrates efficient tool induction with execution feedback, achieving +21.0pp on MATH through execution-driven tool creation",
            "uuids": [
                "e935.7"
            ]
        },
        {
            "text": "CREATOR shows mixed results from per-example tool creation, with small gains on MATH but no improvement on table tasks, suggesting tool-making effectiveness varies by domain",
            "uuids": [
                "e935.5"
            ]
        },
        {
            "text": "Tool learning surveys identify awareness, selection, and invocation as key challenges, with additional emphasis on tool composition and error handling",
            "uuids": [
                "e907.6",
                "e931.0",
                "e931.10",
                "e935.4",
                "e935.5",
                "e935.7"
            ]
        },
        {
            "text": "InterCode shows that interactive coding with execution feedback substantially improves tool invocation and error recovery compared to single-turn generation",
            "uuids": [
                "e947.2",
                "e947.6",
                "e947.10"
            ]
        },
        {
            "text": "Inner Monologue demonstrates that closed-loop feedback (success detection, scene descriptions) improves tool awareness and error recovery in embodied tasks",
            "uuids": [
                "e932.0",
                "e932.5"
            ]
        },
        {
            "text": "ChatDB shows that symbolic memory (database) as tool interface improves multi-step reasoning and tool composition through structured state management",
            "uuids": [
                "e913.0"
            ]
        },
        {
            "text": "FISHNET demonstrates neural conditioning (priming with examples) and HalluciBot filtering improve tool awareness and reduce hallucination in tool invocation",
            "uuids": [
                "e824.4"
            ]
        },
        {
            "text": "StreamBench shows that continuous learning with memory (MemPrompt, Self-StreamICL) improves tool use over time through accumulation of successful examples",
            "uuids": [
                "e903.2",
                "e903.3"
            ]
        },
        {
            "text": "LASER demonstrates that state-space exploration with backtracking improves tool selection and error recovery in web navigation",
            "uuids": [
                "e823.1",
                "e823.2",
                "e823.5"
            ]
        },
        {
            "text": "AgentTuning shows that hybrid instruction tuning (agent trajectories + general data) improves tool use while preserving general capabilities, addressing training trade-offs",
            "uuids": [
                "e820.0",
                "e820.1",
                "e820.4"
            ]
        },
        {
            "text": "DARA demonstrates that hierarchical decomposition improves tool composition in structured domains (KGQA)",
            "uuids": [
                "e841.3"
            ]
        },
        {
            "text": "Triad shows that multi-role LLM agents with KB memory improve tool selection and invocation in KBQA tasks",
            "uuids": [
                "e810.1"
            ]
        },
        {
            "text": "AGILE demonstrates that RL training of tool-use policy improves tool awareness (when to seek advice) and tool selection (when to use search/SQL)",
            "uuids": [
                "e816.0",
                "e816.3"
            ]
        },
        {
            "text": "Retroformer shows that learned retrospective feedback improves error recovery and tool adaptation through policy gradient optimization",
            "uuids": [
                "e941.0",
                "e941.2"
            ]
        },
        {
            "text": "Reflexion demonstrates that verbal self-reflection improves error recovery through episodic memory of failures",
            "uuids": [
                "e821.1",
                "e910.3",
                "e912.4"
            ]
        }
    ],
    "theory_statements": [
        "Tool-use competence requires five distinct capabilities: awareness (knowing when tools are needed), selection (choosing appropriate tools), invocation (correct argument formatting and output handling), composition (chaining multiple tools), and error recovery (detecting and adapting to failures)",
        "Tool awareness involves recognizing when parametric knowledge is insufficient and external tools are needed, often manifesting as overconfidence in models with strong QA performance",
        "Tool selection requires understanding tool capabilities, constraints, applicability, and documentation, with performance depending heavily on tool description quality",
        "Tool invocation requires precise argument formatting, output handling, and execution management, with code interfaces generally outperforming natural language interfaces",
        "Tool composition requires managing dependencies, sequencing, and state across multiple tool calls, with failures increasing non-linearly with the number of tools",
        "Error recovery requires detecting failures, diagnosing root causes, and adapting tool usage, with execution feedback being more effective than static training",
        "The QA-interactive gap manifests differently across these five competencies, with models potentially excelling at some while failing at others",
        "Models can have strong parametric knowledge (QA) but poor tool awareness due to overconfidence in their own knowledge",
        "Execution feedback is more effective than static training for improving tool invocation and error recovery",
        "Tool documentation quality primarily affects tool selection and invocation but has limited impact on tool awareness",
        "All five competencies must be addressed for robust tool-augmented performance, with weakness in any competency limiting overall effectiveness",
        "Training interventions can target specific competencies, with different training methods showing different competency profiles",
        "Tool-use skills show limited transfer across different tool interfaces (e.g., API vs. code) and domains without explicit training on diverse tool sets",
        "Tool composition difficulty increases non-linearly with the number of tools and complexity of dependencies",
        "Interactive learning with execution feedback is more effective than static training for tool invocation and error recovery",
        "Tool-making (creating new tools) can improve task-specific tool use but shows domain-dependent effectiveness"
    ],
    "new_predictions_likely": [
        "Training specifically on tool-awareness examples (when to use tools vs. rely on parametric knowledge) will improve tool-use initiation without necessarily improving selection, invocation, composition, or error recovery",
        "Providing better tool documentation will improve tool selection and invocation more than tool awareness, composition, or error recovery",
        "Models with stronger parametric knowledge will show worse tool awareness (more overconfident) than weaker models, leading to fewer tool invocations even when tools would help",
        "Tool invocation accuracy will improve more from execution feedback than from static training examples, with the gap widening for complex APIs",
        "Multi-tool tasks will show greater failures in tool composition and selection than single-tool tasks, with failure rates increasing non-linearly with tool count",
        "Code-based tool interfaces will show better invocation accuracy than natural language API interfaces due to more precise formatting requirements",
        "Tool-use training on diverse tool sets will improve generalization to unseen tools more than training on narrow tool sets",
        "Error recovery competency will improve more from interactive debugging (like rubber duck debugging) than from static error examples",
        "Tool composition will benefit more from hierarchical decomposition methods than from flat sequential planning",
        "Models trained with execution feedback will show better error recovery than models trained only on successful trajectories"
    ],
    "new_predictions_unknown": [
        "Whether tool awareness can be fully learned through training or requires architectural changes (e.g., explicit confidence estimation modules) to reliably detect knowledge boundaries",
        "The extent to which tool selection skills transfer across fundamentally different tool domains (e.g., from web APIs to database queries to code libraries)",
        "Whether tool invocation skills learned for one API format (e.g., REST) transfer effectively to other formats (e.g., GraphQL, gRPC) without additional training",
        "The optimal balance between tool-specific training and general tool-use training for maximizing both performance and generalization",
        "Whether future models can learn to create and use novel tools on-the-fly without explicit tool-making training",
        "The extent to which tool composition skills learned in one domain (e.g., web navigation) transfer to other domains (e.g., scientific workflows)",
        "Whether error recovery can be learned purely from successful trajectories or requires exposure to failure cases",
        "The degree to which tool-use competencies can be learned through self-supervised methods (like Toolformer) versus requiring human demonstrations or RL",
        "Whether tool awareness can be improved through calibration training or requires fundamental changes to model confidence estimation",
        "The extent to which tool-use skills degrade general language capabilities and whether this trade-off is fundamental or can be eliminated"
    ],
    "negative_experiments": [
        "Demonstrating that tool awareness does not improve with training on diverse tool-use examples would challenge the learnability of this competency and suggest architectural changes are needed",
        "Showing that tool selection accuracy does not depend on tool documentation quality would question the importance of tool descriptions and suggest selection relies on other factors",
        "Finding that tool invocation errors persist despite extensive execution feedback would challenge the value of interactive learning for this competency",
        "Demonstrating that multi-tool tasks show no additional failures beyond single-tool tasks would question the complexity of tool composition as a distinct competency",
        "Showing that tool-use training does not improve generalization to unseen tools would challenge the transferability of tool-use skills and suggest tool-specific training is necessary",
        "Finding that error recovery does not improve with exposure to failure cases would challenge the importance of negative examples in learning",
        "Demonstrating that code interfaces show no advantage over natural language interfaces for tool invocation would question the importance of interface design",
        "Showing that tool composition difficulty does not increase with the number of tools would challenge the non-linear complexity hypothesis",
        "Finding that tool-use training consistently degrades general capabilities would suggest a fundamental trade-off that cannot be resolved",
        "Demonstrating that execution feedback provides no advantage over static training for any competency would challenge the importance of interactive learning"
    ],
    "unaccounted_for": [
        {
            "text": "Some models (e.g., GPT-4) show good tool use without explicit tool-use training, suggesting strong base capabilities or implicit learning during pretraining",
            "uuids": [
                "e944.1",
                "e944.2",
                "e837.4"
            ]
        },
        {
            "text": "The relative importance of the five competencies varies significantly across tasks and domains, with some tasks primarily requiring awareness while others require composition",
            "uuids": [
                "e902.0",
                "e839.5"
            ]
        },
        {
            "text": "Some tool-use failures are due to factors beyond the five competencies, such as context length limitations, computational constraints, or fundamental model limitations",
            "uuids": [
                "e836.1",
                "e901.0"
            ]
        },
        {
            "text": "Tool-use performance can be affected by factors like prompt engineering, temperature settings, and sampling strategies that are orthogonal to the five competencies",
            "uuids": [
                "e840.0",
                "e840.1"
            ]
        },
        {
            "text": "Some interventions (e.g., rejection sampling in WebGPT) improve tool use through inference-time compute rather than improving any specific competency",
            "uuids": [
                "e925.0"
            ]
        },
        {
            "text": "Human-in-the-loop approaches can substitute for or augment tool-use competencies in ways not captured by the five-competency model",
            "uuids": [
                "e847.0",
                "e904.2"
            ]
        },
        {
            "text": "Tool interface design (e.g., function-calling APIs vs. natural language) affects tool use in ways that interact with but are distinct from the five competencies",
            "uuids": [
                "e813.0",
                "e813.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models with strong tool invocation (e.g., code generation) still fail at tool awareness, suggesting competencies may not be as independent as the theory assumes",
            "uuids": [
                "e902.0"
            ]
        },
        {
            "text": "Tool documentation sometimes helps less than expected, with some models performing well without documentation or poorly despite good documentation",
            "uuids": [
                "e813.0",
                "e813.1"
            ]
        },
        {
            "text": "Some tool-use training degrades general capabilities, suggesting a trade-off that conflicts with the theory's assumption that competencies can be independently improved",
            "uuids": [
                "e820.1",
                "e919.11"
            ]
        },
        {
            "text": "Tool-making interventions (CREATOR) show mixed results, with improvements on some tasks but not others, conflicting with predictions about tool-making effectiveness",
            "uuids": [
                "e935.5"
            ]
        },
        {
            "text": "Some models show good tool composition without explicit composition training, suggesting this competency may emerge from other capabilities",
            "uuids": [
                "e944.1"
            ]
        },
        {
            "text": "Execution feedback sometimes provides minimal improvement over static training, conflicting with predictions about feedback effectiveness",
            "uuids": [
                "e947.6"
            ]
        }
    ],
    "special_cases": [
        "Deterministic tools (e.g., calculators) may require less error recovery than stochastic tools (e.g., search engines) due to predictable behavior",
        "Familiar tool domains (e.g., web search) may show better selection than novel domains (e.g., specialized scientific APIs) due to pretraining exposure",
        "Simple APIs with few parameters may show better invocation accuracy than complex APIs with many parameters and constraints",
        "Single-tool tasks may not require tool selection competency, making this competency less critical for such tasks",
        "Retrieval-augmented generation can be viewed as a special case of tool use where the tool is a retrieval system",
        "Human-in-the-loop systems represent a special case where human judgment substitutes for some tool-use competencies",
        "Self-correction and reflection can be viewed as special cases of error recovery where the model acts as its own debugging tool",
        "Tool-making represents a meta-competency that can substitute for tool selection when appropriate tools are not available",
        "Code interfaces may require different competency profiles than natural language interfaces, with stronger emphasis on invocation precision",
        "Closed-loop feedback systems (like Inner Monologue) may reduce the importance of tool awareness by providing continuous environmental signals",
        "Symbolic memory systems (like ChatDB) may reduce the importance of tool composition by providing structured state management",
        "Multi-agent systems may distribute competencies across agents, with specialized agents handling different competencies"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Qin et al. (2023) Tool Learning with Foundation Models [Survey identifying tool-use challenges including awareness, selection, and invocation, but not explicitly proposing five-competency model]",
            "Xu et al. (2023) MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use [Evaluates tool awareness and selection but does not propose comprehensive competency model]",
            "Qin et al. (2023) ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs [Demonstrates importance of diverse tool training but does not explicitly decompose competencies]",
            "Tang et al. (2023) ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases [Shows simulated training improves tool use but does not propose competency framework]",
            "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [Demonstrates self-supervised tool learning but focuses on invocation rather than full competency model]",
            "Paranjape et al. (2023) ART: Automatic multi-step reasoning and tool-use for large language models [Shows importance of tool library and decomposition but does not explicitly model competencies]",
            "Gao et al. (2022) PAL: Program-aided Language Models [Demonstrates code interface benefits but does not propose general competency framework]",
            "Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [Shows interleaving reasoning and actions helps but does not decompose into competencies]",
            "Zhuang et al. (2024) ToolQA: A Dataset for LLM Question Answering with External Tools [Evaluates tool use but does not propose competency model]",
            "Nakano et al. (2021) WebGPT: Browser-assisted question-answering with human feedback [Demonstrates tool-use training methods but does not explicitly model competencies]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 4,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
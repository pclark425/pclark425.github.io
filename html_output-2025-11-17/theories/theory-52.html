<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Positive Experience Memory Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-52</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-52</p>
                <p><strong>Name:</strong> Positive Experience Memory Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how agents can use memory to help solve text games, based on the following results.</p>
                <p><strong>Description:</strong> Memory systems that store and retrieve both successful (positive) and failed (negative) experiences outperform failure-only reflection memory because: (1) positive experiences provide templates for successful strategies that should be reinforced, (2) balanced positive/negative memory prevents 'tilt' (loss of momentum after early success), (3) positive memories enable recognition of partial progress toward goals, and (4) the combination provides richer context for decision-making. The benefit is particularly pronounced for smaller language models and complex multi-step tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Memory systems storing both positive and negative experiences provide 15-30% performance improvement over failure-only memory across diverse LLM sizes and task complexities.</li>
                <li>The benefit of positive experience memory scales inversely with model size: for small models (<10B parameters), benefit is 30-40%; for medium models (10-100B), benefit is 20-30%; for large models (>100B), benefit is 15-25%.</li>
                <li>Positive experience memory prevents performance degradation in multi-attempt scenarios: failure-only memory shows declining performance after 3-4 attempts, while balanced memory maintains or improves performance through 5+ attempts.</li>
                <li>The optimal ratio of positive to negative experiences in memory is approximately 1:1 to 2:1 (positive:negative) for most tasks; ratios outside this range show 10-20% performance degradation.</li>
                <li>Positive experience memory is particularly critical for tasks with sparse rewards or long horizons (>30 steps), where it provides >35% improvement, compared to <15% improvement on short-horizon tasks.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Sweet&Sour with positive+negative reflection memory substantially outperforms failure-only Reflexion (GPT-4o: 54.6 vs 45.3, Mistral: 44.6 vs 27.6, Llama 8B: 32.5 vs 21.7) <a href="../results/extraction-result-232.html#e232.0" class="evidence-link">[e232.0]</a> <a href="../results/extraction-result-232.html#e232.1" class="evidence-link">[e232.1]</a> </li>
    <li>Ablation where Sweet&Sour only sampled from failures produced lower scores similar to Reflexion <a href="../results/extraction-result-232.html#e232.0" class="evidence-link">[e232.0]</a> </li>
    <li>Reflexion's failure-centric memory can lead to 'tilt' when agent is initially successful <a href="../results/extraction-result-232.html#e232.1" class="evidence-link">[e232.1]</a> </li>
    <li>Sweet&Sour's inclusion of positive experiences shows particularly large benefits for smaller LLMs (Llama 8B: +7.9 improvement vs GPT-4o: +9.3) <a href="../results/extraction-result-232.html#e232.0" class="evidence-link">[e232.0]</a> </li>
    <li>Reflexion shows marked performance growth at second try but degrades with subsequent tries, suggesting failure-only memory has limitations <a href="../results/extraction-result-238.html#e238.6" class="evidence-link">[e238.6]</a> </li>
    <li>Managed dual-buffer memory (short-term and long-term) in Sweet&Sour enables better context selection <a href="../results/extraction-result-232.html#e232.0" class="evidence-link">[e232.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In a new multi-step task domain (e.g., software debugging), agents with positive+negative memory will show >25% improvement over failure-only memory, with larger gains for smaller models.</li>
                <li>For tasks where early success is common but completion is rare (e.g., 80% of agents achieve first sub-goal, 20% complete task), positive memory will prevent >40% of performance degradation seen with failure-only memory.</li>
                <li>Agents with balanced positive/negative memory will maintain stable performance across 10+ attempts, while failure-only memory will show >20% degradation after attempt 5.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>For tasks where failures are highly informative but successes are trivial (e.g., constraint satisfaction problems), positive memory might provide minimal benefit or even harm performance - effect could range from -10% to +10%.</li>
                <li>In adversarial environments where successful strategies become obsolete quickly, storing positive experiences might lead to negative transfer - impact unclear, possibly -15% to +5%.</li>
                <li>For tasks requiring creative exploration where past successes might bias toward local optima, positive memory's impact is uncertain - could range from -20% to +30%.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where failure-only memory consistently outperforms balanced memory would challenge the theory.</li>
                <li>Demonstrating that positive memory provides no benefit for large language models (>100B parameters) would challenge the model-size scaling claim.</li>
                <li>Showing that positive memory causes negative transfer in new task domains would challenge the generalization benefit.</li>
                <li>Finding that the optimal positive:negative ratio is <0.5:1 or >5:1 across diverse tasks would challenge the balance claim.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically determine which experiences qualify as 'positive' vs 'negative' in tasks with continuous reward signals is not fully addressed <a href="../results/extraction-result-232.html#e232.0" class="evidence-link">[e232.0]</a> </li>
    <li>The optimal memory capacity (how many positive/negative experiences to store) for different task complexities is not characterized <a href="../results/extraction-result-232.html#e232.0" class="evidence-link">[e232.0]</a> </li>
    <li>How positive experience memory interacts with different prompting strategies (chain-of-thought, etc.) is not explored <a href="../results/extraction-result-232.html#e232.0" class="evidence-link">[e232.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Shinn et al. (2023) Reflexion: Language agents with verbal reinforcement learning [Failure-focused reflection, this theory extends with positive experiences]</li>
    <li>Sutton & Barto (2018) Reinforcement Learning: An Introduction [General RL theory including positive/negative rewards, but not specifically for LLM memory]</li>
    <li>Kahneman & Tversky (1979) Prospect theory [Cognitive theory of how humans weight gains vs losses, related but not applied to agent memory]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Positive Experience Memory Theory",
    "theory_description": "Memory systems that store and retrieve both successful (positive) and failed (negative) experiences outperform failure-only reflection memory because: (1) positive experiences provide templates for successful strategies that should be reinforced, (2) balanced positive/negative memory prevents 'tilt' (loss of momentum after early success), (3) positive memories enable recognition of partial progress toward goals, and (4) the combination provides richer context for decision-making. The benefit is particularly pronounced for smaller language models and complex multi-step tasks.",
    "supporting_evidence": [
        {
            "text": "Sweet&Sour with positive+negative reflection memory substantially outperforms failure-only Reflexion (GPT-4o: 54.6 vs 45.3, Mistral: 44.6 vs 27.6, Llama 8B: 32.5 vs 21.7)",
            "uuids": [
                "e232.0",
                "e232.1"
            ]
        },
        {
            "text": "Ablation where Sweet&Sour only sampled from failures produced lower scores similar to Reflexion",
            "uuids": [
                "e232.0"
            ]
        },
        {
            "text": "Reflexion's failure-centric memory can lead to 'tilt' when agent is initially successful",
            "uuids": [
                "e232.1"
            ]
        },
        {
            "text": "Sweet&Sour's inclusion of positive experiences shows particularly large benefits for smaller LLMs (Llama 8B: +7.9 improvement vs GPT-4o: +9.3)",
            "uuids": [
                "e232.0"
            ]
        },
        {
            "text": "Reflexion shows marked performance growth at second try but degrades with subsequent tries, suggesting failure-only memory has limitations",
            "uuids": [
                "e238.6"
            ]
        },
        {
            "text": "Managed dual-buffer memory (short-term and long-term) in Sweet&Sour enables better context selection",
            "uuids": [
                "e232.0"
            ]
        }
    ],
    "theory_statements": [
        "Memory systems storing both positive and negative experiences provide 15-30% performance improvement over failure-only memory across diverse LLM sizes and task complexities.",
        "The benefit of positive experience memory scales inversely with model size: for small models (&lt;10B parameters), benefit is 30-40%; for medium models (10-100B), benefit is 20-30%; for large models (&gt;100B), benefit is 15-25%.",
        "Positive experience memory prevents performance degradation in multi-attempt scenarios: failure-only memory shows declining performance after 3-4 attempts, while balanced memory maintains or improves performance through 5+ attempts.",
        "The optimal ratio of positive to negative experiences in memory is approximately 1:1 to 2:1 (positive:negative) for most tasks; ratios outside this range show 10-20% performance degradation.",
        "Positive experience memory is particularly critical for tasks with sparse rewards or long horizons (&gt;30 steps), where it provides &gt;35% improvement, compared to &lt;15% improvement on short-horizon tasks."
    ],
    "new_predictions_likely": [
        "In a new multi-step task domain (e.g., software debugging), agents with positive+negative memory will show &gt;25% improvement over failure-only memory, with larger gains for smaller models.",
        "For tasks where early success is common but completion is rare (e.g., 80% of agents achieve first sub-goal, 20% complete task), positive memory will prevent &gt;40% of performance degradation seen with failure-only memory.",
        "Agents with balanced positive/negative memory will maintain stable performance across 10+ attempts, while failure-only memory will show &gt;20% degradation after attempt 5."
    ],
    "new_predictions_unknown": [
        "For tasks where failures are highly informative but successes are trivial (e.g., constraint satisfaction problems), positive memory might provide minimal benefit or even harm performance - effect could range from -10% to +10%.",
        "In adversarial environments where successful strategies become obsolete quickly, storing positive experiences might lead to negative transfer - impact unclear, possibly -15% to +5%.",
        "For tasks requiring creative exploration where past successes might bias toward local optima, positive memory's impact is uncertain - could range from -20% to +30%."
    ],
    "negative_experiments": [
        "Finding tasks where failure-only memory consistently outperforms balanced memory would challenge the theory.",
        "Demonstrating that positive memory provides no benefit for large language models (&gt;100B parameters) would challenge the model-size scaling claim.",
        "Showing that positive memory causes negative transfer in new task domains would challenge the generalization benefit.",
        "Finding that the optimal positive:negative ratio is &lt;0.5:1 or &gt;5:1 across diverse tasks would challenge the balance claim."
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically determine which experiences qualify as 'positive' vs 'negative' in tasks with continuous reward signals is not fully addressed",
            "uuids": [
                "e232.0"
            ]
        },
        {
            "text": "The optimal memory capacity (how many positive/negative experiences to store) for different task complexities is not characterized",
            "uuids": [
                "e232.0"
            ]
        },
        {
            "text": "How positive experience memory interacts with different prompting strategies (chain-of-thought, etc.) is not explored",
            "uuids": [
                "e232.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Reflexion achieves perfect scores on some tasks despite being failure-only, suggesting positive memory is not always necessary",
            "uuids": [
                "e238.6"
            ]
        },
        {
            "text": "Some tasks show only modest improvements from positive memory (e.g., &lt;10% in some ScienceWorld tasks)",
            "uuids": [
                "e232.0"
            ]
        }
    ],
    "special_cases": [
        "For tasks with binary outcomes (complete success or complete failure), the distinction between positive and negative experiences may be less nuanced.",
        "In tasks where the optimal strategy is highly non-obvious, positive experiences from sub-optimal strategies might mislead the agent.",
        "For very short tasks (&lt;5 steps), the overhead of managing positive/negative memory may not be justified."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Shinn et al. (2023) Reflexion: Language agents with verbal reinforcement learning [Failure-focused reflection, this theory extends with positive experiences]",
            "Sutton & Barto (2018) Reinforcement Learning: An Introduction [General RL theory including positive/negative rewards, but not specifically for LLM memory]",
            "Kahneman & Tversky (1979) Prospect theory [Cognitive theory of how humans weight gains vs losses, related but not applied to agent memory]"
        ]
    },
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
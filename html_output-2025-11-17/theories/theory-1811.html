<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain and Prompt Sensitivity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1811</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1811</p>
                <p><strong>Name:</strong> Domain and Prompt Sensitivity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory posits that the accuracy and calibration of LLMs' probability estimates for future scientific discoveries are fundamentally determined by two interacting factors: (1) the density and coherence of domain-specific knowledge encoded in the model's latent space, and (2) the semantic specificity and structure of the prompt. The theory asserts that LLMs act as probabilistic aggregators over their internal knowledge, and that their outputs are most reliable when the prompt precisely targets a well-represented, internally consistent knowledge domain. Conversely, when prompts are vague or target domains with sparse or conflicting internal representations, probability estimates become less reliable and more prone to miscalibration.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Domain Density Calibration Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_high_density_knowledge &#8594; scientific domain<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; targets &#8594; same scientific domain</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_well_calibrated_for &#8594; future_discovery_in_domain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs demonstrate higher factual accuracy and calibration in domains with dense, high-quality training data (e.g., biomedical, physics). </li>
    <li>Calibration studies show LLMs' confidence aligns with accuracy in well-represented domains. </li>
    <li>Latent space analyses reveal clustering of domain knowledge, supporting the idea of domain density. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends known effects of training data density to the calibration of probabilistic scientific forecasting.</p>            <p><strong>What Already Exists:</strong> LLMs' performance is known to correlate with training data density in specific domains.</p>            <p><strong>What is Novel:</strong> The explicit link between domain density in latent space and calibration of future-oriented probability estimates is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and knowledge representation]</li>
    <li>Meng et al. (2022) Locating and Editing Factual Associations in GPT [Latent knowledge in LLMs]</li>
</ul>
            <h3>Statement 1: Prompt-Model Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt &#8594; is_semantically_specific &#8594; scientific discovery<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; matches &#8594; LLM_internal_knowledge_structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_more_accurate_than &#8594; estimate_for_generic_or_misaligned_prompt</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show LLMs are more accurate and calibrated when prompts are specific and context-rich. </li>
    <li>Prompt engineering research demonstrates that alignment between prompt structure and model knowledge improves output reliability. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is a novel extension of prompt engineering effects to the domain of scientific probability estimation.</p>            <p><strong>What Already Exists:</strong> Prompt specificity and alignment are known to improve LLM performance in QA and reasoning.</p>            <p><strong>What is Novel:</strong> This law formalizes the effect for probabilistic forecasting of scientific discoveries.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt specificity and reasoning]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt about a future scientific discovery is highly specific and targets a domain with dense representation in the LLM, the model's probability estimate will be well-calibrated.</li>
                <li>If a prompt is vague or targets a domain with sparse or inconsistent representation, the LLM's probability estimate will be less reliable and more prone to over- or under-confidence.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a prompt is highly specific but targets a domain with conflicting or outdated knowledge in the LLM, the probability estimate may be erratic or systematically biased.</li>
                <li>If LLMs are fine-tuned on a small, high-quality dataset of scientific forecasts, their calibration may improve even for previously sparse domains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs provide well-calibrated probability estimates for generic or vague prompts, this would challenge the prompt-model alignment law.</li>
                <li>If LLMs' probability estimates are equally accurate regardless of domain knowledge density, this would contradict the domain density calibration law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The effect of adversarial or misleading prompts on calibration is not addressed. </li>
    <li>The impact of model size and architecture on domain and prompt sensitivity is not explicitly modeled. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends known LLM properties to a novel, high-impact forecasting application.</p>
            <p><strong>References:</strong> <ul>
    <li>Meng et al. (2022) Locating and Editing Factual Associations in GPT [Latent knowledge in LLMs]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and knowledge representation]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt specificity and reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain and Prompt Sensitivity Theory",
    "theory_description": "This theory posits that the accuracy and calibration of LLMs' probability estimates for future scientific discoveries are fundamentally determined by two interacting factors: (1) the density and coherence of domain-specific knowledge encoded in the model's latent space, and (2) the semantic specificity and structure of the prompt. The theory asserts that LLMs act as probabilistic aggregators over their internal knowledge, and that their outputs are most reliable when the prompt precisely targets a well-represented, internally consistent knowledge domain. Conversely, when prompts are vague or target domains with sparse or conflicting internal representations, probability estimates become less reliable and more prone to miscalibration.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Domain Density Calibration Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_high_density_knowledge",
                        "object": "scientific domain"
                    },
                    {
                        "subject": "prompt",
                        "relation": "targets",
                        "object": "same scientific domain"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_well_calibrated_for",
                        "object": "future_discovery_in_domain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs demonstrate higher factual accuracy and calibration in domains with dense, high-quality training data (e.g., biomedical, physics).",
                        "uuids": []
                    },
                    {
                        "text": "Calibration studies show LLMs' confidence aligns with accuracy in well-represented domains.",
                        "uuids": []
                    },
                    {
                        "text": "Latent space analyses reveal clustering of domain knowledge, supporting the idea of domain density.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs' performance is known to correlate with training data density in specific domains.",
                    "what_is_novel": "The explicit link between domain density in latent space and calibration of future-oriented probability estimates is new.",
                    "classification_explanation": "This law extends known effects of training data density to the calibration of probabilistic scientific forecasting.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and knowledge representation]",
                        "Meng et al. (2022) Locating and Editing Factual Associations in GPT [Latent knowledge in LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt-Model Alignment Law",
                "if": [
                    {
                        "subject": "prompt",
                        "relation": "is_semantically_specific",
                        "object": "scientific discovery"
                    },
                    {
                        "subject": "prompt",
                        "relation": "matches",
                        "object": "LLM_internal_knowledge_structure"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_more_accurate_than",
                        "object": "estimate_for_generic_or_misaligned_prompt"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show LLMs are more accurate and calibrated when prompts are specific and context-rich.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering research demonstrates that alignment between prompt structure and model knowledge improves output reliability.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt specificity and alignment are known to improve LLM performance in QA and reasoning.",
                    "what_is_novel": "This law formalizes the effect for probabilistic forecasting of scientific discoveries.",
                    "classification_explanation": "The law is a novel extension of prompt engineering effects to the domain of scientific probability estimation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt specificity and reasoning]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt about a future scientific discovery is highly specific and targets a domain with dense representation in the LLM, the model's probability estimate will be well-calibrated.",
        "If a prompt is vague or targets a domain with sparse or inconsistent representation, the LLM's probability estimate will be less reliable and more prone to over- or under-confidence."
    ],
    "new_predictions_unknown": [
        "If a prompt is highly specific but targets a domain with conflicting or outdated knowledge in the LLM, the probability estimate may be erratic or systematically biased.",
        "If LLMs are fine-tuned on a small, high-quality dataset of scientific forecasts, their calibration may improve even for previously sparse domains."
    ],
    "negative_experiments": [
        "If LLMs provide well-calibrated probability estimates for generic or vague prompts, this would challenge the prompt-model alignment law.",
        "If LLMs' probability estimates are equally accurate regardless of domain knowledge density, this would contradict the domain density calibration law."
    ],
    "unaccounted_for": [
        {
            "text": "The effect of adversarial or misleading prompts on calibration is not addressed.",
            "uuids": []
        },
        {
            "text": "The impact of model size and architecture on domain and prompt sensitivity is not explicitly modeled.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show overconfidence even in domains with sparse or inconsistent knowledge, which may conflict with the theory.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with explicit uncertainty modeling (e.g., Bayesian LLMs) may behave differently.",
        "Prompts that combine multiple, conflicting knowledge clusters may yield unpredictable calibration.",
        "Domains with rapidly evolving knowledge may not be well-represented even if previously dense."
    ],
    "existing_theory": {
        "what_already_exists": "Latent knowledge and prompt specificity effects are established in LLM literature.",
        "what_is_novel": "The explicit connection to calibration of scientific discovery probability estimates is new.",
        "classification_explanation": "The theory extends known LLM properties to a novel, high-impact forecasting application.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Meng et al. (2022) Locating and Editing Factual Associations in GPT [Latent knowledge in LLMs]",
            "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and knowledge representation]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt specificity and reasoning]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-647",
    "original_theory_name": "Domain and Prompt Sensitivity Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Domain and Prompt Sensitivity Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
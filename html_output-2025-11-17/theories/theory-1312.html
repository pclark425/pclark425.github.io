<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Order-Invariance Robustness Law for Graph Linearization in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1312</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1312</p>
                <p><strong>Name:</strong> Order-Invariance Robustness Law for Graph Linearization in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal graph-to-text representation for LLM training must be robust to permutations in the ordering of nodes and edges, such that the LLM's learned representations and downstream performance are invariant to arbitrary linearization orderings. The theory further asserts that order-invariant encodings promote generalization, reduce spurious correlations, and enable the LLM to focus on the underlying graph structure rather than superficial sequence artifacts.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Order-Invariance Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; is_used_for &#8594; LLM_training<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_linearization &#8594; is_order_invariant &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; learns_graph_structure &#8594; robustly<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; performance_on_graph_tasks &#8594; is_invariant_to_linearization_order</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Graph neural networks and some graph-to-sequence models achieve higher generalization when their encodings are invariant to node/edge order. </li>
    <li>LLMs trained on multiple random orderings of the same graph learn to ignore sequence artifacts and focus on structure. </li>
    <li>Order-sensitive encodings can lead to overfitting to spurious sequence patterns, reducing generalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Order-invariance is established in GNNs, but its explicit formalization for LLM graph linearization and the link to LLM robustness is novel.</p>            <p><strong>What Already Exists:</strong> Order-invariance is a well-known desideratum in graph neural networks and some graph serialization schemes.</p>            <p><strong>What is Novel:</strong> This law extends order-invariance as a formal requirement for LLM-based graph learning and links it to robustness and generalization in language models.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]</li>
    <li>Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Order-invariant graph-to-text mappings]</li>
</ul>
            <h3>Statement 1: Permutation Robustness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; is_permuted &#8594; arbitrary_order</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; output_representation &#8594; remains_consistent<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; downstream_performance &#8594; remains_unchanged</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that models trained on multiple permutations of graph encodings generalize better and are less sensitive to input order. </li>
    <li>Order-sensitive models can produce different outputs for isomorphic graphs with different linearizations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Permutation invariance is established in other domains, but its explicit application to LLM graph linearization is novel.</p>            <p><strong>What Already Exists:</strong> Permutation invariance is a known property in set and graph models, but not always enforced in LLMs.</p>            <p><strong>What is Novel:</strong> This law formalizes permutation robustness as a requirement for LLMs trained on graph linearizations.</p>
            <p><strong>References:</strong> <ul>
    <li>Zaheer et al. (2017) Deep Sets [Permutation invariance in set models]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs trained on order-invariant graph linearizations will outperform those trained on fixed-order encodings in tasks requiring generalization to unseen graph structures.</li>
                <li>Introducing random permutations of node/edge order during training will improve LLM robustness and reduce overfitting to sequence artifacts.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Order-invariant linearizations may enable LLMs to learn latent graph symmetries or automorphism groups without explicit supervision.</li>
                <li>Order-invariance could allow LLMs to generalize to new graph types (e.g., hypergraphs) with minimal retraining.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs trained on order-invariant encodings do not outperform those trained on fixed-order encodings, the theory would be challenged.</li>
                <li>If LLM outputs vary significantly with different linearization orders, the law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The effect of canonical orderings (e.g., lexicographic, BFS, DFS) on LLM learning is not fully explained. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory builds on existing principles but applies them in a new context (LLMs and graph-to-text linearization).</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]</li>
    <li>Zaheer et al. (2017) Deep Sets [Permutation invariance in set models]</li>
    <li>Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Order-invariant graph-to-text mappings]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "theory_description": "This theory posits that the ideal graph-to-text representation for LLM training must be robust to permutations in the ordering of nodes and edges, such that the LLM's learned representations and downstream performance are invariant to arbitrary linearization orderings. The theory further asserts that order-invariant encodings promote generalization, reduce spurious correlations, and enable the LLM to focus on the underlying graph structure rather than superficial sequence artifacts.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Order-Invariance Law",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "is_used_for",
                        "object": "LLM_training"
                    },
                    {
                        "subject": "graph_linearization",
                        "relation": "is_order_invariant",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "learns_graph_structure",
                        "object": "robustly"
                    },
                    {
                        "subject": "LLM",
                        "relation": "performance_on_graph_tasks",
                        "object": "is_invariant_to_linearization_order"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Graph neural networks and some graph-to-sequence models achieve higher generalization when their encodings are invariant to node/edge order.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs trained on multiple random orderings of the same graph learn to ignore sequence artifacts and focus on structure.",
                        "uuids": []
                    },
                    {
                        "text": "Order-sensitive encodings can lead to overfitting to spurious sequence patterns, reducing generalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Order-invariance is a well-known desideratum in graph neural networks and some graph serialization schemes.",
                    "what_is_novel": "This law extends order-invariance as a formal requirement for LLM-based graph learning and links it to robustness and generalization in language models.",
                    "classification_explanation": "Order-invariance is established in GNNs, but its explicit formalization for LLM graph linearization and the link to LLM robustness is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]",
                        "Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Order-invariant graph-to-text mappings]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Permutation Robustness Law",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "is_permuted",
                        "object": "arbitrary_order"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "output_representation",
                        "object": "remains_consistent"
                    },
                    {
                        "subject": "LLM",
                        "relation": "downstream_performance",
                        "object": "remains_unchanged"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that models trained on multiple permutations of graph encodings generalize better and are less sensitive to input order.",
                        "uuids": []
                    },
                    {
                        "text": "Order-sensitive models can produce different outputs for isomorphic graphs with different linearizations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Permutation invariance is a known property in set and graph models, but not always enforced in LLMs.",
                    "what_is_novel": "This law formalizes permutation robustness as a requirement for LLMs trained on graph linearizations.",
                    "classification_explanation": "Permutation invariance is established in other domains, but its explicit application to LLM graph linearization is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zaheer et al. (2017) Deep Sets [Permutation invariance in set models]",
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs trained on order-invariant graph linearizations will outperform those trained on fixed-order encodings in tasks requiring generalization to unseen graph structures.",
        "Introducing random permutations of node/edge order during training will improve LLM robustness and reduce overfitting to sequence artifacts."
    ],
    "new_predictions_unknown": [
        "Order-invariant linearizations may enable LLMs to learn latent graph symmetries or automorphism groups without explicit supervision.",
        "Order-invariance could allow LLMs to generalize to new graph types (e.g., hypergraphs) with minimal retraining."
    ],
    "negative_experiments": [
        "If LLMs trained on order-invariant encodings do not outperform those trained on fixed-order encodings, the theory would be challenged.",
        "If LLM outputs vary significantly with different linearization orders, the law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The effect of canonical orderings (e.g., lexicographic, BFS, DFS) on LLM learning is not fully explained.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs may implicitly learn to canonicalize or ignore order even when trained on fixed-order encodings, partially mitigating order sensitivity.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with natural canonical orderings (e.g., SMILES for molecules) may not require full order-invariance.",
        "Very large graphs may require hierarchical or compressed encodings, which could introduce controlled order dependencies."
    ],
    "existing_theory": {
        "what_already_exists": "Order-invariance is a well-known property in GNNs and set models.",
        "what_is_novel": "The explicit formalization of order-invariance as a requirement for LLM-based graph linearization and its link to robustness and generalization is novel.",
        "classification_explanation": "The theory builds on existing principles but applies them in a new context (LLMs and graph-to-text linearization).",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]",
            "Zaheer et al. (2017) Deep Sets [Permutation invariance in set models]",
            "Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Order-invariant graph-to-text mappings]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-615",
    "original_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
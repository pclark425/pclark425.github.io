<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain and Prompt Sensitivity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-647</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-647</p>
                <p><strong>Name:</strong> Domain and Prompt Sensitivity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries, based on the following results.</p>
                <p><strong>Description:</strong> The ability of LLMs to accurately forecast the probability of future scientific discoveries is highly sensitive to (1) the alignment between the model's pretraining/fine-tuning domain and the target scientific field, and (2) the design of prompts, including the use of explicit reasoning strategies. Domain-specialized models and carefully engineered prompts can improve performance, but poorly matched domains or prompt interventions can degrade accuracy or calibration.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Domain Specialization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_fine_tuned_on &#8594; domain-specific scientific literature</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves &#8594; higher accuracy and better calibration on forecasting tasks within that domain compared to general-purpose LLMs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>BrainGPT (Mistral-7B + LoRA) fine-tuned on neuroscience literature outperforms the base model on BrainBench, with improved calibration and accuracy. <a href="../results/extraction-result-5710.html#e5710.2" class="evidence-link">[e5710.2]</a> </li>
    <li>General-purpose LLMs evaluated on BrainBench perform well, but domain-specialized fine-tuning yields further gains. <a href="../results/extraction-result-5710.html#e5710.0" class="evidence-link">[e5710.0]</a> </li>
    <li>LLM-based property prediction in chemistry/materials/proteins shows that domain-adapted models outperform structure-agnostic baselines. <a href="../results/extraction-result-5693.html#e5693.1" class="evidence-link">[e5693.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Domain adaptation is known, but its necessity for accurate scientific forecasting is a novel, empirically supported extension.</p>            <p><strong>What Already Exists:</strong> Domain adaptation and fine-tuning are established for improving LLM performance on in-domain tasks.</p>            <p><strong>What is Novel:</strong> This law extends domain adaptation to probabilistic forecasting of future scientific discoveries, not just factual QA or property prediction.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation for NLP]</li>
    <li>Wang et al. (2024) Large language models surpass human experts in predicting neuroscience results [domain fine-tuning for forecasting]</li>
</ul>
            <h3>Statement 1: Prompt Sensitivity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; explicit reasoning strategies (e.g., rationale, breakdown, base rates, both sides)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; may_experience &#8594; systematic shifts in forecast probability distributions and calibration, which can improve or degrade accuracy depending on dataset balance and prompt design</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>PaLM2 with rationale prompting ('Answer + Rationale') increased mean predicted probabilities and worsened Brier score on imbalanced datasets. <a href="../results/extraction-result-5706.html#e5706.1" class="evidence-link">[e5706.1]</a> </li>
    <li>PaLM2 with structured forecasting strategies (Breakdown, Base Rates, Both Sides) did not improve over the Basic Baseline and sometimes worsened calibration. <a href="../results/extraction-result-5706.html#e5706.2" class="evidence-link">[e5706.2]</a> </li>
    <li>Superforecasting LLM (treatment) with expert prompt engineering improved human-augmented forecasting accuracy, but prompt-based interventions can also induce overconfidence or bias (as in the noisy LLM treatment). <a href="../results/extraction-result-5704.html#e5704.0" class="evidence-link">[e5704.0]</a> <a href="../results/extraction-result-5704.html#e5704.1" class="evidence-link">[e5704.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Prompt engineering is established, but its nuanced effects on LLM calibration and accuracy in forecasting are newly formalized here.</p>            <p><strong>What Already Exists:</strong> Prompt engineering is known to affect LLM outputs, but its effects on probabilistic calibration and forecasting accuracy are less well characterized.</p>            <p><strong>What is Novel:</strong> This law formalizes the sensitivity of LLM probabilistic forecasts to prompt design, especially for scientific discovery forecasting.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]</li>
    <li>Lin et al. (2024) Can Language Models Use Forecasting Strategies? [prompting and calibration in forecasting]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Fine-tuning a general-purpose LLM on a new scientific domain (e.g., astrophysics) will improve its forecasting accuracy and calibration for that domain's discoveries.</li>
                <li>Prompting an LLM with explicit reasoning strategies will systematically alter its probability distributions and calibration, with effects depending on dataset balance and prompt content.</li>
                <li>Prompt-based interventions that increase mean predicted probabilities will worsen Brier score on datasets with a majority of negative outcomes.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Combining domain-specialized fine-tuning with optimal prompt engineering could yield LLMs that outperform human experts in forecasting highly technical scientific discoveries.</li>
                <li>Prompting LLMs with adversarial or misleading reasoning strategies could induce systematic biases that are difficult to detect without careful calibration analysis.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If domain-specialized fine-tuning does not improve LLM forecasting accuracy on in-domain scientific discovery tasks, the domain specialization law would be falsified.</li>
                <li>If prompt-based interventions do not systematically alter LLM forecast distributions or calibration, the prompt sensitivity law would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Retrieval and ensembling can sometimes compensate for lack of domain specialization or suboptimal prompts, as seen in some ensemble and retrieval-augmented systems. <a href="../results/extraction-result-5823.html#e5823.0" class="evidence-link">[e5823.0]</a> <a href="../results/extraction-result-5790.html#e5790.0" class="evidence-link">[e5790.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While the components are known, their specific, empirically supported roles in scientific forecasting are newly articulated here.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting]</li>
    <li>Lin et al. (2024) Can Language Models Use Forecasting Strategies? [prompting and calibration in forecasting]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain and Prompt Sensitivity Theory",
    "theory_description": "The ability of LLMs to accurately forecast the probability of future scientific discoveries is highly sensitive to (1) the alignment between the model's pretraining/fine-tuning domain and the target scientific field, and (2) the design of prompts, including the use of explicit reasoning strategies. Domain-specialized models and carefully engineered prompts can improve performance, but poorly matched domains or prompt interventions can degrade accuracy or calibration.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Domain Specialization Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_fine_tuned_on",
                        "object": "domain-specific scientific literature"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves",
                        "object": "higher accuracy and better calibration on forecasting tasks within that domain compared to general-purpose LLMs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "BrainGPT (Mistral-7B + LoRA) fine-tuned on neuroscience literature outperforms the base model on BrainBench, with improved calibration and accuracy.",
                        "uuids": [
                            "e5710.2"
                        ]
                    },
                    {
                        "text": "General-purpose LLMs evaluated on BrainBench perform well, but domain-specialized fine-tuning yields further gains.",
                        "uuids": [
                            "e5710.0"
                        ]
                    },
                    {
                        "text": "LLM-based property prediction in chemistry/materials/proteins shows that domain-adapted models outperform structure-agnostic baselines.",
                        "uuids": [
                            "e5693.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Domain adaptation and fine-tuning are established for improving LLM performance on in-domain tasks.",
                    "what_is_novel": "This law extends domain adaptation to probabilistic forecasting of future scientific discoveries, not just factual QA or property prediction.",
                    "classification_explanation": "Domain adaptation is known, but its necessity for accurate scientific forecasting is a novel, empirically supported extension.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation for NLP]",
                        "Wang et al. (2024) Large language models surpass human experts in predicting neuroscience results [domain fine-tuning for forecasting]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Sensitivity Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "explicit reasoning strategies (e.g., rationale, breakdown, base rates, both sides)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "may_experience",
                        "object": "systematic shifts in forecast probability distributions and calibration, which can improve or degrade accuracy depending on dataset balance and prompt design"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "PaLM2 with rationale prompting ('Answer + Rationale') increased mean predicted probabilities and worsened Brier score on imbalanced datasets.",
                        "uuids": [
                            "e5706.1"
                        ]
                    },
                    {
                        "text": "PaLM2 with structured forecasting strategies (Breakdown, Base Rates, Both Sides) did not improve over the Basic Baseline and sometimes worsened calibration.",
                        "uuids": [
                            "e5706.2"
                        ]
                    },
                    {
                        "text": "Superforecasting LLM (treatment) with expert prompt engineering improved human-augmented forecasting accuracy, but prompt-based interventions can also induce overconfidence or bias (as in the noisy LLM treatment).",
                        "uuids": [
                            "e5704.0",
                            "e5704.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering is known to affect LLM outputs, but its effects on probabilistic calibration and forecasting accuracy are less well characterized.",
                    "what_is_novel": "This law formalizes the sensitivity of LLM probabilistic forecasts to prompt design, especially for scientific discovery forecasting.",
                    "classification_explanation": "Prompt engineering is established, but its nuanced effects on LLM calibration and accuracy in forecasting are newly formalized here.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]",
                        "Lin et al. (2024) Can Language Models Use Forecasting Strategies? [prompting and calibration in forecasting]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Fine-tuning a general-purpose LLM on a new scientific domain (e.g., astrophysics) will improve its forecasting accuracy and calibration for that domain's discoveries.",
        "Prompting an LLM with explicit reasoning strategies will systematically alter its probability distributions and calibration, with effects depending on dataset balance and prompt content.",
        "Prompt-based interventions that increase mean predicted probabilities will worsen Brier score on datasets with a majority of negative outcomes."
    ],
    "new_predictions_unknown": [
        "Combining domain-specialized fine-tuning with optimal prompt engineering could yield LLMs that outperform human experts in forecasting highly technical scientific discoveries.",
        "Prompting LLMs with adversarial or misleading reasoning strategies could induce systematic biases that are difficult to detect without careful calibration analysis."
    ],
    "negative_experiments": [
        "If domain-specialized fine-tuning does not improve LLM forecasting accuracy on in-domain scientific discovery tasks, the domain specialization law would be falsified.",
        "If prompt-based interventions do not systematically alter LLM forecast distributions or calibration, the prompt sensitivity law would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Retrieval and ensembling can sometimes compensate for lack of domain specialization or suboptimal prompts, as seen in some ensemble and retrieval-augmented systems.",
            "uuids": [
                "e5823.0",
                "e5790.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "PaLM2 with News API grounding did not improve over the basic baseline, suggesting that domain adaptation and prompt engineering alone are insufficient without high-quality retrieval and integration.",
            "uuids": [
                "e5706.4"
            ]
        }
    ],
    "special_cases": [
        "If the target scientific discovery is highly interdisciplinary or unprecedented, domain specialization may not confer an advantage.",
        "Prompt-based interventions may interact with model safety training or RLHF in unpredictable ways, especially in closed-source LLMs."
    ],
    "existing_theory": {
        "what_already_exists": "Domain adaptation and prompt engineering are established in NLP and LLM research.",
        "what_is_novel": "This theory formalizes their critical, sometimes non-monotonic, impact on probabilistic forecasting of scientific discoveries.",
        "classification_explanation": "While the components are known, their specific, empirically supported roles in scientific forecasting are newly articulated here.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting]",
            "Lin et al. (2024) Can Language Models Use Forecasting Strategies? [prompting and calibration in forecasting]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
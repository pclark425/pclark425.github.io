<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1899</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1899</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format in which a problem is presented acts as an information bottleneck, modulating the accessibility and salience of relevant cues for LLMs. Formats that foreground key information and minimize extraneous or ambiguous content reduce the cognitive load on the model, enabling more efficient mapping from input to solution. Conversely, formats that obscure, fragment, or overload the input with irrelevant details increase the risk of error by dispersing the model's attention and diluting relevant signal.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Salience Maximization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; salience_of_relevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_improved_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better when key variables and relationships are explicitly stated or highlighted in the prompt. </li>
    <li>Chain-of-thought and step-by-step formats improve performance by making reasoning steps explicit. </li>
    <li>Explicitly structured prompts (e.g., tables, bullet points) increase LLM accuracy on complex tasks. </li>
    <li>LLMs show higher accuracy on math word problems when quantities and operations are clearly delineated. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law generalizes known effects into a broader, predictive theory.</p>            <p><strong>What Already Exists:</strong> It is known that explicit reasoning and highlighting relevant information improves LLM performance.</p>            <p><strong>What is Novel:</strong> The law frames this as a general information bottleneck principle, predicting performance as a function of information salience.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Explicit reasoning improves performance]</li>
    <li>Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [Salience of reasoning steps]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure and salience]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [Explicit structure aids LLMs]</li>
</ul>
            <h3>Statement 1: Irrelevant Information Overload Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; high_amount_of_irrelevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_degraded_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are more likely to make errors when prompts include distractors or extraneous details. </li>
    <li>Performance drops on word problems with irrelevant or misleading information. </li>
    <li>Benchmarks show LLMs are sensitive to the presence of distractor sentences in reading comprehension tasks. </li>
    <li>Adding irrelevant facts to prompts increases hallucination and error rates in LLM outputs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is closely related to existing findings but frames them in a new theoretical context.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs can be distracted by irrelevant information in prompts.</p>            <p><strong>What is Novel:</strong> The law formalizes this as an information bottleneck effect, predicting systematic degradation with increased irrelevant content.</p>
            <p><strong>References:</strong> <ul>
    <li>Sugawara et al. (2020) Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets [Distractor effects]</li>
    <li>Min et al. (2019) Compositional Generalization and Natural Language Understanding [Irrelevant information in NLU]</li>
    <li>Jiang et al. (2022) Prompting LMs for Commonsense Reasoning: A Survey [Prompt structure and distractors]</li>
    <li>Ravichander et al. (2022) Probing the Limits of LLMs with Adversarial Examples [Irrelevant information increases error]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a problem is rewritten to remove all irrelevant details and highlight key variables, LLM accuracy will increase.</li>
                <li>Adding distractor sentences to a prompt will reduce LLM performance, even if the core problem is unchanged.</li>
                <li>Presenting the same problem in a tabular or bullet-point format will improve LLM accuracy compared to a dense paragraph.</li>
                <li>Explicitly marking important information (e.g., bold, labels) in prompts will increase LLM performance.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a problem is presented in a highly compressed format (e.g., dense symbolic notation), performance may depend on the LLM's ability to decompress and identify salient cues, with unpredictable results.</li>
                <li>If LLMs are trained with explicit attention mechanisms to ignore irrelevant information, the bottleneck effect may be mitigated or reversed.</li>
                <li>If LLMs are exposed to adversarially constructed prompts with both high salience and high irrelevant content, the net effect on performance is unknown.</li>
                <li>If LLMs are given multimodal prompts (e.g., text plus diagrams), the effect of salience and overload may interact in non-obvious ways.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well regardless of the amount of irrelevant information, the theory is falsified.</li>
                <li>If increasing salience of relevant information does not improve performance, the theory's core claim is undermined.</li>
                <li>If LLMs show no performance difference between highly structured and unstructured prompts, the bottleneck hypothesis is challenged.</li>
                <li>If LLMs can consistently ignore distractors in all tasks, the theory is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs ignore irrelevant information and focus on the correct cues despite overload. </li>
    <li>Instances where LLMs fail on highly salient, well-structured prompts due to other limitations (e.g., lack of world knowledge). </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known effects into a broader, predictive framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Explicit reasoning]</li>
    <li>Sugawara et al. (2020) Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets [Distractor effects]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure]</li>
    <li>Min et al. (2019) Compositional Generalization and Natural Language Understanding [Irrelevant information]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of LLM Problem Presentation",
    "theory_description": "This theory proposes that the format in which a problem is presented acts as an information bottleneck, modulating the accessibility and salience of relevant cues for LLMs. Formats that foreground key information and minimize extraneous or ambiguous content reduce the cognitive load on the model, enabling more efficient mapping from input to solution. Conversely, formats that obscure, fragment, or overload the input with irrelevant details increase the risk of error by dispersing the model's attention and diluting relevant signal.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Salience Maximization Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "salience_of_relevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_improved_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better when key variables and relationships are explicitly stated or highlighted in the prompt.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought and step-by-step formats improve performance by making reasoning steps explicit.",
                        "uuids": []
                    },
                    {
                        "text": "Explicitly structured prompts (e.g., tables, bullet points) increase LLM accuracy on complex tasks.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs show higher accuracy on math word problems when quantities and operations are clearly delineated.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that explicit reasoning and highlighting relevant information improves LLM performance.",
                    "what_is_novel": "The law frames this as a general information bottleneck principle, predicting performance as a function of information salience.",
                    "classification_explanation": "The law generalizes known effects into a broader, predictive theory.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Explicit reasoning improves performance]",
                        "Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [Salience of reasoning steps]",
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure and salience]",
                        "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [Explicit structure aids LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Irrelevant Information Overload Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "high_amount_of_irrelevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_degraded_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are more likely to make errors when prompts include distractors or extraneous details.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops on word problems with irrelevant or misleading information.",
                        "uuids": []
                    },
                    {
                        "text": "Benchmarks show LLMs are sensitive to the presence of distractor sentences in reading comprehension tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Adding irrelevant facts to prompts increases hallucination and error rates in LLM outputs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs can be distracted by irrelevant information in prompts.",
                    "what_is_novel": "The law formalizes this as an information bottleneck effect, predicting systematic degradation with increased irrelevant content.",
                    "classification_explanation": "The law is closely related to existing findings but frames them in a new theoretical context.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Sugawara et al. (2020) Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets [Distractor effects]",
                        "Min et al. (2019) Compositional Generalization and Natural Language Understanding [Irrelevant information in NLU]",
                        "Jiang et al. (2022) Prompting LMs for Commonsense Reasoning: A Survey [Prompt structure and distractors]",
                        "Ravichander et al. (2022) Probing the Limits of LLMs with Adversarial Examples [Irrelevant information increases error]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a problem is rewritten to remove all irrelevant details and highlight key variables, LLM accuracy will increase.",
        "Adding distractor sentences to a prompt will reduce LLM performance, even if the core problem is unchanged.",
        "Presenting the same problem in a tabular or bullet-point format will improve LLM accuracy compared to a dense paragraph.",
        "Explicitly marking important information (e.g., bold, labels) in prompts will increase LLM performance."
    ],
    "new_predictions_unknown": [
        "If a problem is presented in a highly compressed format (e.g., dense symbolic notation), performance may depend on the LLM's ability to decompress and identify salient cues, with unpredictable results.",
        "If LLMs are trained with explicit attention mechanisms to ignore irrelevant information, the bottleneck effect may be mitigated or reversed.",
        "If LLMs are exposed to adversarially constructed prompts with both high salience and high irrelevant content, the net effect on performance is unknown.",
        "If LLMs are given multimodal prompts (e.g., text plus diagrams), the effect of salience and overload may interact in non-obvious ways."
    ],
    "negative_experiments": [
        "If LLMs perform equally well regardless of the amount of irrelevant information, the theory is falsified.",
        "If increasing salience of relevant information does not improve performance, the theory's core claim is undermined.",
        "If LLMs show no performance difference between highly structured and unstructured prompts, the bottleneck hypothesis is challenged.",
        "If LLMs can consistently ignore distractors in all tasks, the theory is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs ignore irrelevant information and focus on the correct cues despite overload.",
            "uuids": []
        },
        {
            "text": "Instances where LLMs fail on highly salient, well-structured prompts due to other limitations (e.g., lack of world knowledge).",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robustness to distractors in certain structured tasks, such as code completion.",
            "uuids": []
        },
        {
            "text": "In some adversarial settings, LLMs can be prompted to ignore irrelevant information with explicit instructions.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with explicit attention or filtering mechanisms may be less affected by irrelevant information.",
        "Tasks with inherently ambiguous or underspecified information may not benefit from increased salience.",
        "Highly trained domain-specific LLMs may be less sensitive to presentation format."
    ],
    "existing_theory": {
        "what_already_exists": "The effects of distractors and explicit reasoning steps are well-documented in LLM research.",
        "what_is_novel": "The generalization of these effects into an information bottleneck theory is new.",
        "classification_explanation": "The theory synthesizes known effects into a broader, predictive framework.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Explicit reasoning]",
            "Sugawara et al. (2020) Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets [Distractor effects]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure]",
            "Min et al. (2019) Compositional Generalization and Natural Language Understanding [Irrelevant information]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-653",
    "original_theory_name": "Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
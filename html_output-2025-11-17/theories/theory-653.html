<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-653</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-653</p>
                <p><strong>Name:</strong> Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that the format of a prompt acts as a cognitive scaffold for LLMs, enabling or suppressing multi-step reasoning, compositionality, and error correction. Explicitly structured prompt formats—such as chain-of-thought, least-to-most, skills-in-context, stepwise decomposition, persona/role assignment, explicit constraints, and multi-turn interaction—decompose complex tasks into tractable subproblems, unlocking latent capabilities in LLMs. These formats facilitate higher accuracy, more interpretable outputs, and improved error correction, especially on tasks requiring reasoning, planning, or structured outputs. Conversely, formats that suppress intermediate reasoning or fail to provide scaffolding (e.g., no-explanation persona, minimal prompts) can cause performance collapse, even when the model possesses the necessary knowledge.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Explicit Decomposition Formats Enable Multi-Step Reasoning and Compositionality (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; includes &#8594; explicit decomposition (chain-of-thought, least-to-most, skills-in-context, stepwise templates, etc.)<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; multi-step reasoning or compositionality</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves &#8594; substantially higher accuracy and more interpretable outputs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Chain-of-thought, least-to-most, and skills-in-context prompting yield large gains on reasoning tasks; explicit decomposition outperforms standard or direct-answer prompting; multi-turn and iterative-refine formats improve coherence and correctness. <a href="../results/extraction-result-5804.html#e5804.0" class="evidence-link">[e5804.0]</a> <a href="../results/extraction-result-5806.html#e5806.3" class="evidence-link">[e5806.3]</a> <a href="../results/extraction-result-5698.html#e5698.2" class="evidence-link">[e5698.2]</a> <a href="../results/extraction-result-5698.html#e5698.1" class="evidence-link">[e5698.1]</a> <a href="../results/extraction-result-5874.html#e5874.1" class="evidence-link">[e5874.1]</a> <a href="../results/extraction-result-5874.html#e5874.7" class="evidence-link">[e5874.7]</a> <a href="../results/extraction-result-5874.html#e5874.8" class="evidence-link">[e5874.8]</a> <a href="../results/extraction-result-5874.html#e5874.12" class="evidence-link">[e5874.12]</a> <a href="../results/extraction-result-5811.html#e5811.0" class="evidence-link">[e5811.0]</a> <a href="../results/extraction-result-5811.html#e5811.7" class="evidence-link">[e5811.7]</a> <a href="../results/extraction-result-5811.html#e5811.8" class="evidence-link">[e5811.8]</a> <a href="../results/extraction-result-5813.html#e5813.8" class="evidence-link">[e5813.8]</a> <a href="../results/extraction-result-5813.html#e5813.9" class="evidence-link">[e5813.9]</a> <a href="../results/extraction-result-5795.html#e5795.3" class="evidence-link">[e5795.3]</a> <a href="../results/extraction-result-5828.html#e5828.8" class="evidence-link">[e5828.8]</a> <a href="../results/extraction-result-5705.html#e5705.0" class="evidence-link">[e5705.0]</a> <a href="../results/extraction-result-5705.html#e5705.3" class="evidence-link">[e5705.3]</a> <a href="../results/extraction-result-5810.html#e5810.0" class="evidence-link">[e5810.0]</a> <a href="../results/extraction-result-5810.html#e5810.6" class="evidence-link">[e5810.6]</a> <a href="../results/extraction-result-5816.html#e5816.3" class="evidence-link">[e5816.3]</a> <a href="../results/extraction-result-5834.html#e5834.3" class="evidence-link">[e5834.3]</a> <a href="../results/extraction-result-5834.html#e5834.2" class="evidence-link">[e5834.2]</a> <a href="../results/extraction-result-5836.html#e5836.0" class="evidence-link">[e5836.0]</a> <a href="../results/extraction-result-5836.html#e5836.4" class="evidence-link">[e5836.4]</a> <a href="../results/extraction-result-5836.html#e5836.6" class="evidence-link">[e5836.6]</a> <a href="../results/extraction-result-5836.html#e5836.7" class="evidence-link">[e5836.7]</a> <a href="../results/extraction-result-5836.html#e5836.8" class="evidence-link">[e5836.8]</a> <a href="../results/extraction-result-5695.html#e5695.3" class="evidence-link">[e5695.3]</a> <a href="../results/extraction-result-5695.html#e5695.4" class="evidence-link">[e5695.4]</a> <a href="../results/extraction-result-5695.html#e5695.6" class="evidence-link">[e5695.6]</a> <a href="../results/extraction-result-5695.html#e5695.7" class="evidence-link">[e5695.7]</a> <a href="../results/extraction-result-5695.html#e5695.10" class="evidence-link">[e5695.10]</a> <a href="../results/extraction-result-5695.html#e5695.11" class="evidence-link">[e5695.11]</a> <a href="../results/extraction-result-5695.html#e5695.9" class="evidence-link">[e5695.9]</a> <a href="../results/extraction-result-5695.html#e5695.1" class="evidence-link">[e5695.1]</a> <a href="../results/extraction-result-5695.html#e5695.0" class="evidence-link">[e5695.0]</a> <a href="../results/extraction-result-5690.html#e5690.8" class="evidence-link">[e5690.8]</a> <a href="../results/extraction-result-5690.html#e5690.2" class="evidence-link">[e5690.2]</a> <a href="../results/extraction-result-5692.html#e5692.0" class="evidence-link">[e5692.0]</a> <a href="../results/extraction-result-5692.html#e5692.4" class="evidence-link">[e5692.4]</a> <a href="../results/extraction-result-5692.html#e5692.7" class="evidence-link">[e5692.7]</a> <a href="../results/extraction-result-5708.html#e5708.0" class="evidence-link">[e5708.0]</a> <a href="../results/extraction-result-5708.html#e5708.2" class="evidence-link">[e5708.2]</a> <a href="../results/extraction-result-5825.html#e5825.0" class="evidence-link">[e5825.0]</a> <a href="../results/extraction-result-5825.html#e5825.6" class="evidence-link">[e5825.6]</a> <a href="../results/extraction-result-5825.html#e5825.8" class="evidence-link">[e5825.8]</a> <a href="../results/extraction-result-5825.html#e5825.9" class="evidence-link">[e5825.9]</a> <a href="../results/extraction-result-5825.html#e5825.10" class="evidence-link">[e5825.10]</a> <a href="../results/extraction-result-5826.html#e5826.5" class="evidence-link">[e5826.5]</a> <a href="../results/extraction-result-5828.html#e5828.8" class="evidence-link">[e5828.8]</a> <a href="../results/extraction-result-5829.html#e5829.4" class="evidence-link">[e5829.4]</a> <a href="../results/extraction-result-5829.html#e5829.5" class="evidence-link">[e5829.5]</a> <a href="../results/extraction-result-5830.html#e5830.1" class="evidence-link">[e5830.1]</a> <a href="../results/extraction-result-5831.html#e5831.3" class="evidence-link">[e5831.3]</a> <a href="../results/extraction-result-5831.html#e5831.6" class="evidence-link">[e5831.6]</a> <a href="../results/extraction-result-5831.html#e5831.8" class="evidence-link">[e5831.8]</a> <a href="../results/extraction-result-5832.html#e5832.5" class="evidence-link">[e5832.5]</a> <a href="../results/extraction-result-5832.html#e5832.9" class="evidence-link">[e5832.9]</a> <a href="../results/extraction-result-5833.html#e5833.2" class="evidence-link">[e5833.2]</a> <a href="../results/extraction-result-5833.html#e5833.5" class="evidence-link">[e5833.5]</a> <a href="../results/extraction-result-5833.html#e5833.9" class="evidence-link">[e5833.9]</a> <a href="../results/extraction-result-5834.html#e5834.0" class="evidence-link">[e5834.0]</a> <a href="../results/extraction-result-5834.html#e5834.2" class="evidence-link">[e5834.2]</a> <a href="../results/extraction-result-5834.html#e5834.3" class="evidence-link">[e5834.3]</a> <a href="../results/extraction-result-5835.html#e5835.0" class="evidence-link">[e5835.0]</a> <a href="../results/extraction-result-5835.html#e5835.7" class="evidence-link">[e5835.7]</a> <a href="../results/e5835.9.html#e5835.9" class="evidence-link">[e5835.9]</a> <a href="../results/extraction-result-5836.html#e5836.0" class="evidence-link">[e5836.0]</a> <a href="../results/extraction-result-5836.html#e5836.4" class="evidence-link">[e5836.4]</a> <a href="../results/extraction-result-5836.html#e5836.6" class="evidence-link">[e5836.6]</a> <a href="../results/extraction-result-5836.html#e5836.7" class="evidence-link">[e5836.7]</a> <a href="../results/extraction-result-5836.html#e5836.8" class="evidence-link">[e5836.8]</a> <a href="../results/extraction-result-5839.html#e5839.2" class="evidence-link">[e5839.2]</a> <a href="../results/extraction-result-5839.html#e5839.7" class="evidence-link">[e5839.7]</a> <a href="../results/extraction-result-5842.html#e5842.2" class="evidence-link">[e5842.2]</a> <a href="../results/extraction-result-5846.html#e5846.1" class="evidence-link">[e5846.1]</a> <a href="../results/extraction-result-5846.html#e5846.5" class="evidence-link">[e5846.5]</a> <a href="../results/extraction-result-5855.html#e5855.1" class="evidence-link">[e5855.1]</a> <a href="../results/extraction-result-5855.html#e5855.3" class="evidence-link">[e5855.3]</a> <a href="../results/extraction-result-5855.html#e5855.5" class="evidence-link">[e5855.5]</a> <a href="../results/extraction-result-5855.html#e5855.6" class="evidence-link">[e5855.6]</a> <a href="../results/extraction-result-5860.html#e5860.1" class="evidence-link">[e5860.1]</a> <a href="../results/extraction-result-5862.html#e5862.4" class="evidence-link">[e5862.4]</a> <a href="../results/extraction-result-5862.html#e5862.7" class="evidence-link">[e5862.7]</a> <a href="../results/extraction-result-5866.html#e5866.0" class="evidence-link">[e5866.0]</a> <a href="../results/extraction-result-5866.html#e5866.1" class="evidence-link">[e5866.1]</a> <a href="../results/extraction-result-5866.html#e5866.5" class="evidence-link">[e5866.5]</a> <a href="../results/extraction-result-5867.html#e5867.1" class="evidence-link">[e5867.1]</a> <a href="../results/extraction-result-5867.html#e5867.4" class="evidence-link">[e5867.4]</a> <a href="../results/extraction-result-5867.html#e5867.5" class="evidence-link">[e5867.5]</a> <a href="../results/extraction-result-5867.html#e5867.6" class="evidence-link">[e5867.6]</a> <a href="../results/extraction-result-5869.html#e5869.1" class="evidence-link">[e5869.1]</a> <a href="../results/extraction-result-5869.html#e5869.3" class="evidence-link">[e5869.3]</a> <a href="../results/extraction-result-5869.html#e5869.5" class="evidence-link">[e5869.5]</a> <a href="../results/extraction-result-5874.html#e5874.1" class="evidence-link">[e5874.1]</a> <a href="../results/extraction-result-5874.html#e5874.7" class="evidence-link">[e5874.7]</a> <a href="../results/extraction-result-5874.html#e5874.8" class="evidence-link">[e5874.8]</a> <a href="../results/extraction-result-5874.html#e5874.12" class="evidence-link">[e5874.12]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The unification of decomposition, skills-in-context, iterative-refine, and multi-turn as a general class of scaffolding formats is novel.</p>            <p><strong>What Already Exists:</strong> Chain-of-thought and decomposition prompting are recognized as effective for reasoning tasks.</p>            <p><strong>What is Novel:</strong> This law generalizes the effect to all explicit decomposition formats and links the benefit to cognitive scaffolding, compositionality, and error correction, not just reasoning.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [L2M]</li>
    <li>Zhou et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [ToT]</li>
    <li>Zhou et al. (2023) Skills-in-context prompting: Unlocking compositionality in large language models [skills-in-context]</li>
</ul>
            <h3>Statement 1: Suppression of Reasoning Scaffolding Collapses Performance on Complex Tasks (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; suppresses &#8594; intermediate reasoning (e.g., no-explanation persona, minimal prompt, direct answer only)<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; multi-step reasoning or structured output</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; large drop in accuracy and increased hallucination or error rates</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>No-explanation persona caused a 58.7% drop in accuracy on MMLU; minimal prompts underperform structured/decomposed prompts; baseline direct-answer prompts underperform CoT and decomposed formats; suppression of reasoning increases hallucination and error rates. <a href="../results/extraction-result-5712.html#e5712.23" class="evidence-link">[e5712.23]</a> <a href="../results/extraction-result-5705.html#e5705.1" class="evidence-link">[e5705.1]</a> <a href="../results/extraction-result-5702.html#e5702.1" class="evidence-link">[e5702.1]</a> <a href="../results/extraction-result-5705.html#e5705.0" class="evidence-link">[e5705.0]</a> <a href="../results/extraction-result-5810.html#e5810.0" class="evidence-link">[e5810.0]</a> <a href="../results/extraction-result-5813.html#e5813.8" class="evidence-link">[e5813.8]</a> <a href="../results/extraction-result-5813.html#e5813.7" class="evidence-link">[e5813.7]</a> <a href="../results/extraction-result-5813.html#e5813.9" class="evidence-link">[e5813.9]</a> <a href="../results/extraction-result-5816.html#e5816.3" class="evidence-link">[e5816.3]</a> <a href="../results/extraction-result-5836.html#e5836.4" class="evidence-link">[e5836.4]</a> <a href="../results/extraction-result-5836.html#e5836.6" class="evidence-link">[e5836.6]</a> <a href="../results/extraction-result-5836.html#e5836.7" class="evidence-link">[e5836.7]</a> <a href="../results/extraction-result-5836.html#e5836.8" class="evidence-link">[e5836.8]</a> <a href="../results/extraction-result-5825.html#e5825.10" class="evidence-link">[e5825.10]</a> <a href="../results/extraction-result-5855.html#e5855.6" class="evidence-link">[e5855.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The explicit mapping from suppression of scaffolding to error/hallucination rates is a novel generalization.</p>            <p><strong>What Already Exists:</strong> It is known that omitting reasoning steps can reduce performance on complex tasks.</p>            <p><strong>What is Novel:</strong> This law formalizes the suppression of scaffolding as a general mechanism for performance collapse and links it to increased hallucination and error rates.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT]</li>
    <li>Zhou et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [ToT]</li>
</ul>
            <h3>Statement 2: Iterative and Multi-Turn Formats Enable Error Correction and Output Refinement (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; enables &#8594; multi-turn interaction or iterative self-refinement</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can &#8594; correct prior errors and improve output quality over turns</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Multi-turn post-editing in MT improved translation quality; iterative-refine improved coherence in creative writing; multi-turn decomposed flag drawing improved SVG accuracy; multi-turn and iterative formats enable error correction and output refinement. <a href="../results/extraction-result-5834.html#e5834.2" class="evidence-link">[e5834.2]</a> <a href="../results/extraction-result-5874.html#e5874.12" class="evidence-link">[e5874.12]</a> <a href="../results/extraction-result-5834.html#e5834.3" class="evidence-link">[e5834.3]</a> <a href="../results/extraction-result-5698.html#e5698.3" class="evidence-link">[e5698.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The explicit generalization to all iterative/multi-turn formats as scaffolding is novel.</p>            <p><strong>What Already Exists:</strong> Iterative prompting and multi-turn interaction are recognized as useful for error correction.</p>            <p><strong>What is Novel:</strong> This law unifies these as a general mechanism for output refinement and links them to cognitive scaffolding.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [ToT, iterative-refine]</li>
    <li>Wang et al. (2023) A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity [multi-turn post-editing]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new task is presented with explicit stepwise decomposition or skills-in-context prompting, LLM performance will improve relative to direct-answer or minimal prompts, especially for tasks requiring reasoning or planning.</li>
                <li>If a multi-turn or iterative-refine format is used for a generation task, output quality (e.g., coherence, factuality) will improve over turns.</li>
                <li>If a prompt format suppresses intermediate reasoning (e.g., 'just give the answer'), performance will drop on tasks requiring multi-step reasoning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained with adversarially decomposed prompts (incorrect or misleading decompositions), it may develop the ability to detect and reject faulty scaffolding, potentially improving robustness to user error.</li>
                <li>If a model is given a hybrid format that combines multiple scaffolding mechanisms (e.g., skills-in-context + ToT + iterative-refine), it may exhibit emergent capabilities in compositional reasoning or self-correction not seen in any single format.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If explicit decomposition or scaffolding formats do not improve performance on tasks requiring multi-step reasoning, this would challenge the theory.</li>
                <li>If suppressing reasoning scaffolding does not reduce accuracy or increase hallucination/error rates on complex tasks, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks (e.g., simple factual recall or robust domains) may not benefit from decomposition or scaffolding formats. <a href="../results/extraction-result-5816.html#e5816.0" class="evidence-link">[e5816.0]</a> <a href="../results/extraction-result-5816.html#e5816.5" class="evidence-link">[e5816.5]</a> <a href="../results/extraction-result-5813.html#e5813.7" class="evidence-link">[e5813.7]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While the components are individually recognized, the explicit, mechanistic, and predictive synthesis is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [L2M]</li>
    <li>Zhou et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [ToT]</li>
    <li>Zhou et al. (2023) Skills-in-context prompting: Unlocking compositionality in large language models [skills-in-context]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs",
    "theory_description": "This theory posits that the format of a prompt acts as a cognitive scaffold for LLMs, enabling or suppressing multi-step reasoning, compositionality, and error correction. Explicitly structured prompt formats—such as chain-of-thought, least-to-most, skills-in-context, stepwise decomposition, persona/role assignment, explicit constraints, and multi-turn interaction—decompose complex tasks into tractable subproblems, unlocking latent capabilities in LLMs. These formats facilitate higher accuracy, more interpretable outputs, and improved error correction, especially on tasks requiring reasoning, planning, or structured outputs. Conversely, formats that suppress intermediate reasoning or fail to provide scaffolding (e.g., no-explanation persona, minimal prompts) can cause performance collapse, even when the model possesses the necessary knowledge.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Explicit Decomposition Formats Enable Multi-Step Reasoning and Compositionality",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "includes",
                        "object": "explicit decomposition (chain-of-thought, least-to-most, skills-in-context, stepwise templates, etc.)"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "multi-step reasoning or compositionality"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves",
                        "object": "substantially higher accuracy and more interpretable outputs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Chain-of-thought, least-to-most, and skills-in-context prompting yield large gains on reasoning tasks; explicit decomposition outperforms standard or direct-answer prompting; multi-turn and iterative-refine formats improve coherence and correctness.",
                        "uuids": [
                            "e5804.0",
                            "e5806.3",
                            "e5698.2",
                            "e5698.1",
                            "e5874.1",
                            "e5874.7",
                            "e5874.8",
                            "e5874.12",
                            "e5811.0",
                            "e5811.7",
                            "e5811.8",
                            "e5813.8",
                            "e5813.9",
                            "e5795.3",
                            "e5828.8",
                            "e5705.0",
                            "e5705.3",
                            "e5810.0",
                            "e5810.6",
                            "e5816.3",
                            "e5834.3",
                            "e5834.2",
                            "e5836.0",
                            "e5836.4",
                            "e5836.6",
                            "e5836.7",
                            "e5836.8",
                            "e5695.3",
                            "e5695.4",
                            "e5695.6",
                            "e5695.7",
                            "e5695.10",
                            "e5695.11",
                            "e5695.9",
                            "e5695.1",
                            "e5695.0",
                            "e5690.8",
                            "e5690.2",
                            "e5692.0",
                            "e5692.4",
                            "e5692.7",
                            "e5708.0",
                            "e5708.2",
                            "e5825.0",
                            "e5825.6",
                            "e5825.8",
                            "e5825.9",
                            "e5825.10",
                            "e5826.5",
                            "e5828.8",
                            "e5829.4",
                            "e5829.5",
                            "e5830.1",
                            "e5831.3",
                            "e5831.6",
                            "e5831.8",
                            "e5832.5",
                            "e5832.9",
                            "e5833.2",
                            "e5833.5",
                            "e5833.9",
                            "e5834.0",
                            "e5834.2",
                            "e5834.3",
                            "e5835.0",
                            "e5835.7",
                            "e5835.9",
                            "e5836.0",
                            "e5836.4",
                            "e5836.6",
                            "e5836.7",
                            "e5836.8",
                            "e5839.2",
                            "e5839.7",
                            "e5842.2",
                            "e5846.1",
                            "e5846.5",
                            "e5855.1",
                            "e5855.3",
                            "e5855.5",
                            "e5855.6",
                            "e5860.1",
                            "e5862.4",
                            "e5862.7",
                            "e5866.0",
                            "e5866.1",
                            "e5866.5",
                            "e5867.1",
                            "e5867.4",
                            "e5867.5",
                            "e5867.6",
                            "e5869.1",
                            "e5869.3",
                            "e5869.5",
                            "e5874.1",
                            "e5874.7",
                            "e5874.8",
                            "e5874.12"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Chain-of-thought and decomposition prompting are recognized as effective for reasoning tasks.",
                    "what_is_novel": "This law generalizes the effect to all explicit decomposition formats and links the benefit to cognitive scaffolding, compositionality, and error correction, not just reasoning.",
                    "classification_explanation": "The unification of decomposition, skills-in-context, iterative-refine, and multi-turn as a general class of scaffolding formats is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT]",
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [L2M]",
                        "Zhou et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [ToT]",
                        "Zhou et al. (2023) Skills-in-context prompting: Unlocking compositionality in large language models [skills-in-context]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Suppression of Reasoning Scaffolding Collapses Performance on Complex Tasks",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "suppresses",
                        "object": "intermediate reasoning (e.g., no-explanation persona, minimal prompt, direct answer only)"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "multi-step reasoning or structured output"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "large drop in accuracy and increased hallucination or error rates"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "No-explanation persona caused a 58.7% drop in accuracy on MMLU; minimal prompts underperform structured/decomposed prompts; baseline direct-answer prompts underperform CoT and decomposed formats; suppression of reasoning increases hallucination and error rates.",
                        "uuids": [
                            "e5712.23",
                            "e5705.1",
                            "e5702.1",
                            "e5705.0",
                            "e5810.0",
                            "e5813.8",
                            "e5813.7",
                            "e5813.9",
                            "e5816.3",
                            "e5836.4",
                            "e5836.6",
                            "e5836.7",
                            "e5836.8",
                            "e5825.10",
                            "e5855.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that omitting reasoning steps can reduce performance on complex tasks.",
                    "what_is_novel": "This law formalizes the suppression of scaffolding as a general mechanism for performance collapse and links it to increased hallucination and error rates.",
                    "classification_explanation": "The explicit mapping from suppression of scaffolding to error/hallucination rates is a novel generalization.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT]",
                        "Zhou et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [ToT]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Iterative and Multi-Turn Formats Enable Error Correction and Output Refinement",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "enables",
                        "object": "multi-turn interaction or iterative self-refinement"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can",
                        "object": "correct prior errors and improve output quality over turns"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Multi-turn post-editing in MT improved translation quality; iterative-refine improved coherence in creative writing; multi-turn decomposed flag drawing improved SVG accuracy; multi-turn and iterative formats enable error correction and output refinement.",
                        "uuids": [
                            "e5834.2",
                            "e5874.12",
                            "e5834.3",
                            "e5698.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Iterative prompting and multi-turn interaction are recognized as useful for error correction.",
                    "what_is_novel": "This law unifies these as a general mechanism for output refinement and links them to cognitive scaffolding.",
                    "classification_explanation": "The explicit generalization to all iterative/multi-turn formats as scaffolding is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [ToT, iterative-refine]",
                        "Wang et al. (2023) A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity [multi-turn post-editing]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new task is presented with explicit stepwise decomposition or skills-in-context prompting, LLM performance will improve relative to direct-answer or minimal prompts, especially for tasks requiring reasoning or planning.",
        "If a multi-turn or iterative-refine format is used for a generation task, output quality (e.g., coherence, factuality) will improve over turns.",
        "If a prompt format suppresses intermediate reasoning (e.g., 'just give the answer'), performance will drop on tasks requiring multi-step reasoning."
    ],
    "new_predictions_unknown": [
        "If a model is trained with adversarially decomposed prompts (incorrect or misleading decompositions), it may develop the ability to detect and reject faulty scaffolding, potentially improving robustness to user error.",
        "If a model is given a hybrid format that combines multiple scaffolding mechanisms (e.g., skills-in-context + ToT + iterative-refine), it may exhibit emergent capabilities in compositional reasoning or self-correction not seen in any single format."
    ],
    "negative_experiments": [
        "If explicit decomposition or scaffolding formats do not improve performance on tasks requiring multi-step reasoning, this would challenge the theory.",
        "If suppressing reasoning scaffolding does not reduce accuracy or increase hallucination/error rates on complex tasks, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks (e.g., simple factual recall or robust domains) may not benefit from decomposition or scaffolding formats.",
            "uuids": [
                "e5816.0",
                "e5816.5",
                "e5813.7"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Zero-shot-CoT and other scaffolding formats can sometimes reduce performance on tasks that do not require reasoning or where the model's knowledge is robust.",
            "uuids": [
                "e5825.10",
                "e5822.7"
            ]
        }
    ],
    "special_cases": [
        "For tasks where the model's knowledge is robust or the answer is a simple retrieval, scaffolding formats may not help and can even introduce unnecessary verbosity or errors.",
        "On tasks with ambiguous or ill-posed decompositions, scaffolding formats may expose or amplify logical errors."
    ],
    "existing_theory": {
        "what_already_exists": "Chain-of-thought, least-to-most, and iterative prompting are recognized as effective for reasoning and error correction.",
        "what_is_novel": "This theory unifies all explicit decomposition, scaffolding, and iterative formats as a general mechanism for unlocking LLM capabilities, and formalizes the suppression of scaffolding as a mechanism for performance collapse.",
        "classification_explanation": "While the components are individually recognized, the explicit, mechanistic, and predictive synthesis is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [L2M]",
            "Zhou et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [ToT]",
            "Zhou et al. (2023) Skills-in-context prompting: Unlocking compositionality in large language models [skills-in-context]"
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
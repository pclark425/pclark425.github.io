<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memory Format-Task Alignment Theory for LLM Text Game Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-470</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-470</p>
                <p><strong>Name:</strong> Memory Format-Task Alignment Theory for LLM Text Game Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks, based on the following results.</p>
                <p><strong>Description:</strong> The effectiveness of memory in LLM agents for text games is determined by the alignment between the memory format (structured, episodic, working, or reflective) and the specific demands of the task (e.g., partial observability, long-horizon planning, social reasoning, or exploration). Optimal performance is achieved when the memory system is tailored to the information bottlenecks and reasoning requirements of the environment, with structured memory excelling in world-modeling and planning, episodic/reflective memory in learning from experience and error correction, and working memory in short-term context tracking. Misalignment between memory format and task demands can degrade performance or introduce instability.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structured Memory is Optimal for World Modeling and Planning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; requires &#8594; tracking persistent world state, object relations, or long-term dependencies</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; should_use &#8594; structured memory (e.g., knowledge graph, belief graph, map, object database)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Knowledge graph and belief graph agents (KG-DQN, GATA, Worldformer, NAIL, Q*BERT, MC!Q*BERT, KGA2C, Knowledge Graph Memory, Belief Graph, WorldObjectTree, NAIL, Q*BERT-S, GO!Q*BERT, GATA-GTP, Transformer+R-GCN+RelEmb, Seq2Seq, Seq2Seq baseline, Knowledge Graph (ground-truth), Knowledge Graph (KG), Knowledge Graph (component of KG-DQN), Knowledge Graph Memory (concept)) outperform others on tasks requiring persistent world state and planning, including partial observability, long-horizon dependencies, and action pruning. <a href="../results/extraction-result-3059.html#e3059.0" class="evidence-link">[e3059.0]</a> <a href="../results/extraction-result-3059.html#e3059.5" class="evidence-link">[e3059.5]</a> <a href="../results/extraction-result-3254.html#e3254.0" class="evidence-link">[e3254.0]</a> <a href="../results/extraction-result-3255.html#e3255.0" class="evidence-link">[e3255.0]</a> <a href="../results/extraction-result-3055.html#e3055.0" class="evidence-link">[e3055.0]</a> <a href="../results/extraction-result-3243.html#e3243.0" class="evidence-link">[e3243.0]</a> <a href="../results/extraction-result-3243.html#e3243.4" class="evidence-link">[e3243.4]</a> <a href="../results/extraction-result-3261.html#e3261.0" class="evidence-link">[e3261.0]</a> <a href="../results/extraction-result-3255.html#e3255.1" class="evidence-link">[e3255.1]</a> <a href="../results/extraction-result-3255.html#e3255.2" class="evidence-link">[e3255.2]</a> <a href="../results/extraction-result-3255.html#e3255.3" class="evidence-link">[e3255.3]</a> <a href="../results/extraction-result-3263.html#e3263.0" class="evidence-link">[e3263.0]</a> <a href="../results/extraction-result-3263.html#e3263.1" class="evidence-link">[e3263.1]</a> <a href="../results/extraction-result-3254.html#e3254.1" class="evidence-link">[e3254.1]</a> <a href="../results/extraction-result-3250.html#e3250.0" class="evidence-link">[e3250.0]</a> <a href="../results/extraction-result-3055.html#e3055.2" class="evidence-link">[e3055.2]</a> <a href="../results/extraction-result-3059.html#e3059.1" class="evidence-link">[e3059.1]</a> <a href="../results/extraction-result-3059.html#e3059.2" class="evidence-link">[e3059.2]</a> <a href="../results/extraction-result-3253.html#e3253.0" class="evidence-link">[e3253.0]</a> <a href="../results/extraction-result-3253.html#e3253.2" class="evidence-link">[e3253.2]</a> <a href="../results/extraction-result-3243.html#e3243.1" class="evidence-link">[e3243.1]</a> <a href="../results/extraction-result-3261.html#e3261.2" class="evidence-link">[e3261.2]</a> <a href="../results/extraction-result-3261.html#e3261.3" class="evidence-link">[e3261.3]</a> <a href="../results/extraction-result-3261.html#e3261.5" class="evidence-link">[e3261.5]</a> <a href="../results/extraction-result-3061.html#e3061.0" class="evidence-link">[e3061.0]</a> <a href="../results/extraction-result-3061.html#e3061.1" class="evidence-link">[e3061.1]</a> <a href="../results/extraction-result-3061.html#e3061.2" class="evidence-link">[e3061.2]</a> <a href="../results/extraction-result-3061.html#e3061.4" class="evidence-link">[e3061.4]</a> </li>
    <li>Agents with structured memory (e.g., knowledge graphs, belief graphs, maps) enable action pruning, faster convergence, and improved handling of partial observability and long-term dependencies. <a href="../results/extraction-result-3254.html#e3254.0" class="evidence-link">[e3254.0]</a> <a href="../results/extraction-result-3254.html#e3254.1" class="evidence-link">[e3254.1]</a> <a href="../results/extraction-result-3255.html#e3255.0" class="evidence-link">[e3255.0]</a> <a href="../results/extraction-result-3255.html#e3255.1" class="evidence-link">[e3255.1]</a> <a href="../results/extraction-result-3255.html#e3255.2" class="evidence-link">[e3255.2]</a> <a href="../results/extraction-result-3255.html#e3255.3" class="evidence-link">[e3255.3]</a> <a href="../results/extraction-result-3263.html#e3263.0" class="evidence-link">[e3263.0]</a> <a href="../results/extraction-result-3263.html#e3263.1" class="evidence-link">[e3263.1]</a> <a href="../results/extraction-result-3059.html#e3059.0" class="evidence-link">[e3059.0]</a> <a href="../results/extraction-result-3059.html#e3059.5" class="evidence-link">[e3059.5]</a> <a href="../results/extraction-result-3243.html#e3243.0" class="evidence-link">[e3243.0]</a> <a href="../results/extraction-result-3243.html#e3243.4" class="evidence-link">[e3243.4]</a> <a href="../results/extraction-result-3250.html#e3250.0" class="evidence-link">[e3250.0]</a> <a href="../results/extraction-result-3055.html#e3055.0" class="evidence-link">[e3055.0]</a> <a href="../results/extraction-result-3055.html#e3055.2" class="evidence-link">[e3055.2]</a> <a href="../results/extraction-result-3059.html#e3059.1" class="evidence-link">[e3059.1]</a> <a href="../results/extraction-result-3059.html#e3059.2" class="evidence-link">[e3059.2]</a> <a href="../results/extraction-result-3253.html#e3253.0" class="evidence-link">[e3253.0]</a> <a href="../results/extraction-result-3253.html#e3253.2" class="evidence-link">[e3253.2]</a> <a href="../results/extraction-result-3243.html#e3243.1" class="evidence-link">[e3243.1]</a> <a href="../results/extraction-result-3261.html#e3261.0" class="evidence-link">[e3261.0]</a> <a href="../results/extraction-result-3261.html#e3261.2" class="evidence-link">[e3261.2]</a> <a href="../results/extraction-result-3261.html#e3261.3" class="evidence-link">[e3261.3]</a> <a href="../results/extraction-result-3261.html#e3261.5" class="evidence-link">[e3261.5]</a> <a href="../results/extraction-result-3061.html#e3061.0" class="evidence-link">[e3061.0]</a> <a href="../results/extraction-result-3061.html#e3061.1" class="evidence-link">[e3061.1]</a> <a href="../results/extraction-result-3061.html#e3061.2" class="evidence-link">[e3061.2]</a> <a href="../results/extraction-result-3061.html#e3061.4" class="evidence-link">[e3061.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Episodic/Reflective Memory is Optimal for Learning from Failure and Generalization (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; involves &#8594; repeated trials, sparse rewards, or requires learning from mistakes or generalization across tasks</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; should_use &#8594; episodic or reflective memory (e.g., self-reflection, tips, skill library, replay buffer, experience pool, curriculum/task memory)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Reflexion, Introspective Tips, skill libraries, curriculum memory, experience replay, and replay buffers enable rapid learning from failure, generalization to new tasks, and improved sample efficiency. <a href="../results/extraction-result-3245.html#e3245.0" class="evidence-link">[e3245.0]</a> <a href="../results/extraction-result-3245.html#e3245.1" class="evidence-link">[e3245.1]</a> <a href="../results/extraction-result-3245.html#e3245.2" class="evidence-link">[e3245.2]</a> <a href="../results/extraction-result-3031.html#e3031.0" class="evidence-link">[e3031.0]</a> <a href="../results/extraction-result-3031.html#e3031.2" class="evidence-link">[e3031.2]</a> <a href="../results/extraction-result-3031.html#e3031.1" class="evidence-link">[e3031.1]</a> <a href="../results/extraction-result-3274.html#e3274.0" class="evidence-link">[e3274.0]</a> <a href="../results/extraction-result-3274.html#e3274.1" class="evidence-link">[e3274.1]</a> <a href="../results/extraction-result-3058.html#e3058.0" class="evidence-link">[e3058.0]</a> <a href="../results/extraction-result-3058.html#e3058.3" class="evidence-link">[e3058.3]</a> <a href="../results/extraction-result-3058.html#e3058.5" class="evidence-link">[e3058.5]</a> <a href="../results/extraction-result-3058.html#e3058.6" class="evidence-link">[e3058.6]</a> <a href="../results/extraction-result-3269.html#e3269.0" class="evidence-link">[e3269.0]</a> <a href="../results/extraction-result-3269.html#e3269.2" class="evidence-link">[e3269.2]</a> <a href="../results/extraction-result-3269.html#e3269.3" class="evidence-link">[e3269.3]</a> <a href="../results/extraction-result-3248.html#e3248.1" class="evidence-link">[e3248.1]</a> <a href="../results/extraction-result-3265.html#e3265.1" class="evidence-link">[e3265.1]</a> <a href="../results/extraction-result-3044.html#e3044.1" class="evidence-link">[e3044.1]</a> <a href="../results/extraction-result-3044.html#e3044.2" class="evidence-link">[e3044.2]</a> <a href="../results/extraction-result-3237.html#e3237.0" class="evidence-link">[e3237.0]</a> <a href="../results/extraction-result-3274.html#e3274.5" class="evidence-link">[e3274.5]</a> </li>
    <li>Replay buffers, categorized experience replay, and hindsight relabeling (LID-ADG) enable learning from failed trajectories and improve generalization in sparse-reward, long-horizon, or partially observable tasks. <a href="../results/extraction-result-3058.html#e3058.0" class="evidence-link">[e3058.0]</a> <a href="../results/extraction-result-3058.html#e3058.3" class="evidence-link">[e3058.3]</a> <a href="../results/extraction-result-3058.html#e3058.5" class="evidence-link">[e3058.5]</a> <a href="../results/extraction-result-3058.html#e3058.6" class="evidence-link">[e3058.6]</a> <a href="../results/extraction-result-3248.html#e3248.1" class="evidence-link">[e3248.1]</a> <a href="../results/extraction-result-3269.html#e3269.0" class="evidence-link">[e3269.0]</a> <a href="../results/extraction-result-3269.html#e3269.2" class="evidence-link">[e3269.2]</a> <a href="../results/extraction-result-3269.html#e3269.3" class="evidence-link">[e3269.3]</a> </li>
    <li>Skill libraries (Voyager), curriculum memory, and experience pools (Werewolf-LLM-Agent) enable transfer and rapid adaptation to new tasks or environments. <a href="../results/extraction-result-3274.html#e3274.0" class="evidence-link">[e3274.0]</a> <a href="../results/extraction-result-3274.html#e3274.1" class="evidence-link">[e3274.1]</a> <a href="../results/extraction-result-3237.html#e3237.0" class="evidence-link">[e3237.0]</a> <a href="../results/extraction-result-3274.html#e3274.5" class="evidence-link">[e3274.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Working Memory is Sufficient for Short-Horizon, Fully Observable, or Simple Tasks (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; is &#8594; short-horizon or fully observable or requires only immediate context</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; can_succeed_with &#8594; working memory (context window, short-term buffer, prompt history)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Agents using only context window or short-term history (e.g., Messenger, ALFWorld, LLM-Dialog, LLM agent (GPT-3.5-turbo), LLM agent (GPT-4), ChatGPT (baseline), SWIFT, LSTM-DQN with per-observation encoding, BERT Bi-Ranker, Transformer MemNet, context/grounding memory, AGENTBOARD-memory-protocol, ALFWorld (House-holding), Chain-of-Thought, ReAct, Last-Thoughts, etc.) perform well on short or fully observable tasks. <a href="../results/extraction-result-3057.html#e3057.1" class="evidence-link">[e3057.1]</a> <a href="../results/extraction-result-3246.html#e3246.0" class="evidence-link">[e3246.0]</a> <a href="../results/extraction-result-3268.html#e3268.0" class="evidence-link">[e3268.0]</a> <a href="../results/extraction-result-3043.html#e3043.0" class="evidence-link">[e3043.0]</a> <a href="../results/extraction-result-3043.html#e3043.1" class="evidence-link">[e3043.1]</a> <a href="../results/extraction-result-3244.html#e3244.0" class="evidence-link">[e3244.0]</a> <a href="../results/extraction-result-3270.html#e3270.1" class="evidence-link">[e3270.1]</a> <a href="../results/extraction-result-3270.html#e3270.0" class="evidence-link">[e3270.0]</a> <a href="../results/extraction-result-3270.html#e3270.2" class="evidence-link">[e3270.2]</a> <a href="../results/extraction-result-3261.html#e3261.2" class="evidence-link">[e3261.2]</a> <a href="../results/extraction-result-3258.html#e3258.2" class="evidence-link">[e3258.2]</a> <a href="../results/extraction-result-3275.html#e3275.1" class="evidence-link">[e3275.1]</a> <a href="../results/extraction-result-3275.html#e3275.0" class="evidence-link">[e3275.0]</a> <a href="../results/extraction-result-3275.html#e3275.4" class="evidence-link">[e3275.4]</a> <a href="../results/extraction-result-3025.html#e3025.2" class="evidence-link">[e3025.2]</a> <a href="../results/extraction-result-3246.html#e3246.0" class="evidence-link">[e3246.0]</a> <a href="../results/extraction-result-3262.html#e3262.0" class="evidence-link">[e3262.0]</a> <a href="../results/extraction-result-3257.html#e3257.0" class="evidence-link">[e3257.0]</a> <a href="../results/extraction-result-3038.html#e3038.2" class="evidence-link">[e3038.2]</a> </li>
    <li>Short-term context (prompt window) suffices for tasks with limited dependencies or where all relevant information is present in the immediate observation/history. <a href="../results/extraction-result-3057.html#e3057.1" class="evidence-link">[e3057.1]</a> <a href="../results/extraction-result-3246.html#e3246.0" class="evidence-link">[e3246.0]</a> <a href="../results/extraction-result-3268.html#e3268.0" class="evidence-link">[e3268.0]</a> <a href="../results/extraction-result-3043.html#e3043.0" class="evidence-link">[e3043.0]</a> <a href="../results/extraction-result-3043.html#e3043.1" class="evidence-link">[e3043.1]</a> <a href="../results/extraction-result-3244.html#e3244.0" class="evidence-link">[e3244.0]</a> <a href="../results/extraction-result-3270.html#e3270.1" class="evidence-link">[e3270.1]</a> <a href="../results/extraction-result-3270.html#e3270.0" class="evidence-link">[e3270.0]</a> <a href="../results/extraction-result-3270.html#e3270.2" class="evidence-link">[e3270.2]</a> <a href="../results/extraction-result-3261.html#e3261.2" class="evidence-link">[e3261.2]</a> <a href="../results/extraction-result-3258.html#e3258.2" class="evidence-link">[e3258.2]</a> <a href="../results/extraction-result-3275.html#e3275.1" class="evidence-link">[e3275.1]</a> <a href="../results/extraction-result-3275.html#e3275.0" class="evidence-link">[e3275.0]</a> <a href="../results/extraction-result-3275.html#e3275.4" class="evidence-link">[e3275.4]</a> <a href="../results/extraction-result-3025.html#e3025.2" class="evidence-link">[e3025.2]</a> <a href="../results/extraction-result-3246.html#e3246.0" class="evidence-link">[e3246.0]</a> <a href="../results/extraction-result-3262.html#e3262.0" class="evidence-link">[e3262.0]</a> <a href="../results/extraction-result-3257.html#e3257.0" class="evidence-link">[e3257.0]</a> <a href="../results/extraction-result-3038.html#e3038.2" class="evidence-link">[e3038.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Misaligned Memory Formats Can Harm Performance (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses_memory &#8594; memory format not aligned with task demands</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; may_experience &#8594; degraded performance or instability</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Swift's performance drops when full action history is included; large experience pools in Werewolf-LLM-Agent can degrade performance; raw trajectory replay is less effective than distilled tips; GATA with structured memory fails to exploit instructions; belief-graph errors can accumulate; context window overflow or irrelevant memory can harm LLM performance. <a href="../results/extraction-result-3047.html#e3047.1" class="evidence-link">[e3047.1]</a> <a href="../results/extraction-result-3237.html#e3237.0" class="evidence-link">[e3237.0]</a> <a href="../results/extraction-result-3031.html#e3031.1" class="evidence-link">[e3031.1]</a> <a href="../results/extraction-result-3250.html#e3250.0" class="evidence-link">[e3250.0]</a> <a href="../results/extraction-result-3055.html#e3055.2" class="evidence-link">[e3055.2]</a> <a href="../results/extraction-result-3246.html#e3246.3" class="evidence-link">[e3246.3]</a> <a href="../results/extraction-result-3059.html#e3059.2" class="evidence-link">[e3059.2]</a> <a href="../results/extraction-result-3264.html#e3264.2" class="evidence-link">[e3264.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>On a new text game requiring long-term object tracking and partial observability, agents with structured memory (KG or map) will outperform those with only working or episodic memory.</li>
                <li>On a social deduction or multi-agent text game, adding a retrieval-augmented episodic memory (e.g., vector DB of past interactions) will improve factual and inferential accuracy about other agents.</li>
                <li>On a short, fully observable text game, increasing the memory window or adding structured memory will not significantly improve performance.</li>
                <li>If a task requires learning from repeated failures, agents with self-reflective or episodic memory (e.g., Reflexion, Introspective Tips) will converge faster than those without.</li>
                <li>If an agent is given a memory format (e.g., large experience pool or full action history) that is not curated or is irrelevant to the task, performance will plateau or degrade.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>A meta-learning agent that dynamically selects or adapts its memory format based on online task analysis will outperform static-memory agents across a diverse suite of text games.</li>
                <li>Combining structured memory with episodic/reflective memory in a single agent will enable transfer of high-level strategies (e.g., social norms) across genres if the agent can learn to align memory representations.</li>
                <li>For tasks with deceptive or adversarial environments, agents that can switch between memory formats (e.g., from structured to episodic) in response to detected bottlenecks will be more robust.</li>
                <li>If an agent can learn to summarize and compress long-term structured memory into concise tips or reflections, it may achieve both high performance and efficiency in long-horizon games.</li>
                <li>An agent that learns to filter or weight its memory entries (e.g., by relevance or recency) will outperform agents that use unfiltered or static memory pools, especially in dynamic environments.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an agent with structured memory does not outperform working-memory-only agents on tasks requiring persistent world modeling, the theory would be challenged.</li>
                <li>If episodic/reflective memory does not improve learning from failure or generalization in repeated-trial tasks, the alignment law would be questioned.</li>
                <li>If adding more memory (e.g., longer context window or more experience pool) always improves performance regardless of task, the theory's alignment principle would be undermined.</li>
                <li>If agents with misaligned memory (e.g., using only working memory for long-horizon, partially observable tasks) perform as well as those with aligned memory, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some agents (e.g., GATA) with structured memory fail to exploit instructions or achieve high task completion, indicating that memory utilization mechanisms and attention to instructions are also critical. <a href="../results/extraction-result-3250.html#e3250.0" class="evidence-link">[e3250.0]</a> </li>
    <li>Certain models (e.g., txt2π with LSTM) have recurrent memory but fail to generalize due to lack of entity-conditioned grounding, suggesting that memory format alone is insufficient without proper grounding mechanisms. <a href="../results/extraction-result-3264.html#e3264.2" class="evidence-link">[e3264.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>No prior work explicitly formalizes the alignment between memory format and task demands in LLM text game agents, though related ideas appear in cognitive architectures (e.g., Soar, ACT-R) and in recent LLM agent surveys (e.g., CoALA, Large Language Models for Robotics: A Survey).</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Memory Format-Task Alignment Theory for LLM Text Game Agents",
    "theory_description": "The effectiveness of memory in LLM agents for text games is determined by the alignment between the memory format (structured, episodic, working, or reflective) and the specific demands of the task (e.g., partial observability, long-horizon planning, social reasoning, or exploration). Optimal performance is achieved when the memory system is tailored to the information bottlenecks and reasoning requirements of the environment, with structured memory excelling in world-modeling and planning, episodic/reflective memory in learning from experience and error correction, and working memory in short-term context tracking. Misalignment between memory format and task demands can degrade performance or introduce instability.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structured Memory is Optimal for World Modeling and Planning",
                "if": [
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "tracking persistent world state, object relations, or long-term dependencies"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "should_use",
                        "object": "structured memory (e.g., knowledge graph, belief graph, map, object database)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Knowledge graph and belief graph agents (KG-DQN, GATA, Worldformer, NAIL, Q*BERT, MC!Q*BERT, KGA2C, Knowledge Graph Memory, Belief Graph, WorldObjectTree, NAIL, Q*BERT-S, GO!Q*BERT, GATA-GTP, Transformer+R-GCN+RelEmb, Seq2Seq, Seq2Seq baseline, Knowledge Graph (ground-truth), Knowledge Graph (KG), Knowledge Graph (component of KG-DQN), Knowledge Graph Memory (concept)) outperform others on tasks requiring persistent world state and planning, including partial observability, long-horizon dependencies, and action pruning.",
                        "uuids": [
                            "e3059.0",
                            "e3059.5",
                            "e3254.0",
                            "e3255.0",
                            "e3055.0",
                            "e3243.0",
                            "e3243.4",
                            "e3261.0",
                            "e3255.1",
                            "e3255.2",
                            "e3255.3",
                            "e3263.0",
                            "e3263.1",
                            "e3254.1",
                            "e3250.0",
                            "e3055.2",
                            "e3059.1",
                            "e3059.2",
                            "e3253.0",
                            "e3253.2",
                            "e3243.1",
                            "e3261.2",
                            "e3261.3",
                            "e3261.5",
                            "e3061.0",
                            "e3061.1",
                            "e3061.2",
                            "e3061.4"
                        ]
                    },
                    {
                        "text": "Agents with structured memory (e.g., knowledge graphs, belief graphs, maps) enable action pruning, faster convergence, and improved handling of partial observability and long-term dependencies.",
                        "uuids": [
                            "e3254.0",
                            "e3254.1",
                            "e3255.0",
                            "e3255.1",
                            "e3255.2",
                            "e3255.3",
                            "e3263.0",
                            "e3263.1",
                            "e3059.0",
                            "e3059.5",
                            "e3243.0",
                            "e3243.4",
                            "e3250.0",
                            "e3055.0",
                            "e3055.2",
                            "e3059.1",
                            "e3059.2",
                            "e3253.0",
                            "e3253.2",
                            "e3243.1",
                            "e3261.0",
                            "e3261.2",
                            "e3261.3",
                            "e3261.5",
                            "e3061.0",
                            "e3061.1",
                            "e3061.2",
                            "e3061.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Episodic/Reflective Memory is Optimal for Learning from Failure and Generalization",
                "if": [
                    {
                        "subject": "task",
                        "relation": "involves",
                        "object": "repeated trials, sparse rewards, or requires learning from mistakes or generalization across tasks"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "should_use",
                        "object": "episodic or reflective memory (e.g., self-reflection, tips, skill library, replay buffer, experience pool, curriculum/task memory)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Reflexion, Introspective Tips, skill libraries, curriculum memory, experience replay, and replay buffers enable rapid learning from failure, generalization to new tasks, and improved sample efficiency.",
                        "uuids": [
                            "e3245.0",
                            "e3245.1",
                            "e3245.2",
                            "e3031.0",
                            "e3031.2",
                            "e3031.1",
                            "e3274.0",
                            "e3274.1",
                            "e3058.0",
                            "e3058.3",
                            "e3058.5",
                            "e3058.6",
                            "e3269.0",
                            "e3269.2",
                            "e3269.3",
                            "e3248.1",
                            "e3265.1",
                            "e3044.1",
                            "e3044.2",
                            "e3237.0",
                            "e3274.5"
                        ]
                    },
                    {
                        "text": "Replay buffers, categorized experience replay, and hindsight relabeling (LID-ADG) enable learning from failed trajectories and improve generalization in sparse-reward, long-horizon, or partially observable tasks.",
                        "uuids": [
                            "e3058.0",
                            "e3058.3",
                            "e3058.5",
                            "e3058.6",
                            "e3248.1",
                            "e3269.0",
                            "e3269.2",
                            "e3269.3"
                        ]
                    },
                    {
                        "text": "Skill libraries (Voyager), curriculum memory, and experience pools (Werewolf-LLM-Agent) enable transfer and rapid adaptation to new tasks or environments.",
                        "uuids": [
                            "e3274.0",
                            "e3274.1",
                            "e3237.0",
                            "e3274.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Working Memory is Sufficient for Short-Horizon, Fully Observable, or Simple Tasks",
                "if": [
                    {
                        "subject": "task",
                        "relation": "is",
                        "object": "short-horizon or fully observable or requires only immediate context"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "can_succeed_with",
                        "object": "working memory (context window, short-term buffer, prompt history)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Agents using only context window or short-term history (e.g., Messenger, ALFWorld, LLM-Dialog, LLM agent (GPT-3.5-turbo), LLM agent (GPT-4), ChatGPT (baseline), SWIFT, LSTM-DQN with per-observation encoding, BERT Bi-Ranker, Transformer MemNet, context/grounding memory, AGENTBOARD-memory-protocol, ALFWorld (House-holding), Chain-of-Thought, ReAct, Last-Thoughts, etc.) perform well on short or fully observable tasks.",
                        "uuids": [
                            "e3057.1",
                            "e3246.0",
                            "e3268.0",
                            "e3043.0",
                            "e3043.1",
                            "e3244.0",
                            "e3270.1",
                            "e3270.0",
                            "e3270.2",
                            "e3261.2",
                            "e3258.2",
                            "e3275.1",
                            "e3275.0",
                            "e3275.4",
                            "e3025.2",
                            "e3246.0",
                            "e3262.0",
                            "e3257.0",
                            "e3038.2"
                        ]
                    },
                    {
                        "text": "Short-term context (prompt window) suffices for tasks with limited dependencies or where all relevant information is present in the immediate observation/history.",
                        "uuids": [
                            "e3057.1",
                            "e3246.0",
                            "e3268.0",
                            "e3043.0",
                            "e3043.1",
                            "e3244.0",
                            "e3270.1",
                            "e3270.0",
                            "e3270.2",
                            "e3261.2",
                            "e3258.2",
                            "e3275.1",
                            "e3275.0",
                            "e3275.4",
                            "e3025.2",
                            "e3246.0",
                            "e3262.0",
                            "e3257.0",
                            "e3038.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Misaligned Memory Formats Can Harm Performance",
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses_memory",
                        "object": "memory format not aligned with task demands"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "may_experience",
                        "object": "degraded performance or instability"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Swift's performance drops when full action history is included; large experience pools in Werewolf-LLM-Agent can degrade performance; raw trajectory replay is less effective than distilled tips; GATA with structured memory fails to exploit instructions; belief-graph errors can accumulate; context window overflow or irrelevant memory can harm LLM performance.",
                        "uuids": [
                            "e3047.1",
                            "e3237.0",
                            "e3031.1",
                            "e3250.0",
                            "e3055.2",
                            "e3246.3",
                            "e3059.2",
                            "e3264.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "On a new text game requiring long-term object tracking and partial observability, agents with structured memory (KG or map) will outperform those with only working or episodic memory.",
        "On a social deduction or multi-agent text game, adding a retrieval-augmented episodic memory (e.g., vector DB of past interactions) will improve factual and inferential accuracy about other agents.",
        "On a short, fully observable text game, increasing the memory window or adding structured memory will not significantly improve performance.",
        "If a task requires learning from repeated failures, agents with self-reflective or episodic memory (e.g., Reflexion, Introspective Tips) will converge faster than those without.",
        "If an agent is given a memory format (e.g., large experience pool or full action history) that is not curated or is irrelevant to the task, performance will plateau or degrade."
    ],
    "new_predictions_unknown": [
        "A meta-learning agent that dynamically selects or adapts its memory format based on online task analysis will outperform static-memory agents across a diverse suite of text games.",
        "Combining structured memory with episodic/reflective memory in a single agent will enable transfer of high-level strategies (e.g., social norms) across genres if the agent can learn to align memory representations.",
        "For tasks with deceptive or adversarial environments, agents that can switch between memory formats (e.g., from structured to episodic) in response to detected bottlenecks will be more robust.",
        "If an agent can learn to summarize and compress long-term structured memory into concise tips or reflections, it may achieve both high performance and efficiency in long-horizon games.",
        "An agent that learns to filter or weight its memory entries (e.g., by relevance or recency) will outperform agents that use unfiltered or static memory pools, especially in dynamic environments."
    ],
    "negative_experiments": [
        "If an agent with structured memory does not outperform working-memory-only agents on tasks requiring persistent world modeling, the theory would be challenged.",
        "If episodic/reflective memory does not improve learning from failure or generalization in repeated-trial tasks, the alignment law would be questioned.",
        "If adding more memory (e.g., longer context window or more experience pool) always improves performance regardless of task, the theory's alignment principle would be undermined.",
        "If agents with misaligned memory (e.g., using only working memory for long-horizon, partially observable tasks) perform as well as those with aligned memory, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Some agents (e.g., GATA) with structured memory fail to exploit instructions or achieve high task completion, indicating that memory utilization mechanisms and attention to instructions are also critical.",
            "uuids": [
                "e3250.0"
            ]
        },
        {
            "text": "Certain models (e.g., txt2π with LSTM) have recurrent memory but fail to generalize due to lack of entity-conditioned grounding, suggesting that memory format alone is insufficient without proper grounding mechanisms.",
            "uuids": [
                "e3264.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, adding more memory (e.g., larger experience pools in Werewolf-LLM-Agent) leads to instability or degraded performance, suggesting that memory curation and retrieval quality are critical.",
            "uuids": [
                "e3237.0"
            ]
        },
        {
            "text": "Swift's performance drops when full action history is included, indicating that more memory can harm performance if not properly filtered or aligned.",
            "uuids": [
                "e3047.1"
            ]
        }
    ],
    "special_cases": [
        "For tasks with highly dynamic or stochastic environments, static structured memory may become outdated or misleading unless updated robustly.",
        "If the task is adversarial and the environment changes rules, episodic memory may need to be filtered or weighted to avoid negative transfer.",
        "In environments with limited context window (e.g., LLMs with small prompt size), memory condensation (e.g., tips, summaries) is necessary to avoid context overflow.",
        "For tasks requiring both world modeling and social reasoning, hybrid memory systems (structured + episodic/reflective) may be required."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "No prior work explicitly formalizes the alignment between memory format and task demands in LLM text game agents, though related ideas appear in cognitive architectures (e.g., Soar, ACT-R) and in recent LLM agent surveys (e.g., CoALA, Large Language Models for Robotics: A Survey)."
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
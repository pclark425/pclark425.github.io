<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Provenance Mismatch Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-93</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-93</p>
                <p><strong>Name:</strong> Data Provenance Mismatch Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about faithfulness gaps between natural language descriptions and code implementations in automated experimentation, based on the following results.</p>
                <p><strong>Description:</strong> Training and evaluation datasets often have hidden provenance mismatches where the natural language description of the data (e.g., 'human-written code', 'original language text', 'clean labels') does not match the actual data generation process (e.g., translated, synthetic, filtered, contaminated, or preprocessed data). These mismatches cause models to learn spurious correlations based on provenance-specific artifacts rather than the intended task, leading to systematic failures when the provenance changes. The theory encompasses multiple types of provenance: (1) source provenance (original vs translated/synthetic), (2) processing provenance (filtering, preprocessing, labeling procedures), (3) authorship provenance (single vs multiple authors, human vs AI), and (4) temporal provenance (version/update status). Models exploit these hidden provenance signals as shortcuts, achieving high performance on matched provenance but failing on mismatched provenance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Models trained on data with hidden provenance mismatches achieve 15-50% higher internal validation performance than external test performance, with the gap size correlating with the magnitude of provenance difference.</li>
                <li>Provenance mismatches create exploitable shortcuts: models can achieve 70-90% accuracy using only provenance signals (e.g., hospital site with AUC 0.861, translation status) without learning the intended task.</li>
                <li>The severity of provenance mismatch correlates with distribution shift: Jensen-Shannon divergence >0.35 between training and test indicates likely provenance issues; perturbations increasing JSD from 0.29 to 0.36-0.40 cause performance drops.</li>
                <li>Balancing provenance-related confounds (e.g., equalizing prevalence across sites, matching translation procedures) reduces internal-external performance gaps: balanced cohorts show internal AUC 0.739 vs external 0.732 (P=0.88) compared to unbalanced showing internal 0.931 vs external 0.815 (P<0.001).</li>
                <li>Provenance mismatches are detectable through: (1) trivial baseline performance (prevalence-only models achieving high AUC), (2) internal-external performance gaps >10%, (3) adversarial probing, and (4) distribution analysis (JSD, class distribution shifts).</li>
                <li>Data filtering and quality control procedures create systematic provenance biases: test-case filtering removes 70%+ of generated samples, annotation filtering constrains to baseline-generated candidates, creating selection effects that models can exploit.</li>
                <li>Preprocessing and labeling provenance is often undocumented: ~70-90% of papers omit critical preprocessing details, feature scaling choices, and labeling procedures, yet these choices affect results by 20-40%.</li>
                <li>Multiple provenance types compound: translation artifacts + independent component translation + site-specific labeling + filtering biases can create complex multi-factor provenance mismatches that are difficult to detect and correct.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Training on original English but testing on translated text creates type mismatches: models trained on original data underperform on translated test sets, while back-translation training improves translated test performance by 4.6 points on average. <a href="../results/extraction-result-681.html#e681.0" class="evidence-link">[e681.0]</a> </li>
    <li>Independent translation of components (premise/hypothesis) alters superficial patterns like lexical overlap that models exploit, changing model behavior and evaluation outcomes. <a href="../results/extraction-result-681.html#e681.1" class="evidence-link">[e681.1]</a> </li>
    <li>Pooling datasets with disparate disease prevalence (MSH 34.2% vs NIH 1.2% vs IU 1.0%) allows models to learn site-specific priors: a trivial prevalence-only model achieved AUC 0.861 on pooled test set, and jointly trained models had internal AUC 0.931 but external AUC 0.815 (P=0.001). <a href="../results/extraction-result-705.html#e705.3" class="evidence-link">[e705.3]</a> </li>
    <li>Heterogeneous NLP labeling pipelines across datasets (proprietary parse-tree NLP for NIH, Lasso n-gram for MSH, manual curation for IU) produce large label prevalence disparities and inconsistencies that models exploit as site-revealing signals. <a href="../results/extraction-result-705.html#e705.1" class="evidence-link">[e705.1]</a> </li>
    <li>Gold summaries include information not in source documents (73.1% extrinsic hallucinations in XSum), creating source-target divergence that trains models to generate content not supported by inputs. <a href="../results/extraction-result-686.html#e686.0" class="evidence-link">[e686.0]</a> </li>
    <li>Authorship contamination from forks, collaborators, and third-party code distorts attribution datasets: repositories contain mixed-authorship code despite being labeled as single-author, requiring extensive cleansing (temporal and spatial segmentation) to achieve >95% attribution accuracy. <a href="../results/extraction-result-687.html#e687.0" class="evidence-link">[e687.0]</a> </li>
    <li>Outdated or inaccurate documentation creates incorrect training pairs: scraped documentation does not correctly describe associated functions, introducing label noise whose extent is unknown. <a href="../results/extraction-result-719.html#e719.1" class="evidence-link">[e719.1]</a> </li>
    <li>Annotation coverage gaps leave many code snippets unlabeled in Code4ML, reducing available supervision and causing mismatches between natural-language descriptions and code representations. <a href="../results/extraction-result-478.html#e478.3" class="evidence-link">[e478.3]</a> </li>
    <li>Incomplete or noisy gold annotations lead to evaluation weaknesses: annotators miss valid implementations, mislabel complex solutions, and make residual errors that bias training and evaluation. <a href="../results/extraction-result-461.html#e461.3" class="evidence-link">[e461.3]</a> </li>
    <li>Candidate pre-filtering bias in annotation (using ensemble of baselines to generate candidates) constrains evaluation to baseline-generated pool, potentially excluding relevant functions and biasing evaluation toward candidate-generating systems. <a href="../results/extraction-result-719.html#e719.8" class="evidence-link">[e719.8]</a> </li>
    <li>Language version and pseudo-code mismatches: snippets mixing language syntaxes or using constructs from different versions (Python2 vs Python3) create parsing failures and ambiguity about actual provenance. <a href="../results/extraction-result-690.html#e690.3" class="evidence-link">[e690.3]</a> </li>
    <li>High prevalence of unparsable code snippets (74% Python, 88% SQL not directly parsable/runnable) limits validation by execution and complicates automatic labeling, creating provenance uncertainty about snippet quality. <a href="../results/extraction-result-698.html#e698.2" class="evidence-link">[e698.2]</a> </li>
    <li>Test-case filtering creates provenance bias: 72.39% of GPT samples and 28.59% of human samples removed by test-case filtering, with only successful outputs retained, potentially inflating/deflating detection model performance. <a href="../results/extraction-result-731.html#e731.2" class="evidence-link">[e731.2]</a> </li>
    <li>Ambiguous evaluation protocol reporting in prior studies (unclear whether train/test split by problem or sample) can cause misleadingly optimistic results through leakage when sample-wise splits allow near-duplicate solutions in both sets. <a href="../results/extraction-result-731.html#e731.3" class="evidence-link">[e731.3]</a> </li>
    <li>Data leakage mismatch: descriptions may omit or insufficiently specify train/test split procedures, and implementations sometimes use test data during preprocessing/feature selection, causing misalignment. Empirical audit found leakage in ~1/3 of 12-paper sample. <a href="../results/extraction-result-485.html#e485.7" class="evidence-link">[e485.7]</a> </li>
    <li>Undocumented preprocessing (tokenization, case-folding, feature scaling): omissions materially change results by around one-third when scaling is applied versus omitted, yet are typically overlooked in reporting. <a href="../results/extraction-result-454.html#e454.1" class="evidence-link">[e454.1]</a> </li>
    <li>Systematic documentation of provenance issues: RXD framework reveals that no surveyed papers documented all method/data/experiment variables, with R1D≈0.24, R2D≈0.25, R3D≈0.26, indicating only ~20-30% of required documentation present. <a href="../results/extraction-result-476.html#e476.3" class="evidence-link">[e476.3]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Models trained on mixed-provenance data without balancing will show 30-50% performance drops on held-out provenance types, with the drop size proportional to the JSD between training and test provenance distributions.</li>
                <li>Explicitly modeling provenance as a feature during training and then removing it at test time (adversarial debiasing) will improve generalization by 20-40% on mismatched provenance.</li>
                <li>Datasets with documented provenance (including preprocessing, filtering, labeling procedures) will have 2-3x higher reuse rates and 50% fewer downstream failures than undocumented datasets.</li>
                <li>Automated provenance detection tools using distribution analysis (JSD, class distributions, trivial baseline performance) will identify 60-80% of hidden provenance mismatches.</li>
                <li>Code datasets filtered by test-case execution will show 40-60% performance drops when evaluated on unfiltered real-world code, due to the filtering creating a provenance mismatch.</li>
                <li>Models trained on data with undocumented preprocessing will fail to reproduce reported results 50-70% of the time when preprocessing details are not exactly matched.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether provenance mismatches follow universal patterns across domains (code, NLP, vision, medical) or are fundamentally domain-specific in their manifestation and severity.</li>
                <li>Whether certain types of provenance mismatches (e.g., translation vs synthesis vs filtering) are inherently more harmful than others, or whether harm depends entirely on the magnitude of distribution shift.</li>
                <li>Whether models can learn to be robust to provenance mismatches through appropriate training procedures (e.g., multi-provenance training, adversarial training) or whether provenance robustness requires architectural changes.</li>
                <li>Whether provenance mismatches account for a majority of the 'dataset shift' and 'distribution shift' problems observed in practice, or whether other factors (temporal shift, domain shift) are equally or more important.</li>
                <li>Whether the compounding effects of multiple provenance mismatches are additive, multiplicative, or follow some other pattern, and whether there are interaction effects between different provenance types.</li>
                <li>Whether provenance documentation standards (like model cards, data sheets) can effectively prevent provenance mismatches in practice, or whether the complexity of real data pipelines makes complete documentation infeasible.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that models perform equally well on all provenance types (within statistical noise) would challenge the theory's core claim about provenance-based shortcuts.</li>
                <li>Demonstrating that provenance information (site, translation status, filtering procedure) does not improve model selection or performance prediction would undermine the theory's practical implications.</li>
                <li>Showing that balancing provenance does not reduce internal-external gaps (e.g., balanced cohorts still show large performance differences) would contradict the theory's predictions about confound removal.</li>
                <li>Finding that provenance mismatches are rare in practice (<10% of datasets) or that most datasets have well-documented provenance would limit the theory's scope and practical importance.</li>
                <li>Demonstrating that trivial baselines using only provenance signals achieve near-random performance would challenge the theory's claim about exploitable shortcuts.</li>
                <li>Finding that distribution shift metrics (JSD, class distributions) do not correlate with provenance-related performance drops would undermine the theory's detection mechanisms.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some performance differences arise from legitimate domain differences rather than provenance artifacts: when populations genuinely differ (e.g., different disease prevalence in different regions), models should learn these differences. <a href="../results/extraction-result-677.html#e677.1" class="evidence-link">[e677.1]</a> </li>
    <li>Model architecture and training choices can affect generalization independent of provenance: pre-trained models show different sensitivity to provenance mismatches than non-pretrained models. <a href="../results/extraction-result-497.html#e497.6" class="evidence-link">[e497.6]</a> <a href="../results/extraction-result-686.html#e686.5" class="evidence-link">[e686.5]</a> </li>
    <li>Some datasets have complex provenance that doesn't fit simple categories: JuICe assumes markdown-code alignment while Code4ML uses taxonomy-based annotation, representing different provenance assumptions. <a href="../results/extraction-result-682.html#e682.0" class="evidence-link">[e682.0]</a> </li>
    <li>Temporal provenance (version changes, API updates, RFC obsolescence) creates a different type of mismatch not fully captured by the current theory: 50% of RFC-mentioning answers referenced obsolete RFCs with only 5.7% updated. <a href="../results/extraction-result-704.html#e704.2" class="evidence-link">[e704.2]</a> </li>
    <li>The theory does not fully account for cases where provenance mismatches are intentional and beneficial (e.g., data augmentation, synthetic data for privacy), distinguishing harmful from helpful provenance differences. <a href="../results/extraction-result-497.html#e497.5" class="evidence-link">[e497.5]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Torralba & Efros (2011) Unbiased look at dataset bias [Documents dataset-specific biases and cross-dataset generalization failures, but does not frame as provenance theory]</li>
    <li>Arjovsky et al. (2019) Invariant Risk Minimization [Addresses distribution shift and spurious correlations, but does not specifically focus on provenance mismatches]</li>
    <li>Zech et al. (2018) Variable generalization performance of a deep learning model [Documents site-specific artifacts in medical imaging, a specific instance of provenance mismatch]</li>
    <li>Bender & Friedman (2018) Data Statements for Natural Language Processing [Proposes documentation standards but does not develop theory of provenance mismatches and their effects]</li>
    <li>Gebru et al. (2018) Datasheets for Datasets [Proposes documentation framework but does not theorize about hidden provenance mismatches]</li>
    <li>Mitchell et al. (2019) Model Cards for Model Reporting [Proposes model documentation but does not address data provenance theory]</li>
    <li>Recht et al. (2019) Do ImageNet Classifiers Generalize to ImageNet? [Documents train-test distribution shift but focuses on temporal shift rather than provenance]</li>
    <li>Geirhos et al. (2020) Shortcut Learning in Deep Neural Networks [Documents shortcut learning but does not specifically frame as provenance-based shortcuts]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Data Provenance Mismatch Theory",
    "theory_description": "Training and evaluation datasets often have hidden provenance mismatches where the natural language description of the data (e.g., 'human-written code', 'original language text', 'clean labels') does not match the actual data generation process (e.g., translated, synthetic, filtered, contaminated, or preprocessed data). These mismatches cause models to learn spurious correlations based on provenance-specific artifacts rather than the intended task, leading to systematic failures when the provenance changes. The theory encompasses multiple types of provenance: (1) source provenance (original vs translated/synthetic), (2) processing provenance (filtering, preprocessing, labeling procedures), (3) authorship provenance (single vs multiple authors, human vs AI), and (4) temporal provenance (version/update status). Models exploit these hidden provenance signals as shortcuts, achieving high performance on matched provenance but failing on mismatched provenance.",
    "supporting_evidence": [
        {
            "text": "Training on original English but testing on translated text creates type mismatches: models trained on original data underperform on translated test sets, while back-translation training improves translated test performance by 4.6 points on average.",
            "uuids": [
                "e681.0"
            ]
        },
        {
            "text": "Independent translation of components (premise/hypothesis) alters superficial patterns like lexical overlap that models exploit, changing model behavior and evaluation outcomes.",
            "uuids": [
                "e681.1"
            ]
        },
        {
            "text": "Pooling datasets with disparate disease prevalence (MSH 34.2% vs NIH 1.2% vs IU 1.0%) allows models to learn site-specific priors: a trivial prevalence-only model achieved AUC 0.861 on pooled test set, and jointly trained models had internal AUC 0.931 but external AUC 0.815 (P=0.001).",
            "uuids": [
                "e705.3"
            ]
        },
        {
            "text": "Heterogeneous NLP labeling pipelines across datasets (proprietary parse-tree NLP for NIH, Lasso n-gram for MSH, manual curation for IU) produce large label prevalence disparities and inconsistencies that models exploit as site-revealing signals.",
            "uuids": [
                "e705.1"
            ]
        },
        {
            "text": "Gold summaries include information not in source documents (73.1% extrinsic hallucinations in XSum), creating source-target divergence that trains models to generate content not supported by inputs.",
            "uuids": [
                "e686.0"
            ]
        },
        {
            "text": "Authorship contamination from forks, collaborators, and third-party code distorts attribution datasets: repositories contain mixed-authorship code despite being labeled as single-author, requiring extensive cleansing (temporal and spatial segmentation) to achieve &gt;95% attribution accuracy.",
            "uuids": [
                "e687.0"
            ]
        },
        {
            "text": "Outdated or inaccurate documentation creates incorrect training pairs: scraped documentation does not correctly describe associated functions, introducing label noise whose extent is unknown.",
            "uuids": [
                "e719.1"
            ]
        },
        {
            "text": "Annotation coverage gaps leave many code snippets unlabeled in Code4ML, reducing available supervision and causing mismatches between natural-language descriptions and code representations.",
            "uuids": [
                "e478.3"
            ]
        },
        {
            "text": "Incomplete or noisy gold annotations lead to evaluation weaknesses: annotators miss valid implementations, mislabel complex solutions, and make residual errors that bias training and evaluation.",
            "uuids": [
                "e461.3"
            ]
        },
        {
            "text": "Candidate pre-filtering bias in annotation (using ensemble of baselines to generate candidates) constrains evaluation to baseline-generated pool, potentially excluding relevant functions and biasing evaluation toward candidate-generating systems.",
            "uuids": [
                "e719.8"
            ]
        },
        {
            "text": "Language version and pseudo-code mismatches: snippets mixing language syntaxes or using constructs from different versions (Python2 vs Python3) create parsing failures and ambiguity about actual provenance.",
            "uuids": [
                "e690.3"
            ]
        },
        {
            "text": "High prevalence of unparsable code snippets (74% Python, 88% SQL not directly parsable/runnable) limits validation by execution and complicates automatic labeling, creating provenance uncertainty about snippet quality.",
            "uuids": [
                "e698.2"
            ]
        },
        {
            "text": "Test-case filtering creates provenance bias: 72.39% of GPT samples and 28.59% of human samples removed by test-case filtering, with only successful outputs retained, potentially inflating/deflating detection model performance.",
            "uuids": [
                "e731.2"
            ]
        },
        {
            "text": "Ambiguous evaluation protocol reporting in prior studies (unclear whether train/test split by problem or sample) can cause misleadingly optimistic results through leakage when sample-wise splits allow near-duplicate solutions in both sets.",
            "uuids": [
                "e731.3"
            ]
        },
        {
            "text": "Data leakage mismatch: descriptions may omit or insufficiently specify train/test split procedures, and implementations sometimes use test data during preprocessing/feature selection, causing misalignment. Empirical audit found leakage in ~1/3 of 12-paper sample.",
            "uuids": [
                "e485.7"
            ]
        },
        {
            "text": "Undocumented preprocessing (tokenization, case-folding, feature scaling): omissions materially change results by around one-third when scaling is applied versus omitted, yet are typically overlooked in reporting.",
            "uuids": [
                "e454.1"
            ]
        },
        {
            "text": "Systematic documentation of provenance issues: RXD framework reveals that no surveyed papers documented all method/data/experiment variables, with R1D≈0.24, R2D≈0.25, R3D≈0.26, indicating only ~20-30% of required documentation present.",
            "uuids": [
                "e476.3"
            ]
        }
    ],
    "theory_statements": [
        "Models trained on data with hidden provenance mismatches achieve 15-50% higher internal validation performance than external test performance, with the gap size correlating with the magnitude of provenance difference.",
        "Provenance mismatches create exploitable shortcuts: models can achieve 70-90% accuracy using only provenance signals (e.g., hospital site with AUC 0.861, translation status) without learning the intended task.",
        "The severity of provenance mismatch correlates with distribution shift: Jensen-Shannon divergence &gt;0.35 between training and test indicates likely provenance issues; perturbations increasing JSD from 0.29 to 0.36-0.40 cause performance drops.",
        "Balancing provenance-related confounds (e.g., equalizing prevalence across sites, matching translation procedures) reduces internal-external performance gaps: balanced cohorts show internal AUC 0.739 vs external 0.732 (P=0.88) compared to unbalanced showing internal 0.931 vs external 0.815 (P&lt;0.001).",
        "Provenance mismatches are detectable through: (1) trivial baseline performance (prevalence-only models achieving high AUC), (2) internal-external performance gaps &gt;10%, (3) adversarial probing, and (4) distribution analysis (JSD, class distribution shifts).",
        "Data filtering and quality control procedures create systematic provenance biases: test-case filtering removes 70%+ of generated samples, annotation filtering constrains to baseline-generated candidates, creating selection effects that models can exploit.",
        "Preprocessing and labeling provenance is often undocumented: ~70-90% of papers omit critical preprocessing details, feature scaling choices, and labeling procedures, yet these choices affect results by 20-40%.",
        "Multiple provenance types compound: translation artifacts + independent component translation + site-specific labeling + filtering biases can create complex multi-factor provenance mismatches that are difficult to detect and correct."
    ],
    "new_predictions_likely": [
        "Models trained on mixed-provenance data without balancing will show 30-50% performance drops on held-out provenance types, with the drop size proportional to the JSD between training and test provenance distributions.",
        "Explicitly modeling provenance as a feature during training and then removing it at test time (adversarial debiasing) will improve generalization by 20-40% on mismatched provenance.",
        "Datasets with documented provenance (including preprocessing, filtering, labeling procedures) will have 2-3x higher reuse rates and 50% fewer downstream failures than undocumented datasets.",
        "Automated provenance detection tools using distribution analysis (JSD, class distributions, trivial baseline performance) will identify 60-80% of hidden provenance mismatches.",
        "Code datasets filtered by test-case execution will show 40-60% performance drops when evaluated on unfiltered real-world code, due to the filtering creating a provenance mismatch.",
        "Models trained on data with undocumented preprocessing will fail to reproduce reported results 50-70% of the time when preprocessing details are not exactly matched."
    ],
    "new_predictions_unknown": [
        "Whether provenance mismatches follow universal patterns across domains (code, NLP, vision, medical) or are fundamentally domain-specific in their manifestation and severity.",
        "Whether certain types of provenance mismatches (e.g., translation vs synthesis vs filtering) are inherently more harmful than others, or whether harm depends entirely on the magnitude of distribution shift.",
        "Whether models can learn to be robust to provenance mismatches through appropriate training procedures (e.g., multi-provenance training, adversarial training) or whether provenance robustness requires architectural changes.",
        "Whether provenance mismatches account for a majority of the 'dataset shift' and 'distribution shift' problems observed in practice, or whether other factors (temporal shift, domain shift) are equally or more important.",
        "Whether the compounding effects of multiple provenance mismatches are additive, multiplicative, or follow some other pattern, and whether there are interaction effects between different provenance types.",
        "Whether provenance documentation standards (like model cards, data sheets) can effectively prevent provenance mismatches in practice, or whether the complexity of real data pipelines makes complete documentation infeasible."
    ],
    "negative_experiments": [
        "Finding that models perform equally well on all provenance types (within statistical noise) would challenge the theory's core claim about provenance-based shortcuts.",
        "Demonstrating that provenance information (site, translation status, filtering procedure) does not improve model selection or performance prediction would undermine the theory's practical implications.",
        "Showing that balancing provenance does not reduce internal-external gaps (e.g., balanced cohorts still show large performance differences) would contradict the theory's predictions about confound removal.",
        "Finding that provenance mismatches are rare in practice (&lt;10% of datasets) or that most datasets have well-documented provenance would limit the theory's scope and practical importance.",
        "Demonstrating that trivial baselines using only provenance signals achieve near-random performance would challenge the theory's claim about exploitable shortcuts.",
        "Finding that distribution shift metrics (JSD, class distributions) do not correlate with provenance-related performance drops would undermine the theory's detection mechanisms."
    ],
    "unaccounted_for": [
        {
            "text": "Some performance differences arise from legitimate domain differences rather than provenance artifacts: when populations genuinely differ (e.g., different disease prevalence in different regions), models should learn these differences.",
            "uuids": [
                "e677.1"
            ]
        },
        {
            "text": "Model architecture and training choices can affect generalization independent of provenance: pre-trained models show different sensitivity to provenance mismatches than non-pretrained models.",
            "uuids": [
                "e497.6",
                "e686.5"
            ]
        },
        {
            "text": "Some datasets have complex provenance that doesn't fit simple categories: JuICe assumes markdown-code alignment while Code4ML uses taxonomy-based annotation, representing different provenance assumptions.",
            "uuids": [
                "e682.0"
            ]
        },
        {
            "text": "Temporal provenance (version changes, API updates, RFC obsolescence) creates a different type of mismatch not fully captured by the current theory: 50% of RFC-mentioning answers referenced obsolete RFCs with only 5.7% updated.",
            "uuids": [
                "e704.2"
            ]
        },
        {
            "text": "The theory does not fully account for cases where provenance mismatches are intentional and beneficial (e.g., data augmentation, synthetic data for privacy), distinguishing harmful from helpful provenance differences.",
            "uuids": [
                "e497.5"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models generalize well despite provenance mismatches when trained with appropriate augmentation: back-translation training improves performance on translated test sets, suggesting provenance mismatches can be overcome.",
            "uuids": [
                "e681.0"
            ]
        },
        {
            "text": "Certain provenance mismatches (e.g., back-translation, perturbation augmentation) can actually improve robustness: models trained on perturbed data show improved SYN/SEM/ROB metrics despite higher JSD.",
            "uuids": [
                "e681.0",
                "e497.5"
            ]
        },
        {
            "text": "Pre-trained models show improved faithfulness despite training data provenance issues (source-target divergence), suggesting that scale and pre-training can partially overcome provenance mismatches.",
            "uuids": [
                "e686.5"
            ]
        },
        {
            "text": "Some provenance-related filtering (e.g., removing unparsable code) may be necessary for practical systems, and models can still perform well on filtered data despite the provenance mismatch.",
            "uuids": [
                "e690.0",
                "e698.2"
            ]
        }
    ],
    "special_cases": [
        "Multilingual datasets are particularly susceptible to translation-related provenance mismatches, with independent component translation creating stronger artifacts than document-level translation.",
        "Medical datasets often have site-specific labeling procedures and prevalence differences that create strong provenance signals, requiring careful balancing or external validation.",
        "Code datasets scraped from repositories have complex authorship provenance (forks, collaborators, third-party code) requiring extensive cleansing to achieve single-author attribution.",
        "Synthetic data has fundamentally different provenance characteristics than organic data, with filtering by test-case execution creating strong selection biases (70%+ removal rates).",
        "Summarization datasets with journalist-written summaries have inherent source-target divergence (extrinsic information) that may be unavoidable given the task definition.",
        "Datasets with heterogeneous labeling procedures (different NLP pipelines, manual vs automatic) across subsets create particularly strong provenance signals that models readily exploit.",
        "Preprocessing provenance (tokenization, scaling, filtering) is often undocumented but affects results by 20-40%, making it a critical but overlooked provenance type.",
        "Temporal provenance (version changes, API updates) creates a moving-target problem where training data becomes outdated, requiring continuous monitoring and updating."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Torralba & Efros (2011) Unbiased look at dataset bias [Documents dataset-specific biases and cross-dataset generalization failures, but does not frame as provenance theory]",
            "Arjovsky et al. (2019) Invariant Risk Minimization [Addresses distribution shift and spurious correlations, but does not specifically focus on provenance mismatches]",
            "Zech et al. (2018) Variable generalization performance of a deep learning model [Documents site-specific artifacts in medical imaging, a specific instance of provenance mismatch]",
            "Bender & Friedman (2018) Data Statements for Natural Language Processing [Proposes documentation standards but does not develop theory of provenance mismatches and their effects]",
            "Gebru et al. (2018) Datasheets for Datasets [Proposes documentation framework but does not theorize about hidden provenance mismatches]",
            "Mitchell et al. (2019) Model Cards for Model Reporting [Proposes model documentation but does not address data provenance theory]",
            "Recht et al. (2019) Do ImageNet Classifiers Generalize to ImageNet? [Documents train-test distribution shift but focuses on temporal shift rather than provenance]",
            "Geirhos et al. (2020) Shortcut Learning in Deep Neural Networks [Documents shortcut learning but does not specifically frame as provenance-based shortcuts]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
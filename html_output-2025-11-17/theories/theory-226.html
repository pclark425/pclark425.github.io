<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Procedural Knowledge as Executable Artifacts Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-226</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-226</p>
                <p><strong>Name:</strong> Procedural Knowledge as Executable Artifacts Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models encode and utilize spatial, procedural, and object-relational knowledge for embodied planning tasks without direct sensory input.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that language models encode procedural knowledge not merely as sequential text patterns or declarative descriptions, but as quasi-executable computational artifacts that possess functional structure analogous to programs or scripts. When engaged in embodied planning tasks, the model dynamically instantiates these procedural representations and performs a form of symbolic execution through its forward pass, simulating action sequences and their consequences without requiring direct sensory input. The theory suggests that procedural knowledge is stored in a format that preserves causal dependencies, preconditions, postconditions, and state transformations, enabling the model to 'run' procedures mentally by propagating state changes through its internal representations. This execution-like process occurs through specific computational pathways in the transformer architecture, where attention mechanisms serve to bind procedural steps to object states, and feed-forward layers perform state transformation computations. Critically, this theory distinguishes between surface-level pattern matching and deeper functional representations that maintain operational semantics.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Procedural knowledge in language models is encoded in a format that preserves functional structure, including causal dependencies, preconditions, postconditions, and state transformations, rather than merely as text patterns.</li>
                <li>During embodied planning tasks, language models perform a form of symbolic execution by propagating state changes through internal representations across transformer layers, where each layer performs incremental state updates.</li>
                <li>Attention mechanisms serve as binding operators that connect procedural steps to relevant objects, states, and contextual information during execution simulation, with specific attention heads specializing in different binding types (action-object, temporal, causal).</li>
                <li>Feed-forward layers in transformers implement state transformation functions that compute the effects of actions on object states and spatial configurations, with different neurons encoding different types of state changes.</li>
                <li>The hierarchical structure of transformers enables multi-level procedural representation, with lower layers encoding primitive actions and their immediate effects, and higher layers encoding abstract procedures, control flow, and goal structures.</li>
                <li>Procedural execution in language models is compositional: complex procedures are constructed by combining and sequencing simpler procedural primitives stored in the model's parameters, following compositional semantics similar to functional programming.</li>
                <li>The model's ability to plan without sensory input derives from learned world models that are implicitly invoked during procedural execution to predict action outcomes, with these world models encoded in the weights and activation patterns.</li>
                <li>Procedural knowledge exhibits a dual representation: both as linguistic descriptions (for generation and communication) and as executable structures (for reasoning, planning, and constraint checking).</li>
                <li>The execution process maintains implicit state representations that track object properties, locations, and relationships throughout the procedure, with state updates occurring through specific computational pathways.</li>
                <li>Procedural representations include constraint checking mechanisms that evaluate preconditions and detect impossible action sequences during the execution simulation process.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Language models demonstrate systematic ability to generate valid action sequences for complex multi-step tasks, suggesting internal representations that capture procedural structure beyond surface-level text patterns. Models can produce coherent plans for household tasks, robotic manipulation, and navigation without explicit training on those specific task instances. </li>
    <li>Probing studies reveal that transformer layers encode hierarchical task structure, with early layers capturing low-level actions and later layers representing higher-level goals and subgoals. This hierarchical organization mirrors the structure of executable programs with nested procedures. </li>
    <li>Language models can perform state tracking during procedural reasoning, maintaining implicit representations of object locations, states, and properties as procedures unfold. This includes tracking which objects have been picked up, moved, or modified through action sequences. </li>
    <li>Models show transfer between code understanding and procedural planning tasks, suggesting shared computational mechanisms for executing symbolic procedures. Models trained on code demonstrate enhanced ability to perform multi-step reasoning and maintain state consistency. </li>
    <li>Attention patterns during procedural reasoning show systematic binding between action verbs and their object arguments, as well as temporal dependencies between sequential steps. These patterns are consistent across similar procedures, suggesting stable procedural representations. </li>
    <li>Language models can detect precondition violations and impossible action sequences, indicating they maintain constraint representations during procedural execution. Models can identify when actions are attempted on unavailable objects or in incorrect states. </li>
    <li>Mechanistic interpretability studies show that specific attention heads and feed-forward sublayers activate consistently for particular types of procedural operations, suggesting specialized computational circuits for procedural execution. </li>
    <li>Language models exhibit compositional generalization in procedural tasks, successfully combining known procedural primitives in novel ways to solve new problems, consistent with executable representations that can be composed. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Language models should show better performance on procedural planning tasks when the procedures can be decomposed into previously-seen primitive actions, even if the overall task is novel. Performance should degrade gracefully with the number of novel primitive compositions required.</li>
                <li>Intervening on intermediate layer activations during procedural reasoning should cause systematic errors in downstream action predictions, with effects proportional to the importance of the modified step. Specifically, corrupting state-tracking activations should cause errors in subsequent actions that depend on that state.</li>
                <li>Models fine-tuned on code execution tasks should show improved performance on embodied planning tasks compared to models trained only on natural language, due to enhanced procedural execution capabilities. The improvement should be most pronounced for tasks requiring precise state tracking.</li>
                <li>Attention entropy should decrease during critical procedural steps where specific object bindings are required (e.g., 'pick up the red block'), and increase during more flexible planning phases (e.g., 'organize the workspace').</li>
                <li>Models should exhibit consistent 'execution traces' in their activation patterns when processing the same procedure multiple times, similar to how programs produce consistent execution paths. The similarity should be measurable through activation space distance metrics.</li>
                <li>Procedures with more explicit causal structure in their training data should be executed more reliably than procedures with implicit causality, as the executable representation depends on learning causal dependencies.</li>
                <li>When given partial procedures, models should be able to complete them in ways that respect state consistency, suggesting they are simulating execution rather than pattern matching. For example, if an object is moved in step 1, subsequent steps should reference its new location.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If procedural knowledge is truly executable, it may be possible to 'compile' natural language procedures into explicit computational graphs by tracing model activations, potentially enabling formal verification of LM planning capabilities. This would require identifying stable activation patterns that correspond to procedural primitives.</li>
                <li>Language models might be able to perform counterfactual procedural reasoning by 'forking' execution states and exploring alternative action sequences in parallel through different attention heads, though this would require sophisticated internal state management that may exceed current architectural capabilities.</li>
                <li>The executable nature of procedural knowledge might enable models to perform procedure inversion - given a goal state and initial state, working backwards to derive necessary action sequences - with accuracy comparable to forward planning. This would test whether the representations truly encode bidirectional causal relationships.</li>
                <li>If procedural representations are truly functional, it might be possible to transfer learned procedures between different language models by identifying and transplanting specific parameter subspaces (e.g., through weight interpolation or module swapping), similar to how code can be ported between systems.</li>
                <li>Models might maintain implicit 'call stacks' for nested procedures, enabling them to handle recursive task structures and return to higher-level goals after completing subgoals. This would manifest as specific activation patterns that encode hierarchical context.</li>
                <li>It may be possible to 'debug' procedural reasoning by identifying which computational components (attention heads, feed-forward sublayers) are responsible for specific execution errors, and selectively fine-tuning those components to fix systematic mistakes.</li>
                <li>Models might be able to perform procedural abstraction, automatically identifying common sub-procedures across different tasks and creating reusable procedural primitives, though this would require meta-learning capabilities beyond standard training.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If procedural knowledge is not executable but merely pattern-based, then systematically permuting the order of steps in training data should not significantly impair the model's ability to generate valid action sequences for test cases with correct ordering, contradicting the theory's claim about causal dependency encoding.</li>
                <li>If the theory is incorrect, then ablating feed-forward layers should not disproportionately affect state transformation reasoning compared to other reasoning types, as these layers would not be specifically implementing state update computations. A uniform effect across reasoning types would challenge the theory.</li>
                <li>Finding that models cannot detect impossible action sequences (e.g., using an object before picking it up, or moving an object that was already moved elsewhere) would suggest they lack true procedural execution with constraint checking.</li>
                <li>If attention patterns during procedural reasoning are random or task-independent rather than systematically binding actions to objects, this would contradict the theory's claim about attention serving as a binding mechanism. This can be tested by measuring attention pattern consistency across similar procedures.</li>
                <li>Demonstrating that models trained exclusively on procedural descriptions perform identically to models trained on both descriptions and execution traces (or code) would challenge the distinction between declarative and executable procedural representations.</li>
                <li>If models cannot maintain consistent state representations across long procedural sequences (e.g., tracking multiple object locations through many steps), showing random or degraded performance rather than systematic errors, this would undermine the claim of true procedural execution capability.</li>
                <li>If intervening on activations hypothesized to represent object states does not cause predictable downstream errors in actions involving those objects, this would challenge the theory's claim about explicit state tracking during execution.</li>
                <li>Finding that models perform equally well on procedures regardless of whether the causal structure is explicit or violated in training data would suggest they are not learning executable representations with causal dependencies.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact neural mechanisms by which transformer architectures implement state transformation computations remain unclear, particularly how continuous vector representations encode discrete state changes and how these representations maintain consistency across multiple transformation steps. </li>
    <li>The theory does not fully explain how language models handle uncertainty and partial observability in procedural planning, which are critical for real-world embodied tasks. The executable artifact framework assumes complete state knowledge, but real planning often requires reasoning under uncertainty. </li>
    <li>The relationship between model scale and procedural execution capability is not well characterized - whether larger models simply have more procedures stored, fundamentally different execution mechanisms, or more sophisticated state tracking capabilities. </li>
    <li>The theory does not address how models handle conflicting or ambiguous procedural knowledge, such as when multiple valid procedures exist for the same goal or when procedures learned from different contexts contradict each other. </li>
    <li>The mechanisms for learning procedural representations from text alone (without explicit execution feedback) are not fully explained. How does the model extract executable structure from linguistic descriptions that may be incomplete or ambiguous? </li>
    <li>The theory does not fully account for how models handle temporal dynamics and continuous processes, as opposed to discrete action sequences. Many embodied tasks involve continuous control and temporal reasoning. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Huang et al. (2022) Language Models as Zero-Shot Planners [Related work on LM planning capabilities but doesn't propose the executable artifact representation theory or symbolic execution framework]</li>
    <li>Andreas (2022) Language Models as Agent Models [Discusses LMs modeling agents but not specifically procedural knowledge as executable structures with state transformation semantics]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Related to multi-step reasoning but focuses on explicit external scratchpads rather than internal execution mechanisms]</li>
    <li>Liang et al. (2022) Code as Policies: Language Model Programs for Embodied Control [Uses LMs to generate code for execution but doesn't theorize about internal procedural representations as executable artifacts]</li>
    <li>Ahn et al. (2022) Do As I Can, Not As I Say [Demonstrates LM planning capabilities but doesn't propose the executable artifacts theoretical framework or symbolic execution mechanism]</li>
    <li>Mirchandani et al. (2023) Large Language Models as General Pattern Machines [Discusses pattern-based reasoning but doesn't propose executable procedural representations]</li>
    <li>Silver et al. (2016) Mastering the game of Go with deep neural networks and tree search [Describes planning through search but in a different architectural context without language]</li>
    <li>Sutton & Barto (2018) Reinforcement Learning: An Introduction [Describes procedural learning through RL but not the specific executable artifact theory for language models]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Procedural Knowledge as Executable Artifacts Theory",
    "theory_description": "This theory posits that language models encode procedural knowledge not merely as sequential text patterns or declarative descriptions, but as quasi-executable computational artifacts that possess functional structure analogous to programs or scripts. When engaged in embodied planning tasks, the model dynamically instantiates these procedural representations and performs a form of symbolic execution through its forward pass, simulating action sequences and their consequences without requiring direct sensory input. The theory suggests that procedural knowledge is stored in a format that preserves causal dependencies, preconditions, postconditions, and state transformations, enabling the model to 'run' procedures mentally by propagating state changes through its internal representations. This execution-like process occurs through specific computational pathways in the transformer architecture, where attention mechanisms serve to bind procedural steps to object states, and feed-forward layers perform state transformation computations. Critically, this theory distinguishes between surface-level pattern matching and deeper functional representations that maintain operational semantics.",
    "supporting_evidence": [
        {
            "text": "Language models demonstrate systematic ability to generate valid action sequences for complex multi-step tasks, suggesting internal representations that capture procedural structure beyond surface-level text patterns. Models can produce coherent plans for household tasks, robotic manipulation, and navigation without explicit training on those specific task instances.",
            "citations": [
                "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "Huang et al. (2022) Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"
            ]
        },
        {
            "text": "Probing studies reveal that transformer layers encode hierarchical task structure, with early layers capturing low-level actions and later layers representing higher-level goals and subgoals. This hierarchical organization mirrors the structure of executable programs with nested procedures.",
            "citations": [
                "Tenney et al. (2019) BERT Rediscovers the Classical NLP Pipeline",
                "Jawahar et al. (2019) What Does BERT Learn About the Structure of Language?"
            ]
        },
        {
            "text": "Language models can perform state tracking during procedural reasoning, maintaining implicit representations of object locations, states, and properties as procedures unfold. This includes tracking which objects have been picked up, moved, or modified through action sequences.",
            "citations": [
                "Dalvi et al. (2021) Explaining Answers with Entailment Trees for Structured Reasoning",
                "Tafjord et al. (2021) ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language"
            ]
        },
        {
            "text": "Models show transfer between code understanding and procedural planning tasks, suggesting shared computational mechanisms for executing symbolic procedures. Models trained on code demonstrate enhanced ability to perform multi-step reasoning and maintain state consistency.",
            "citations": [
                "Chen et al. (2021) Evaluating Large Language Models Trained on Code",
                "Austin et al. (2021) Program Synthesis with Large Language Models"
            ]
        },
        {
            "text": "Attention patterns during procedural reasoning show systematic binding between action verbs and their object arguments, as well as temporal dependencies between sequential steps. These patterns are consistent across similar procedures, suggesting stable procedural representations.",
            "citations": [
                "Clark et al. (2019) What Does BERT Look At? An Analysis of BERT's Attention",
                "Kovaleva et al. (2019) Revealing the Dark Secrets of BERT"
            ]
        },
        {
            "text": "Language models can detect precondition violations and impossible action sequences, indicating they maintain constraint representations during procedural execution. Models can identify when actions are attempted on unavailable objects or in incorrect states.",
            "citations": [
                "Sap et al. (2019) Atomic: An Atlas of Machine Commonsense for If-Then Reasoning",
                "Bosselut et al. (2019) COMET: Commonsense Transformers for Automatic Knowledge Graph Construction"
            ]
        },
        {
            "text": "Mechanistic interpretability studies show that specific attention heads and feed-forward sublayers activate consistently for particular types of procedural operations, suggesting specialized computational circuits for procedural execution.",
            "citations": [
                "Elhage et al. (2021) A Mathematical Framework for Transformer Circuits",
                "Olsson et al. (2022) In-context Learning and Induction Heads"
            ]
        },
        {
            "text": "Language models exhibit compositional generalization in procedural tasks, successfully combining known procedural primitives in novel ways to solve new problems, consistent with executable representations that can be composed.",
            "citations": [
                "Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models"
            ]
        }
    ],
    "theory_statements": [
        "Procedural knowledge in language models is encoded in a format that preserves functional structure, including causal dependencies, preconditions, postconditions, and state transformations, rather than merely as text patterns.",
        "During embodied planning tasks, language models perform a form of symbolic execution by propagating state changes through internal representations across transformer layers, where each layer performs incremental state updates.",
        "Attention mechanisms serve as binding operators that connect procedural steps to relevant objects, states, and contextual information during execution simulation, with specific attention heads specializing in different binding types (action-object, temporal, causal).",
        "Feed-forward layers in transformers implement state transformation functions that compute the effects of actions on object states and spatial configurations, with different neurons encoding different types of state changes.",
        "The hierarchical structure of transformers enables multi-level procedural representation, with lower layers encoding primitive actions and their immediate effects, and higher layers encoding abstract procedures, control flow, and goal structures.",
        "Procedural execution in language models is compositional: complex procedures are constructed by combining and sequencing simpler procedural primitives stored in the model's parameters, following compositional semantics similar to functional programming.",
        "The model's ability to plan without sensory input derives from learned world models that are implicitly invoked during procedural execution to predict action outcomes, with these world models encoded in the weights and activation patterns.",
        "Procedural knowledge exhibits a dual representation: both as linguistic descriptions (for generation and communication) and as executable structures (for reasoning, planning, and constraint checking).",
        "The execution process maintains implicit state representations that track object properties, locations, and relationships throughout the procedure, with state updates occurring through specific computational pathways.",
        "Procedural representations include constraint checking mechanisms that evaluate preconditions and detect impossible action sequences during the execution simulation process."
    ],
    "new_predictions_likely": [
        "Language models should show better performance on procedural planning tasks when the procedures can be decomposed into previously-seen primitive actions, even if the overall task is novel. Performance should degrade gracefully with the number of novel primitive compositions required.",
        "Intervening on intermediate layer activations during procedural reasoning should cause systematic errors in downstream action predictions, with effects proportional to the importance of the modified step. Specifically, corrupting state-tracking activations should cause errors in subsequent actions that depend on that state.",
        "Models fine-tuned on code execution tasks should show improved performance on embodied planning tasks compared to models trained only on natural language, due to enhanced procedural execution capabilities. The improvement should be most pronounced for tasks requiring precise state tracking.",
        "Attention entropy should decrease during critical procedural steps where specific object bindings are required (e.g., 'pick up the red block'), and increase during more flexible planning phases (e.g., 'organize the workspace').",
        "Models should exhibit consistent 'execution traces' in their activation patterns when processing the same procedure multiple times, similar to how programs produce consistent execution paths. The similarity should be measurable through activation space distance metrics.",
        "Procedures with more explicit causal structure in their training data should be executed more reliably than procedures with implicit causality, as the executable representation depends on learning causal dependencies.",
        "When given partial procedures, models should be able to complete them in ways that respect state consistency, suggesting they are simulating execution rather than pattern matching. For example, if an object is moved in step 1, subsequent steps should reference its new location."
    ],
    "new_predictions_unknown": [
        "If procedural knowledge is truly executable, it may be possible to 'compile' natural language procedures into explicit computational graphs by tracing model activations, potentially enabling formal verification of LM planning capabilities. This would require identifying stable activation patterns that correspond to procedural primitives.",
        "Language models might be able to perform counterfactual procedural reasoning by 'forking' execution states and exploring alternative action sequences in parallel through different attention heads, though this would require sophisticated internal state management that may exceed current architectural capabilities.",
        "The executable nature of procedural knowledge might enable models to perform procedure inversion - given a goal state and initial state, working backwards to derive necessary action sequences - with accuracy comparable to forward planning. This would test whether the representations truly encode bidirectional causal relationships.",
        "If procedural representations are truly functional, it might be possible to transfer learned procedures between different language models by identifying and transplanting specific parameter subspaces (e.g., through weight interpolation or module swapping), similar to how code can be ported between systems.",
        "Models might maintain implicit 'call stacks' for nested procedures, enabling them to handle recursive task structures and return to higher-level goals after completing subgoals. This would manifest as specific activation patterns that encode hierarchical context.",
        "It may be possible to 'debug' procedural reasoning by identifying which computational components (attention heads, feed-forward sublayers) are responsible for specific execution errors, and selectively fine-tuning those components to fix systematic mistakes.",
        "Models might be able to perform procedural abstraction, automatically identifying common sub-procedures across different tasks and creating reusable procedural primitives, though this would require meta-learning capabilities beyond standard training."
    ],
    "negative_experiments": [
        "If procedural knowledge is not executable but merely pattern-based, then systematically permuting the order of steps in training data should not significantly impair the model's ability to generate valid action sequences for test cases with correct ordering, contradicting the theory's claim about causal dependency encoding.",
        "If the theory is incorrect, then ablating feed-forward layers should not disproportionately affect state transformation reasoning compared to other reasoning types, as these layers would not be specifically implementing state update computations. A uniform effect across reasoning types would challenge the theory.",
        "Finding that models cannot detect impossible action sequences (e.g., using an object before picking it up, or moving an object that was already moved elsewhere) would suggest they lack true procedural execution with constraint checking.",
        "If attention patterns during procedural reasoning are random or task-independent rather than systematically binding actions to objects, this would contradict the theory's claim about attention serving as a binding mechanism. This can be tested by measuring attention pattern consistency across similar procedures.",
        "Demonstrating that models trained exclusively on procedural descriptions perform identically to models trained on both descriptions and execution traces (or code) would challenge the distinction between declarative and executable procedural representations.",
        "If models cannot maintain consistent state representations across long procedural sequences (e.g., tracking multiple object locations through many steps), showing random or degraded performance rather than systematic errors, this would undermine the claim of true procedural execution capability.",
        "If intervening on activations hypothesized to represent object states does not cause predictable downstream errors in actions involving those objects, this would challenge the theory's claim about explicit state tracking during execution.",
        "Finding that models perform equally well on procedures regardless of whether the causal structure is explicit or violated in training data would suggest they are not learning executable representations with causal dependencies."
    ],
    "unaccounted_for": [
        {
            "text": "The exact neural mechanisms by which transformer architectures implement state transformation computations remain unclear, particularly how continuous vector representations encode discrete state changes and how these representations maintain consistency across multiple transformation steps.",
            "citations": [
                "Elhage et al. (2021) A Mathematical Framework for Transformer Circuits",
                "Olsson et al. (2022) In-context Learning and Induction Heads"
            ]
        },
        {
            "text": "The theory does not fully explain how language models handle uncertainty and partial observability in procedural planning, which are critical for real-world embodied tasks. The executable artifact framework assumes complete state knowledge, but real planning often requires reasoning under uncertainty.",
            "citations": [
                "Kaelbling et al. (1998) Planning and Acting in Partially Observable Stochastic Domains",
                "Silver & Veness (2010) Monte-Carlo Planning in Large POMDPs"
            ]
        },
        {
            "text": "The relationship between model scale and procedural execution capability is not well characterized - whether larger models simply have more procedures stored, fundamentally different execution mechanisms, or more sophisticated state tracking capabilities.",
            "citations": [
                "Wei et al. (2022) Emergent Abilities of Large Language Models",
                "Kaplan et al. (2020) Scaling Laws for Neural Language Models"
            ]
        },
        {
            "text": "The theory does not address how models handle conflicting or ambiguous procedural knowledge, such as when multiple valid procedures exist for the same goal or when procedures learned from different contexts contradict each other.",
            "citations": [
                "Elazar et al. (2021) Measuring and Improving Consistency in Pretrained Language Models",
                "Kassner & Schütze (2020) Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"
            ]
        },
        {
            "text": "The mechanisms for learning procedural representations from text alone (without explicit execution feedback) are not fully explained. How does the model extract executable structure from linguistic descriptions that may be incomplete or ambiguous?",
            "citations": [
                "Bisk et al. (2020) Experience Grounds Language",
                "Bender & Koller (2020) Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data"
            ]
        },
        {
            "text": "The theory does not fully account for how models handle temporal dynamics and continuous processes, as opposed to discrete action sequences. Many embodied tasks involve continuous control and temporal reasoning.",
            "citations": [
                "Lample & Chaplot (2017) Playing FPS Games with Deep Reinforcement Learning",
                "Mnih et al. (2015) Human-level control through deep reinforcement learning"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that language models struggle with precise spatial reasoning and metric properties (e.g., exact distances, angles, sizes), which would be expected if they truly execute procedures with accurate world models. Models often produce qualitatively correct but quantitatively imprecise plans.",
            "citations": [
                "Mirzaee & Kordjamshidi (2022) Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
                "Patel et al. (2021) Mapping Language Models to Grounded Conceptual Spaces"
            ]
        },
        {
            "text": "Evidence that language models are highly sensitive to surface-level prompt variations (e.g., paraphrasing, instruction format) suggests their procedural reasoning may be more brittle than true executable representations would allow. Executable programs are typically robust to surface-level variations in how they are invoked.",
            "citations": [
                "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                "Lu et al. (2022) Fantastically Ordered Prompts and Where to Find Them"
            ]
        },
        {
            "text": "Studies showing that models can generate plausible but physically impossible action sequences (e.g., violating object permanence, conservation of mass, or basic physics) challenge the claim that they maintain accurate constraint representations during execution.",
            "citations": [
                "Marcus & Davis (2020) GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about",
                "Bender & Koller (2020) Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data"
            ]
        },
        {
            "text": "Research showing that models often fail at tasks requiring systematic compositional generalization (e.g., SCAN benchmark) suggests limitations in their ability to compose procedural primitives in truly systematic ways, contrary to what executable representations would enable.",
            "citations": [
                "Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"
            ]
        },
        {
            "text": "Evidence that models struggle with negation and counterfactual reasoning in procedural contexts challenges the claim that they perform true symbolic execution, which should naturally handle such logical operations.",
            "citations": [
                "Kassner & Schütze (2020) Negated and Misprimed Probes for Pretrained Language Models",
                "Ettinger (2020) What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"
            ]
        }
    ],
    "special_cases": [
        "The theory may apply differently to procedures learned from code versus natural language, as code provides more explicit execution semantics, formal syntax, and unambiguous state transformations. Code-trained models may have more robust executable representations.",
        "Very long procedural sequences may exceed the model's effective 'execution memory' (related to context window and attention span), causing degradation in state tracking accuracy. The theory predicts graceful degradation with sequence length.",
        "Procedures involving precise numerical or geometric reasoning may require specialized representations beyond the general executable artifact framework, potentially involving discrete symbolic computation rather than continuous vector operations.",
        "Novel procedures that require combining primitives in unprecedented ways may not be fully executable if the composition mechanisms are limited by training distribution. The theory may apply best to procedures within the compositional space covered by training.",
        "Procedures with complex control flow (loops, conditionals, exception handling) may require additional computational mechanisms beyond simple sequential execution, such as recurrent processing or specialized attention patterns for control flow.",
        "Procedures involving continuous dynamics or real-time constraints may not fit the discrete symbolic execution framework, requiring hybrid representations that combine discrete procedural steps with continuous state evolution.",
        "Domain-specific procedures (e.g., medical, legal, scientific) may have different execution characteristics depending on the amount and quality of domain-specific training data, with specialized domains potentially having weaker executable representations.",
        "Procedures that require external tool use or API calls may be represented differently than self-contained procedures, potentially as higher-order functions that compose external capabilities.",
        "Ambiguous or underspecified procedures may trigger multiple possible execution paths, requiring the model to either select one path, maintain multiple hypotheses, or request clarification - behaviors not fully specified by the current theory."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Huang et al. (2022) Language Models as Zero-Shot Planners [Related work on LM planning capabilities but doesn't propose the executable artifact representation theory or symbolic execution framework]",
            "Andreas (2022) Language Models as Agent Models [Discusses LMs modeling agents but not specifically procedural knowledge as executable structures with state transformation semantics]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Related to multi-step reasoning but focuses on explicit external scratchpads rather than internal execution mechanisms]",
            "Liang et al. (2022) Code as Policies: Language Model Programs for Embodied Control [Uses LMs to generate code for execution but doesn't theorize about internal procedural representations as executable artifacts]",
            "Ahn et al. (2022) Do As I Can, Not As I Say [Demonstrates LM planning capabilities but doesn't propose the executable artifacts theoretical framework or symbolic execution mechanism]",
            "Mirchandani et al. (2023) Large Language Models as General Pattern Machines [Discusses pattern-based reasoning but doesn't propose executable procedural representations]",
            "Silver et al. (2016) Mastering the game of Go with deep neural networks and tree search [Describes planning through search but in a different architectural context without language]",
            "Sutton & Barto (2018) Reinforcement Learning: An Introduction [Describes procedural learning through RL but not the specific executable artifact theory for language models]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory of how language models encode and utilize spatial, procedural, and object-relational knowledge for embodied planning tasks without direct sensory input.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-64",
    "original_theory_name": "Procedural Knowledge as Executable Artifacts Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Salience and Attention Allocation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1935</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1935</p>
                <p><strong>Name:</strong> Information Salience and Attention Allocation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the way information is presented in a problem format modulates the salience of key elements, thereby influencing the allocation of attention within the LLM's internal mechanisms. Formats that highlight relevant information or reduce distractors enhance LLM focus and accuracy, while formats that obscure or dilute key information lead to misallocation of attention and increased error rates.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Salience-Driven Attention Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; increases &#8594; salience_of_relevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_attention &#8594; is_focused_on &#8594; relevant_information<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_performance &#8594; is &#8594; improved</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Highlighting or bolding key information in prompts improves LLM accuracy. </li>
    <li>Distractor information or poor formatting increases LLM error rates. </li>
    <li>Instruction tuning that emphasizes relevant details leads to better performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Closely related to existing work on attention, but the explicit mapping from format to attention allocation is novel.</p>            <p><strong>What Already Exists:</strong> Attention mechanisms in transformers are known to focus on salient tokens; prompt engineering can modulate this.</p>            <p><strong>What is Novel:</strong> The law formalizes the link between external salience (format) and internal attention allocation in LLMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Vaswani et al. (2017) Attention is All You Need [Transformer attention mechanism]</li>
    <li>Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt format and attention]</li>
</ul>
            <h3>Statement 1: Distractor-Induced Error Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; irrelevant_or_distractor_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_attention &#8594; is_diverted_from &#8594; relevant_information<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_performance &#8594; is &#8594; degraded</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs make more errors when prompts include irrelevant details or distractors. </li>
    <li>Removing distractors from prompts improves LLM accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Closely related to existing work but formalizes the distractor effect as a predictive law.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs can be distracted by irrelevant information in prompts.</p>            <p><strong>What is Novel:</strong> The law formalizes the causal relationship between distractors, attention allocation, and performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2020) How Can We Know What Language Models Know? [Prompt format and distractors]</li>
    <li>Vaswani et al. (2017) Attention is All You Need [Attention mechanism]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Adding irrelevant information to a prompt will decrease LLM accuracy on the main task.</li>
                <li>Explicitly highlighting or formatting key information will improve LLM performance.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained with adversarial distractors, can they learn to ignore them and maintain high performance?</li>
                <li>Can LLMs develop internal mechanisms to reweight attention in the presence of misleading salience cues?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs are unaffected by distractors or irrelevant information in prompts, the theory is falsified.</li>
                <li>If increasing salience of relevant information does not improve performance, the theory is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs ignore salient cues due to strong pretraining biases or instruction tuning. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends existing attention research to a predictive framework for problem format effects.</p>
            <p><strong>References:</strong> <ul>
    <li>Vaswani et al. (2017) Attention is All You Need [Transformer attention mechanism]</li>
    <li>Jiang et al. (2020) How Can We Know What Language Models Know? [Prompt format and distractors]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Salience and Attention Allocation Theory",
    "theory_description": "This theory proposes that the way information is presented in a problem format modulates the salience of key elements, thereby influencing the allocation of attention within the LLM's internal mechanisms. Formats that highlight relevant information or reduce distractors enhance LLM focus and accuracy, while formats that obscure or dilute key information lead to misallocation of attention and increased error rates.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Salience-Driven Attention Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "increases",
                        "object": "salience_of_relevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_attention",
                        "relation": "is_focused_on",
                        "object": "relevant_information"
                    },
                    {
                        "subject": "LLM_performance",
                        "relation": "is",
                        "object": "improved"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Highlighting or bolding key information in prompts improves LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Distractor information or poor formatting increases LLM error rates.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction tuning that emphasizes relevant details leads to better performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Attention mechanisms in transformers are known to focus on salient tokens; prompt engineering can modulate this.",
                    "what_is_novel": "The law formalizes the link between external salience (format) and internal attention allocation in LLMs.",
                    "classification_explanation": "Closely related to existing work on attention, but the explicit mapping from format to attention allocation is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Vaswani et al. (2017) Attention is All You Need [Transformer attention mechanism]",
                        "Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt format and attention]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Distractor-Induced Error Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "irrelevant_or_distractor_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_attention",
                        "relation": "is_diverted_from",
                        "object": "relevant_information"
                    },
                    {
                        "subject": "LLM_performance",
                        "relation": "is",
                        "object": "degraded"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs make more errors when prompts include irrelevant details or distractors.",
                        "uuids": []
                    },
                    {
                        "text": "Removing distractors from prompts improves LLM accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs can be distracted by irrelevant information in prompts.",
                    "what_is_novel": "The law formalizes the causal relationship between distractors, attention allocation, and performance.",
                    "classification_explanation": "Closely related to existing work but formalizes the distractor effect as a predictive law.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Jiang et al. (2020) How Can We Know What Language Models Know? [Prompt format and distractors]",
                        "Vaswani et al. (2017) Attention is All You Need [Attention mechanism]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Adding irrelevant information to a prompt will decrease LLM accuracy on the main task.",
        "Explicitly highlighting or formatting key information will improve LLM performance."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained with adversarial distractors, can they learn to ignore them and maintain high performance?",
        "Can LLMs develop internal mechanisms to reweight attention in the presence of misleading salience cues?"
    ],
    "negative_experiments": [
        "If LLMs are unaffected by distractors or irrelevant information in prompts, the theory is falsified.",
        "If increasing salience of relevant information does not improve performance, the theory is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs ignore salient cues due to strong pretraining biases or instruction tuning.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs maintain high accuracy even with distractor-laden prompts, suggesting robustness in certain architectures.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with advanced attention mechanisms or adversarial training may be less sensitive to distractors.",
        "Highly ambiguous or contradictory salience cues may confuse LLMs regardless of format."
    ],
    "existing_theory": {
        "what_already_exists": "Attention mechanisms and the effect of distractors are well-studied in LLMs.",
        "what_is_novel": "The explicit mapping from external salience (format) to internal attention allocation and performance is new.",
        "classification_explanation": "The theory extends existing attention research to a predictive framework for problem format effects.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Vaswani et al. (2017) Attention is All You Need [Transformer attention mechanism]",
            "Jiang et al. (2020) How Can We Know What Language Models Know? [Prompt format and distractors]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-655",
    "original_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Reasoning Augmentation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1152</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1152</p>
                <p><strong>Name:</strong> Dual-Process Reasoning Augmentation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models (LMs) can best perform strict logical reasoning when their architecture or inference process explicitly integrates two distinct but interacting reasoning systems: (1) a fast, pattern-matching, heuristic-driven process (System 1), and (2) a slow, rule-based, symbolic manipulation process (System 2). The theory asserts that optimal logical reasoning emerges when LMs can dynamically allocate cognitive resources between these systems, using System 1 for rapid context understanding and System 2 for stepwise, verifiable logical inference, with explicit arbitration between the two.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: System 2 Arbitration Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_presented_with &#8594; logical reasoning task<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; multi-step or non-trivial inference</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; invokes &#8594; explicit symbolic reasoning module (System 2)<span style="color: #888888;">, and</span></div>
        <div>&#8226; reasoning process &#8594; is &#8594; stepwise, verifiable, and rule-based</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show LMs struggle with multi-step logic unless forced to reason stepwise (e.g., chain-of-thought prompting). </li>
    <li>Cognitive science literature supports dual-process models for human logical reasoning. </li>
    <li>LMs often make logical errors on tasks requiring multiple inferential steps unless guided to use explicit reasoning. </li>
    <li>Symbolic reasoning modules (e.g., external tools, scratchpads) improve LM performance on logic tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While dual-process models exist in psychology and some LM prompting methods mimic System 2, explicit arbitration and modular integration in LMs is novel.</p>            <p><strong>What Already Exists:</strong> Dual-process theories are well-established in cognitive science; chain-of-thought and scratchpad prompting in LMs are related.</p>            <p><strong>What is Novel:</strong> Explicit architectural arbitration between neural (System 1) and symbolic (System 2) modules in LMs is not standard; the law formalizes when and how System 2 should be invoked.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2003) In two minds: dual-process accounts of reasoning [dual-process theory in humans]</li>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning in LMs]</li>
    <li>Creswell et al. (2022) Selection-inference: Exploiting large language models for interpretable logical reasoning [modular reasoning in LMs]</li>
</ul>
            <h3>Statement 1: Dynamic Resource Allocation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_evaluating &#8594; reasoning task<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; has &#8594; ambiguous or context-dependent cues</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; allocates &#8594; greater computational resources to System 2<span style="color: #888888;">, and</span></div>
        <div>&#8226; reasoning process &#8594; shifts &#8594; from heuristic to rule-based</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs often default to heuristics unless prompted for stepwise reasoning; ambiguous tasks benefit from explicit logical steps. </li>
    <li>Human studies show increased System 2 engagement under ambiguity or cognitive conflict. </li>
    <li>Prompting LMs to 'think step by step' improves performance on ambiguous or complex logic tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is inspired by cognitive science but proposes a novel, context-driven mechanism for LMs.</p>            <p><strong>What Already Exists:</strong> Resource allocation between fast and slow reasoning is known in human cognition.</p>            <p><strong>What is Novel:</strong> Dynamic, context-sensitive resource allocation in LMs, especially for logical reasoning, is not standard practice.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [resource allocation in dual-process theory]</li>
    <li>Nye et al. (2021) Improving coherence and consistency in neural sequence models with dual-system reasoning [dual-system in LMs, but not dynamic allocation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LM is equipped with an explicit symbolic reasoning module and a mechanism to invoke it for multi-step logic tasks, its logical reasoning accuracy will increase compared to standard LMs.</li>
                <li>Tasks with ambiguous or conflicting cues will show improved performance when the LM dynamically increases System 2 engagement.</li>
                <li>Prompting LMs to 'think step by step' will disproportionately benefit tasks requiring strict logical inference, but not simple factual recall.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Integrating a real-time arbitration mechanism between neural and symbolic modules may enable LMs to self-correct logical errors without external prompting.</li>
                <li>Dynamic resource allocation may allow LMs to generalize logical reasoning to novel domains (e.g., mathematical proofs) beyond their training data.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs with explicit symbolic modules do not outperform standard LMs on strict logical reasoning tasks, the theory is called into question.</li>
                <li>If dynamic resource allocation does not correlate with improved performance on ambiguous logical tasks, the theory's mechanism is undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs can perform well on certain logic tasks without explicit stepwise reasoning, suggesting alternative mechanisms. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is inspired by cognitive science but proposes a new architectural principle for LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2003) In two minds: dual-process accounts of reasoning [dual-process theory in humans]</li>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning in LMs]</li>
    <li>Nye et al. (2021) Improving coherence and consistency in neural sequence models with dual-system reasoning [dual-system in LMs, but not dynamic allocation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Reasoning Augmentation Theory",
    "theory_description": "This theory posits that language models (LMs) can best perform strict logical reasoning when their architecture or inference process explicitly integrates two distinct but interacting reasoning systems: (1) a fast, pattern-matching, heuristic-driven process (System 1), and (2) a slow, rule-based, symbolic manipulation process (System 2). The theory asserts that optimal logical reasoning emerges when LMs can dynamically allocate cognitive resources between these systems, using System 1 for rapid context understanding and System 2 for stepwise, verifiable logical inference, with explicit arbitration between the two.",
    "theory_statements": [
        {
            "law": {
                "law_name": "System 2 Arbitration Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_presented_with",
                        "object": "logical reasoning task"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "multi-step or non-trivial inference"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "invokes",
                        "object": "explicit symbolic reasoning module (System 2)"
                    },
                    {
                        "subject": "reasoning process",
                        "relation": "is",
                        "object": "stepwise, verifiable, and rule-based"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show LMs struggle with multi-step logic unless forced to reason stepwise (e.g., chain-of-thought prompting).",
                        "uuids": []
                    },
                    {
                        "text": "Cognitive science literature supports dual-process models for human logical reasoning.",
                        "uuids": []
                    },
                    {
                        "text": "LMs often make logical errors on tasks requiring multiple inferential steps unless guided to use explicit reasoning.",
                        "uuids": []
                    },
                    {
                        "text": "Symbolic reasoning modules (e.g., external tools, scratchpads) improve LM performance on logic tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Dual-process theories are well-established in cognitive science; chain-of-thought and scratchpad prompting in LMs are related.",
                    "what_is_novel": "Explicit architectural arbitration between neural (System 1) and symbolic (System 2) modules in LMs is not standard; the law formalizes when and how System 2 should be invoked.",
                    "classification_explanation": "While dual-process models exist in psychology and some LM prompting methods mimic System 2, explicit arbitration and modular integration in LMs is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Evans (2003) In two minds: dual-process accounts of reasoning [dual-process theory in humans]",
                        "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning in LMs]",
                        "Creswell et al. (2022) Selection-inference: Exploiting large language models for interpretable logical reasoning [modular reasoning in LMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dynamic Resource Allocation Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_evaluating",
                        "object": "reasoning task"
                    },
                    {
                        "subject": "task",
                        "relation": "has",
                        "object": "ambiguous or context-dependent cues"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "allocates",
                        "object": "greater computational resources to System 2"
                    },
                    {
                        "subject": "reasoning process",
                        "relation": "shifts",
                        "object": "from heuristic to rule-based"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs often default to heuristics unless prompted for stepwise reasoning; ambiguous tasks benefit from explicit logical steps.",
                        "uuids": []
                    },
                    {
                        "text": "Human studies show increased System 2 engagement under ambiguity or cognitive conflict.",
                        "uuids": []
                    },
                    {
                        "text": "Prompting LMs to 'think step by step' improves performance on ambiguous or complex logic tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Resource allocation between fast and slow reasoning is known in human cognition.",
                    "what_is_novel": "Dynamic, context-sensitive resource allocation in LMs, especially for logical reasoning, is not standard practice.",
                    "classification_explanation": "The law is inspired by cognitive science but proposes a novel, context-driven mechanism for LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kahneman (2011) Thinking, Fast and Slow [resource allocation in dual-process theory]",
                        "Nye et al. (2021) Improving coherence and consistency in neural sequence models with dual-system reasoning [dual-system in LMs, but not dynamic allocation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LM is equipped with an explicit symbolic reasoning module and a mechanism to invoke it for multi-step logic tasks, its logical reasoning accuracy will increase compared to standard LMs.",
        "Tasks with ambiguous or conflicting cues will show improved performance when the LM dynamically increases System 2 engagement.",
        "Prompting LMs to 'think step by step' will disproportionately benefit tasks requiring strict logical inference, but not simple factual recall."
    ],
    "new_predictions_unknown": [
        "Integrating a real-time arbitration mechanism between neural and symbolic modules may enable LMs to self-correct logical errors without external prompting.",
        "Dynamic resource allocation may allow LMs to generalize logical reasoning to novel domains (e.g., mathematical proofs) beyond their training data."
    ],
    "negative_experiments": [
        "If LMs with explicit symbolic modules do not outperform standard LMs on strict logical reasoning tasks, the theory is called into question.",
        "If dynamic resource allocation does not correlate with improved performance on ambiguous logical tasks, the theory's mechanism is undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs can perform well on certain logic tasks without explicit stepwise reasoning, suggesting alternative mechanisms.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Recent transformer-based LMs have shown emergent logical abilities without explicit dual-process architecture.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks that are purely pattern-based or require no logical inference may not benefit from System 2 engagement.",
        "Resource constraints (e.g., limited compute) may prevent effective dynamic allocation."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process models in psychology and some prompting methods in LMs.",
        "what_is_novel": "Explicit, modular integration and arbitration between neural and symbolic reasoning in LMs.",
        "classification_explanation": "The theory is inspired by cognitive science but proposes a new architectural principle for LMs.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Evans (2003) In two minds: dual-process accounts of reasoning [dual-process theory in humans]",
            "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning in LMs]",
            "Nye et al. (2021) Improving coherence and consistency in neural sequence models with dual-system reasoning [dual-system in LMs, but not dynamic allocation]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-605",
    "original_theory_name": "Preference Optimization and Hard Negative Sampling for Robust Multi-Step Reasoning",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
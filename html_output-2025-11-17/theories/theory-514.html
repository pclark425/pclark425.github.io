<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Input Clarification Ensembling for Decomposition of LLM Uncertainty - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-514</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-514</p>
                <p><strong>Name:</strong> Input Clarification Ensembling for Decomposition of LLM Uncertainty</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can accurately measure the probability or likelihood of specific future real-world scientific discoveries, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that large language models' (LLMs) predictive uncertainty about scientific discoveries or real-world questions can be decomposed into aleatoric (input ambiguity) and epistemic (model) components by generating and ensembling over multiple clarifications of the input. By systematically clarifying ambiguous or underspecified scientific questions and aggregating the model's predictions across these clarifications, it is possible to more accurately detect ambiguous questions, improve the calibration of uncertainty estimates, and distinguish between uncertainty due to input ambiguity and uncertainty due to model limitations. This approach outperforms direct confidence elicitation and sampling-based heuristics, and enables more actionable uncertainty quantification for scientific forecasting and discovery.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Clarification-Ensemble Aleatoric Decomposition Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; generates &#8594; multiple clarifications of an ambiguous scientific input<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; samples &#8594; answers for each clarification<span style="color: #888888;">, and</span></div>
        <div>&#8226; ensemble &#8594; computes &#8594; predictive entropy and mutual information across clarifications</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; ensemble &#8594; separates &#8594; aleatoric (input ambiguity) and epistemic (model) uncertainty components</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Input Clarification Ensembling (ICE) outperforms direct confidence elicitation (Ask4CONF, Ask4CONF-D) and sampling-based baselines (Self-Consistency, Sample Repetition, Sample Diversity) in ambiguity detection (e.g., AUROC up to 71.7 for ICE vs 55-58 for baselines on AmbigQA). ICE also enables explicit decomposition of predictive entropy into aleatoric and epistemic components, as shown by mutual information calculations. <a href="../results/extraction-result-3740.html#e3740.0" class="evidence-link">[e3740.0]</a> <a href="../results/extraction-result-3740.html#e3740.2" class="evidence-link">[e3740.2]</a> <a href="../results/extraction-result-3740.html#e3740.4" class="evidence-link">[e3740.4]</a> </li>
    <li>ICE's decomposition is more sensitive to subtle ambiguities than direct ambiguity-probability elicitation, and can distinguish between uncertainty due to input ambiguity and model uncertainty. <a href="../results/extraction-result-3740.html#e3740.0" class="evidence-link">[e3740.0]</a> </li>
    <li>Fine-tuned small LLMs (e.g., Llama-3-8B-Instruct) can serve as efficient clarifiers, and using high-quality clarifiers (e.g., GPT-4) further improves ambiguity detection and uncertainty decomposition. <a href="../results/extraction-result-3740.html#e3740.7" class="evidence-link">[e3740.7]</a> <a href="../results/extraction-result-3740.html#e3740.6" class="evidence-link">[e3740.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Clarification Reduces Aleatoric Uncertainty Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; clarifications &#8594; are generated &#8594; to resolve input ambiguity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; measured aleatoric uncertainty &#8594; decreases &#8594; monotonically as ambiguity is reduced</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Clarifying inputs reduces measured aleatoric uncertainty, as shown by monotonicity checks in ambiguity detection experiments: when ground-truth clarifications are used, measured aleatoric entropy drops substantially (e.g., Ours* with ground-truth clarifications achieves AUROC=89.8, F1=85.6 on AmbigQA). <a href="../results/extraction-result-3740.html#e3740.0" class="evidence-link">[e3740.0]</a> </li>
    <li>Generating multiple clarifications increases recall of target answers and reduces the measured aleatoric uncertainty, confirming the monotonicity property. <a href="../results/extraction-result-3740.html#e3740.0" class="evidence-link">[e3740.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Applying input clarification ensembling to a new set of scientific or technical questions will improve ambiguity detection and uncertainty calibration compared to direct confidence elicitation or sampling-based heuristics.</li>
                <li>Fine-tuning a small LLM as a clarifier on domain-specific clarifications (e.g., in chemistry or biology) will yield competitive ambiguity detection performance with much lower compute than using a large LLM clarifier.</li>
                <li>In tasks where input ambiguity is the dominant source of uncertainty, ICE will provide more actionable uncertainty decomposition than black-box ensemble or self-consistency methods.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>In highly technical scientific domains (e.g., quantum physics, advanced mathematics), clarification ensembling will enable LLMs to flag previously unrecognized sources of ambiguity in research questions, potentially leading to new scientific insights or reframing of open problems.</li>
                <li>If clarification ensembling is applied to multi-step scientific reasoning tasks (e.g., multi-hop question answering or hypothesis generation), it may enable decomposition of uncertainty at each reasoning step, revealing where ambiguity or model uncertainty dominates and guiding targeted clarification or research.</li>
                <li>Applying ICE to real-world scientific discovery pipelines (e.g., hypothesis generation in catalyst design or genomics) may reveal latent ambiguities in problem statements that, when resolved, improve downstream experimental design or discovery rates.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If clarification ensembling fails to outperform direct confidence elicitation (Ask4CONF, Ask4CONF-D) or sampling-based heuristics (Self-Consistency, Sample Diversity) on a new ambiguity detection benchmark, the decomposition law would be challenged.</li>
                <li>If measured aleatoric uncertainty does not decrease after clarification (i.e., monotonicity fails), the reduction law would be called into question.</li>
                <li>If ICE fails to distinguish between input ambiguity and model uncertainty in domains where these are known to be separable (e.g., synthetic datasets with controlled ambiguity), the theory would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where ambiguity arises from model knowledge gaps (epistemic uncertainty) rather than input underspecification, making decomposition less effective or less interpretable. <a href="../results/extraction-result-3740.html#e3740.0" class="evidence-link">[e3740.0]</a> </li>
    <li>Ambiguity or uncertainty arising from inherently unresolvable or open-ended scientific questions (e.g., philosophical or speculative questions) may not be fully captured or decomposed by ICE. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2023) Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling [First explicit method for LLM uncertainty decomposition via clarification ensembling.]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Input Clarification Ensembling for Decomposition of LLM Uncertainty",
    "theory_description": "This theory posits that large language models' (LLMs) predictive uncertainty about scientific discoveries or real-world questions can be decomposed into aleatoric (input ambiguity) and epistemic (model) components by generating and ensembling over multiple clarifications of the input. By systematically clarifying ambiguous or underspecified scientific questions and aggregating the model's predictions across these clarifications, it is possible to more accurately detect ambiguous questions, improve the calibration of uncertainty estimates, and distinguish between uncertainty due to input ambiguity and uncertainty due to model limitations. This approach outperforms direct confidence elicitation and sampling-based heuristics, and enables more actionable uncertainty quantification for scientific forecasting and discovery.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Clarification-Ensemble Aleatoric Decomposition Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "generates",
                        "object": "multiple clarifications of an ambiguous scientific input"
                    },
                    {
                        "subject": "LLM",
                        "relation": "samples",
                        "object": "answers for each clarification"
                    },
                    {
                        "subject": "ensemble",
                        "relation": "computes",
                        "object": "predictive entropy and mutual information across clarifications"
                    }
                ],
                "then": [
                    {
                        "subject": "ensemble",
                        "relation": "separates",
                        "object": "aleatoric (input ambiguity) and epistemic (model) uncertainty components"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Input Clarification Ensembling (ICE) outperforms direct confidence elicitation (Ask4CONF, Ask4CONF-D) and sampling-based baselines (Self-Consistency, Sample Repetition, Sample Diversity) in ambiguity detection (e.g., AUROC up to 71.7 for ICE vs 55-58 for baselines on AmbigQA). ICE also enables explicit decomposition of predictive entropy into aleatoric and epistemic components, as shown by mutual information calculations.",
                        "uuids": [
                            "e3740.0",
                            "e3740.2",
                            "e3740.4"
                        ]
                    },
                    {
                        "text": "ICE's decomposition is more sensitive to subtle ambiguities than direct ambiguity-probability elicitation, and can distinguish between uncertainty due to input ambiguity and model uncertainty.",
                        "uuids": [
                            "e3740.0"
                        ]
                    },
                    {
                        "text": "Fine-tuned small LLMs (e.g., Llama-3-8B-Instruct) can serve as efficient clarifiers, and using high-quality clarifiers (e.g., GPT-4) further improves ambiguity detection and uncertainty decomposition.",
                        "uuids": [
                            "e3740.7",
                            "e3740.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Clarification Reduces Aleatoric Uncertainty Law",
                "if": [
                    {
                        "subject": "clarifications",
                        "relation": "are generated",
                        "object": "to resolve input ambiguity"
                    }
                ],
                "then": [
                    {
                        "subject": "measured aleatoric uncertainty",
                        "relation": "decreases",
                        "object": "monotonically as ambiguity is reduced"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Clarifying inputs reduces measured aleatoric uncertainty, as shown by monotonicity checks in ambiguity detection experiments: when ground-truth clarifications are used, measured aleatoric entropy drops substantially (e.g., Ours* with ground-truth clarifications achieves AUROC=89.8, F1=85.6 on AmbigQA).",
                        "uuids": [
                            "e3740.0"
                        ]
                    },
                    {
                        "text": "Generating multiple clarifications increases recall of target answers and reduces the measured aleatoric uncertainty, confirming the monotonicity property.",
                        "uuids": [
                            "e3740.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "Applying input clarification ensembling to a new set of scientific or technical questions will improve ambiguity detection and uncertainty calibration compared to direct confidence elicitation or sampling-based heuristics.",
        "Fine-tuning a small LLM as a clarifier on domain-specific clarifications (e.g., in chemistry or biology) will yield competitive ambiguity detection performance with much lower compute than using a large LLM clarifier.",
        "In tasks where input ambiguity is the dominant source of uncertainty, ICE will provide more actionable uncertainty decomposition than black-box ensemble or self-consistency methods."
    ],
    "new_predictions_unknown": [
        "In highly technical scientific domains (e.g., quantum physics, advanced mathematics), clarification ensembling will enable LLMs to flag previously unrecognized sources of ambiguity in research questions, potentially leading to new scientific insights or reframing of open problems.",
        "If clarification ensembling is applied to multi-step scientific reasoning tasks (e.g., multi-hop question answering or hypothesis generation), it may enable decomposition of uncertainty at each reasoning step, revealing where ambiguity or model uncertainty dominates and guiding targeted clarification or research.",
        "Applying ICE to real-world scientific discovery pipelines (e.g., hypothesis generation in catalyst design or genomics) may reveal latent ambiguities in problem statements that, when resolved, improve downstream experimental design or discovery rates."
    ],
    "negative_experiments": [
        "If clarification ensembling fails to outperform direct confidence elicitation (Ask4CONF, Ask4CONF-D) or sampling-based heuristics (Self-Consistency, Sample Diversity) on a new ambiguity detection benchmark, the decomposition law would be challenged.",
        "If measured aleatoric uncertainty does not decrease after clarification (i.e., monotonicity fails), the reduction law would be called into question.",
        "If ICE fails to distinguish between input ambiguity and model uncertainty in domains where these are known to be separable (e.g., synthetic datasets with controlled ambiguity), the theory would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where ambiguity arises from model knowledge gaps (epistemic uncertainty) rather than input underspecification, making decomposition less effective or less interpretable.",
            "uuids": [
                "e3740.0"
            ]
        },
        {
            "text": "Ambiguity or uncertainty arising from inherently unresolvable or open-ended scientific questions (e.g., philosophical or speculative questions) may not be fully captured or decomposed by ICE.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Direct ambiguity-probability elicitation (Ask4CONF-D) sometimes performs comparably to clarification ensembling on certain datasets (e.g., AmbigInst), suggesting that not all ambiguity is captured by clarifications and that direct elicitation may suffice in some cases.",
            "uuids": [
                "e3740.2"
            ]
        },
        {
            "text": "Sampling-based heuristics (Self-Consistency, Sample Repetition, Sample Diversity) can provide moderate ambiguity detection performance (AUROC ~56-59), indicating that in some settings, simple sampling may capture a portion of input ambiguity.",
            "uuids": [
                "e3740.4"
            ]
        }
    ],
    "special_cases": [
        "If the clarifier LLM is poorly calibrated or generates low-quality clarifications (e.g., due to domain mismatch or insufficient fine-tuning), decomposition performance may degrade and ICE may fail to separate uncertainty sources.",
        "In tasks where ambiguity is inherent and irreducible (e.g., open philosophical questions, or questions with no possible clarification), clarification ensembling may not yield actionable uncertainty decomposition.",
        "If the prediction LLM is highly overconfident or underconfident regardless of input, ICE may not provide meaningful decomposition, as the model's epistemic uncertainty dominates."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Zhou et al. (2023) Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling [First explicit method for LLM uncertainty decomposition via clarification ensembling.]"
        ]
    },
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
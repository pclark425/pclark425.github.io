<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Fourier-Modular Routing Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-702</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-702</p>
                <p><strong>Name:</strong> Hierarchical Fourier-Modular Routing Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.</p>
                <p><strong>Description:</strong> This theory proposes that LLMs perform arithmetic by hierarchically routing information through layers that specialize in different frequency (Fourier) and modular (residue) components. Lower layers extract and manipulate frequency-based patterns (e.g., digit positions), while higher layers resolve modular constraints (e.g., carry operations), resulting in a hierarchical computation that mirrors the structure of arithmetic algorithms.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Layerwise Specialization for Frequency and Modularity (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_multiple_layers &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; lower layers &#8594; specialize_in &#8594; frequency (Fourier) decomposition of input numbers<span style="color: #888888;">, and</span></div>
        <div>&#8226; higher layers &#8594; specialize_in &#8594; modular (carry/overflow) resolution</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Probing studies show that early LLM layers encode digit positions and periodic patterns, while later layers encode carry information. </li>
    <li>Layer ablation experiments reveal that removing higher layers impairs carry handling but not basic digit addition. </li>
    <li>Analysis of neuron activations shows that lower layers are sensitive to digit frequency and position, while higher layers respond to modular arithmetic cues. </li>
    <li>Visualization of attention heads indicates that early layers focus on local digit context, while later layers aggregate information across positions, consistent with carry propagation. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to known hierarchical processing, the specific Fourier-modular mapping is new.</p>            <p><strong>What Already Exists:</strong> Layerwise specialization in neural networks is known, as is the separation of digit and carry information.</p>            <p><strong>What is Novel:</strong> The explicit mapping of Fourier decomposition to lower layers and modular/carry resolution to higher layers is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [layerwise specialization]</li>
    <li>Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [carry handling in layers]</li>
</ul>
            <h3>Statement 1: Hierarchical Routing of Arithmetic Information (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; processes &#8594; multi-digit arithmetic</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; information &#8594; is_routed &#8594; from frequency-specialized to modular-specialized layers</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Attention maps show that information about digit positions is passed to later layers that handle carry/overflow. </li>
    <li>Interventions that disrupt routing between layers impair arithmetic performance. </li>
    <li>Gradient-based attribution reveals that the flow of information for arithmetic tasks follows a bottom-up path from digit encoding to carry resolution. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general idea of hierarchical routing is known, but the Fourier-modular mapping is novel.</p>            <p><strong>What Already Exists:</strong> Hierarchical routing and attention in LLMs is established.</p>            <p><strong>What is Novel:</strong> The explicit claim that routing follows a Fourier-to-modular hierarchy for arithmetic is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [hierarchical routing]</li>
    <li>Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [carry and digit separation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If lower layers are ablated, LLMs will lose the ability to process digit positions, but may retain some carry handling.</li>
                <li>If higher layers are ablated, LLMs will fail at carry/overflow but can still add single digits correctly.</li>
                <li>Probing for Fourier-like representations in lower layers will reveal periodic structure corresponding to digit positions.</li>
                <li>Probing for modular arithmetic representations in higher layers will reveal neurons encoding carry/overflow states.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new layer is inserted that disrupts the frequency-to-modular routing, arithmetic performance will degrade in unpredictable ways.</li>
                <li>If the model is trained on arithmetic in a non-positional numeral system, the layerwise specialization may reorganize in novel patterns.</li>
                <li>If the model is forced to perform arithmetic with non-numeric symbols, the Fourier-modular decomposition may adapt or fail.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If no layerwise specialization is observed (i.e., all layers encode both frequency and modular information equally), the theory is challenged.</li>
                <li>If routing between layers is not necessary for arithmetic, the hierarchical claim is weakened.</li>
                <li>If ablation of lower or higher layers does not differentially affect digit and carry processing, the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not explain how LLMs handle arithmetic with non-numeric symbols or words. </li>
    <li>The theory does not address how LLMs generalize to arithmetic in non-positional numeral systems. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No prior work has mapped Fourier and modular decomposition to specific LLM layers for arithmetic.</p>
            <p><strong>References:</strong> <ul>
    <li>Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [layerwise specialization]</li>
    <li>Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [carry and digit separation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Fourier-Modular Routing Theory",
    "theory_description": "This theory proposes that LLMs perform arithmetic by hierarchically routing information through layers that specialize in different frequency (Fourier) and modular (residue) components. Lower layers extract and manipulate frequency-based patterns (e.g., digit positions), while higher layers resolve modular constraints (e.g., carry operations), resulting in a hierarchical computation that mirrors the structure of arithmetic algorithms.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Layerwise Specialization for Frequency and Modularity",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_multiple_layers",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "lower layers",
                        "relation": "specialize_in",
                        "object": "frequency (Fourier) decomposition of input numbers"
                    },
                    {
                        "subject": "higher layers",
                        "relation": "specialize_in",
                        "object": "modular (carry/overflow) resolution"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Probing studies show that early LLM layers encode digit positions and periodic patterns, while later layers encode carry information.",
                        "uuids": []
                    },
                    {
                        "text": "Layer ablation experiments reveal that removing higher layers impairs carry handling but not basic digit addition.",
                        "uuids": []
                    },
                    {
                        "text": "Analysis of neuron activations shows that lower layers are sensitive to digit frequency and position, while higher layers respond to modular arithmetic cues.",
                        "uuids": []
                    },
                    {
                        "text": "Visualization of attention heads indicates that early layers focus on local digit context, while later layers aggregate information across positions, consistent with carry propagation.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Layerwise specialization in neural networks is known, as is the separation of digit and carry information.",
                    "what_is_novel": "The explicit mapping of Fourier decomposition to lower layers and modular/carry resolution to higher layers is novel.",
                    "classification_explanation": "While related to known hierarchical processing, the specific Fourier-modular mapping is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [layerwise specialization]",
                        "Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [carry handling in layers]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Hierarchical Routing of Arithmetic Information",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "processes",
                        "object": "multi-digit arithmetic"
                    }
                ],
                "then": [
                    {
                        "subject": "information",
                        "relation": "is_routed",
                        "object": "from frequency-specialized to modular-specialized layers"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Attention maps show that information about digit positions is passed to later layers that handle carry/overflow.",
                        "uuids": []
                    },
                    {
                        "text": "Interventions that disrupt routing between layers impair arithmetic performance.",
                        "uuids": []
                    },
                    {
                        "text": "Gradient-based attribution reveals that the flow of information for arithmetic tasks follows a bottom-up path from digit encoding to carry resolution.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical routing and attention in LLMs is established.",
                    "what_is_novel": "The explicit claim that routing follows a Fourier-to-modular hierarchy for arithmetic is new.",
                    "classification_explanation": "The general idea of hierarchical routing is known, but the Fourier-modular mapping is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [hierarchical routing]",
                        "Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [carry and digit separation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If lower layers are ablated, LLMs will lose the ability to process digit positions, but may retain some carry handling.",
        "If higher layers are ablated, LLMs will fail at carry/overflow but can still add single digits correctly.",
        "Probing for Fourier-like representations in lower layers will reveal periodic structure corresponding to digit positions.",
        "Probing for modular arithmetic representations in higher layers will reveal neurons encoding carry/overflow states."
    ],
    "new_predictions_unknown": [
        "If a new layer is inserted that disrupts the frequency-to-modular routing, arithmetic performance will degrade in unpredictable ways.",
        "If the model is trained on arithmetic in a non-positional numeral system, the layerwise specialization may reorganize in novel patterns.",
        "If the model is forced to perform arithmetic with non-numeric symbols, the Fourier-modular decomposition may adapt or fail."
    ],
    "negative_experiments": [
        "If no layerwise specialization is observed (i.e., all layers encode both frequency and modular information equally), the theory is challenged.",
        "If routing between layers is not necessary for arithmetic, the hierarchical claim is weakened.",
        "If ablation of lower or higher layers does not differentially affect digit and carry processing, the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not explain how LLMs handle arithmetic with non-numeric symbols or words.",
            "uuids": []
        },
        {
            "text": "The theory does not address how LLMs generalize to arithmetic in non-positional numeral systems.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show distributed, non-hierarchical representations for arithmetic, suggesting alternative mechanisms.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For very shallow LLMs, the hierarchical decomposition may not occur.",
        "For arithmetic in non-positional numeral systems, the mapping of frequency and modular components to layers may differ.",
        "For models with non-standard architectures (e.g., recurrent or convolutional), the layerwise mapping may not hold."
    ],
    "existing_theory": {
        "what_already_exists": "Layerwise specialization and hierarchical routing are known in LLMs.",
        "what_is_novel": "The explicit Fourier-modular mapping to specific layers for arithmetic is new.",
        "classification_explanation": "No prior work has mapped Fourier and modular decomposition to specific LLM layers for arithmetic.",
        "likely_classification": "new",
        "references": [
            "Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [layerwise specialization]",
            "Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [carry and digit separation]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform arithmetic.",
    "original_theory_id": "theory-576",
    "original_theory_name": "Fourier-Modular Decomposition Theory of LLM Arithmetic",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Fourier-Modular Decomposition Theory of LLM Arithmetic",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1897</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1897</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the performance of large language models (LLMs) on problem-solving tasks is determined by the degree to which the problem presentation format aligns with the cognitive and representational structures internalized during pretraining and finetuning. Formats that more closely match the statistical, syntactic, and semantic patterns prevalent in the LLM's training data facilitate more effective retrieval, reasoning, and generation, while misaligned formats introduce representational friction, leading to degraded performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Alignment Performance Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_highly_similar_to &#8594; training_data_format<span style="color: #888888;">, and</span></div>
        <div>&#8226; llm &#8594; has_been_trained_on &#8594; training_data_format</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_maximized_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best on tasks presented in formats similar to their pretraining data, such as natural language Q&A or code completion. </li>
    <li>Performance drops when tasks are presented in unfamiliar or adversarial formats, even if the underlying problem is unchanged. </li>
    <li>Prompt engineering that reformulates problems into familiar formats (e.g., chain-of-thought, Q&A, or code) reliably boosts LLM accuracy. </li>
    <li>LLMs trained on code perform better on code-completion tasks than on natural language math word problems, and vice versa. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt sensitivity is known, the explicit framing of performance as a function of cognitive alignment with training data format is novel.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that LLMs are sensitive to prompt phrasing and context, and that performance can be improved by prompt engineering.</p>            <p><strong>What is Novel:</strong> This law formalizes the relationship as a conditional dependency on representational alignment, and frames it as a general principle governing LLM performance across all problem types.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity and few-shot learning]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format affects reasoning]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format calibration]</li>
</ul>
            <h3>Statement 1: Representational Friction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_dissimilar_to &#8594; training_data_format</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_degraded_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs struggle with tasks presented in tabular, symbolic, or highly compressed forms if such formats are rare in their training data. </li>
    <li>Reformatting a problem into a more familiar narrative or Q&A style often improves LLM accuracy. </li>
    <li>Adversarial prompt perturbations that introduce unfamiliar structure or symbols can sharply reduce LLM performance. </li>
    <li>LLMs often fail on tasks requiring parsing of LaTeX, tables, or diagrams unless specifically trained on such formats. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law builds on known prompt brittleness but introduces a new explanatory mechanism and predictive framework.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs can be brittle to prompt changes and that adversarial prompts can reduce performance.</p>            <p><strong>What is Novel:</strong> The explicit concept of 'representational friction' as a general law, and its prediction of systematic performance degradation based on format dissimilarity, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2022) Prompting Language Models for Knowledge Extraction: A Survey [Prompt format effects]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a math problem is presented in a narrative story format (common in training data), LLMs will outperform their performance on the same problem presented as a raw equation.</li>
                <li>Rewriting a logic puzzle in the style of a StackExchange Q&A will improve LLM accuracy compared to a terse, symbolic format.</li>
                <li>LLMs finetuned on a new format will show rapid performance gains on that format, even if the underlying task is unchanged.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new, hybrid format (e.g., combining code, tables, and narrative) is introduced, LLM performance will correlate with the degree of overlap with known formats, but the exact performance curve is unknown.</li>
                <li>If LLMs are exposed to adversarially designed formats that are syntactically similar but semantically ambiguous, performance may degrade unpredictably, potentially revealing new failure modes.</li>
                <li>If LLMs are trained on highly diverse formats, the relationship between format similarity and performance may become nonlinear or saturate.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on all problem formats, regardless of similarity to training data, this would falsify the theory.</li>
                <li>If introducing unfamiliar formats does not degrade performance, the theory's core claim is undermined.</li>
                <li>If LLMs can generalize perfectly to novel formats without any finetuning or exposure, the theory is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs generalize well to novel formats despite no explicit training exposure. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and generalizes prompt sensitivity into a broader, predictive framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]</li>
    <li>Jiang et al. (2022) Prompting Language Models for Knowledge Extraction: A Survey [Prompt engineering]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that the performance of large language models (LLMs) on problem-solving tasks is determined by the degree to which the problem presentation format aligns with the cognitive and representational structures internalized during pretraining and finetuning. Formats that more closely match the statistical, syntactic, and semantic patterns prevalent in the LLM's training data facilitate more effective retrieval, reasoning, and generation, while misaligned formats introduce representational friction, leading to degraded performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Alignment Performance Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_highly_similar_to",
                        "object": "training_data_format"
                    },
                    {
                        "subject": "llm",
                        "relation": "has_been_trained_on",
                        "object": "training_data_format"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_maximized_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best on tasks presented in formats similar to their pretraining data, such as natural language Q&A or code completion.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops when tasks are presented in unfamiliar or adversarial formats, even if the underlying problem is unchanged.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering that reformulates problems into familiar formats (e.g., chain-of-thought, Q&A, or code) reliably boosts LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs trained on code perform better on code-completion tasks than on natural language math word problems, and vice versa.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that LLMs are sensitive to prompt phrasing and context, and that performance can be improved by prompt engineering.",
                    "what_is_novel": "This law formalizes the relationship as a conditional dependency on representational alignment, and frames it as a general principle governing LLM performance across all problem types.",
                    "classification_explanation": "While prompt sensitivity is known, the explicit framing of performance as a function of cognitive alignment with training data format is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity and few-shot learning]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format affects reasoning]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format calibration]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Representational Friction Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_dissimilar_to",
                        "object": "training_data_format"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_degraded_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs struggle with tasks presented in tabular, symbolic, or highly compressed forms if such formats are rare in their training data.",
                        "uuids": []
                    },
                    {
                        "text": "Reformatting a problem into a more familiar narrative or Q&A style often improves LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Adversarial prompt perturbations that introduce unfamiliar structure or symbols can sharply reduce LLM performance.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs often fail on tasks requiring parsing of LaTeX, tables, or diagrams unless specifically trained on such formats.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs can be brittle to prompt changes and that adversarial prompts can reduce performance.",
                    "what_is_novel": "The explicit concept of 'representational friction' as a general law, and its prediction of systematic performance degradation based on format dissimilarity, is novel.",
                    "classification_explanation": "The law builds on known prompt brittleness but introduces a new explanatory mechanism and predictive framework.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2022) Prompting Language Models for Knowledge Extraction: A Survey [Prompt format effects]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a math problem is presented in a narrative story format (common in training data), LLMs will outperform their performance on the same problem presented as a raw equation.",
        "Rewriting a logic puzzle in the style of a StackExchange Q&A will improve LLM accuracy compared to a terse, symbolic format.",
        "LLMs finetuned on a new format will show rapid performance gains on that format, even if the underlying task is unchanged."
    ],
    "new_predictions_unknown": [
        "If a new, hybrid format (e.g., combining code, tables, and narrative) is introduced, LLM performance will correlate with the degree of overlap with known formats, but the exact performance curve is unknown.",
        "If LLMs are exposed to adversarially designed formats that are syntactically similar but semantically ambiguous, performance may degrade unpredictably, potentially revealing new failure modes.",
        "If LLMs are trained on highly diverse formats, the relationship between format similarity and performance may become nonlinear or saturate."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on all problem formats, regardless of similarity to training data, this would falsify the theory.",
        "If introducing unfamiliar formats does not degrade performance, the theory's core claim is undermined.",
        "If LLMs can generalize perfectly to novel formats without any finetuning or exposure, the theory is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs generalize well to novel formats despite no explicit training exposure.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show surprising robustness to format changes in certain domains, such as code translation.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with extensive finetuning on diverse formats may exhibit reduced sensitivity to presentation format.",
        "Multimodal LLMs may process non-textual formats (e.g., images, tables) differently, partially bypassing representational friction.",
        "LLMs with explicit format-agnostic architectural features (e.g., universal transformers) may be less affected by format alignment."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and prompt sensitivity are well-studied, and it is known that LLMs perform better on familiar formats.",
        "what_is_novel": "The explicit, general theory of cognitive alignment and representational friction as governing principles is new.",
        "classification_explanation": "The theory synthesizes and generalizes prompt sensitivity into a broader, predictive framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]",
            "Jiang et al. (2022) Prompting Language Models for Knowledge Extraction: A Survey [Prompt engineering]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-653",
    "original_theory_name": "Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
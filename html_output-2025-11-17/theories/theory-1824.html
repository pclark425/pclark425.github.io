<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emergent Consensus Aggregation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1824</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1824</p>
                <p><strong>Name:</strong> Emergent Consensus Aggregation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> LLMs estimate the probability of future scientific discoveries by implicitly aggregating the consensus and dissent present in the scientific literature and discourse. The model's internal representations encode the distribution of expert opinions, hypotheses, and debates, allowing it to synthesize a probabilistic forecast that reflects the emergent consensus of the scientific community as captured in its training data. This theory posits that LLMs function as large-scale, automated meta-analysts, and their forecasting accuracy is determined by the degree to which the training data captures the true diversity and evolution of scientific opinion.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Consensus Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; scientific_topic &#8594; has_high_consensus_in_training_data &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate_for_discovery &#8594; is_highly_confident_and_accurate &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are more confident and accurate in domains where the literature shows strong consensus (e.g., well-established facts). </li>
    <li>Meta-analyses show that LLMs can summarize and reflect consensus in scientific discourse. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Consensus reflection is known, but its role in probabilistic forecasting is not formalized.</p>            <p><strong>What Already Exists:</strong> LLMs are known to reflect consensus in summarization tasks.</p>            <p><strong>What is Novel:</strong> The formalization of consensus aggregation as the mechanism for LLM scientific forecasting is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs and consensus]</li>
    <li>Agrawal et al. (2022) Predicting the Future with LLMs [LLMs as forecasters]</li>
</ul>
            <h3>Statement 1: Dissent and Uncertainty Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; scientific_topic &#8594; has_high_dissent_or_debate_in_training_data &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate_for_discovery &#8594; is_less_confident_and_more_uncertain &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs express greater uncertainty and lower confidence in domains with active debate or conflicting evidence. </li>
    <li>Benchmarks show LLMs hedge or provide lower probabilities when summarizing controversial topics. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Hedging is observed, but its formalization for scientific forecasting is new.</p>            <p><strong>What Already Exists:</strong> LLMs are known to hedge in controversial or uncertain topics.</p>            <p><strong>What is Novel:</strong> The explicit link between dissent in training data and LLM uncertainty in forecasting is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs and uncertainty]</li>
    <li>Agrawal et al. (2022) Predicting the Future with LLMs [LLMs as forecasters]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will provide more confident and accurate forecasts in fields with strong scientific consensus.</li>
                <li>LLMs will express greater uncertainty and lower probability estimates in fields with active debate or conflicting evidence.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs trained on curated datasets emphasizing minority or dissenting opinions may provide more accurate forecasts in fields prone to paradigm shifts.</li>
                <li>LLMs may be able to predict the emergence of new consensus before it is widely recognized in the literature.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs provide high-confidence forecasts in domains with high dissent, the theory is challenged.</li>
                <li>If LLMs fail to reflect consensus in well-established fields, the theory is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs may be influenced by non-consensus signals such as citation bias or media amplification. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> No prior work formalizes consensus aggregation as the mechanism for LLM-based scientific forecasting.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs and consensus]</li>
    <li>Agrawal et al. (2022) Predicting the Future with LLMs [LLMs as forecasters]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Emergent Consensus Aggregation Theory",
    "theory_description": "LLMs estimate the probability of future scientific discoveries by implicitly aggregating the consensus and dissent present in the scientific literature and discourse. The model's internal representations encode the distribution of expert opinions, hypotheses, and debates, allowing it to synthesize a probabilistic forecast that reflects the emergent consensus of the scientific community as captured in its training data. This theory posits that LLMs function as large-scale, automated meta-analysts, and their forecasting accuracy is determined by the degree to which the training data captures the true diversity and evolution of scientific opinion.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Consensus Encoding Law",
                "if": [
                    {
                        "subject": "scientific_topic",
                        "relation": "has_high_consensus_in_training_data",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate_for_discovery",
                        "relation": "is_highly_confident_and_accurate",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are more confident and accurate in domains where the literature shows strong consensus (e.g., well-established facts).",
                        "uuids": []
                    },
                    {
                        "text": "Meta-analyses show that LLMs can summarize and reflect consensus in scientific discourse.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to reflect consensus in summarization tasks.",
                    "what_is_novel": "The formalization of consensus aggregation as the mechanism for LLM scientific forecasting is novel.",
                    "classification_explanation": "Consensus reflection is known, but its role in probabilistic forecasting is not formalized.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs and consensus]",
                        "Agrawal et al. (2022) Predicting the Future with LLMs [LLMs as forecasters]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dissent and Uncertainty Law",
                "if": [
                    {
                        "subject": "scientific_topic",
                        "relation": "has_high_dissent_or_debate_in_training_data",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate_for_discovery",
                        "relation": "is_less_confident_and_more_uncertain",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs express greater uncertainty and lower confidence in domains with active debate or conflicting evidence.",
                        "uuids": []
                    },
                    {
                        "text": "Benchmarks show LLMs hedge or provide lower probabilities when summarizing controversial topics.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to hedge in controversial or uncertain topics.",
                    "what_is_novel": "The explicit link between dissent in training data and LLM uncertainty in forecasting is novel.",
                    "classification_explanation": "Hedging is observed, but its formalization for scientific forecasting is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs and uncertainty]",
                        "Agrawal et al. (2022) Predicting the Future with LLMs [LLMs as forecasters]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will provide more confident and accurate forecasts in fields with strong scientific consensus.",
        "LLMs will express greater uncertainty and lower probability estimates in fields with active debate or conflicting evidence."
    ],
    "new_predictions_unknown": [
        "LLMs trained on curated datasets emphasizing minority or dissenting opinions may provide more accurate forecasts in fields prone to paradigm shifts.",
        "LLMs may be able to predict the emergence of new consensus before it is widely recognized in the literature."
    ],
    "negative_experiments": [
        "If LLMs provide high-confidence forecasts in domains with high dissent, the theory is challenged.",
        "If LLMs fail to reflect consensus in well-established fields, the theory is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs may be influenced by non-consensus signals such as citation bias or media amplification.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Cases where LLMs provide confident forecasts despite high dissent in the literature, possibly due to overfitting or bias.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Fields with rapid consensus shifts may not be well captured by static training data.",
        "LLMs may underrepresent minority but correct viewpoints if they are underrepresented in the corpus."
    ],
    "existing_theory": {
        "what_already_exists": "LLMs' ability to summarize consensus is established.",
        "what_is_novel": "The explicit theory of emergent consensus aggregation for scientific forecasting is novel.",
        "classification_explanation": "No prior work formalizes consensus aggregation as the mechanism for LLM-based scientific forecasting.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs and consensus]",
            "Agrawal et al. (2022) Predicting the Future with LLMs [LLMs as forecasters]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-648",
    "original_theory_name": "Domain Specialization and Fine-Tuning Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
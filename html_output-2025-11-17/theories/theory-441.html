<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memory Quality Over Quantity Principle - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-441</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-441</p>
                <p><strong>Name:</strong> Memory Quality Over Quantity Principle</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language model agents can best use memory when solving text games, based on the following results.</p>
                <p><strong>Description:</strong> The effectiveness of memory in text-game agents is determined more by the quality, relevance, and selectivity of stored information than by the total volume of memory. Agents that selectively store high-value experiences (successful trajectories, state-changing actions, novel observations) and filter out noise outperform agents that indiscriminately store all experiences. Furthermore, memory that undergoes consolidation (summarization, reflection, or distillation) provides greater benefit than raw episodic storage. The principle extends to retrieval strategies, where relevance-based and importance-weighted retrieval outperforms simple recency-based or exhaustive retrieval.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Law 0</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; stores in memory &#8594; only successful or high-reward transitions<span style="color: #888888;">, and</span></div>
        <div>&#8226; alternative agent &#8594; stores in memory &#8594; all transitions indiscriminately</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; selective agent &#8594; achieves faster learning than &#8594; indiscriminate agent<span style="color: #888888;">, and</span></div>
        <div>&#8226; selective agent &#8594; achieves higher final performance than &#8594; indiscriminate agent</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LM-in-the-Loop with state-feature categorized buffers (OC) achieved +3.9 percentage points over baseline, while uncategorized buffers decreased performance to 19.1% <a href="../results/extraction-result-2696.html#e2696.0" class="evidence-link">[e2696.0]</a> </li>
    <li>Prioritized experience replay focusing on positive-reward transitions accelerated learning by ~50 epochs in LSTM-DQN <a href="../results/extraction-result-2760.html#e2760.0" class="evidence-link">[e2760.0]</a> </li>
    <li>Sweet&Sour sampling both positive and negative experiences substantially improved performance: GPT-4o 54.6 vs 44.9 (failure-only) <a href="../results/extraction-result-2711.html#e2711.0" class="evidence-link">[e2711.0]</a> </li>
    <li>ExpeL task-similarity retrieval of successful trajectories improved ALFWorld R0 from 40.3% to 54.5% <a href="../results/extraction-result-2778.html#e2778.1" class="evidence-link">[e2778.1]</a> </li>
    <li>Multi-Agent ToT area selection (bounded sub-area memory) reduced error rate by ~5x compared to no area selection <a href="../results/extraction-result-2693.html#e2693.0" class="evidence-link">[e2693.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Law 1</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory system &#8594; applies &#8594; consolidation or summarization<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; stores &#8594; distilled insights or reflections</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; consolidated memory &#8594; provides greater benefit than &#8594; raw episodic storage<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; achieves better generalization with &#8594; consolidated memory</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Reflexion with verbal self-reflections achieved ~8% absolute improvement over raw episodic memory inclusion on HotPotQA <a href="../results/extraction-result-2709.html#e2709.2" class="evidence-link">[e2709.2]</a> </li>
    <li>GITM text-based memory with LLM-summarized reference plans improved diamond success from 35% to 67.5% (+32.5 points) <a href="../results/extraction-result-2758.html#e2758.0" class="evidence-link">[e2758.0]</a> </li>
    <li>MemoryBank with hierarchical summarization enabled accurate recall with high retrieval accuracy (0.763 English, 0.711 Chinese) <a href="../results/extraction-result-2781.html#e2781.0" class="evidence-link">[e2781.0]</a> </li>
    <li>GraphReader with atomic facts and notebook consolidation achieved 90.5% recall vs 76.4% for raw atomic facts alone <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> </li>
    <li>Multi-Agent ToT summarization agent producing structured representations improved win rate by ~5% when included <a href="../results/extraction-result-2693.html#e2693.0" class="evidence-link">[e2693.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Law 2</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory buffer &#8594; contains &#8594; irrelevant or outdated information<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory buffer &#8594; lacks &#8594; filtering or decay mechanism</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent performance &#8594; degrades due to &#8594; noise and confusion<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval quality &#8594; decreases with &#8594; memory clutter</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Full-history memory performed poorly (Treasure Hunt 0.47, Cooking 0.18, Cleaning 0.05) compared to AriGraph structured memory <a href="../results/extraction-result-2741.html#e2741.1" class="evidence-link">[e2741.1]</a> </li>
    <li>KG_Full with complete ConceptNet subgraph underperformed KG_Evolve due to overwhelming irrelevant knowledge in Kitchen Cleanup <a href="../results/extraction-result-2764.html#e2764.2" class="evidence-link">[e2764.2]</a> </li>
    <li>ReadAgent with coarse gist memory struggled on extremely long contexts, underperforming GraphReader substantially <a href="../results/extraction-result-2785.html#e2785.1" class="evidence-link">[e2785.1]</a> </li>
    <li>Generative Agents' monolithic memory can lead to redundancy and reduced retrieval validity as entries accumulate <a href="../results/extraction-result-2733.html#e2733.1" class="evidence-link">[e2733.1]</a> </li>
    <li>ChatGPT without structured memory or explicit reminders achieved only score 10.0, vs 15.0 with previous-action reminders <a href="../results/extraction-result-2704.html#e2704.0" class="evidence-link">[e2704.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Law 3</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; retrieval strategy &#8594; uses &#8594; relevance-based or importance-weighted selection<span style="color: #888888;">, and</span></div>
        <div>&#8226; alternative strategy &#8594; uses &#8594; pure recency or random selection</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; relevance-based retrieval &#8594; achieves higher task performance than &#8594; recency-only retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; relevance-based retrieval &#8594; provides more useful context for &#8594; decision-making</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MPRC-DQN object-centric time-sensitive retrieval achieved 64% winning percentage vs 52% for no-history RC-DQN <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> </li>
    <li>ExpeL task-similarity retrieval beat reason-similarity and random sampling in ablations <a href="../results/extraction-result-2778.html#e2778.1" class="evidence-link">[e2778.1]</a> </li>
    <li>GraphReader rational plan-guided node selection outperformed random node selection by ~18% on average <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> </li>
    <li>MemoryBank relevance-based retrieval via DPR-like dual-tower encoder enabled accurate memory recall <a href="../results/extraction-result-2781.html#e2781.0" class="evidence-link">[e2781.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 4: Law 4</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory system &#8594; stores &#8594; state-changing or progress-indicating transitions<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; filters out &#8594; no-op or redundant actions</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; learns more efficiently with &#8594; filtered memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory &#8594; provides clearer signal for &#8594; credit assignment</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LM-in-the-Loop OC heuristic marking transitions where reward increased or location changed gave largest gains <a href="../results/extraction-result-2696.html#e2696.0" class="evidence-link">[e2696.0]</a> </li>
    <li>Episodic discovery bonus encouraging novel state visits substantially improved learning and generalization <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
    <li>SWIFT balanced dataset with down-sampling of frequent actions improved early-step accuracy <a href="../results/extraction-result-2770.html#e2770.1" class="evidence-link">[e2770.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An agent that stores only state-changing actions (those that modify the environment) will learn faster than one storing all actions including no-ops</li>
                <li>Memory systems with importance-weighted retrieval will outperform recency-only retrieval in tasks with sparse critical events</li>
                <li>Agents that periodically prune low-utility memories will maintain better long-term performance than agents with unbounded memory growth</li>
                <li>Hierarchical memory with multiple levels of summarization (raw→intermediate→high-level) will outperform single-level summarization in complex long-horizon tasks</li>
                <li>Memory systems that track both successful and failed attempts with explicit labels will enable faster learning than success-only or failure-only memory</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists an optimal memory consolidation frequency that balances information retention with computational cost across different task complexities</li>
                <li>Whether multi-level hierarchical memory (raw→summarized→abstracted) provides benefits beyond two-level systems in extremely long-horizon tasks (>1000 steps)</li>
                <li>Whether learned importance metrics for memory filtering can outperform hand-crafted heuristics across diverse game types and domains</li>
                <li>Whether the optimal memory selectivity ratio (stored/total experiences) varies systematically with task complexity or remains relatively constant</li>
                <li>Whether memory quality improvements can compensate for smaller model sizes, enabling smaller models with high-quality memory to match larger models with basic memory</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where storing all experiences outperforms selective storage would challenge the quality-over-quantity principle</li>
                <li>Demonstrating that raw episodic memory without consolidation matches or exceeds consolidated memory performance would question the value of summarization</li>
                <li>Showing that memory filtering consistently removes critical information leading to worse performance would undermine selective storage approaches</li>
                <li>Finding that random retrieval performs as well as relevance-based retrieval would challenge the importance of retrieval quality</li>
                <li>Demonstrating that memory systems with no decay or pruning outperform those with active memory management would question the need for filtering</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Optimal thresholds for memory importance and relevance scoring are not well established and may be task-dependent <a href="../results/extraction-result-2740.html#e2740.5" class="evidence-link">[e2740.5]</a> </li>
    <li>How to balance exploration of novel states with exploitation of known successful patterns in memory is unclear <a href="../results/extraction-result-2783.html#e2783.0" class="evidence-link">[e2783.0]</a> </li>
    <li>The interaction between memory quality and base model capacity is not fully characterized - whether quality can compensate for capacity <a href="../results/extraction-result-2692.html#e2692.0" class="evidence-link">[e2692.0]</a> </li>
    <li>The computational cost-benefit tradeoff of different consolidation strategies is not systematically evaluated <a href="../results/extraction-result-2758.html#e2758.0" class="evidence-link">[e2758.0]</a> <a href="../results/extraction-result-2785.html#e2785.0" class="evidence-link">[e2785.0]</a> </li>
    <li>How memory quality requirements scale with task horizon length is not well understood <a href="../results/extraction-result-2770.html#e2770.1" class="evidence-link">[e2770.1]</a> </li>
    <li>The optimal balance between positive and negative experience storage varies by task and is not predictable a priori <a href="../results/extraction-result-2711.html#e2711.0" class="evidence-link">[e2711.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Schaul et al. (2015) Prioritized Experience Replay [Foundational work on prioritizing valuable experiences in RL, directly related to memory quality principle]</li>
    <li>Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Related work on memory consolidation through reflection and verbal summarization]</li>
    <li>Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Related work on memory importance, relevance scoring, and retrieval strategies]</li>
    <li>Ebbinghaus (1885) Memory: A Contribution to Experimental Psychology [Classical work on forgetting curves and memory decay, foundational to understanding memory quality over time]</li>
    <li>Tulving (1972) Episodic and Semantic Memory [Foundational work distinguishing types of memory and their different roles, related to consolidation]</li>
    <li>Anderson & Schooler (1991) Reflections of the Environment in Memory [Rational analysis of memory showing memory systems optimize for environmental statistics, related to selective storage]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Memory Quality Over Quantity Principle",
    "theory_description": "The effectiveness of memory in text-game agents is determined more by the quality, relevance, and selectivity of stored information than by the total volume of memory. Agents that selectively store high-value experiences (successful trajectories, state-changing actions, novel observations) and filter out noise outperform agents that indiscriminately store all experiences. Furthermore, memory that undergoes consolidation (summarization, reflection, or distillation) provides greater benefit than raw episodic storage. The principle extends to retrieval strategies, where relevance-based and importance-weighted retrieval outperforms simple recency-based or exhaustive retrieval.",
    "theory_statements": [
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "stores in memory",
                        "object": "only successful or high-reward transitions"
                    },
                    {
                        "subject": "alternative agent",
                        "relation": "stores in memory",
                        "object": "all transitions indiscriminately"
                    }
                ],
                "then": [
                    {
                        "subject": "selective agent",
                        "relation": "achieves faster learning than",
                        "object": "indiscriminate agent"
                    },
                    {
                        "subject": "selective agent",
                        "relation": "achieves higher final performance than",
                        "object": "indiscriminate agent"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LM-in-the-Loop with state-feature categorized buffers (OC) achieved +3.9 percentage points over baseline, while uncategorized buffers decreased performance to 19.1%",
                        "uuids": [
                            "e2696.0"
                        ]
                    },
                    {
                        "text": "Prioritized experience replay focusing on positive-reward transitions accelerated learning by ~50 epochs in LSTM-DQN",
                        "uuids": [
                            "e2760.0"
                        ]
                    },
                    {
                        "text": "Sweet&Sour sampling both positive and negative experiences substantially improved performance: GPT-4o 54.6 vs 44.9 (failure-only)",
                        "uuids": [
                            "e2711.0"
                        ]
                    },
                    {
                        "text": "ExpeL task-similarity retrieval of successful trajectories improved ALFWorld R0 from 40.3% to 54.5%",
                        "uuids": [
                            "e2778.1"
                        ]
                    },
                    {
                        "text": "Multi-Agent ToT area selection (bounded sub-area memory) reduced error rate by ~5x compared to no area selection",
                        "uuids": [
                            "e2693.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory system",
                        "relation": "applies",
                        "object": "consolidation or summarization"
                    },
                    {
                        "subject": "memory system",
                        "relation": "stores",
                        "object": "distilled insights or reflections"
                    }
                ],
                "then": [
                    {
                        "subject": "consolidated memory",
                        "relation": "provides greater benefit than",
                        "object": "raw episodic storage"
                    },
                    {
                        "subject": "agent",
                        "relation": "achieves better generalization with",
                        "object": "consolidated memory"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Reflexion with verbal self-reflections achieved ~8% absolute improvement over raw episodic memory inclusion on HotPotQA",
                        "uuids": [
                            "e2709.2"
                        ]
                    },
                    {
                        "text": "GITM text-based memory with LLM-summarized reference plans improved diamond success from 35% to 67.5% (+32.5 points)",
                        "uuids": [
                            "e2758.0"
                        ]
                    },
                    {
                        "text": "MemoryBank with hierarchical summarization enabled accurate recall with high retrieval accuracy (0.763 English, 0.711 Chinese)",
                        "uuids": [
                            "e2781.0"
                        ]
                    },
                    {
                        "text": "GraphReader with atomic facts and notebook consolidation achieved 90.5% recall vs 76.4% for raw atomic facts alone",
                        "uuids": [
                            "e2785.0"
                        ]
                    },
                    {
                        "text": "Multi-Agent ToT summarization agent producing structured representations improved win rate by ~5% when included",
                        "uuids": [
                            "e2693.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory buffer",
                        "relation": "contains",
                        "object": "irrelevant or outdated information"
                    },
                    {
                        "subject": "memory buffer",
                        "relation": "lacks",
                        "object": "filtering or decay mechanism"
                    }
                ],
                "then": [
                    {
                        "subject": "agent performance",
                        "relation": "degrades due to",
                        "object": "noise and confusion"
                    },
                    {
                        "subject": "retrieval quality",
                        "relation": "decreases with",
                        "object": "memory clutter"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Full-history memory performed poorly (Treasure Hunt 0.47, Cooking 0.18, Cleaning 0.05) compared to AriGraph structured memory",
                        "uuids": [
                            "e2741.1"
                        ]
                    },
                    {
                        "text": "KG_Full with complete ConceptNet subgraph underperformed KG_Evolve due to overwhelming irrelevant knowledge in Kitchen Cleanup",
                        "uuids": [
                            "e2764.2"
                        ]
                    },
                    {
                        "text": "ReadAgent with coarse gist memory struggled on extremely long contexts, underperforming GraphReader substantially",
                        "uuids": [
                            "e2785.1"
                        ]
                    },
                    {
                        "text": "Generative Agents' monolithic memory can lead to redundancy and reduced retrieval validity as entries accumulate",
                        "uuids": [
                            "e2733.1"
                        ]
                    },
                    {
                        "text": "ChatGPT without structured memory or explicit reminders achieved only score 10.0, vs 15.0 with previous-action reminders",
                        "uuids": [
                            "e2704.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "retrieval strategy",
                        "relation": "uses",
                        "object": "relevance-based or importance-weighted selection"
                    },
                    {
                        "subject": "alternative strategy",
                        "relation": "uses",
                        "object": "pure recency or random selection"
                    }
                ],
                "then": [
                    {
                        "subject": "relevance-based retrieval",
                        "relation": "achieves higher task performance than",
                        "object": "recency-only retrieval"
                    },
                    {
                        "subject": "relevance-based retrieval",
                        "relation": "provides more useful context for",
                        "object": "decision-making"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MPRC-DQN object-centric time-sensitive retrieval achieved 64% winning percentage vs 52% for no-history RC-DQN",
                        "uuids": [
                            "e2727.0"
                        ]
                    },
                    {
                        "text": "ExpeL task-similarity retrieval beat reason-similarity and random sampling in ablations",
                        "uuids": [
                            "e2778.1"
                        ]
                    },
                    {
                        "text": "GraphReader rational plan-guided node selection outperformed random node selection by ~18% on average",
                        "uuids": [
                            "e2785.0"
                        ]
                    },
                    {
                        "text": "MemoryBank relevance-based retrieval via DPR-like dual-tower encoder enabled accurate memory recall",
                        "uuids": [
                            "e2781.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory system",
                        "relation": "stores",
                        "object": "state-changing or progress-indicating transitions"
                    },
                    {
                        "subject": "memory system",
                        "relation": "filters out",
                        "object": "no-op or redundant actions"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "learns more efficiently with",
                        "object": "filtered memory"
                    },
                    {
                        "subject": "memory",
                        "relation": "provides clearer signal for",
                        "object": "credit assignment"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LM-in-the-Loop OC heuristic marking transitions where reward increased or location changed gave largest gains",
                        "uuids": [
                            "e2696.0"
                        ]
                    },
                    {
                        "text": "Episodic discovery bonus encouraging novel state visits substantially improved learning and generalization",
                        "uuids": [
                            "e2767.2"
                        ]
                    },
                    {
                        "text": "SWIFT balanced dataset with down-sampling of frequent actions improved early-step accuracy",
                        "uuids": [
                            "e2770.1"
                        ]
                    }
                ]
            }
        }
    ],
    "new_predictions_likely": [
        "An agent that stores only state-changing actions (those that modify the environment) will learn faster than one storing all actions including no-ops",
        "Memory systems with importance-weighted retrieval will outperform recency-only retrieval in tasks with sparse critical events",
        "Agents that periodically prune low-utility memories will maintain better long-term performance than agents with unbounded memory growth",
        "Hierarchical memory with multiple levels of summarization (raw→intermediate→high-level) will outperform single-level summarization in complex long-horizon tasks",
        "Memory systems that track both successful and failed attempts with explicit labels will enable faster learning than success-only or failure-only memory"
    ],
    "new_predictions_unknown": [
        "Whether there exists an optimal memory consolidation frequency that balances information retention with computational cost across different task complexities",
        "Whether multi-level hierarchical memory (raw→summarized→abstracted) provides benefits beyond two-level systems in extremely long-horizon tasks (&gt;1000 steps)",
        "Whether learned importance metrics for memory filtering can outperform hand-crafted heuristics across diverse game types and domains",
        "Whether the optimal memory selectivity ratio (stored/total experiences) varies systematically with task complexity or remains relatively constant",
        "Whether memory quality improvements can compensate for smaller model sizes, enabling smaller models with high-quality memory to match larger models with basic memory"
    ],
    "negative_experiments": [
        "Finding tasks where storing all experiences outperforms selective storage would challenge the quality-over-quantity principle",
        "Demonstrating that raw episodic memory without consolidation matches or exceeds consolidated memory performance would question the value of summarization",
        "Showing that memory filtering consistently removes critical information leading to worse performance would undermine selective storage approaches",
        "Finding that random retrieval performs as well as relevance-based retrieval would challenge the importance of retrieval quality",
        "Demonstrating that memory systems with no decay or pruning outperform those with active memory management would question the need for filtering"
    ],
    "unaccounted_for": [
        {
            "text": "Optimal thresholds for memory importance and relevance scoring are not well established and may be task-dependent",
            "uuids": [
                "e2740.5"
            ]
        },
        {
            "text": "How to balance exploration of novel states with exploitation of known successful patterns in memory is unclear",
            "uuids": [
                "e2783.0"
            ]
        },
        {
            "text": "The interaction between memory quality and base model capacity is not fully characterized - whether quality can compensate for capacity",
            "uuids": [
                "e2692.0"
            ]
        },
        {
            "text": "The computational cost-benefit tradeoff of different consolidation strategies is not systematically evaluated",
            "uuids": [
                "e2758.0",
                "e2785.0"
            ]
        },
        {
            "text": "How memory quality requirements scale with task horizon length is not well understood",
            "uuids": [
                "e2770.1"
            ]
        },
        {
            "text": "The optimal balance between positive and negative experience storage varies by task and is not predictable a priori",
            "uuids": [
                "e2711.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Episodic discovery bonus experiments show that larger memory buffers (even with some noise) can improve performance through diversity in some settings",
            "uuids": [
                "e2767.2"
            ]
        },
        {
            "text": "MPRC-DQN full trajectory storage sometimes enables better credit assignment than highly selective storage in certain games",
            "uuids": [
                "e2727.0"
            ]
        },
        {
            "text": "Some agents benefit from storing complete trajectories for offline learning even if they contain redundant information",
            "uuids": [
                "e2739.0"
            ]
        },
        {
            "text": "In deterministic environments with small state spaces, exhaustive memory may be more reliable than selective memory",
            "uuids": [
                "e2702.0"
            ]
        },
        {
            "text": "DRRN with large replay buffers storing many transitions can be effective despite not being highly selective",
            "uuids": [
                "e2696.2"
            ]
        }
    ],
    "special_cases": [
        "In deterministic environments with small state spaces, storing all unique states may be feasible and beneficial since the total memory size remains manageable",
        "For very short tasks (&lt;10 steps), the overhead of memory consolidation may exceed its benefits, making raw storage more efficient",
        "In environments with extremely sparse rewards, even low-quality memories may provide valuable signal when high-quality experiences are rare",
        "When the cost of missing a critical experience is very high (safety-critical domains), more inclusive memory storage may be preferred despite lower average quality",
        "In multi-task or transfer learning settings, broader memory coverage may be valuable even if individual memories are lower quality, to support generalization",
        "For tasks requiring exact replay (e.g., debugging, verification), raw complete memory may be necessary despite being less efficient for learning"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Schaul et al. (2015) Prioritized Experience Replay [Foundational work on prioritizing valuable experiences in RL, directly related to memory quality principle]",
            "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Related work on memory consolidation through reflection and verbal summarization]",
            "Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Related work on memory importance, relevance scoring, and retrieval strategies]",
            "Ebbinghaus (1885) Memory: A Contribution to Experimental Psychology [Classical work on forgetting curves and memory decay, foundational to understanding memory quality over time]",
            "Tulving (1972) Episodic and Semantic Memory [Foundational work distinguishing types of memory and their different roles, related to consolidation]",
            "Anderson & Schooler (1991) Reflections of the Environment in Memory [Rational analysis of memory showing memory systems optimize for environmental statistics, related to selective storage]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
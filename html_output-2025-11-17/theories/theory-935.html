<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Episodic-Semantic Memory Integration Theory for LLM Text Game Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-935</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-935</p>
                <p><strong>Name:</strong> Hierarchical Episodic-Semantic Memory Integration Theory for LLM Text Game Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory posits that LLM agents perform best in text games when they maintain a hierarchical memory structure that integrates both episodic (event-specific) and semantic (generalized, abstracted) knowledge. The agent dynamically transitions between detailed recall of specific past events and abstraction/generalization of patterns, using semantic memory to guide high-level planning and episodic memory for context-sensitive actions. This integration allows for efficient memory usage, transfer learning across games, and robust adaptation to novel situations.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Memory Utilization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; encounters &#8594; complex or novel text game scenario</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; retrieves &#8594; semantic memory for high-level planning<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; retrieves &#8594; episodic memory for context-specific details</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human cognition relies on both episodic and semantic memory for problem solving. </li>
    <li>LLM agents with both event-specific and generalized knowledge perform better in transfer and adaptation tasks. </li>
    <li>Hierarchical memory architectures in RL and cognitive models improve sample efficiency and generalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is closely related to existing dual-memory theories, but its application and formalization for LLM text game agents is new.</p>            <p><strong>What Already Exists:</strong> Hierarchical and dual-memory systems are established in cognitive science and some AI architectures.</p>            <p><strong>What is Novel:</strong> The explicit, dynamic integration of episodic and semantic memory in LLM agents for text games is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [dual-memory theory in humans]</li>
    <li>Kumaran et al. (2016) What learning systems do intelligent agents need? [hierarchical memory in AI]</li>
    <li>Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]</li>
</ul>
            <h3>Statement 1: Dynamic Abstraction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; detects &#8594; repeated patterns or regularities in text game events</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; abstracts &#8594; semantic knowledge from episodic experiences<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; compresses &#8594; episodic memory by replacing redundant details with abstractions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Humans generalize from repeated experiences, forming semantic knowledge. </li>
    <li>AI agents that abstract patterns from episodic data improve transfer and reduce memory load. </li>
    <li>In text games, agents that learn general rules from specific events adapt to new but structurally similar tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is based on known principles but its explicit operationalization for LLM text game agents is new.</p>            <p><strong>What Already Exists:</strong> Abstraction and compression from episodic to semantic memory is established in cognitive science.</p>            <p><strong>What is Novel:</strong> The dynamic, on-the-fly abstraction and memory compression in LLM agents for text games is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>McClelland et al. (1995) Why there are complementary learning systems in the hippocampus and neocortex [episodic-semantic abstraction]</li>
    <li>Kumaran et al. (2016) What learning systems do intelligent agents need? [abstraction in AI]</li>
    <li>Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with explicit episodic-semantic memory integration will outperform agents with only one type of memory in games requiring both recall and generalization.</li>
                <li>Agents that dynamically abstract and compress memory will use less memory while maintaining or improving performance in long or repetitive games.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>The optimal balance between episodic and semantic memory for different genres of text games is unknown and may vary with game complexity.</li>
                <li>Emergent forms of memory abstraction may allow LLM agents to invent novel strategies not present in training data.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with only episodic or only semantic memory perform as well as those with both, the theory is challenged.</li>
                <li>If dynamic abstraction leads to loss of critical information and performance drops, the theory's assumptions are called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address catastrophic forgetting when semantic abstraction overwrites rare but important episodic details. </li>
    <li>The impact of noisy or misleading patterns on semantic abstraction is not considered. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory builds on dual-memory and abstraction principles but applies them in a new, operational way to LLM text game agents.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [dual-memory theory in humans]</li>
    <li>McClelland et al. (1995) Why there are complementary learning systems in the hippocampus and neocortex [episodic-semantic abstraction]</li>
    <li>Kumaran et al. (2016) What learning systems do intelligent agents need? [hierarchical memory in AI]</li>
    <li>Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Episodic-Semantic Memory Integration Theory for LLM Text Game Agents",
    "theory_description": "This theory posits that LLM agents perform best in text games when they maintain a hierarchical memory structure that integrates both episodic (event-specific) and semantic (generalized, abstracted) knowledge. The agent dynamically transitions between detailed recall of specific past events and abstraction/generalization of patterns, using semantic memory to guide high-level planning and episodic memory for context-sensitive actions. This integration allows for efficient memory usage, transfer learning across games, and robust adaptation to novel situations.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Memory Utilization Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "encounters",
                        "object": "complex or novel text game scenario"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "retrieves",
                        "object": "semantic memory for high-level planning"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "retrieves",
                        "object": "episodic memory for context-specific details"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human cognition relies on both episodic and semantic memory for problem solving.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents with both event-specific and generalized knowledge perform better in transfer and adaptation tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Hierarchical memory architectures in RL and cognitive models improve sample efficiency and generalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical and dual-memory systems are established in cognitive science and some AI architectures.",
                    "what_is_novel": "The explicit, dynamic integration of episodic and semantic memory in LLM agents for text games is novel.",
                    "classification_explanation": "The law is closely related to existing dual-memory theories, but its application and formalization for LLM text game agents is new.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [dual-memory theory in humans]",
                        "Kumaran et al. (2016) What learning systems do intelligent agents need? [hierarchical memory in AI]",
                        "Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dynamic Abstraction Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "detects",
                        "object": "repeated patterns or regularities in text game events"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "abstracts",
                        "object": "semantic knowledge from episodic experiences"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "compresses",
                        "object": "episodic memory by replacing redundant details with abstractions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Humans generalize from repeated experiences, forming semantic knowledge.",
                        "uuids": []
                    },
                    {
                        "text": "AI agents that abstract patterns from episodic data improve transfer and reduce memory load.",
                        "uuids": []
                    },
                    {
                        "text": "In text games, agents that learn general rules from specific events adapt to new but structurally similar tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Abstraction and compression from episodic to semantic memory is established in cognitive science.",
                    "what_is_novel": "The dynamic, on-the-fly abstraction and memory compression in LLM agents for text games is novel.",
                    "classification_explanation": "The law is based on known principles but its explicit operationalization for LLM text game agents is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "McClelland et al. (1995) Why there are complementary learning systems in the hippocampus and neocortex [episodic-semantic abstraction]",
                        "Kumaran et al. (2016) What learning systems do intelligent agents need? [abstraction in AI]",
                        "Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with explicit episodic-semantic memory integration will outperform agents with only one type of memory in games requiring both recall and generalization.",
        "Agents that dynamically abstract and compress memory will use less memory while maintaining or improving performance in long or repetitive games."
    ],
    "new_predictions_unknown": [
        "The optimal balance between episodic and semantic memory for different genres of text games is unknown and may vary with game complexity.",
        "Emergent forms of memory abstraction may allow LLM agents to invent novel strategies not present in training data."
    ],
    "negative_experiments": [
        "If agents with only episodic or only semantic memory perform as well as those with both, the theory is challenged.",
        "If dynamic abstraction leads to loss of critical information and performance drops, the theory's assumptions are called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address catastrophic forgetting when semantic abstraction overwrites rare but important episodic details.",
            "uuids": []
        },
        {
            "text": "The impact of noisy or misleading patterns on semantic abstraction is not considered.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some simple text games can be solved with only episodic or only semantic memory, suggesting integration is not always necessary.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Games with highly random or non-repetitive events may not benefit from semantic abstraction.",
        "Very short games may not require hierarchical memory structures."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-memory and hierarchical memory systems are established in cognitive science and some AI models.",
        "what_is_novel": "The explicit, dynamic integration and abstraction process in LLM agents for text games is novel.",
        "classification_explanation": "The theory builds on dual-memory and abstraction principles but applies them in a new, operational way to LLM text game agents.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [dual-memory theory in humans]",
            "McClelland et al. (1995) Why there are complementary learning systems in the hippocampus and neocortex [episodic-semantic abstraction]",
            "Kumaran et al. (2016) What learning systems do intelligent agents need? [hierarchical memory in AI]",
            "Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-591",
    "original_theory_name": "Task-Structure-Dependent Memory Utility Law for LLM Agents",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
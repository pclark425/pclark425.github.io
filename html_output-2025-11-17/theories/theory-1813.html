<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain and Prompt Sensitivity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1813</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1813</p>
                <p><strong>Name:</strong> Domain and Prompt Sensitivity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory posits that the ability of large language models (LLMs) to accurately estimate the probability of future scientific discoveries is fundamentally governed by the interplay between the domain-specific density of knowledge in their training data and the sensitivity of their outputs to prompt structure. The theory asserts that LLMs' forecasting accuracy is maximized when the domain is richly represented in the training corpus and when prompts are constructed to elicit reasoning aligned with the model's internal representations of scientific progress.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Domain Representation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; scientific_domain &#8594; is_well_represented_in &#8594; LLM_training_data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_more_accurate_for &#8594; future_discoveries_in_domain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs demonstrate higher factual accuracy and calibration in domains with abundant, high-quality training data. </li>
    <li>Empirical studies show LLMs' performance degrades in underrepresented or niche scientific fields. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law generalizes known LLM knowledge representation effects to the context of scientific forecasting.</p>            <p><strong>What Already Exists:</strong> It is established that LLMs perform better in domains with more training data.</p>            <p><strong>What is Novel:</strong> The explicit link to forecasting accuracy for future discoveries is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [LLM performance and data coverage]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration and knowledge representation]</li>
</ul>
            <h3>Statement 1: Prompt Sensitivity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM_prompt &#8594; is_structured_to_align_with &#8594; model_internal_reasoning_patterns</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_more_calibrated_for &#8594; future_scientific_discoveries</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompt engineering can significantly affect LLM output calibration and reasoning depth. </li>
    <li>Studies show that chain-of-thought and context-rich prompts improve LLM forecasting and reasoning. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends prompt engineering effects to the domain of scientific discovery forecasting.</p>            <p><strong>What Already Exists:</strong> Prompt engineering is known to affect LLM output quality.</p>            <p><strong>What is Novel:</strong> The law's focus on probability calibration for scientific forecasting is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompting and reasoning]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure and output quality]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will provide more accurate probability estimates for future discoveries in genomics than in paleontology, given the former's richer representation in training data.</li>
                <li>Rewriting prompts to include explicit reasoning steps will improve LLM calibration for scientific forecasting tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new scientific field rapidly emerges and is underrepresented in training data, LLMs may systematically underestimate discovery likelihood, regardless of prompt structure.</li>
                <li>Highly optimized prompts may partially compensate for sparse domain representation, but the extent of this compensation is unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs provide accurate forecasts in domains with little or no training data, the theory would be challenged.</li>
                <li>If prompt structure has no effect on LLM calibration for scientific forecasting, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The effect of multimodal data (e.g., images, code) on LLM forecasting accuracy is not addressed. </li>
    <li>The role of model size and architecture in modulating domain and prompt sensitivity is not explicitly considered. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes two known effects into a new framework for LLM-based scientific forecasting.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [LLM performance and data coverage]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompting and reasoning]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration and knowledge representation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain and Prompt Sensitivity Theory",
    "theory_description": "This theory posits that the ability of large language models (LLMs) to accurately estimate the probability of future scientific discoveries is fundamentally governed by the interplay between the domain-specific density of knowledge in their training data and the sensitivity of their outputs to prompt structure. The theory asserts that LLMs' forecasting accuracy is maximized when the domain is richly represented in the training corpus and when prompts are constructed to elicit reasoning aligned with the model's internal representations of scientific progress.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Domain Representation Law",
                "if": [
                    {
                        "subject": "scientific_domain",
                        "relation": "is_well_represented_in",
                        "object": "LLM_training_data"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_more_accurate_for",
                        "object": "future_discoveries_in_domain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs demonstrate higher factual accuracy and calibration in domains with abundant, high-quality training data.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LLMs' performance degrades in underrepresented or niche scientific fields.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is established that LLMs perform better in domains with more training data.",
                    "what_is_novel": "The explicit link to forecasting accuracy for future discoveries is new.",
                    "classification_explanation": "This law generalizes known LLM knowledge representation effects to the context of scientific forecasting.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [LLM performance and data coverage]",
                        "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration and knowledge representation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Sensitivity Law",
                "if": [
                    {
                        "subject": "LLM_prompt",
                        "relation": "is_structured_to_align_with",
                        "object": "model_internal_reasoning_patterns"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_more_calibrated_for",
                        "object": "future_scientific_discoveries"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompt engineering can significantly affect LLM output calibration and reasoning depth.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that chain-of-thought and context-rich prompts improve LLM forecasting and reasoning.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering is known to affect LLM output quality.",
                    "what_is_novel": "The law's focus on probability calibration for scientific forecasting is new.",
                    "classification_explanation": "This law extends prompt engineering effects to the domain of scientific discovery forecasting.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompting and reasoning]",
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure and output quality]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will provide more accurate probability estimates for future discoveries in genomics than in paleontology, given the former's richer representation in training data.",
        "Rewriting prompts to include explicit reasoning steps will improve LLM calibration for scientific forecasting tasks."
    ],
    "new_predictions_unknown": [
        "If a new scientific field rapidly emerges and is underrepresented in training data, LLMs may systematically underestimate discovery likelihood, regardless of prompt structure.",
        "Highly optimized prompts may partially compensate for sparse domain representation, but the extent of this compensation is unknown."
    ],
    "negative_experiments": [
        "If LLMs provide accurate forecasts in domains with little or no training data, the theory would be challenged.",
        "If prompt structure has no effect on LLM calibration for scientific forecasting, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "The effect of multimodal data (e.g., images, code) on LLM forecasting accuracy is not addressed.",
            "uuids": []
        },
        {
            "text": "The role of model size and architecture in modulating domain and prompt sensitivity is not explicitly considered.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs have shown surprising generalization to underrepresented domains, possibly due to transfer learning or emergent reasoning.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Domains with high data volume but low conceptual diversity may not benefit as much from domain representation.",
        "Prompts that are adversarial or misleading can degrade LLM calibration even in well-represented domains."
    ],
    "existing_theory": {
        "what_already_exists": "LLM performance is known to depend on data coverage and prompt engineering.",
        "what_is_novel": "The explicit, joint focus on domain representation and prompt sensitivity for scientific discovery forecasting is new.",
        "classification_explanation": "This theory synthesizes two known effects into a new framework for LLM-based scientific forecasting.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [LLM performance and data coverage]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompting and reasoning]",
            "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [LLM calibration and knowledge representation]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-647",
    "original_theory_name": "Domain and Prompt Sensitivity Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Domain and Prompt Sensitivity Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
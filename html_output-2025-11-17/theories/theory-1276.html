<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Fidelity Principle for Graph-to-Text Representations - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1276</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1276</p>
                <p><strong>Name:</strong> Semantic Fidelity Principle for Graph-to-Text Representations</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal representation for converting graphs into text for language model training is one that maximizes semantic fidelity: the preservation of all graph-encoded meaning, including structure, node/edge attributes, and higher-order relationships. The theory claims that representations which explicitly encode both local and global semantics, and which are invertible (i.e., allow reconstruction of the original graph), will yield superior language model performance and generalization.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; encodes &#8594; all_graph_semantics<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_representation &#8594; is_invertible &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; can_learn &#8594; accurate_graph-to-text_mappings<span style="color: #888888;">, and</span></div>
        <div>&#8226; generated_text &#8594; preserves &#8594; original_graph_meaning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Lossy representations (e.g., simple linearizations) often fail to capture all graph semantics, leading to degraded LM performance. </li>
    <li>Invertible representations enable round-trip conversion and are used in code-to-text and AMR-to-text tasks. </li>
    <li>Explicit encoding of node/edge types and attributes improves downstream LM tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While invertibility and semantic preservation are valued in some domains, this law elevates them to a universal principle for graph-to-text LM training.</p>            <p><strong>What Already Exists:</strong> Invertible and semantically rich representations are used in AMR-to-text and code summarization tasks.</p>            <p><strong>What is Novel:</strong> The law generalizes semantic fidelity and invertibility as necessary for all graph-to-text LM training, not just specific domains.</p>
            <p><strong>References:</strong> <ul>
    <li>Konstas et al. (2017) Neural AMR: Sequence-to-sequence models for parsing and generation [AMR-to-text, invertible representations]</li>
    <li>Iyer et al. (2016) Summarizing source code using a neural attention model [code-to-text, semantic preservation]</li>
</ul>
            <h3>Statement 1: Global-Local Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; encodes &#8594; local_node_and_edge_information<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_representation &#8594; encodes &#8594; global_graph_structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; can_generalize &#8594; to_unseen_graph_structures</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Encoding only local or only global information leads to poor generalization in graph-based LM tasks. </li>
    <li>Hybrid representations (e.g., node sequences with global summaries) improve LM performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes local and global encoding as a requirement for generalization in graph-to-text LMs.</p>            <p><strong>What Already Exists:</strong> Hybrid local-global encoding is used in some graph neural networks and summarization models.</p>            <p><strong>What is Novel:</strong> The law asserts that both are necessary for ideal graph-to-text LM representations, not just beneficial.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How powerful are graph neural networks? [local/global information in GNNs]</li>
    <li>Liu et al. (2019) Hierarchical Transformers for Long Document Classification [local/global encoding in text]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Representations that are not invertible will result in lower LM accuracy and semantic fidelity in graph-to-text tasks.</li>
                <li>LMs trained on representations encoding both local and global information will generalize better to novel graph structures.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Invertible, semantically rich representations may enable LMs to perform graph reasoning tasks (e.g., subgraph isomorphism) via text.</li>
                <li>Explicit global structure encoding may allow LMs to extrapolate to graphs much larger than those seen in training.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If lossy, non-invertible representations yield equal or better LM performance, the theory is challenged.</li>
                <li>If encoding only local or only global information suffices for generalization, the theory's necessity claim is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify the optimal method for encoding global structure (e.g., adjacency lists, spectral features, etc.). </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and elevates existing best practices to a universal principle.</p>
            <p><strong>References:</strong> <ul>
    <li>Konstas et al. (2017) Neural AMR: Sequence-to-sequence models for parsing and generation [AMR-to-text, invertible representations]</li>
    <li>Iyer et al. (2016) Summarizing source code using a neural attention model [code-to-text, semantic preservation]</li>
    <li>Xu et al. (2019) How powerful are graph neural networks? [local/global information in GNNs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Fidelity Principle for Graph-to-Text Representations",
    "theory_description": "This theory posits that the ideal representation for converting graphs into text for language model training is one that maximizes semantic fidelity: the preservation of all graph-encoded meaning, including structure, node/edge attributes, and higher-order relationships. The theory claims that representations which explicitly encode both local and global semantics, and which are invertible (i.e., allow reconstruction of the original graph), will yield superior language model performance and generalization.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Preservation Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "encodes",
                        "object": "all_graph_semantics"
                    },
                    {
                        "subject": "graph_representation",
                        "relation": "is_invertible",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "can_learn",
                        "object": "accurate_graph-to-text_mappings"
                    },
                    {
                        "subject": "generated_text",
                        "relation": "preserves",
                        "object": "original_graph_meaning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Lossy representations (e.g., simple linearizations) often fail to capture all graph semantics, leading to degraded LM performance.",
                        "uuids": []
                    },
                    {
                        "text": "Invertible representations enable round-trip conversion and are used in code-to-text and AMR-to-text tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Explicit encoding of node/edge types and attributes improves downstream LM tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Invertible and semantically rich representations are used in AMR-to-text and code summarization tasks.",
                    "what_is_novel": "The law generalizes semantic fidelity and invertibility as necessary for all graph-to-text LM training, not just specific domains.",
                    "classification_explanation": "While invertibility and semantic preservation are valued in some domains, this law elevates them to a universal principle for graph-to-text LM training.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Konstas et al. (2017) Neural AMR: Sequence-to-sequence models for parsing and generation [AMR-to-text, invertible representations]",
                        "Iyer et al. (2016) Summarizing source code using a neural attention model [code-to-text, semantic preservation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Global-Local Encoding Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "encodes",
                        "object": "local_node_and_edge_information"
                    },
                    {
                        "subject": "graph_representation",
                        "relation": "encodes",
                        "object": "global_graph_structure"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "can_generalize",
                        "object": "to_unseen_graph_structures"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Encoding only local or only global information leads to poor generalization in graph-based LM tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Hybrid representations (e.g., node sequences with global summaries) improve LM performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hybrid local-global encoding is used in some graph neural networks and summarization models.",
                    "what_is_novel": "The law asserts that both are necessary for ideal graph-to-text LM representations, not just beneficial.",
                    "classification_explanation": "The law synthesizes local and global encoding as a requirement for generalization in graph-to-text LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How powerful are graph neural networks? [local/global information in GNNs]",
                        "Liu et al. (2019) Hierarchical Transformers for Long Document Classification [local/global encoding in text]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Representations that are not invertible will result in lower LM accuracy and semantic fidelity in graph-to-text tasks.",
        "LMs trained on representations encoding both local and global information will generalize better to novel graph structures."
    ],
    "new_predictions_unknown": [
        "Invertible, semantically rich representations may enable LMs to perform graph reasoning tasks (e.g., subgraph isomorphism) via text.",
        "Explicit global structure encoding may allow LMs to extrapolate to graphs much larger than those seen in training."
    ],
    "negative_experiments": [
        "If lossy, non-invertible representations yield equal or better LM performance, the theory is challenged.",
        "If encoding only local or only global information suffices for generalization, the theory's necessity claim is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify the optimal method for encoding global structure (e.g., adjacency lists, spectral features, etc.).",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LMs can learn useful mappings from lossy, non-invertible representations in highly regular graph domains.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For trivial graphs (e.g., chains or trees), local encoding may suffice.",
        "For highly regular or repetitive graphs, global encoding may be redundant."
    ],
    "existing_theory": {
        "what_already_exists": "Semantic fidelity and invertibility are valued in some graph-to-text and code-to-text domains.",
        "what_is_novel": "The theory generalizes these principles as necessary for all graph-to-text LM training.",
        "classification_explanation": "The theory synthesizes and elevates existing best practices to a universal principle.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Konstas et al. (2017) Neural AMR: Sequence-to-sequence models for parsing and generation [AMR-to-text, invertible representations]",
            "Iyer et al. (2016) Summarizing source code using a neural attention model [code-to-text, semantic preservation]",
            "Xu et al. (2019) How powerful are graph neural networks? [local/global information in GNNs]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-613",
    "original_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
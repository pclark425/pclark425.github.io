<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Load and Complexity Threshold Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1669</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1669</p>
                <p><strong>Name:</strong> Cognitive Load and Complexity Threshold Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> The accuracy of LLMs as scientific simulators is governed by a complexity threshold: as the cognitive load (i.e., the number of interacting concepts, steps, or dependencies) in a subdomain task increases, LLM accuracy drops sharply beyond a certain threshold, regardless of training data coverage. This threshold is determined by the LLM's architecture, context window, and internal capacity for abstraction and memory.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Complexity Threshold Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; subdomain task &#8594; has_cognitive_load &#8594; above LLM complexity threshold</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; drops_sharply &#8594; for that task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform well on simple tasks but fail on tasks requiring many interacting steps or concepts (e.g., multi-step math, complex causal inference). </li>
    <li>Increasing the context window or model size can raise the complexity threshold, improving performance on more complex tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The threshold concept is a novel synthesis of observed performance drops.</p>            <p><strong>What Already Exists:</strong> LLM performance degrades with increasing task complexity.</p>            <p><strong>What is Novel:</strong> The explicit framing of a sharp complexity threshold, and its dependence on architecture and context window.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [multi-step reasoning limitations]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [reasoning bottlenecks]</li>
</ul>
            <h3>Statement 1: Capacity-Dependent Simulation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_architecture_capacity &#8594; sufficient for subdomain task complexity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; remains_high &#8594; for that task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Larger LLMs with greater context windows and more parameters perform better on complex scientific tasks. </li>
    <li>Smaller LLMs or those with limited context windows fail on the same tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes scaling laws with a threshold-based view of simulation accuracy.</p>            <p><strong>What Already Exists:</strong> Scaling laws for LLMs are established.</p>            <p><strong>What is Novel:</strong> The explicit link between architecture capacity and a complexity threshold for simulation accuracy.</p>
            <p><strong>References:</strong> <ul>
    <li>Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling and performance]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [reasoning and capacity]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If the context window or model size is increased, the complexity threshold will rise, allowing accurate simulation of more complex tasks.</li>
                <li>If a task is decomposed into simpler subtasks, LLM simulation accuracy will improve.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are augmented with external memory or reasoning modules, the complexity threshold may be eliminated.</li>
                <li>If a new architecture is developed with fundamentally different abstraction mechanisms, the threshold may shift or disappear.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform well on tasks above their predicted complexity threshold, the theory would be challenged.</li>
                <li>If increasing model size or context window does not raise the complexity threshold, the law would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs succeed on complex tasks via memorized patterns rather than genuine reasoning. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes scaling and complexity into a threshold-based model.</p>
            <p><strong>References:</strong> <ul>
    <li>Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling and performance]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [reasoning and capacity]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Load and Complexity Threshold Theory",
    "theory_description": "The accuracy of LLMs as scientific simulators is governed by a complexity threshold: as the cognitive load (i.e., the number of interacting concepts, steps, or dependencies) in a subdomain task increases, LLM accuracy drops sharply beyond a certain threshold, regardless of training data coverage. This threshold is determined by the LLM's architecture, context window, and internal capacity for abstraction and memory.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Complexity Threshold Law",
                "if": [
                    {
                        "subject": "subdomain task",
                        "relation": "has_cognitive_load",
                        "object": "above LLM complexity threshold"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "drops_sharply",
                        "object": "for that task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform well on simple tasks but fail on tasks requiring many interacting steps or concepts (e.g., multi-step math, complex causal inference).",
                        "uuids": []
                    },
                    {
                        "text": "Increasing the context window or model size can raise the complexity threshold, improving performance on more complex tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLM performance degrades with increasing task complexity.",
                    "what_is_novel": "The explicit framing of a sharp complexity threshold, and its dependence on architecture and context window.",
                    "classification_explanation": "The threshold concept is a novel synthesis of observed performance drops.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [multi-step reasoning limitations]",
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [reasoning bottlenecks]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Capacity-Dependent Simulation Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_architecture_capacity",
                        "object": "sufficient for subdomain task complexity"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "remains_high",
                        "object": "for that task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Larger LLMs with greater context windows and more parameters perform better on complex scientific tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Smaller LLMs or those with limited context windows fail on the same tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Scaling laws for LLMs are established.",
                    "what_is_novel": "The explicit link between architecture capacity and a complexity threshold for simulation accuracy.",
                    "classification_explanation": "The law synthesizes scaling laws with a threshold-based view of simulation accuracy.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling and performance]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [reasoning and capacity]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If the context window or model size is increased, the complexity threshold will rise, allowing accurate simulation of more complex tasks.",
        "If a task is decomposed into simpler subtasks, LLM simulation accuracy will improve."
    ],
    "new_predictions_unknown": [
        "If LLMs are augmented with external memory or reasoning modules, the complexity threshold may be eliminated.",
        "If a new architecture is developed with fundamentally different abstraction mechanisms, the threshold may shift or disappear."
    ],
    "negative_experiments": [
        "If LLMs perform well on tasks above their predicted complexity threshold, the theory would be challenged.",
        "If increasing model size or context window does not raise the complexity threshold, the law would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs succeed on complex tasks via memorized patterns rather than genuine reasoning.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Emergent abilities in very large LLMs sometimes allow them to exceed expected complexity thresholds.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with highly repetitive or formulaic structure may be less affected by the complexity threshold."
    ],
    "existing_theory": {
        "what_already_exists": "Scaling laws and performance drops with complexity are known.",
        "what_is_novel": "The explicit threshold framing and its dependence on architecture and context window.",
        "classification_explanation": "The theory synthesizes scaling and complexity into a threshold-based model.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling and performance]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [reasoning and capacity]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-638",
    "original_theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
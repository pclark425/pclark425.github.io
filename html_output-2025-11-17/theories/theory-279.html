<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Competency Tool-Use Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-279</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-279</p>
                <p><strong>Name:</strong> Multi-Competency Tool-Use Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory explaining the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents, and the architectural/training interventions required to close the gap.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that the dissociation between strong QA performance and weak interactive procedural performance arises from the requirement for coordinated execution of multiple distinct competencies in tool use. Specifically, successful interactive tool use requires at least five core competencies: (1) Goal Decomposition & Planning - breaking high-level goals into executable sub-goals, (2) Tool Selection - choosing appropriate tools for each sub-goal, (3) Parameter Binding - correctly mapping task variables to tool parameters, (4) State Monitoring - tracking environmental state and action outcomes, and (5) Error Recovery - detecting and correcting failures. QA tasks primarily require only comprehension and retrieval competencies, which are well-trained during pre-training and instruction tuning. In contrast, interactive tasks require all five competencies to execute in coordination, with outputs from one competency serving as inputs to others (e.g., state monitoring informs planning, planning guides tool selection). The theory posits that current LLM training provides implicit exposure to individual competencies but lacks explicit training on competency coordination and inter-competency dependencies. This creates three failure modes: (1) Competency Isolation - individual competencies may function well in isolation but fail when integrated, (2) Cascade Failures - errors in one competency propagate to dependent competencies, and (3) Coordination Overhead - the cognitive load of managing multiple competencies simultaneously exceeds model capacity. The theory predicts that interventions targeting competency coordination (e.g., modular architectures with explicit competency modules, training curricula that progressively increase coordination requirements, or prompting strategies that externalize coordination) will show larger improvements on interactive tasks than interventions targeting individual competencies.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Interactive tool use requires coordinated execution of at least five distinct competencies: goal decomposition, tool selection, parameter binding, state monitoring, and error recovery.</li>
                <li>QA tasks primarily require only 1-2 competencies (comprehension and retrieval), while interactive tasks require all competencies working in coordination.</li>
                <li>Current LLM training provides implicit exposure to individual competencies but lacks explicit training on competency coordination.</li>
                <li>The performance gap between QA and interactive tasks scales with the number of competencies that must be coordinated and the strength of inter-competency dependencies.</li>
                <li>Failures in one competency cascade to dependent competencies, causing exponential degradation in multi-step tasks.</li>
                <li>The cognitive load of coordinating multiple competencies simultaneously can exceed model capacity, even when individual competencies are well-learned.</li>
                <li>Interventions that externalize or modularize competency coordination will show larger improvements on interactive tasks than on QA tasks.</li>
                <li>The benefit of coordination-focused interventions will scale with task complexity (number of steps, tools, and state variables).</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Analysis of agent failures reveals multiple distinct error types including planning errors, tool selection errors, parameter binding errors, and state tracking errors, suggesting these represent separable competencies. </li>
    <li>Agent performance degrades significantly as task complexity increases, particularly when tasks require coordinating multiple tools or multi-step reasoning, consistent with coordination overhead. </li>
    <li>Modular approaches that separate reasoning and acting (e.g., ReAct) improve performance over monolithic approaches, suggesting benefits from explicit competency separation. </li>
    <li>Agents show cascading failures where an early error (e.g., incorrect tool selection) leads to subsequent errors (e.g., invalid parameters, incorrect state updates), indicating inter-competency dependencies. </li>
    <li>Scratchpads and chain-of-thought prompting improve performance by externalizing intermediate reasoning, reducing coordination overhead. </li>
    <li>LLMs can perform well on isolated sub-tasks (e.g., tool selection given a clear goal, parameter extraction given a tool) but fail when these must be coordinated in full task execution. </li>
    <li>Training with explicit task decomposition and step-by-step execution improves interactive performance more than general instruction tuning. </li>
    <li>Attention analysis shows that models struggle to simultaneously attend to multiple types of information (goals, state, tool specifications, previous actions) required for competency coordination. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Training curricula that progressively increase coordination requirements (starting with single-competency tasks, then two-competency, etc.) will improve interactive performance by 25-50% compared to standard instruction tuning.</li>
                <li>Modular architectures with separate modules for each competency and explicit coordination mechanisms will outperform monolithic architectures by 30-60% on complex interactive tasks while showing minimal difference on QA tasks.</li>
                <li>Prompting strategies that explicitly externalize each competency (e.g., 'First identify the goal, then select tools, then bind parameters, then monitor state') will improve performance by 15-30% compared to standard prompting.</li>
                <li>The performance gap between QA and interactive tasks will correlate strongly (r > 0.7) with the number of competencies required and the number of inter-competency dependencies.</li>
                <li>Fine-tuning on tasks that isolate specific competency pairs (e.g., planning + tool selection, tool selection + parameter binding) will improve performance on those competency combinations more than general fine-tuning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists an optimal ordering of competency execution (e.g., always plan before tool selection vs. interleaving planning and selection) that could improve performance by an order of magnitude, or whether optimal ordering is task-dependent.</li>
                <li>Whether competency coordination can be learned implicitly through sufficient exposure to complex interactive tasks, or whether explicit architectural or training interventions are necessary for human-level performance.</li>
                <li>Whether some competencies are fundamentally more difficult to coordinate than others (e.g., state monitoring + error recovery vs. planning + tool selection), potentially requiring different architectural solutions.</li>
                <li>Whether the number of competencies is fixed at five or whether finer-grained decomposition (e.g., splitting parameter binding into type checking, value extraction, and mapping) would reveal additional coordination challenges.</li>
                <li>Whether competency coordination requirements explain other AI capability gaps beyond tool use (e.g., multi-modal reasoning, long-horizon planning), suggesting a general theory of multi-competency intelligence.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If modular architectures with explicit competency separation do not outperform monolithic architectures on interactive tasks, this would challenge the claim that coordination is a primary bottleneck.</li>
                <li>If training curricula that progressively increase coordination requirements do not improve performance more than standard training, this would question whether coordination is learnable through structured exposure.</li>
                <li>If the performance gap between QA and interactive tasks does not correlate with the number of required competencies, this would challenge the multi-competency framework.</li>
                <li>If interventions that improve individual competencies (e.g., better state tracking) show equal improvements on interactive tasks as interventions targeting coordination, this would suggest coordination is not the key issue.</li>
                <li>If agents show similar failure rates on tasks requiring different numbers of competencies (controlling for other complexity factors), this would challenge the scaling predictions.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some agents achieve reasonable performance on complex interactive tasks without explicit competency modularization, suggesting other mechanisms may enable coordination. </li>
    <li>The theory does not fully explain why some competencies (e.g., tool selection) appear easier for LLMs than others (e.g., error recovery), even when both are required in isolation. </li>
    <li>The theory does not address how competency requirements might vary across different tool-use domains (e.g., web navigation vs. API composition vs. robotic control). </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models [Proposes separating reasoning and acting but doesn't explicitly theorize about multiple competencies and their coordination as the source of QA-interactive dissociation]</li>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [Addresses tool use but focuses on learning to use tools rather than competency coordination theory]</li>
    <li>Mialon et al. (2023) Augmented Language Models: a Survey [Reviews various augmentation approaches but doesn't propose multi-competency coordination as a theoretical framework]</li>
    <li>Anderson (1983) The Architecture of Cognition [Classic cognitive architecture work on skill composition and procedural knowledge, but predates LLMs and doesn't address the QA-interactive dissociation]</li>
    <li>Newell (1990) Unified Theories of Cognition [Proposes multiple cognitive capabilities but doesn't specifically address LLM tool use or the dissociation phenomenon]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Competency Tool-Use Theory",
    "theory_description": "This theory proposes that the dissociation between strong QA performance and weak interactive procedural performance arises from the requirement for coordinated execution of multiple distinct competencies in tool use. Specifically, successful interactive tool use requires at least five core competencies: (1) Goal Decomposition & Planning - breaking high-level goals into executable sub-goals, (2) Tool Selection - choosing appropriate tools for each sub-goal, (3) Parameter Binding - correctly mapping task variables to tool parameters, (4) State Monitoring - tracking environmental state and action outcomes, and (5) Error Recovery - detecting and correcting failures. QA tasks primarily require only comprehension and retrieval competencies, which are well-trained during pre-training and instruction tuning. In contrast, interactive tasks require all five competencies to execute in coordination, with outputs from one competency serving as inputs to others (e.g., state monitoring informs planning, planning guides tool selection). The theory posits that current LLM training provides implicit exposure to individual competencies but lacks explicit training on competency coordination and inter-competency dependencies. This creates three failure modes: (1) Competency Isolation - individual competencies may function well in isolation but fail when integrated, (2) Cascade Failures - errors in one competency propagate to dependent competencies, and (3) Coordination Overhead - the cognitive load of managing multiple competencies simultaneously exceeds model capacity. The theory predicts that interventions targeting competency coordination (e.g., modular architectures with explicit competency modules, training curricula that progressively increase coordination requirements, or prompting strategies that externalize coordination) will show larger improvements on interactive tasks than interventions targeting individual competencies.",
    "supporting_evidence": [
        {
            "text": "Analysis of agent failures reveals multiple distinct error types including planning errors, tool selection errors, parameter binding errors, and state tracking errors, suggesting these represent separable competencies.",
            "citations": [
                "Liu et al. (2023) AgentBench: Evaluating LLMs as Agents",
                "Ruan et al. (2023) Identifying the Risks of LM Agents with an LM-Emulated Sandbox"
            ]
        },
        {
            "text": "Agent performance degrades significantly as task complexity increases, particularly when tasks require coordinating multiple tools or multi-step reasoning, consistent with coordination overhead.",
            "citations": [
                "Zhou et al. (2023) WebArena: A Realistic Web Environment for Building Autonomous Agents",
                "Liu et al. (2023) AgentBench: Evaluating LLMs as Agents"
            ]
        },
        {
            "text": "Modular approaches that separate reasoning and acting (e.g., ReAct) improve performance over monolithic approaches, suggesting benefits from explicit competency separation.",
            "citations": [
                "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models"
            ]
        },
        {
            "text": "Agents show cascading failures where an early error (e.g., incorrect tool selection) leads to subsequent errors (e.g., invalid parameters, incorrect state updates), indicating inter-competency dependencies.",
            "citations": [
                "Ruan et al. (2023) Identifying the Risks of LM Agents with an LM-Emulated Sandbox"
            ]
        },
        {
            "text": "Scratchpads and chain-of-thought prompting improve performance by externalizing intermediate reasoning, reducing coordination overhead.",
            "citations": [
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
            ]
        },
        {
            "text": "LLMs can perform well on isolated sub-tasks (e.g., tool selection given a clear goal, parameter extraction given a tool) but fail when these must be coordinated in full task execution.",
            "citations": [
                "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools"
            ]
        },
        {
            "text": "Training with explicit task decomposition and step-by-step execution improves interactive performance more than general instruction tuning.",
            "citations": [
                "Zhou et al. (2023) WebArena: A Realistic Web Environment for Building Autonomous Agents"
            ]
        },
        {
            "text": "Attention analysis shows that models struggle to simultaneously attend to multiple types of information (goals, state, tool specifications, previous actions) required for competency coordination.",
            "citations": [
                "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts"
            ]
        }
    ],
    "theory_statements": [
        "Interactive tool use requires coordinated execution of at least five distinct competencies: goal decomposition, tool selection, parameter binding, state monitoring, and error recovery.",
        "QA tasks primarily require only 1-2 competencies (comprehension and retrieval), while interactive tasks require all competencies working in coordination.",
        "Current LLM training provides implicit exposure to individual competencies but lacks explicit training on competency coordination.",
        "The performance gap between QA and interactive tasks scales with the number of competencies that must be coordinated and the strength of inter-competency dependencies.",
        "Failures in one competency cascade to dependent competencies, causing exponential degradation in multi-step tasks.",
        "The cognitive load of coordinating multiple competencies simultaneously can exceed model capacity, even when individual competencies are well-learned.",
        "Interventions that externalize or modularize competency coordination will show larger improvements on interactive tasks than on QA tasks.",
        "The benefit of coordination-focused interventions will scale with task complexity (number of steps, tools, and state variables)."
    ],
    "new_predictions_likely": [
        "Training curricula that progressively increase coordination requirements (starting with single-competency tasks, then two-competency, etc.) will improve interactive performance by 25-50% compared to standard instruction tuning.",
        "Modular architectures with separate modules for each competency and explicit coordination mechanisms will outperform monolithic architectures by 30-60% on complex interactive tasks while showing minimal difference on QA tasks.",
        "Prompting strategies that explicitly externalize each competency (e.g., 'First identify the goal, then select tools, then bind parameters, then monitor state') will improve performance by 15-30% compared to standard prompting.",
        "The performance gap between QA and interactive tasks will correlate strongly (r &gt; 0.7) with the number of competencies required and the number of inter-competency dependencies.",
        "Fine-tuning on tasks that isolate specific competency pairs (e.g., planning + tool selection, tool selection + parameter binding) will improve performance on those competency combinations more than general fine-tuning."
    ],
    "new_predictions_unknown": [
        "Whether there exists an optimal ordering of competency execution (e.g., always plan before tool selection vs. interleaving planning and selection) that could improve performance by an order of magnitude, or whether optimal ordering is task-dependent.",
        "Whether competency coordination can be learned implicitly through sufficient exposure to complex interactive tasks, or whether explicit architectural or training interventions are necessary for human-level performance.",
        "Whether some competencies are fundamentally more difficult to coordinate than others (e.g., state monitoring + error recovery vs. planning + tool selection), potentially requiring different architectural solutions.",
        "Whether the number of competencies is fixed at five or whether finer-grained decomposition (e.g., splitting parameter binding into type checking, value extraction, and mapping) would reveal additional coordination challenges.",
        "Whether competency coordination requirements explain other AI capability gaps beyond tool use (e.g., multi-modal reasoning, long-horizon planning), suggesting a general theory of multi-competency intelligence."
    ],
    "negative_experiments": [
        "If modular architectures with explicit competency separation do not outperform monolithic architectures on interactive tasks, this would challenge the claim that coordination is a primary bottleneck.",
        "If training curricula that progressively increase coordination requirements do not improve performance more than standard training, this would question whether coordination is learnable through structured exposure.",
        "If the performance gap between QA and interactive tasks does not correlate with the number of required competencies, this would challenge the multi-competency framework.",
        "If interventions that improve individual competencies (e.g., better state tracking) show equal improvements on interactive tasks as interventions targeting coordination, this would suggest coordination is not the key issue.",
        "If agents show similar failure rates on tasks requiring different numbers of competencies (controlling for other complexity factors), this would challenge the scaling predictions."
    ],
    "unaccounted_for": [
        {
            "text": "Some agents achieve reasonable performance on complex interactive tasks without explicit competency modularization, suggesting other mechanisms may enable coordination.",
            "citations": [
                "Nakano et al. (2021) WebGPT: Browser-assisted question-answering with human feedback"
            ]
        },
        {
            "text": "The theory does not fully explain why some competencies (e.g., tool selection) appear easier for LLMs than others (e.g., error recovery), even when both are required in isolation.",
            "citations": [
                "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools"
            ]
        },
        {
            "text": "The theory does not address how competency requirements might vary across different tool-use domains (e.g., web navigation vs. API composition vs. robotic control).",
            "citations": [
                "Zhou et al. (2023) WebArena: A Realistic Web Environment for Building Autonomous Agents",
                "Liu et al. (2023) AgentBench: Evaluating LLMs as Agents"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that simple prompting improvements (e.g., chain-of-thought) can substantially close the QA-interactive gap, suggesting coordination may not require architectural changes.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback"
            ]
        },
        {
            "text": "Larger models show improvements on both QA and interactive tasks with similar scaling rates, which might suggest the gap is not specifically about coordination but general capability.",
            "citations": [
                "OpenAI (2023) GPT-4 Technical Report"
            ]
        }
    ],
    "special_cases": [
        "For tasks where competencies can be executed sequentially without feedback loops (e.g., simple linear workflows), coordination overhead may be minimal and the theory's predictions may not hold.",
        "In domains where tools provide rich feedback and error messages, the error recovery competency may be less critical, reducing coordination requirements.",
        "When tasks require only 2-3 competencies, the coordination overhead may be manageable even without explicit interventions.",
        "For highly practiced task types that appear frequently in training data, competency coordination may be implicitly learned, reducing the gap.",
        "In cases where external scaffolding (e.g., structured interfaces, constrained action spaces) reduces coordination requirements, the theory's predictions about architectural interventions may be less applicable."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models [Proposes separating reasoning and acting but doesn't explicitly theorize about multiple competencies and their coordination as the source of QA-interactive dissociation]",
            "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [Addresses tool use but focuses on learning to use tools rather than competency coordination theory]",
            "Mialon et al. (2023) Augmented Language Models: a Survey [Reviews various augmentation approaches but doesn't propose multi-competency coordination as a theoretical framework]",
            "Anderson (1983) The Architecture of Cognition [Classic cognitive architecture work on skill composition and procedural knowledge, but predates LLMs and doesn't address the QA-interactive dissociation]",
            "Newell (1990) Unified Theories of Cognition [Proposes multiple cognitive capabilities but doesn't specifically address LLM tool use or the dissociation phenomenon]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 1,
    "theory_query": "Build a theory explaining the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents, and the architectural/training interventions required to close the gap.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-109",
    "original_theory_name": "Multi-Competency Tool-Use Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Alignment Theory of LLM Scientific Simulation Accuracy - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1614</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1614</p>
                <p><strong>Name:</strong> Hierarchical Alignment Theory of LLM Scientific Simulation Accuracy</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory posits that the accuracy of LLMs as text-based simulators in scientific subdomains is determined by a hierarchy of alignment factors: (1) alignment between the LLM's training data and the subdomain's knowledge corpus, (2) alignment between the LLM's internal representations and the subdomain's conceptual structure, and (3) alignment between the LLM's output format and the subdomain's epistemic standards. Deficiencies at any level of this hierarchy propagate downward, limiting simulation accuracy, and improvements at higher levels yield multiplicative gains in downstream accuracy.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Data-Concept-Output Alignment Cascade (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM training data &#8594; is_highly_similar_to &#8594; target subdomain knowledge corpus<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM internal representations &#8594; are_structurally_aligned_with &#8594; subdomain conceptual structure<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM output &#8594; matches &#8594; subdomain epistemic standards</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves_high_accuracy_as &#8594; text-based simulator in the subdomain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs fine-tuned on domain-specific corpora (e.g., biomedical, legal) outperform general LLMs in those domains. </li>
    <li>LLMs with explicit architectural or prompt-based alignment to scientific reasoning (e.g., chain-of-thought, tool use) show improved simulation fidelity. </li>
    <li>Output formats that mirror scientific reporting (e.g., structured tables, equations) are more likely to be accepted as accurate by domain experts. </li>
    <li>Empirical studies show that LLMs trained on data closely matching the target subdomain demonstrate higher factual accuracy and reasoning ability. </li>
    <li>Prompt engineering that encodes subdomain-specific reasoning steps (e.g., step-by-step scientific method) increases LLM performance on simulation tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing work on domain adaptation and prompt engineering, the explicit hierarchical, multiplicative structure and its implications for simulation accuracy are novel.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that domain-specific fine-tuning and prompt engineering improve LLM performance in scientific tasks.</p>            <p><strong>What is Novel:</strong> The explicit hierarchical cascade—where alignment at each level multiplies or constrains accuracy at lower levels—has not been formalized as a theory.</p>
            <p><strong>References:</strong> <ul>
    <li>Garg et al. (2022) Can Large Language Models Reason About Science? [Shows domain-specific fine-tuning improves scientific reasoning]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompting improves reasoning, but not formalized as hierarchical alignment]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Discusses alignment, but not as a hierarchical cascade]</li>
</ul>
            <h3>Statement 1: Alignment Deficiency Propagation (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_alignment_deficiency_at &#8594; any hierarchy level (data, concept, output)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits_degraded_accuracy_as &#8594; text-based simulator in the subdomain<span style="color: #888888;">, and</span></div>
        <div>&#8226; downstream alignment levels &#8594; cannot_compensate_for &#8594; upstream deficiencies</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs trained on general data but prompted with domain-specific formats still make factual errors due to lack of knowledge. </li>
    <li>LLMs with strong domain data but poor output formatting are rejected by domain experts. </li>
    <li>Empirical studies show that lack of domain data or poor prompt design each independently limit LLM performance. </li>
    <li>Attempts to improve output formatting without addressing knowledge gaps do not yield high simulation accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While the effect of individual deficiencies is known, the non-compensatory, cascading nature is a novel theoretical contribution.</p>            <p><strong>What Already Exists:</strong> Empirical studies show that lack of domain data or poor prompt design each independently limit LLM performance.</p>            <p><strong>What is Novel:</strong> The assertion that deficiencies at higher levels cannot be compensated by improvements at lower levels is a new, formalized principle.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2023) Do Prompt-Based Models Really Understand the Task? [Shows prompt design matters, but not the non-compensatory cascade]</li>
    <li>Singhal et al. (2023) Large Language Models Encode Clinical Knowledge [Shows domain data matters, but not the hierarchy]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM is fine-tuned on a subdomain corpus, uses prompts that mirror the subdomain's conceptual structure, and outputs in the subdomain's standard format, its simulation accuracy will be significantly higher than a general LLM.</li>
                <li>If an LLM lacks domain-specific data, no amount of prompt engineering or output formatting will yield high simulation accuracy.</li>
                <li>Improving alignment at the data level will yield greater accuracy gains than improving only output formatting.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new alignment method is developed that directly manipulates internal representations to match subdomain concepts (without changing data or output), it may yield multiplicative accuracy gains.</li>
                <li>If a subdomain's epistemic standards are fundamentally incompatible with LLM output (e.g., requiring non-textual reasoning), even perfect data and concept alignment may not yield high accuracy.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an LLM with poor training data but perfect output formatting achieves high simulation accuracy, the theory is called into question.</li>
                <li>If improvements at lower levels (e.g., output formatting) fully compensate for deficiencies at higher levels (e.g., data), the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs achieve high accuracy through emergent reasoning not present in training data or explicit alignment. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and formalizes disparate findings into a new, hierarchical framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Garg et al. (2022) Can Large Language Models Reason About Science? [Domain adaptation]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt engineering]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Alignment, but not hierarchical]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Alignment Theory of LLM Scientific Simulation Accuracy",
    "theory_description": "This theory posits that the accuracy of LLMs as text-based simulators in scientific subdomains is determined by a hierarchy of alignment factors: (1) alignment between the LLM's training data and the subdomain's knowledge corpus, (2) alignment between the LLM's internal representations and the subdomain's conceptual structure, and (3) alignment between the LLM's output format and the subdomain's epistemic standards. Deficiencies at any level of this hierarchy propagate downward, limiting simulation accuracy, and improvements at higher levels yield multiplicative gains in downstream accuracy.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Data-Concept-Output Alignment Cascade",
                "if": [
                    {
                        "subject": "LLM training data",
                        "relation": "is_highly_similar_to",
                        "object": "target subdomain knowledge corpus"
                    },
                    {
                        "subject": "LLM internal representations",
                        "relation": "are_structurally_aligned_with",
                        "object": "subdomain conceptual structure"
                    },
                    {
                        "subject": "LLM output",
                        "relation": "matches",
                        "object": "subdomain epistemic standards"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves_high_accuracy_as",
                        "object": "text-based simulator in the subdomain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs fine-tuned on domain-specific corpora (e.g., biomedical, legal) outperform general LLMs in those domains.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs with explicit architectural or prompt-based alignment to scientific reasoning (e.g., chain-of-thought, tool use) show improved simulation fidelity.",
                        "uuids": []
                    },
                    {
                        "text": "Output formats that mirror scientific reporting (e.g., structured tables, equations) are more likely to be accepted as accurate by domain experts.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that LLMs trained on data closely matching the target subdomain demonstrate higher factual accuracy and reasoning ability.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering that encodes subdomain-specific reasoning steps (e.g., step-by-step scientific method) increases LLM performance on simulation tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that domain-specific fine-tuning and prompt engineering improve LLM performance in scientific tasks.",
                    "what_is_novel": "The explicit hierarchical cascade—where alignment at each level multiplies or constrains accuracy at lower levels—has not been formalized as a theory.",
                    "classification_explanation": "While related to existing work on domain adaptation and prompt engineering, the explicit hierarchical, multiplicative structure and its implications for simulation accuracy are novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Garg et al. (2022) Can Large Language Models Reason About Science? [Shows domain-specific fine-tuning improves scientific reasoning]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompting improves reasoning, but not formalized as hierarchical alignment]",
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Discusses alignment, but not as a hierarchical cascade]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Alignment Deficiency Propagation",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_alignment_deficiency_at",
                        "object": "any hierarchy level (data, concept, output)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits_degraded_accuracy_as",
                        "object": "text-based simulator in the subdomain"
                    },
                    {
                        "subject": "downstream alignment levels",
                        "relation": "cannot_compensate_for",
                        "object": "upstream deficiencies"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs trained on general data but prompted with domain-specific formats still make factual errors due to lack of knowledge.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs with strong domain data but poor output formatting are rejected by domain experts.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that lack of domain data or poor prompt design each independently limit LLM performance.",
                        "uuids": []
                    },
                    {
                        "text": "Attempts to improve output formatting without addressing knowledge gaps do not yield high simulation accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical studies show that lack of domain data or poor prompt design each independently limit LLM performance.",
                    "what_is_novel": "The assertion that deficiencies at higher levels cannot be compensated by improvements at lower levels is a new, formalized principle.",
                    "classification_explanation": "While the effect of individual deficiencies is known, the non-compensatory, cascading nature is a novel theoretical contribution.",
                    "likely_classification": "new",
                    "references": [
                        "Min et al. (2023) Do Prompt-Based Models Really Understand the Task? [Shows prompt design matters, but not the non-compensatory cascade]",
                        "Singhal et al. (2023) Large Language Models Encode Clinical Knowledge [Shows domain data matters, but not the hierarchy]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM is fine-tuned on a subdomain corpus, uses prompts that mirror the subdomain's conceptual structure, and outputs in the subdomain's standard format, its simulation accuracy will be significantly higher than a general LLM.",
        "If an LLM lacks domain-specific data, no amount of prompt engineering or output formatting will yield high simulation accuracy.",
        "Improving alignment at the data level will yield greater accuracy gains than improving only output formatting."
    ],
    "new_predictions_unknown": [
        "If a new alignment method is developed that directly manipulates internal representations to match subdomain concepts (without changing data or output), it may yield multiplicative accuracy gains.",
        "If a subdomain's epistemic standards are fundamentally incompatible with LLM output (e.g., requiring non-textual reasoning), even perfect data and concept alignment may not yield high accuracy."
    ],
    "negative_experiments": [
        "If an LLM with poor training data but perfect output formatting achieves high simulation accuracy, the theory is called into question.",
        "If improvements at lower levels (e.g., output formatting) fully compensate for deficiencies at higher levels (e.g., data), the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs achieve high accuracy through emergent reasoning not present in training data or explicit alignment.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies report LLMs can generalize to new domains with minimal data, suggesting possible compensation across levels.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Domains with highly redundant or universal knowledge may be less sensitive to alignment deficiencies.",
        "Subdomains with non-textual epistemic standards (e.g., visual proofs) may not fit the hierarchy."
    ],
    "existing_theory": {
        "what_already_exists": "Domain adaptation and prompt engineering are known to improve LLM performance.",
        "what_is_novel": "The explicit hierarchical, multiplicative alignment cascade and the non-compensatory propagation of deficiencies are new.",
        "classification_explanation": "The theory synthesizes and formalizes disparate findings into a new, hierarchical framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Garg et al. (2022) Can Large Language Models Reason About Science? [Domain adaptation]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt engineering]",
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Alignment, but not hierarchical]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-635",
    "original_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
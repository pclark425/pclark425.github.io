<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Format as a High-Dimensional Control Signal for LLM Computation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-652</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-652</p>
                <p><strong>Name:</strong> Prompt Format as a High-Dimensional Control Signal for LLM Computation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that the presentation format of a problem—including wording, structure, context, exemplars, constraints, metadata, and even subtle formatting—acts as a high-dimensional control signal that deterministically configures the internal computation and output distribution of large language models (LLMs). The format not only guides the model's attention and reasoning pathways but also interacts with pretraining and fine-tuning priors, leading to large, often unpredictable, swings in performance, robustness, and output style. The theory further asserts that the mapping from prompt format to model behavior is highly non-linear, model-dependent, and sensitive to both surface and semantic features, with certain formats acting as attractors for specific behaviors or failure modes.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt Format Deterministically Alters LLM Output Distribution (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; is_varied &#8594; across semantically-equivalent forms<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_fixed &#8594; model weights and decoding parameters</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_output_distribution &#8594; changes &#8594; substantially (accuracy, style, robustness, degeneration rate, etc.)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Observed swings up to 76 percentage points in accuracy across prompt formats for the same model and task; prompt-format identifiability in embeddings; high variance in performance with prompt wording, structure, and example selection. <a href="../results/extraction-result-5671.html#e5671.0" class="evidence-link">[e5671.0]</a> <a href="../results/extraction-result-5799.html#e5799.1" class="evidence-link">[e5799.1]</a> <a href="../results/extraction-result-5685.html#e5685.0" class="evidence-link">[e5685.0]</a> <a href="../results/extraction-result-5813.html#e5813.3" class="evidence-link">[e5813.3]</a> <a href="../results/extraction-result-5671.html#e5671.5" class="evidence-link">[e5671.5]</a> <a href="../results/extraction-result-5798.html#e5798.0" class="evidence-link">[e5798.0]</a> <a href="../results/extraction-result-5801.html#e5801.7" class="evidence-link">[e5801.7]</a> <a href="../results/extraction-result-5807.html#e5807.0" class="evidence-link">[e5807.0]</a> <a href="../results/extraction-result-5807.html#e5807.2" class="evidence-link">[e5807.2]</a> <a href="../results/extraction-result-5818.html#e5818.9" class="evidence-link">[e5818.9]</a> <a href="../results/extraction-result-5807.html#e5807.3" class="evidence-link">[e5807.3]</a> <a href="../results/extraction-result-5813.html#e5813.7" class="evidence-link">[e5813.7]</a> <a href="../results/extraction-result-5813.html#e5813.9" class="evidence-link">[e5813.9]</a> <a href="../results/extraction-result-5695.html#e5695.0" class="evidence-link">[e5695.0]</a> <a href="../results/extraction-result-5695.html#e5695.1" class="evidence-link">[e5695.1]</a> <a href="../results/extraction-result-5695.html#e5695.4" class="evidence-link">[e5695.4]</a> <a href="../results/extraction-result-5695.html#e5695.6" class="evidence-link">[e5695.6]</a> <a href="../results/extraction-result-5695.html#e5695.7" class="evidence-link">[e5695.7]</a> <a href="../results/extraction-result-5695.html#e5695.9" class="evidence-link">[e5695.9]</a> <a href="../results/extraction-result-5695.html#e5695.10" class="evidence-link">[e5695.10]</a> <a href="../results/extraction-result-5695.html#e5695.11" class="evidence-link">[e5695.11]</a> <a href="../results/extraction-result-5698.html#e5698.3" class="evidence-link">[e5698.3]</a> <a href="../results/extraction-result-5698.html#e5698.2" class="evidence-link">[e5698.2]</a> <a href="../results/extraction-result-5698.html#e5698.1" class="evidence-link">[e5698.1]</a> <a href="../results/extraction-result-5702.html#e5702.6" class="evidence-link">[e5702.6]</a> <a href="../results/extraction-result-5705.html#e5705.0" class="evidence-link">[e5705.0]</a> <a href="../results/extraction-result-5705.html#e5705.1" class="evidence-link">[e5705.1]</a> <a href="../results/extraction-result-5707.html#e5707.1" class="evidence-link">[e5707.1]</a> <a href="../results/extraction-result-5707.html#e5707.6" class="evidence-link">[e5707.6]</a> <a href="../results/extraction-result-5712.html#e5712.3" class="evidence-link">[e5712.3]</a> <a href="../results/extraction-result-5712.html#e5712.5" class="evidence-link">[e5712.5]</a> <a href="../results/extraction-result-5712.html#e5712.6" class="evidence-link">[e5712.6]</a> <a href="../results/extraction-result-5712.html#e5712.8" class="evidence-link">[e5712.8]</a> <a href="../results/extraction-result-5712.html#e5712.10" class="evidence-link">[e5712.10]</a> <a href="../results/extraction-result-5712.html#e5712.11" class="evidence-link">[e5712.11]</a> <a href="../results/extraction-result-5712.html#e5712.15" class="evidence-link">[e5712.15]</a> <a href="../results/extraction-result-5712.html#e5712.23" class="evidence-link">[e5712.23]</a> <a href="../results/extraction-result-5712.html#e5712.24" class="evidence-link">[e5712.24]</a> <a href="../results/extraction-result-5712.html#e5712.25" class="evidence-link">[e5712.25]</a> <a href="../results/extraction-result-5712.html#e5712.26" class="evidence-link">[e5712.26]</a> <a href="../results/extraction-result-5712.html#e5712.29" class="evidence-link">[e5712.29]</a> <a href="../results/extraction-result-5712.html#e5712.32" class="evidence-link">[e5712.32]</a> <a href="../results/extraction-result-5821.html#e5821.0" class="evidence-link">[e5821.0]</a> <a href="../results/extraction-result-5821.html#e5821.5" class="evidence-link">[e5821.5]</a> <a href="../results/extraction-result-5821.html#e5821.6" class="evidence-link">[e5821.6]</a> <a href="../results/extraction-result-5822.html#e5822.2" class="evidence-link">[e5822.2]</a> <a href="../results/extraction-result-5822.html#e5822.5" class="evidence-link">[e5822.5]</a> <a href="../results/extraction-result-5822.html#e5822.7" class="evidence-link">[e5822.7]</a> <a href="../results/extraction-result-5824.html#e5824.0" class="evidence-link">[e5824.0]</a> <a href="../results/extraction-result-5824.html#e5824.1" class="evidence-link">[e5824.1]</a> <a href="../results/extraction-result-5825.html#e5825.0" class="evidence-link">[e5825.0]</a> <a href="../results/extraction-result-5825.html#e5825.6" class="evidence-link">[e5825.6]</a> <a href="../results/extraction-result-5825.html#e5825.8" class="evidence-link">[e5825.8]</a> <a href="../results/extraction-result-5825.html#e5825.9" class="evidence-link">[e5825.9]</a> <a href="../results/extraction-result-5825.html#e5825.10" class="evidence-link">[e5825.10]</a> <a href="../results/extraction-result-5826.html#e5826.5" class="evidence-link">[e5826.5]</a> <a href="../results/extraction-result-5828.html#e5828.8" class="evidence-link">[e5828.8]</a> <a href="../results/extraction-result-5829.html#e5829.4" class="evidence-link">[e5829.4]</a> <a href="../results/extraction-result-5829.html#e5829.5" class="evidence-link">[e5829.5]</a> <a href="../results/extraction-result-5830.html#e5830.1" class="evidence-link">[e5830.1]</a> <a href="../results/extraction-result-5831.html#e5831.3" class="evidence-link">[e5831.3]</a> <a href="../results/extraction-result-5831.html#e5831.6" class="evidence-link">[e5831.6]</a> <a href="../results/extraction-result-5831.html#e5831.8" class="evidence-link">[e5831.8]</a> <a href="../results/extraction-result-5832.html#e5832.5" class="evidence-link">[e5832.5]</a> <a href="../results/extraction-result-5832.html#e5832.9" class="evidence-link">[e5832.9]</a> <a href="../results/extraction-result-5833.html#e5833.2" class="evidence-link">[e5833.2]</a> <a href="../results/extraction-result-5833.html#e5833.5" class="evidence-link">[e5833.5]</a> <a href="../results/extraction-result-5833.html#e5833.9" class="evidence-link">[e5833.9]</a> <a href="../results/extraction-result-5834.html#e5834.0" class="evidence-link">[e5834.0]</a> <a href="../results/extraction-result-5834.html#e5834.2" class="evidence-link">[e5834.2]</a> <a href="../results/extraction-result-5834.html#e5834.3" class="evidence-link">[e5834.3]</a> <a href="../results/extraction-result-5835.html#e5835.0" class="evidence-link">[e5835.0]</a> <a href="../results/extraction-result-5835.html#e5835.7" class="evidence-link">[e5835.7]</a> <a href="../results/e5835.9.html#e5835.9" class="evidence-link">[e5835.9]</a> <a href="../results/extraction-result-5836.html#e5836.0" class="evidence-link">[e5836.0]</a> <a href="../results/extraction-result-5836.html#e5836.4" class="evidence-link">[e5836.4]</a> <a href="../results/extraction-result-5836.html#e5836.6" class="evidence-link">[e5836.6]</a> <a href="../results/extraction-result-5836.html#e5836.7" class="evidence-link">[e5836.7]</a> <a href="../results/extraction-result-5836.html#e5836.8" class="evidence-link">[e5836.8]</a> <a href="../results/extraction-result-5839.html#e5839.2" class="evidence-link">[e5839.2]</a> <a href="../results/extraction-result-5839.html#e5839.7" class="evidence-link">[e5839.7]</a> <a href="../results/extraction-result-5842.html#e5842.2" class="evidence-link">[e5842.2]</a> <a href="../results/extraction-result-5846.html#e5846.1" class="evidence-link">[e5846.1]</a> <a href="../results/extraction-result-5846.html#e5846.5" class="evidence-link">[e5846.5]</a> <a href="../results/extraction-result-5855.html#e5855.1" class="evidence-link">[e5855.1]</a> <a href="../results/extraction-result-5855.html#e5855.3" class="evidence-link">[e5855.3]</a> <a href="../results/extraction-result-5855.html#e5855.5" class="evidence-link">[e5855.5]</a> <a href="../results/extraction-result-5855.html#e5855.6" class="evidence-link">[e5855.6]</a> <a href="../results/extraction-result-5860.html#e5860.1" class="evidence-link">[e5860.1]</a> <a href="../results/extraction-result-5862.html#e5862.4" class="evidence-link">[e5862.4]</a> <a href="../results/extraction-result-5862.html#e5862.7" class="evidence-link">[e5862.7]</a> <a href="../results/extraction-result-5866.html#e5866.0" class="evidence-link">[e5866.0]</a> <a href="../results/extraction-result-5866.html#e5866.1" class="evidence-link">[e5866.1]</a> <a href="../results/extraction-result-5866.html#e5866.5" class="evidence-link">[e5866.5]</a> <a href="../results/extraction-result-5867.html#e5867.1" class="evidence-link">[e5867.1]</a> <a href="../results/extraction-result-5867.html#e5867.4" class="evidence-link">[e5867.4]</a> <a href="../results/extraction-result-5867.html#e5867.5" class="evidence-link">[e5867.5]</a> <a href="../results/extraction-result-5867.html#e5867.6" class="evidence-link">[e5867.6]</a> <a href="../results/extraction-result-5869.html#e5869.1" class="evidence-link">[e5869.1]</a> <a href="../results/extraction-result-5869.html#e5869.3" class="evidence-link">[e5869.3]</a> <a href="../results/extraction-result-5869.html#e5869.5" class="evidence-link">[e5869.5]</a> <a href="../results/extraction-result-5874.html#e5874.1" class="evidence-link">[e5874.1]</a> <a href="../results/extraction-result-5874.html#e5874.7" class="evidence-link">[e5874.7]</a> <a href="../results/extraction-result-5874.html#e5874.8" class="evidence-link">[e5874.8]</a> <a href="../results/extraction-result-5874.html#e5874.12" class="evidence-link">[e5874.12]</a> <a href="../results/extraction-result-5686.html#e5686.1" class="evidence-link">[e5686.1]</a> <a href="../results/extraction-result-5686.html#e5686.10" class="evidence-link">[e5686.10]</a> <a href="../results/extraction-result-5686.html#e5686.11" class="evidence-link">[e5686.11]</a> <a href="../results/extraction-result-5652.html#e5652.0" class="evidence-link">[e5652.0]</a> <a href="../results/extraction-result-5652.html#e5652.7" class="evidence-link">[e5652.7]</a> <a href="../results/extraction-result-5659.html#e5659.0" class="evidence-link">[e5659.0]</a> <a href="../results/extraction-result-5659.html#e5659.4" class="evidence-link">[e5659.4]</a> <a href="../results/extraction-result-5659.html#e5659.7" class="evidence-link">[e5659.7]</a> <a href="../results/extraction-result-5662.html#e5662.2" class="evidence-link">[e5662.2]</a> <a href="../results/extraction-result-5663.html#e5663.3" class="evidence-link">[e5663.3]</a> <a href="../results/extraction-result-5668.html#e5668.3" class="evidence-link">[e5668.3]</a> <a href="../results/extraction-result-5669.html#e5669.1" class="evidence-link">[e5669.1]</a> <a href="../results/extraction-result-5677.html#e5677.4" class="evidence-link">[e5677.4]</a> <a href="../results/extraction-result-5677.html#e5677.6" class="evidence-link">[e5677.6]</a> <a href="../results/extraction-result-5685.html#e5685.1" class="evidence-link">[e5685.1]</a> <a href="../results/extraction-result-5685.html#e5685.4" class="evidence-link">[e5685.4]</a> <a href="../results/extraction-result-5689.html#e5689.4" class="evidence-link">[e5689.4]</a> <a href="../results/extraction-result-5690.html#e5690.2" class="evidence-link">[e5690.2]</a> <a href="../results/extraction-result-5690.html#e5690.8" class="evidence-link">[e5690.8]</a> <a href="../results/extraction-result-5692.html#e5692.0" class="evidence-link">[e5692.0]</a> <a href="../results/extraction-result-5692.html#e5692.4" class="evidence-link">[e5692.4]</a> <a href="../results/extraction-result-5692.html#e5692.7" class="evidence-link">[e5692.7]</a> <a href="../results/extraction-result-5708.html#e5708.0" class="evidence-link">[e5708.0]</a> <a href="../results/extraction-result-5708.html#e5708.2" class="evidence-link">[e5708.2]</a> <a href="../results/extraction-result-5791.html#e5791.0" class="evidence-link">[e5791.0]</a> <a href="../results/extraction-result-5791.html#e5791.2" class="evidence-link">[e5791.2]</a> <a href="../results/extraction-result-5791.html#e5791.4" class="evidence-link">[e5791.4]</a> <a href="../results/extraction-result-5791.html#e5791.6" class="evidence-link">[e5791.6]</a> <a href="../results/extraction-result-5794.html#e5794.0" class="evidence-link">[e5794.0]</a> <a href="../results/extraction-result-5795.html#e5795.3" class="evidence-link">[e5795.3]</a> <a href="../results/extraction-result-5795.html#e5795.4" class="evidence-link">[e5795.4]</a> <a href="../results/extraction-result-5797.html#e5797.0" class="evidence-link">[e5797.0]</a> <a href="../results/extraction-result-5798.html#e5798.0" class="evidence-link">[e5798.0]</a> <a href="../results/extraction-result-5799.html#e5799.0" class="evidence-link">[e5799.0]</a> <a href="../results/extraction-result-5799.html#e5799.3" class="evidence-link">[e5799.3]</a> <a href="../results/extraction-result-5801.html#e5801.3" class="evidence-link">[e5801.3]</a> <a href="../results/extraction-result-5801.html#e5801.6" class="evidence-link">[e5801.6]</a> <a href="../results/extraction-result-5801.html#e5801.7" class="evidence-link">[e5801.7]</a> <a href="../results/extraction-result-5804.html#e5804.0" class="evidence-link">[e5804.0]</a> <a href="../results/extraction-result-5805.html#e5805.4" class="evidence-link">[e5805.4]</a> <a href="../results/extraction-result-5805.html#e5805.5" class="evidence-link">[e5805.5]</a> <a href="../results/extraction-result-5806.html#e5806.3" class="evidence-link">[e5806.3]</a> <a href="../results/extraction-result-5807.html#e5807.0" class="evidence-link">[e5807.0]</a> <a href="../results/extraction-result-5807.html#e5807.2" class="evidence-link">[e5807.2]</a> <a href="../results/extraction-result-5807.html#e5807.3" class="evidence-link">[e5807.3]</a> <a href="../results/extraction-result-5807.html#e5807.5" class="evidence-link">[e5807.5]</a> <a href="../results/extraction-result-5808.html#e5808.0" class="evidence-link">[e5808.0]</a> <a href="../results/extraction-result-5808.html#e5808.1" class="evidence-link">[e5808.1]</a> <a href="../results/extraction-result-5809.html#e5809.5" class="evidence-link">[e5809.5]</a> <a href="../results/extraction-result-5809.html#e5809.7" class="evidence-link">[e5809.7]</a> <a href="../results/extraction-result-5809.html#e5809.11" class="evidence-link">[e5809.11]</a> <a href="../results/extraction-result-5809.html#e5809.13" class="evidence-link">[e5809.13]</a> <a href="../results/extraction-result-5810.html#e5810.0" class="evidence-link">[e5810.0]</a> <a href="../results/extraction-result-5810.html#e5810.4" class="evidence-link">[e5810.4]</a> <a href="../results/extraction-result-5810.html#e5810.6" class="evidence-link">[e5810.6]</a> <a href="../results/extraction-result-5811.html#e5811.0" class="evidence-link">[e5811.0]</a> <a href="../results/extraction-result-5811.html#e5811.7" class="evidence-link">[e5811.7]</a> <a href="../results/extraction-result-5811.html#e5811.8" class="evidence-link">[e5811.8]</a> <a href="../results/extraction-result-5812.html#e5812.0" class="evidence-link">[e5812.0]</a> <a href="../results/extraction-result-5813.html#e5813.3" class="evidence-link">[e5813.3]</a> <a href="../results/extraction-result-5813.html#e5813.7" class="evidence-link">[e5813.7]</a> <a href="../results/extraction-result-5813.html#e5813.8" class="evidence-link">[e5813.8]</a> <a href="../results/extraction-result-5813.html#e5813.9" class="evidence-link">[e5813.9]</a> <a href="../results/extraction-result-5816.html#e5816.0" class="evidence-link">[e5816.0]</a> <a href="../results/extraction-result-5816.html#e5816.1" class="evidence-link">[e5816.1]</a> <a href="../results/extraction-result-5816.html#e5816.2" class="evidence-link">[e5816.2]</a> <a href="../results/extraction-result-5816.html#e5816.3" class="evidence-link">[e5816.3]</a> <a href="../results/extraction-result-5816.html#e5816.4" class="evidence-link">[e5816.4]</a> <a href="../results/extraction-result-5816.html#e5816.5" class="evidence-link">[e5816.5]</a> <a href="../results/extraction-result-5817.html#e5817.0" class="evidence-link">[e5817.0]</a> <a href="../results/extraction-result-5817.html#e5817.1" class="evidence-link">[e5817.1]</a> <a href="../results/extraction-result-5817.html#e5817.2" class="evidence-link">[e5817.2]</a> <a href="../results/extraction-result-5817.html#e5817.4" class="evidence-link">[e5817.4]</a> <a href="../results/extraction-result-5818.html#e5818.0" class="evidence-link">[e5818.0]</a> <a href="../results/extraction-result-5818.html#e5818.3" class="evidence-link">[e5818.3]</a> <a href="../results/extraction-result-5818.html#e5818.9" class="evidence-link">[e5818.9]</a> <a href="../results/extraction-result-5819.html#e5819.0" class="evidence-link">[e5819.0]</a> <a href="../results/extraction-result-5820.html#e5820.6" class="evidence-link">[e5820.6]</a> </li>
    <li>Prompt-format identifiability in model embeddings: classifiers can identify prompt format from hidden states with >98% accuracy, and separability correlates with performance spread. <a href="../results/extraction-result-5671.html#e5671.5" class="evidence-link">[e5671.5]</a> </li>
    <li>Multi-prompt instability: small changes in instruction templates (wording, punctuation, rephrasing) produce large changes in both absolute accuracy and relative model ranking. <a href="../results/extraction-result-5798.html#e5798.0" class="evidence-link">[e5798.0]</a> </li>
    <li>Prompt-format and in-context example sensitivity: nearly all evaluated models are highly sensitive to prompt wording, formatting, choice of in-context examples, and the number of examples, producing large swings in performance. <a href="../results/extraction-result-5799.html#e5799.1" class="evidence-link">[e5799.1]</a> <a href="../results/extraction-result-5685.html#e5685.0" class="evidence-link">[e5685.0]</a> <a href="../results/extraction-result-5813.html#e5813.3" class="evidence-link">[e5813.3]</a> <a href="../results/extraction-result-5685.html#e5685.1" class="evidence-link">[e5685.1]</a> </li>
    <li>Manual Wording Sensitivity: small changes in prompt wording can yield large differences in accuracy (e.g., 40.9% to 71.2% on AG News with OPT 175B). <a href="../results/extraction-result-5802.html#e5802.6" class="evidence-link">[e5802.6]</a> </li>
    <li>Semantically similar prompts phenomenon: paraphrases or semantically overlapping instructions can have widely varying performance, increasing variance and oscillation during optimization. <a href="../results/extraction-result-5818.html#e5818.9" class="evidence-link">[e5818.9]</a> </li>
    <li>Prompt-order sensitivity: the order of in-context (few-shot) examples in a prompt can materially affect LLM outputs and task performance. <a href="../results/extraction-result-5685.html#e5685.1" class="evidence-link">[e5685.1]</a> </li>
    <li>Prompt-format sensitivity (Liang et al. 2022): LLM outputs are highly sensitive to prompt formatting, the particular choice and number of in-context examples, and related template design, with substantial variability across models and tasks. <a href="../results/extraction-result-5685.html#e5685.0" class="evidence-link">[e5685.0]</a> </li>
    <li>Formatting Sensitivity (Global): semantically-equivalent but differently formatted prompts produce large variability in LLM task performance across many tasks and models, with spreads up to 76 percentage points. <a href="../results/extraction-result-5671.html#e5671.0" class="evidence-link">[e5671.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt sensitivity is recognized, the explicit mapping of prompt format to internal embedding separability and output distribution, and the identification of format as a deterministic control signal, is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> It is well-known that prompt wording and format affect LLM outputs, and prompt engineering is a recognized practice.</p>            <p><strong>What is Novel:</strong> This law formalizes prompt format as a deterministic, high-dimensional control signal that can be mapped in embedding space, and links format-induced embedding separability to performance spread, suggesting a mechanistic basis for prompt sensitivity.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [prompting as control signal]</li>
    <li>Liang et al. (2022) Holistic Evaluation of Language Models [prompt-format sensitivity]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [prompt calibration]</li>
    <li>Zhou et al. (2023) Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design [formatting sensitivity, embedding separability]</li>
</ul>
            <h3>Statement 1: Prompt Format Interacts Nonlinearly with Model Pretraining and Fine-Tuning Priors (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; matches &#8594; patterns frequent in pretraining/fine-tuning data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_more_likely_to &#8594; produce accurate, robust, and expected outputs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Open-ended QA formats outperform restrictive formats due to higher frequency in pretraining data; observed-instruction templates outperform semantically-correct but unobserved instructions; common-token/label-name bias; instruction-tuned models are more robust to prompt variation but can be less sensitive to semantics. <a href="../results/extraction-result-5692.html#e5692.0" class="evidence-link">[e5692.0]</a> <a href="../results/extraction-result-5808.html#e5808.0" class="evidence-link">[e5808.0]</a> <a href="../results/extraction-result-5808.html#e5808.1" class="evidence-link">[e5808.1]</a> <a href="../results/extraction-result-5833.html#e5833.5" class="evidence-link">[e5833.5]</a> <a href="../results/extraction-result-5820.html#e5820.6" class="evidence-link">[e5820.6]</a> <a href="../results/extraction-result-5799.html#e5799.3" class="evidence-link">[e5799.3]</a> <a href="../results/extraction-result-5855.html#e5855.5" class="evidence-link">[e5855.5]</a> <a href="../results/extraction-result-5813.html#e5813.9" class="evidence-link">[e5813.9]</a> <a href="../results/extraction-result-5813.html#e5813.7" class="evidence-link">[e5813.7]</a> <a href="../results/extraction-result-5791.html#e5791.2" class="evidence-link">[e5791.2]</a> <a href="../results/extraction-result-5791.html#e5791.4" class="evidence-link">[e5791.4]</a> <a href="../results/extraction-result-5791.html#e5791.6" class="evidence-link">[e5791.6]</a> <a href="../results/extraction-result-5821.html#e5821.6" class="evidence-link">[e5821.6]</a> <a href="../results/extraction-result-5707.html#e5707.6" class="evidence-link">[e5707.6]</a> </li>
    <li>Incorrect but observed instructions can outperform correct but unobserved instructions (i.e., observed-inappropriate > unobserved-appropriate) due to distributional familiarity. <a href="../results/extraction-result-5808.html#e5808.1" class="evidence-link">[e5808.1]</a> </li>
    <li>Instruction-tuned models (T0 family) are more robust across prompt variations but can become less sensitive to prompt semantics, sometimes producing strong performance even with pathological prompts. <a href="../results/extraction-result-5820.html#e5820.6" class="evidence-link">[e5820.6]</a> </li>
    <li>Pretraining objective and prompt alignment: matching prompt shape (cloze/prefix) to pre-training objective improves performance and applicability. <a href="../results/extraction-result-5813.html#e5813.9" class="evidence-link">[e5813.9]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The explicit prediction and empirical support for observed-inappropriate > unobserved-appropriate, and the mapping to pretraining/fine-tuning frequency, is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Prompt engineering and the importance of matching prompt format to pretraining/fine-tuning distribution is recognized.</p>            <p><strong>What is Novel:</strong> This law formalizes the non-linear interaction and predicts that even semantically-correct but distributionally-novel prompts can underperform, and that observed-inappropriate instructions can outperform unobserved-appropriate ones.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [instruction tuning]</li>
    <li>Mishra et al. (2022) Cross-task Generalization via Declarative Instructions [prompt diversity]</li>
    <li>Zhou et al. (2023) Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design [formatting sensitivity]</li>
</ul>
            <h3>Statement 2: Prompt Format Can Induce or Suppress Specific Failure Modes (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; includes &#8594; strict output constraints, rare tokens, or adversarial perturbations</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_more_likely_to &#8594; exhibit degeneration, misformatting, or attention misallocation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Strict answer-format enforcement caused high misformatting rates and apparent performance drops; adversarial prompt attacks shift attention away from task-relevant tokens; formatting can increase degeneration (no valid output). <a href="../results/extraction-result-5702.html#e5702.6" class="evidence-link">[e5702.6]</a> <a href="../results/extraction-result-5831.html#e5831.6" class="evidence-link">[e5831.6]</a> <a href="../results/extraction-result-5671.html#e5671.9" class="evidence-link">[e5671.9]</a> <a href="../results/extraction-result-5695.html#e5695.9" class="evidence-link">[e5695.9]</a> </li>
    <li>Degeneration / Centered Mass: prompt formatting affects not just accuracy but the frequency a model outputs a valid option at all; strong correlation reported between accuracy and 'centered mass' (ratio of outputs matching any valid option). <a href="../results/extraction-result-5671.html#e5671.9" class="evidence-link">[e5671.9]</a> </li>
    <li>Adversarial prompt attacks (character-, word-, sentence-, semantic-level) reduce performance and can cause models to focus on irrelevant or misleading tokens. <a href="../results/extraction-result-5695.html#e5695.9" class="evidence-link">[e5695.9]</a> <a href="../results/extraction-result-5831.html#e5831.6" class="evidence-link">[e5831.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The explicit mapping from format features to failure modes and attention shifts is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Prompt attacks and degeneration are known phenomena.</p>            <p><strong>What is Novel:</strong> This law unifies these as format-induced failure modes and links them to specific prompt features (e.g., strict constraints, adversarial noise) and attention mechanisms.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2020) How Can We Know What Language Models Know? [prompt sensitivity]</li>
    <li>Zhou et al. (2023) Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design [degeneration, format-induced errors]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new prompt format is constructed that closely matches a high-frequency pretraining pattern, even if semantically suboptimal, it will outperform a semantically-perfect but distributionally-novel format.</li>
                <li>If prompt formats are adversarially perturbed (e.g., with distractor sentences or rare tokens), attention will shift away from task-relevant content and accuracy will drop.</li>
                <li>If prompt formats are sampled and their embeddings are visualized, formats with higher separability will correspond to higher performance spread.</li>
                <li>If strict output formatting is enforced on a model not trained for it, misformatting rates and apparent performance drops will increase.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained with adversarially generated prompt formats as negative examples, it may develop robustness to a broader class of spurious features, potentially at the cost of reduced sensitivity to genuine instruction semantics.</li>
                <li>If a prompt format is constructed to be maximally orthogonal (in embedding space) to all training formats, the model may fail catastrophically or default to generic outputs, regardless of semantic content.</li>
                <li>If a model is trained to ignore all formatting cues and only attend to semantic content, prompt sensitivity may be reduced but overall performance may also decrease.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a set of semantically-equivalent prompts with highly divergent surface forms all produce identical outputs and accuracy, this would challenge the theory's assertion of high format sensitivity.</li>
                <li>If attention heatmaps do not change with adversarial prompt perturbations, or if degeneration rates are unaffected by strict output constraints, this would call into question the law linking format to failure modes.</li>
                <li>If observed-inappropriate instructions do not outperform unobserved-appropriate ones in any setting, the non-linear interaction law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where prompt length alone does not affect performance, suggesting that not all surface features are equally impactful. <a href="../results/extraction-result-5810.html#e5810.4" class="evidence-link">[e5810.4]</a> </li>
    <li>Null/minimal prompts can sometimes be competitive, indicating that format sensitivity is not universal. <a href="../results/extraction-result-5813.html#e5813.7" class="evidence-link">[e5813.7]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While the components are individually recognized, the explicit, mechanistic, and predictive synthesis is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [prompting as control signal]</li>
    <li>Liang et al. (2022) Holistic Evaluation of Language Models [prompt-format sensitivity]</li>
    <li>Zhou et al. (2023) Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design [formatting sensitivity, embedding separability]</li>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [instruction tuning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt Format as a High-Dimensional Control Signal for LLM Computation",
    "theory_description": "This theory posits that the presentation format of a problem—including wording, structure, context, exemplars, constraints, metadata, and even subtle formatting—acts as a high-dimensional control signal that deterministically configures the internal computation and output distribution of large language models (LLMs). The format not only guides the model's attention and reasoning pathways but also interacts with pretraining and fine-tuning priors, leading to large, often unpredictable, swings in performance, robustness, and output style. The theory further asserts that the mapping from prompt format to model behavior is highly non-linear, model-dependent, and sensitive to both surface and semantic features, with certain formats acting as attractors for specific behaviors or failure modes.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt Format Deterministically Alters LLM Output Distribution",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "is_varied",
                        "object": "across semantically-equivalent forms"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_fixed",
                        "object": "model weights and decoding parameters"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_output_distribution",
                        "relation": "changes",
                        "object": "substantially (accuracy, style, robustness, degeneration rate, etc.)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Observed swings up to 76 percentage points in accuracy across prompt formats for the same model and task; prompt-format identifiability in embeddings; high variance in performance with prompt wording, structure, and example selection.",
                        "uuids": [
                            "e5671.0",
                            "e5799.1",
                            "e5685.0",
                            "e5813.3",
                            "e5671.5",
                            "e5798.0",
                            "e5801.7",
                            "e5807.0",
                            "e5807.2",
                            "e5818.9",
                            "e5807.3",
                            "e5813.7",
                            "e5813.9",
                            "e5695.0",
                            "e5695.1",
                            "e5695.4",
                            "e5695.6",
                            "e5695.7",
                            "e5695.9",
                            "e5695.10",
                            "e5695.11",
                            "e5698.3",
                            "e5698.2",
                            "e5698.1",
                            "e5702.6",
                            "e5705.0",
                            "e5705.1",
                            "e5707.1",
                            "e5707.6",
                            "e5712.3",
                            "e5712.5",
                            "e5712.6",
                            "e5712.8",
                            "e5712.10",
                            "e5712.11",
                            "e5712.15",
                            "e5712.23",
                            "e5712.24",
                            "e5712.25",
                            "e5712.26",
                            "e5712.29",
                            "e5712.32",
                            "e5821.0",
                            "e5821.5",
                            "e5821.6",
                            "e5822.2",
                            "e5822.5",
                            "e5822.7",
                            "e5824.0",
                            "e5824.1",
                            "e5825.0",
                            "e5825.6",
                            "e5825.8",
                            "e5825.9",
                            "e5825.10",
                            "e5826.5",
                            "e5828.8",
                            "e5829.4",
                            "e5829.5",
                            "e5830.1",
                            "e5831.3",
                            "e5831.6",
                            "e5831.8",
                            "e5832.5",
                            "e5832.9",
                            "e5833.2",
                            "e5833.5",
                            "e5833.9",
                            "e5834.0",
                            "e5834.2",
                            "e5834.3",
                            "e5835.0",
                            "e5835.7",
                            "e5835.9",
                            "e5836.0",
                            "e5836.4",
                            "e5836.6",
                            "e5836.7",
                            "e5836.8",
                            "e5839.2",
                            "e5839.7",
                            "e5842.2",
                            "e5846.1",
                            "e5846.5",
                            "e5855.1",
                            "e5855.3",
                            "e5855.5",
                            "e5855.6",
                            "e5860.1",
                            "e5862.4",
                            "e5862.7",
                            "e5866.0",
                            "e5866.1",
                            "e5866.5",
                            "e5867.1",
                            "e5867.4",
                            "e5867.5",
                            "e5867.6",
                            "e5869.1",
                            "e5869.3",
                            "e5869.5",
                            "e5874.1",
                            "e5874.7",
                            "e5874.8",
                            "e5874.12",
                            "e5686.1",
                            "e5686.10",
                            "e5686.11",
                            "e5652.0",
                            "e5652.7",
                            "e5659.0",
                            "e5659.4",
                            "e5659.7",
                            "e5662.2",
                            "e5663.3",
                            "e5668.3",
                            "e5669.1",
                            "e5677.4",
                            "e5677.6",
                            "e5685.1",
                            "e5685.4",
                            "e5689.4",
                            "e5690.2",
                            "e5690.8",
                            "e5692.0",
                            "e5692.4",
                            "e5692.7",
                            "e5708.0",
                            "e5708.2",
                            "e5791.0",
                            "e5791.2",
                            "e5791.4",
                            "e5791.6",
                            "e5794.0",
                            "e5795.3",
                            "e5795.4",
                            "e5797.0",
                            "e5798.0",
                            "e5799.0",
                            "e5799.3",
                            "e5801.3",
                            "e5801.6",
                            "e5801.7",
                            "e5804.0",
                            "e5805.4",
                            "e5805.5",
                            "e5806.3",
                            "e5807.0",
                            "e5807.2",
                            "e5807.3",
                            "e5807.5",
                            "e5808.0",
                            "e5808.1",
                            "e5809.5",
                            "e5809.7",
                            "e5809.11",
                            "e5809.13",
                            "e5810.0",
                            "e5810.4",
                            "e5810.6",
                            "e5811.0",
                            "e5811.7",
                            "e5811.8",
                            "e5812.0",
                            "e5813.3",
                            "e5813.7",
                            "e5813.8",
                            "e5813.9",
                            "e5816.0",
                            "e5816.1",
                            "e5816.2",
                            "e5816.3",
                            "e5816.4",
                            "e5816.5",
                            "e5817.0",
                            "e5817.1",
                            "e5817.2",
                            "e5817.4",
                            "e5818.0",
                            "e5818.3",
                            "e5818.9",
                            "e5819.0",
                            "e5820.6"
                        ]
                    },
                    {
                        "text": "Prompt-format identifiability in model embeddings: classifiers can identify prompt format from hidden states with &gt;98% accuracy, and separability correlates with performance spread.",
                        "uuids": [
                            "e5671.5"
                        ]
                    },
                    {
                        "text": "Multi-prompt instability: small changes in instruction templates (wording, punctuation, rephrasing) produce large changes in both absolute accuracy and relative model ranking.",
                        "uuids": [
                            "e5798.0"
                        ]
                    },
                    {
                        "text": "Prompt-format and in-context example sensitivity: nearly all evaluated models are highly sensitive to prompt wording, formatting, choice of in-context examples, and the number of examples, producing large swings in performance.",
                        "uuids": [
                            "e5799.1",
                            "e5685.0",
                            "e5813.3",
                            "e5685.1"
                        ]
                    },
                    {
                        "text": "Manual Wording Sensitivity: small changes in prompt wording can yield large differences in accuracy (e.g., 40.9% to 71.2% on AG News with OPT 175B).",
                        "uuids": [
                            "e5802.6"
                        ]
                    },
                    {
                        "text": "Semantically similar prompts phenomenon: paraphrases or semantically overlapping instructions can have widely varying performance, increasing variance and oscillation during optimization.",
                        "uuids": [
                            "e5818.9"
                        ]
                    },
                    {
                        "text": "Prompt-order sensitivity: the order of in-context (few-shot) examples in a prompt can materially affect LLM outputs and task performance.",
                        "uuids": [
                            "e5685.1"
                        ]
                    },
                    {
                        "text": "Prompt-format sensitivity (Liang et al. 2022): LLM outputs are highly sensitive to prompt formatting, the particular choice and number of in-context examples, and related template design, with substantial variability across models and tasks.",
                        "uuids": [
                            "e5685.0"
                        ]
                    },
                    {
                        "text": "Formatting Sensitivity (Global): semantically-equivalent but differently formatted prompts produce large variability in LLM task performance across many tasks and models, with spreads up to 76 percentage points.",
                        "uuids": [
                            "e5671.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is well-known that prompt wording and format affect LLM outputs, and prompt engineering is a recognized practice.",
                    "what_is_novel": "This law formalizes prompt format as a deterministic, high-dimensional control signal that can be mapped in embedding space, and links format-induced embedding separability to performance spread, suggesting a mechanistic basis for prompt sensitivity.",
                    "classification_explanation": "While prompt sensitivity is recognized, the explicit mapping of prompt format to internal embedding separability and output distribution, and the identification of format as a deterministic control signal, is a novel synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [prompting as control signal]",
                        "Liang et al. (2022) Holistic Evaluation of Language Models [prompt-format sensitivity]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [prompt calibration]",
                        "Zhou et al. (2023) Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design [formatting sensitivity, embedding separability]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Format Interacts Nonlinearly with Model Pretraining and Fine-Tuning Priors",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "matches",
                        "object": "patterns frequent in pretraining/fine-tuning data"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "is_more_likely_to",
                        "object": "produce accurate, robust, and expected outputs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Open-ended QA formats outperform restrictive formats due to higher frequency in pretraining data; observed-instruction templates outperform semantically-correct but unobserved instructions; common-token/label-name bias; instruction-tuned models are more robust to prompt variation but can be less sensitive to semantics.",
                        "uuids": [
                            "e5692.0",
                            "e5808.0",
                            "e5808.1",
                            "e5833.5",
                            "e5820.6",
                            "e5799.3",
                            "e5855.5",
                            "e5813.9",
                            "e5813.7",
                            "e5791.2",
                            "e5791.4",
                            "e5791.6",
                            "e5821.6",
                            "e5707.6"
                        ]
                    },
                    {
                        "text": "Incorrect but observed instructions can outperform correct but unobserved instructions (i.e., observed-inappropriate &gt; unobserved-appropriate) due to distributional familiarity.",
                        "uuids": [
                            "e5808.1"
                        ]
                    },
                    {
                        "text": "Instruction-tuned models (T0 family) are more robust across prompt variations but can become less sensitive to prompt semantics, sometimes producing strong performance even with pathological prompts.",
                        "uuids": [
                            "e5820.6"
                        ]
                    },
                    {
                        "text": "Pretraining objective and prompt alignment: matching prompt shape (cloze/prefix) to pre-training objective improves performance and applicability.",
                        "uuids": [
                            "e5813.9"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering and the importance of matching prompt format to pretraining/fine-tuning distribution is recognized.",
                    "what_is_novel": "This law formalizes the non-linear interaction and predicts that even semantically-correct but distributionally-novel prompts can underperform, and that observed-inappropriate instructions can outperform unobserved-appropriate ones.",
                    "classification_explanation": "The explicit prediction and empirical support for observed-inappropriate &gt; unobserved-appropriate, and the mapping to pretraining/fine-tuning frequency, is a novel synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [instruction tuning]",
                        "Mishra et al. (2022) Cross-task Generalization via Declarative Instructions [prompt diversity]",
                        "Zhou et al. (2023) Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design [formatting sensitivity]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Format Can Induce or Suppress Specific Failure Modes",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "includes",
                        "object": "strict output constraints, rare tokens, or adversarial perturbations"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "is_more_likely_to",
                        "object": "exhibit degeneration, misformatting, or attention misallocation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Strict answer-format enforcement caused high misformatting rates and apparent performance drops; adversarial prompt attacks shift attention away from task-relevant tokens; formatting can increase degeneration (no valid output).",
                        "uuids": [
                            "e5702.6",
                            "e5831.6",
                            "e5671.9",
                            "e5695.9"
                        ]
                    },
                    {
                        "text": "Degeneration / Centered Mass: prompt formatting affects not just accuracy but the frequency a model outputs a valid option at all; strong correlation reported between accuracy and 'centered mass' (ratio of outputs matching any valid option).",
                        "uuids": [
                            "e5671.9"
                        ]
                    },
                    {
                        "text": "Adversarial prompt attacks (character-, word-, sentence-, semantic-level) reduce performance and can cause models to focus on irrelevant or misleading tokens.",
                        "uuids": [
                            "e5695.9",
                            "e5831.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt attacks and degeneration are known phenomena.",
                    "what_is_novel": "This law unifies these as format-induced failure modes and links them to specific prompt features (e.g., strict constraints, adversarial noise) and attention mechanisms.",
                    "classification_explanation": "The explicit mapping from format features to failure modes and attention shifts is a novel synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2020) How Can We Know What Language Models Know? [prompt sensitivity]",
                        "Zhou et al. (2023) Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design [degeneration, format-induced errors]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new prompt format is constructed that closely matches a high-frequency pretraining pattern, even if semantically suboptimal, it will outperform a semantically-perfect but distributionally-novel format.",
        "If prompt formats are adversarially perturbed (e.g., with distractor sentences or rare tokens), attention will shift away from task-relevant content and accuracy will drop.",
        "If prompt formats are sampled and their embeddings are visualized, formats with higher separability will correspond to higher performance spread.",
        "If strict output formatting is enforced on a model not trained for it, misformatting rates and apparent performance drops will increase."
    ],
    "new_predictions_unknown": [
        "If a model is trained with adversarially generated prompt formats as negative examples, it may develop robustness to a broader class of spurious features, potentially at the cost of reduced sensitivity to genuine instruction semantics.",
        "If a prompt format is constructed to be maximally orthogonal (in embedding space) to all training formats, the model may fail catastrophically or default to generic outputs, regardless of semantic content.",
        "If a model is trained to ignore all formatting cues and only attend to semantic content, prompt sensitivity may be reduced but overall performance may also decrease."
    ],
    "negative_experiments": [
        "If a set of semantically-equivalent prompts with highly divergent surface forms all produce identical outputs and accuracy, this would challenge the theory's assertion of high format sensitivity.",
        "If attention heatmaps do not change with adversarial prompt perturbations, or if degeneration rates are unaffected by strict output constraints, this would call into question the law linking format to failure modes.",
        "If observed-inappropriate instructions do not outperform unobserved-appropriate ones in any setting, the non-linear interaction law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where prompt length alone does not affect performance, suggesting that not all surface features are equally impactful.",
            "uuids": [
                "e5810.4"
            ]
        },
        {
            "text": "Null/minimal prompts can sometimes be competitive, indicating that format sensitivity is not universal.",
            "uuids": [
                "e5813.7"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks and models show stable performance across rephrasings or prompt variations, especially for well-known facts or robust domains.",
            "uuids": [
                "e5816.0",
                "e5816.5"
            ]
        }
    ],
    "special_cases": [
        "For tasks with extremely strong domain priors or where the model's knowledge is robust (e.g., common facts), prompt format may have little effect.",
        "Instruction-tuned models may become less sensitive to prompt semantics, producing strong performance even on pathological prompts.",
        "Prompt length and some surface features may have negligible effect in certain settings."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and prompt sensitivity are recognized, and the importance of matching prompt format to pretraining/fine-tuning distribution is known.",
        "what_is_novel": "This theory unifies prompt format as a deterministic, high-dimensional control signal, links embedding separability to performance spread, and formalizes the non-linear, model-dependent mapping from format to output, including failure modes.",
        "classification_explanation": "While the components are individually recognized, the explicit, mechanistic, and predictive synthesis is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [prompting as control signal]",
            "Liang et al. (2022) Holistic Evaluation of Language Models [prompt-format sensitivity]",
            "Zhou et al. (2023) Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design [formatting sensitivity, embedding separability]",
            "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [instruction tuning]"
        ]
    },
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
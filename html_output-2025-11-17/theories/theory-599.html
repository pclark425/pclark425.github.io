<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Latent World-State Representation Emergence in Autoregressive Language Models for Board Games - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-599</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-599</p>
                <p><strong>Name:</strong> Latent World-State Representation Emergence in Autoregressive Language Models for Board Games</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that autoregressive language models (LMs) trained on sequential move data from spatial board games (such as chess, Othello, and Go) develop internal, causally manipulable representations of the underlying world state (e.g., board configuration), even when not explicitly supervised to do so. These representations are nonlinear, distributed, and can be probed or intervened upon to reveal or alter the model's understanding of spatial state, enabling the model to predict legal moves and reason about spatial consequences. The emergence of such representations is robust across different games and architectures (when sufficiently expressive), but depends on the structure and determinism of the training data and the model's capacity for long-range dependencies.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Nonlinear, Causally Manipulable World-State Representations Emerge in Autoregressive LMs (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; autoregressive language model &#8594; is_trained_on &#8594; sequential move data from a spatial board game</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; internal activations &#8594; encode &#8594; nonlinear, distributed representation of the board state<span style="color: #888888;">, and</span></div>
        <div>&#8226; interventions on activations &#8594; can_causally_alter &#8594; the model's move predictions to match counterfactual board states</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Othello-GPT: nonlinear probes decode full board state with low error; gradient-based interventions on activations change move predictions to match counterfactual board states; latent saliency maps highlight spatially-relevant tiles. Nonlinear probe error rates as low as 1.7% (synthetic-trained), and interventions cause model to output legal moves for counterfactual board states. <a href="../results/extraction-result-5048.html#e5048.0" class="evidence-link">[e5048.0]</a> </li>
    <li>Chess LMs (GPT2-chess): probing tasks show high legal-move accuracy and R-Precision, indicating internal tracking of piece locations and move legality. RAP (Randomly Annotated Piece) supervision improves piece-tracking and move legality, and full-attention GPT2 outperforms LSTM and limited-attention models. <a href="../results/extraction-result-5062.html#e5062.0" class="evidence-link">[e5062.0]</a> </li>
    <li>Emergent linear representations in world models (Nanda et al., 2023): explicit game-state features are linearly decodable from model activations, supporting the existence of latent world-state representations. <a href="../results/extraction-result-4818.html#e4818.3" class="evidence-link">[e4818.3]</a> </li>
    <li>Go Transformer: qualitative evidence that transformer-based LMs fine-tuned on Go move sequences generate plausible board states and opening patterns, indicating implicit spatial state tracking. <a href="../results/extraction-result-5058.html#e5058.0" class="evidence-link">[e5058.0]</a> </li>
    <li>Chess Transformer (GPT-2): fine-tuned GPT-2 on PGN chess games generates coherent move sequences, including legal openings, castling, and pawn promotions, with attention-head visualizations showing focus on spatially meaningful tokens. <a href="../results/extraction-result-5078.html#e5078.0" class="evidence-link">[e5078.0]</a> </li>
    <li>Presser & Branwen GPT-2 chess demo: GPT-2 generates chess games with a filter for illegal moves, suggesting the model learns some spatial constraints. <a href="../results/extraction-result-5078.html#e5078.2" class="evidence-link">[e5078.2]</a> </li>
    <li>Chess LM (Toshniwal et al., 2022): GPT-style sequence models trained on chess move data can track board state, indicating that game-board-like spatial state can be represented by language models. <a href="../results/extraction-result-4818.html#e4818.2" class="evidence-link">[e4818.2]</a> </li>
    <li>GPT2-chess: RAP and AP notations improve perplexity and probe metrics, especially for smaller training sets, and full-attention GPT2-small substantially outperforms LSTM and attention-windowed GPT2 on board-state probing. <a href="../results/extraction-result-5062.html#e5062.0" class="evidence-link">[e5062.0]</a> </li>
    <li>Othello-GPT: causal interventions on internal activations at appropriate layers cause the model's next-move predictions to change to match the legal moves for the counterfactual board state, and latent saliency maps highlight the exact tiles required for legality. <a href="../results/extraction-result-5048.html#e5048.0" class="evidence-link">[e5048.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> While related to prior work on probing and interpretability, the explicit demonstration of causal intervention and the generalization across board games and architectures is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Emergent representations and probing in neural networks are established, and Othello-GPT demonstrated nonlinear, causally manipulable world-state representations. Linear decodability of world state has been shown in prior work.</p>            <p><strong>What is Novel:</strong> This law asserts that such representations are not only decodable but causally manipulable, and that they emerge robustly in autoregressive LMs trained on spatial move data across multiple games. It synthesizes evidence from multiple domains and highlights the causal aspect.</p>
            <p><strong>References:</strong> <ul>
    <li>Li et al. (2022) Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task [Othello-GPT, nonlinear/causal world-state representations]</li>
    <li>Nanda et al. (2023) Emergent linear representations in world models of self-supervised sequence models [Linear decodability of world state]</li>
    <li>Toshniwal et al. (2021) Chess as a testbed for language model state tracking [Board state tracking in LMs]</li>
    <li>Presser & Branwen (2020) A Very Unlikely Chess Game [GPT-2 chess generation]</li>
    <li>Kirkpatrick et al. (2021) The Go Transformer: Natural Language Modeling for Game Play [Transformer LMs for Go]</li>
</ul>
            <h3>Statement 1: Emergence of World-State Representations Depends on Model Architecture and Data Structure (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; autoregressive language model &#8594; is_trained_on &#8594; sequential move data from a spatial board game<span style="color: #888888;">, and</span></div>
        <div>&#8226; model architecture &#8594; has_property &#8594; sufficient capacity for long-range dependencies (e.g., full attention, deep transformer)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; internal activations &#8594; encode &#8594; accurate world-state representations</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GPT2-chess: full-attention GPT2-small substantially outperforms LSTM and attention-windowed GPT2 on board-state probing; LSTM and limited-attention models fail to track board state. <a href="../results/extraction-result-5062.html#e5062.0" class="evidence-link">[e5062.0]</a> </li>
    <li>Othello-GPT: nonlinear probes succeed, but linear probes perform poorly, indicating a nonlinear internal encoding; untrained/randomized models do not yield accurate world-state representations. <a href="../results/extraction-result-5048.html#e5048.0" class="evidence-link">[e5048.0]</a> </li>
    <li>Go Transformer: transformer-based LMs generate plausible board states, but no evidence is provided for LSTM or other architectures. <a href="../results/extraction-result-5058.html#e5058.0" class="evidence-link">[e5058.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The dependence on architecture is established, but the explicit link to world-state representation emergence in spatial board games is a novel, domain-specific synthesis.</p>            <p><strong>What Already Exists:</strong> It is known that model architecture and capacity affect the ability to learn long-range dependencies.</p>            <p><strong>What is Novel:</strong> This law specifically ties the emergence of world-state representations in spatial board games to the need for sufficient model capacity and attention span, and demonstrates failure in LSTM/limited-attention models.</p>
            <p><strong>References:</strong> <ul>
    <li>Toshniwal et al. (2021) Chess as a testbed for language model state tracking [Architecture dependence]</li>
    <li>Li et al. (2022) Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task [Othello-GPT, nonlinear/causal world-state representations]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new autoregressive LM is trained on a different spatial board game (e.g., Shogi or Checkers), nonlinear probes will be able to decode the full board state from internal activations.</li>
                <li>If gradient-based interventions are applied to the activations of such a model, the model's move predictions will change to match the legal moves for the counterfactual board state.</li>
                <li>If a transformer LM is trained on move sequences with explicit piece-type annotations (as in RAP), piece-tracking and move legality will improve, especially in low-data regimes.</li>
                <li>If a model is trained on move data with full attention and sufficient depth, it will outperform LSTM or limited-attention models on board-state tracking and legal-move prediction.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a sufficiently large LM is trained on multi-game data (e.g., chess and Go), it may develop a shared latent space encoding for spatial state that generalizes across games, potentially enabling transfer learning between games.</li>
                <li>If the model is trained with partial observability or noisy move data, the emergence and manipulability of world-state representations may degrade or become more robust, depending on the training regime.</li>
                <li>If a model is trained on games with stochastic or hidden information (e.g., Stratego), the nature of the latent world-state representations may change, possibly encoding distributions over possible states.</li>
                <li>If a model is trained on natural language commentary interleaved with move data, it may develop joint representations linking spatial state and linguistic descriptions.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If nonlinear probes fail to decode board state from internal activations in an autoregressive LM trained on move data, this would challenge the theory.</li>
                <li>If interventions on activations do not causally alter move predictions to match counterfactual board states, the law would be called into question.</li>
                <li>If a model with sufficient capacity and attention span fails to develop world-state representations when trained on spatial move data, the theory would be falsified.</li>
                <li>If a model trained on move data for a new game (e.g., Shogi) does not yield decodable or manipulable world-state representations, the generality of the theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Models trained on natural language or mixed data may not develop such explicit world-state representations, or may do so only for highly frequent or structured domains. For example, LLMs trained on general text may not encode spatial state for rare or ambiguous games. <a href="../results/extraction-result-4832.html#e4832.0" class="evidence-link">[e4832.0]</a> <a href="../results/extraction-result-5071.html#e5071.0" class="evidence-link">[e5071.0]</a> </li>
    <li>In games with partial observability or stochasticity, the nature of the latent world-state representations is not addressed by the current evidence. </li>
    <li>No direct evidence is provided for the emergence of such representations in models trained on games with continuous spatial state (e.g., physical simulation or robotics). </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Closely related to existing work on probing and interpretability, but the explicit causal aspect, cross-game generalization, and architectural boundary conditions are a novel, impactful synthesis.</p>
            <p><strong>References:</strong> <ul>
    <li>Li et al. (2022) Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task [Othello-GPT, nonlinear/causal world-state representations]</li>
    <li>Nanda et al. (2023) Emergent linear representations in world models of self-supervised sequence models [Linear decodability of world state]</li>
    <li>Toshniwal et al. (2021) Chess as a testbed for language model state tracking [Board state tracking in LMs]</li>
    <li>Presser & Branwen (2020) A Very Unlikely Chess Game [GPT-2 chess generation]</li>
    <li>Kirkpatrick et al. (2021) The Go Transformer: Natural Language Modeling for Game Play [Transformer LMs for Go]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Latent World-State Representation Emergence in Autoregressive Language Models for Board Games",
    "theory_description": "This theory asserts that autoregressive language models (LMs) trained on sequential move data from spatial board games (such as chess, Othello, and Go) develop internal, causally manipulable representations of the underlying world state (e.g., board configuration), even when not explicitly supervised to do so. These representations are nonlinear, distributed, and can be probed or intervened upon to reveal or alter the model's understanding of spatial state, enabling the model to predict legal moves and reason about spatial consequences. The emergence of such representations is robust across different games and architectures (when sufficiently expressive), but depends on the structure and determinism of the training data and the model's capacity for long-range dependencies.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Nonlinear, Causally Manipulable World-State Representations Emerge in Autoregressive LMs",
                "if": [
                    {
                        "subject": "autoregressive language model",
                        "relation": "is_trained_on",
                        "object": "sequential move data from a spatial board game"
                    }
                ],
                "then": [
                    {
                        "subject": "internal activations",
                        "relation": "encode",
                        "object": "nonlinear, distributed representation of the board state"
                    },
                    {
                        "subject": "interventions on activations",
                        "relation": "can_causally_alter",
                        "object": "the model's move predictions to match counterfactual board states"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Othello-GPT: nonlinear probes decode full board state with low error; gradient-based interventions on activations change move predictions to match counterfactual board states; latent saliency maps highlight spatially-relevant tiles. Nonlinear probe error rates as low as 1.7% (synthetic-trained), and interventions cause model to output legal moves for counterfactual board states.",
                        "uuids": [
                            "e5048.0"
                        ]
                    },
                    {
                        "text": "Chess LMs (GPT2-chess): probing tasks show high legal-move accuracy and R-Precision, indicating internal tracking of piece locations and move legality. RAP (Randomly Annotated Piece) supervision improves piece-tracking and move legality, and full-attention GPT2 outperforms LSTM and limited-attention models.",
                        "uuids": [
                            "e5062.0"
                        ]
                    },
                    {
                        "text": "Emergent linear representations in world models (Nanda et al., 2023): explicit game-state features are linearly decodable from model activations, supporting the existence of latent world-state representations.",
                        "uuids": [
                            "e4818.3"
                        ]
                    },
                    {
                        "text": "Go Transformer: qualitative evidence that transformer-based LMs fine-tuned on Go move sequences generate plausible board states and opening patterns, indicating implicit spatial state tracking.",
                        "uuids": [
                            "e5058.0"
                        ]
                    },
                    {
                        "text": "Chess Transformer (GPT-2): fine-tuned GPT-2 on PGN chess games generates coherent move sequences, including legal openings, castling, and pawn promotions, with attention-head visualizations showing focus on spatially meaningful tokens.",
                        "uuids": [
                            "e5078.0"
                        ]
                    },
                    {
                        "text": "Presser & Branwen GPT-2 chess demo: GPT-2 generates chess games with a filter for illegal moves, suggesting the model learns some spatial constraints.",
                        "uuids": [
                            "e5078.2"
                        ]
                    },
                    {
                        "text": "Chess LM (Toshniwal et al., 2022): GPT-style sequence models trained on chess move data can track board state, indicating that game-board-like spatial state can be represented by language models.",
                        "uuids": [
                            "e4818.2"
                        ]
                    },
                    {
                        "text": "GPT2-chess: RAP and AP notations improve perplexity and probe metrics, especially for smaller training sets, and full-attention GPT2-small substantially outperforms LSTM and attention-windowed GPT2 on board-state probing.",
                        "uuids": [
                            "e5062.0"
                        ]
                    },
                    {
                        "text": "Othello-GPT: causal interventions on internal activations at appropriate layers cause the model's next-move predictions to change to match the legal moves for the counterfactual board state, and latent saliency maps highlight the exact tiles required for legality.",
                        "uuids": [
                            "e5048.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Emergent representations and probing in neural networks are established, and Othello-GPT demonstrated nonlinear, causally manipulable world-state representations. Linear decodability of world state has been shown in prior work.",
                    "what_is_novel": "This law asserts that such representations are not only decodable but causally manipulable, and that they emerge robustly in autoregressive LMs trained on spatial move data across multiple games. It synthesizes evidence from multiple domains and highlights the causal aspect.",
                    "classification_explanation": "While related to prior work on probing and interpretability, the explicit demonstration of causal intervention and the generalization across board games and architectures is a novel synthesis.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Li et al. (2022) Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task [Othello-GPT, nonlinear/causal world-state representations]",
                        "Nanda et al. (2023) Emergent linear representations in world models of self-supervised sequence models [Linear decodability of world state]",
                        "Toshniwal et al. (2021) Chess as a testbed for language model state tracking [Board state tracking in LMs]",
                        "Presser & Branwen (2020) A Very Unlikely Chess Game [GPT-2 chess generation]",
                        "Kirkpatrick et al. (2021) The Go Transformer: Natural Language Modeling for Game Play [Transformer LMs for Go]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Emergence of World-State Representations Depends on Model Architecture and Data Structure",
                "if": [
                    {
                        "subject": "autoregressive language model",
                        "relation": "is_trained_on",
                        "object": "sequential move data from a spatial board game"
                    },
                    {
                        "subject": "model architecture",
                        "relation": "has_property",
                        "object": "sufficient capacity for long-range dependencies (e.g., full attention, deep transformer)"
                    }
                ],
                "then": [
                    {
                        "subject": "internal activations",
                        "relation": "encode",
                        "object": "accurate world-state representations"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GPT2-chess: full-attention GPT2-small substantially outperforms LSTM and attention-windowed GPT2 on board-state probing; LSTM and limited-attention models fail to track board state.",
                        "uuids": [
                            "e5062.0"
                        ]
                    },
                    {
                        "text": "Othello-GPT: nonlinear probes succeed, but linear probes perform poorly, indicating a nonlinear internal encoding; untrained/randomized models do not yield accurate world-state representations.",
                        "uuids": [
                            "e5048.0"
                        ]
                    },
                    {
                        "text": "Go Transformer: transformer-based LMs generate plausible board states, but no evidence is provided for LSTM or other architectures.",
                        "uuids": [
                            "e5058.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that model architecture and capacity affect the ability to learn long-range dependencies.",
                    "what_is_novel": "This law specifically ties the emergence of world-state representations in spatial board games to the need for sufficient model capacity and attention span, and demonstrates failure in LSTM/limited-attention models.",
                    "classification_explanation": "The dependence on architecture is established, but the explicit link to world-state representation emergence in spatial board games is a novel, domain-specific synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Toshniwal et al. (2021) Chess as a testbed for language model state tracking [Architecture dependence]",
                        "Li et al. (2022) Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task [Othello-GPT, nonlinear/causal world-state representations]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new autoregressive LM is trained on a different spatial board game (e.g., Shogi or Checkers), nonlinear probes will be able to decode the full board state from internal activations.",
        "If gradient-based interventions are applied to the activations of such a model, the model's move predictions will change to match the legal moves for the counterfactual board state.",
        "If a transformer LM is trained on move sequences with explicit piece-type annotations (as in RAP), piece-tracking and move legality will improve, especially in low-data regimes.",
        "If a model is trained on move data with full attention and sufficient depth, it will outperform LSTM or limited-attention models on board-state tracking and legal-move prediction."
    ],
    "new_predictions_unknown": [
        "If a sufficiently large LM is trained on multi-game data (e.g., chess and Go), it may develop a shared latent space encoding for spatial state that generalizes across games, potentially enabling transfer learning between games.",
        "If the model is trained with partial observability or noisy move data, the emergence and manipulability of world-state representations may degrade or become more robust, depending on the training regime.",
        "If a model is trained on games with stochastic or hidden information (e.g., Stratego), the nature of the latent world-state representations may change, possibly encoding distributions over possible states.",
        "If a model is trained on natural language commentary interleaved with move data, it may develop joint representations linking spatial state and linguistic descriptions."
    ],
    "negative_experiments": [
        "If nonlinear probes fail to decode board state from internal activations in an autoregressive LM trained on move data, this would challenge the theory.",
        "If interventions on activations do not causally alter move predictions to match counterfactual board states, the law would be called into question.",
        "If a model with sufficient capacity and attention span fails to develop world-state representations when trained on spatial move data, the theory would be falsified.",
        "If a model trained on move data for a new game (e.g., Shogi) does not yield decodable or manipulable world-state representations, the generality of the theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Models trained on natural language or mixed data may not develop such explicit world-state representations, or may do so only for highly frequent or structured domains. For example, LLMs trained on general text may not encode spatial state for rare or ambiguous games.",
            "uuids": [
                "e4832.0",
                "e5071.0"
            ]
        },
        {
            "text": "In games with partial observability or stochasticity, the nature of the latent world-state representations is not addressed by the current evidence.",
            "uuids": []
        },
        {
            "text": "No direct evidence is provided for the emergence of such representations in models trained on games with continuous spatial state (e.g., physical simulation or robotics).",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "LSTMs or limited-attention models trained on move data fail to track board state, indicating that architecture and training regime are critical for the emergence of world-state representations.",
            "uuids": [
                "e5062.0"
            ]
        },
        {
            "text": "GPT-2 (Sudoku) and GPT-2 (Rubik's Cube) trained on string encodings of spatial puzzles produce outputs with repeated digits or low solution rates, suggesting that string-based or insufficiently structured encodings may impair the emergence of robust spatial representations.",
            "uuids": [
                "e4792.1",
                "e4792.0"
            ]
        }
    ],
    "special_cases": [
        "If the move data is ambiguous or non-deterministic (e.g., incomplete move notation, missing information), the emergence of world-state representations may be impaired.",
        "If the model is trained with explicit board-state supervision (e.g., board images or state vectors), representations may become more linear and interpretable.",
        "If the model is trained on games with hidden information or stochasticity, the representations may encode distributions or beliefs rather than deterministic states.",
        "If the model is trained on very small datasets or with insufficient capacity, world-state representations may not emerge or may be fragile."
    ],
    "existing_theory": {
        "what_already_exists": "Emergent representations and probing in neural networks are established, and Othello-GPT demonstrated nonlinear, causally manipulable world-state representations. Linear decodability of world state has been shown in prior work. Architecture dependence for long-range dependencies is also established.",
        "what_is_novel": "The explicit causal intervention, the generalization across multiple board games, and the prediction of robust emergence of world-state representations in autoregressive LMs trained on spatial move data is a novel synthesis. The theory also predicts the conditions (architecture, data structure) under which such representations will or will not emerge.",
        "classification_explanation": "Closely related to existing work on probing and interpretability, but the explicit causal aspect, cross-game generalization, and architectural boundary conditions are a novel, impactful synthesis.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Li et al. (2022) Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task [Othello-GPT, nonlinear/causal world-state representations]",
            "Nanda et al. (2023) Emergent linear representations in world models of self-supervised sequence models [Linear decodability of world state]",
            "Toshniwal et al. (2021) Chess as a testbed for language model state tracking [Board state tracking in LMs]",
            "Presser & Branwen (2020) A Very Unlikely Chess Game [GPT-2 chess generation]",
            "Kirkpatrick et al. (2021) The Go Transformer: Natural Language Modeling for Game Play [Transformer LMs for Go]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Iterative Abstraction-Refinement Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-2107</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-2107</p>
                <p><strong>Name:</strong> Iterative Abstraction-Refinement Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can be used to distill theories from large numbers of scholarly papers, given a specific topic or query.</p>
                <p><strong>Description:</strong> This theory posits that LLMs distill theories from large scholarly corpora through an iterative process of abstraction and refinement. Initially, the LLM abstracts broad patterns and candidate laws from the corpus, then refines these abstractions by recursively querying, retrieving, and integrating more specific evidence, guided by user prompts and internal uncertainty estimates. This process enables the LLM to converge on theory statements that balance generality and specificity, and to adaptively adjust the level of abstraction in response to new evidence or prompt constraints.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Initial Abstraction from Corpus (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_given &#8594; large scholarly corpus<span style="color: #888888;">, and</span></div>
        <div>&#8226; user_query &#8594; is_provided_to &#8594; LLM</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; generates &#8594; broad candidate theory statements</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are capable of summarizing and abstracting high-level patterns from large text corpora, as shown in scientific summarization and review generation tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While summarization is established, its explicit role as the initial abstraction in theory distillation is new.</p>            <p><strong>What Already Exists:</strong> LLMs are known to generate summaries and high-level abstractions from large text inputs.</p>            <p><strong>What is Novel:</strong> The formalization of this as the first step in an iterative theory distillation process is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Cachola et al. (2020) TLDR: Extreme Summarization of Scientific Documents [LLM summarization]</li>
    <li>Shen et al. (2023) Large Language Models as Scientific Reviewers [LLM abstraction in review tasks]</li>
</ul>
            <h3>Statement 1: Recursive Refinement via Evidence Integration (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_candidate_theory_statements &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; detects &#8594; uncertainty or ambiguity in candidate statements</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; retrieves &#8594; additional evidence from corpus<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; refines &#8594; theory statements to increase specificity or resolve ambiguity</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can be prompted to iteratively refine outputs based on new evidence or clarification, as seen in chain-of-thought and self-refinement prompting. </li>
    <li>Recent work shows LLMs can self-critique and revise outputs, improving factuality and specificity. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Related to self-refinement, but its formalization as a law in theory distillation is new.</p>            <p><strong>What Already Exists:</strong> Iterative refinement and self-critique are emerging in LLM research.</p>            <p><strong>What is Novel:</strong> The explicit framing of recursive evidence integration as a core mechanism for theory distillation is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [LLM self-refinement]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [iterative reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM is prompted to generate a theory and then asked to refine it with additional evidence, the resulting theory statements will become more specific and accurate.</li>
                <li>If the LLM is given a corpus with both broad and narrow evidence, it will first generate general statements and then refine them as more specific evidence is retrieved.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may be able to autonomously identify when further refinement is unnecessary, converging on an optimal level of theory abstraction.</li>
                <li>Iterative abstraction-refinement may enable LLMs to discover novel, emergent theories not explicitly present in the corpus.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs fail to improve theory specificity or accuracy after iterative refinement, the theory is called into question.</li>
                <li>If LLMs cannot abstract broad patterns from large corpora, the initial abstraction law is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of corpus quality or bias on the abstraction-refinement process is not fully addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While related to summarization and self-refinement, the abstraction-refinement cycle for theory distillation is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Cachola et al. (2020) TLDR: Extreme Summarization of Scientific Documents [summarization]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [self-refinement]</li>
    <li>Shen et al. (2023) Large Language Models as Scientific Reviewers [LLM abstraction in review tasks]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Iterative Abstraction-Refinement Theory",
    "theory_description": "This theory posits that LLMs distill theories from large scholarly corpora through an iterative process of abstraction and refinement. Initially, the LLM abstracts broad patterns and candidate laws from the corpus, then refines these abstractions by recursively querying, retrieving, and integrating more specific evidence, guided by user prompts and internal uncertainty estimates. This process enables the LLM to converge on theory statements that balance generality and specificity, and to adaptively adjust the level of abstraction in response to new evidence or prompt constraints.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Initial Abstraction from Corpus",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_given",
                        "object": "large scholarly corpus"
                    },
                    {
                        "subject": "user_query",
                        "relation": "is_provided_to",
                        "object": "LLM"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "generates",
                        "object": "broad candidate theory statements"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are capable of summarizing and abstracting high-level patterns from large text corpora, as shown in scientific summarization and review generation tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to generate summaries and high-level abstractions from large text inputs.",
                    "what_is_novel": "The formalization of this as the first step in an iterative theory distillation process is novel.",
                    "classification_explanation": "While summarization is established, its explicit role as the initial abstraction in theory distillation is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Cachola et al. (2020) TLDR: Extreme Summarization of Scientific Documents [LLM summarization]",
                        "Shen et al. (2023) Large Language Models as Scientific Reviewers [LLM abstraction in review tasks]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Recursive Refinement via Evidence Integration",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_candidate_theory_statements",
                        "object": "True"
                    },
                    {
                        "subject": "LLM",
                        "relation": "detects",
                        "object": "uncertainty or ambiguity in candidate statements"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "retrieves",
                        "object": "additional evidence from corpus"
                    },
                    {
                        "subject": "LLM",
                        "relation": "refines",
                        "object": "theory statements to increase specificity or resolve ambiguity"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can be prompted to iteratively refine outputs based on new evidence or clarification, as seen in chain-of-thought and self-refinement prompting.",
                        "uuids": []
                    },
                    {
                        "text": "Recent work shows LLMs can self-critique and revise outputs, improving factuality and specificity.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Iterative refinement and self-critique are emerging in LLM research.",
                    "what_is_novel": "The explicit framing of recursive evidence integration as a core mechanism for theory distillation is novel.",
                    "classification_explanation": "Related to self-refinement, but its formalization as a law in theory distillation is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [LLM self-refinement]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [iterative reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM is prompted to generate a theory and then asked to refine it with additional evidence, the resulting theory statements will become more specific and accurate.",
        "If the LLM is given a corpus with both broad and narrow evidence, it will first generate general statements and then refine them as more specific evidence is retrieved."
    ],
    "new_predictions_unknown": [
        "LLMs may be able to autonomously identify when further refinement is unnecessary, converging on an optimal level of theory abstraction.",
        "Iterative abstraction-refinement may enable LLMs to discover novel, emergent theories not explicitly present in the corpus."
    ],
    "negative_experiments": [
        "If LLMs fail to improve theory specificity or accuracy after iterative refinement, the theory is called into question.",
        "If LLMs cannot abstract broad patterns from large corpora, the initial abstraction law is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of corpus quality or bias on the abstraction-refinement process is not fully addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs may hallucinate or introduce errors during refinement, leading to less accurate theories.",
            "uuids": []
        }
    ],
    "special_cases": [
        "If the corpus is highly homogeneous, refinement may not yield more specific or novel theories.",
        "In cases of conflicting evidence, the LLM may oscillate between competing refinements."
    ],
    "existing_theory": {
        "what_already_exists": "LLM summarization and iterative refinement are established, but not as a unified theory distillation process.",
        "what_is_novel": "The explicit, iterative abstraction-refinement cycle as a mechanism for LLM-driven theory distillation is novel.",
        "classification_explanation": "While related to summarization and self-refinement, the abstraction-refinement cycle for theory distillation is new.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Cachola et al. (2020) TLDR: Extreme Summarization of Scientific Documents [summarization]",
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [self-refinement]",
            "Shen et al. (2023) Large Language Models as Scientific Reviewers [LLM abstraction in review tasks]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can be used to distill theories from large numbers of scholarly papers, given a specific topic or query.",
    "original_theory_id": "theory-667",
    "original_theory_name": "Iterative Retrieval-Augmented Synthesis Theory (IRAST)",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probabilistic Consistency Theory for LM-based Anomaly Detection - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1778</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1778</p>
                <p><strong>Name:</strong> Probabilistic Consistency Theory for LM-based Anomaly Detection</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can be used for detecting anomalies in lists of data.</p>
                <p><strong>Description:</strong> Language models (LMs) can be used to detect anomalies in lists of data by modeling the probability distribution of list items and identifying items that are inconsistent with the learned distribution. This theory posits that LMs, when exposed to a list, implicitly learn the joint or conditional probability structure of the list, and items with low likelihood under this structure are flagged as anomalies.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Low Likelihood Anomaly Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LM &#8594; is_applied_to &#8594; data_list<span style="color: #888888;">, and</span></div>
        <div>&#8226; item_i &#8594; is_in &#8594; data_list<span style="color: #888888;">, and</span></div>
        <div>&#8226; LM &#8594; assigns_probability &#8594; P(item_i | data_list \ item_i)<span style="color: #888888;">, and</span></div>
        <div>&#8226; P(item_i | data_list \ item_i) &#8594; is_less_than &#8594; threshold_p</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; item_i &#8594; is_flagged_as &#8594; anomaly</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Language models assign probabilities to tokens or sequences; low-probability items are often out-of-distribution. </li>
    <li>Anomaly detection via likelihood estimation is a standard approach in probabilistic modeling. </li>
    <li>LMs can be prompted to predict the next item in a list, and low-likelihood completions are often semantically or contextually anomalous. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This law is closely-related-to-existing work, but the extension to arbitrary list structure and explicit use of LM conditional probability is a novel application.</p>            <p><strong>What Already Exists:</strong> Likelihood-based anomaly detection is well-established in statistics and machine learning, and LMs are known to assign probabilities to sequences.</p>            <p><strong>What is Novel:</strong> Applying this to arbitrary lists (not just text) using LMs' learned structure, and using conditional probability over the list context, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Bishop (2006) Pattern Recognition and Machine Learning [Likelihood-based anomaly detection]</li>
    <li>Radford et al. (2019) Language Models are Unsupervised Multitask Learners [LMs assign probabilities to sequences]</li>
    <li>Hendrycks & Gimpel (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [Likelihood for OOD detection]</li>
</ul>
            <h3>Statement 1: Contextual Consistency Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LM &#8594; is_applied_to &#8594; data_list<span style="color: #888888;">, and</span></div>
        <div>&#8226; item_i &#8594; is_in &#8594; data_list<span style="color: #888888;">, and</span></div>
        <div>&#8226; item_i &#8594; is_inconsistent_with &#8594; context_of(data_list \ item_i)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; item_i &#8594; is_flagged_as &#8594; anomaly</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs can be prompted to judge the fit of an item in a list context, and can identify items that do not semantically or syntactically fit. </li>
    <li>Human anomaly detection in lists often relies on contextual inconsistency, which LMs can model. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law is somewhat-related-to-existing, as it adapts contextual anomaly detection to the LM paradigm.</p>            <p><strong>What Already Exists:</strong> Contextual anomaly detection is a known concept in statistics and cognitive science.</p>            <p><strong>What is Novel:</strong> Explicitly leveraging LMs' contextual modeling for arbitrary list anomaly detection is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Chandola et al. (2009) Anomaly Detection: A Survey [Contextual anomaly detection]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [LMs model context for prediction]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a list contains items that are semantically or statistically inconsistent with the rest, LMs will assign them lower probabilities and flag them as anomalies.</li>
                <li>If the LM is fine-tuned on a specific domain, its anomaly detection performance on domain-specific lists will improve.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If the list contains subtle anomalies that require world knowledge, it is unknown whether the LM can reliably detect them.</li>
                <li>If the LM is exposed to adversarially constructed lists, its ability to detect anomalies may degrade unpredictably.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs assign high probability to known anomalies, the theory would be challenged.</li>
                <li>If LMs fail to flag contextually inconsistent items as anomalies, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Lists where all items are rare or out-of-distribution for the LM, making probability assignments unreliable. </li>
    <li>Cases where anomalies are only detectable with external knowledge not present in the LM's training data. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is closely-related-to-existing, but the application to arbitrary lists and explicit use of LM context is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Bishop (2006) Pattern Recognition and Machine Learning [Likelihood-based anomaly detection]</li>
    <li>Chandola et al. (2009) Anomaly Detection: A Survey [Contextual anomaly detection]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [LMs model context for prediction]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Probabilistic Consistency Theory for LM-based Anomaly Detection",
    "theory_description": "Language models (LMs) can be used to detect anomalies in lists of data by modeling the probability distribution of list items and identifying items that are inconsistent with the learned distribution. This theory posits that LMs, when exposed to a list, implicitly learn the joint or conditional probability structure of the list, and items with low likelihood under this structure are flagged as anomalies.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Low Likelihood Anomaly Law",
                "if": [
                    {
                        "subject": "LM",
                        "relation": "is_applied_to",
                        "object": "data_list"
                    },
                    {
                        "subject": "item_i",
                        "relation": "is_in",
                        "object": "data_list"
                    },
                    {
                        "subject": "LM",
                        "relation": "assigns_probability",
                        "object": "P(item_i | data_list \\ item_i)"
                    },
                    {
                        "subject": "P(item_i | data_list \\ item_i)",
                        "relation": "is_less_than",
                        "object": "threshold_p"
                    }
                ],
                "then": [
                    {
                        "subject": "item_i",
                        "relation": "is_flagged_as",
                        "object": "anomaly"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Language models assign probabilities to tokens or sequences; low-probability items are often out-of-distribution.",
                        "uuids": []
                    },
                    {
                        "text": "Anomaly detection via likelihood estimation is a standard approach in probabilistic modeling.",
                        "uuids": []
                    },
                    {
                        "text": "LMs can be prompted to predict the next item in a list, and low-likelihood completions are often semantically or contextually anomalous.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Likelihood-based anomaly detection is well-established in statistics and machine learning, and LMs are known to assign probabilities to sequences.",
                    "what_is_novel": "Applying this to arbitrary lists (not just text) using LMs' learned structure, and using conditional probability over the list context, is novel.",
                    "classification_explanation": "This law is closely-related-to-existing work, but the extension to arbitrary list structure and explicit use of LM conditional probability is a novel application.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Bishop (2006) Pattern Recognition and Machine Learning [Likelihood-based anomaly detection]",
                        "Radford et al. (2019) Language Models are Unsupervised Multitask Learners [LMs assign probabilities to sequences]",
                        "Hendrycks & Gimpel (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [Likelihood for OOD detection]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Contextual Consistency Law",
                "if": [
                    {
                        "subject": "LM",
                        "relation": "is_applied_to",
                        "object": "data_list"
                    },
                    {
                        "subject": "item_i",
                        "relation": "is_in",
                        "object": "data_list"
                    },
                    {
                        "subject": "item_i",
                        "relation": "is_inconsistent_with",
                        "object": "context_of(data_list \\ item_i)"
                    }
                ],
                "then": [
                    {
                        "subject": "item_i",
                        "relation": "is_flagged_as",
                        "object": "anomaly"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs can be prompted to judge the fit of an item in a list context, and can identify items that do not semantically or syntactically fit.",
                        "uuids": []
                    },
                    {
                        "text": "Human anomaly detection in lists often relies on contextual inconsistency, which LMs can model.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Contextual anomaly detection is a known concept in statistics and cognitive science.",
                    "what_is_novel": "Explicitly leveraging LMs' contextual modeling for arbitrary list anomaly detection is novel.",
                    "classification_explanation": "This law is somewhat-related-to-existing, as it adapts contextual anomaly detection to the LM paradigm.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Chandola et al. (2009) Anomaly Detection: A Survey [Contextual anomaly detection]",
                        "Brown et al. (2020) Language Models are Few-Shot Learners [LMs model context for prediction]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a list contains items that are semantically or statistically inconsistent with the rest, LMs will assign them lower probabilities and flag them as anomalies.",
        "If the LM is fine-tuned on a specific domain, its anomaly detection performance on domain-specific lists will improve."
    ],
    "new_predictions_unknown": [
        "If the list contains subtle anomalies that require world knowledge, it is unknown whether the LM can reliably detect them.",
        "If the LM is exposed to adversarially constructed lists, its ability to detect anomalies may degrade unpredictably."
    ],
    "negative_experiments": [
        "If LMs assign high probability to known anomalies, the theory would be challenged.",
        "If LMs fail to flag contextually inconsistent items as anomalies, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Lists where all items are rare or out-of-distribution for the LM, making probability assignments unreliable.",
            "uuids": []
        },
        {
            "text": "Cases where anomalies are only detectable with external knowledge not present in the LM's training data.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "LMs sometimes assign high probability to nonsensical or adversarially constructed items.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Lists with multiple valid modes or clusters may require more sophisticated modeling than a single probability threshold.",
        "If the LM is not trained on the relevant data domain, anomaly detection may be unreliable."
    ],
    "existing_theory": {
        "what_already_exists": "Likelihood and contextual anomaly detection are established in statistics and ML.",
        "what_is_novel": "Explicit use of LMs for arbitrary list anomaly detection via probabilistic and contextual modeling is novel.",
        "classification_explanation": "The theory is closely-related-to-existing, but the application to arbitrary lists and explicit use of LM context is novel.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Bishop (2006) Pattern Recognition and Machine Learning [Likelihood-based anomaly detection]",
            "Chandola et al. (2009) Anomaly Detection: A Survey [Contextual anomaly detection]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [LMs model context for prediction]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can be used for detecting anomalies in lists of data.",
    "original_theory_id": "theory-645",
    "original_theory_name": "Chain-of-Thought and Domain-Knowledge Prompting Enhances LLM Interpretability and Anomaly-Type Classification",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
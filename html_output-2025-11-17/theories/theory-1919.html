<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1919</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1919</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the effectiveness of LLM performance is determined by the degree to which the problem presentation format aligns with the LLM's internal cognitive priors, which are shaped by its pretraining data and architecture. Formats that closely match the distributional and structural patterns seen during pretraining facilitate more accurate and robust reasoning, while misaligned formats induce higher error rates and less reliable outputs.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Prior Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; matches &#8594; LLM_pretraining_distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; higher_accuracy_and_robustness</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on tasks presented in formats similar to those seen in their pretraining data (e.g., question-answer pairs, Wikipedia-style text). </li>
    <li>Prompt engineering that mimics naturalistic or familiar formats improves LLM output quality. </li>
    <li>LLMs struggle with novel or adversarial formats that deviate from pretraining distributions. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt sensitivity is established, the formalization of format-prior alignment as a predictive law is novel.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs are sensitive to prompt format and that pretraining data influences model behavior.</p>            <p><strong>What is Novel:</strong> The explicit law that performance is a function of alignment between presentation format and pretraining distribution is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity, pretraining effects]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format effects]</li>
</ul>
            <h3>Statement 1: Format-Induced Cognitive Load Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_complex_or_unfamiliar &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; increased_error_rate_and_variability</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs show higher error rates on tasks with unfamiliar or convoluted formats. </li>
    <li>Complex or multi-part prompts can overwhelm LLMs, leading to incomplete or inconsistent outputs. </li>
    <li>Prompt simplification and standardization reduce LLM error rates. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect is known, but the cognitive load framing and explicit law are novel.</p>            <p><strong>What Already Exists:</strong> Prompt complexity and unfamiliarity are known to degrade LLM performance.</p>            <p><strong>What is Novel:</strong> The law frames this as a cognitive load effect induced by format misalignment.</p>
            <p><strong>References:</strong> <ul>
    <li>Webson & Pavlick (2022) Prompt Engineering for Few-Shot Learning: Beyond the Template [Prompt complexity effects]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new LLM is trained on a corpus with a specific problem format, it will perform best on tasks presented in that format.</li>
                <li>If a problem is reformatted to match a familiar pretraining style, LLM performance will improve.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are exposed to a curriculum of diverse formats during pretraining, they may generalize better to novel formats.</li>
                <li>If LLMs are fine-tuned on adversarial formats, their robustness to format variation may increase or decrease depending on the nature of the fine-tuning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on all formats regardless of pretraining exposure, the theory would be falsified.</li>
                <li>If increasing format complexity does not increase error rates, the cognitive load law would be invalid.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs may have architectural features (e.g., retrieval augmentation) that mitigate format effects. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known prompt effects into a general principle of cognitive alignment.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity, pretraining effects]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that the effectiveness of LLM performance is determined by the degree to which the problem presentation format aligns with the LLM's internal cognitive priors, which are shaped by its pretraining data and architecture. Formats that closely match the distributional and structural patterns seen during pretraining facilitate more accurate and robust reasoning, while misaligned formats induce higher error rates and less reliable outputs.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Prior Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "matches",
                        "object": "LLM_pretraining_distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "higher_accuracy_and_robustness"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on tasks presented in formats similar to those seen in their pretraining data (e.g., question-answer pairs, Wikipedia-style text).",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering that mimics naturalistic or familiar formats improves LLM output quality.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs struggle with novel or adversarial formats that deviate from pretraining distributions.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs are sensitive to prompt format and that pretraining data influences model behavior.",
                    "what_is_novel": "The explicit law that performance is a function of alignment between presentation format and pretraining distribution is new.",
                    "classification_explanation": "While prompt sensitivity is established, the formalization of format-prior alignment as a predictive law is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity, pretraining effects]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Format-Induced Cognitive Load Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_complex_or_unfamiliar",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "increased_error_rate_and_variability"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs show higher error rates on tasks with unfamiliar or convoluted formats.",
                        "uuids": []
                    },
                    {
                        "text": "Complex or multi-part prompts can overwhelm LLMs, leading to incomplete or inconsistent outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt simplification and standardization reduce LLM error rates.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt complexity and unfamiliarity are known to degrade LLM performance.",
                    "what_is_novel": "The law frames this as a cognitive load effect induced by format misalignment.",
                    "classification_explanation": "The effect is known, but the cognitive load framing and explicit law are novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Webson & Pavlick (2022) Prompt Engineering for Few-Shot Learning: Beyond the Template [Prompt complexity effects]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new LLM is trained on a corpus with a specific problem format, it will perform best on tasks presented in that format.",
        "If a problem is reformatted to match a familiar pretraining style, LLM performance will improve."
    ],
    "new_predictions_unknown": [
        "If LLMs are exposed to a curriculum of diverse formats during pretraining, they may generalize better to novel formats.",
        "If LLMs are fine-tuned on adversarial formats, their robustness to format variation may increase or decrease depending on the nature of the fine-tuning."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on all formats regardless of pretraining exposure, the theory would be falsified.",
        "If increasing format complexity does not increase error rates, the cognitive load law would be invalid."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs may have architectural features (e.g., retrieval augmentation) that mitigate format effects.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Certain LLMs show surprising robustness to format changes after extensive instruction tuning.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very simple tasks may be unaffected by format alignment.",
        "Highly instruction-tuned models may partially overcome format-prior misalignment."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt sensitivity and pretraining effects are established.",
        "what_is_novel": "The explicit theory of cognitive alignment between format and LLM priors is new.",
        "classification_explanation": "The theory synthesizes known prompt effects into a general principle of cognitive alignment.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt sensitivity, pretraining effects]",
            "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format effects]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
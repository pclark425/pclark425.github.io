<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Statistical Test Calibration and Weighting Theory for Constraint-Based Discovery - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-122</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-122</p>
                <p><strong>Name:</strong> Statistical Test Calibration and Weighting Theory for Constraint-Based Discovery</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of distractor-robust causal discovery in open-ended virtual labs, including methods to detect, downweight, and refute spurious signals during inquiry, based on the following results.</p>
                <p><strong>Description:</strong> Spurious edges in constraint-based causal discovery arise from multiple sources: (1) finite-sample errors in conditional independence (CI) tests, (2) conflicting test results due to order-dependence and multiple testing, (3) inappropriate handling of missing data, and (4) failure to account for test power variations with conditioning set size. These errors can be systematically reduced through: (a) calibrating p-values using Bayesian or heuristic transforms that account for test asymmetry and multiple comparisons, (b) weighting tests by conditioning set size and sample size to penalize low-power tests, (c) using argumentation frameworks to resolve conflicting CI test results through d-separation reasoning and attack relations, (d) bootstrapping or resampling to assess edge stability and filter unstable edges, (e) explicitly modeling missingness mechanisms when data are not MCAR, and (f) using bounded conditioning sets (k-PC) to avoid exponential growth in test complexity while preserving identifiability. The theory predicts that proper test calibration combined with conflict resolution can reduce false discovery rates by 50-80% compared to naive threshold-based approaches, with the exact improvement depending on graph density, sample size, and the presence of latent confounders or missing data.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The strength of a conditional independence test should be weighted by S(p,α,s,d) = (1 - s/(d-2)) * γ(p,α) where s is conditioning set size, d is number of variables, and γ is a calibrated p-value transform that accounts for test asymmetry</li>
                <li>Conflicting test results can be resolved through argumentation frameworks where independence/dependence assumptions attack each other based on d-separation constraints, blocked-path reasoning, and collider-tree activation rules</li>
                <li>Edge stability across bootstrap resamples provides a confidence score; edges with low frequency (<50-65% depending on context) across resamples should be considered spurious or unreliable</li>
                <li>In the presence of missing data, naive test-wise deletion produces spurious edges when missingness indicators are descendants of colliders or correlated with substantive variables under MAR/MNAR mechanisms</li>
                <li>Bounding conditioning set size to k (k-PC) avoids exponential test complexity while preserving identifiability through k-closure properties and additional orientation rules (R11, R12)</li>
                <li>The false discovery rate of constraint-based methods decreases with sample size but increases with graph density, conditioning set size, and the presence of latent confounders</li>
                <li>Kernel-based CI tests (KCI, HSIC) should be used for non-Gaussian data to avoid spurious independence/dependence conclusions from parametric tests</li>
                <li>Test power decreases exponentially with conditioning set size due to sample splitting, making high-order tests unreliable in finite samples</li>
                <li>Argumentation-based stable extensions correspond to DAGs that are internally consistent with respect to d-separation and observed (in)dependencies</li>
                <li>Missingness-aware methods (MVPC, modified PC) are necessary when data are not MCAR; they model missingness mechanisms to avoid spurious edges from selection bias</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>ABA-PC uses argumentation and weighted test selection (S(p,α,s,d) = (1 - s/(d-2)) * γ(p,α)) to resolve conflicting CI tests, achieving significantly better worst-case SID than Majority-PC baseline across four bnlearn benchmarks <a href="../results/extraction-result-714.html#e714.0" class="evidence-link">[e714.0]</a> </li>
    <li>P-value to probability transformations (Jabbari, Claassen, Triantafillou refs) provide principled Bayesian alternatives to raw p-value thresholding for weighting constraints <a href="../results/extraction-result-714.html#e714.5" class="evidence-link">[e714.5]</a> </li>
    <li>Bootstrap/subsampling stability estimation detects spurious edges via low frequency across resamples; recommended as practical technique to estimate edge probabilities <a href="../results/extraction-result-766.html#e766.7" class="evidence-link">[e766.7]</a> <a href="../results/extraction-result-743.html#e743.3" class="evidence-link">[e743.3]</a> </li>
    <li>Ensemble methods with edge filtering (threshold 0.65) increase robustness of predicted edges by approximately 6% on average <a href="../results/extraction-result-743.html#e743.0" class="evidence-link">[e743.0]</a> <a href="../results/extraction-result-743.html#e743.1" class="evidence-link">[e743.1]</a> </li>
    <li>Test-wise deletion PC (TD-PC) produces extraneous edges under MAR/MNAR missingness when missingness indicators are correlated with substantive variables or are descendants of colliders <a href="../results/extraction-result-997.html#e997.3" class="evidence-link">[e997.3]</a> </li>
    <li>MVPC corrects for missingness-induced spurious associations by modeling missingness mechanisms and adjusting CI tests, avoiding extraneous edges that TD-PC produces <a href="../results/extraction-result-997.html#e997.0" class="evidence-link">[e997.0]</a> </li>
    <li>k-PC with bounded conditioning sets (k) produces k-essential graphs that are more informative than AnytimeFCI PAGs in causally sufficient systems, while avoiding exponential test complexity <a href="../results/extraction-result-730.html#e730.4" class="evidence-link">[e730.4]</a> <a href="../results/extraction-result-730.html#e730.6" class="evidence-link">[e730.6]</a> </li>
    <li>Resampling robustness metrics (edge frequency across subsamples) effectively detect unstable/spurious edges; robustness increases with sample size and restricted algorithm ensembles show higher stability <a href="../results/extraction-result-743.html#e743.3" class="evidence-link">[e743.3]</a> </li>
    <li>PC algorithm assumes Causal Markov, Faithfulness, i.i.d., and no latent confounders; violations lead to spurious edges <a href="../results/extraction-result-766.html#e766.2" class="evidence-link">[e766.2]</a> </li>
    <li>Modified PC for missing data (Tu et al., 2019) adjusts CI testing to account for missingness patterns, producing asymptotically correct output under missingness assumptions <a href="../results/extraction-result-766.html#e766.11" class="evidence-link">[e766.11]</a> </li>
    <li>Time-lagged PC with KCI tests successfully removes edges when conditional independence is detected, with KCI providing robust nonparametric testing for nonlinear relationships <a href="../results/extraction-result-699.html#e699.0" class="evidence-link">[e699.0]</a> </li>
    <li>RFCI (Really Fast Causal Inference) combined with bootstrapping shows directed edges are generally well calibrated according to Naeini et al. calibration studies <a href="../results/extraction-result-966.html#e966.4" class="evidence-link">[e966.4]</a> </li>
    <li>Argumentation-based conflict resolution uses attack relations where dependence facts (ap assumptions) attack blocked-path (bp) assumptions, preventing acceptance of incorrect independencies <a href="../results/extraction-result-714.html#e714.0" class="evidence-link">[e714.0]</a> </li>
    <li>IPW (Inverse Probability Weighting) for CI tests can correct for selection bias when missingness model is known, but assumption is often unrealistic <a href="../results/extraction-result-997.html#e997.4" class="evidence-link">[e997.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Applying ABA-PC style argumentation to PC algorithm outputs should reduce SHD by 20-40% on graphs with 8-10 nodes compared to standard PC with fixed α threshold</li>
                <li>Bootstrap stability filtering with 50-100 resamples should identify 80-90% of truly spurious edges in finite samples (n=500-1000) for sparse graphs (average degree 2-3)</li>
                <li>Calibrated p-value transforms (Bayesian or Sellke-Bayarri-Berger style) should reduce false discovery rate by 30-50% compared to raw p-value thresholding at α=0.05</li>
                <li>k-PC with k=3 should produce more oriented edges than AnytimeFCI with k=3 in causally sufficient systems with sparse graphs</li>
                <li>MVPC should produce 30-50% fewer extraneous edges than TD-PC in datasets with 20-30% MAR missingness</li>
                <li>Ensemble methods restricted to top 3-4 performing algorithms should show 10-15% higher edge robustness than all-algorithm ensembles</li>
                <li>Time-lagged PC with KCI should outperform linear Granger causality by 20-30% AUROC on nonlinear time series with 5-10 variables</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether argumentation-based conflict resolution scales to graphs with 50+ nodes and thousands of tests while maintaining computational tractability</li>
                <li>The optimal weighting function for combining conditioning set size penalties with p-value calibration across different data types (continuous, discrete, mixed)</li>
                <li>Whether stability-based filtering can distinguish between spurious edges and true edges in low-power settings (n<100, dense graphs)</li>
                <li>The effectiveness of these methods when the true graph has high-order interactions requiring conditioning sets larger than k=5</li>
                <li>Whether k-PC's additional orientation rules (R11, R12) introduce systematic biases in certain graph structures</li>
                <li>The relative importance of test calibration vs. conflict resolution vs. stability filtering in different regimes (sample size, density, latent confounders)</li>
                <li>Whether missingness-aware methods can handle complex MNAR mechanisms where missingness depends on unobserved variables</li>
                <li>The interaction between bootstrap sample size, number of resamples, and stability threshold for optimal spurious edge detection</li>
                <li>Whether argumentation frameworks can be extended to handle soft constraints and probabilistic CI test results</li>
                <li>The performance of these methods on graphs with feedback loops or cyclic structures</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding cases where weighted test selection (S(p,α,s,d)) performs worse than uniform weighting would challenge the calibration approach and suggest the penalty function is misspecified</li>
                <li>Demonstrating that argumentation-based resolution introduces systematic biases (e.g., preferring certain graph structures) would undermine the conflict resolution principle</li>
                <li>Showing that bootstrap stability is uncorrelated with edge correctness in certain graph structures (e.g., dense graphs, graphs with many v-structures) would limit the stability criterion</li>
                <li>Finding that missingness-aware methods (MVPC) perform worse than naive deletion (TD-PC) in MCAR settings would question the added complexity</li>
                <li>Demonstrating that k-PC with small k systematically misses important edges that full PC would find would challenge the bounded conditioning approach</li>
                <li>Finding cases where calibrated p-value transforms increase false discovery rate compared to raw p-values would invalidate the calibration theory</li>
                <li>Showing that ensemble filtering removes more true edges than spurious edges would undermine the filtering approach</li>
                <li>Demonstrating that argumentation-based methods converge to incorrect graphs when all individual tests are correct would reveal fundamental flaws in the argumentation framework</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to choose the calibration function γ(p,α) for different test types (Fisher Z, KCI, HSIC, χ²) or how to adapt it to different data distributions <a href="../results/extraction-result-714.html#e714.5" class="evidence-link">[e714.5]</a> </li>
    <li>Computational complexity of argumentation-based resolution (ASP solving) limits scalability to ~10 variables; the theory doesn't address how to scale to larger graphs <a href="../results/extraction-result-714.html#e714.0" class="evidence-link">[e714.0]</a> </li>
    <li>The relationship between bootstrap sample size, number of resamples, and stability estimation accuracy is not fully characterized; optimal parameters are unclear <a href="../results/extraction-result-766.html#e766.7" class="evidence-link">[e766.7]</a> <a href="../results/extraction-result-743.html#e743.3" class="evidence-link">[e743.3]</a> </li>
    <li>The theory doesn't explain why k-PC can be more informative than AnytimeFCI in causally sufficient systems beyond computational efficiency <a href="../results/extraction-result-730.html#e730.4" class="evidence-link">[e730.4]</a> <a href="../results/extraction-result-730.html#e730.6" class="evidence-link">[e730.6]</a> </li>
    <li>The optimal choice of k in k-PC for different graph structures and sample sizes is not specified <a href="../results/extraction-result-730.html#e730.4" class="evidence-link">[e730.4]</a> </li>
    <li>The theory doesn't address how to combine multiple types of evidence (test results, stability scores, argumentation outcomes) into a single edge confidence measure <a href="../results/extraction-result-714.html#e714.0" class="evidence-link">[e714.0]</a> <a href="../results/extraction-result-743.html#e743.3" class="evidence-link">[e743.3]</a> </li>
    <li>The interaction between preprocessing choices (filtering, normalization) and CI test results is not accounted for <a href="../results/extraction-result-766.html#e766.2" class="evidence-link">[e766.2]</a> </li>
    <li>The theory doesn't specify how to handle mixed data types (continuous, discrete, ordinal) in a unified framework <a href="../results/extraction-result-736.html#e736.1" class="evidence-link">[e736.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Spirtes et al. (2000) Causation, Prediction, and Search [Original PC algorithm and constraint-based framework; foundational theory]</li>
    <li>Colombo & Maathuis (2014) Order-independent constraint-based causal structure learning [Stable PC variants addressing order-dependence]</li>
    <li>Claassen & Heskes (2012) A Bayesian approach to constraint based causal inference [Bayesian scoring of constraints and probabilistic CI tests]</li>
    <li>Triantafillou & Tsamardinos (2015) Constraint-based causal discovery from multiple interventions [Handling interventional data in constraint-based methods]</li>
    <li>Tu et al. (2019) Causal discovery in the presence of missing data [Modified PC for missing data]</li>
    <li>Strobl et al. (2019) Approximate kernel-based conditional independence tests for fast non-parametric causal discovery [KCI and HSIC tests for non-Gaussian data]</li>
    <li>Sanchez-Romero et al. (2019) Estimating feedforward and feedback effective connections from fMRI time series [Time-lagged PC with KCI]</li>
    <li>Jabbari et al. (2017) Discovery of Causal Models that Contain Latent Variables Through Bayesian Scoring of Independence Constraints [Bayesian p-value transforms]</li>
    <li>Naeini et al. (2019) Calibration of causal relationships learned from data [Bootstrap-based calibration studies]</li>
    <li>Rantanen et al. (2020) Discovering causal graphs with cycles and latent confounders: An exact branch-and-bound approach [Solver-based approaches with argumentation-like reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Statistical Test Calibration and Weighting Theory for Constraint-Based Discovery",
    "theory_description": "Spurious edges in constraint-based causal discovery arise from multiple sources: (1) finite-sample errors in conditional independence (CI) tests, (2) conflicting test results due to order-dependence and multiple testing, (3) inappropriate handling of missing data, and (4) failure to account for test power variations with conditioning set size. These errors can be systematically reduced through: (a) calibrating p-values using Bayesian or heuristic transforms that account for test asymmetry and multiple comparisons, (b) weighting tests by conditioning set size and sample size to penalize low-power tests, (c) using argumentation frameworks to resolve conflicting CI test results through d-separation reasoning and attack relations, (d) bootstrapping or resampling to assess edge stability and filter unstable edges, (e) explicitly modeling missingness mechanisms when data are not MCAR, and (f) using bounded conditioning sets (k-PC) to avoid exponential growth in test complexity while preserving identifiability. The theory predicts that proper test calibration combined with conflict resolution can reduce false discovery rates by 50-80% compared to naive threshold-based approaches, with the exact improvement depending on graph density, sample size, and the presence of latent confounders or missing data.",
    "supporting_evidence": [
        {
            "text": "ABA-PC uses argumentation and weighted test selection (S(p,α,s,d) = (1 - s/(d-2)) * γ(p,α)) to resolve conflicting CI tests, achieving significantly better worst-case SID than Majority-PC baseline across four bnlearn benchmarks",
            "uuids": [
                "e714.0"
            ]
        },
        {
            "text": "P-value to probability transformations (Jabbari, Claassen, Triantafillou refs) provide principled Bayesian alternatives to raw p-value thresholding for weighting constraints",
            "uuids": [
                "e714.5"
            ]
        },
        {
            "text": "Bootstrap/subsampling stability estimation detects spurious edges via low frequency across resamples; recommended as practical technique to estimate edge probabilities",
            "uuids": [
                "e766.7",
                "e743.3"
            ]
        },
        {
            "text": "Ensemble methods with edge filtering (threshold 0.65) increase robustness of predicted edges by approximately 6% on average",
            "uuids": [
                "e743.0",
                "e743.1"
            ]
        },
        {
            "text": "Test-wise deletion PC (TD-PC) produces extraneous edges under MAR/MNAR missingness when missingness indicators are correlated with substantive variables or are descendants of colliders",
            "uuids": [
                "e997.3"
            ]
        },
        {
            "text": "MVPC corrects for missingness-induced spurious associations by modeling missingness mechanisms and adjusting CI tests, avoiding extraneous edges that TD-PC produces",
            "uuids": [
                "e997.0"
            ]
        },
        {
            "text": "k-PC with bounded conditioning sets (k) produces k-essential graphs that are more informative than AnytimeFCI PAGs in causally sufficient systems, while avoiding exponential test complexity",
            "uuids": [
                "e730.4",
                "e730.6"
            ]
        },
        {
            "text": "Resampling robustness metrics (edge frequency across subsamples) effectively detect unstable/spurious edges; robustness increases with sample size and restricted algorithm ensembles show higher stability",
            "uuids": [
                "e743.3"
            ]
        },
        {
            "text": "PC algorithm assumes Causal Markov, Faithfulness, i.i.d., and no latent confounders; violations lead to spurious edges",
            "uuids": [
                "e766.2"
            ]
        },
        {
            "text": "Modified PC for missing data (Tu et al., 2019) adjusts CI testing to account for missingness patterns, producing asymptotically correct output under missingness assumptions",
            "uuids": [
                "e766.11"
            ]
        },
        {
            "text": "Time-lagged PC with KCI tests successfully removes edges when conditional independence is detected, with KCI providing robust nonparametric testing for nonlinear relationships",
            "uuids": [
                "e699.0"
            ]
        },
        {
            "text": "RFCI (Really Fast Causal Inference) combined with bootstrapping shows directed edges are generally well calibrated according to Naeini et al. calibration studies",
            "uuids": [
                "e966.4"
            ]
        },
        {
            "text": "Argumentation-based conflict resolution uses attack relations where dependence facts (ap assumptions) attack blocked-path (bp) assumptions, preventing acceptance of incorrect independencies",
            "uuids": [
                "e714.0"
            ]
        },
        {
            "text": "IPW (Inverse Probability Weighting) for CI tests can correct for selection bias when missingness model is known, but assumption is often unrealistic",
            "uuids": [
                "e997.4"
            ]
        }
    ],
    "theory_statements": [
        "The strength of a conditional independence test should be weighted by S(p,α,s,d) = (1 - s/(d-2)) * γ(p,α) where s is conditioning set size, d is number of variables, and γ is a calibrated p-value transform that accounts for test asymmetry",
        "Conflicting test results can be resolved through argumentation frameworks where independence/dependence assumptions attack each other based on d-separation constraints, blocked-path reasoning, and collider-tree activation rules",
        "Edge stability across bootstrap resamples provides a confidence score; edges with low frequency (&lt;50-65% depending on context) across resamples should be considered spurious or unreliable",
        "In the presence of missing data, naive test-wise deletion produces spurious edges when missingness indicators are descendants of colliders or correlated with substantive variables under MAR/MNAR mechanisms",
        "Bounding conditioning set size to k (k-PC) avoids exponential test complexity while preserving identifiability through k-closure properties and additional orientation rules (R11, R12)",
        "The false discovery rate of constraint-based methods decreases with sample size but increases with graph density, conditioning set size, and the presence of latent confounders",
        "Kernel-based CI tests (KCI, HSIC) should be used for non-Gaussian data to avoid spurious independence/dependence conclusions from parametric tests",
        "Test power decreases exponentially with conditioning set size due to sample splitting, making high-order tests unreliable in finite samples",
        "Argumentation-based stable extensions correspond to DAGs that are internally consistent with respect to d-separation and observed (in)dependencies",
        "Missingness-aware methods (MVPC, modified PC) are necessary when data are not MCAR; they model missingness mechanisms to avoid spurious edges from selection bias"
    ],
    "new_predictions_likely": [
        "Applying ABA-PC style argumentation to PC algorithm outputs should reduce SHD by 20-40% on graphs with 8-10 nodes compared to standard PC with fixed α threshold",
        "Bootstrap stability filtering with 50-100 resamples should identify 80-90% of truly spurious edges in finite samples (n=500-1000) for sparse graphs (average degree 2-3)",
        "Calibrated p-value transforms (Bayesian or Sellke-Bayarri-Berger style) should reduce false discovery rate by 30-50% compared to raw p-value thresholding at α=0.05",
        "k-PC with k=3 should produce more oriented edges than AnytimeFCI with k=3 in causally sufficient systems with sparse graphs",
        "MVPC should produce 30-50% fewer extraneous edges than TD-PC in datasets with 20-30% MAR missingness",
        "Ensemble methods restricted to top 3-4 performing algorithms should show 10-15% higher edge robustness than all-algorithm ensembles",
        "Time-lagged PC with KCI should outperform linear Granger causality by 20-30% AUROC on nonlinear time series with 5-10 variables"
    ],
    "new_predictions_unknown": [
        "Whether argumentation-based conflict resolution scales to graphs with 50+ nodes and thousands of tests while maintaining computational tractability",
        "The optimal weighting function for combining conditioning set size penalties with p-value calibration across different data types (continuous, discrete, mixed)",
        "Whether stability-based filtering can distinguish between spurious edges and true edges in low-power settings (n&lt;100, dense graphs)",
        "The effectiveness of these methods when the true graph has high-order interactions requiring conditioning sets larger than k=5",
        "Whether k-PC's additional orientation rules (R11, R12) introduce systematic biases in certain graph structures",
        "The relative importance of test calibration vs. conflict resolution vs. stability filtering in different regimes (sample size, density, latent confounders)",
        "Whether missingness-aware methods can handle complex MNAR mechanisms where missingness depends on unobserved variables",
        "The interaction between bootstrap sample size, number of resamples, and stability threshold for optimal spurious edge detection",
        "Whether argumentation frameworks can be extended to handle soft constraints and probabilistic CI test results",
        "The performance of these methods on graphs with feedback loops or cyclic structures"
    ],
    "negative_experiments": [
        "Finding cases where weighted test selection (S(p,α,s,d)) performs worse than uniform weighting would challenge the calibration approach and suggest the penalty function is misspecified",
        "Demonstrating that argumentation-based resolution introduces systematic biases (e.g., preferring certain graph structures) would undermine the conflict resolution principle",
        "Showing that bootstrap stability is uncorrelated with edge correctness in certain graph structures (e.g., dense graphs, graphs with many v-structures) would limit the stability criterion",
        "Finding that missingness-aware methods (MVPC) perform worse than naive deletion (TD-PC) in MCAR settings would question the added complexity",
        "Demonstrating that k-PC with small k systematically misses important edges that full PC would find would challenge the bounded conditioning approach",
        "Finding cases where calibrated p-value transforms increase false discovery rate compared to raw p-values would invalidate the calibration theory",
        "Showing that ensemble filtering removes more true edges than spurious edges would undermine the filtering approach",
        "Demonstrating that argumentation-based methods converge to incorrect graphs when all individual tests are correct would reveal fundamental flaws in the argumentation framework"
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to choose the calibration function γ(p,α) for different test types (Fisher Z, KCI, HSIC, χ²) or how to adapt it to different data distributions",
            "uuids": [
                "e714.5"
            ]
        },
        {
            "text": "Computational complexity of argumentation-based resolution (ASP solving) limits scalability to ~10 variables; the theory doesn't address how to scale to larger graphs",
            "uuids": [
                "e714.0"
            ]
        },
        {
            "text": "The relationship between bootstrap sample size, number of resamples, and stability estimation accuracy is not fully characterized; optimal parameters are unclear",
            "uuids": [
                "e766.7",
                "e743.3"
            ]
        },
        {
            "text": "The theory doesn't explain why k-PC can be more informative than AnytimeFCI in causally sufficient systems beyond computational efficiency",
            "uuids": [
                "e730.4",
                "e730.6"
            ]
        },
        {
            "text": "The optimal choice of k in k-PC for different graph structures and sample sizes is not specified",
            "uuids": [
                "e730.4"
            ]
        },
        {
            "text": "The theory doesn't address how to combine multiple types of evidence (test results, stability scores, argumentation outcomes) into a single edge confidence measure",
            "uuids": [
                "e714.0",
                "e743.3"
            ]
        },
        {
            "text": "The interaction between preprocessing choices (filtering, normalization) and CI test results is not accounted for",
            "uuids": [
                "e766.2"
            ]
        },
        {
            "text": "The theory doesn't specify how to handle mixed data types (continuous, discrete, ordinal) in a unified framework",
            "uuids": [
                "e736.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "ABA-PC is limited to ~10 variables due to computational cost, suggesting argumentation may not scale",
            "uuids": [
                "e714.0"
            ]
        },
        {
            "text": "Bootstrap methods can be computationally expensive, requiring many runs of base algorithms (e.g., 50-100 resamples)",
            "uuids": [
                "e987.3",
                "e743.3"
            ]
        },
        {
            "text": "Stability does not guarantee correctness; stable outputs can still be wrong if the base algorithm is systematically biased",
            "uuids": [
                "e766.7"
            ]
        },
        {
            "text": "k-PC may be less informative than full PC when true conditioning sets exceed k, creating a trade-off between computational efficiency and identifiability",
            "uuids": [
                "e730.4"
            ]
        },
        {
            "text": "Ensemble methods can inherit instability from constituent algorithms; restricting to top algorithms helps but doesn't eliminate the problem",
            "uuids": [
                "e743.0"
            ]
        },
        {
            "text": "MVPC requires correct specification of missingness mechanism; misspecification can lead to worse performance than naive methods",
            "uuids": [
                "e997.0"
            ]
        },
        {
            "text": "Conservative multiple testing correction (Bonferroni) may be too stringent in small samples, while liberal correction (FDR) may allow too many false positives",
            "uuids": [
                "e714.0",
                "e766.7"
            ]
        }
    ],
    "special_cases": [
        "For non-Gaussian data, kernel-based tests (HSIC, KCI) should be used instead of parametric tests (Fisher Z) to avoid spurious independence conclusions",
        "When sample size is very small (n &lt; 100), conservative Bonferroni correction may be too stringent and FDR control may be preferable, but this increases false positives",
        "In the presence of latent confounders, FCI-based methods with PAG outputs are required instead of PC; k-PC is only valid for causally sufficient systems",
        "For time-series data, time-lagged PC with appropriate lag selection is necessary; standard PC assumes i.i.d. data",
        "When data are MCAR, naive test-wise deletion is valid and simpler than missingness-aware methods",
        "When data are MAR or MNAR, missingness-aware methods (MVPC, modified PC) are necessary to avoid spurious edges from selection bias",
        "For graphs with bounded in-degree k, k-PC with appropriate k can achieve full identifiability with polynomial test complexity",
        "When conditioning sets are large (s &gt; 5-7), test power becomes very low and results should be treated with caution",
        "For dense graphs (average degree &gt; 5), constraint-based methods may require very large samples (n &gt; 10,000) to achieve reliable results",
        "When using ensemble methods, restricting to top 3-4 performing algorithms improves stability compared to all-algorithm ensembles",
        "For bootstrap stability estimation, 50-100 resamples are typically sufficient; more resamples improve precision but with diminishing returns",
        "When argumentation produces no stable extension, iterative exclusion of weakest tests (lowest S scores) is necessary to obtain valid models"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Spirtes et al. (2000) Causation, Prediction, and Search [Original PC algorithm and constraint-based framework; foundational theory]",
            "Colombo & Maathuis (2014) Order-independent constraint-based causal structure learning [Stable PC variants addressing order-dependence]",
            "Claassen & Heskes (2012) A Bayesian approach to constraint based causal inference [Bayesian scoring of constraints and probabilistic CI tests]",
            "Triantafillou & Tsamardinos (2015) Constraint-based causal discovery from multiple interventions [Handling interventional data in constraint-based methods]",
            "Tu et al. (2019) Causal discovery in the presence of missing data [Modified PC for missing data]",
            "Strobl et al. (2019) Approximate kernel-based conditional independence tests for fast non-parametric causal discovery [KCI and HSIC tests for non-Gaussian data]",
            "Sanchez-Romero et al. (2019) Estimating feedforward and feedback effective connections from fMRI time series [Time-lagged PC with KCI]",
            "Jabbari et al. (2017) Discovery of Causal Models that Contain Latent Variables Through Bayesian Scoring of Independence Constraints [Bayesian p-value transforms]",
            "Naeini et al. (2019) Calibration of causal relationships learned from data [Bootstrap-based calibration studies]",
            "Rantanen et al. (2020) Discovering causal graphs with cycles and latent confounders: An exact branch-and-bound approach [Solver-based approaches with argumentation-like reasoning]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 3,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
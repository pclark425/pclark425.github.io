<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hybrid LLM-Augmented Curriculum Theory for Verifiable Domains - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-381</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-381</p>
                <p><strong>Name:</strong> Hybrid LLM-Augmented Curriculum Theory for Verifiable Domains</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that Large Language Models (LLMs) combined with complementary mechanisms (such as vision-language model refinement, embedding-based selection, or difficulty prediction) can generate effective curricula for compositional skill acquisition in verifiable task domains (mathematics, code generation, robotics with verifiable rewards). The effectiveness stems from: (1) LLMs' ability to leverage pre-trained knowledge to generate diverse task variants and identify conceptual relationships; (2) complementary mechanisms that address LLM limitations (stochasticity, quality control, difficulty calibration); (3) iterative refinement processes that improve curriculum reliability. The theory posits that hybrid LLM-augmented approaches achieve superior performance compared to naive baselines (random sampling, no curriculum) but may not consistently outperform well-designed rule-based or algorithmic curricula. Success is highly dependent on: baseline quality, domain characteristics (verifiability, task structure), computational resources for quality control, and the specific augmentation mechanisms used. The theory explicitly acknowledges high variability in LLM outputs requiring multiple attempts or refinement, substantial prompt engineering needs, and computational overhead from quality control.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-339.html">[theory-339]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>CRITICAL SCOPE CHANGE: Narrowed domain from 'compositional acquisition of commonsense and science procedures in interactive text environments' to 'verifiable task domains (mathematics, code generation, robotics)' to match available evidence. Original domain marked as speculative.</li>
                <li>MAJOR REVISION: Changed from 'LLM-generated curricula are superior' to 'Hybrid LLM-augmented approaches (LLM + complementary mechanisms) can achieve superior results compared to naive baselines but may not outperform well-designed rule-based curricula'.</li>
                <li>Added explicit acknowledgment of high variability and stochasticity in LLM outputs, requiring multiple attempts and refinement mechanisms for reliable results.</li>
                <li>Revised 'minimal domain-specific fine-tuning' claim to acknowledge substantial prompt engineering, domain-specific training, and quality control needs.</li>
                <li>Added computational cost and sample efficiency as key evaluation dimensions, noting LLM methods can incur substantial overhead while non-LLM methods achieve comparable results with lower cost.</li>
                <li>Modified prerequisite identification claim to acknowledge both LLM and non-LLM methods can effectively identify prerequisites; this is a property of good curriculum design rather than unique to LLMs.</li>
                <li>Added explicit statement that LLM superiority is baseline-dependent: dramatic when baselines fail but modest or absent compared to well-designed alternatives.</li>
                <li>Clarified that successful approaches are 'hybrid' combining LLM generation with complementary mechanisms (VLM refinement, embedding selection, difficulty prediction) rather than pure LLM generation.</li>
                <li>Added model scale nuance: relationship between scale and curriculum quality is context-dependent, with smaller models + augmentation sometimes matching larger models.</li>
                <li>Added to unaccounted_for: reliability metrics, computational cost comparisons, baseline quality dependencies, domain-specific failure modes, and off-policy training fragility.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Hybrid LLM-augmented curricula (LLM generation + complementary mechanisms like VLM refinement, embedding selection, or difficulty prediction) will achieve higher success rates than naive baselines (random sampling, no curriculum) in verifiable task domains (math, code, robotics).</li>
                <li>The magnitude of improvement from hybrid LLM-augmented curricula is baseline-dependent: dramatic when baselines completely fail (0-10% → 60-100%) but modest or absent when compared to well-designed rule-based curricula.</li>
                <li>LLM-generated curricula exhibit high variability (20-100% success rates across runs) and require multiple attempts, refinement mechanisms, or quality control (e.g., filtering unsolvable variants, API validation) to achieve reliable results.</li>
                <li>Successful LLM curriculum generation requires substantial engineering: prompt engineering, domain-specific training (e.g., 200 steps for difficulty prediction), quality control pipelines, and often prior exposure to related tasks.</li>
                <li>Hybrid approaches combining LLM generation with VLM evaluation, embedding-based selection, or learned difficulty prediction consistently outperform pure LLM generation alone.</li>
                <li>LLM-generated curricula can identify prerequisite relationships and generate intermediate tasks, but this capability is not unique to LLMs—well-designed non-LLM methods (difficulty labels, achievability estimates, advantage signals) can achieve comparable prerequisite identification.</li>
                <li>Computational cost of LLM curriculum generation can be substantial due to quality control needs (filtering unsolvable variants, handling API hallucinations, multiple refinement iterations), while non-LLM methods often achieve comparable results with negligible overhead.</li>
                <li>LLM curriculum effectiveness is domain-dependent: most effective in verifiable domains (math, code) with clear success criteria; effectiveness in interactive text environments for commonsense/science procedures remains speculative pending further evidence.</li>
                <li>The relationship between LLM model scale and curriculum quality is context-dependent: larger models may generate better curricula, but smaller models with appropriate augmentation (e.g., difficulty filtering) can also be effective.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>CRAFT (LLM curriculum generation + VLM refinement) achieved 60-100% success rates on multi-robot coordination tasks where baseline methods achieved 0-10%, and policies transferred to real hardware with 65% success, demonstrating hybrid LLM-augmented approaches can enable learning where naive methods fail. <a href="../results/extraction-result-2030.html#e2030.0" class="evidence-link">[e2030.0]</a> <a href="../results/extraction-result-2030.html#e2030.1" class="evidence-link">[e2030.1]</a> <a href="../results/extraction-result-2030.html#e2030.2" class="evidence-link">[e2030.2]</a> <a href="../results/extraction-result-2030.html#e2030.4" class="evidence-link">[e2030.4]</a> </li>
    <li>LADDER (LLM recursive variant generation with quality control) enabled 82% test accuracy on compositional integration compared to 1-2% for sampling and 0% for RL without variants, showing LLM-generated intermediate tasks can create effective difficulty gradients when properly filtered. <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> <a href="../results/extraction-result-2034.html#e2034.2" class="evidence-link">[e2034.2]</a> </li>
    <li>Self-aware RL (LLM task generation + difficulty prediction training) achieved 53.8% relative improvement on mathematical reasoning while using minimal external guidance (1.23% of tasks), demonstrating LLMs can adapt curriculum difficulty when augmented with learned difficulty prediction. <a href="../results/extraction-result-2035.html#e2035.0" class="evidence-link">[e2035.0]</a> </li>
    <li>TTC-RL (LLM embeddings + SIFT selection + on-policy RL) achieved 1.8-2.1x improvements on hard benchmarks, showing LLM representations can identify relevant training tasks when combined with principled selection algorithms. <a href="../results/extraction-result-2038.html#e2038.0" class="evidence-link">[e2038.0]</a> <a href="../results/extraction-result-2038.html#e2038.1" class="evidence-link">[e2038.1]</a> </li>
    <li>COGENT (LLM generation + curriculum conditioning + readability constraints) significantly improved curriculum alignment (4.62 vs 4.08 baseline, 4.15 vs 3.49 human) and controlled readability, demonstrating LLMs can leverage curriculum structure when properly constrained. <a href="../results/extraction-result-2031.html#e2031.0" class="evidence-link">[e2031.0]</a> </li>
    <li>Theoretical analysis (CRL Theory) provides formal justification that curriculum RL can require fewer samples than direct learning under appropriate conditions, supporting the potential for sample efficiency gains. <a href="../results/extraction-result-2036.html#e2036.7" class="evidence-link">[e2036.7]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In verifiable domains (math, code, robotics), hybrid LLM-augmented curricula will consistently outperform random sampling baselines by 50-200% on final task success rates, but the improvement will diminish to 0-30% when compared to well-designed rule-based curricula (e.g., difficulty-ordered, achievability-filtered).</li>
                <li>LLM-generated curricula will require 2-5 refinement iterations or multiple generation attempts to achieve reliable results (>80% effective curricula ratio), with single-shot generation showing high failure rates (20-50%).</li>
                <li>Hybrid approaches combining LLM generation with VLM refinement will achieve 2-3x higher effective curricula ratios compared to pure LLM generation without refinement in robotics domains.</li>
                <li>In domains with verifiable rewards, LLM-generated intermediate tasks will enable learning where direct training fails, but approximately 5-15% of generated tasks will be unsolvable or require filtering.</li>
                <li>Computational cost of LLM curriculum generation (including quality control) will be 3-10x higher than rule-based curriculum methods when measured in total compute time, though wall-clock time may be competitive if generation is parallelized.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether hybrid LLM-augmented curricula can achieve superior results in interactive text environments for commonsense and science procedures (the original theory domain) remains unknown, as current evidence is primarily from math, code, and robotics domains.</li>
                <li>Whether combining multiple LLM curriculum generation strategies (e.g., CRAFT-style staged curricula + LADDER-style variant generation + Self-aware RL difficulty prediction) produces synergistic improvements or diminishing returns is unknown.</li>
                <li>Whether LLM-generated curricula can effectively handle multi-objective learning scenarios where learners must balance competing goals (e.g., speed vs. safety, accuracy vs. efficiency) without explicit multi-objective optimization mechanisms is unknown.</li>
                <li>Whether fine-tuning LLMs specifically on curriculum design tasks with expert feedback produces dramatically better curricula (>50% improvement) or only marginal gains (<20% improvement) compared to zero-shot/few-shot approaches is unknown.</li>
                <li>Whether LLM-generated curricula can maintain effectiveness when scaled to very large task spaces (>100k tasks) or whether quality degrades due to coverage limitations is unknown.</li>
                <li>Whether adversarial or deliberately misleading task sequences generated by LLMs can be reliably detected and filtered using automated methods, or whether human oversight is necessary to prevent harmful curriculum orderings is unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If well-designed rule-based curricula (e.g., E2H, SEC, A-TTC) consistently outperform hybrid LLM-augmented curricula across multiple verifiable domains with comparable computational budgets, this would challenge the practical utility of LLM approaches.</li>
                <li>If the effective curricula ratio of LLM-generated curricula remains below 50% even with multiple refinement iterations and quality control mechanisms, this would challenge the reliability claims.</li>
                <li>If computational cost analysis shows LLM curriculum generation requires >10x more compute than rule-based methods for equivalent or worse performance, this would challenge the efficiency claims.</li>
                <li>If LLM-generated curricula show no significant improvement over random task ordering when both are given equivalent quality control and filtering (e.g., removing unsolvable tasks), this would suggest the LLM's contribution is primarily in task generation rather than intelligent sequencing.</li>
                <li>If removing the complementary mechanisms (VLM refinement, embedding selection, difficulty prediction) from hybrid approaches reduces performance to baseline levels, this would suggest LLMs alone provide minimal curriculum benefit.</li>
                <li>If LLM-generated curricula fail to generalize to the theory's original target domain (interactive text environments for commonsense/science procedures) despite success in verifiable domains, this would challenge the generality of the approach.</li>
                <li>If smaller, less capable language models produce equally effective curricula as large state-of-the-art LLMs when both use the same augmentation mechanisms, this would challenge claims about the importance of pre-trained knowledge and model scale.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify optimal prompting strategies or the amount of prompt engineering required for different domains, though evidence shows substantial engineering is often needed (COGENT careful prompting, Self-aware RL 200 training steps). <a href="../results/extraction-result-2031.html#e2031.0" class="evidence-link">[e2031.0]</a> <a href="../results/extraction-result-2035.html#e2035.0" class="evidence-link">[e2035.0]</a> </li>
    <li>The theory does not account for the high variability and stochasticity in LLM outputs (EvoCurr 20% success rate, CRAFT variable effective curricula ratio, LADDER 8% unsolvable variants) or specify how many attempts are needed for reliable results. <a href="../results/extraction-result-2028.html#e2028.0" class="evidence-link">[e2028.0]</a> <a href="../results/extraction-result-2030.html#e2030.0" class="evidence-link">[e2030.0]</a> <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> </li>
    <li>The theory does not address computational cost comparisons: LLM methods often incur substantial overhead from quality control while non-LLM methods achieve comparable results with negligible overhead. <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> <a href="../results/extraction-result-2037.html#e2037.0" class="evidence-link">[e2037.0]</a> </li>
    <li>The theory does not specify when LLM approaches are preferred over rule-based methods: evidence suggests LLMs are most valuable when baselines completely fail but offer little advantage over well-designed alternatives. <a href="../results/extraction-result-2030.html#e2030.0" class="evidence-link">[e2030.0]</a> <a href="../results/extraction-result-2034.html#e2034.0" class="evidence-link">[e2034.0]</a> <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> <a href="../results/extraction-result-2037.html#e2037.0" class="evidence-link">[e2037.0]</a> </li>
    <li>The theory does not account for domain-specific failure modes: LLM hallucination of non-existent APIs in code generation, poor initial difficulty prediction requiring training, readability miscalibration in educational content. <a href="../results/extraction-result-2028.html#e2028.0" class="evidence-link">[e2028.0]</a> <a href="../results/extraction-result-2028.html#e2028.2" class="evidence-link">[e2028.2]</a> <a href="../results/extraction-result-2035.html#e2035.0" class="evidence-link">[e2035.0]</a> <a href="../results/extraction-result-2031.html#e2031.1" class="evidence-link">[e2031.1]</a> </li>
    <li>The theory does not address the fragility of off-policy training on LLM-curated curricula (TTC-SFT catastrophic drops) or specify when on-policy vs off-policy training is appropriate. <a href="../results/extraction-result-2038.html#e2038.5" class="evidence-link">[e2038.5]</a> </li>
    <li>The theory does not specify the relationship between curriculum quality and specific LLM architectural choices, training procedures, or the role of model scale in different contexts. <a href="../results/extraction-result-2038.html#e2038.3" class="evidence-link">[e2038.3]</a> <a href="../results/extraction-result-2036.html#e2036.0" class="evidence-link">[e2036.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hybrid LLM-Augmented Curriculum Theory for Verifiable Domains",
    "type": "specific",
    "theory_description": "This theory proposes that Large Language Models (LLMs) combined with complementary mechanisms (such as vision-language model refinement, embedding-based selection, or difficulty prediction) can generate effective curricula for compositional skill acquisition in verifiable task domains (mathematics, code generation, robotics with verifiable rewards). The effectiveness stems from: (1) LLMs' ability to leverage pre-trained knowledge to generate diverse task variants and identify conceptual relationships; (2) complementary mechanisms that address LLM limitations (stochasticity, quality control, difficulty calibration); (3) iterative refinement processes that improve curriculum reliability. The theory posits that hybrid LLM-augmented approaches achieve superior performance compared to naive baselines (random sampling, no curriculum) but may not consistently outperform well-designed rule-based or algorithmic curricula. Success is highly dependent on: baseline quality, domain characteristics (verifiability, task structure), computational resources for quality control, and the specific augmentation mechanisms used. The theory explicitly acknowledges high variability in LLM outputs requiring multiple attempts or refinement, substantial prompt engineering needs, and computational overhead from quality control.",
    "supporting_evidence": [
        {
            "text": "CRAFT (LLM curriculum generation + VLM refinement) achieved 60-100% success rates on multi-robot coordination tasks where baseline methods achieved 0-10%, and policies transferred to real hardware with 65% success, demonstrating hybrid LLM-augmented approaches can enable learning where naive methods fail.",
            "uuids": [
                "e2030.0",
                "e2030.1",
                "e2030.2",
                "e2030.4"
            ]
        },
        {
            "text": "LADDER (LLM recursive variant generation with quality control) enabled 82% test accuracy on compositional integration compared to 1-2% for sampling and 0% for RL without variants, showing LLM-generated intermediate tasks can create effective difficulty gradients when properly filtered.",
            "uuids": [
                "e2034.0",
                "e2034.2"
            ]
        },
        {
            "text": "Self-aware RL (LLM task generation + difficulty prediction training) achieved 53.8% relative improvement on mathematical reasoning while using minimal external guidance (1.23% of tasks), demonstrating LLMs can adapt curriculum difficulty when augmented with learned difficulty prediction.",
            "uuids": [
                "e2035.0"
            ]
        },
        {
            "text": "TTC-RL (LLM embeddings + SIFT selection + on-policy RL) achieved 1.8-2.1x improvements on hard benchmarks, showing LLM representations can identify relevant training tasks when combined with principled selection algorithms.",
            "uuids": [
                "e2038.0",
                "e2038.1"
            ]
        },
        {
            "text": "COGENT (LLM generation + curriculum conditioning + readability constraints) significantly improved curriculum alignment (4.62 vs 4.08 baseline, 4.15 vs 3.49 human) and controlled readability, demonstrating LLMs can leverage curriculum structure when properly constrained.",
            "uuids": [
                "e2031.0"
            ]
        },
        {
            "text": "Theoretical analysis (CRL Theory) provides formal justification that curriculum RL can require fewer samples than direct learning under appropriate conditions, supporting the potential for sample efficiency gains.",
            "uuids": [
                "e2036.7"
            ]
        }
    ],
    "theory_statements": [
        "Hybrid LLM-augmented curricula (LLM generation + complementary mechanisms like VLM refinement, embedding selection, or difficulty prediction) will achieve higher success rates than naive baselines (random sampling, no curriculum) in verifiable task domains (math, code, robotics).",
        "The magnitude of improvement from hybrid LLM-augmented curricula is baseline-dependent: dramatic when baselines completely fail (0-10% → 60-100%) but modest or absent when compared to well-designed rule-based curricula.",
        "LLM-generated curricula exhibit high variability (20-100% success rates across runs) and require multiple attempts, refinement mechanisms, or quality control (e.g., filtering unsolvable variants, API validation) to achieve reliable results.",
        "Successful LLM curriculum generation requires substantial engineering: prompt engineering, domain-specific training (e.g., 200 steps for difficulty prediction), quality control pipelines, and often prior exposure to related tasks.",
        "Hybrid approaches combining LLM generation with VLM evaluation, embedding-based selection, or learned difficulty prediction consistently outperform pure LLM generation alone.",
        "LLM-generated curricula can identify prerequisite relationships and generate intermediate tasks, but this capability is not unique to LLMs—well-designed non-LLM methods (difficulty labels, achievability estimates, advantage signals) can achieve comparable prerequisite identification.",
        "Computational cost of LLM curriculum generation can be substantial due to quality control needs (filtering unsolvable variants, handling API hallucinations, multiple refinement iterations), while non-LLM methods often achieve comparable results with negligible overhead.",
        "LLM curriculum effectiveness is domain-dependent: most effective in verifiable domains (math, code) with clear success criteria; effectiveness in interactive text environments for commonsense/science procedures remains speculative pending further evidence.",
        "The relationship between LLM model scale and curriculum quality is context-dependent: larger models may generate better curricula, but smaller models with appropriate augmentation (e.g., difficulty filtering) can also be effective."
    ],
    "new_predictions_likely": [
        "In verifiable domains (math, code, robotics), hybrid LLM-augmented curricula will consistently outperform random sampling baselines by 50-200% on final task success rates, but the improvement will diminish to 0-30% when compared to well-designed rule-based curricula (e.g., difficulty-ordered, achievability-filtered).",
        "LLM-generated curricula will require 2-5 refinement iterations or multiple generation attempts to achieve reliable results (&gt;80% effective curricula ratio), with single-shot generation showing high failure rates (20-50%).",
        "Hybrid approaches combining LLM generation with VLM refinement will achieve 2-3x higher effective curricula ratios compared to pure LLM generation without refinement in robotics domains.",
        "In domains with verifiable rewards, LLM-generated intermediate tasks will enable learning where direct training fails, but approximately 5-15% of generated tasks will be unsolvable or require filtering.",
        "Computational cost of LLM curriculum generation (including quality control) will be 3-10x higher than rule-based curriculum methods when measured in total compute time, though wall-clock time may be competitive if generation is parallelized."
    ],
    "new_predictions_unknown": [
        "Whether hybrid LLM-augmented curricula can achieve superior results in interactive text environments for commonsense and science procedures (the original theory domain) remains unknown, as current evidence is primarily from math, code, and robotics domains.",
        "Whether combining multiple LLM curriculum generation strategies (e.g., CRAFT-style staged curricula + LADDER-style variant generation + Self-aware RL difficulty prediction) produces synergistic improvements or diminishing returns is unknown.",
        "Whether LLM-generated curricula can effectively handle multi-objective learning scenarios where learners must balance competing goals (e.g., speed vs. safety, accuracy vs. efficiency) without explicit multi-objective optimization mechanisms is unknown.",
        "Whether fine-tuning LLMs specifically on curriculum design tasks with expert feedback produces dramatically better curricula (&gt;50% improvement) or only marginal gains (&lt;20% improvement) compared to zero-shot/few-shot approaches is unknown.",
        "Whether LLM-generated curricula can maintain effectiveness when scaled to very large task spaces (&gt;100k tasks) or whether quality degrades due to coverage limitations is unknown.",
        "Whether adversarial or deliberately misleading task sequences generated by LLMs can be reliably detected and filtered using automated methods, or whether human oversight is necessary to prevent harmful curriculum orderings is unknown."
    ],
    "negative_experiments": [
        "If well-designed rule-based curricula (e.g., E2H, SEC, A-TTC) consistently outperform hybrid LLM-augmented curricula across multiple verifiable domains with comparable computational budgets, this would challenge the practical utility of LLM approaches.",
        "If the effective curricula ratio of LLM-generated curricula remains below 50% even with multiple refinement iterations and quality control mechanisms, this would challenge the reliability claims.",
        "If computational cost analysis shows LLM curriculum generation requires &gt;10x more compute than rule-based methods for equivalent or worse performance, this would challenge the efficiency claims.",
        "If LLM-generated curricula show no significant improvement over random task ordering when both are given equivalent quality control and filtering (e.g., removing unsolvable tasks), this would suggest the LLM's contribution is primarily in task generation rather than intelligent sequencing.",
        "If removing the complementary mechanisms (VLM refinement, embedding selection, difficulty prediction) from hybrid approaches reduces performance to baseline levels, this would suggest LLMs alone provide minimal curriculum benefit.",
        "If LLM-generated curricula fail to generalize to the theory's original target domain (interactive text environments for commonsense/science procedures) despite success in verifiable domains, this would challenge the generality of the approach.",
        "If smaller, less capable language models produce equally effective curricula as large state-of-the-art LLMs when both use the same augmentation mechanisms, this would challenge claims about the importance of pre-trained knowledge and model scale."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify optimal prompting strategies or the amount of prompt engineering required for different domains, though evidence shows substantial engineering is often needed (COGENT careful prompting, Self-aware RL 200 training steps).",
            "uuids": [
                "e2031.0",
                "e2035.0"
            ]
        },
        {
            "text": "The theory does not account for the high variability and stochasticity in LLM outputs (EvoCurr 20% success rate, CRAFT variable effective curricula ratio, LADDER 8% unsolvable variants) or specify how many attempts are needed for reliable results.",
            "uuids": [
                "e2028.0",
                "e2030.0",
                "e2034.0"
            ]
        },
        {
            "text": "The theory does not address computational cost comparisons: LLM methods often incur substantial overhead from quality control while non-LLM methods achieve comparable results with negligible overhead.",
            "uuids": [
                "e2034.0",
                "e2036.0",
                "e2037.0"
            ]
        },
        {
            "text": "The theory does not specify when LLM approaches are preferred over rule-based methods: evidence suggests LLMs are most valuable when baselines completely fail but offer little advantage over well-designed alternatives.",
            "uuids": [
                "e2030.0",
                "e2034.0",
                "e2036.0",
                "e2037.0"
            ]
        },
        {
            "text": "The theory does not account for domain-specific failure modes: LLM hallucination of non-existent APIs in code generation, poor initial difficulty prediction requiring training, readability miscalibration in educational content.",
            "uuids": [
                "e2028.0",
                "e2028.2",
                "e2035.0",
                "e2031.1"
            ]
        },
        {
            "text": "The theory does not address the fragility of off-policy training on LLM-curated curricula (TTC-SFT catastrophic drops) or specify when on-policy vs off-policy training is appropriate.",
            "uuids": [
                "e2038.5"
            ]
        },
        {
            "text": "The theory does not specify the relationship between curriculum quality and specific LLM architectural choices, training procedures, or the role of model scale in different contexts.",
            "uuids": [
                "e2038.3",
                "e2036.0"
            ]
        }
    ],
    "change_log": [
        "CRITICAL SCOPE CHANGE: Narrowed domain from 'compositional acquisition of commonsense and science procedures in interactive text environments' to 'verifiable task domains (mathematics, code generation, robotics)' to match available evidence. Original domain marked as speculative.",
        "MAJOR REVISION: Changed from 'LLM-generated curricula are superior' to 'Hybrid LLM-augmented approaches (LLM + complementary mechanisms) can achieve superior results compared to naive baselines but may not outperform well-designed rule-based curricula'.",
        "Added explicit acknowledgment of high variability and stochasticity in LLM outputs, requiring multiple attempts and refinement mechanisms for reliable results.",
        "Revised 'minimal domain-specific fine-tuning' claim to acknowledge substantial prompt engineering, domain-specific training, and quality control needs.",
        "Added computational cost and sample efficiency as key evaluation dimensions, noting LLM methods can incur substantial overhead while non-LLM methods achieve comparable results with lower cost.",
        "Modified prerequisite identification claim to acknowledge both LLM and non-LLM methods can effectively identify prerequisites; this is a property of good curriculum design rather than unique to LLMs.",
        "Added explicit statement that LLM superiority is baseline-dependent: dramatic when baselines fail but modest or absent compared to well-designed alternatives.",
        "Clarified that successful approaches are 'hybrid' combining LLM generation with complementary mechanisms (VLM refinement, embedding selection, difficulty prediction) rather than pure LLM generation.",
        "Added model scale nuance: relationship between scale and curriculum quality is context-dependent, with smaller models + augmentation sometimes matching larger models.",
        "Added to unaccounted_for: reliability metrics, computational cost comparisons, baseline quality dependencies, domain-specific failure modes, and off-policy training fragility."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Task- and Model-Dependence of Self-Reflection Efficacy in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1443</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1443</p>
                <p><strong>Name:</strong> Task- and Model-Dependence of Self-Reflection Efficacy in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.</p>
                <p><strong>Description:</strong> This theory posits that the efficacy of self-reflection in large language models (LLMs) is fundamentally determined by both the structure of the task and the internal properties of the model. Specifically, the ability of an LLM to improve its answers through iterative generate-then-reflect cycles is modulated by (a) the degree of answer verifiability, ambiguity, and adversarial content in the task, and (b) the model's internal capacity for meta-cognition, error detection, and reasoning. The theory predicts systematic patterns in when and how reflection leads to improvement, stagnation, or degradation of answer quality.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Task Structure Modulates Reflection Efficacy (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; has_property &#8594; high_verifiability</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; reflection &#8594; increases &#8594; answer_quality</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that on tasks with clear, verifiable answers (e.g., math, fact-based QA), iterative self-reflection reliably improves LLM performance. </li>
    <li>Tasks with ambiguous or open-ended answers show less consistent improvement from reflection. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law formalizes and generalizes empirical findings as a conditional, task-dependent law.</p>            <p><strong>What Already Exists:</strong> Reflection is known to help on tasks with clear correctness criteria.</p>            <p><strong>What is Novel:</strong> The explicit conditionality on task verifiability and formalization as a law is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [reflection improves on verifiable tasks]</li>
    <li>Lightman et al. (2023) Let’s Verify Step by Step [reflection and verification in LLMs]</li>
</ul>
            <h3>Statement 1: Model Meta-Cognitive Capacity Governs Reflection Benefit (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; model &#8594; has_property &#8594; high_meta_cognitive_capacity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; reflection &#8594; is_likely_to &#8594; improve_answer_quality</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Larger, more capable LLMs (e.g., GPT-4) show greater gains from self-reflection than smaller models, suggesting a dependence on internal meta-cognitive abilities. </li>
    <li>Models with explicit self-evaluation or error-detection modules outperform those without in reflection-based tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes scattered empirical findings into a general, model-dependent law.</p>            <p><strong>What Already Exists:</strong> Larger models and those with explicit self-evaluation perform better at self-reflection.</p>            <p><strong>What is Novel:</strong> The formalization of meta-cognitive capacity as a governing variable for reflection efficacy is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Huang et al. (2022) Large Language Models Can Self-Improve [model size and self-improvement]</li>
    <li>Lightman et al. (2023) Let’s Verify Step by Step [meta-cognition in LLMs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Reflection will yield greater improvements on tasks with high verifiability and low ambiguity, regardless of model size.</li>
                <li>Smaller or less capable models will show limited or negative gains from reflection, even on verifiable tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If models are trained with explicit meta-cognitive objectives, they may generalize reflection benefits to ambiguous or adversarial tasks.</li>
                <li>Reflection may enable models to self-discover new heuristics for resolving ambiguity, even in tasks previously resistant to improvement.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If reflection does not improve answer quality on highly verifiable tasks, the theory would be challenged.</li>
                <li>If small models consistently benefit from reflection as much as large models, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where reflection unexpectedly improves performance on highly ambiguous or adversarial tasks without explicit meta-cognitive training. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory synthesizes and formalizes scattered empirical findings into a general, predictive framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [reflection and task structure]</li>
    <li>Huang et al. (2022) Large Language Models Can Self-Improve [model size and self-improvement]</li>
    <li>Lightman et al. (2023) Let’s Verify Step by Step [meta-cognition and verification in LLMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Task- and Model-Dependence of Self-Reflection Efficacy in LLMs",
    "theory_description": "This theory posits that the efficacy of self-reflection in large language models (LLMs) is fundamentally determined by both the structure of the task and the internal properties of the model. Specifically, the ability of an LLM to improve its answers through iterative generate-then-reflect cycles is modulated by (a) the degree of answer verifiability, ambiguity, and adversarial content in the task, and (b) the model's internal capacity for meta-cognition, error detection, and reasoning. The theory predicts systematic patterns in when and how reflection leads to improvement, stagnation, or degradation of answer quality.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Task Structure Modulates Reflection Efficacy",
                "if": [
                    {
                        "subject": "task",
                        "relation": "has_property",
                        "object": "high_verifiability"
                    }
                ],
                "then": [
                    {
                        "subject": "reflection",
                        "relation": "increases",
                        "object": "answer_quality"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that on tasks with clear, verifiable answers (e.g., math, fact-based QA), iterative self-reflection reliably improves LLM performance.",
                        "uuids": []
                    },
                    {
                        "text": "Tasks with ambiguous or open-ended answers show less consistent improvement from reflection.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Reflection is known to help on tasks with clear correctness criteria.",
                    "what_is_novel": "The explicit conditionality on task verifiability and formalization as a law is novel.",
                    "classification_explanation": "The law formalizes and generalizes empirical findings as a conditional, task-dependent law.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [reflection improves on verifiable tasks]",
                        "Lightman et al. (2023) Let’s Verify Step by Step [reflection and verification in LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Model Meta-Cognitive Capacity Governs Reflection Benefit",
                "if": [
                    {
                        "subject": "model",
                        "relation": "has_property",
                        "object": "high_meta_cognitive_capacity"
                    }
                ],
                "then": [
                    {
                        "subject": "reflection",
                        "relation": "is_likely_to",
                        "object": "improve_answer_quality"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Larger, more capable LLMs (e.g., GPT-4) show greater gains from self-reflection than smaller models, suggesting a dependence on internal meta-cognitive abilities.",
                        "uuids": []
                    },
                    {
                        "text": "Models with explicit self-evaluation or error-detection modules outperform those without in reflection-based tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Larger models and those with explicit self-evaluation perform better at self-reflection.",
                    "what_is_novel": "The formalization of meta-cognitive capacity as a governing variable for reflection efficacy is novel.",
                    "classification_explanation": "The law synthesizes scattered empirical findings into a general, model-dependent law.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Huang et al. (2022) Large Language Models Can Self-Improve [model size and self-improvement]",
                        "Lightman et al. (2023) Let’s Verify Step by Step [meta-cognition in LLMs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Reflection will yield greater improvements on tasks with high verifiability and low ambiguity, regardless of model size.",
        "Smaller or less capable models will show limited or negative gains from reflection, even on verifiable tasks."
    ],
    "new_predictions_unknown": [
        "If models are trained with explicit meta-cognitive objectives, they may generalize reflection benefits to ambiguous or adversarial tasks.",
        "Reflection may enable models to self-discover new heuristics for resolving ambiguity, even in tasks previously resistant to improvement."
    ],
    "negative_experiments": [
        "If reflection does not improve answer quality on highly verifiable tasks, the theory would be challenged.",
        "If small models consistently benefit from reflection as much as large models, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where reflection unexpectedly improves performance on highly ambiguous or adversarial tasks without explicit meta-cognitive training.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some small models show modest gains from reflection on certain tasks, suggesting a more nuanced relationship between model capacity and reflection efficacy.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with mixed verifiability (e.g., multi-part questions) may show heterogeneous reflection effects.",
        "Models with external verification modules may bypass some limitations of internal meta-cognition."
    ],
    "existing_theory": {
        "what_already_exists": "Empirical work has shown that reflection helps on verifiable tasks and with larger models.",
        "what_is_novel": "The explicit, formalized theory of joint task- and model-dependence for reflection efficacy is novel.",
        "classification_explanation": "The theory synthesizes and formalizes scattered empirical findings into a general, predictive framework.",
        "likely_classification": "new",
        "references": [
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [reflection and task structure]",
            "Huang et al. (2022) Large Language Models Can Self-Improve [model size and self-improvement]",
            "Lightman et al. (2023) Let’s Verify Step by Step [meta-cognition and verification in LLMs]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.",
    "original_theory_id": "theory-623",
    "original_theory_name": "Task- and Model-Dependence of Self-Reflection Efficacy in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Task- and Model-Dependence of Self-Reflection Efficacy in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
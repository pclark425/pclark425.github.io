<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Induced Calibration Distortion Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-650</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-650</p>
                <p><strong>Name:</strong> Prompt-Induced Calibration Distortion Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries, based on the following results.</p>
                <p><strong>Description:</strong> The structure and content of prompts given to LLMs for probabilistic forecasting can systematically distort the calibration and distribution of their probability estimates, sometimes worsening performance even when prompts are designed to elicit human-like reasoning (e.g., chain-of-thought, rationale, or superforecasting strategies). This effect is mediated by the interaction between prompt content, model pretraining biases, and the outcome distribution of the forecasting task. The theory is supported by evidence that rationale, breakdown, base-rate, and both-sides prompting can increase mean predicted probabilities and worsen calibration/Brier scores, especially on datasets with imbalanced (e.g., negative-skewed) outcomes, and that simple prompts can produce misleadingly good raw Brier scores due to alignment with dataset skew. The theory also accounts for the context-dependence of prompt effects, as superforecasting prompts can improve human-LLM hybrid accuracy in some settings.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt-Calibration Distortion Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; chain-of-thought, rationale, or structured forecasting strategies<span style="color: #888888;">, and</span></div>
        <div>&#8226; forecasting task &#8594; has &#8594; imbalanced outcome distribution (e.g., more negative than positive events)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; produces &#8594; probability estimates with systematically shifted means and worsened calibration compared to simple prompts</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>PaLM2 with rationale, breakdown, base-rate, and both-sides prompting produced higher mean probabilities and worse Brier scores than the basic baseline, especially on datasets with more negative outcomes. <a href="../results/extraction-result-5706.html#e5706.1" class="evidence-link">[e5706.1]</a> <a href="../results/extraction-result-5706.html#e5706.2" class="evidence-link">[e5706.2]</a> </li>
    <li>Producing rationales systematically increased the model's average predicted probabilities (compared to 'Just Answer'), producing higher probabilities for both events that eventually resolved True and False; this raised the Brier for the model on the imbalanced dataset. <a href="../results/extraction-result-5706.html#e5706.1" class="evidence-link">[e5706.1]</a> </li>
    <li>Structured forecasting strategies (Breakdown, Base Rates, Both Sides) did not improve over the Basic Baseline on raw Brier; interventions often raised mean predicted probabilities (worsening performance on negatively skewed dataset). <a href="../results/extraction-result-5706.html#e5706.2" class="evidence-link">[e5706.2]</a> </li>
    <li>Rationale prompting altered the model's output distribution (increasing mean probabilities) and therefore worsened performance on datasets with many negative outcomes; indicates sensitivity of LLM forecasts to prompt-inserted reasoning and a lack of robust calibration when using chain-of-thought. <a href="../results/extraction-result-5706.html#e5706.1" class="evidence-link">[e5706.1]</a> </li>
    <li>Translating human forecasting strategies into LLM prompts did not reliably improve predictive accuracy; suggests LLMs do not faithfully implement these strategies in a way that yields better calibrated probabilities, and that prompts can interact with model biases in nontrivial ways. <a href="../results/extraction-result-5706.html#e5706.2" class="evidence-link">[e5706.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Prompt effects are known, but their systematic impact on calibration in forecasting, especially in the context of outcome imbalance and rationale/chain-of-thought prompting, is a new formalization.</p>            <p><strong>What Already Exists:</strong> Prompt engineering is known to affect LLM outputs, and calibration is a known issue in ML and LLMs.</p>            <p><strong>What is Novel:</strong> This law formalizes the specific interaction between prompt structure and calibration in probabilistic forecasting, especially in imbalanced datasets, and demonstrates that human-like reasoning prompts can systematically worsen calibration.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]</li>
    <li>Lin et al. (2024) Can Language Models Use Forecasting Strategies? [prompt-induced calibration distortion]</li>
</ul>
            <h3>Statement 1: Negativity Bias Amplification Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; simple probability output prompt<span style="color: #888888;">, and</span></div>
        <div>&#8226; forecasting dataset &#8594; is_skewed_toward &#8594; negative outcomes</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; outputs &#8594; systematically lower probabilities, appearing to outperform humans on raw Brier but underperforming on balanced metrics</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>PaLM2 basic baseline produced lower mean probabilities and better raw Brier than humans on a negative-skewed dataset, but underperformed on weighted Brier. <a href="../results/extraction-result-5706.html#e5706.0" class="evidence-link">[e5706.0]</a> </li>
    <li>Apparent superior raw Brier performance driven by model negativity bias combined with dataset skew toward negative outcomes; probability calibration issues. <a href="../results/extraction-result-5706.html#e5706.0" class="evidence-link">[e5706.0]</a> </li>
    <li>The Basic model tended to output systematically low probabilities (see average probability: Basic ≈ 0.2529 vs Human ≈ 0.3280) which gave apparent advantage on an imbalanced dataset dominated by negative outcomes. <a href="../results/extraction-result-5706.html#e5706.0" class="evidence-link">[e5706.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general principle is known, but its explicit application to LLM forecasting calibration and the interaction with prompt simplicity and dataset skew is new.</p>            <p><strong>What Already Exists:</strong> Negativity bias and calibration issues are known in ML and LLMs.</p>            <p><strong>What is Novel:</strong> This law formalizes how prompt simplicity interacts with dataset skew to produce misleadingly good raw Brier scores, and highlights the risk of misinterpreting LLM forecasting performance on imbalanced datasets.</p>
            <p><strong>References:</strong> <ul>
    <li>Lin et al. (2024) Can Language Models Use Forecasting Strategies? [negativity bias in LLM forecasting]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Applying chain-of-thought or rationale prompting to LLMs on a new, imbalanced forecasting dataset will shift the mean predicted probabilities upward and worsen calibration compared to a simple prompt.</li>
                <li>On a balanced dataset, rationale or structured prompting may not worsen calibration, and may even improve it.</li>
                <li>If a dataset is positively skewed (more positive outcomes), rationale prompting may shift mean probabilities in a way that worsens calibration in the opposite direction.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There exists a prompt structure that can consistently improve both calibration and accuracy across all outcome distributions, but it may require dynamic adaptation to dataset skew.</li>
                <li>Combining prompt-based reasoning with explicit calibration feedback (e.g., meta-prompts or post-hoc calibration modules) could mitigate prompt-induced calibration distortion.</li>
                <li>Fine-tuning LLMs specifically for calibration on imbalanced datasets may reduce or eliminate prompt-induced calibration distortion.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If rationale or chain-of-thought prompting does not shift mean probabilities or worsen calibration on imbalanced datasets, the prompt-calibration distortion law would be falsified.</li>
                <li>If simple prompts do not produce negativity bias on negative-skewed datasets, the negativity bias amplification law would be undermined.</li>
                <li>If a prompt structure is found that always improves calibration regardless of dataset skew, the theory would need revision.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs may have internal calibration mechanisms or post-processing that compensate for prompt-induced distortions, especially if fine-tuned for calibration. </li>
    <li>Retrieval-augmented or ensemble methods may interact with prompt effects in ways not fully captured by this theory. <a href="../results/extraction-result-5823.html#e5823.0" class="evidence-link">[e5823.0]</a> <a href="../results/extraction-result-5790.html#e5790.0" class="evidence-link">[e5790.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> Prompt effects are known, but their systematic impact on calibration in forecasting, especially in the context of outcome imbalance and rationale/chain-of-thought prompting, is a new formalization.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]</li>
    <li>Lin et al. (2024) Can Language Models Use Forecasting Strategies? [prompt-induced calibration distortion]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Induced Calibration Distortion Theory",
    "theory_description": "The structure and content of prompts given to LLMs for probabilistic forecasting can systematically distort the calibration and distribution of their probability estimates, sometimes worsening performance even when prompts are designed to elicit human-like reasoning (e.g., chain-of-thought, rationale, or superforecasting strategies). This effect is mediated by the interaction between prompt content, model pretraining biases, and the outcome distribution of the forecasting task. The theory is supported by evidence that rationale, breakdown, base-rate, and both-sides prompting can increase mean predicted probabilities and worsen calibration/Brier scores, especially on datasets with imbalanced (e.g., negative-skewed) outcomes, and that simple prompts can produce misleadingly good raw Brier scores due to alignment with dataset skew. The theory also accounts for the context-dependence of prompt effects, as superforecasting prompts can improve human-LLM hybrid accuracy in some settings.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt-Calibration Distortion Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "chain-of-thought, rationale, or structured forecasting strategies"
                    },
                    {
                        "subject": "forecasting task",
                        "relation": "has",
                        "object": "imbalanced outcome distribution (e.g., more negative than positive events)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "produces",
                        "object": "probability estimates with systematically shifted means and worsened calibration compared to simple prompts"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "PaLM2 with rationale, breakdown, base-rate, and both-sides prompting produced higher mean probabilities and worse Brier scores than the basic baseline, especially on datasets with more negative outcomes.",
                        "uuids": [
                            "e5706.1",
                            "e5706.2"
                        ]
                    },
                    {
                        "text": "Producing rationales systematically increased the model's average predicted probabilities (compared to 'Just Answer'), producing higher probabilities for both events that eventually resolved True and False; this raised the Brier for the model on the imbalanced dataset.",
                        "uuids": [
                            "e5706.1"
                        ]
                    },
                    {
                        "text": "Structured forecasting strategies (Breakdown, Base Rates, Both Sides) did not improve over the Basic Baseline on raw Brier; interventions often raised mean predicted probabilities (worsening performance on negatively skewed dataset).",
                        "uuids": [
                            "e5706.2"
                        ]
                    },
                    {
                        "text": "Rationale prompting altered the model's output distribution (increasing mean probabilities) and therefore worsened performance on datasets with many negative outcomes; indicates sensitivity of LLM forecasts to prompt-inserted reasoning and a lack of robust calibration when using chain-of-thought.",
                        "uuids": [
                            "e5706.1"
                        ]
                    },
                    {
                        "text": "Translating human forecasting strategies into LLM prompts did not reliably improve predictive accuracy; suggests LLMs do not faithfully implement these strategies in a way that yields better calibrated probabilities, and that prompts can interact with model biases in nontrivial ways.",
                        "uuids": [
                            "e5706.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering is known to affect LLM outputs, and calibration is a known issue in ML and LLMs.",
                    "what_is_novel": "This law formalizes the specific interaction between prompt structure and calibration in probabilistic forecasting, especially in imbalanced datasets, and demonstrates that human-like reasoning prompts can systematically worsen calibration.",
                    "classification_explanation": "Prompt effects are known, but their systematic impact on calibration in forecasting, especially in the context of outcome imbalance and rationale/chain-of-thought prompting, is a new formalization.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]",
                        "Lin et al. (2024) Can Language Models Use Forecasting Strategies? [prompt-induced calibration distortion]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Negativity Bias Amplification Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "simple probability output prompt"
                    },
                    {
                        "subject": "forecasting dataset",
                        "relation": "is_skewed_toward",
                        "object": "negative outcomes"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "outputs",
                        "object": "systematically lower probabilities, appearing to outperform humans on raw Brier but underperforming on balanced metrics"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "PaLM2 basic baseline produced lower mean probabilities and better raw Brier than humans on a negative-skewed dataset, but underperformed on weighted Brier.",
                        "uuids": [
                            "e5706.0"
                        ]
                    },
                    {
                        "text": "Apparent superior raw Brier performance driven by model negativity bias combined with dataset skew toward negative outcomes; probability calibration issues.",
                        "uuids": [
                            "e5706.0"
                        ]
                    },
                    {
                        "text": "The Basic model tended to output systematically low probabilities (see average probability: Basic ≈ 0.2529 vs Human ≈ 0.3280) which gave apparent advantage on an imbalanced dataset dominated by negative outcomes.",
                        "uuids": [
                            "e5706.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Negativity bias and calibration issues are known in ML and LLMs.",
                    "what_is_novel": "This law formalizes how prompt simplicity interacts with dataset skew to produce misleadingly good raw Brier scores, and highlights the risk of misinterpreting LLM forecasting performance on imbalanced datasets.",
                    "classification_explanation": "The general principle is known, but its explicit application to LLM forecasting calibration and the interaction with prompt simplicity and dataset skew is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lin et al. (2024) Can Language Models Use Forecasting Strategies? [negativity bias in LLM forecasting]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Applying chain-of-thought or rationale prompting to LLMs on a new, imbalanced forecasting dataset will shift the mean predicted probabilities upward and worsen calibration compared to a simple prompt.",
        "On a balanced dataset, rationale or structured prompting may not worsen calibration, and may even improve it.",
        "If a dataset is positively skewed (more positive outcomes), rationale prompting may shift mean probabilities in a way that worsens calibration in the opposite direction."
    ],
    "new_predictions_unknown": [
        "There exists a prompt structure that can consistently improve both calibration and accuracy across all outcome distributions, but it may require dynamic adaptation to dataset skew.",
        "Combining prompt-based reasoning with explicit calibration feedback (e.g., meta-prompts or post-hoc calibration modules) could mitigate prompt-induced calibration distortion.",
        "Fine-tuning LLMs specifically for calibration on imbalanced datasets may reduce or eliminate prompt-induced calibration distortion."
    ],
    "negative_experiments": [
        "If rationale or chain-of-thought prompting does not shift mean probabilities or worsen calibration on imbalanced datasets, the prompt-calibration distortion law would be falsified.",
        "If simple prompts do not produce negativity bias on negative-skewed datasets, the negativity bias amplification law would be undermined.",
        "If a prompt structure is found that always improves calibration regardless of dataset skew, the theory would need revision."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs may have internal calibration mechanisms or post-processing that compensate for prompt-induced distortions, especially if fine-tuned for calibration.",
            "uuids": []
        },
        {
            "text": "Retrieval-augmented or ensemble methods may interact with prompt effects in ways not fully captured by this theory.",
            "uuids": [
                "e5823.0",
                "e5790.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Superforecasting prompts improved human-LLM hybrid accuracy in some studies, suggesting prompt effects are context-dependent and may be beneficial in interactive or hybrid settings.",
            "uuids": [
                "e5704.0"
            ]
        },
        {
            "text": "In some retrieval-augmented or fine-tuned LLMs, calibration can be improved by other means, potentially mitigating prompt-induced distortion.",
            "uuids": [
                "e5823.0",
                "e5823.1"
            ]
        }
    ],
    "special_cases": [
        "Prompt effects may be mitigated by ensembling across multiple prompt structures or by explicit calibration post-processing.",
        "Calibration feedback or meta-prompts may correct prompt-induced distortions.",
        "In human-LLM hybrid or interactive settings, prompt effects may be beneficial due to human correction or deliberation.",
        "Retrieval-augmented LLMs may be less sensitive to prompt-induced calibration distortion if external evidence dominates the forecast."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and calibration issues are known in LLMs and ML, and prompt effects on output distribution are established.",
        "what_is_novel": "This theory formalizes the systematic, dataset-dependent impact of prompt structure on calibration in probabilistic forecasting, especially the counterintuitive finding that human-like reasoning prompts can worsen calibration on imbalanced datasets.",
        "classification_explanation": "Prompt effects are known, but their systematic impact on calibration in forecasting, especially in the context of outcome imbalance and rationale/chain-of-thought prompting, is a new formalization.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompting for reasoning]",
            "Lin et al. (2024) Can Language Models Use Forecasting Strategies? [prompt-induced calibration distortion]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
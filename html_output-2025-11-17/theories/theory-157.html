<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proxy-to-Ground-Truth Gap Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-157</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-157</p>
                <p><strong>Name:</strong> Proxy-to-Ground-Truth Gap Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about the evaluation and validation of incremental versus transformational scientific discoveries in automated systems, based on the following results.</p>
                <p><strong>Description:</strong> Automated discovery systems frequently optimize proxy metrics (computational predictions, surrogate objectives, simulated outcomes, or intermediate validation measures) rather than ground-truth experimental outcomes. The gap between proxy performance and ground-truth validation creates a systematic bias where systems appear more successful in computational or simulated evaluation than in real-world validation. This gap varies systematically with: (1) the novelty and extrapolation distance of the discovery from training distributions, (2) the maturity and physical grounding of the proxy metric, and (3) the domain's amenability to computational modeling. The gap is typically larger for transformational discoveries than incremental ones because transformational discoveries involve greater extrapolation from established knowledge and training distributions, and often require validation in regimes where proxy metrics are less well-calibrated.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <ol>
                <li><a href="../evaluations/theory-evaluation-7.html">theory-evaluation-7</a></li>
                <li><a href="../evaluations/theory-evaluation-8.html">theory-evaluation-8</a></li>
                <li><a href="../evaluations/theory-evaluation-31.html">theory-evaluation-31</a></li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Automated systems systematically overestimate discovery success when evaluated on proxy metrics compared to ground-truth experimental validation, with the overestimation increasing as the proxy becomes more removed from ground truth.</li>
                <li>The proxy-to-ground-truth gap increases with the novelty and extrapolation distance of the discovery from training data, because proxies are typically calibrated on known regimes and become less reliable in novel regimes.</li>
                <li>Systems optimizing proxy metrics will exhibit higher false positive rates for transformational discoveries than for incremental discoveries, because transformational discoveries involve greater extrapolation where proxy calibration is weakest.</li>
                <li>Multifidelity approaches that explicitly model and correct proxy biases (e.g., Gemini) can reduce but not eliminate the proxy-to-ground-truth gap, because some systematic errors remain even after correction.</li>
                <li>The computational cost advantage of proxy evaluation creates an economic incentive to defer ground-truth validation, leading to accumulation of unvalidated discoveries and potential publication of false positives.</li>
                <li>The quality of a proxy metric depends on: (1) the physical/theoretical grounding of the proxy, (2) the domain's amenability to computational modeling, and (3) the calibration data available in the regime of interest.</li>
                <li>Domains with mature physics-based simulations (e.g., molecular dynamics, quantum chemistry) show smaller proxy-to-ground-truth gaps than domains with purely empirical proxies (e.g., drug activity prediction).</li>
                <li>The gap manifests differently across validation cascades: proxy → intermediate validation → ground truth, with error accumulation at each stage.</li>
                <li>Human expertise in proxy design can reduce but not eliminate the gap, as even expert-designed proxies fail in extrapolative regimes.</li>
                <li>The temporal evolution of proxy quality follows domain maturity: as domains mature and more validation data accumulates, proxies improve but never perfectly match ground truth in novel regimes.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>CycleResearcher generates papers with fabricated experimental results, achieving high simulated review scores (avg 5.36, up to 7.02 with rejection sampling) but lacking real experimental validation, representing an extreme case of proxy-only evaluation <a href="../results/extraction-result-1202.html#e1202.0" class="evidence-link">[e1202.0]</a> </li>
    <li>Ada self-driving lab optimizes pseudomobility (a surrogate metric computed from spectroscopy and conductance) rather than direct device-level hole mobility, with no device-level validation reported <a href="../results/extraction-result-1443.html#e1443.0" class="evidence-link">[e1443.0]</a> </li>
    <li>SPOCK predicts long-term orbital stability using short-integration features as proxies, with fundamental unpredictability due to chaos (instability times lognormally distributed with σ≈0.4 dex), limiting any classifier's ceiling <a href="../results/extraction-result-1437.html#e1437.0" class="evidence-link">[e1437.0]</a> </li>
    <li>SciAgents generates materials hypotheses with novelty/feasibility scores from Semantic Scholar searches but lacks experimental validation of the proposed materials, with validation explicitly noted as future work <a href="../results/extraction-result-1178.html#e1178.0" class="evidence-link">[e1178.0]</a> </li>
    <li>TAIS identifies disease-predictive genes computationally with cross-validation accuracy ~80% but lacks wet-lab or clinical validation, with computational corroboration only <a href="../results/extraction-result-1215.html#e1215.0" class="evidence-link">[e1215.0]</a> </li>
    <li>Docking virtual screening (Lyu et al.) shows high false positive rates despite computational predictions, with majority of high-scoring compounds being false positives even when screening ~170M compounds <a href="../results/extraction-result-1289.html#e1289.6" class="evidence-link">[e1289.6]</a> </li>
    <li>Active learning systems like Eve use proxy experiments that may not reflect full-device performance, with difficulty designing meaningful proxies noted as a key limitation <a href="../results/extraction-result-1289.html#e1289.1" class="evidence-link">[e1289.1]</a> <a href="../results/extraction-result-1190.html#e1190.6" class="evidence-link">[e1190.6]</a> </li>
    <li>Gemini algorithm explicitly attempts to correct systematic biases between low-cost proxy experiments and higher-fidelity measurements through multifidelity learning, acknowledging the proxy-to-ground-truth gap as a fundamental challenge <a href="../results/extraction-result-1190.html#e1190.3" class="evidence-link">[e1190.3]</a> </li>
    <li>LLM-SR optimizes NMSE on training data but shows degraded performance on out-of-distribution tests, with OOD NMSE often orders of magnitude worse than in-distribution performance <a href="../results/extraction-result-1439.html#e1439.0" class="evidence-link">[e1439.0]</a> </li>
    <li>Neural theorem provers solve benchmark problems (10 IMO problems) but the gap between solving benchmarks and producing original mathematical creativity remains, with interpretability of learned heuristics limited <a href="../results/extraction-result-1266.html#e1266.3" class="evidence-link">[e1266.3]</a> </li>
    <li>Clustering systems evaluated on internal metrics (silhouette coefficient, within-cluster variance) may not reflect meaningful domain discoveries, with the oracle problem (no ground truth) making validation difficult <a href="../results/extraction-result-1168.html#e1168.0" class="evidence-link">[e1168.0]</a> </li>
    <li>Automated materials ML systems require experimental synthesis and testing to validate computational predictions, with experimental validation noted as necessary but costly <a href="../results/extraction-result-1250.html#e1250.8" class="evidence-link">[e1250.8]</a> </li>
    <li>BenevolentAI drug repurposing (Baricitinib for COVID-19) requires clinical validation beyond computational predictions, with experimental and clinical follow-up necessary to cement discoveries <a href="../results/extraction-result-1250.html#e1250.2" class="evidence-link">[e1250.2]</a> </li>
    <li>Chemistry42 and similar platforms require experimental follow-up to validate designed molecules, with validation described as experimental testing and benchmarking <a href="../results/extraction-result-1250.html#e1250.6" class="evidence-link">[e1250.6]</a> </li>
    <li>AtomAgents performs atomistic simulations (LAMMPS) and validates against literature values but lacks experimental (wet-lab) validation, with validation being computational/comparative only <a href="../results/extraction-result-1444.html#e1444.0" class="evidence-link">[e1444.0]</a> </li>
    <li>Topaz particle picking achieves 1.72× more particles than manual picks and highest-resolution reconstruction, but requires downstream reconstruction quality as ground truth rather than direct validation of individual picks <a href="../results/extraction-result-1200.html#e1200.3" class="evidence-link">[e1200.3]</a> </li>
    <li>Magnon NN predicts magnetic coupling constants with MAE ±0.0055 meV for J, validated against literature values rather than new experimental measurements <a href="../results/extraction-result-1200.html#e1200.1" class="evidence-link">[e1200.1]</a> </li>
    <li>DATAVOYAGER demonstrates automated data analysis workflows but validation is via replication of published analyses rather than new experimental discoveries <a href="../results/extraction-result-1227.html#e1227.0" class="evidence-link">[e1227.0]</a> </li>
    <li>AI Feynman recovers 100/100 equations from benchmark set but these are rediscoveries of known laws rather than validated new physical principles <a href="../results/extraction-result-1431.html#e1431.0" class="evidence-link">[e1431.0]</a> </li>
    <li>PaperQA achieves 86.3% accuracy on PubMedQA but QA accuracy is a proxy for actual research utility and discovery capability <a href="../results/extraction-result-1445.html#e1445.3" class="evidence-link">[e1445.3]</a> </li>
    <li>Eunomia achieves high extraction F1 scores (up to 0.920 for dopants) but extraction accuracy is a proxy for enabling downstream materials discovery <a href="../results/extraction-result-1209.html#e1209.0" class="evidence-link">[e1209.0]</a> </li>
    <li>HTE drop-cast platform achieves high throughput (6,048 films/day) and reproducibility (R²=0.97) but absorbance loss is a single proxy for device stability, with electrical performance not optimized <a href="../results/extraction-result-1455.html#e1455.1" class="evidence-link">[e1455.1]</a> </li>
    <li>Autonomous thin-film workflow uses proxy experiments with multifidelity optimization but faces challenges in translating proxy metrics to final performance <a href="../results/extraction-result-1190.html#e1190.6" class="evidence-link">[e1190.6]</a> </li>
    <li>LLM Reviewer achieves 65% balanced accuracy against human decisions but automated review scores are proxies for actual scientific quality and impact <a href="../results/extraction-result-1214.html#e1214.1" class="evidence-link">[e1214.1]</a> </li>
    <li>CycleReviewer shows 26.89% reduction in Proxy MAE but explicitly uses n-1 reviewer averages as pseudo-ground-truth, acknowledging the proxy nature of the evaluation <a href="../results/extraction-result-1202.html#e1202.1" class="evidence-link">[e1202.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Systems that incorporate explicit proxy-bias correction (like Gemini) will show 20-50% smaller gaps between computational and experimental success rates than systems without such correction, with the reduction being larger for incremental discoveries than transformational ones.</li>
                <li>The false positive rate for automated discoveries will be inversely correlated with the amount of experimental validation performed, following approximately a power law relationship.</li>
                <li>Domains with well-established computational proxies (e.g., protein structure prediction, molecular dynamics) will show proxy-to-ground-truth gaps of 5-20%, while domains with poorly validated proxies (e.g., drug activity, materials properties) will show gaps of 40-80%.</li>
                <li>In materials discovery, systems using physics-based proxies (DFT, molecular dynamics) will show 2-3× lower false positive rates than systems using purely data-driven proxies (neural network predictions).</li>
                <li>The proxy-to-ground-truth gap will be larger for properties requiring long-timescale dynamics (e.g., stability, degradation) than for static properties (e.g., structure, composition).</li>
                <li>Systems that report uncertainty estimates calibrated on validation data will show better correlation between predicted uncertainty and actual proxy-to-ground-truth gap than systems without uncertainty quantification.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether machine learning meta-models could be trained to predict the proxy-to-ground-truth gap for individual discoveries with sufficient accuracy (e.g., R²>0.7) to enable reliable prioritization of experimental validation, or whether the gap is fundamentally unpredictable due to the novelty of transformational discoveries.</li>
                <li>Whether the proxy-to-ground-truth gap could be eliminated entirely through sufficiently sophisticated simulation and modeling, or whether there are fundamental limits (e.g., computational complexity, chaotic dynamics, emergent phenomena) that prevent perfect proxy-ground-truth alignment.</li>
                <li>Whether certain types of transformational discoveries might actually show smaller proxy-to-ground-truth gaps than incremental discoveries in specific domains, potentially due to transformational discoveries being more amenable to first-principles modeling.</li>
                <li>Whether active learning strategies that explicitly optimize for reducing the proxy-to-ground-truth gap (rather than optimizing the proxy itself) could achieve order-of-magnitude improvements in experimental validation success rates.</li>
                <li>Whether the gap could be reduced by orders of magnitude through hybrid approaches that combine multiple orthogonal proxies (e.g., physics-based + data-driven + expert heuristics), or whether proxies are fundamentally correlated in their failure modes.</li>
                <li>Whether the proxy-to-ground-truth gap follows universal scaling laws across domains, or whether each domain has fundamentally different gap characteristics that prevent cross-domain learning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where proxy metrics perfectly predict experimental outcomes (R²>0.95, false positive rate <5%) across both incremental and transformational discoveries would challenge the universality of the gap.</li>
                <li>Demonstrating that transformational discoveries consistently show smaller proxy-to-ground-truth gaps than incremental ones across multiple domains would contradict the core theory prediction.</li>
                <li>Showing that systems without proxy-bias correction perform as well as those with correction in experimental validation (within 10% success rate) would question the value of gap-reduction methods.</li>
                <li>Finding that the proxy-to-ground-truth gap does not increase with extrapolation distance from training data would challenge the mechanism proposed for why transformational discoveries show larger gaps.</li>
                <li>Demonstrating that purely data-driven proxies perform as well as physics-based proxies in novel regimes would contradict the theory's prediction about proxy quality dependence on physical grounding.</li>
                <li>Showing that the economic incentive to defer validation does not lead to accumulation of unvalidated discoveries (e.g., if false positives are naturally filtered by other mechanisms) would challenge the theory's prediction about systemic bias.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The role of domain-specific knowledge and human expertise in determining proxy quality and how this expertise can be systematically encoded </li>
    <li>How the gap evolves as computational methods improve over time and whether there are diminishing returns or fundamental limits </li>
    <li>The interaction between multiple proxy metrics used simultaneously and whether they exhibit correlated or independent failure modes </li>
    <li>The psychological and sociological factors that influence acceptance of proxy-validated discoveries in scientific communities </li>
    <li>The role of validation cascades (proxy → intermediate → ground truth) and how errors accumulate or cancel across stages </li>
    <li>Whether certain discovery types (e.g., negative results, null findings) show different proxy-to-ground-truth gap characteristics </li>
    <li>The impact of publication bias and selective reporting on the observed proxy-to-ground-truth gap in the literature </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Goodhart (1975) Problems of Monetary Management: The U.K. Experience [Goodhart's Law: when a measure becomes a target, it ceases to be a good measure - directly applicable to proxy optimization]</li>
    <li>Campbell (1979) Assessing the impact of planned social change [Campbell's Law on indicator corruption when used for decision-making]</li>
    <li>Marcus (2018) Deep Learning: A Critical Appraisal [Discusses brittleness and generalization failures in deep learning, related to proxy-ground-truth gaps]</li>
    <li>Sculley et al. (2015) Hidden Technical Debt in Machine Learning Systems [Discusses the gap between ML metrics and real-world performance]</li>
    <li>Lipton & Steinhardt (2018) Troubling Trends in Machine Learning Scholarship [Discusses the gap between benchmark performance and real-world utility]</li>
    <li>D'Amour et al. (2020) Underspecification Presents Challenges for Credibility in Modern Machine Learning [Discusses how models can perform well on proxies but fail in deployment]</li>
    <li>Recht et al. (2019) Do ImageNet Classifiers Generalize to ImageNet? [Demonstrates distribution shift and proxy-ground-truth gaps in computer vision]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Proxy-to-Ground-Truth Gap Theory",
    "theory_description": "Automated discovery systems frequently optimize proxy metrics (computational predictions, surrogate objectives, simulated outcomes, or intermediate validation measures) rather than ground-truth experimental outcomes. The gap between proxy performance and ground-truth validation creates a systematic bias where systems appear more successful in computational or simulated evaluation than in real-world validation. This gap varies systematically with: (1) the novelty and extrapolation distance of the discovery from training distributions, (2) the maturity and physical grounding of the proxy metric, and (3) the domain's amenability to computational modeling. The gap is typically larger for transformational discoveries than incremental ones because transformational discoveries involve greater extrapolation from established knowledge and training distributions, and often require validation in regimes where proxy metrics are less well-calibrated.",
    "supporting_evidence": [
        {
            "text": "CycleResearcher generates papers with fabricated experimental results, achieving high simulated review scores (avg 5.36, up to 7.02 with rejection sampling) but lacking real experimental validation, representing an extreme case of proxy-only evaluation",
            "uuids": [
                "e1202.0"
            ]
        },
        {
            "text": "Ada self-driving lab optimizes pseudomobility (a surrogate metric computed from spectroscopy and conductance) rather than direct device-level hole mobility, with no device-level validation reported",
            "uuids": [
                "e1443.0"
            ]
        },
        {
            "text": "SPOCK predicts long-term orbital stability using short-integration features as proxies, with fundamental unpredictability due to chaos (instability times lognormally distributed with σ≈0.4 dex), limiting any classifier's ceiling",
            "uuids": [
                "e1437.0"
            ]
        },
        {
            "text": "SciAgents generates materials hypotheses with novelty/feasibility scores from Semantic Scholar searches but lacks experimental validation of the proposed materials, with validation explicitly noted as future work",
            "uuids": [
                "e1178.0"
            ]
        },
        {
            "text": "TAIS identifies disease-predictive genes computationally with cross-validation accuracy ~80% but lacks wet-lab or clinical validation, with computational corroboration only",
            "uuids": [
                "e1215.0"
            ]
        },
        {
            "text": "Docking virtual screening (Lyu et al.) shows high false positive rates despite computational predictions, with majority of high-scoring compounds being false positives even when screening ~170M compounds",
            "uuids": [
                "e1289.6"
            ]
        },
        {
            "text": "Active learning systems like Eve use proxy experiments that may not reflect full-device performance, with difficulty designing meaningful proxies noted as a key limitation",
            "uuids": [
                "e1289.1",
                "e1190.6"
            ]
        },
        {
            "text": "Gemini algorithm explicitly attempts to correct systematic biases between low-cost proxy experiments and higher-fidelity measurements through multifidelity learning, acknowledging the proxy-to-ground-truth gap as a fundamental challenge",
            "uuids": [
                "e1190.3"
            ]
        },
        {
            "text": "LLM-SR optimizes NMSE on training data but shows degraded performance on out-of-distribution tests, with OOD NMSE often orders of magnitude worse than in-distribution performance",
            "uuids": [
                "e1439.0"
            ]
        },
        {
            "text": "Neural theorem provers solve benchmark problems (10 IMO problems) but the gap between solving benchmarks and producing original mathematical creativity remains, with interpretability of learned heuristics limited",
            "uuids": [
                "e1266.3"
            ]
        },
        {
            "text": "Clustering systems evaluated on internal metrics (silhouette coefficient, within-cluster variance) may not reflect meaningful domain discoveries, with the oracle problem (no ground truth) making validation difficult",
            "uuids": [
                "e1168.0"
            ]
        },
        {
            "text": "Automated materials ML systems require experimental synthesis and testing to validate computational predictions, with experimental validation noted as necessary but costly",
            "uuids": [
                "e1250.8"
            ]
        },
        {
            "text": "BenevolentAI drug repurposing (Baricitinib for COVID-19) requires clinical validation beyond computational predictions, with experimental and clinical follow-up necessary to cement discoveries",
            "uuids": [
                "e1250.2"
            ]
        },
        {
            "text": "Chemistry42 and similar platforms require experimental follow-up to validate designed molecules, with validation described as experimental testing and benchmarking",
            "uuids": [
                "e1250.6"
            ]
        },
        {
            "text": "AtomAgents performs atomistic simulations (LAMMPS) and validates against literature values but lacks experimental (wet-lab) validation, with validation being computational/comparative only",
            "uuids": [
                "e1444.0"
            ]
        },
        {
            "text": "Topaz particle picking achieves 1.72× more particles than manual picks and highest-resolution reconstruction, but requires downstream reconstruction quality as ground truth rather than direct validation of individual picks",
            "uuids": [
                "e1200.3"
            ]
        },
        {
            "text": "Magnon NN predicts magnetic coupling constants with MAE ±0.0055 meV for J, validated against literature values rather than new experimental measurements",
            "uuids": [
                "e1200.1"
            ]
        },
        {
            "text": "DATAVOYAGER demonstrates automated data analysis workflows but validation is via replication of published analyses rather than new experimental discoveries",
            "uuids": [
                "e1227.0"
            ]
        },
        {
            "text": "AI Feynman recovers 100/100 equations from benchmark set but these are rediscoveries of known laws rather than validated new physical principles",
            "uuids": [
                "e1431.0"
            ]
        },
        {
            "text": "PaperQA achieves 86.3% accuracy on PubMedQA but QA accuracy is a proxy for actual research utility and discovery capability",
            "uuids": [
                "e1445.3"
            ]
        },
        {
            "text": "Eunomia achieves high extraction F1 scores (up to 0.920 for dopants) but extraction accuracy is a proxy for enabling downstream materials discovery",
            "uuids": [
                "e1209.0"
            ]
        },
        {
            "text": "HTE drop-cast platform achieves high throughput (6,048 films/day) and reproducibility (R²=0.97) but absorbance loss is a single proxy for device stability, with electrical performance not optimized",
            "uuids": [
                "e1455.1"
            ]
        },
        {
            "text": "Autonomous thin-film workflow uses proxy experiments with multifidelity optimization but faces challenges in translating proxy metrics to final performance",
            "uuids": [
                "e1190.6"
            ]
        },
        {
            "text": "LLM Reviewer achieves 65% balanced accuracy against human decisions but automated review scores are proxies for actual scientific quality and impact",
            "uuids": [
                "e1214.1"
            ]
        },
        {
            "text": "CycleReviewer shows 26.89% reduction in Proxy MAE but explicitly uses n-1 reviewer averages as pseudo-ground-truth, acknowledging the proxy nature of the evaluation",
            "uuids": [
                "e1202.1"
            ]
        }
    ],
    "theory_statements": [
        "Automated systems systematically overestimate discovery success when evaluated on proxy metrics compared to ground-truth experimental validation, with the overestimation increasing as the proxy becomes more removed from ground truth.",
        "The proxy-to-ground-truth gap increases with the novelty and extrapolation distance of the discovery from training data, because proxies are typically calibrated on known regimes and become less reliable in novel regimes.",
        "Systems optimizing proxy metrics will exhibit higher false positive rates for transformational discoveries than for incremental discoveries, because transformational discoveries involve greater extrapolation where proxy calibration is weakest.",
        "Multifidelity approaches that explicitly model and correct proxy biases (e.g., Gemini) can reduce but not eliminate the proxy-to-ground-truth gap, because some systematic errors remain even after correction.",
        "The computational cost advantage of proxy evaluation creates an economic incentive to defer ground-truth validation, leading to accumulation of unvalidated discoveries and potential publication of false positives.",
        "The quality of a proxy metric depends on: (1) the physical/theoretical grounding of the proxy, (2) the domain's amenability to computational modeling, and (3) the calibration data available in the regime of interest.",
        "Domains with mature physics-based simulations (e.g., molecular dynamics, quantum chemistry) show smaller proxy-to-ground-truth gaps than domains with purely empirical proxies (e.g., drug activity prediction).",
        "The gap manifests differently across validation cascades: proxy → intermediate validation → ground truth, with error accumulation at each stage.",
        "Human expertise in proxy design can reduce but not eliminate the gap, as even expert-designed proxies fail in extrapolative regimes.",
        "The temporal evolution of proxy quality follows domain maturity: as domains mature and more validation data accumulates, proxies improve but never perfectly match ground truth in novel regimes."
    ],
    "new_predictions_likely": [
        "Systems that incorporate explicit proxy-bias correction (like Gemini) will show 20-50% smaller gaps between computational and experimental success rates than systems without such correction, with the reduction being larger for incremental discoveries than transformational ones.",
        "The false positive rate for automated discoveries will be inversely correlated with the amount of experimental validation performed, following approximately a power law relationship.",
        "Domains with well-established computational proxies (e.g., protein structure prediction, molecular dynamics) will show proxy-to-ground-truth gaps of 5-20%, while domains with poorly validated proxies (e.g., drug activity, materials properties) will show gaps of 40-80%.",
        "In materials discovery, systems using physics-based proxies (DFT, molecular dynamics) will show 2-3× lower false positive rates than systems using purely data-driven proxies (neural network predictions).",
        "The proxy-to-ground-truth gap will be larger for properties requiring long-timescale dynamics (e.g., stability, degradation) than for static properties (e.g., structure, composition).",
        "Systems that report uncertainty estimates calibrated on validation data will show better correlation between predicted uncertainty and actual proxy-to-ground-truth gap than systems without uncertainty quantification."
    ],
    "new_predictions_unknown": [
        "Whether machine learning meta-models could be trained to predict the proxy-to-ground-truth gap for individual discoveries with sufficient accuracy (e.g., R²&gt;0.7) to enable reliable prioritization of experimental validation, or whether the gap is fundamentally unpredictable due to the novelty of transformational discoveries.",
        "Whether the proxy-to-ground-truth gap could be eliminated entirely through sufficiently sophisticated simulation and modeling, or whether there are fundamental limits (e.g., computational complexity, chaotic dynamics, emergent phenomena) that prevent perfect proxy-ground-truth alignment.",
        "Whether certain types of transformational discoveries might actually show smaller proxy-to-ground-truth gaps than incremental discoveries in specific domains, potentially due to transformational discoveries being more amenable to first-principles modeling.",
        "Whether active learning strategies that explicitly optimize for reducing the proxy-to-ground-truth gap (rather than optimizing the proxy itself) could achieve order-of-magnitude improvements in experimental validation success rates.",
        "Whether the gap could be reduced by orders of magnitude through hybrid approaches that combine multiple orthogonal proxies (e.g., physics-based + data-driven + expert heuristics), or whether proxies are fundamentally correlated in their failure modes.",
        "Whether the proxy-to-ground-truth gap follows universal scaling laws across domains, or whether each domain has fundamentally different gap characteristics that prevent cross-domain learning."
    ],
    "negative_experiments": [
        "Finding domains where proxy metrics perfectly predict experimental outcomes (R²&gt;0.95, false positive rate &lt;5%) across both incremental and transformational discoveries would challenge the universality of the gap.",
        "Demonstrating that transformational discoveries consistently show smaller proxy-to-ground-truth gaps than incremental ones across multiple domains would contradict the core theory prediction.",
        "Showing that systems without proxy-bias correction perform as well as those with correction in experimental validation (within 10% success rate) would question the value of gap-reduction methods.",
        "Finding that the proxy-to-ground-truth gap does not increase with extrapolation distance from training data would challenge the mechanism proposed for why transformational discoveries show larger gaps.",
        "Demonstrating that purely data-driven proxies perform as well as physics-based proxies in novel regimes would contradict the theory's prediction about proxy quality dependence on physical grounding.",
        "Showing that the economic incentive to defer validation does not lead to accumulation of unvalidated discoveries (e.g., if false positives are naturally filtered by other mechanisms) would challenge the theory's prediction about systemic bias."
    ],
    "unaccounted_for": [
        {
            "text": "The role of domain-specific knowledge and human expertise in determining proxy quality and how this expertise can be systematically encoded",
            "uuids": []
        },
        {
            "text": "How the gap evolves as computational methods improve over time and whether there are diminishing returns or fundamental limits",
            "uuids": []
        },
        {
            "text": "The interaction between multiple proxy metrics used simultaneously and whether they exhibit correlated or independent failure modes",
            "uuids": []
        },
        {
            "text": "The psychological and sociological factors that influence acceptance of proxy-validated discoveries in scientific communities",
            "uuids": []
        },
        {
            "text": "The role of validation cascades (proxy → intermediate → ground truth) and how errors accumulate or cancel across stages",
            "uuids": []
        },
        {
            "text": "Whether certain discovery types (e.g., negative results, null findings) show different proxy-to-ground-truth gap characteristics",
            "uuids": []
        },
        {
            "text": "The impact of publication bias and selective reporting on the observed proxy-to-ground-truth gap in the literature",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "AlphaFold shows very small proxy-to-ground-truth gap, with computational predictions matching experimental structures closely (median backbone Cα r.m.s.d. = 0.96 Å on CASP14, competitive with experimental accuracy in majority of cases), despite being a purely computational method. This suggests that in domains with mature physics-based modeling and abundant training data, the gap can be minimized even for novel predictions.",
            "uuids": [
                "e1164.0",
                "e1233.0"
            ]
        },
        {
            "text": "Mathematical systems like MITM-RF and Ramanujan Machine show high numerical precision (validated to 100-2000 digits) that translates to proven theorems, with many conjectures subsequently proven by mathematicians. This suggests that in pure mathematics, numerical proxies can have near-zero gap to ground truth (formal proof).",
            "uuids": [
                "e1449.1",
                "e1453.1",
                "e1243.4"
            ]
        },
        {
            "text": "SPOCK achieves high accuracy (TPR ~85-94% at FPR=10%) in predicting long-term stability from short integrations, despite fundamental chaos-induced unpredictability, suggesting that even with irreducible uncertainty, proxies can be highly informative.",
            "uuids": [
                "e1437.0"
            ]
        },
        {
            "text": "Magnon NN predictions (J=0.6763 meV, J'=0.0104 meV) show excellent agreement with literature experimental values, suggesting that well-designed neural network proxies can achieve small gaps even for subtle physical parameters.",
            "uuids": [
                "e1200.1"
            ]
        }
    ],
    "special_cases": [
        "In domains with physics-based simulations grounded in first principles (e.g., quantum chemistry, molecular dynamics, protein structure prediction), the proxy-to-ground-truth gap may be small (5-20%) due to the theoretical foundation, especially when abundant training data is available.",
        "For rediscovery tasks where ground truth is known and the system is evaluated on reproducing known results, the gap can be directly measured and minimized through calibration, but this does not guarantee small gaps for genuinely novel discoveries.",
        "In pure computational domains (e.g., algorithm design, theorem proving, mathematical conjecturing), proxy and ground truth may be identical or nearly so, as the 'experiment' is itself computational and deterministic.",
        "For properties that can be measured non-destructively and rapidly (e.g., spectroscopy, imaging), the distinction between proxy and ground truth may blur, as the 'proxy' measurement becomes the practical ground truth.",
        "In domains where the cost of ground-truth validation is extremely high (e.g., clinical trials, long-term stability testing), the proxy-to-ground-truth gap may be accepted as unavoidable, leading to different validation standards.",
        "For discoveries that involve emergent phenomena or complex system behaviors, the gap may be fundamentally irreducible due to computational intractability of full simulation.",
        "In cases where multiple orthogonal proxies all agree, the effective gap may be smaller than for any single proxy, though correlated failure modes can still lead to systematic errors.",
        "For incremental discoveries that stay close to the training distribution, proxies may be well-calibrated and show small gaps (10-30%), while transformational discoveries that extrapolate far from training data show large gaps (50-90%)."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Goodhart (1975) Problems of Monetary Management: The U.K. Experience [Goodhart's Law: when a measure becomes a target, it ceases to be a good measure - directly applicable to proxy optimization]",
            "Campbell (1979) Assessing the impact of planned social change [Campbell's Law on indicator corruption when used for decision-making]",
            "Marcus (2018) Deep Learning: A Critical Appraisal [Discusses brittleness and generalization failures in deep learning, related to proxy-ground-truth gaps]",
            "Sculley et al. (2015) Hidden Technical Debt in Machine Learning Systems [Discusses the gap between ML metrics and real-world performance]",
            "Lipton & Steinhardt (2018) Troubling Trends in Machine Learning Scholarship [Discusses the gap between benchmark performance and real-world utility]",
            "D'Amour et al. (2020) Underspecification Presents Challenges for Credibility in Modern Machine Learning [Discusses how models can perform well on proxies but fail in deployment]",
            "Recht et al. (2019) Do ImageNet Classifiers Generalize to ImageNet? [Demonstrates distribution shift and proxy-ground-truth gaps in computer vision]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
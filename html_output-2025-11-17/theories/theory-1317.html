<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Fidelity Law for Graph Linearization in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1317</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1317</p>
                <p><strong>Name:</strong> Semantic Fidelity Law for Graph Linearization in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that the ideal graph-to-text linearization must preserve all semantic information present in the original graph, such that the LLM can reconstruct the full graph structure and attributes from the text alone. Semantic fidelity is hypothesized to be necessary for downstream graph reasoning, generation, and interpretability.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Fidelity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; is_used_for &#8594; LLM_training<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_linearization &#8594; preserves_semantics &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_reconstruct_graph &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; performs_graph_reasoning &#8594; with_high_accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Lossy graph-to-text conversions reduce LLM performance on graph reconstruction and reasoning tasks. </li>
    <li>Semantic-preserving encodings are critical in AMR-to-text and knowledge graph-to-sequence tasks. </li>
    <li>Empirical studies show that LLMs can reconstruct graphs from text if and only if the linearization is semantically complete. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law generalizes semantic preservation principles to the broader context of LLM-based graph learning.</p>            <p><strong>What Already Exists:</strong> Semantic fidelity is a known requirement in AMR and knowledge graph-to-sequence tasks.</p>            <p><strong>What is Novel:</strong> Its formalization as a law for ideal graph linearization in LLMs is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [Semantic preservation in AMR linearization]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic fidelity in graph-to-sequence]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs trained on semantically complete graph linearizations will outperform those trained on lossy or ambiguous encodings in graph reconstruction and reasoning tasks.</li>
                <li>Semantic fidelity in linearization will improve LLM interpretability and error analysis.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Semantically faithful linearizations may enable LLMs to perform novel graph generation or completion tasks.</li>
                <li>Semantic fidelity could allow LLMs to transfer reasoning skills across different graph domains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs trained on lossy or ambiguous linearizations perform as well as those trained on semantically complete ones, the law would be challenged.</li>
                <li>If semantic fidelity does not improve graph reasoning or reconstruction, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The effect of redundant or over-specified encodings on LLM learning is not fully explained. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory adapts a known principle to a broader, more general context (LLMs and arbitrary graphs).</p>
            <p><strong>References:</strong> <ul>
    <li>Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [Semantic preservation in AMR linearization]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic fidelity in graph-to-sequence]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Fidelity Law for Graph Linearization in LLMs",
    "theory_description": "This theory asserts that the ideal graph-to-text linearization must preserve all semantic information present in the original graph, such that the LLM can reconstruct the full graph structure and attributes from the text alone. Semantic fidelity is hypothesized to be necessary for downstream graph reasoning, generation, and interpretability.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Fidelity Law",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "is_used_for",
                        "object": "LLM_training"
                    },
                    {
                        "subject": "graph_linearization",
                        "relation": "preserves_semantics",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_reconstruct_graph",
                        "object": "True"
                    },
                    {
                        "subject": "LLM",
                        "relation": "performs_graph_reasoning",
                        "object": "with_high_accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Lossy graph-to-text conversions reduce LLM performance on graph reconstruction and reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Semantic-preserving encodings are critical in AMR-to-text and knowledge graph-to-sequence tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that LLMs can reconstruct graphs from text if and only if the linearization is semantically complete.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Semantic fidelity is a known requirement in AMR and knowledge graph-to-sequence tasks.",
                    "what_is_novel": "Its formalization as a law for ideal graph linearization in LLMs is new.",
                    "classification_explanation": "The law generalizes semantic preservation principles to the broader context of LLM-based graph learning.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [Semantic preservation in AMR linearization]",
                        "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic fidelity in graph-to-sequence]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs trained on semantically complete graph linearizations will outperform those trained on lossy or ambiguous encodings in graph reconstruction and reasoning tasks.",
        "Semantic fidelity in linearization will improve LLM interpretability and error analysis."
    ],
    "new_predictions_unknown": [
        "Semantically faithful linearizations may enable LLMs to perform novel graph generation or completion tasks.",
        "Semantic fidelity could allow LLMs to transfer reasoning skills across different graph domains."
    ],
    "negative_experiments": [
        "If LLMs trained on lossy or ambiguous linearizations perform as well as those trained on semantically complete ones, the law would be challenged.",
        "If semantic fidelity does not improve graph reasoning or reconstruction, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The effect of redundant or over-specified encodings on LLM learning is not fully explained.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs may infer missing graph information from context or pretraining, even with incomplete linearizations.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with implicit or context-dependent semantics may require additional disambiguation.",
        "Very large graphs may require compressed or hierarchical encodings to maintain semantic fidelity."
    ],
    "existing_theory": {
        "what_already_exists": "Semantic fidelity is established in AMR and knowledge graph-to-sequence literature.",
        "what_is_novel": "Its formalization as a general law for LLM-based graph linearization is new.",
        "classification_explanation": "The theory adapts a known principle to a broader, more general context (LLMs and arbitrary graphs).",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [Semantic preservation in AMR linearization]",
            "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic fidelity in graph-to-sequence]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-615",
    "original_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation Metric Misalignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-95</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-95</p>
                <p><strong>Name:</strong> Evaluation Metric Misalignment Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about faithfulness gaps between natural language descriptions and code implementations in automated experimentation, based on the following results.</p>
                <p><strong>Description:</strong> Evaluation metrics used in code often measure different properties than what natural language descriptions claim to measure. This misalignment occurs because: (1) metrics are chosen for computational convenience rather than conceptual alignment, (2) metric implementations have hidden assumptions and defaults, (3) metrics conflate multiple properties (e.g., content selection and faithfulness), and (4) metric behavior changes across data distributions in ways not captured by descriptions. The theory predicts that metric misalignment leads to incorrect conclusions about model performance and misleading comparisons.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Evaluation metrics correlate with their claimed properties at only 0.3-0.6 Spearman correlation on average, indicating substantial misalignment.</li>
                <li>Metric misalignment is highest when: (1) metrics are borrowed from related but different tasks (correlation ~0.2-0.4), (2) metrics have hidden hyperparameters (variance 30-50%), and (3) metrics conflate multiple properties (correlation with any single property <0.5).</li>
                <li>Using misaligned metrics leads to incorrect model rankings in 20-40% of pairwise comparisons.</li>
                <li>Metric implementations differ from descriptions in 40-60% of cases due to default hyperparameters, preprocessing choices, and edge case handling.</li>
                <li>The misalignment compounds: using multiple misaligned metrics does not improve evaluation quality and can worsen it through conflicting signals.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Content-selection metrics (ROUGE, BLEU) do not reliably measure faithfulness in abstractive summarization. <a href="../results/extraction-result-708.html#e708.1" class="evidence-link">[e708.1]</a> </li>
    <li>NLI-based metrics perform poorly for faithfulness detection compared to dedicated classifiers. <a href="../results/extraction-result-426.html#e426.1" class="evidence-link">[e426.1]</a> </li>
    <li>QA-based evaluation compounds errors from question generation and answering components. <a href="../results/extraction-result-686.html#e686.3" class="evidence-link">[e686.3]</a> </li>
    <li>Pass@k estimation using naive methods is biased; unbiased estimators are needed. <a href="../results/extraction-result-752.html#e752.5" class="evidence-link">[e752.5]</a> </li>
    <li>Deletion-based evaluation without retraining conflates distribution shift with feature importance. <a href="../results/extraction-result-725.html#e725.0" class="evidence-link">[e725.0]</a> </li>
    <li>Test-case coverage gaps in benchmarks create misleading pass@k scores. <a href="../results/extraction-result-440.html#e440.3" class="evidence-link">[e440.3]</a> </li>
    <li>Hyperparameter variation across papers leads to contrasting metric performance conclusions. <a href="../results/extraction-result-689.html#e689.0" class="evidence-link">[e689.0]</a> </li>
    <li>Reporting best-run results versus distributions misrepresents method robustness. <a href="../results/extraction-result-728.html#e728.1" class="evidence-link">[e728.1]</a> </li>
    <li>Grammar and vocabulary coverage limitations in oracle generation reduce bug-finding recall. <a href="../results/extraction-result-759.html#e759.3" class="evidence-link">[e759.3]</a> </li>
    <li>Weak type-check postconditions have low bug-discriminative power despite high prevalence. <a href="../results/extraction-result-498.html#e498.4" class="evidence-link">[e498.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Developing task-specific metrics aligned with evaluation goals will improve correlation with human judgments by 40-60% compared to borrowed metrics.</li>
                <li>Explicitly separating conflated properties (e.g., content selection vs faithfulness) into separate metrics will improve model selection accuracy by 30-50%.</li>
                <li>Documenting and standardizing metric hyperparameters will reduce cross-study variance by 40-60%.</li>
                <li>Automated metric validation tools that check alignment with claimed properties will identify 60-80% of misaligned metrics.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exist universal evaluation principles that can guide metric design across all tasks.</li>
                <li>Whether metric misalignment follows predictable patterns based on task characteristics.</li>
                <li>Whether human evaluation is itself subject to similar misalignment issues as automated metrics.</li>
                <li>Whether the cost of developing aligned metrics is justified by the improvement in evaluation quality.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that borrowed metrics perform as well as task-specific metrics would challenge the theory.</li>
                <li>Demonstrating that metric hyperparameters do not affect rankings would undermine concerns about implementation details.</li>
                <li>Showing that conflated metrics are as useful as separated metrics would contradict the theory's predictions.</li>
                <li>Finding that metric misalignment does not affect model selection would limit the theory's practical importance.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some evaluation challenges arise from fundamental task ambiguity rather than metric misalignment. <a href="../results/extraction-result-498.html#e498.0" class="evidence-link">[e498.0]</a> </li>
    <li>Human evaluation also has reliability issues independent of automated metrics. <a href="../results/extraction-result-674.html#e674.3" class="evidence-link">[e674.3]</a> </li>
    <li>Some metrics are intentionally approximate for computational efficiency. <a href="../results/extraction-result-722.html#e722.2" class="evidence-link">[e722.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Reiter (2018) A Structured Review of the Validity of BLEU [Critiques BLEU misalignment for NLG]</li>
    <li>Novikova et al. (2017) Why We Need New Evaluation Metrics for NLG [Argues for task-specific metrics]</li>
    <li>Maynez et al. (2020) On Faithfulness and Factuality in Abstractive Summarization [Documents faithfulness metric misalignment]</li>
    <li>Ribeiro et al. (2020) Beyond Accuracy: Behavioral Testing of NLP Models [Proposes behavioral testing to complement metrics]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Evaluation Metric Misalignment Theory",
    "theory_description": "Evaluation metrics used in code often measure different properties than what natural language descriptions claim to measure. This misalignment occurs because: (1) metrics are chosen for computational convenience rather than conceptual alignment, (2) metric implementations have hidden assumptions and defaults, (3) metrics conflate multiple properties (e.g., content selection and faithfulness), and (4) metric behavior changes across data distributions in ways not captured by descriptions. The theory predicts that metric misalignment leads to incorrect conclusions about model performance and misleading comparisons.",
    "supporting_evidence": [
        {
            "text": "Content-selection metrics (ROUGE, BLEU) do not reliably measure faithfulness in abstractive summarization.",
            "uuids": [
                "e708.1"
            ]
        },
        {
            "text": "NLI-based metrics perform poorly for faithfulness detection compared to dedicated classifiers.",
            "uuids": [
                "e426.1"
            ]
        },
        {
            "text": "QA-based evaluation compounds errors from question generation and answering components.",
            "uuids": [
                "e686.3"
            ]
        },
        {
            "text": "Pass@k estimation using naive methods is biased; unbiased estimators are needed.",
            "uuids": [
                "e752.5"
            ]
        },
        {
            "text": "Deletion-based evaluation without retraining conflates distribution shift with feature importance.",
            "uuids": [
                "e725.0"
            ]
        },
        {
            "text": "Test-case coverage gaps in benchmarks create misleading pass@k scores.",
            "uuids": [
                "e440.3"
            ]
        },
        {
            "text": "Hyperparameter variation across papers leads to contrasting metric performance conclusions.",
            "uuids": [
                "e689.0"
            ]
        },
        {
            "text": "Reporting best-run results versus distributions misrepresents method robustness.",
            "uuids": [
                "e728.1"
            ]
        },
        {
            "text": "Grammar and vocabulary coverage limitations in oracle generation reduce bug-finding recall.",
            "uuids": [
                "e759.3"
            ]
        },
        {
            "text": "Weak type-check postconditions have low bug-discriminative power despite high prevalence.",
            "uuids": [
                "e498.4"
            ]
        }
    ],
    "theory_statements": [
        "Evaluation metrics correlate with their claimed properties at only 0.3-0.6 Spearman correlation on average, indicating substantial misalignment.",
        "Metric misalignment is highest when: (1) metrics are borrowed from related but different tasks (correlation ~0.2-0.4), (2) metrics have hidden hyperparameters (variance 30-50%), and (3) metrics conflate multiple properties (correlation with any single property &lt;0.5).",
        "Using misaligned metrics leads to incorrect model rankings in 20-40% of pairwise comparisons.",
        "Metric implementations differ from descriptions in 40-60% of cases due to default hyperparameters, preprocessing choices, and edge case handling.",
        "The misalignment compounds: using multiple misaligned metrics does not improve evaluation quality and can worsen it through conflicting signals."
    ],
    "new_predictions_likely": [
        "Developing task-specific metrics aligned with evaluation goals will improve correlation with human judgments by 40-60% compared to borrowed metrics.",
        "Explicitly separating conflated properties (e.g., content selection vs faithfulness) into separate metrics will improve model selection accuracy by 30-50%.",
        "Documenting and standardizing metric hyperparameters will reduce cross-study variance by 40-60%.",
        "Automated metric validation tools that check alignment with claimed properties will identify 60-80% of misaligned metrics."
    ],
    "new_predictions_unknown": [
        "Whether there exist universal evaluation principles that can guide metric design across all tasks.",
        "Whether metric misalignment follows predictable patterns based on task characteristics.",
        "Whether human evaluation is itself subject to similar misalignment issues as automated metrics.",
        "Whether the cost of developing aligned metrics is justified by the improvement in evaluation quality."
    ],
    "negative_experiments": [
        "Finding that borrowed metrics perform as well as task-specific metrics would challenge the theory.",
        "Demonstrating that metric hyperparameters do not affect rankings would undermine concerns about implementation details.",
        "Showing that conflated metrics are as useful as separated metrics would contradict the theory's predictions.",
        "Finding that metric misalignment does not affect model selection would limit the theory's practical importance."
    ],
    "unaccounted_for": [
        {
            "text": "Some evaluation challenges arise from fundamental task ambiguity rather than metric misalignment.",
            "uuids": [
                "e498.0"
            ]
        },
        {
            "text": "Human evaluation also has reliability issues independent of automated metrics.",
            "uuids": [
                "e674.3"
            ]
        },
        {
            "text": "Some metrics are intentionally approximate for computational efficiency.",
            "uuids": [
                "e722.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some borrowed metrics (e.g., BLEU for translation) work well despite not being designed for the specific task variant.",
            "uuids": [
                "e483.2"
            ]
        },
        {
            "text": "Ensemble metrics that combine multiple misaligned metrics sometimes improve evaluation quality.",
            "uuids": [
                "e689.0"
            ]
        }
    ],
    "special_cases": [
        "Metrics for generative tasks are particularly prone to misalignment due to output diversity.",
        "Metrics for code are more alignable when execution-based (pass@k) than when similarity-based (BLEU).",
        "Metrics for fairness and safety require explicit alignment with ethical principles, not just statistical properties.",
        "Metrics for few-shot learning must account for variance across random seeds and sample selection."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Reiter (2018) A Structured Review of the Validity of BLEU [Critiques BLEU misalignment for NLG]",
            "Novikova et al. (2017) Why We Need New Evaluation Metrics for NLG [Argues for task-specific metrics]",
            "Maynez et al. (2020) On Faithfulness and Factuality in Abstractive Summarization [Documents faithfulness metric misalignment]",
            "Ribeiro et al. (2020) Beyond Accuracy: Behavioral Testing of NLP Models [Proposes behavioral testing to complement metrics]"
        ]
    },
    "theory_type_general_specific": "specific",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
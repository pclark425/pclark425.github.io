<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Axial Evaluation Theory for LLM-Generated Scientific Theories - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-2197</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-2197</p>
                <p><strong>Name:</strong> Multi-Axial Evaluation Theory for LLM-Generated Scientific Theories</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories.</p>
                <p><strong>Description:</strong> This theory posits that the evaluation of LLM-generated scientific theories should be conducted along multiple, orthogonal axes: internal logical coherence, empirical adequacy, novelty, and epistemic utility. Each axis is assessed independently, and a theory's overall value is a function of its performance across all axes. This approach is designed to capture the unique strengths and weaknesses of LLM-generated outputs, which may excel in some dimensions (e.g., creativity) while lacking in others (e.g., empirical grounding).</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Orthogonal Axes of Evaluation (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-generated theory &#8594; is_to_be_evaluated &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation &#8594; must_include &#8594; logical_coherence<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation &#8594; must_include &#8594; empirical_adequacy<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation &#8594; must_include &#8594; novelty<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation &#8594; must_include &#8594; epistemic_utility</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Scientific theories are traditionally evaluated for logical consistency, empirical fit, and novelty; LLMs can generate plausible but ungrounded or redundant theories, necessitating explicit multi-criteria evaluation. </li>
    <li>Philosophy of science (Kuhn, Lakatos) emphasizes multiple criteria for theory choice, but LLMs introduce new risks (e.g., plausible but unoriginal or untestable outputs) that require explicit, orthogonal evaluation. </li>
    <li>LLMs can generate outputs that are highly creative but lack empirical support, or vice versa, so evaluation must not be hierarchical but orthogonal. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing theory evaluation frameworks, this law is novel in its explicit, orthogonal, and LLM-specific formulation.</p>            <p><strong>What Already Exists:</strong> Multi-criteria evaluation is common in philosophy of science (e.g., Kuhn, Lakatos), but not formalized for LLM outputs.</p>            <p><strong>What is Novel:</strong> Explicitly formalizes the axes for LLM-generated theory evaluation and treats them as orthogonal, not hierarchical.</p>
            <p><strong>References:</strong> <ul>
    <li>Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]</li>
    <li>Lakatos (1970) Falsification and the Methodology of Scientific Research Programmes [criteria for theory evaluation]</li>
    <li>Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM-specific risks, not evaluation axes]</li>
</ul>
            <h3>Statement 1: Composite Evaluation Function (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; theory &#8594; has_scores &#8594; S_logical, S_empirical, S_novelty, S_utility</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; theory &#8594; has_overall_score &#8594; F(S_logical, S_empirical, S_novelty, S_utility)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Multi-criteria decision analysis is used in other domains to combine orthogonal scores; LLMs may generate theories that are strong in one axis but weak in others, requiring a composite function. </li>
    <li>No single axis (e.g., novelty) is sufficient for scientific value; a composite function is needed to reflect the multi-dimensional nature of theory quality. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law adapts existing multi-criteria scoring to the novel context of LLM-generated scientific theory evaluation.</p>            <p><strong>What Already Exists:</strong> Composite scoring is used in multi-criteria decision analysis, but not formalized for LLM theory evaluation.</p>            <p><strong>What is Novel:</strong> Proposes a formal, multi-axis scoring function for LLM-generated scientific theories.</p>
            <p><strong>References:</strong> <ul>
    <li>Keeney & Raiffa (1976) Decisions with Multiple Objectives [multi-criteria decision analysis]</li>
    <li>Bromley (2020) Machine Learning for Science: State of the Art and Future Prospects [no formal scoring for LLM-generated theories]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM-generated theory is highly novel but empirically unsupported, its overall evaluation will be moderate or low, not high.</li>
                <li>If a theory is logically inconsistent, its overall score will be low regardless of novelty or utility.</li>
                <li>Theories that are both empirically adequate and novel will be rated highest by this framework.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Some LLM-generated theories may score highly on epistemic utility (e.g., heuristic value) despite low empirical adequacy, leading to debate about their scientific value.</li>
                <li>The composite function may reveal new classes of theories (e.g., highly creative but currently untestable) that are undervalued by traditional evaluation.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a theory that is logically inconsistent receives a high overall score, the evaluation framework is invalid.</li>
                <li>If empirical adequacy is ignored and the framework still produces reliable scientific judgments, the theory is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of social or institutional biases in the evaluation process is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory is a novel adaptation of existing evaluation frameworks to the unique context of LLM-generated scientific theories.</p>
            <p><strong>References:</strong> <ul>
    <li>Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]</li>
    <li>Keeney & Raiffa (1976) Decisions with Multiple Objectives [multi-criteria decision analysis]</li>
    <li>Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM-specific risks]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Axial Evaluation Theory for LLM-Generated Scientific Theories",
    "theory_description": "This theory posits that the evaluation of LLM-generated scientific theories should be conducted along multiple, orthogonal axes: internal logical coherence, empirical adequacy, novelty, and epistemic utility. Each axis is assessed independently, and a theory's overall value is a function of its performance across all axes. This approach is designed to capture the unique strengths and weaknesses of LLM-generated outputs, which may excel in some dimensions (e.g., creativity) while lacking in others (e.g., empirical grounding).",
    "theory_statements": [
        {
            "law": {
                "law_name": "Orthogonal Axes of Evaluation",
                "if": [
                    {
                        "subject": "LLM-generated theory",
                        "relation": "is_to_be_evaluated",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation",
                        "relation": "must_include",
                        "object": "logical_coherence"
                    },
                    {
                        "subject": "evaluation",
                        "relation": "must_include",
                        "object": "empirical_adequacy"
                    },
                    {
                        "subject": "evaluation",
                        "relation": "must_include",
                        "object": "novelty"
                    },
                    {
                        "subject": "evaluation",
                        "relation": "must_include",
                        "object": "epistemic_utility"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Scientific theories are traditionally evaluated for logical consistency, empirical fit, and novelty; LLMs can generate plausible but ungrounded or redundant theories, necessitating explicit multi-criteria evaluation.",
                        "uuids": []
                    },
                    {
                        "text": "Philosophy of science (Kuhn, Lakatos) emphasizes multiple criteria for theory choice, but LLMs introduce new risks (e.g., plausible but unoriginal or untestable outputs) that require explicit, orthogonal evaluation.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can generate outputs that are highly creative but lack empirical support, or vice versa, so evaluation must not be hierarchical but orthogonal.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Multi-criteria evaluation is common in philosophy of science (e.g., Kuhn, Lakatos), but not formalized for LLM outputs.",
                    "what_is_novel": "Explicitly formalizes the axes for LLM-generated theory evaluation and treats them as orthogonal, not hierarchical.",
                    "classification_explanation": "While related to existing theory evaluation frameworks, this law is novel in its explicit, orthogonal, and LLM-specific formulation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]",
                        "Lakatos (1970) Falsification and the Methodology of Scientific Research Programmes [criteria for theory evaluation]",
                        "Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM-specific risks, not evaluation axes]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Composite Evaluation Function",
                "if": [
                    {
                        "subject": "theory",
                        "relation": "has_scores",
                        "object": "S_logical, S_empirical, S_novelty, S_utility"
                    }
                ],
                "then": [
                    {
                        "subject": "theory",
                        "relation": "has_overall_score",
                        "object": "F(S_logical, S_empirical, S_novelty, S_utility)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Multi-criteria decision analysis is used in other domains to combine orthogonal scores; LLMs may generate theories that are strong in one axis but weak in others, requiring a composite function.",
                        "uuids": []
                    },
                    {
                        "text": "No single axis (e.g., novelty) is sufficient for scientific value; a composite function is needed to reflect the multi-dimensional nature of theory quality.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Composite scoring is used in multi-criteria decision analysis, but not formalized for LLM theory evaluation.",
                    "what_is_novel": "Proposes a formal, multi-axis scoring function for LLM-generated scientific theories.",
                    "classification_explanation": "This law adapts existing multi-criteria scoring to the novel context of LLM-generated scientific theory evaluation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Keeney & Raiffa (1976) Decisions with Multiple Objectives [multi-criteria decision analysis]",
                        "Bromley (2020) Machine Learning for Science: State of the Art and Future Prospects [no formal scoring for LLM-generated theories]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM-generated theory is highly novel but empirically unsupported, its overall evaluation will be moderate or low, not high.",
        "If a theory is logically inconsistent, its overall score will be low regardless of novelty or utility.",
        "Theories that are both empirically adequate and novel will be rated highest by this framework."
    ],
    "new_predictions_unknown": [
        "Some LLM-generated theories may score highly on epistemic utility (e.g., heuristic value) despite low empirical adequacy, leading to debate about their scientific value.",
        "The composite function may reveal new classes of theories (e.g., highly creative but currently untestable) that are undervalued by traditional evaluation."
    ],
    "negative_experiments": [
        "If a theory that is logically inconsistent receives a high overall score, the evaluation framework is invalid.",
        "If empirical adequacy is ignored and the framework still produces reliable scientific judgments, the theory is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of social or institutional biases in the evaluation process is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some scientific breakthroughs were initially logically inconsistent or empirically unsupported but later validated, challenging strict multi-axis evaluation.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Theories that are untestable due to current technological limitations may be undervalued.",
        "The framework may not account for theories that are valuable for their heuristic or pedagogical roles."
    ],
    "existing_theory": {
        "what_already_exists": "Multi-criteria evaluation is discussed in philosophy of science, but not formalized for LLM-generated scientific theories.",
        "what_is_novel": "Explicit, orthogonal, and LLM-specific axes and composite scoring for theory evaluation.",
        "classification_explanation": "This theory is a novel adaptation of existing evaluation frameworks to the unique context of LLM-generated scientific theories.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]",
            "Keeney & Raiffa (1976) Decisions with Multiple Objectives [multi-criteria decision analysis]",
            "Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM-specific risks]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how to evaluate LLM-generated scientific theories.",
    "original_theory_id": "theory-673",
    "original_theory_name": "Evaluator-Process Coupling Theory for LLM-Generated Scientific Theories",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Preservation Law for Graph Linearization in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1313</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1313</p>
                <p><strong>Name:</strong> Semantic Preservation Law for Graph Linearization in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that the ideal graph-to-text representation for LLM training must preserve all semantic and structural information of the original graph, such that the LLM can reconstruct the graph (up to isomorphism) from the text. The theory further claims that lossless, semantically faithful linearizations are necessary for downstream tasks requiring graph understanding, and that any ambiguity or information loss in the linearization impairs LLM performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Faithfulness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; is_used_for &#8594; LLM_training<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_linearization &#8594; is_semantically_faithful &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_reconstruct_graph &#8594; up_to_isomorphism<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; performs_well_on_graph_tasks &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Lossless graph encodings (e.g., adjacency lists, edge lists) enable perfect reconstruction and high downstream task performance. </li>
    <li>Ambiguous or lossy linearizations (e.g., natural language summaries) reduce LLM accuracy on graph-based tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While semantic preservation is implicit in some prior work, its explicit connection to LLM reconstructability and performance is novel.</p>            <p><strong>What Already Exists:</strong> Semantic preservation is a known requirement in graph serialization and some graph-to-sequence models.</p>            <p><strong>What is Novel:</strong> This law formalizes the necessity of semantic faithfulness for LLM-based graph learning and links it to reconstructability and downstream performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic preservation in graph-to-sequence]</li>
    <li>Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Faithful graph-to-text mappings]</li>
</ul>
            <h3>Statement 1: Ambiguity Impairment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; is_ambiguous_or_lossy &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; cannot_reconstruct_graph &#8594; reliably<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; performs_poorly_on_graph_tasks &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Natural language summaries or incomplete encodings lead to information loss and reduced LLM accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The explicit focus on LLMs and graph-to-text linearization is novel.</p>            <p><strong>What Already Exists:</strong> Information loss and ambiguity are known to impair model performance in NLP and graph learning.</p>            <p><strong>What is Novel:</strong> This law directly connects ambiguity/lossiness in graph linearization to LLM reconstructability and task performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Faithful graph-to-text mappings]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic preservation in graph-to-sequence]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs trained on semantically faithful, lossless graph linearizations will outperform those trained on ambiguous or lossy representations in graph reconstruction and reasoning tasks.</li>
                <li>Introducing ambiguity or information loss in the linearization will degrade LLM performance on tasks requiring precise graph understanding.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Semantically faithful linearizations may enable LLMs to learn latent graph properties (e.g., symmetry, automorphism groups) not explicitly present in the text.</li>
                <li>Lossless linearizations could allow LLMs to generalize to novel graph types (e.g., hypergraphs, multigraphs) with minimal adaptation.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs trained on ambiguous or lossy linearizations perform as well as those trained on lossless ones, the theory would be challenged.</li>
                <li>If LLMs can reconstruct graphs from ambiguous text with high accuracy, the law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The effect of redundancy (e.g., multiple encodings of the same graph) on LLM learning is not fully explained. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory builds on existing principles but applies them in a new context (LLMs and graph-to-text linearization).</p>
            <p><strong>References:</strong> <ul>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic preservation in graph-to-sequence]</li>
    <li>Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Faithful graph-to-text mappings]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Preservation Law for Graph Linearization in LLMs",
    "theory_description": "This theory asserts that the ideal graph-to-text representation for LLM training must preserve all semantic and structural information of the original graph, such that the LLM can reconstruct the graph (up to isomorphism) from the text. The theory further claims that lossless, semantically faithful linearizations are necessary for downstream tasks requiring graph understanding, and that any ambiguity or information loss in the linearization impairs LLM performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Faithfulness Law",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "is_used_for",
                        "object": "LLM_training"
                    },
                    {
                        "subject": "graph_linearization",
                        "relation": "is_semantically_faithful",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_reconstruct_graph",
                        "object": "up_to_isomorphism"
                    },
                    {
                        "subject": "LLM",
                        "relation": "performs_well_on_graph_tasks",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Lossless graph encodings (e.g., adjacency lists, edge lists) enable perfect reconstruction and high downstream task performance.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or lossy linearizations (e.g., natural language summaries) reduce LLM accuracy on graph-based tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Semantic preservation is a known requirement in graph serialization and some graph-to-sequence models.",
                    "what_is_novel": "This law formalizes the necessity of semantic faithfulness for LLM-based graph learning and links it to reconstructability and downstream performance.",
                    "classification_explanation": "While semantic preservation is implicit in some prior work, its explicit connection to LLM reconstructability and performance is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic preservation in graph-to-sequence]",
                        "Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Faithful graph-to-text mappings]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Ambiguity Impairment Law",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "is_ambiguous_or_lossy",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "cannot_reconstruct_graph",
                        "object": "reliably"
                    },
                    {
                        "subject": "LLM",
                        "relation": "performs_poorly_on_graph_tasks",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Natural language summaries or incomplete encodings lead to information loss and reduced LLM accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Information loss and ambiguity are known to impair model performance in NLP and graph learning.",
                    "what_is_novel": "This law directly connects ambiguity/lossiness in graph linearization to LLM reconstructability and task performance.",
                    "classification_explanation": "The explicit focus on LLMs and graph-to-text linearization is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Faithful graph-to-text mappings]",
                        "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic preservation in graph-to-sequence]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs trained on semantically faithful, lossless graph linearizations will outperform those trained on ambiguous or lossy representations in graph reconstruction and reasoning tasks.",
        "Introducing ambiguity or information loss in the linearization will degrade LLM performance on tasks requiring precise graph understanding."
    ],
    "new_predictions_unknown": [
        "Semantically faithful linearizations may enable LLMs to learn latent graph properties (e.g., symmetry, automorphism groups) not explicitly present in the text.",
        "Lossless linearizations could allow LLMs to generalize to novel graph types (e.g., hypergraphs, multigraphs) with minimal adaptation."
    ],
    "negative_experiments": [
        "If LLMs trained on ambiguous or lossy linearizations perform as well as those trained on lossless ones, the theory would be challenged.",
        "If LLMs can reconstruct graphs from ambiguous text with high accuracy, the law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The effect of redundancy (e.g., multiple encodings of the same graph) on LLM learning is not fully explained.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can infer missing graph information from context or world knowledge, partially mitigating information loss.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with canonical, domain-specific encodings (e.g., SMILES for molecules) may allow for some ambiguity without loss of performance.",
        "Very large graphs may require compressed or hierarchical encodings, which could introduce controlled ambiguity."
    ],
    "existing_theory": {
        "what_already_exists": "Semantic preservation is a known requirement in graph serialization and some graph-to-sequence models.",
        "what_is_novel": "The explicit link between semantic faithfulness, LLM reconstructability, and downstream performance is novel.",
        "classification_explanation": "The theory builds on existing principles but applies them in a new context (LLMs and graph-to-text linearization).",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Semantic preservation in graph-to-sequence]",
            "Bevilacqua et al. (2021) One-to-Set Mapping for Graph-to-Text Generation [Faithful graph-to-text mappings]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-615",
    "original_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
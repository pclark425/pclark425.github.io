<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Consistency Uncertainty Quantification Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-440</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-440</p>
                <p><strong>Name:</strong> Self-Consistency Uncertainty Quantification Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how AI systems can systematically generate and validate scientific hypotheses, balancing novelty with plausibility, quantifying hypothesis quality, ensuring reproducibility, preventing hallucinations, and integrating statistical rigor, based on the following results.</p>
                <p><strong>Description:</strong> Uncertainty in AI-generated hypotheses can be quantified through self-consistency: sampling multiple independent generations and measuring agreement. The theory states that: (1) disagreement across samples correlates with prediction uncertainty; (2) majority voting improves accuracy; (3) optimal sample size follows a power law with diminishing returns; (4) self-consistency provides calibrated confidence estimates when combined with proper aggregation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Disagreement D across N independent samples correlates with prediction error E according to D ∝ E^α where α ∈ [0.7, 1.0].</li>
                <li>Majority voting across N samples reduces error rate by approximately 1/√N compared to single-sample generation, up to a saturation point.</li>
                <li>The optimal sample size N* follows N* ∝ log(1/ε) where ε is the target error rate, with diminishing returns beyond N* ≈ 20-30 for most tasks.</li>
                <li>Self-consistency confidence C = (votes_for_majority / N) provides calibrated probability estimates when N ≥ 10 and samples are truly independent.</li>
                <li>The computational cost-benefit ratio of self-consistency is maximized at N ≈ 5-10 samples for most practical applications.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Self-consistency improves KG-CoI accuracy and F1 for most LLMs through multiple-run aggregation <a href="../results/extraction-result-2517.html#e2517.4" class="evidence-link">[e2517.4]</a> </li>
    <li>Self-consistency with 21 samples improves CoT reasoning accuracy on multiple benchmarks <a href="../results/extraction-result-2682.html#e2682.1" class="evidence-link">[e2682.1]</a> </li>
    <li>Ensemble variance query selection uses disagreement across ensemble members to identify uncertain predictions <a href="../results/extraction-result-2659.html#e2659.1" class="evidence-link">[e2659.1]</a> </li>
    <li>Multi-step verbalized confidence aggregates per-step confidences multiplicatively <a href="../results/extraction-result-2687.html#e2687.3" class="evidence-link">[e2687.3]</a> </li>
    <li>CoT-SC with majority voting improves HotpotQA and FEVER performance <a href="../results/extraction-result-2673.html#e2673.2" class="evidence-link">[e2673.2]</a> </li>
    <li>Self-consistency provides empirical measure of output uncertainty via variability across runs <a href="../results/extraction-result-2517.html#e2517.4" class="evidence-link">[e2517.4]</a> </li>
    <li>Deep ensembles use disagreement across models for uncertainty estimation <a href="../results/extraction-result-2687.html#e2687.11" class="evidence-link">[e2687.11]</a> </li>
    <li>Sampling-based methods provide uncertainty quantification in black-box settings <a href="../results/extraction-result-2687.html#e2687.2" class="evidence-link">[e2687.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Self-consistency with N=10 samples will improve accuracy by 10-30% across diverse scientific hypothesis generation tasks compared to single-sample generation.</li>
                <li>Disagreement across samples will correlate with actual error rate at r>0.6, enabling reliable uncertainty quantification.</li>
                <li>Optimal sample size will be task-dependent but will consistently fall in the range N=5-20 for scientific reasoning tasks.</li>
                <li>Combining self-consistency with external verification will provide better calibrated confidence than either method alone.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether self-consistency uncertainty estimates generalize across different model architectures and sizes or if calibration is model-specific.</li>
                <li>If there exist tasks where self-consistency degrades rather than improves performance due to systematic biases in the model.</li>
                <li>Whether adaptive sampling (stopping when confidence threshold is reached) can reduce computational cost while maintaining accuracy.</li>
                <li>If self-consistency can be combined with other uncertainty quantification methods (Bayesian, ensemble) for improved calibration.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that disagreement does not correlate with error would invalidate the uncertainty quantification principle.</li>
                <li>Demonstrating that single-sample generation performs as well as majority voting would question the value of self-consistency.</li>
                <li>Showing that optimal sample size does not follow a predictable pattern would challenge the scaling principle.</li>
                <li>Evidence that self-consistency confidence is poorly calibrated (ECE > 0.2) would contradict the calibration claim.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to ensure true independence of samples when using temperature-based sampling </li>
    <li>Mechanisms for handling cases where all samples are incorrect are not addressed </li>
    <li>The theory does not explain how to aggregate samples when there is no clear majority </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [Original self-consistency paper, but doesn't formalize as uncertainty quantification theory]</li>
    <li>Lakshminarayanan et al. (2017) Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles [Related ensemble uncertainty work, but not for LLM self-consistency]</li>
    <li>Gal & Ghahramani (2016) Dropout as a Bayesian Approximation [Related uncertainty quantification, but different mechanism]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Self-Consistency Uncertainty Quantification Theory",
    "theory_description": "Uncertainty in AI-generated hypotheses can be quantified through self-consistency: sampling multiple independent generations and measuring agreement. The theory states that: (1) disagreement across samples correlates with prediction uncertainty; (2) majority voting improves accuracy; (3) optimal sample size follows a power law with diminishing returns; (4) self-consistency provides calibrated confidence estimates when combined with proper aggregation.",
    "supporting_evidence": [
        {
            "text": "Self-consistency improves KG-CoI accuracy and F1 for most LLMs through multiple-run aggregation",
            "uuids": [
                "e2517.4"
            ]
        },
        {
            "text": "Self-consistency with 21 samples improves CoT reasoning accuracy on multiple benchmarks",
            "uuids": [
                "e2682.1"
            ]
        },
        {
            "text": "Ensemble variance query selection uses disagreement across ensemble members to identify uncertain predictions",
            "uuids": [
                "e2659.1"
            ]
        },
        {
            "text": "Multi-step verbalized confidence aggregates per-step confidences multiplicatively",
            "uuids": [
                "e2687.3"
            ]
        },
        {
            "text": "CoT-SC with majority voting improves HotpotQA and FEVER performance",
            "uuids": [
                "e2673.2"
            ]
        },
        {
            "text": "Self-consistency provides empirical measure of output uncertainty via variability across runs",
            "uuids": [
                "e2517.4"
            ]
        },
        {
            "text": "Deep ensembles use disagreement across models for uncertainty estimation",
            "uuids": [
                "e2687.11"
            ]
        },
        {
            "text": "Sampling-based methods provide uncertainty quantification in black-box settings",
            "uuids": [
                "e2687.2"
            ]
        }
    ],
    "theory_statements": [
        "Disagreement D across N independent samples correlates with prediction error E according to D ∝ E^α where α ∈ [0.7, 1.0].",
        "Majority voting across N samples reduces error rate by approximately 1/√N compared to single-sample generation, up to a saturation point.",
        "The optimal sample size N* follows N* ∝ log(1/ε) where ε is the target error rate, with diminishing returns beyond N* ≈ 20-30 for most tasks.",
        "Self-consistency confidence C = (votes_for_majority / N) provides calibrated probability estimates when N ≥ 10 and samples are truly independent.",
        "The computational cost-benefit ratio of self-consistency is maximized at N ≈ 5-10 samples for most practical applications."
    ],
    "new_predictions_likely": [
        "Self-consistency with N=10 samples will improve accuracy by 10-30% across diverse scientific hypothesis generation tasks compared to single-sample generation.",
        "Disagreement across samples will correlate with actual error rate at r&gt;0.6, enabling reliable uncertainty quantification.",
        "Optimal sample size will be task-dependent but will consistently fall in the range N=5-20 for scientific reasoning tasks.",
        "Combining self-consistency with external verification will provide better calibrated confidence than either method alone."
    ],
    "new_predictions_unknown": [
        "Whether self-consistency uncertainty estimates generalize across different model architectures and sizes or if calibration is model-specific.",
        "If there exist tasks where self-consistency degrades rather than improves performance due to systematic biases in the model.",
        "Whether adaptive sampling (stopping when confidence threshold is reached) can reduce computational cost while maintaining accuracy.",
        "If self-consistency can be combined with other uncertainty quantification methods (Bayesian, ensemble) for improved calibration."
    ],
    "negative_experiments": [
        "Finding that disagreement does not correlate with error would invalidate the uncertainty quantification principle.",
        "Demonstrating that single-sample generation performs as well as majority voting would question the value of self-consistency.",
        "Showing that optimal sample size does not follow a predictable pattern would challenge the scaling principle.",
        "Evidence that self-consistency confidence is poorly calibrated (ECE &gt; 0.2) would contradict the calibration claim."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to ensure true independence of samples when using temperature-based sampling",
            "uuids": []
        },
        {
            "text": "Mechanisms for handling cases where all samples are incorrect are not addressed",
            "uuids": []
        },
        {
            "text": "The theory does not explain how to aggregate samples when there is no clear majority",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Self-consistency can increase output uncertainty and decrease confidence even when improving accuracy",
            "uuids": [
                "e2517.4"
            ]
        },
        {
            "text": "Computational and monetary costs increase linearly with number of samples",
            "uuids": [
                "e2517.4"
            ]
        }
    ],
    "special_cases": [
        "For deterministic tasks with single correct answers, self-consistency is most effective; for creative tasks with multiple valid answers, alternative aggregation methods may be needed.",
        "When model has systematic biases, all samples may share the same error, reducing self-consistency effectiveness.",
        "For very large models, even N=3-5 samples may provide sufficient uncertainty quantification due to higher sample quality."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [Original self-consistency paper, but doesn't formalize as uncertainty quantification theory]",
            "Lakshminarayanan et al. (2017) Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles [Related ensemble uncertainty work, but not for LLM self-consistency]",
            "Gal & Ghahramani (2016) Dropout as a Bayesian Approximation [Related uncertainty quantification, but different mechanism]"
        ]
    },
    "theory_type_general_specific": "specific",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Task World Model Synergy Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-53</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-53</p>
                <p><strong>Name:</strong> Multi-Task World Model Synergy Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how agents can use memory to help solve text games, based on the following results.</p>
                <p><strong>Description:</strong> Training world models to simultaneously predict multiple related outputs (knowledge graph updates AND valid actions) provides super-additive benefits compared to single-task training because: (1) shared representations capture common structure across tasks, (2) graph prediction provides grounding that reduces label imbalance in action prediction, (3) action prediction provides pragmatic constraints that improve graph prediction, and (4) multi-task gradients provide richer learning signal. The synergy is strongest when tasks share underlying structure but have complementary supervision signals.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Multi-task training of world models (graph + actions) provides 20-35% improvement in graph prediction and 15-25% improvement in action prediction compared to single-task training.</li>
                <li>The synergy between tasks is super-additive: multi-task performance > (single-task-A performance + single-task-B performance) / 2 + 10-15%.</li>
                <li>Multi-task benefits scale with task relatedness: for highly related tasks (e.g., graph and actions in same domain), benefit is 25-35%; for moderately related tasks, benefit is 15-25%; for weakly related tasks, benefit is <10%.</li>
                <li>The optimal task weighting in multi-task loss is approximately equal (0.5:0.5) for graph and action prediction; deviations >0.3 from equal weighting reduce performance by 10-20%.</li>
                <li>Multi-task learning provides stronger benefits for smaller models (<100M parameters) where shared representations are more critical, showing 30-40% improvement, compared to 20-30% for larger models.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Worldformer with multi-task training (graph + actions) achieves 39.15% graph EM and 23.22% action EM, outperforming single-task baselines <a href="../results/extraction-result-235.html#e235.0" class="evidence-link">[e235.0]</a> </li>
    <li>Multi-task learning alleviates label imbalance in action prediction via conditioning on next-graph predictions <a href="../results/extraction-result-235.html#e235.0" class="evidence-link">[e235.0]</a> </li>
    <li>Worldformer's multi-task objective with SOS loss yields state-of-the-art performance on both tasks <a href="../results/extraction-result-235.html#e235.0" class="evidence-link">[e235.0]</a> </li>
    <li>Single-task Seq2Seq baseline achieves only 14.29% graph EM and 18.10% action EM, substantially worse than multi-task Worldformer <a href="../results/extraction-result-235.html#e235.4" class="evidence-link">[e235.4]</a> </li>
    <li>CALM (single-task action generation) shows high variance and is outperformed by multi-task Worldformer <a href="../results/extraction-result-235.html#e235.5" class="evidence-link">[e235.5]</a> </li>
    <li>Graph-difference formulation combined with multi-task learning produces largest performance gains <a href="../results/extraction-result-235.html#e235.0" class="evidence-link">[e235.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Adding a third related task (e.g., predicting reward or episode termination) to the multi-task objective will provide additional 5-10% improvement in graph and action prediction.</li>
                <li>For text game domains with very different action structures (e.g., parser-based vs. choice-based), multi-task training across domains will show reduced synergy (<15% improvement) compared to within-domain multi-task training.</li>
                <li>Multi-task models will show better sample efficiency: achieving target performance with 30-40% fewer training examples compared to single-task models.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>For tasks where graph and action predictions have conflicting objectives (e.g., graph requires conservative prediction, actions require exploration), multi-task training might hurt performance - effect could range from -10% to +15%.</li>
                <li>In domains with highly imbalanced task difficulties (one task much harder than the other), multi-task training might lead to negative transfer - impact unclear, possibly -5% to +20%.</li>
                <li>For very large models (>1B parameters) with sufficient capacity, multi-task benefits might diminish or disappear - effect unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where single-task training consistently outperforms multi-task training would challenge the synergy claim.</li>
                <li>Demonstrating that multi-task benefits are purely additive (no super-additive synergy) would challenge the shared representation mechanism.</li>
                <li>Showing that multi-task training with unrelated tasks (e.g., graph prediction + image classification) provides similar benefits would challenge the task-relatedness requirement.</li>
                <li>Finding that extreme task weighting (e.g., 0.9:0.1) performs as well as balanced weighting would challenge the optimal weighting claim.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically determine optimal task weights for new domains is not addressed <a href="../results/extraction-result-235.html#e235.0" class="evidence-link">[e235.0]</a> </li>
    <li>The interaction between multi-task learning and different architectural choices (shared vs. separate encoders) is not fully explored <a href="../results/extraction-result-235.html#e235.0" class="evidence-link">[e235.0]</a> </li>
    <li>How multi-task benefits scale with the number of tasks (3, 4, 5+ tasks) is not characterized <a href="../results/extraction-result-235.html#e235.0" class="evidence-link">[e235.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Caruana (1997) Multitask learning [Foundational multi-task learning theory]</li>
    <li>Ruder (2017) An overview of multi-task learning in deep neural networks [Survey of multi-task learning approaches]</li>
    <li>Ammanabrolu et al. (2021) Learning Knowledge Graph-based World Models of Textual Environments [Applies multi-task learning to text game world models]</li>
    <li>Crawshaw (2020) Multi-task learning with deep neural networks: A survey [Recent survey including negative transfer]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Task World Model Synergy Theory",
    "theory_description": "Training world models to simultaneously predict multiple related outputs (knowledge graph updates AND valid actions) provides super-additive benefits compared to single-task training because: (1) shared representations capture common structure across tasks, (2) graph prediction provides grounding that reduces label imbalance in action prediction, (3) action prediction provides pragmatic constraints that improve graph prediction, and (4) multi-task gradients provide richer learning signal. The synergy is strongest when tasks share underlying structure but have complementary supervision signals.",
    "supporting_evidence": [
        {
            "text": "Worldformer with multi-task training (graph + actions) achieves 39.15% graph EM and 23.22% action EM, outperforming single-task baselines",
            "uuids": [
                "e235.0"
            ]
        },
        {
            "text": "Multi-task learning alleviates label imbalance in action prediction via conditioning on next-graph predictions",
            "uuids": [
                "e235.0"
            ]
        },
        {
            "text": "Worldformer's multi-task objective with SOS loss yields state-of-the-art performance on both tasks",
            "uuids": [
                "e235.0"
            ]
        },
        {
            "text": "Single-task Seq2Seq baseline achieves only 14.29% graph EM and 18.10% action EM, substantially worse than multi-task Worldformer",
            "uuids": [
                "e235.4"
            ]
        },
        {
            "text": "CALM (single-task action generation) shows high variance and is outperformed by multi-task Worldformer",
            "uuids": [
                "e235.5"
            ]
        },
        {
            "text": "Graph-difference formulation combined with multi-task learning produces largest performance gains",
            "uuids": [
                "e235.0"
            ]
        }
    ],
    "theory_statements": [
        "Multi-task training of world models (graph + actions) provides 20-35% improvement in graph prediction and 15-25% improvement in action prediction compared to single-task training.",
        "The synergy between tasks is super-additive: multi-task performance &gt; (single-task-A performance + single-task-B performance) / 2 + 10-15%.",
        "Multi-task benefits scale with task relatedness: for highly related tasks (e.g., graph and actions in same domain), benefit is 25-35%; for moderately related tasks, benefit is 15-25%; for weakly related tasks, benefit is &lt;10%.",
        "The optimal task weighting in multi-task loss is approximately equal (0.5:0.5) for graph and action prediction; deviations &gt;0.3 from equal weighting reduce performance by 10-20%.",
        "Multi-task learning provides stronger benefits for smaller models (&lt;100M parameters) where shared representations are more critical, showing 30-40% improvement, compared to 20-30% for larger models."
    ],
    "new_predictions_likely": [
        "Adding a third related task (e.g., predicting reward or episode termination) to the multi-task objective will provide additional 5-10% improvement in graph and action prediction.",
        "For text game domains with very different action structures (e.g., parser-based vs. choice-based), multi-task training across domains will show reduced synergy (&lt;15% improvement) compared to within-domain multi-task training.",
        "Multi-task models will show better sample efficiency: achieving target performance with 30-40% fewer training examples compared to single-task models."
    ],
    "new_predictions_unknown": [
        "For tasks where graph and action predictions have conflicting objectives (e.g., graph requires conservative prediction, actions require exploration), multi-task training might hurt performance - effect could range from -10% to +15%.",
        "In domains with highly imbalanced task difficulties (one task much harder than the other), multi-task training might lead to negative transfer - impact unclear, possibly -5% to +20%.",
        "For very large models (&gt;1B parameters) with sufficient capacity, multi-task benefits might diminish or disappear - effect unknown."
    ],
    "negative_experiments": [
        "Finding domains where single-task training consistently outperforms multi-task training would challenge the synergy claim.",
        "Demonstrating that multi-task benefits are purely additive (no super-additive synergy) would challenge the shared representation mechanism.",
        "Showing that multi-task training with unrelated tasks (e.g., graph prediction + image classification) provides similar benefits would challenge the task-relatedness requirement.",
        "Finding that extreme task weighting (e.g., 0.9:0.1) performs as well as balanced weighting would challenge the optimal weighting claim."
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically determine optimal task weights for new domains is not addressed",
            "uuids": [
                "e235.0"
            ]
        },
        {
            "text": "The interaction between multi-task learning and different architectural choices (shared vs. separate encoders) is not fully explored",
            "uuids": [
                "e235.0"
            ]
        },
        {
            "text": "How multi-task benefits scale with the number of tasks (3, 4, 5+ tasks) is not characterized",
            "uuids": [
                "e235.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some single-task approaches (e.g., Q*BERT for extraction) achieve competitive performance on specific metrics, suggesting multi-task is not always necessary",
            "uuids": [
                "e235.2"
            ]
        }
    ],
    "special_cases": [
        "For tasks with very different output structures (e.g., continuous vs. discrete), multi-task training may require careful architectural design to achieve synergy.",
        "In domains with extreme label imbalance in one task, multi-task training might be dominated by the balanced task.",
        "For very simple tasks where single-task models already achieve &gt;90% performance, multi-task benefits may be minimal."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Caruana (1997) Multitask learning [Foundational multi-task learning theory]",
            "Ruder (2017) An overview of multi-task learning in deep neural networks [Survey of multi-task learning approaches]",
            "Ammanabrolu et al. (2021) Learning Knowledge Graph-based World Models of Textual Environments [Applies multi-task learning to text game world models]",
            "Crawshaw (2020) Multi-task learning with deep neural networks: A survey [Recent survey including negative transfer]"
        ]
    },
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Context-Dependent Evaluation Validity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-575</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-575</p>
                <p><strong>Name:</strong> Context-Dependent Evaluation Validity Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> The validity and reliability of evaluation methods for LLM-generated scientific theories are highly context-dependent, varying systematically with factors including: (1) the availability and quality of ground truth, (2) the complexity and structure of the domain, (3) the stage of the scientific process (hypothesis generation vs. validation vs. refinement), and (4) the intended use of the evaluation (screening vs. ranking vs. selection). No single evaluation method is universally optimal; instead, evaluation method selection must be matched to the specific context. Methods that work well in one context may fail or produce misleading results in another. Furthermore, hybrid approaches combining automated and human evaluation often outperform purely automated or purely human methods, and cost-effectiveness considerations must be balanced against validity requirements.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Ground Truth Availability Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_task &#8594; has_ground_truth &#8594; available<span style="color: #888888;">, and</span></div>
        <div>&#8226; ground_truth &#8594; has_quality &#8594; high<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_method &#8594; is_type &#8594; supervised</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_method &#8594; achieves &#8594; high_validity<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_method &#8594; is_preferred_over &#8594; unsupervised_methods</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Deterministic domain validators with exact enumeration of admissible hypothesis sets enable precise coverage measurement and remove subjectivity from evaluation. <a href="../results/extraction-result-4469.html#e4469.4" class="evidence-link">[e4469.4]</a> </li>
    <li>HDR multiplies Feature Discovery Rate by Relationship Correctness, computed per dataset and averaged across configurations, requiring ground-truth hypotheses. On synthetic datasets: best-performing model+method achieved HDR=93.8% at base difficulty. <a href="../results/extraction-result-4448.html#e4448.1" class="evidence-link">[e4448.1]</a> </li>
    <li>HMS evaluates data-driven hypotheses by comparing predicted and gold sub-hypotheses using context F1, variable F1, and relationship accuracy. <a href="../results/extraction-result-4559.html#e4559.2" class="evidence-link">[e4559.2]</a> </li>
    <li>CSX-Sim validation was performed on 124 published hypotheses (30 research questions) with known experimental outcomes, achieving Spearman ρ=0.960, PCI=26/30, RMSE=0.213. <a href="../results/extraction-result-4525.html#e4525.1" class="evidence-link">[e4525.1]</a> </li>
    <li>TruthHypo benchmark uses temporally split PubTator-derived knowledge graph with controlled retrieval settings to evaluate hypothesis generation with ground-truth labels. <a href="../results/extraction-result-4461.html#e4461.0" class="evidence-link">[e4461.0]</a> </li>
    <li>TOMATO-Chem benchmark of 51 high-impact chemistry papers with expert-identified inspirations and ground-truth hypotheses enables evaluation of hypothesis rediscovery and ranking. <a href="../results/extraction-result-4563.html#e4563.7" class="evidence-link">[e4563.7]</a> </li>
    <li>Masked-edge PubTator3 hypothesis generation dataset contains 300 examples balanced across relation types with ground truth for hypothesis classification evaluation. <a href="../results/extraction-result-4540.html#e4540.3" class="evidence-link">[e4540.3]</a> </li>
    <li>DiscoveryBench uses ground-truth data-driven hypotheses to compute HMS for DB-REAL and DB-SYNTH tasks. <a href="../results/extraction-result-4559.html#e4559.2" class="evidence-link">[e4559.2]</a> </li>
    <li>HypoBench synthetic datasets with controlled difficulty levels and explicit ground-truth hypothesis generation enable precise HDR computation. <a href="../results/extraction-result-4448.html#e4448.1" class="evidence-link">[e4448.1]</a> </li>
    <li>Comprehensiveness measured as Reference Recall against Ground-Truth Bibliography provides objective coverage metric when GT exists. <a href="../results/extraction-result-4428.html#e4428.0" class="evidence-link">[e4428.0]</a> </li>
    <li>Three-level Citation Accuracy via NLI uses ground-truth bibliographies to measure document, section, and sentence-level citation appropriateness. <a href="../results/extraction-result-4428.html#e4428.1" class="evidence-link">[e4428.1]</a> </li>
    <li>OOD generalization assessment with held-out test sets measures extrapolation ability using ground-truth labels in reserved data ranges. <a href="../results/extraction-result-4453.html#e4453.4" class="evidence-link">[e4453.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The advantage of supervised over unsupervised methods when ground truth is available is well-known in ML evaluation, but this specific formulation for scientific theory evaluation with emphasis on ground-truth quality and domain-specific validation is a novel synthesis.</p>
            <p><strong>References:</strong> <ul>
    <li>Caruana & Niculescu-Mizil (2006) An Empirical Comparison of Supervised Learning Algorithms [supervised method advantages in ML]</li>
    <li>Japkowicz & Shah (2011) Evaluating Learning Algorithms: A Classification Perspective [importance of ground truth quality]</li>
</ul>
            <h3>Statement 1: Domain Complexity Scaling Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; domain &#8594; has_complexity &#8594; C<span style="color: #888888;">, and</span></div>
        <div>&#8226; C &#8594; exceeds &#8594; complexity_threshold<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_method &#8594; is_type &#8594; simple_metric</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_method &#8594; has_validity &#8594; low<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_method &#8594; requires_replacement_with &#8594; multi_dimensional_method</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Communication-based model discovery showed explanations were more helpful in simpler domains (e.g., animal growth) and less helpful in complex domains (moral judgments). <a href="../results/extraction-result-4422.html#e4422.3" class="evidence-link">[e4422.3]</a> </li>
    <li>Different domains show different patterns in symbolic equation discovery: chemistry & biology had larger ID/OOD gaps than other domains. <a href="../results/extraction-result-4453.html#e4453.4" class="evidence-link">[e4453.4]</a> </li>
    <li>Domain-specific evaluation criteria vary substantially: Problems use Clarity/Relevance/Originality/Feasibility/Significance; Methods use Clarity/Validity/Rigorousness/Innovativeness/Generalizability; Experiments use Clarity/Validity/Robustness/Feasibility/Reproducibility. <a href="../results/extraction-result-4568.html#e4568.0" class="evidence-link">[e4568.0]</a> </li>
    <li>MaSTeA evaluation on undergraduate-level materials science questions may not reflect advanced hypothesis-generation evaluation needs in more complex domains. <a href="../results/extraction-result-4442.html#e4442.4" class="evidence-link">[e4442.4]</a> </li>
    <li>ACCELMAT evaluation framework uses dual-component scoring (Closeness + Quality with six expert-derived criteria) to handle complexity of materials discovery domain. <a href="../results/extraction-result-4452.html#e4452.0" class="evidence-link">[e4452.0]</a> </li>
    <li>Multi-dimensional SciHorizon framework evaluates across Understanding, Reasoning, Knowledge (with subdimensions), and Values to capture complexity of scientific domains. <a href="../results/extraction-result-4446.html#e4446.1" class="evidence-link">[e4446.1]</a> <a href="../results/extraction-result-4446.html#e4446.5" class="evidence-link">[e4446.5]</a> <a href="../results/extraction-result-4446.html#e4446.7" class="evidence-link">[e4446.7]</a> </li>
    <li>IBE-Eval uses multiple explicit features (consistency, parsimony, coherence, uncertainty) combined with learned linear model to handle complexity of causal explanation evaluation. <a href="../results/extraction-result-4466.html#e4466.0" class="evidence-link">[e4466.0]</a> </li>
    <li>Surface metrics (BLEU/ROUGE) are inadequate for hypothesis evaluation because they fail to capture semantic depth, novelty, or testability in complex scientific domains. <a href="../results/extraction-result-4612.html#e4612.3" class="evidence-link">[e4612.3]</a> </li>
    <li>Automated similarity metrics reward surface-level similarity and templates, fail to capture novelty or technical depth, and penalize longer detailed outputs in complex domains. <a href="../results/extraction-result-4433.html#e4433.3" class="evidence-link">[e4433.3]</a> </li>
    <li>ChemQA multimodal benchmark shows models perform poorly on image-only inputs in chemistry domain, indicating complexity of multimodal scientific reasoning. <a href="../results/extraction-result-4470.html#e4470.8" class="evidence-link">[e4470.8]</a> </li>
    <li>Consensus/alignment checks are needed in clinical domains to evaluate whether outputs align with scientific/clinical consensus or expert guidelines. <a href="../results/extraction-result-4480.html#e4480.12" class="evidence-link">[e4480.12]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While the relationship between task complexity and evaluation difficulty is recognized in ML, this explicit formulation as a scaling law for scientific theory evaluation with domain-specific thresholds is novel.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 2: Process Stage Matching Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; scientific_process &#8594; is_at_stage &#8594; stage_S<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_method &#8594; is_designed_for &#8594; stage_T<span style="color: #888888;">, and</span></div>
        <div>&#8226; stage_S &#8594; differs_from &#8594; stage_T</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_method &#8594; produces &#8594; suboptimal_results<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_method &#8594; may_mislead &#8594; decision_making</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Formative evaluation focused on generating empirically grounded interpretations to improve artifacts during development; restricts evaluative criteria to relevance, alignment, effectiveness, and unintended effects rather than summative assessment. <a href="../results/extraction-result-4424.html#e4424.5" class="evidence-link">[e4424.5]</a> </li>
    <li>Metric-Based Screening automatically evaluates candidates for large-scale screening and ranking, often combined with downstream human review for high-stakes validation (hybrid approach for different stages). <a href="../results/extraction-result-4450.html#e4450.2" class="evidence-link">[e4450.2]</a> </li>
    <li>Entropy-guided refinement selection applies different strategies (Deepening, Counterfactual, Hybridization) at different stages based on uncertainty thresholds. <a href="../results/extraction-result-4419.html#e4419.5" class="evidence-link">[e4419.5]</a> </li>
    <li>SCIMON novelty-driven evaluation uses iterative revision until novelty threshold is reached, appropriate for generation stage but not validation stage. <a href="../results/extraction-result-4444.html#e4444.1" class="evidence-link">[e4444.1]</a> </li>
    <li>Multi-agent framework with Analyst/Engineer/Scientist/Critic roles shows iterative refinement improves hypothesis quality through stage-appropriate evaluation. <a href="../results/extraction-result-4443.html#e4443.6" class="evidence-link">[e4443.6]</a> </li>
    <li>CycleResearcher/CycleReviewer iterative preference-training framework alternates generation and review phases with stage-appropriate feedback. <a href="../results/extraction-result-4479.html#e4479.9" class="evidence-link">[e4479.9]</a> </li>
    <li>Self-critique and chain-of-thought prompting techniques induce iterative refinement appropriate for generation/refinement stages but not final validation. <a href="../results/extraction-result-4440.html#e4440.4" class="evidence-link">[e4440.4]</a> </li>
    <li>POPPER automated validation framework applies sequential falsification strategies appropriate for validation stage, achieving human-level accuracy at higher speed. <a href="../results/extraction-result-4450.html#e4450.3" class="evidence-link">[e4450.3]</a> </li>
    <li>Simulation-based validation using digital twins is appropriate for pre-testing stage before real-world experiments. <a href="../results/extraction-result-4605.html#e4605.6" class="evidence-link">[e4605.6]</a> </li>
    <li>Human-in-the-loop protocols combine model suggestions with expert judgment, appropriate for refinement and validation stages. <a href="../results/extraction-result-4612.html#e4612.12" class="evidence-link">[e4612.12]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The need to match evaluation to process stage is recognized in design science (formative vs summative evaluation), but this explicit formulation as a law for scientific theory evaluation across generation/validation/refinement stages is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Scriven (1967) The Methodology of Evaluation [formative vs summative evaluation distinction]</li>
    <li>Venable et al. (2016) FEDS: A Framework for Evaluation in Design Science Research [evaluation strategy framework]</li>
</ul>
            <h3>Statement 3: Use Case Optimization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_use_case &#8594; is &#8594; screening<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_method &#8594; optimizes_for &#8594; precision</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_method &#8594; is_suboptimal_for &#8594; screening<span style="color: #888888;">, and</span></div>
        <div>&#8226; screening_task &#8594; requires_optimization_for &#8594; recall</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Metric-Based Screening uses predefined objective metrics for large-scale screening and ranking to triage candidate theories, optimizing for recall to avoid missing promising candidates. <a href="../results/extraction-result-4450.html#e4450.2" class="evidence-link">[e4450.2]</a> </li>
    <li>Hybrid evaluation combining automated screening to top-K candidates followed by human evaluation of top-K achieves high coverage (recall), high validity (precision), and remains cost-effective. <a href="../results/extraction-result-4450.html#e4450.2" class="evidence-link">[e4450.2]</a> </li>
    <li>Pass@N reports whether at least one of N sampled outputs passes evaluation, intended to approximate upper-bound quality (optimizes for recall of any good output). <a href="../results/extraction-result-4562.html#e4562.2" class="evidence-link">[e4562.2]</a> </li>
    <li>Oracle inference retrospectively selects best hypothesis to measure maximum predictive potential of generated pool (upper-bound recall measure). <a href="../results/extraction-result-4464.html#e4464.2" class="evidence-link">[e4464.2]</a> </li>
    <li>Inspiration Retrieval using Hit Ratio with stratified negative candidates measures recall of ground-truth inspirations at different retention rates (top 4% vs top 20%). <a href="../results/extraction-result-4460.html#e4460.1" class="evidence-link">[e4460.1]</a> </li>
    <li>Reference Recall measures coverage of central literature as proportion of GT citations retrieved, appropriate for screening comprehensiveness. <a href="../results/extraction-result-4428.html#e4428.0" class="evidence-link">[e4428.0]</a> </li>
    <li>Recall/Precision retrieval metrics measure whether correct items are present and ranked, with different optimization for screening vs selection. <a href="../results/extraction-result-4575.html#e4575.11" class="evidence-link">[e4575.11]</a> </li>
    <li>WRecall (Weighted Recall) approximates recall of generated rules when ground-truth unavailable, used for screening rule quality. <a href="../results/extraction-result-4603.html#e4603.1" class="evidence-link">[e4603.1]</a> </li>
    <li>Ranking/Average Rank Ratio quantifies how highly promising hypotheses are ranked, balancing recall (finding good candidates) with precision (ranking them highly). <a href="../results/extraction-result-4418.html#e4418.3" class="evidence-link">[e4418.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The precision-recall trade-off is well-known in information retrieval, but this specific formulation for evaluation method selection in scientific theory assessment with explicit use-case matching is a novel application.</p>
            <p><strong>References:</strong> <ul>
    <li>Manning et al. (2008) Introduction to Information Retrieval [precision-recall trade-offs in IR]</li>
    <li>Davis & Goadrich (2006) The Relationship Between Precision-Recall and ROC Curves [precision-recall optimization]</li>
</ul>
            <h3>Statement 4: Hybrid Evaluation Superiority Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_approach &#8594; combines &#8594; automated_and_human_methods<span style="color: #888888;">, and</span></div>
        <div>&#8226; automated_methods &#8594; used_for &#8594; large_scale_screening<span style="color: #888888;">, and</span></div>
        <div>&#8226; human_methods &#8594; used_for &#8594; high_stakes_validation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; hybrid_approach &#8594; outperforms &#8594; purely_automated_methods<span style="color: #888888;">, and</span></div>
        <div>&#8226; hybrid_approach &#8594; outperforms &#8594; purely_human_methods<span style="color: #888888;">, and</span></div>
        <div>&#8226; hybrid_approach &#8594; achieves &#8594; optimal_cost_effectiveness</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Metric-Based Screening integrates automated metrics with human oversight for interpretation and experimental prioritization, combining scalability with validity. <a href="../results/extraction-result-4450.html#e4450.2" class="evidence-link">[e4450.2]</a> </li>
    <li>Human expert evaluation used as independent ground truth for validating automated metrics (ON and LLM-review), with hybrid analysis combining automated metrics and human ratings. <a href="../results/extraction-result-4439.html#e4439.3" class="evidence-link">[e4439.3]</a> </li>
    <li>Multi-Rater Reliability Protocols use explicit scoring rubrics with multiple annotators, combining structured evaluation with human judgment. <a href="../results/extraction-result-4612.html#e4612.2" class="evidence-link">[e4612.2]</a> </li>
    <li>Expert Panel Review with Inter-rater Concordance (Cohen's κ=0.82) combines independent expert ratings with consensus resolution for high-stakes validation. <a href="../results/extraction-result-4410.html#e4410.2" class="evidence-link">[e4410.2]</a> </li>
    <li>Meta-evaluation via correlation to human judgments validates automated metrics by measuring statistical correlations with human ratings. <a href="../results/extraction-result-4610.html#e4610.1" class="evidence-link">[e4610.1]</a> </li>
    <li>Human expert Likert-scale scoring and pairwise comparison used as gold-standard reference for alignment and validation of automated methods. <a href="../results/extraction-result-4568.html#e4568.0" class="evidence-link">[e4568.0]</a> </li>
    <li>YESciEval alignment method combines supervised fine-tuning with RL to train evaluators that balance automated scalability with human-aligned judgments. <a href="../results/extraction-result-4451.html#e4451.3" class="evidence-link">[e4451.3]</a> </li>
    <li>ACCELMAT evaluation framework uses automated LLM scoring validated by independent human expert evaluations showing alignment. <a href="../results/extraction-result-4452.html#e4452.0" class="evidence-link">[e4452.0]</a> </li>
    <li>TruthHypo benchmark uses automated metrics with human expert annotation in open-ended tasks (80% agreement) for validation. <a href="../results/extraction-result-4461.html#e4461.0" class="evidence-link">[e4461.0]</a> </li>
    <li>ResearchBench expert validation pipeline where domain experts assess automated extraction accuracy (91.9% with major issues only). <a href="../results/extraction-result-4460.html#e4460.4" class="evidence-link">[e4460.4]</a> </li>
    <li>Human-in-the-loop protocols combine model suggestions with expert judgment to produce higher-quality proposals than either alone. <a href="../results/extraction-result-4612.html#e4612.12" class="evidence-link">[e4612.12]</a> </li>
    <li>Consensus/alignment checks combine automated mapping with clinician adjudication for clinical domain evaluation. <a href="../results/extraction-result-4480.html#e4480.12" class="evidence-link">[e4480.12]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While human-in-the-loop and hybrid evaluation are recognized practices, this explicit formulation as a superiority law with cost-effectiveness optimization for scientific theory evaluation is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Amershi et al. (2014) Power to the People: The Role of Humans in Interactive Machine Learning [human-in-the-loop ML]</li>
    <li>Mosqueira-Rey et al. (2023) Human-in-the-loop machine learning: A state of the art [hybrid approaches]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an evaluation method designed for hypothesis generation (emphasizing diversity and novelty) is applied to hypothesis validation (requiring accuracy and groundedness), it will produce rankings that poorly predict experimental success, with correlation to experimental outcomes dropping by at least 30% compared to validation-appropriate methods.</li>
                <li>If a simple single-metric evaluation (e.g., BLEU or ROUGE) is applied to a highly complex domain (e.g., multi-scale biological systems), it will show correlation with expert judgments at least 0.3 lower (Spearman ρ) than in simpler domains (e.g., single-variable physics problems).</li>
                <li>If a screening evaluation optimized for high recall (e.g., Pass@N with large N) is used for final selection without subsequent precision-focused filtering, it will include at least 2-3x more false positives compared to a selection-optimized evaluation with appropriate precision-recall balance.</li>
                <li>If hybrid evaluation (automated screening + human validation of top-K) is compared to purely automated or purely human evaluation on the same budget, the hybrid approach will achieve at least 20% better cost-effectiveness (measured as valid hypotheses identified per dollar spent).</li>
                <li>If evaluation methods are matched to domain complexity (multi-dimensional methods for complex domains, simple metrics for simple domains), the average correlation with expert judgments will be at least 0.2 higher (Spearman ρ) than mismatched methods.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If evaluation methods are adaptively selected based on automatically detected context features (ground truth availability, domain complexity, process stage, use case), they might substantially outperform fixed-method approaches by 30-50%, but the optimal selection algorithm and feature weights are unknown and may vary significantly by domain.</li>
                <li>If the same evaluation method is applied across domains of varying complexity, the validity degradation rate as a function of complexity might follow a power law, exponential decay, or threshold-based step function - the functional form is unknown and could have major implications for when to switch methods.</li>
                <li>If process stage transitions are gradual rather than discrete (e.g., moving from generation to validation over multiple iterations), the optimal timing and method for switching evaluation approaches might require continuous adaptation, but whether smooth interpolation between methods or discrete switching is better is unknown.</li>
                <li>If cost-effectiveness is explicitly optimized in hybrid evaluation design (varying the automated/human ratio), there might be a universal optimal ratio (e.g., 80/20 rule) or it might vary dramatically by domain and stage - the existence and nature of such optima is unknown.</li>
                <li>If evaluation methods are calibrated using small amounts of ground truth and then applied to larger datasets without ground truth, the calibration might transfer well (enabling efficient scaling) or might fail catastrophically due to distribution shift - the robustness of such transfer is unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding a single evaluation method that achieves correlation >0.9 with expert judgments across all contexts (ground truth availability, domain complexity, process stages, use cases) would challenge the entire theory and suggest context-independence is achievable.</li>
                <li>Demonstrating that evaluation method validity is independent of domain complexity (i.e., simple metrics work as well in complex domains as in simple domains) would directly contradict the Domain Complexity Scaling Law.</li>
                <li>Showing that the same evaluation method works equally well for screening (requiring high recall) and selection (requiring high precision) with no performance trade-off would challenge the Use Case Optimization Law.</li>
                <li>Finding that purely automated evaluation consistently outperforms hybrid approaches across multiple domains and stages would challenge the Hybrid Evaluation Superiority Law.</li>
                <li>Demonstrating that evaluation methods designed for one process stage (e.g., generation) work equally well at other stages (e.g., validation) without adaptation would contradict the Process Stage Matching Law.</li>
                <li>Showing that evaluation validity remains constant as ground truth quality degrades would challenge the Ground Truth Availability Law's emphasis on ground truth quality.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory doesn't specify how to quantitatively measure domain complexity - whether it should be based on number of variables, interaction complexity, required background knowledge, or other factors. </li>
    <li>Optimal methods for contexts with partial or noisy ground truth (between full ground truth and no ground truth) are not specified, though this is a common real-world scenario. </li>
    <li>The theory doesn't address how to handle transitions between process stages - whether to switch methods abruptly or gradually interpolate between stage-appropriate methods. </li>
    <li>The role of evaluator calibration and how calibration requirements vary by context is not explicitly addressed, though calibration is mentioned in several pieces of evidence. <a href="../results/extraction-result-4560.html#e4560.3" class="evidence-link">[e4560.3]</a> <a href="../results/extraction-result-4549.html#e4549.1" class="evidence-link">[e4549.1]</a> </li>
    <li>Temporal aspects of evaluation (e.g., how to evaluate truly novel theories vs. incremental advances) are not fully captured by the current laws. </li>
    <li>The theory doesn't specify how to handle multimodal evaluation (combining different types of evidence like text, images, code, experimental data) which is increasingly important. <a href="../results/extraction-result-4470.html#e4470.8" class="evidence-link">[e4470.8]</a> </li>
    <li>Cross-domain transfer of evaluation methods and when such transfer is appropriate is not addressed. <a href="../results/extraction-result-4447.html#e4447.0" class="evidence-link">[e4447.0]</a> </li>
    <li>The theory doesn't specify how evaluation requirements change with the intended audience (expert vs. non-expert, domain-specific vs. general). </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes observations about context-dependent evaluation validity into explicit laws. While context-dependence of evaluation is recognized in educational assessment and design science (formative vs summative evaluation), and precision-recall trade-offs are known in information retrieval, the explicit formulation of laws governing when evaluation methods succeed or fail in scientific theory assessment, particularly the integration of ground truth quality, domain complexity, process stage, and use case factors, represents a novel synthesis and formalization.</p>
            <p><strong>References:</strong> <ul>
    <li>Scriven (1967) The Methodology of Evaluation [context-dependent evaluation, formative vs summative]</li>
    <li>Shadish et al. (2002) Experimental and Quasi-Experimental Designs for Generalized Causal Inference [validity types and contexts]</li>
    <li>Messick (1989) Validity [unified validity theory emphasizing context]</li>
    <li>Manning et al. (2008) Introduction to Information Retrieval [precision-recall trade-offs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Context-Dependent Evaluation Validity Theory",
    "theory_description": "The validity and reliability of evaluation methods for LLM-generated scientific theories are highly context-dependent, varying systematically with factors including: (1) the availability and quality of ground truth, (2) the complexity and structure of the domain, (3) the stage of the scientific process (hypothesis generation vs. validation vs. refinement), and (4) the intended use of the evaluation (screening vs. ranking vs. selection). No single evaluation method is universally optimal; instead, evaluation method selection must be matched to the specific context. Methods that work well in one context may fail or produce misleading results in another. Furthermore, hybrid approaches combining automated and human evaluation often outperform purely automated or purely human methods, and cost-effectiveness considerations must be balanced against validity requirements.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Ground Truth Availability Law",
                "if": [
                    {
                        "subject": "evaluation_task",
                        "relation": "has_ground_truth",
                        "object": "available"
                    },
                    {
                        "subject": "ground_truth",
                        "relation": "has_quality",
                        "object": "high"
                    },
                    {
                        "subject": "evaluation_method",
                        "relation": "is_type",
                        "object": "supervised"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_method",
                        "relation": "achieves",
                        "object": "high_validity"
                    },
                    {
                        "subject": "evaluation_method",
                        "relation": "is_preferred_over",
                        "object": "unsupervised_methods"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Deterministic domain validators with exact enumeration of admissible hypothesis sets enable precise coverage measurement and remove subjectivity from evaluation.",
                        "uuids": [
                            "e4469.4"
                        ]
                    },
                    {
                        "text": "HDR multiplies Feature Discovery Rate by Relationship Correctness, computed per dataset and averaged across configurations, requiring ground-truth hypotheses. On synthetic datasets: best-performing model+method achieved HDR=93.8% at base difficulty.",
                        "uuids": [
                            "e4448.1"
                        ]
                    },
                    {
                        "text": "HMS evaluates data-driven hypotheses by comparing predicted and gold sub-hypotheses using context F1, variable F1, and relationship accuracy.",
                        "uuids": [
                            "e4559.2"
                        ]
                    },
                    {
                        "text": "CSX-Sim validation was performed on 124 published hypotheses (30 research questions) with known experimental outcomes, achieving Spearman ρ=0.960, PCI=26/30, RMSE=0.213.",
                        "uuids": [
                            "e4525.1"
                        ]
                    },
                    {
                        "text": "TruthHypo benchmark uses temporally split PubTator-derived knowledge graph with controlled retrieval settings to evaluate hypothesis generation with ground-truth labels.",
                        "uuids": [
                            "e4461.0"
                        ]
                    },
                    {
                        "text": "TOMATO-Chem benchmark of 51 high-impact chemistry papers with expert-identified inspirations and ground-truth hypotheses enables evaluation of hypothesis rediscovery and ranking.",
                        "uuids": [
                            "e4563.7"
                        ]
                    },
                    {
                        "text": "Masked-edge PubTator3 hypothesis generation dataset contains 300 examples balanced across relation types with ground truth for hypothesis classification evaluation.",
                        "uuids": [
                            "e4540.3"
                        ]
                    },
                    {
                        "text": "DiscoveryBench uses ground-truth data-driven hypotheses to compute HMS for DB-REAL and DB-SYNTH tasks.",
                        "uuids": [
                            "e4559.2"
                        ]
                    },
                    {
                        "text": "HypoBench synthetic datasets with controlled difficulty levels and explicit ground-truth hypothesis generation enable precise HDR computation.",
                        "uuids": [
                            "e4448.1"
                        ]
                    },
                    {
                        "text": "Comprehensiveness measured as Reference Recall against Ground-Truth Bibliography provides objective coverage metric when GT exists.",
                        "uuids": [
                            "e4428.0"
                        ]
                    },
                    {
                        "text": "Three-level Citation Accuracy via NLI uses ground-truth bibliographies to measure document, section, and sentence-level citation appropriateness.",
                        "uuids": [
                            "e4428.1"
                        ]
                    },
                    {
                        "text": "OOD generalization assessment with held-out test sets measures extrapolation ability using ground-truth labels in reserved data ranges.",
                        "uuids": [
                            "e4453.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "The advantage of supervised over unsupervised methods when ground truth is available is well-known in ML evaluation, but this specific formulation for scientific theory evaluation with emphasis on ground-truth quality and domain-specific validation is a novel synthesis.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Caruana & Niculescu-Mizil (2006) An Empirical Comparison of Supervised Learning Algorithms [supervised method advantages in ML]",
                        "Japkowicz & Shah (2011) Evaluating Learning Algorithms: A Classification Perspective [importance of ground truth quality]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Domain Complexity Scaling Law",
                "if": [
                    {
                        "subject": "domain",
                        "relation": "has_complexity",
                        "object": "C"
                    },
                    {
                        "subject": "C",
                        "relation": "exceeds",
                        "object": "complexity_threshold"
                    },
                    {
                        "subject": "evaluation_method",
                        "relation": "is_type",
                        "object": "simple_metric"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_method",
                        "relation": "has_validity",
                        "object": "low"
                    },
                    {
                        "subject": "evaluation_method",
                        "relation": "requires_replacement_with",
                        "object": "multi_dimensional_method"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Communication-based model discovery showed explanations were more helpful in simpler domains (e.g., animal growth) and less helpful in complex domains (moral judgments).",
                        "uuids": [
                            "e4422.3"
                        ]
                    },
                    {
                        "text": "Different domains show different patterns in symbolic equation discovery: chemistry & biology had larger ID/OOD gaps than other domains.",
                        "uuids": [
                            "e4453.4"
                        ]
                    },
                    {
                        "text": "Domain-specific evaluation criteria vary substantially: Problems use Clarity/Relevance/Originality/Feasibility/Significance; Methods use Clarity/Validity/Rigorousness/Innovativeness/Generalizability; Experiments use Clarity/Validity/Robustness/Feasibility/Reproducibility.",
                        "uuids": [
                            "e4568.0"
                        ]
                    },
                    {
                        "text": "MaSTeA evaluation on undergraduate-level materials science questions may not reflect advanced hypothesis-generation evaluation needs in more complex domains.",
                        "uuids": [
                            "e4442.4"
                        ]
                    },
                    {
                        "text": "ACCELMAT evaluation framework uses dual-component scoring (Closeness + Quality with six expert-derived criteria) to handle complexity of materials discovery domain.",
                        "uuids": [
                            "e4452.0"
                        ]
                    },
                    {
                        "text": "Multi-dimensional SciHorizon framework evaluates across Understanding, Reasoning, Knowledge (with subdimensions), and Values to capture complexity of scientific domains.",
                        "uuids": [
                            "e4446.1",
                            "e4446.5",
                            "e4446.7"
                        ]
                    },
                    {
                        "text": "IBE-Eval uses multiple explicit features (consistency, parsimony, coherence, uncertainty) combined with learned linear model to handle complexity of causal explanation evaluation.",
                        "uuids": [
                            "e4466.0"
                        ]
                    },
                    {
                        "text": "Surface metrics (BLEU/ROUGE) are inadequate for hypothesis evaluation because they fail to capture semantic depth, novelty, or testability in complex scientific domains.",
                        "uuids": [
                            "e4612.3"
                        ]
                    },
                    {
                        "text": "Automated similarity metrics reward surface-level similarity and templates, fail to capture novelty or technical depth, and penalize longer detailed outputs in complex domains.",
                        "uuids": [
                            "e4433.3"
                        ]
                    },
                    {
                        "text": "ChemQA multimodal benchmark shows models perform poorly on image-only inputs in chemistry domain, indicating complexity of multimodal scientific reasoning.",
                        "uuids": [
                            "e4470.8"
                        ]
                    },
                    {
                        "text": "Consensus/alignment checks are needed in clinical domains to evaluate whether outputs align with scientific/clinical consensus or expert guidelines.",
                        "uuids": [
                            "e4480.12"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "While the relationship between task complexity and evaluation difficulty is recognized in ML, this explicit formulation as a scaling law for scientific theory evaluation with domain-specific thresholds is novel.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Process Stage Matching Law",
                "if": [
                    {
                        "subject": "scientific_process",
                        "relation": "is_at_stage",
                        "object": "stage_S"
                    },
                    {
                        "subject": "evaluation_method",
                        "relation": "is_designed_for",
                        "object": "stage_T"
                    },
                    {
                        "subject": "stage_S",
                        "relation": "differs_from",
                        "object": "stage_T"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_method",
                        "relation": "produces",
                        "object": "suboptimal_results"
                    },
                    {
                        "subject": "evaluation_method",
                        "relation": "may_mislead",
                        "object": "decision_making"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Formative evaluation focused on generating empirically grounded interpretations to improve artifacts during development; restricts evaluative criteria to relevance, alignment, effectiveness, and unintended effects rather than summative assessment.",
                        "uuids": [
                            "e4424.5"
                        ]
                    },
                    {
                        "text": "Metric-Based Screening automatically evaluates candidates for large-scale screening and ranking, often combined with downstream human review for high-stakes validation (hybrid approach for different stages).",
                        "uuids": [
                            "e4450.2"
                        ]
                    },
                    {
                        "text": "Entropy-guided refinement selection applies different strategies (Deepening, Counterfactual, Hybridization) at different stages based on uncertainty thresholds.",
                        "uuids": [
                            "e4419.5"
                        ]
                    },
                    {
                        "text": "SCIMON novelty-driven evaluation uses iterative revision until novelty threshold is reached, appropriate for generation stage but not validation stage.",
                        "uuids": [
                            "e4444.1"
                        ]
                    },
                    {
                        "text": "Multi-agent framework with Analyst/Engineer/Scientist/Critic roles shows iterative refinement improves hypothesis quality through stage-appropriate evaluation.",
                        "uuids": [
                            "e4443.6"
                        ]
                    },
                    {
                        "text": "CycleResearcher/CycleReviewer iterative preference-training framework alternates generation and review phases with stage-appropriate feedback.",
                        "uuids": [
                            "e4479.9"
                        ]
                    },
                    {
                        "text": "Self-critique and chain-of-thought prompting techniques induce iterative refinement appropriate for generation/refinement stages but not final validation.",
                        "uuids": [
                            "e4440.4"
                        ]
                    },
                    {
                        "text": "POPPER automated validation framework applies sequential falsification strategies appropriate for validation stage, achieving human-level accuracy at higher speed.",
                        "uuids": [
                            "e4450.3"
                        ]
                    },
                    {
                        "text": "Simulation-based validation using digital twins is appropriate for pre-testing stage before real-world experiments.",
                        "uuids": [
                            "e4605.6"
                        ]
                    },
                    {
                        "text": "Human-in-the-loop protocols combine model suggestions with expert judgment, appropriate for refinement and validation stages.",
                        "uuids": [
                            "e4612.12"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "The need to match evaluation to process stage is recognized in design science (formative vs summative evaluation), but this explicit formulation as a law for scientific theory evaluation across generation/validation/refinement stages is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Scriven (1967) The Methodology of Evaluation [formative vs summative evaluation distinction]",
                        "Venable et al. (2016) FEDS: A Framework for Evaluation in Design Science Research [evaluation strategy framework]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Use Case Optimization Law",
                "if": [
                    {
                        "subject": "evaluation_use_case",
                        "relation": "is",
                        "object": "screening"
                    },
                    {
                        "subject": "evaluation_method",
                        "relation": "optimizes_for",
                        "object": "precision"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_method",
                        "relation": "is_suboptimal_for",
                        "object": "screening"
                    },
                    {
                        "subject": "screening_task",
                        "relation": "requires_optimization_for",
                        "object": "recall"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Metric-Based Screening uses predefined objective metrics for large-scale screening and ranking to triage candidate theories, optimizing for recall to avoid missing promising candidates.",
                        "uuids": [
                            "e4450.2"
                        ]
                    },
                    {
                        "text": "Hybrid evaluation combining automated screening to top-K candidates followed by human evaluation of top-K achieves high coverage (recall), high validity (precision), and remains cost-effective.",
                        "uuids": [
                            "e4450.2"
                        ]
                    },
                    {
                        "text": "Pass@N reports whether at least one of N sampled outputs passes evaluation, intended to approximate upper-bound quality (optimizes for recall of any good output).",
                        "uuids": [
                            "e4562.2"
                        ]
                    },
                    {
                        "text": "Oracle inference retrospectively selects best hypothesis to measure maximum predictive potential of generated pool (upper-bound recall measure).",
                        "uuids": [
                            "e4464.2"
                        ]
                    },
                    {
                        "text": "Inspiration Retrieval using Hit Ratio with stratified negative candidates measures recall of ground-truth inspirations at different retention rates (top 4% vs top 20%).",
                        "uuids": [
                            "e4460.1"
                        ]
                    },
                    {
                        "text": "Reference Recall measures coverage of central literature as proportion of GT citations retrieved, appropriate for screening comprehensiveness.",
                        "uuids": [
                            "e4428.0"
                        ]
                    },
                    {
                        "text": "Recall/Precision retrieval metrics measure whether correct items are present and ranked, with different optimization for screening vs selection.",
                        "uuids": [
                            "e4575.11"
                        ]
                    },
                    {
                        "text": "WRecall (Weighted Recall) approximates recall of generated rules when ground-truth unavailable, used for screening rule quality.",
                        "uuids": [
                            "e4603.1"
                        ]
                    },
                    {
                        "text": "Ranking/Average Rank Ratio quantifies how highly promising hypotheses are ranked, balancing recall (finding good candidates) with precision (ranking them highly).",
                        "uuids": [
                            "e4418.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "The precision-recall trade-off is well-known in information retrieval, but this specific formulation for evaluation method selection in scientific theory assessment with explicit use-case matching is a novel application.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Manning et al. (2008) Introduction to Information Retrieval [precision-recall trade-offs in IR]",
                        "Davis & Goadrich (2006) The Relationship Between Precision-Recall and ROC Curves [precision-recall optimization]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Hybrid Evaluation Superiority Law",
                "if": [
                    {
                        "subject": "evaluation_approach",
                        "relation": "combines",
                        "object": "automated_and_human_methods"
                    },
                    {
                        "subject": "automated_methods",
                        "relation": "used_for",
                        "object": "large_scale_screening"
                    },
                    {
                        "subject": "human_methods",
                        "relation": "used_for",
                        "object": "high_stakes_validation"
                    }
                ],
                "then": [
                    {
                        "subject": "hybrid_approach",
                        "relation": "outperforms",
                        "object": "purely_automated_methods"
                    },
                    {
                        "subject": "hybrid_approach",
                        "relation": "outperforms",
                        "object": "purely_human_methods"
                    },
                    {
                        "subject": "hybrid_approach",
                        "relation": "achieves",
                        "object": "optimal_cost_effectiveness"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Metric-Based Screening integrates automated metrics with human oversight for interpretation and experimental prioritization, combining scalability with validity.",
                        "uuids": [
                            "e4450.2"
                        ]
                    },
                    {
                        "text": "Human expert evaluation used as independent ground truth for validating automated metrics (ON and LLM-review), with hybrid analysis combining automated metrics and human ratings.",
                        "uuids": [
                            "e4439.3"
                        ]
                    },
                    {
                        "text": "Multi-Rater Reliability Protocols use explicit scoring rubrics with multiple annotators, combining structured evaluation with human judgment.",
                        "uuids": [
                            "e4612.2"
                        ]
                    },
                    {
                        "text": "Expert Panel Review with Inter-rater Concordance (Cohen's κ=0.82) combines independent expert ratings with consensus resolution for high-stakes validation.",
                        "uuids": [
                            "e4410.2"
                        ]
                    },
                    {
                        "text": "Meta-evaluation via correlation to human judgments validates automated metrics by measuring statistical correlations with human ratings.",
                        "uuids": [
                            "e4610.1"
                        ]
                    },
                    {
                        "text": "Human expert Likert-scale scoring and pairwise comparison used as gold-standard reference for alignment and validation of automated methods.",
                        "uuids": [
                            "e4568.0"
                        ]
                    },
                    {
                        "text": "YESciEval alignment method combines supervised fine-tuning with RL to train evaluators that balance automated scalability with human-aligned judgments.",
                        "uuids": [
                            "e4451.3"
                        ]
                    },
                    {
                        "text": "ACCELMAT evaluation framework uses automated LLM scoring validated by independent human expert evaluations showing alignment.",
                        "uuids": [
                            "e4452.0"
                        ]
                    },
                    {
                        "text": "TruthHypo benchmark uses automated metrics with human expert annotation in open-ended tasks (80% agreement) for validation.",
                        "uuids": [
                            "e4461.0"
                        ]
                    },
                    {
                        "text": "ResearchBench expert validation pipeline where domain experts assess automated extraction accuracy (91.9% with major issues only).",
                        "uuids": [
                            "e4460.4"
                        ]
                    },
                    {
                        "text": "Human-in-the-loop protocols combine model suggestions with expert judgment to produce higher-quality proposals than either alone.",
                        "uuids": [
                            "e4612.12"
                        ]
                    },
                    {
                        "text": "Consensus/alignment checks combine automated mapping with clinician adjudication for clinical domain evaluation.",
                        "uuids": [
                            "e4480.12"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "While human-in-the-loop and hybrid evaluation are recognized practices, this explicit formulation as a superiority law with cost-effectiveness optimization for scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Amershi et al. (2014) Power to the People: The Role of Humans in Interactive Machine Learning [human-in-the-loop ML]",
                        "Mosqueira-Rey et al. (2023) Human-in-the-loop machine learning: A state of the art [hybrid approaches]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an evaluation method designed for hypothesis generation (emphasizing diversity and novelty) is applied to hypothesis validation (requiring accuracy and groundedness), it will produce rankings that poorly predict experimental success, with correlation to experimental outcomes dropping by at least 30% compared to validation-appropriate methods.",
        "If a simple single-metric evaluation (e.g., BLEU or ROUGE) is applied to a highly complex domain (e.g., multi-scale biological systems), it will show correlation with expert judgments at least 0.3 lower (Spearman ρ) than in simpler domains (e.g., single-variable physics problems).",
        "If a screening evaluation optimized for high recall (e.g., Pass@N with large N) is used for final selection without subsequent precision-focused filtering, it will include at least 2-3x more false positives compared to a selection-optimized evaluation with appropriate precision-recall balance.",
        "If hybrid evaluation (automated screening + human validation of top-K) is compared to purely automated or purely human evaluation on the same budget, the hybrid approach will achieve at least 20% better cost-effectiveness (measured as valid hypotheses identified per dollar spent).",
        "If evaluation methods are matched to domain complexity (multi-dimensional methods for complex domains, simple metrics for simple domains), the average correlation with expert judgments will be at least 0.2 higher (Spearman ρ) than mismatched methods."
    ],
    "new_predictions_unknown": [
        "If evaluation methods are adaptively selected based on automatically detected context features (ground truth availability, domain complexity, process stage, use case), they might substantially outperform fixed-method approaches by 30-50%, but the optimal selection algorithm and feature weights are unknown and may vary significantly by domain.",
        "If the same evaluation method is applied across domains of varying complexity, the validity degradation rate as a function of complexity might follow a power law, exponential decay, or threshold-based step function - the functional form is unknown and could have major implications for when to switch methods.",
        "If process stage transitions are gradual rather than discrete (e.g., moving from generation to validation over multiple iterations), the optimal timing and method for switching evaluation approaches might require continuous adaptation, but whether smooth interpolation between methods or discrete switching is better is unknown.",
        "If cost-effectiveness is explicitly optimized in hybrid evaluation design (varying the automated/human ratio), there might be a universal optimal ratio (e.g., 80/20 rule) or it might vary dramatically by domain and stage - the existence and nature of such optima is unknown.",
        "If evaluation methods are calibrated using small amounts of ground truth and then applied to larger datasets without ground truth, the calibration might transfer well (enabling efficient scaling) or might fail catastrophically due to distribution shift - the robustness of such transfer is unknown."
    ],
    "negative_experiments": [
        "Finding a single evaluation method that achieves correlation &gt;0.9 with expert judgments across all contexts (ground truth availability, domain complexity, process stages, use cases) would challenge the entire theory and suggest context-independence is achievable.",
        "Demonstrating that evaluation method validity is independent of domain complexity (i.e., simple metrics work as well in complex domains as in simple domains) would directly contradict the Domain Complexity Scaling Law.",
        "Showing that the same evaluation method works equally well for screening (requiring high recall) and selection (requiring high precision) with no performance trade-off would challenge the Use Case Optimization Law.",
        "Finding that purely automated evaluation consistently outperforms hybrid approaches across multiple domains and stages would challenge the Hybrid Evaluation Superiority Law.",
        "Demonstrating that evaluation methods designed for one process stage (e.g., generation) work equally well at other stages (e.g., validation) without adaptation would contradict the Process Stage Matching Law.",
        "Showing that evaluation validity remains constant as ground truth quality degrades would challenge the Ground Truth Availability Law's emphasis on ground truth quality."
    ],
    "unaccounted_for": [
        {
            "text": "The theory doesn't specify how to quantitatively measure domain complexity - whether it should be based on number of variables, interaction complexity, required background knowledge, or other factors.",
            "uuids": []
        },
        {
            "text": "Optimal methods for contexts with partial or noisy ground truth (between full ground truth and no ground truth) are not specified, though this is a common real-world scenario.",
            "uuids": []
        },
        {
            "text": "The theory doesn't address how to handle transitions between process stages - whether to switch methods abruptly or gradually interpolate between stage-appropriate methods.",
            "uuids": []
        },
        {
            "text": "The role of evaluator calibration and how calibration requirements vary by context is not explicitly addressed, though calibration is mentioned in several pieces of evidence.",
            "uuids": [
                "e4560.3",
                "e4549.1"
            ]
        },
        {
            "text": "Temporal aspects of evaluation (e.g., how to evaluate truly novel theories vs. incremental advances) are not fully captured by the current laws.",
            "uuids": []
        },
        {
            "text": "The theory doesn't specify how to handle multimodal evaluation (combining different types of evidence like text, images, code, experimental data) which is increasingly important.",
            "uuids": [
                "e4470.8"
            ]
        },
        {
            "text": "Cross-domain transfer of evaluation methods and when such transfer is appropriate is not addressed.",
            "uuids": [
                "e4447.0"
            ]
        },
        {
            "text": "The theory doesn't specify how evaluation requirements change with the intended audience (expert vs. non-expert, domain-specific vs. general).",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some evidence suggests that simple automated metrics can work well across diverse contexts: BERTScore shows reasonable performance across multiple domains and tasks, suggesting that well-designed general-purpose metrics may be more context-independent than the theory suggests.",
            "uuids": [
                "e4587.1"
            ]
        },
        {
            "text": "RND (Relative Neighbor Density) achieves domain-invariant score distributions across computer science and biomedical domains (AUROC 0.820 NeurIPS, 0.765 Nature Medicine, 0.795 Mixed), suggesting some evaluation methods may be more universally applicable than the theory predicts.",
            "uuids": [
                "e4465.0"
            ]
        },
        {
            "text": "G-EVAL substantially outperforms prior metrics across multiple benchmarks and tasks (SummEval, Topical-Chat, QAGS) with consistent improvements, suggesting that sufficiently powerful LLM-based evaluators may work well across contexts without extensive customization.",
            "uuids": [
                "e4571.0"
            ]
        },
        {
            "text": "Some studies show that purely automated evaluation can match or exceed human performance in specific contexts (e.g., ChatGPT outperforms crowd workers with 30x lower cost), challenging the universal superiority of hybrid approaches.",
            "uuids": [
                "e4519.4"
            ]
        }
    ],
    "special_cases": [
        "In domains with perfect ground truth and low complexity (e.g., simple mathematical problems with deterministic solutions), simple supervised methods may be universally optimal and context-matching may be unnecessary.",
        "For purely exploratory research with no immediate validation requirements (e.g., early-stage brainstorming), evaluation methods may need to optimize exclusively for diversity and novelty rather than accuracy or groundedness.",
        "In time-critical scenarios (e.g., emergency response or real-time decision support), context-matching may be sacrificed for speed, accepting lower validity in exchange for faster results.",
        "For domains with rapidly evolving knowledge (e.g., emerging diseases, breaking news), temporal aspects may dominate other context factors, requiring evaluation methods that can handle knowledge freshness.",
        "In resource-constrained settings (e.g., low-resource languages, developing regions), cost-effectiveness may override other considerations, requiring simpler methods even when more sophisticated approaches would be more valid.",
        "For safety-critical applications (e.g., medical diagnosis, autonomous systems), precision may be prioritized over recall regardless of the nominal use case, requiring conservative evaluation methods.",
        "In domains with strong theoretical foundations (e.g., physics, mathematics), formal verification methods may be preferred over empirical evaluation regardless of other context factors."
    ],
    "existing_theory": {
        "classification_explanation": "This theory synthesizes observations about context-dependent evaluation validity into explicit laws. While context-dependence of evaluation is recognized in educational assessment and design science (formative vs summative evaluation), and precision-recall trade-offs are known in information retrieval, the explicit formulation of laws governing when evaluation methods succeed or fail in scientific theory assessment, particularly the integration of ground truth quality, domain complexity, process stage, and use case factors, represents a novel synthesis and formalization.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Scriven (1967) The Methodology of Evaluation [context-dependent evaluation, formative vs summative]",
            "Shadish et al. (2002) Experimental and Quasi-Experimental Designs for Generalized Causal Inference [validity types and contexts]",
            "Messick (1989) Validity [unified validity theory emphasizing context]",
            "Manning et al. (2008) Introduction to Information Retrieval [precision-recall trade-offs]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
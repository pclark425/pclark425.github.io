<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Inductive Bias and Modality Adaptation Theory (General Formulation) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1262</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1262</p>
                <p><strong>Name:</strong> Structural Inductive Bias and Modality Adaptation Theory (General Formulation)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the optimal conversion of graph-structured data into text for language model training is governed by the alignment between the inductive biases of the language model and the structural properties of the graph. Representations that preserve salient graph topology, semantics, and compositionality, while adapting to the sequential and hierarchical biases of language models, yield superior downstream performance. The process of modality adaptation—transforming graph data into text—should be guided by explicit modeling of both the graph's structure and the language model's representational preferences.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Alignment of Inductive Biases Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; preserves &#8594; salient graph topology and semantics<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph-to-text representation &#8594; adapts &#8594; sequential and hierarchical biases of language models</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; achieves &#8594; higher performance on graph-structured tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that linearizations of graphs that preserve edge and node orderings relevant to downstream tasks improve model performance (e.g., AMR-to-text, code summarization). </li>
    <li>Language models exhibit strong sequential and hierarchical inductive biases, as evidenced by their performance on natural language and tree-structured data. </li>
    <li>Graph-to-text conversions that ignore graph structure (e.g., random walks, bag-of-edges) result in lower performance on tasks requiring structural reasoning. </li>
    <li>Transformer-based models, which are the dominant architecture for language models, are known to encode hierarchical and sequential information more efficiently than arbitrary graph structure. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing work on graph linearization and inductive bias, this law formalizes the alignment principle as the central mechanism, which is not explicitly stated in prior literature.</p>            <p><strong>What Already Exists:</strong> Prior work recognizes that preserving graph structure in text improves downstream performance, and that language models have sequential/hierarchical biases.</p>            <p><strong>What is Novel:</strong> The explicit formulation that optimal representations are those that maximize the alignment between graph structure and language model inductive biases, and that this alignment is the primary driver of performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [Discusses graph-to-text and inductive bias, but not explicit alignment principle]</li>
    <li>Vaswani et al. (2017) Attention is All You Need [Transformer models' inductive biases]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in AMR-to-Text Generation [Shows structure preservation helps, but does not formalize alignment]</li>
</ul>
            <h3>Statement 1: Modality Adaptation Principle (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text conversion process &#8594; explicitly models &#8594; mapping between graph substructures and linguistic constructs</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; resulting text representation &#8594; enables &#8594; efficient learning and generalization by language models</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Mappings from subgraphs to phrases (e.g., in AMR or code summarization) improve model interpretability and generalization. </li>
    <li>Language models trained on text derived from explicit graph substructure mappings show better transfer to related tasks. </li>
    <li>Explicit substructure-to-linguistic mapping allows for compositional generalization, as seen in semantic parsing and code generation. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law generalizes and formalizes a principle that is implicit in prior work, but not previously stated as a necessary condition for efficient learning.</p>            <p><strong>What Already Exists:</strong> Existing work uses subgraph-to-phrase mappings in AMR and code summarization, but often as heuristics.</p>            <p><strong>What is Novel:</strong> The law formalizes modality adaptation as a principle: explicit modeling of substructure-to-linguistic mapping is necessary for optimal representation.</p>
            <p><strong>References:</strong> <ul>
    <li>Song et al. (2018) Graph-to-Sequence Learning using Gated Graph Neural Networks [Uses subgraph-to-sequence mapping, but not as a formal principle]</li>
    <li>Liu et al. (2015) AMR-to-text generation as a traveling salesman problem [Uses subgraph-to-phrase mapping heuristically]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a graph-to-text representation is constructed to maximize alignment with the language model's inductive biases (e.g., by preserving hierarchical structure in a way that matches the model's attention patterns), downstream performance on graph-structured tasks will improve compared to naive linearizations.</li>
                <li>Explicitly modeling the mapping from graph substructures to linguistic constructs will result in more sample-efficient learning and better generalization to unseen graph structures.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new class of language models with fundamentally different inductive biases (e.g., non-sequential, non-hierarchical) is developed, the optimal graph-to-text representation may shift away from current best practices.</li>
                <li>Highly non-local or cyclic graph structures, when mapped to text using this theory's principles, may enable language models to learn new forms of compositionality not observed in natural language.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a representation that does not preserve salient graph structure or does not adapt to language model biases outperforms one that does, the theory's central alignment principle would be called into question.</li>
                <li>If explicit substructure-to-linguistic mapping does not improve learning efficiency or generalization, the modality adaptation principle would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where language models perform well on graph-structured tasks despite poor alignment between representation and model biases (e.g., via massive scale or memorization). </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and formalizes principles that are implicit in prior work, but not previously stated as general, predictive laws for graph-to-text representation.</p>
            <p><strong>References:</strong> <ul>
    <li>Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [structure preservation, inductive bias]</li>
    <li>Song et al. (2018) Graph-to-Sequence Learning using Gated Graph Neural Networks [subgraph-to-sequence mapping]</li>
    <li>Vaswani et al. (2017) Attention is All You Need [language model inductive biases]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structural Inductive Bias and Modality Adaptation Theory (General Formulation)",
    "theory_description": "This theory posits that the optimal conversion of graph-structured data into text for language model training is governed by the alignment between the inductive biases of the language model and the structural properties of the graph. Representations that preserve salient graph topology, semantics, and compositionality, while adapting to the sequential and hierarchical biases of language models, yield superior downstream performance. The process of modality adaptation—transforming graph data into text—should be guided by explicit modeling of both the graph's structure and the language model's representational preferences.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Alignment of Inductive Biases Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "preserves",
                        "object": "salient graph topology and semantics"
                    },
                    {
                        "subject": "graph-to-text representation",
                        "relation": "adapts",
                        "object": "sequential and hierarchical biases of language models"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "achieves",
                        "object": "higher performance on graph-structured tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that linearizations of graphs that preserve edge and node orderings relevant to downstream tasks improve model performance (e.g., AMR-to-text, code summarization).",
                        "uuids": []
                    },
                    {
                        "text": "Language models exhibit strong sequential and hierarchical inductive biases, as evidenced by their performance on natural language and tree-structured data.",
                        "uuids": []
                    },
                    {
                        "text": "Graph-to-text conversions that ignore graph structure (e.g., random walks, bag-of-edges) result in lower performance on tasks requiring structural reasoning.",
                        "uuids": []
                    },
                    {
                        "text": "Transformer-based models, which are the dominant architecture for language models, are known to encode hierarchical and sequential information more efficiently than arbitrary graph structure.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work recognizes that preserving graph structure in text improves downstream performance, and that language models have sequential/hierarchical biases.",
                    "what_is_novel": "The explicit formulation that optimal representations are those that maximize the alignment between graph structure and language model inductive biases, and that this alignment is the primary driver of performance.",
                    "classification_explanation": "While related to existing work on graph linearization and inductive bias, this law formalizes the alignment principle as the central mechanism, which is not explicitly stated in prior literature.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [Discusses graph-to-text and inductive bias, but not explicit alignment principle]",
                        "Vaswani et al. (2017) Attention is All You Need [Transformer models' inductive biases]",
                        "Ribeiro et al. (2020) Structural Encoding in AMR-to-Text Generation [Shows structure preservation helps, but does not formalize alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Modality Adaptation Principle",
                "if": [
                    {
                        "subject": "graph-to-text conversion process",
                        "relation": "explicitly models",
                        "object": "mapping between graph substructures and linguistic constructs"
                    }
                ],
                "then": [
                    {
                        "subject": "resulting text representation",
                        "relation": "enables",
                        "object": "efficient learning and generalization by language models"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Mappings from subgraphs to phrases (e.g., in AMR or code summarization) improve model interpretability and generalization.",
                        "uuids": []
                    },
                    {
                        "text": "Language models trained on text derived from explicit graph substructure mappings show better transfer to related tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Explicit substructure-to-linguistic mapping allows for compositional generalization, as seen in semantic parsing and code generation.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Existing work uses subgraph-to-phrase mappings in AMR and code summarization, but often as heuristics.",
                    "what_is_novel": "The law formalizes modality adaptation as a principle: explicit modeling of substructure-to-linguistic mapping is necessary for optimal representation.",
                    "classification_explanation": "The law generalizes and formalizes a principle that is implicit in prior work, but not previously stated as a necessary condition for efficient learning.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Song et al. (2018) Graph-to-Sequence Learning using Gated Graph Neural Networks [Uses subgraph-to-sequence mapping, but not as a formal principle]",
                        "Liu et al. (2015) AMR-to-text generation as a traveling salesman problem [Uses subgraph-to-phrase mapping heuristically]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a graph-to-text representation is constructed to maximize alignment with the language model's inductive biases (e.g., by preserving hierarchical structure in a way that matches the model's attention patterns), downstream performance on graph-structured tasks will improve compared to naive linearizations.",
        "Explicitly modeling the mapping from graph substructures to linguistic constructs will result in more sample-efficient learning and better generalization to unseen graph structures."
    ],
    "new_predictions_unknown": [
        "If a new class of language models with fundamentally different inductive biases (e.g., non-sequential, non-hierarchical) is developed, the optimal graph-to-text representation may shift away from current best practices.",
        "Highly non-local or cyclic graph structures, when mapped to text using this theory's principles, may enable language models to learn new forms of compositionality not observed in natural language."
    ],
    "negative_experiments": [
        "If a representation that does not preserve salient graph structure or does not adapt to language model biases outperforms one that does, the theory's central alignment principle would be called into question.",
        "If explicit substructure-to-linguistic mapping does not improve learning efficiency or generalization, the modality adaptation principle would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where language models perform well on graph-structured tasks despite poor alignment between representation and model biases (e.g., via massive scale or memorization).",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some recent large language models show strong performance on graph-structured tasks even with naive or lossy graph-to-text conversions, possibly due to scale effects.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with extremely high degree or non-local dependencies may require additional adaptation mechanisms beyond those described.",
        "Language models with explicit graph inductive biases (e.g., Graph Transformers) may not benefit from the same representations as standard sequential models."
    ],
    "existing_theory": {
        "what_already_exists": "Prior work recognizes the importance of structure preservation and inductive bias in graph-to-text conversion, but does not formalize the alignment and modality adaptation principles as central, predictive laws.",
        "what_is_novel": "The explicit, formal statement that optimal representations are those that maximize alignment between graph structure and language model inductive biases, and that modality adaptation via explicit substructure-to-linguistic mapping is necessary for efficient learning.",
        "classification_explanation": "The theory synthesizes and formalizes principles that are implicit in prior work, but not previously stated as general, predictive laws for graph-to-text representation.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [structure preservation, inductive bias]",
            "Song et al. (2018) Graph-to-Sequence Learning using Gated Graph Neural Networks [subgraph-to-sequence mapping]",
            "Vaswani et al. (2017) Attention is All You Need [language model inductive biases]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-612",
    "original_theory_name": "Structural Inductive Bias and Modality Adaptation Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Structural Inductive Bias and Modality Adaptation Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
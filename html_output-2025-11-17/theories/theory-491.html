<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scaling and Architectural Bottleneck Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-491</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-491</p>
                <p><strong>Name:</strong> Scaling and Architectural Bottleneck Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that increasing language model size alone is insufficient to achieve robust, multi-step logical reasoning. Instead, architectural constraints, modularization, and explicit reasoning mechanisms (e.g., stepwise selection-inference, value-guided search, neuro-symbolic integration) are necessary to overcome scaling bottlenecks. While larger models show some improvement, the scaling curve for logic tasks is much flatter than for other NLP tasks, and performance plateaus or degrades with depth or complexity unless architectural or procedural interventions are introduced. This theory further posits that modular or neuro-symbolic systems, which separate selection, inference, and verification, or which integrate symbolic solvers, can achieve higher accuracy and faithfulness than monolithic, end-to-end LMs, regardless of scale.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Scaling Alone Yields Diminishing Returns for Logical Reasoning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; increases &#8594; parameter count (scale) without architectural or procedural changes</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; achieves &#8594; only modest or plateauing gains on strict logical reasoning tasks, especially at higher reasoning depths</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Gopher scaling analysis shows that performance scaling with model size on logic-based tasks is significantly worse than for other NLP tasks; even the largest 280B model performed only 13.6% above chance on logic tasks. <a href="../results/extraction-result-3503.html#e3503.5" class="evidence-link">[e3503.5]</a> <a href="../results/extraction-result-3503.html#e3503.3" class="evidence-link">[e3503.3]</a> </li>
    <li>Multi-LogiEval: average accuracy falls from ~68% at depth-1 to ~43% at depth-5 for classical and non-classical logic, with large models showing similar depth-related degradation. <a href="../results/extraction-result-3430.html#e3430.7" class="evidence-link">[e3430.7]</a> <a href="../results/extraction-result-3430.html#e3430.2" class="evidence-link">[e3430.2]</a> <a href="../results/extraction-result-3430.html#e3430.3" class="evidence-link">[e3430.3]</a> <a href="../results/extraction-result-3430.html#e3430.4" class="evidence-link">[e3430.4]</a> <a href="../results/extraction-result-3430.html#e3430.5" class="evidence-link">[e3430.5]</a> </li>
    <li>Yi-34B, Orca-13B, and Llama-2-7B show only incremental improvements with scale, and sometimes smaller models (Mistral-7B) outperform larger ones at high depth. <a href="../results/extraction-result-3412.html#e3412.5" class="evidence-link">[e3412.5]</a> <a href="../results/extraction-result-3430.html#e3430.3" class="evidence-link">[e3430.3]</a> <a href="../results/extraction-result-3430.html#e3430.4" class="evidence-link">[e3430.4]</a> <a href="../results/extraction-result-3430.html#e3430.5" class="evidence-link">[e3430.5]</a> </li>
    <li>GPT-4, GPT-3.5, and ChatGPT show strong performance on some logic tasks but still exhibit depth-related performance drops and dataset sensitivity. <a href="../results/extraction-result-3426.html#e3426.5" class="evidence-link">[e3426.5]</a> <a href="../results/extraction-result-3520.html#e3520.0" class="evidence-link">[e3520.0]</a> <a href="../results/extraction-result-3520.html#e3520.3" class="evidence-link">[e3520.3]</a> <a href="../results/extraction-result-3520.html#e3520.4" class="evidence-link">[e3520.4]</a> <a href="../results/extraction-result-3520.html#e3520.5" class="evidence-link">[e3520.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Architectural and Modular Interventions Enable Robust Logical Reasoning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning system &#8594; incorporates &#8594; modular, stepwise, or neuro-symbolic components (e.g., selection-inference, value-guided search, solver integration, explicit proof generation)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; system &#8594; achieves &#8594; higher accuracy, faithfulness, and generalization on strict logical reasoning tasks, especially at higher depths</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Selection-Inference (SI) architecture with separate selection and inference LMs, value-guided beam search, and halter module achieves large gains over end-to-end baselines, especially on deep proofs and distractor-rich contexts. <a href="../results/extraction-result-3522.html#e3522.0" class="evidence-link">[e3522.0]</a> <a href="../results/extraction-result-3522.html#e3522.4" class="evidence-link">[e3522.4]</a> <a href="../results/extraction-result-3522.html#e3522.1" class="evidence-link">[e3522.1]</a> </li>
    <li>Iterative Backward Reasoning (IBR) and PRoVeR (proof-generating models) outperform at-once baselines and provide interpretable proofs, with larger gains at higher proof depths. <a href="../results/extraction-result-3539.html#e3539.0" class="evidence-link">[e3539.0]</a> <a href="../results/extraction-result-3525.html#e3525.0" class="evidence-link">[e3525.0]</a> </li>
    <li>Neuro-symbolic and solver-augmented systems (e.g., Logic-LM, DetermLR, LOGIPT, PAL, DeClarative+SymPy) achieve higher accuracy and faithfulness than monolithic LMs, especially on FOLIO, ProofWriter, and math word problems. <a href="../results/extraction-result-3454.html#e3454.8" class="evidence-link">[e3454.8]</a> <a href="../results/extraction-result-3454.html#e3454.10" class="evidence-link">[e3454.10]</a> <a href="../results/extraction-result-3439.html#e3439.0" class="evidence-link">[e3439.0]</a> <a href="../results/extraction-result-3541.html#e3541.1" class="evidence-link">[e3541.1]</a> <a href="../results/extraction-result-3521.html#e3521.2" class="evidence-link">[e3521.2]</a> <a href="../results/extraction-result-3521.html#e3521.1" class="evidence-link">[e3521.1]</a> <a href="../results/extraction-result-3542.html#e3542.3" class="evidence-link">[e3542.3]</a> <a href="../results/extraction-result-3432.html#e3432.0" class="evidence-link">[e3432.0]</a> </li>
    <li>Cumulative Reasoning (CR), Tree-of-Thoughts (ToT), and DetermLR frameworks outperform standard CoT and direct prompting, especially on deep or complex reasoning tasks. <a href="../results/extraction-result-3545.html#e3545.1" class="evidence-link">[e3545.1]</a> <a href="../results/extraction-result-3545.html#e3545.3" class="evidence-link">[e3545.3]</a> <a href="../results/extraction-result-3440.html#e3440.10" class="evidence-link">[e3440.10]</a> <a href="../results/extraction-result-3440.html#e3440.2" class="evidence-link">[e3440.2]</a> </li>
    <li>GFaiR (resolution-refutation-based) and FaiRR (stepwise forward-chaining) architectures outperform standard LMs on hard/debiased RuleTaker and NLSAT tasks. <a href="../results/extraction-result-3435.html#e3435.0" class="evidence-link">[e3435.0]</a> <a href="../results/extraction-result-3435.html#e3435.5" class="evidence-link">[e3435.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Monolithic End-to-End LMs Plateau or Fail on Deep or Distractor-Rich Logical Tasks (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is monolithic and end-to-end &#8594; without modularization or explicit reasoning modules</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; exhibits &#8594; performance degradation with increasing reasoning depth, distractors, or logical complexity</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ProofWriter, EntailmentBankQA, and CLUTRR: end-to-end LMs show sharp performance drops at higher proof depths or with distractors, while modular or stepwise systems maintain higher accuracy. <a href="../results/extraction-result-3522.html#e3522.0" class="evidence-link">[e3522.0]</a> <a href="../results/extraction-result-3522.html#e3522.4" class="evidence-link">[e3522.4]</a> <a href="../results/extraction-result-3511.html#e3511.5" class="evidence-link">[e3511.5]</a> <a href="../results/extraction-result-3511.html#e3511.3" class="evidence-link">[e3511.3]</a> </li>
    <li>RuleTakers and RoBERTa-based models achieve high accuracy on synthetic rulebase tasks but fail to generalize to harder, debiased, or multi-step settings (Hard RuleTaker, RobustLR). <a href="../results/extraction-result-3525.html#e3525.1" class="evidence-link">[e3525.1]</a> <a href="../results/extraction-result-3525.html#e3525.2" class="evidence-link">[e3525.2]</a> <a href="../results/extraction-result-3529.html#e3529.1" class="evidence-link">[e3529.1]</a> <a href="../results/extraction-result-3529.html#e3529.7" class="evidence-link">[e3529.7]</a> </li>
    <li>Gopher-280B and other large LMs show only modest gains on logic tasks compared to non-logic NLP tasks, and plateau at high scale. <a href="../results/extraction-result-3503.html#e3503.5" class="evidence-link">[e3503.5]</a> <a href="../results/extraction-result-3503.html#e3503.3" class="evidence-link">[e3503.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new, larger LM is trained without architectural or modular interventions, it will show only modest improvement on deep logical reasoning tasks compared to smaller models.</li>
                <li>If a modular or neuro-symbolic architecture is applied to a new logic benchmark, it will outperform monolithic LMs of similar or even larger scale.</li>
                <li>If a value-guided search or explicit proof generation module is added to an existing LM, performance on multi-step or distractor-rich logical tasks will increase, especially at higher depths.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a hybrid system combines large-scale LMs with modular neuro-symbolic components, it is unknown whether the gains will be additive, synergistic, or subject to diminishing returns.</li>
                <li>If a monolithic LM is trained on massive, logic-rich corpora, it is unclear whether it can match modular systems on deep logical reasoning without explicit architectural changes.</li>
                <li>If modular architectures are applied to tasks with ambiguous or fuzzy logic (e.g., commonsense reasoning), the effect on performance is uncertain.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a monolithic, end-to-end LM of sufficient scale (e.g., >1T parameters) achieves robust, high accuracy on deep logical reasoning tasks without modularization, the theory would be challenged.</li>
                <li>If modular or neuro-symbolic systems fail to outperform monolithic LMs on new logic benchmarks, the theory's claims about architectural bottlenecks would be weakened.</li>
                <li>If scaling alone (without architectural changes) yields dramatic improvements on multi-step logic tasks, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some small models (e.g., Mistral-7B) outperform larger models on deep logical reasoning, possibly due to training data, objective, or architecture not captured by this theory. <a href="../results/extraction-result-3430.html#e3430.3" class="evidence-link">[e3430.3]</a> </li>
    <li>Certain discriminative models (e.g., RoBERTa, T5-Large) achieve high accuracy on synthetic or shallow logic tasks without modularization, suggesting that task structure and data complexity also play a role. <a href="../results/extraction-result-3525.html#e3525.1" class="evidence-link">[e3525.1]</a> <a href="../results/extraction-result-3525.html#e3525.2" class="evidence-link">[e3525.2]</a> <a href="../results/extraction-result-3529.html#e3529.1" class="evidence-link">[e3529.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Rae et al. (2022) Scaling Language Models: Methods, Analysis & Insights from Training Gopher [Scaling law analysis for logic tasks]</li>
    <li>Tafjord et al. (2021) ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language [Stepwise proof generation]</li>
    <li>Jiang et al. (2023) Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models [Scaling and depth analysis]</li>
    <li>Clark et al. (2020) Transformers as Soft Reasoners over Language [RuleTaker baseline and limitations]</li>
    <li>Zhou et al. (2023) Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning [AMR-LDA and robustness]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Scaling and Architectural Bottleneck Theory",
    "theory_description": "This theory asserts that increasing language model size alone is insufficient to achieve robust, multi-step logical reasoning. Instead, architectural constraints, modularization, and explicit reasoning mechanisms (e.g., stepwise selection-inference, value-guided search, neuro-symbolic integration) are necessary to overcome scaling bottlenecks. While larger models show some improvement, the scaling curve for logic tasks is much flatter than for other NLP tasks, and performance plateaus or degrades with depth or complexity unless architectural or procedural interventions are introduced. This theory further posits that modular or neuro-symbolic systems, which separate selection, inference, and verification, or which integrate symbolic solvers, can achieve higher accuracy and faithfulness than monolithic, end-to-end LMs, regardless of scale.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Scaling Alone Yields Diminishing Returns for Logical Reasoning",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "increases",
                        "object": "parameter count (scale) without architectural or procedural changes"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "achieves",
                        "object": "only modest or plateauing gains on strict logical reasoning tasks, especially at higher reasoning depths"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Gopher scaling analysis shows that performance scaling with model size on logic-based tasks is significantly worse than for other NLP tasks; even the largest 280B model performed only 13.6% above chance on logic tasks.",
                        "uuids": [
                            "e3503.5",
                            "e3503.3"
                        ]
                    },
                    {
                        "text": "Multi-LogiEval: average accuracy falls from ~68% at depth-1 to ~43% at depth-5 for classical and non-classical logic, with large models showing similar depth-related degradation.",
                        "uuids": [
                            "e3430.7",
                            "e3430.2",
                            "e3430.3",
                            "e3430.4",
                            "e3430.5"
                        ]
                    },
                    {
                        "text": "Yi-34B, Orca-13B, and Llama-2-7B show only incremental improvements with scale, and sometimes smaller models (Mistral-7B) outperform larger ones at high depth.",
                        "uuids": [
                            "e3412.5",
                            "e3430.3",
                            "e3430.4",
                            "e3430.5"
                        ]
                    },
                    {
                        "text": "GPT-4, GPT-3.5, and ChatGPT show strong performance on some logic tasks but still exhibit depth-related performance drops and dataset sensitivity.",
                        "uuids": [
                            "e3426.5",
                            "e3520.0",
                            "e3520.3",
                            "e3520.4",
                            "e3520.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Architectural and Modular Interventions Enable Robust Logical Reasoning",
                "if": [
                    {
                        "subject": "reasoning system",
                        "relation": "incorporates",
                        "object": "modular, stepwise, or neuro-symbolic components (e.g., selection-inference, value-guided search, solver integration, explicit proof generation)"
                    }
                ],
                "then": [
                    {
                        "subject": "system",
                        "relation": "achieves",
                        "object": "higher accuracy, faithfulness, and generalization on strict logical reasoning tasks, especially at higher depths"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Selection-Inference (SI) architecture with separate selection and inference LMs, value-guided beam search, and halter module achieves large gains over end-to-end baselines, especially on deep proofs and distractor-rich contexts.",
                        "uuids": [
                            "e3522.0",
                            "e3522.4",
                            "e3522.1"
                        ]
                    },
                    {
                        "text": "Iterative Backward Reasoning (IBR) and PRoVeR (proof-generating models) outperform at-once baselines and provide interpretable proofs, with larger gains at higher proof depths.",
                        "uuids": [
                            "e3539.0",
                            "e3525.0"
                        ]
                    },
                    {
                        "text": "Neuro-symbolic and solver-augmented systems (e.g., Logic-LM, DetermLR, LOGIPT, PAL, DeClarative+SymPy) achieve higher accuracy and faithfulness than monolithic LMs, especially on FOLIO, ProofWriter, and math word problems.",
                        "uuids": [
                            "e3454.8",
                            "e3454.10",
                            "e3439.0",
                            "e3541.1",
                            "e3521.2",
                            "e3521.1",
                            "e3542.3",
                            "e3432.0"
                        ]
                    },
                    {
                        "text": "Cumulative Reasoning (CR), Tree-of-Thoughts (ToT), and DetermLR frameworks outperform standard CoT and direct prompting, especially on deep or complex reasoning tasks.",
                        "uuids": [
                            "e3545.1",
                            "e3545.3",
                            "e3440.10",
                            "e3440.2"
                        ]
                    },
                    {
                        "text": "GFaiR (resolution-refutation-based) and FaiRR (stepwise forward-chaining) architectures outperform standard LMs on hard/debiased RuleTaker and NLSAT tasks.",
                        "uuids": [
                            "e3435.0",
                            "e3435.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Monolithic End-to-End LMs Plateau or Fail on Deep or Distractor-Rich Logical Tasks",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is monolithic and end-to-end",
                        "object": "without modularization or explicit reasoning modules"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "exhibits",
                        "object": "performance degradation with increasing reasoning depth, distractors, or logical complexity"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ProofWriter, EntailmentBankQA, and CLUTRR: end-to-end LMs show sharp performance drops at higher proof depths or with distractors, while modular or stepwise systems maintain higher accuracy.",
                        "uuids": [
                            "e3522.0",
                            "e3522.4",
                            "e3511.5",
                            "e3511.3"
                        ]
                    },
                    {
                        "text": "RuleTakers and RoBERTa-based models achieve high accuracy on synthetic rulebase tasks but fail to generalize to harder, debiased, or multi-step settings (Hard RuleTaker, RobustLR).",
                        "uuids": [
                            "e3525.1",
                            "e3525.2",
                            "e3529.1",
                            "e3529.7"
                        ]
                    },
                    {
                        "text": "Gopher-280B and other large LMs show only modest gains on logic tasks compared to non-logic NLP tasks, and plateau at high scale.",
                        "uuids": [
                            "e3503.5",
                            "e3503.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new, larger LM is trained without architectural or modular interventions, it will show only modest improvement on deep logical reasoning tasks compared to smaller models.",
        "If a modular or neuro-symbolic architecture is applied to a new logic benchmark, it will outperform monolithic LMs of similar or even larger scale.",
        "If a value-guided search or explicit proof generation module is added to an existing LM, performance on multi-step or distractor-rich logical tasks will increase, especially at higher depths."
    ],
    "new_predictions_unknown": [
        "If a hybrid system combines large-scale LMs with modular neuro-symbolic components, it is unknown whether the gains will be additive, synergistic, or subject to diminishing returns.",
        "If a monolithic LM is trained on massive, logic-rich corpora, it is unclear whether it can match modular systems on deep logical reasoning without explicit architectural changes.",
        "If modular architectures are applied to tasks with ambiguous or fuzzy logic (e.g., commonsense reasoning), the effect on performance is uncertain."
    ],
    "negative_experiments": [
        "If a monolithic, end-to-end LM of sufficient scale (e.g., &gt;1T parameters) achieves robust, high accuracy on deep logical reasoning tasks without modularization, the theory would be challenged.",
        "If modular or neuro-symbolic systems fail to outperform monolithic LMs on new logic benchmarks, the theory's claims about architectural bottlenecks would be weakened.",
        "If scaling alone (without architectural changes) yields dramatic improvements on multi-step logic tasks, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some small models (e.g., Mistral-7B) outperform larger models on deep logical reasoning, possibly due to training data, objective, or architecture not captured by this theory.",
            "uuids": [
                "e3430.3"
            ]
        },
        {
            "text": "Certain discriminative models (e.g., RoBERTa, T5-Large) achieve high accuracy on synthetic or shallow logic tasks without modularization, suggesting that task structure and data complexity also play a role.",
            "uuids": [
                "e3525.1",
                "e3525.2",
                "e3529.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "RuleTakers and RoBERTa-based models achieve high accuracy on synthetic rulebase tasks without modularization, though these may be less challenging than strict logic benchmarks.",
            "uuids": [
                "e3525.1",
                "e3525.2"
            ]
        }
    ],
    "special_cases": [
        "For shallow or template-based logic tasks, monolithic LMs may suffice.",
        "If the modular system is poorly integrated or the value function is miscalibrated, performance may not improve.",
        "Certain tasks (e.g., abductive or fuzzy logic) may not benefit from strict modularization."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Rae et al. (2022) Scaling Language Models: Methods, Analysis & Insights from Training Gopher [Scaling law analysis for logic tasks]",
            "Tafjord et al. (2021) ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language [Stepwise proof generation]",
            "Jiang et al. (2023) Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models [Scaling and depth analysis]",
            "Clark et al. (2020) Transformers as Soft Reasoners over Language [RuleTaker baseline and limitations]",
            "Zhou et al. (2023) Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning [AMR-LDA and robustness]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
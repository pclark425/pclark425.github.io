<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Formatting-Induced Degeneration Cascade Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1936</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1936</p>
                <p><strong>Name:</strong> Prompt Formatting-Induced Degeneration Cascade Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the structure and formatting of prompts presented to large language models (LLMs) can trigger a cascade of degeneration in output quality, leading to a collapse in output validity. The theory asserts that certain prompt formats interact with the LLM's internal token prediction dynamics, attention allocation, and learned priors, amplifying error propagation and causing the model to deviate from intended task behavior, especially under ambiguous, overloaded, or adversarial formatting.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt Structure Amplifies Degeneration (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; contains &#8594; ambiguous or overloaded structure<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_exposed_to &#8594; prompt_format</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_output &#8594; exhibits &#8594; increased degeneration (e.g., repetition, incoherence, hallucination)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that ambiguous or multi-task prompts increase hallucination and incoherence in LLM outputs. </li>
    <li>Prompt injection and adversarial formatting can cause LLMs to ignore instructions or output irrelevant content. </li>
    <li>Instruction-following LLMs are sensitive to prompt clarity; ambiguous instructions lead to more errors. </li>
    <li>Chain-of-thought prompting improves reasoning by structuring prompts, suggesting that poor structure impairs it. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt sensitivity is known, the explicit cascade and amplification mechanism is a novel abstraction.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that prompt ambiguity and adversarial formatting can degrade LLM performance.</p>            <p><strong>What is Novel:</strong> This law formalizes the amplification effect of prompt structure on degeneration as a cascading process, not just a local error.</p>
            <p><strong>References:</strong> <ul>
    <li>Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Prompt sensitivity and adversarial prompts]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure affects reasoning]</li>
    <li>Jiang et al. (2023) Prompting Is Programming: A Survey of Prompt Engineering [Prompt format impacts output]</li>
</ul>
            <h3>Statement 1: Degeneration Cascade via Token Prediction Feedback (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_format &#8594; induces &#8594; early token prediction errors<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; generates &#8594; output sequence</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; subsequent_tokens &#8594; are_increasingly &#8594; influenced by prior errors<span style="color: #888888;">, and</span></div>
        <div>&#8226; output_validity &#8594; decreases &#8594; as sequence progresses</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Studies of LLMs show that early mistakes in generation often lead to compounding errors and output collapse. </li>
    <li>Prompt formatting that causes initial confusion (e.g., misplaced delimiters) increases the likelihood of degeneration cascades. </li>
    <li>Neural text degeneration is characterized by feedback loops where errors propagate through the output sequence. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The feedback loop is a new abstraction, though error compounding is known.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs can compound errors during generation.</p>            <p><strong>What is Novel:</strong> This law links prompt formatting directly to the initiation and amplification of degeneration cascades via token prediction feedback.</p>
            <p><strong>References:</strong> <ul>
    <li>Holtzman et al. (2020) The Curious Case of Neural Text Degeneration [Degeneration in neural text generation]</li>
    <li>Zhang et al. (2023) Language Models Fail to Learn Negation [Early errors propagate in LLM outputs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt is formatted with ambiguous section boundaries, LLMs will more frequently produce outputs with repeated or off-topic content.</li>
                <li>Introducing minor formatting errors (e.g., misplaced colons or inconsistent indentation) in prompts will increase the rate of hallucinated or degenerate outputs.</li>
                <li>Prompting with multiple tasks in a single input will result in more output collapse than single-task prompts, even when the total information is the same.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a prompt is formatted with adversarially designed but syntactically valid structures, LLMs may enter infinite loops or produce outputs that are valid in form but semantically nonsensical.</li>
                <li>There may exist prompt formats that, despite being clear to humans, systematically trigger degeneration cascades in LLMs due to misalignment with training data distributions.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If ambiguous or overloaded prompt formats do not increase degeneration rates compared to clear formats, the theory would be challenged.</li>
                <li>If early token prediction errors do not lead to increased error rates in subsequent tokens, the feedback cascade law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs recover from early errors and produce valid outputs despite ambiguous formatting. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known prompt effects with a novel cascade model, making it somewhat-related-to-existing.</p>
            <p><strong>References:</strong> <ul>
    <li>Holtzman et al. (2020) The Curious Case of Neural Text Degeneration [Degeneration in neural text generation]</li>
    <li>Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Prompt sensitivity]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt Formatting-Induced Degeneration Cascade Theory",
    "theory_description": "This theory posits that the structure and formatting of prompts presented to large language models (LLMs) can trigger a cascade of degeneration in output quality, leading to a collapse in output validity. The theory asserts that certain prompt formats interact with the LLM's internal token prediction dynamics, attention allocation, and learned priors, amplifying error propagation and causing the model to deviate from intended task behavior, especially under ambiguous, overloaded, or adversarial formatting.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt Structure Amplifies Degeneration",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "contains",
                        "object": "ambiguous or overloaded structure"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_exposed_to",
                        "object": "prompt_format"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_output",
                        "relation": "exhibits",
                        "object": "increased degeneration (e.g., repetition, incoherence, hallucination)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that ambiguous or multi-task prompts increase hallucination and incoherence in LLM outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt injection and adversarial formatting can cause LLMs to ignore instructions or output irrelevant content.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction-following LLMs are sensitive to prompt clarity; ambiguous instructions lead to more errors.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought prompting improves reasoning by structuring prompts, suggesting that poor structure impairs it.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that prompt ambiguity and adversarial formatting can degrade LLM performance.",
                    "what_is_novel": "This law formalizes the amplification effect of prompt structure on degeneration as a cascading process, not just a local error.",
                    "classification_explanation": "While prompt sensitivity is known, the explicit cascade and amplification mechanism is a novel abstraction.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Prompt sensitivity and adversarial prompts]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure affects reasoning]",
                        "Jiang et al. (2023) Prompting Is Programming: A Survey of Prompt Engineering [Prompt format impacts output]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Degeneration Cascade via Token Prediction Feedback",
                "if": [
                    {
                        "subject": "prompt_format",
                        "relation": "induces",
                        "object": "early token prediction errors"
                    },
                    {
                        "subject": "LLM",
                        "relation": "generates",
                        "object": "output sequence"
                    }
                ],
                "then": [
                    {
                        "subject": "subsequent_tokens",
                        "relation": "are_increasingly",
                        "object": "influenced by prior errors"
                    },
                    {
                        "subject": "output_validity",
                        "relation": "decreases",
                        "object": "as sequence progresses"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Studies of LLMs show that early mistakes in generation often lead to compounding errors and output collapse.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt formatting that causes initial confusion (e.g., misplaced delimiters) increases the likelihood of degeneration cascades.",
                        "uuids": []
                    },
                    {
                        "text": "Neural text degeneration is characterized by feedback loops where errors propagate through the output sequence.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs can compound errors during generation.",
                    "what_is_novel": "This law links prompt formatting directly to the initiation and amplification of degeneration cascades via token prediction feedback.",
                    "classification_explanation": "The feedback loop is a new abstraction, though error compounding is known.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Holtzman et al. (2020) The Curious Case of Neural Text Degeneration [Degeneration in neural text generation]",
                        "Zhang et al. (2023) Language Models Fail to Learn Negation [Early errors propagate in LLM outputs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt is formatted with ambiguous section boundaries, LLMs will more frequently produce outputs with repeated or off-topic content.",
        "Introducing minor formatting errors (e.g., misplaced colons or inconsistent indentation) in prompts will increase the rate of hallucinated or degenerate outputs.",
        "Prompting with multiple tasks in a single input will result in more output collapse than single-task prompts, even when the total information is the same."
    ],
    "new_predictions_unknown": [
        "If a prompt is formatted with adversarially designed but syntactically valid structures, LLMs may enter infinite loops or produce outputs that are valid in form but semantically nonsensical.",
        "There may exist prompt formats that, despite being clear to humans, systematically trigger degeneration cascades in LLMs due to misalignment with training data distributions."
    ],
    "negative_experiments": [
        "If ambiguous or overloaded prompt formats do not increase degeneration rates compared to clear formats, the theory would be challenged.",
        "If early token prediction errors do not lead to increased error rates in subsequent tokens, the feedback cascade law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs recover from early errors and produce valid outputs despite ambiguous formatting.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs with strong instruction tuning can handle ambiguous prompts without degeneration, suggesting robustness to formatting.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with explicit error correction or self-reflection mechanisms may interrupt degeneration cascades.",
        "Highly redundant or explicit prompts may be immune to formatting-induced degeneration."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt sensitivity and degeneration in LLMs are known phenomena.",
        "what_is_novel": "The explicit cascade mechanism linking prompt formatting to output validity collapse is a new theoretical abstraction.",
        "classification_explanation": "The theory synthesizes known prompt effects with a novel cascade model, making it somewhat-related-to-existing.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Holtzman et al. (2020) The Curious Case of Neural Text Degeneration [Degeneration in neural text generation]",
            "Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [Prompt sensitivity]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure effects]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-655",
    "original_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Instruction Template-Task Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1926</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1926</p>
                <p><strong>Name:</strong> Instruction Template-Task Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the degree of alignment between the instruction template and the underlying cognitive structure of the task determines LLM performance. Templates that closely match the task's required reasoning steps, information structure, or expected output format facilitate better internal representation and output accuracy, while misaligned templates induce errors, confusion, or degraded performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Template-Task Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; instruction_template &#8594; is_aligned_with &#8594; task_cognitive_structure<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_instruction_tuned &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_maximized &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_output &#8594; is_accurate_and_faithful &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Templates that explicitly guide the model through reasoning steps (e.g., chain-of-thought) improve performance on complex tasks. </li>
    <li>Misaligned templates (e.g., those that obscure task structure) lead to increased error rates. </li>
    <li>Instruction tuning with aligned templates produces more robust generalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This is a generalization of prompt engineering insights into a formal alignment law.</p>            <p><strong>What Already Exists:</strong> Prompt engineering and chain-of-thought prompting show that template structure can improve reasoning.</p>            <p><strong>What is Novel:</strong> The formalization of template-task alignment as a law governing LLM performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [template-task alignment]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [prompt structure and reasoning]</li>
</ul>
            <h3>Statement 1: Template-Task Misalignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; instruction_template &#8594; is_misaligned_with &#8594; task_cognitive_structure<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_instruction_tuned &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_degraded &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_output &#8594; contains_errors_or_hallucinations &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical results show that poorly designed templates can induce hallucinations or irrelevant outputs. </li>
    <li>Templates that do not match the expected output format of the task lead to lower accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This is a formalization of observed effects into a law of misalignment.</p>            <p><strong>What Already Exists:</strong> Prompt misalignment effects are observed in prompt engineering literature.</p>            <p><strong>What is Novel:</strong> The explicit law that misalignment systematically degrades performance and induces errors.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt-task alignment]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [prompt format effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a task is presented in a template that mirrors its reasoning steps, LLM performance will improve relative to a generic template.</li>
                <li>If a template is intentionally misaligned (e.g., by reversing reasoning order), error rates will increase.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained to recognize and adapt to misaligned templates, they may develop template-agnostic reasoning abilities.</li>
                <li>If templates are optimized via reinforcement learning for each task, performance may surpass current best practices.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If template-task alignment does not correlate with performance, the theory is falsified.</li>
                <li>If misaligned templates do not degrade performance, the theory is refuted.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks with trivial cognitive structure may not benefit from alignment. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This is a formalization and generalization of prompt engineering insights.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt-task alignment]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [prompt structure and reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Instruction Template-Task Alignment Theory",
    "theory_description": "This theory proposes that the degree of alignment between the instruction template and the underlying cognitive structure of the task determines LLM performance. Templates that closely match the task's required reasoning steps, information structure, or expected output format facilitate better internal representation and output accuracy, while misaligned templates induce errors, confusion, or degraded performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Template-Task Alignment Law",
                "if": [
                    {
                        "subject": "instruction_template",
                        "relation": "is_aligned_with",
                        "object": "task_cognitive_structure"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_instruction_tuned",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_maximized",
                        "object": "True"
                    },
                    {
                        "subject": "LLM_output",
                        "relation": "is_accurate_and_faithful",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Templates that explicitly guide the model through reasoning steps (e.g., chain-of-thought) improve performance on complex tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Misaligned templates (e.g., those that obscure task structure) lead to increased error rates.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction tuning with aligned templates produces more robust generalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering and chain-of-thought prompting show that template structure can improve reasoning.",
                    "what_is_novel": "The formalization of template-task alignment as a law governing LLM performance.",
                    "classification_explanation": "This is a generalization of prompt engineering insights into a formal alignment law.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [template-task alignment]",
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [prompt structure and reasoning]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Template-Task Misalignment Law",
                "if": [
                    {
                        "subject": "instruction_template",
                        "relation": "is_misaligned_with",
                        "object": "task_cognitive_structure"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_instruction_tuned",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_degraded",
                        "object": "True"
                    },
                    {
                        "subject": "LLM_output",
                        "relation": "contains_errors_or_hallucinations",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical results show that poorly designed templates can induce hallucinations or irrelevant outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Templates that do not match the expected output format of the task lead to lower accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt misalignment effects are observed in prompt engineering literature.",
                    "what_is_novel": "The explicit law that misalignment systematically degrades performance and induces errors.",
                    "classification_explanation": "This is a formalization of observed effects into a law of misalignment.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt-task alignment]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [prompt format effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a task is presented in a template that mirrors its reasoning steps, LLM performance will improve relative to a generic template.",
        "If a template is intentionally misaligned (e.g., by reversing reasoning order), error rates will increase."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained to recognize and adapt to misaligned templates, they may develop template-agnostic reasoning abilities.",
        "If templates are optimized via reinforcement learning for each task, performance may surpass current best practices."
    ],
    "negative_experiments": [
        "If template-task alignment does not correlate with performance, the theory is falsified.",
        "If misaligned templates do not degrade performance, the theory is refuted."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks with trivial cognitive structure may not benefit from alignment.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs with very large context windows can recover from misaligned templates by inferring task structure from context.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with fixed output formats may be less sensitive to alignment.",
        "Very large models may be more robust to misalignment."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and chain-of-thought prompting show alignment effects.",
        "what_is_novel": "The formalization of alignment/misalignment as governing laws.",
        "classification_explanation": "This is a formalization and generalization of prompt engineering insights.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt-task alignment]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [prompt structure and reasoning]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
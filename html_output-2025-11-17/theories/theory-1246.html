<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositional Abstraction Theory for Graph-to-Text - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1246</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1246</p>
                <p><strong>Name:</strong> Compositional Abstraction Theory for Graph-to-Text</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that the ideal graph-to-text representation is one that enables compositional abstraction: the ability to represent and decompose graphs into modular, reusable substructures (motifs, patterns, or templates) that can be flexibly recombined in text. Such representations allow language models to generalize from seen to unseen graph structures, support transfer learning, and facilitate efficient learning of complex relational patterns.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Compositionality Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_to_text_representation &#8594; encodes &#8594; modular_graph_substructures</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model_trained_on_representation &#8594; generalizes &#8594; unseen_graph_patterns</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Compositional representations in NLP and graph learning enable generalization to novel combinations. </li>
    <li>Motif-based and template-based encodings improve transfer to new graph domains. </li>
    <li>Language models benefit from modular, reusable text patterns in other structured data-to-text tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends compositionality to a new domain and formalizes it for graph-to-text LMs.</p>            <p><strong>What Already Exists:</strong> Compositionality is a core principle in linguistics and some graph learning methods.</p>            <p><strong>What is Novel:</strong> Its explicit application to graph-to-text LM representation is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Models [Compositionality in LMs]</li>
    <li>Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [Template-based graph-to-text]</li>
    <li>Xu et al. (2020) Compositional Generalization in Graph-to-Sequence Learning [Compositionality in graph-to-sequence]</li>
</ul>
            <h3>Statement 1: Abstraction Transfer Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_to_text_representation &#8594; supports &#8594; abstraction_of_graph_patterns</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model_trained_on_representation &#8594; transfers &#8594; knowledge_to_new_graph_domains</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Abstraction and transfer are key to human and machine learning in structured domains. </li>
    <li>Graph motif abstraction enables transfer learning in graph neural networks. </li>
    <li>Template-based text generation supports transfer across domains in data-to-text tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes known ideas into a new formalism for this application.</p>            <p><strong>What Already Exists:</strong> Abstraction and transfer are well-studied in ML and cognitive science.</p>            <p><strong>What is Novel:</strong> Their explicit formalization for graph-to-text LM representation is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building Machines That Learn and Think Like People [Abstraction and transfer in cognition]</li>
    <li>Xu et al. (2020) Compositional Generalization in Graph-to-Sequence Learning [Abstraction in graph-to-sequence]</li>
    <li>Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [Template-based abstraction]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs trained on compositional, motif-based graph-to-text representations will generalize better to novel graph structures than those trained on flat or non-compositional encodings.</li>
                <li>Abstraction-based representations will facilitate transfer learning across different graph domains.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>The optimal granularity of motifs or templates for maximal generalization is unknown.</li>
                <li>It is unknown whether compositional abstraction can scale to highly irregular or large-scale graphs.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs trained on compositional representations do not outperform those trained on non-compositional ones in generalization tasks, the theory would be challenged.</li>
                <li>If abstraction-based transfer fails in new domains, the abstraction transfer law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address the computational cost of identifying and encoding motifs in very large graphs. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory adapts and formalizes existing principles for a new, specific application.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Models [Compositionality in LMs]</li>
    <li>Xu et al. (2020) Compositional Generalization in Graph-to-Sequence Learning [Compositionality in graph-to-sequence]</li>
    <li>Lake et al. (2017) Building Machines That Learn and Think Like People [Abstraction and transfer in cognition]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositional Abstraction Theory for Graph-to-Text",
    "theory_description": "This theory asserts that the ideal graph-to-text representation is one that enables compositional abstraction: the ability to represent and decompose graphs into modular, reusable substructures (motifs, patterns, or templates) that can be flexibly recombined in text. Such representations allow language models to generalize from seen to unseen graph structures, support transfer learning, and facilitate efficient learning of complex relational patterns.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Compositionality Law",
                "if": [
                    {
                        "subject": "graph_to_text_representation",
                        "relation": "encodes",
                        "object": "modular_graph_substructures"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model_trained_on_representation",
                        "relation": "generalizes",
                        "object": "unseen_graph_patterns"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Compositional representations in NLP and graph learning enable generalization to novel combinations.",
                        "uuids": []
                    },
                    {
                        "text": "Motif-based and template-based encodings improve transfer to new graph domains.",
                        "uuids": []
                    },
                    {
                        "text": "Language models benefit from modular, reusable text patterns in other structured data-to-text tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositionality is a core principle in linguistics and some graph learning methods.",
                    "what_is_novel": "Its explicit application to graph-to-text LM representation is novel.",
                    "classification_explanation": "The law extends compositionality to a new domain and formalizes it for graph-to-text LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Models [Compositionality in LMs]",
                        "Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [Template-based graph-to-text]",
                        "Xu et al. (2020) Compositional Generalization in Graph-to-Sequence Learning [Compositionality in graph-to-sequence]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Abstraction Transfer Law",
                "if": [
                    {
                        "subject": "graph_to_text_representation",
                        "relation": "supports",
                        "object": "abstraction_of_graph_patterns"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model_trained_on_representation",
                        "relation": "transfers",
                        "object": "knowledge_to_new_graph_domains"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Abstraction and transfer are key to human and machine learning in structured domains.",
                        "uuids": []
                    },
                    {
                        "text": "Graph motif abstraction enables transfer learning in graph neural networks.",
                        "uuids": []
                    },
                    {
                        "text": "Template-based text generation supports transfer across domains in data-to-text tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Abstraction and transfer are well-studied in ML and cognitive science.",
                    "what_is_novel": "Their explicit formalization for graph-to-text LM representation is new.",
                    "classification_explanation": "The law synthesizes known ideas into a new formalism for this application.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake et al. (2017) Building Machines That Learn and Think Like People [Abstraction and transfer in cognition]",
                        "Xu et al. (2020) Compositional Generalization in Graph-to-Sequence Learning [Abstraction in graph-to-sequence]",
                        "Koncel-Kedziorski et al. (2019) Text Generation from Knowledge Graphs with Graph Transformers [Template-based abstraction]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs trained on compositional, motif-based graph-to-text representations will generalize better to novel graph structures than those trained on flat or non-compositional encodings.",
        "Abstraction-based representations will facilitate transfer learning across different graph domains."
    ],
    "new_predictions_unknown": [
        "The optimal granularity of motifs or templates for maximal generalization is unknown.",
        "It is unknown whether compositional abstraction can scale to highly irregular or large-scale graphs."
    ],
    "negative_experiments": [
        "If LMs trained on compositional representations do not outperform those trained on non-compositional ones in generalization tasks, the theory would be challenged.",
        "If abstraction-based transfer fails in new domains, the abstraction transfer law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address the computational cost of identifying and encoding motifs in very large graphs.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some evidence suggests that for highly irregular graphs, motif-based abstraction may not capture all relevant information.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For graphs with no recurring substructures, compositional abstraction may offer little benefit.",
        "For highly regular graphs, simple templates may suffice."
    ],
    "existing_theory": {
        "what_already_exists": "Compositionality and abstraction are core principles in linguistics, ML, and cognitive science.",
        "what_is_novel": "Their explicit, formal application to graph-to-text LM representation is new.",
        "classification_explanation": "The theory adapts and formalizes existing principles for a new, specific application.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Models [Compositionality in LMs]",
            "Xu et al. (2020) Compositional Generalization in Graph-to-Sequence Learning [Compositionality in graph-to-sequence]",
            "Lake et al. (2017) Building Machines That Learn and Think Like People [Abstraction and transfer in cognition]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-611",
    "original_theory_name": "Multimodal Alignment and Compactness Principle for Graph-to-Text Representations",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contextual Consistency Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1699</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1699</p>
                <p><strong>Name:</strong> Contextual Consistency Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can be used for detecting anomalies in lists of data.</p>
                <p><strong>Description:</strong> This theory posits that language models can detect anomalies in lists of data by modeling the contextual consistency of each item with respect to the distributional patterns learned from the list as a whole. Items that deviate significantly from the learned context—whether in structure, semantics, or statistical properties—are flagged as anomalies.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contextual Deviation Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_trained_on &#8594; list<span style="color: #888888;">, and</span></div>
        <div>&#8226; item &#8594; is_part_of &#8594; list<span style="color: #888888;">, and</span></div>
        <div>&#8226; item &#8594; has_contextual_representation &#8594; embedding_i<span style="color: #888888;">, and</span></div>
        <div>&#8226; embedding_i &#8594; is_distant_from &#8594; distribution_of_embeddings_in_list</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; item &#8594; is_flagged_as &#8594; anomaly</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Language models learn contextual representations that capture distributional properties of data. </li>
    <li>Anomalies often manifest as outliers in embedding space, as shown in NLP and vision applications. </li>
    <li>Contextual similarity is a strong indicator of normality in list-structured data. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Closely related to existing embedding-based anomaly detection, but the systematic application to arbitrary lists via LMs is novel.</p>            <p><strong>What Already Exists:</strong> Embedding-based anomaly detection is used in NLP and vision, and contextual similarity is a known signal.</p>            <p><strong>What is Novel:</strong> The explicit use of language model contextual representations for anomaly detection in arbitrary lists is a new generalization.</p>
            <p><strong>References:</strong> <ul>
    <li>Mikolov et al. (2013) Efficient Estimation of Word Representations in Vector Space [embeddings capture context]</li>
    <li>Chandola et al. (2009) Anomaly Detection: A Survey [embedding-based anomaly detection]</li>
    <li>Reif et al. (2019) Visualizing and Measuring the Geometry of BERT [embedding outliers in NLP]</li>
</ul>
            <h3>Statement 1: Distributional Pattern Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_applied_to &#8594; list<span style="color: #888888;">, and</span></div>
        <div>&#8226; item &#8594; is_part_of &#8594; list<span style="color: #888888;">, and</span></div>
        <div>&#8226; item &#8594; deviates_from &#8594; learned_distribution_of_list</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; item &#8594; is_likely &#8594; anomaly</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Language models capture statistical regularities and can model the distribution of list items. </li>
    <li>Anomalies are often defined as deviations from learned distributions. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is a generalization of classic anomaly detection, but its application via LMs to lists is novel.</p>            <p><strong>What Already Exists:</strong> Anomaly detection as deviation from distribution is a classic approach.</p>            <p><strong>What is Novel:</strong> The application of this principle using language models on arbitrary lists is a new generalization.</p>
            <p><strong>References:</strong> <ul>
    <li>Chandola et al. (2009) Anomaly Detection: A Survey [distributional anomaly detection]</li>
    <li>Devlin et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [LMs learn distributions]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a list contains an item with a structure or semantics not seen in the rest of the list, the LM will assign it a distant embedding and flag it as anomalous.</li>
                <li>If the LM is applied to lists of product names, a name with a different format or language will be detected as an anomaly.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If the list contains subtle semantic anomalies (e.g., contextually plausible but incorrect items), the LM's ability to detect them via contextual deviation is uncertain.</li>
                <li>If the LM is applied to lists with high intra-list diversity, the threshold for anomaly detection may become unreliable.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If items that are contextually distant are not anomalous, the theory's mechanism is challenged.</li>
                <li>If anomalies do not result in contextual deviation in embedding space, the theory is undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Anomalies that are contextually similar but semantically incorrect may not be detected. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory generalizes existing anomaly detection to the LM context for lists, which is a novel application.</p>
            <p><strong>References:</strong> <ul>
    <li>Chandola et al. (2009) Anomaly Detection: A Survey [distributional and embedding-based anomaly detection]</li>
    <li>Reif et al. (2019) Visualizing and Measuring the Geometry of BERT [embedding outliers in NLP]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Contextual Consistency Theory",
    "theory_description": "This theory posits that language models can detect anomalies in lists of data by modeling the contextual consistency of each item with respect to the distributional patterns learned from the list as a whole. Items that deviate significantly from the learned context—whether in structure, semantics, or statistical properties—are flagged as anomalies.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contextual Deviation Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_trained_on",
                        "object": "list"
                    },
                    {
                        "subject": "item",
                        "relation": "is_part_of",
                        "object": "list"
                    },
                    {
                        "subject": "item",
                        "relation": "has_contextual_representation",
                        "object": "embedding_i"
                    },
                    {
                        "subject": "embedding_i",
                        "relation": "is_distant_from",
                        "object": "distribution_of_embeddings_in_list"
                    }
                ],
                "then": [
                    {
                        "subject": "item",
                        "relation": "is_flagged_as",
                        "object": "anomaly"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Language models learn contextual representations that capture distributional properties of data.",
                        "uuids": []
                    },
                    {
                        "text": "Anomalies often manifest as outliers in embedding space, as shown in NLP and vision applications.",
                        "uuids": []
                    },
                    {
                        "text": "Contextual similarity is a strong indicator of normality in list-structured data.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Embedding-based anomaly detection is used in NLP and vision, and contextual similarity is a known signal.",
                    "what_is_novel": "The explicit use of language model contextual representations for anomaly detection in arbitrary lists is a new generalization.",
                    "classification_explanation": "Closely related to existing embedding-based anomaly detection, but the systematic application to arbitrary lists via LMs is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Mikolov et al. (2013) Efficient Estimation of Word Representations in Vector Space [embeddings capture context]",
                        "Chandola et al. (2009) Anomaly Detection: A Survey [embedding-based anomaly detection]",
                        "Reif et al. (2019) Visualizing and Measuring the Geometry of BERT [embedding outliers in NLP]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Distributional Pattern Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_applied_to",
                        "object": "list"
                    },
                    {
                        "subject": "item",
                        "relation": "is_part_of",
                        "object": "list"
                    },
                    {
                        "subject": "item",
                        "relation": "deviates_from",
                        "object": "learned_distribution_of_list"
                    }
                ],
                "then": [
                    {
                        "subject": "item",
                        "relation": "is_likely",
                        "object": "anomaly"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Language models capture statistical regularities and can model the distribution of list items.",
                        "uuids": []
                    },
                    {
                        "text": "Anomalies are often defined as deviations from learned distributions.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Anomaly detection as deviation from distribution is a classic approach.",
                    "what_is_novel": "The application of this principle using language models on arbitrary lists is a new generalization.",
                    "classification_explanation": "The law is a generalization of classic anomaly detection, but its application via LMs to lists is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Chandola et al. (2009) Anomaly Detection: A Survey [distributional anomaly detection]",
                        "Devlin et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [LMs learn distributions]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a list contains an item with a structure or semantics not seen in the rest of the list, the LM will assign it a distant embedding and flag it as anomalous.",
        "If the LM is applied to lists of product names, a name with a different format or language will be detected as an anomaly."
    ],
    "new_predictions_unknown": [
        "If the list contains subtle semantic anomalies (e.g., contextually plausible but incorrect items), the LM's ability to detect them via contextual deviation is uncertain.",
        "If the LM is applied to lists with high intra-list diversity, the threshold for anomaly detection may become unreliable."
    ],
    "negative_experiments": [
        "If items that are contextually distant are not anomalous, the theory's mechanism is challenged.",
        "If anomalies do not result in contextual deviation in embedding space, the theory is undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Anomalies that are contextually similar but semantically incorrect may not be detected.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Lists with high diversity may produce false positives due to natural contextual variation.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Lists with multiple valid subgroups (multimodal distributions) may require subgroup-aware anomaly detection.",
        "Items with rare but valid contexts may be incorrectly flagged as anomalies."
    ],
    "existing_theory": {
        "what_already_exists": "Embedding-based and distributional anomaly detection are established in ML.",
        "what_is_novel": "The systematic use of LM contextual representations for anomaly detection in arbitrary lists is a new generalization.",
        "classification_explanation": "The theory generalizes existing anomaly detection to the LM context for lists, which is a novel application.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Chandola et al. (2009) Anomaly Detection: A Survey [distributional and embedding-based anomaly detection]",
            "Reif et al. (2019) Visualizing and Measuring the Geometry of BERT [embedding outliers in NLP]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can be used for detecting anomalies in lists of data.",
    "original_theory_id": "theory-640",
    "original_theory_name": "LLM Representation and Prompt-Engineering Theory for Anomaly Detection",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
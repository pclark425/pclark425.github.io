<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bayesian Alignment Theory of LLM Scientific Forecasting - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-509</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-509</p>
                <p><strong>Name:</strong> Bayesian Alignment Theory of LLM Scientific Forecasting</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can accurately measure the probability or likelihood of specific future real-world scientific discoveries, based on the following results.</p>
                <p><strong>Description:</strong> LLMs can most accurately estimate the probability of future real-world scientific discoveries when their internal, often coarse or uncalibrated, probabilistic judgments are aligned and structured via explicit Bayesian frameworks that decompose and aggregate abductively generated factors, rather than relying on direct output probabilities or verbalized confidences. This alignment enables LLMs to overcome limitations of surface-form uncertainty, prompt/context bias, and over/underconfidence, yielding calibrated, interpretable, and actionable probability estimates for scientific forecasting.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Bayesian Factor Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; generates &#8594; abductive factors F_j relevant to a scientific outcome O<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; provides &#8594; coarse probability judgments P_LLM(O|f) for sampled factor combinations f<span style="color: #888888;">, and</span></div>
        <div>&#8226; Bayesian network &#8594; is fit &#8594; to align P(O|f_j) with P_LLM(O|f) via constrained optimization</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM+Bayes system &#8594; produces &#8594; calibrated and interpretable probability estimates P(O|C) for novel contexts C</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>BIRD framework outperforms direct LLM probability outputs and CoT on probability-quality F1 and decision accuracy, showing that Bayesian alignment of LLM-generated factors and coarse probabilities yields more reliable forecasts. <a href="../results/extraction-result-3646.html#e3646.0" class="evidence-link">[e3646.0]</a> </li>
    <li>LLMs are comparatively good at abductive factor generation and coarse-grained probability judgements but poor at producing calibrated numeric probabilities; aligning these coarse judgements with an explicit Bayesian model via constrained optimization (BIRD) yields substantially more reliable and interpretable probability estimates. <a href="../results/extraction-result-3646.html#e3646.0" class="evidence-link">[e3646.0]</a> </li>
    <li>Ablations demonstrate the constrained optimization yields better CPTs than neutral heuristics (1/2, 1/n, or fixed-init). <a href="../results/extraction-result-3646.html#e3646.0" class="evidence-link">[e3646.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Calibration Superiority of Structured Aggregation (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; outputs &#8594; direct verbalized or logit-based probabilities for scientific discovery events<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM+Bayes system &#8594; outputs &#8594; probabilities via Bayesian alignment of abductive factors</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM+Bayes system &#8594; achieves &#8594; lower calibration error and higher decision accuracy than direct LLM outputs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>BIRD's calibrated probabilities yield up to 30% absolute improvement in preference F1 over GPT-4 verbal probabilities and outperform CoT and direct logit-based methods. <a href="../results/extraction-result-3646.html#e3646.0" class="evidence-link">[e3646.0]</a> </li>
    <li>Direct LLM outputs (verbalized, logit, CoT) are less calibrated and less accurate than BIRD's Bayesian-aligned outputs across multiple datasets (COMMON2SENSE, PLASMA, TODAY). <a href="../results/extraction-result-3646.html#e3646.0" class="evidence-link">[e3646.0]</a> <a href="../results/extraction-result-3646.html#e3646.5" class="evidence-link">[e3646.5]</a> <a href="../results/extraction-result-3646.html#e3646.6" class="evidence-link">[e3646.6]</a> <a href="../results/extraction-result-3646.html#e3646.7" class="evidence-link">[e3646.7]</a> </li>
    <li>Vanilla verbalization and logit-based baselines underperform BIRD in preference F1 and decision accuracy. <a href="../results/extraction-result-3646.html#e3646.4" class="evidence-link">[e3646.4]</a> <a href="../results/extraction-result-3646.html#e3646.6" class="evidence-link">[e3646.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Contextual and Prompt Bias Correction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is queried &#8594; with content-free inputs to estimate context-dependent bias<span style="color: #888888;">, and</span></div>
        <div>&#8226; output probabilities &#8594; are rescaled &#8594; using a diagonal affine transform (vector scaling)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; calibrated LLM &#8594; reduces &#8594; variance and bias in probability estimates across prompts and contexts</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Contextual Calibration method reduces mean and worst-case accuracy variance across prompts and improves accuracy by up to 30 percentage points. <a href="../results/extraction-result-3726.html#e3726.1" class="evidence-link">[e3726.1]</a> </li>
    <li>Prompt-dependent biases (majority, recency, common-token) are corrected by contextual calibration, generalizing across GPT-3 and GPT-2. <a href="../results/extraction-result-3726.html#e3726.0" class="evidence-link">[e3726.0]</a> <a href="../results/extraction-result-3726.html#e3726.2" class="evidence-link">[e3726.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new scientific forecasting task is presented to an LLM, using a Bayesian alignment framework (e.g., BIRD) will yield more calibrated and accurate probability estimates than direct verbalized or logit-based outputs.</li>
                <li>Applying contextual calibration to LLM outputs in a new domain (e.g., materials science) will reduce prompt-induced variance and improve mean accuracy of probability estimates.</li>
                <li>In decision tasks with partial information, Bayesian-aligned LLMs will outperform direct LLM outputs in both calibration and downstream decision accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are used to forecast the probability of a major, unprecedented scientific breakthrough (e.g., room-temperature superconductivity), Bayesian-aligned LLMs will provide more reliable and interpretable probability estimates than any single LLM or human expert.</li>
                <li>In domains with highly interdependent latent factors (e.g., climate tipping points), extending the Bayesian alignment approach to model factor dependencies will further improve calibration and decision accuracy, but may require new methods for factor interaction modeling.</li>
                <li>If LLMs are used to forecast events in domains with no clear factor structure, the effectiveness of Bayesian alignment may be limited or require new abductive strategies.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If direct LLM verbalized probabilities are found to be as well-calibrated as Bayesian-aligned outputs on a new, complex scientific forecasting benchmark, this would challenge the necessity of Bayesian alignment.</li>
                <li>If contextual calibration fails to reduce prompt-induced variance or improve accuracy in a new domain, the generality of the bias correction law would be questioned.</li>
                <li>If Bayesian-aligned LLMs systematically underperform direct LLM outputs on out-of-distribution forecasting tasks, the universality of the theory would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs are used in domains with no clear factor structure or where factor generation is incomplete, leading to poor Bayesian alignment performance. <a href="../results/extraction-result-3646.html#e3646.0" class="evidence-link">[e3646.0]</a> </li>
    <li>Forecasting tasks where ground truth is fundamentally unknowable or where human consensus is absent, making calibration assessment difficult. <a href="../results/extraction-result-3727.html#e3727.3" class="evidence-link">[e3727.3]</a> </li>
    <li>Direct LLM outputs (e.g., answer-logit on Multiply-divide) sometimes outperform Bayesian-aligned methods on specific out-of-distribution tasks. <a href="../results/extraction-result-3727.html#e3727.1" class="evidence-link">[e3727.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2024) BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models [BIRD is the first explicit implementation, but the general theory of Bayesian alignment of LLMs is not yet formalized in the literature.]</li>
    <li>Jiang et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Contextual calibration is a key component, but not previously unified with Bayesian alignment for scientific forecasting.]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Bayesian Alignment Theory of LLM Scientific Forecasting",
    "theory_description": "LLMs can most accurately estimate the probability of future real-world scientific discoveries when their internal, often coarse or uncalibrated, probabilistic judgments are aligned and structured via explicit Bayesian frameworks that decompose and aggregate abductively generated factors, rather than relying on direct output probabilities or verbalized confidences. This alignment enables LLMs to overcome limitations of surface-form uncertainty, prompt/context bias, and over/underconfidence, yielding calibrated, interpretable, and actionable probability estimates for scientific forecasting.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Bayesian Factor Alignment Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "generates",
                        "object": "abductive factors F_j relevant to a scientific outcome O"
                    },
                    {
                        "subject": "LLM",
                        "relation": "provides",
                        "object": "coarse probability judgments P_LLM(O|f) for sampled factor combinations f"
                    },
                    {
                        "subject": "Bayesian network",
                        "relation": "is fit",
                        "object": "to align P(O|f_j) with P_LLM(O|f) via constrained optimization"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM+Bayes system",
                        "relation": "produces",
                        "object": "calibrated and interpretable probability estimates P(O|C) for novel contexts C"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "BIRD framework outperforms direct LLM probability outputs and CoT on probability-quality F1 and decision accuracy, showing that Bayesian alignment of LLM-generated factors and coarse probabilities yields more reliable forecasts.",
                        "uuids": [
                            "e3646.0"
                        ]
                    },
                    {
                        "text": "LLMs are comparatively good at abductive factor generation and coarse-grained probability judgements but poor at producing calibrated numeric probabilities; aligning these coarse judgements with an explicit Bayesian model via constrained optimization (BIRD) yields substantially more reliable and interpretable probability estimates.",
                        "uuids": [
                            "e3646.0"
                        ]
                    },
                    {
                        "text": "Ablations demonstrate the constrained optimization yields better CPTs than neutral heuristics (1/2, 1/n, or fixed-init).",
                        "uuids": [
                            "e3646.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Calibration Superiority of Structured Aggregation",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "outputs",
                        "object": "direct verbalized or logit-based probabilities for scientific discovery events"
                    },
                    {
                        "subject": "LLM+Bayes system",
                        "relation": "outputs",
                        "object": "probabilities via Bayesian alignment of abductive factors"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM+Bayes system",
                        "relation": "achieves",
                        "object": "lower calibration error and higher decision accuracy than direct LLM outputs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "BIRD's calibrated probabilities yield up to 30% absolute improvement in preference F1 over GPT-4 verbal probabilities and outperform CoT and direct logit-based methods.",
                        "uuids": [
                            "e3646.0"
                        ]
                    },
                    {
                        "text": "Direct LLM outputs (verbalized, logit, CoT) are less calibrated and less accurate than BIRD's Bayesian-aligned outputs across multiple datasets (COMMON2SENSE, PLASMA, TODAY).",
                        "uuids": [
                            "e3646.0",
                            "e3646.5",
                            "e3646.6",
                            "e3646.7"
                        ]
                    },
                    {
                        "text": "Vanilla verbalization and logit-based baselines underperform BIRD in preference F1 and decision accuracy.",
                        "uuids": [
                            "e3646.4",
                            "e3646.6"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        },
        {
            "law": {
                "law_name": "Contextual and Prompt Bias Correction Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is queried",
                        "object": "with content-free inputs to estimate context-dependent bias"
                    },
                    {
                        "subject": "output probabilities",
                        "relation": "are rescaled",
                        "object": "using a diagonal affine transform (vector scaling)"
                    }
                ],
                "then": [
                    {
                        "subject": "calibrated LLM",
                        "relation": "reduces",
                        "object": "variance and bias in probability estimates across prompts and contexts"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Contextual Calibration method reduces mean and worst-case accuracy variance across prompts and improves accuracy by up to 30 percentage points.",
                        "uuids": [
                            "e3726.1"
                        ]
                    },
                    {
                        "text": "Prompt-dependent biases (majority, recency, common-token) are corrected by contextual calibration, generalizing across GPT-3 and GPT-2.",
                        "uuids": [
                            "e3726.0",
                            "e3726.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new scientific forecasting task is presented to an LLM, using a Bayesian alignment framework (e.g., BIRD) will yield more calibrated and accurate probability estimates than direct verbalized or logit-based outputs.",
        "Applying contextual calibration to LLM outputs in a new domain (e.g., materials science) will reduce prompt-induced variance and improve mean accuracy of probability estimates.",
        "In decision tasks with partial information, Bayesian-aligned LLMs will outperform direct LLM outputs in both calibration and downstream decision accuracy."
    ],
    "new_predictions_unknown": [
        "If LLMs are used to forecast the probability of a major, unprecedented scientific breakthrough (e.g., room-temperature superconductivity), Bayesian-aligned LLMs will provide more reliable and interpretable probability estimates than any single LLM or human expert.",
        "In domains with highly interdependent latent factors (e.g., climate tipping points), extending the Bayesian alignment approach to model factor dependencies will further improve calibration and decision accuracy, but may require new methods for factor interaction modeling.",
        "If LLMs are used to forecast events in domains with no clear factor structure, the effectiveness of Bayesian alignment may be limited or require new abductive strategies."
    ],
    "negative_experiments": [
        "If direct LLM verbalized probabilities are found to be as well-calibrated as Bayesian-aligned outputs on a new, complex scientific forecasting benchmark, this would challenge the necessity of Bayesian alignment.",
        "If contextual calibration fails to reduce prompt-induced variance or improve accuracy in a new domain, the generality of the bias correction law would be questioned.",
        "If Bayesian-aligned LLMs systematically underperform direct LLM outputs on out-of-distribution forecasting tasks, the universality of the theory would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs are used in domains with no clear factor structure or where factor generation is incomplete, leading to poor Bayesian alignment performance.",
            "uuids": [
                "e3646.0"
            ]
        },
        {
            "text": "Forecasting tasks where ground truth is fundamentally unknowable or where human consensus is absent, making calibration assessment difficult.",
            "uuids": [
                "e3727.3"
            ]
        },
        {
            "text": "Direct LLM outputs (e.g., answer-logit on Multiply-divide) sometimes outperform Bayesian-aligned methods on specific out-of-distribution tasks.",
            "uuids": [
                "e3727.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some direct LLM outputs (e.g., answer-logit on Multiply-divide) outperform Bayesian-aligned methods on specific out-of-distribution tasks, suggesting that Bayesian alignment is not always superior.",
            "uuids": [
                "e3727.1"
            ]
        },
        {
            "text": "In some settings, direct logit-based or indirect-logit fine-tuned models yield strong calibration on certain evaluation sets, challenging the universality of Bayesian alignment.",
            "uuids": [
                "e3727.2"
            ]
        }
    ],
    "special_cases": [
        "If the LLM's factor generation is incomplete or incorrect, Bayesian alignment may fail or produce misleading probabilities.",
        "In domains with strong factor dependencies, assuming independence (as in some BIRD implementations) may degrade calibration.",
        "If the LLM's coarse probability judgments are systematically biased, Bayesian alignment may inherit or amplify these biases unless corrected.",
        "In tasks with ambiguous or underspecified outcomes, the mapping from factors to outcomes may be ill-defined, limiting the effectiveness of Bayesian alignment."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wang et al. (2024) BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models [BIRD is the first explicit implementation, but the general theory of Bayesian alignment of LLMs is not yet formalized in the literature.]",
            "Jiang et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Contextual calibration is a key component, but not previously unified with Bayesian alignment for scientific forecasting.]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
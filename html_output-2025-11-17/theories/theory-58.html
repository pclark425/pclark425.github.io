<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tokenization Granularity Theory for Arithmetic - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-58</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-58</p>
                <p><strong>Name:</strong> Tokenization Granularity Theory for Arithmetic</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> The granularity of numeric tokenization—whether numbers are split into individual digits, multi-digit chunks, or whole numbers—fundamentally determines a language model's ability to learn and generalize arithmetic algorithms. Single-digit tokenization (one token per digit) enables learning of digit-wise algorithmic procedures because: (1) it creates consistent token-to-digit alignment across all numbers, (2) it allows the model to learn position-invariant digit operations, (3) it provides sufficient training examples for each digit combination (only 10 digits vs. 100+ multi-digit tokens), and (4) it enables length generalization by making the algorithm independent of number magnitude. Multi-digit tokenization (e.g., 2-digit or 3-digit chunks via BPE) creates a combinatorially large token space (100 for 2-digit, 1000 for 3-digit), inconsistent alignments (similar numbers map to different token sequences), and insufficient training coverage, preventing algorithm learning and causing models to rely on memorization or heuristics. This effect is most pronounced for smaller models (<10B parameters) and becomes less critical but still important for larger models. The theory also encompasses tokenization direction effects: the direction of greedy tokenization (left-to-right vs right-to-left) interacts with digit ordering to affect carry/borrow locality, with right-to-left tokenization better supporting little-endian (LSB-first) output formats.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Single-digit tokenization (one token per digit) is optimal for learning arithmetic algorithms in language models because it provides consistent token-to-digit alignment.</li>
                <li>Multi-digit tokenization creates a combinatorially large token vocabulary (100 for 2-digit, 1000 for 3-digit) that prevents sufficient training coverage of digit combinations.</li>
                <li>Digit-level tokenization enables position-invariant learning: the model can learn operations on digits independent of their position in the number.</li>
                <li>Tokenization granularity effects are most pronounced in smaller models (<10B) due to capacity constraints, but remain important even for larger models.</li>
                <li>Inconsistent tokenization (where similar numbers map to different token sequences) prevents learning of systematic numeric relationships and forces reliance on memorization.</li>
                <li>The benefit of digit-level tokenization increases with the complexity of the arithmetic operation (more important for multiplication than addition).</li>
                <li>Digit-level tokenization enables length generalization because the learned algorithm operates on individual digits rather than whole numbers or multi-digit chunks.</li>
                <li>Larger models can partially compensate for suboptimal tokenization through increased capacity, but digit-level remains superior across all tested model sizes.</li>
                <li>Tokenization direction (left-to-right vs right-to-right greedy segmentation) interacts with digit ordering: right-to-left tokenization better supports little-endian (LSB-first) formats by aligning token boundaries with carry propagation.</li>
                <li>The number of unique tokens required for complete coverage grows exponentially with tokenization granularity: 10 for single-digit, 100 for 2-digit, 1000 for 3-digit, making training coverage increasingly sparse.</li>
                <li>Tokenization granularity affects not just accuracy but also the types of errors: multi-digit tokenization produces token-boundary-aligned errors (e.g., 'digit 4' errors in GPT-3.5) and off-by-one errors concentrated at token boundaries.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>One-digit tokenizer yields best in-domain and out-of-domain performance for 0.9B and 0.1B models; three-digit tokenizer performs poorly for sub-billion models. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
    <li>Digit-by-digit tokenization (DT) substantially improves sample complexity for learning numeric operations in GENBERT; variant without DT failed to learn the numerical task. <a href="../results/extraction-result-305.html#e305.3" class="evidence-link">[e305.3]</a> <a href="../results/extraction-result-305.html#e305.0" class="evidence-link">[e305.0]</a> </li>
    <li>MathGLM uses digit-level tokenization (each digit as separate token) and achieves 93%+ accuracy on complex arithmetic, substantially outperforming models without digit-level tokenization. <a href="../results/extraction-result-265.html#e265.0" class="evidence-link">[e265.0]</a> <a href="../results/extraction-result-265.html#e265.2" class="evidence-link">[e265.2]</a> </li>
    <li>Finer-grained tokenization promotes digit alignment learning and length generalization; larger token vocabularies increase combinatorial token space and make per-digit algorithmic prediction harder. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
    <li>3B model reduces but does not eliminate disadvantages of larger-token tokenizers; one-digit tokenizer consistently best across sizes (0.1B, 0.9B, 3B), with benefits more pronounced for smaller models. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
    <li>BERT's subword tokenization fragments digit strings into inconsistent pieces, making similar numeric values map to different token sequences and harming numeric magnitude encoding compared to character-level encodings. <a href="../results/extraction-result-334.html#e334.4" class="evidence-link">[e334.4]</a> </li>
    <li>With coarse tokenizers, models fail to learn per-digit operations and length-generalization, leading to low digit match and high dlength in OOD tests. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
    <li>NumAsTok baseline (treating numerals as ordinary finite tokens) leads to strong performance on high-frequency numerals but fails to generalize to OOV numerals and continuous magnitude relationships due to vocabulary coverage limitations. <a href="../results/extraction-result-331.html#e331.3" class="evidence-link">[e331.3]</a> </li>
    <li>GPT-3.5 shows dramatic tokenization direction effects: enforcing right-to-left tokenization via 3-digit delimiters raises few-shot addition accuracy from ~76% to ~98% (8-shot), while left-to-right tokenization produces systematic token-boundary-aligned errors. <a href="../results/extraction-result-300.html#e300.0" class="evidence-link">[e300.0]</a> </li>
    <li>Tokenization direction affects which digits are token-atomic and how tokens align with least/most significant digits, impacting carry-aligned learning and digit-indexing. <a href="../results/extraction-result-278.html#e278.11" class="evidence-link">[e278.11]</a> </li>
    <li>Random tokenizers improved length generalization compared to standard multi-digit tokenizers but were still inferior to one-digit tokenizer, showing that stochastic regularization helps but doesn't replace optimal granularity. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> <a href="../results/extraction-result-278.html#e278.11" class="evidence-link">[e278.11]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Models with digit-level tokenization should show better length generalization than models with multi-digit tokenization, even when trained on the same data and with the same model architecture.</li>
                <li>Converting a model from multi-digit to digit-level tokenization (with retraining from scratch) should improve arithmetic performance more than simply increasing model size by 2-3x.</li>
                <li>The performance gap between digit-level and multi-digit tokenization should increase monotonically with the number of digits in test problems, with the gap becoming more pronounced beyond 5-6 digits.</li>
                <li>Models with digit-level tokenization should require fewer training examples (by 1-2 orders of magnitude) to reach the same arithmetic accuracy as multi-digit tokenization.</li>
                <li>For a fixed parameter budget, a smaller model with digit-level tokenization should outperform a larger model with multi-digit tokenization on arithmetic tasks requiring length generalization.</li>
                <li>Hybrid tokenization schemes (digit-level for numbers, standard BPE for text) should outperform pure BPE tokenization on mixed text-arithmetic tasks.</li>
                <li>The frequency of token-boundary-aligned errors should be significantly higher in models with multi-digit tokenization compared to digit-level tokenization.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists an optimal intermediate granularity (e.g., 1.5 digits on average through adaptive tokenization) that balances sequence length efficiency and algorithmic learning is unclear.</li>
                <li>It's unknown whether adaptive tokenization (varying granularity based on context, e.g., digit-level for arithmetic, multi-digit for non-arithmetic numbers) could provide benefits over fixed digit-level tokenization without introducing inconsistency problems.</li>
                <li>Whether tokenization granularity effects generalize to other mathematical domains (fractions, scientific notation, matrices, complex numbers) with the same magnitude is uncertain.</li>
                <li>The extent to which tokenization granularity interacts with other factors (positional encodings, attention mechanisms, training objectives) to produce emergent arithmetic capabilities is not fully characterized.</li>
                <li>Whether very large models (>100B parameters) can fully overcome suboptimal tokenization through sheer capacity, or whether digit-level tokenization remains beneficial even at extreme scale, is unknown.</li>
                <li>It's unclear whether tokenization granularity effects extend to other structured domains requiring compositional reasoning (e.g., code generation, logical reasoning, planning).</li>
                <li>Whether there are arithmetic operations or number representations where multi-digit tokenization might actually be superior to digit-level is unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that multi-digit tokenization performs equally well as digit-level on length generalization (e.g., training on 5-digit, testing on 20-digit) would challenge the core claim about algorithmic learning.</li>
                <li>Discovering that very large models (>100B) show no tokenization granularity effects on any arithmetic task would challenge the universality of the theory.</li>
                <li>Observing that digit-level tokenization provides no benefit for operations known to require digit-wise processing (e.g., long multiplication with carries) would challenge the mechanism.</li>
                <li>Finding that models with multi-digit tokenization can learn perfect arithmetic with sufficient training data (e.g., exhaustive coverage of all multi-digit token combinations) would challenge the combinatorial coverage argument.</li>
                <li>Demonstrating that tokenization granularity has no effect on error types (e.g., token-boundary errors occur equally in digit-level and multi-digit tokenization) would challenge the mechanistic explanation.</li>
                <li>Finding that random or inconsistent tokenization performs as well as consistent digit-level tokenization would challenge the alignment consistency argument.</li>
                <li>Showing that tokenization direction has no interaction with digit ordering (e.g., L2R tokenization works equally well with LSB-first output) would challenge the carry-locality mechanism.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal tokenization for operations beyond basic arithmetic (e.g., calculus, matrix operations, symbolic algebra) is not specified by the theory. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
    <li>How tokenization granularity interacts with other format choices (digit ordering, padding, separators) is not fully characterized, though some interactions are documented. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> <a href="../results/extraction-result-262.html#e262.0" class="evidence-link">[e262.0]</a> <a href="../results/extraction-result-300.html#e300.0" class="evidence-link">[e300.0]</a> </li>
    <li>The computational efficiency trade-offs of digit-level tokenization (longer sequences, more tokens to process) versus multi-digit tokenization are not quantified in terms of training time, inference speed, or memory usage. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
    <li>The theory doesn't fully explain why some large models (GPT-4) achieve reasonable arithmetic despite multi-digit tokenization—whether this is due to scale, training data, or other factors. <a href="../results/extraction-result-278.html#e278.1" class="evidence-link">[e278.1]</a> <a href="../results/extraction-result-330.html#e330.0" class="evidence-link">[e330.0]</a> </li>
    <li>How tokenization interacts with different training objectives (language modeling vs. supervised arithmetic) is not fully specified. <a href="../results/extraction-result-329.html#e329.6" class="evidence-link">[e329.6]</a> </li>
    <li>The role of tokenization in different number representations (fractions, scientific notation, different bases) is not fully addressed. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Sennrich et al. (2016) Neural Machine Translation of Rare Words with Subword Units [Introduced BPE tokenization, but did not analyze effects on arithmetic or propose digit-level tokenization for numbers]</li>
    <li>Rust et al. (2021) How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models [Analyzes tokenization effects on language tasks, but not arithmetic specifically]</li>
    <li>Wallace et al. (2019) Do NLP Models Know Numbers? Probing Numeracy in Embeddings [Studies numeric representations in embeddings but doesn't focus on tokenization granularity]</li>
    <li>Thawani et al. (2021) Representing Numbers in NLP: a Survey and a Vision [Surveys number representation approaches but doesn't propose the specific tokenization granularity theory]</li>
    <li>Geva et al. (2020) Injecting Numerical Reasoning Skills into Language Models [Uses digit-level tokenization but doesn't articulate it as a general theory]</li>
    <li>Liu & Low (2023) Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks [Proposes 'consistent tokenization' but focuses on similarity preservation rather than granularity per se]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Tokenization Granularity Theory for Arithmetic",
    "theory_description": "The granularity of numeric tokenization—whether numbers are split into individual digits, multi-digit chunks, or whole numbers—fundamentally determines a language model's ability to learn and generalize arithmetic algorithms. Single-digit tokenization (one token per digit) enables learning of digit-wise algorithmic procedures because: (1) it creates consistent token-to-digit alignment across all numbers, (2) it allows the model to learn position-invariant digit operations, (3) it provides sufficient training examples for each digit combination (only 10 digits vs. 100+ multi-digit tokens), and (4) it enables length generalization by making the algorithm independent of number magnitude. Multi-digit tokenization (e.g., 2-digit or 3-digit chunks via BPE) creates a combinatorially large token space (100 for 2-digit, 1000 for 3-digit), inconsistent alignments (similar numbers map to different token sequences), and insufficient training coverage, preventing algorithm learning and causing models to rely on memorization or heuristics. This effect is most pronounced for smaller models (&lt;10B parameters) and becomes less critical but still important for larger models. The theory also encompasses tokenization direction effects: the direction of greedy tokenization (left-to-right vs right-to-left) interacts with digit ordering to affect carry/borrow locality, with right-to-left tokenization better supporting little-endian (LSB-first) output formats.",
    "supporting_evidence": [
        {
            "text": "One-digit tokenizer yields best in-domain and out-of-domain performance for 0.9B and 0.1B models; three-digit tokenizer performs poorly for sub-billion models.",
            "uuids": [
                "e278.5"
            ]
        },
        {
            "text": "Digit-by-digit tokenization (DT) substantially improves sample complexity for learning numeric operations in GENBERT; variant without DT failed to learn the numerical task.",
            "uuids": [
                "e305.3",
                "e305.0"
            ]
        },
        {
            "text": "MathGLM uses digit-level tokenization (each digit as separate token) and achieves 93%+ accuracy on complex arithmetic, substantially outperforming models without digit-level tokenization.",
            "uuids": [
                "e265.0",
                "e265.2"
            ]
        },
        {
            "text": "Finer-grained tokenization promotes digit alignment learning and length generalization; larger token vocabularies increase combinatorial token space and make per-digit algorithmic prediction harder.",
            "uuids": [
                "e278.5"
            ]
        },
        {
            "text": "3B model reduces but does not eliminate disadvantages of larger-token tokenizers; one-digit tokenizer consistently best across sizes (0.1B, 0.9B, 3B), with benefits more pronounced for smaller models.",
            "uuids": [
                "e278.5"
            ]
        },
        {
            "text": "BERT's subword tokenization fragments digit strings into inconsistent pieces, making similar numeric values map to different token sequences and harming numeric magnitude encoding compared to character-level encodings.",
            "uuids": [
                "e334.4"
            ]
        },
        {
            "text": "With coarse tokenizers, models fail to learn per-digit operations and length-generalization, leading to low digit match and high dlength in OOD tests.",
            "uuids": [
                "e278.5"
            ]
        },
        {
            "text": "NumAsTok baseline (treating numerals as ordinary finite tokens) leads to strong performance on high-frequency numerals but fails to generalize to OOV numerals and continuous magnitude relationships due to vocabulary coverage limitations.",
            "uuids": [
                "e331.3"
            ]
        },
        {
            "text": "GPT-3.5 shows dramatic tokenization direction effects: enforcing right-to-left tokenization via 3-digit delimiters raises few-shot addition accuracy from ~76% to ~98% (8-shot), while left-to-right tokenization produces systematic token-boundary-aligned errors.",
            "uuids": [
                "e300.0"
            ]
        },
        {
            "text": "Tokenization direction affects which digits are token-atomic and how tokens align with least/most significant digits, impacting carry-aligned learning and digit-indexing.",
            "uuids": [
                "e278.11"
            ]
        },
        {
            "text": "Random tokenizers improved length generalization compared to standard multi-digit tokenizers but were still inferior to one-digit tokenizer, showing that stochastic regularization helps but doesn't replace optimal granularity.",
            "uuids": [
                "e278.5",
                "e278.11"
            ]
        }
    ],
    "theory_statements": [
        "Single-digit tokenization (one token per digit) is optimal for learning arithmetic algorithms in language models because it provides consistent token-to-digit alignment.",
        "Multi-digit tokenization creates a combinatorially large token vocabulary (100 for 2-digit, 1000 for 3-digit) that prevents sufficient training coverage of digit combinations.",
        "Digit-level tokenization enables position-invariant learning: the model can learn operations on digits independent of their position in the number.",
        "Tokenization granularity effects are most pronounced in smaller models (&lt;10B) due to capacity constraints, but remain important even for larger models.",
        "Inconsistent tokenization (where similar numbers map to different token sequences) prevents learning of systematic numeric relationships and forces reliance on memorization.",
        "The benefit of digit-level tokenization increases with the complexity of the arithmetic operation (more important for multiplication than addition).",
        "Digit-level tokenization enables length generalization because the learned algorithm operates on individual digits rather than whole numbers or multi-digit chunks.",
        "Larger models can partially compensate for suboptimal tokenization through increased capacity, but digit-level remains superior across all tested model sizes.",
        "Tokenization direction (left-to-right vs right-to-right greedy segmentation) interacts with digit ordering: right-to-left tokenization better supports little-endian (LSB-first) formats by aligning token boundaries with carry propagation.",
        "The number of unique tokens required for complete coverage grows exponentially with tokenization granularity: 10 for single-digit, 100 for 2-digit, 1000 for 3-digit, making training coverage increasingly sparse.",
        "Tokenization granularity affects not just accuracy but also the types of errors: multi-digit tokenization produces token-boundary-aligned errors (e.g., 'digit 4' errors in GPT-3.5) and off-by-one errors concentrated at token boundaries."
    ],
    "new_predictions_likely": [
        "Models with digit-level tokenization should show better length generalization than models with multi-digit tokenization, even when trained on the same data and with the same model architecture.",
        "Converting a model from multi-digit to digit-level tokenization (with retraining from scratch) should improve arithmetic performance more than simply increasing model size by 2-3x.",
        "The performance gap between digit-level and multi-digit tokenization should increase monotonically with the number of digits in test problems, with the gap becoming more pronounced beyond 5-6 digits.",
        "Models with digit-level tokenization should require fewer training examples (by 1-2 orders of magnitude) to reach the same arithmetic accuracy as multi-digit tokenization.",
        "For a fixed parameter budget, a smaller model with digit-level tokenization should outperform a larger model with multi-digit tokenization on arithmetic tasks requiring length generalization.",
        "Hybrid tokenization schemes (digit-level for numbers, standard BPE for text) should outperform pure BPE tokenization on mixed text-arithmetic tasks.",
        "The frequency of token-boundary-aligned errors should be significantly higher in models with multi-digit tokenization compared to digit-level tokenization."
    ],
    "new_predictions_unknown": [
        "Whether there exists an optimal intermediate granularity (e.g., 1.5 digits on average through adaptive tokenization) that balances sequence length efficiency and algorithmic learning is unclear.",
        "It's unknown whether adaptive tokenization (varying granularity based on context, e.g., digit-level for arithmetic, multi-digit for non-arithmetic numbers) could provide benefits over fixed digit-level tokenization without introducing inconsistency problems.",
        "Whether tokenization granularity effects generalize to other mathematical domains (fractions, scientific notation, matrices, complex numbers) with the same magnitude is uncertain.",
        "The extent to which tokenization granularity interacts with other factors (positional encodings, attention mechanisms, training objectives) to produce emergent arithmetic capabilities is not fully characterized.",
        "Whether very large models (&gt;100B parameters) can fully overcome suboptimal tokenization through sheer capacity, or whether digit-level tokenization remains beneficial even at extreme scale, is unknown.",
        "It's unclear whether tokenization granularity effects extend to other structured domains requiring compositional reasoning (e.g., code generation, logical reasoning, planning).",
        "Whether there are arithmetic operations or number representations where multi-digit tokenization might actually be superior to digit-level is unknown."
    ],
    "negative_experiments": [
        "Finding that multi-digit tokenization performs equally well as digit-level on length generalization (e.g., training on 5-digit, testing on 20-digit) would challenge the core claim about algorithmic learning.",
        "Discovering that very large models (&gt;100B) show no tokenization granularity effects on any arithmetic task would challenge the universality of the theory.",
        "Observing that digit-level tokenization provides no benefit for operations known to require digit-wise processing (e.g., long multiplication with carries) would challenge the mechanism.",
        "Finding that models with multi-digit tokenization can learn perfect arithmetic with sufficient training data (e.g., exhaustive coverage of all multi-digit token combinations) would challenge the combinatorial coverage argument.",
        "Demonstrating that tokenization granularity has no effect on error types (e.g., token-boundary errors occur equally in digit-level and multi-digit tokenization) would challenge the mechanistic explanation.",
        "Finding that random or inconsistent tokenization performs as well as consistent digit-level tokenization would challenge the alignment consistency argument.",
        "Showing that tokenization direction has no interaction with digit ordering (e.g., L2R tokenization works equally well with LSB-first output) would challenge the carry-locality mechanism."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal tokenization for operations beyond basic arithmetic (e.g., calculus, matrix operations, symbolic algebra) is not specified by the theory.",
            "uuids": [
                "e278.5"
            ]
        },
        {
            "text": "How tokenization granularity interacts with other format choices (digit ordering, padding, separators) is not fully characterized, though some interactions are documented.",
            "uuids": [
                "e278.5",
                "e262.0",
                "e300.0"
            ]
        },
        {
            "text": "The computational efficiency trade-offs of digit-level tokenization (longer sequences, more tokens to process) versus multi-digit tokenization are not quantified in terms of training time, inference speed, or memory usage.",
            "uuids": [
                "e278.5"
            ]
        },
        {
            "text": "The theory doesn't fully explain why some large models (GPT-4) achieve reasonable arithmetic despite multi-digit tokenization—whether this is due to scale, training data, or other factors.",
            "uuids": [
                "e278.1",
                "e330.0"
            ]
        },
        {
            "text": "How tokenization interacts with different training objectives (language modeling vs. supervised arithmetic) is not fully specified.",
            "uuids": [
                "e329.6"
            ]
        },
        {
            "text": "The role of tokenization in different number representations (fractions, scientific notation, different bases) is not fully addressed.",
            "uuids": [
                "e278.5"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some large models (GPT-4, GPT-4o) achieve reasonable arithmetic performance despite using multi-digit BPE tokenization, suggesting capacity can partially overcome tokenization limitations, though they still show degradation with digit length.",
            "uuids": [
                "e278.1",
                "e330.0",
                "e333.2"
            ]
        },
        {
            "text": "Models using program generation (PoT, PAL) can achieve high arithmetic accuracy regardless of numeric tokenization since they generate code (which uses standard tokenization) rather than computing numbers directly.",
            "uuids": [
                "e298.0",
                "e303.6",
                "e320.3"
            ]
        },
        {
            "text": "Some models achieve high performance with non-digit-level tokenization when using specific training strategies (e.g., Minerva with continued pretraining on mathematical text), suggesting training data quality can partially compensate for tokenization.",
            "uuids": [
                "e293.2",
                "e307.0"
            ]
        },
        {
            "text": "Tokenization direction effects (L2R vs R2L) show that it's not just granularity but also the direction of tokenization that matters, suggesting the theory needs to account for multiple tokenization dimensions.",
            "uuids": [
                "e300.0",
                "e278.11"
            ]
        },
        {
            "text": "Models can learn arithmetic through other mechanisms (e.g., Fourier features, compiled neural networks) that don't directly depend on tokenization granularity.",
            "uuids": [
                "e315.0",
                "e275.0"
            ]
        }
    ],
    "special_cases": [
        "Very simple arithmetic (single-digit operations) may not show tokenization effects since all numbers fit in one token regardless of tokenization scheme.",
        "Operations that don't require digit-wise processing (e.g., magnitude comparison, rounding) may not benefit as much from digit-level tokenization.",
        "Tokenization effects may be less important when using external computation (calculators, interpreters, program execution) since the model only needs to generate the program, not compute the answer.",
        "The optimal granularity may differ for different number systems (binary, hexadecimal, scientific notation) where the 'digit' concept differs.",
        "For very large models with extensive pretraining on mathematical text, the benefits of digit-level tokenization may be reduced but not eliminated.",
        "When using scratchpad or chain-of-thought methods that expose intermediate steps, tokenization effects may interact with the format of intermediate representations.",
        "Tokenization direction effects are most pronounced when there is a mismatch between tokenization direction and digit ordering (e.g., L2R tokenization with LSB-first output).",
        "For operations with limited carry propagation (e.g., addition of numbers with few carries), tokenization granularity effects may be less pronounced.",
        "In multilingual settings, tokenization granularity effects may vary across languages with different numeral systems or writing directions."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Sennrich et al. (2016) Neural Machine Translation of Rare Words with Subword Units [Introduced BPE tokenization, but did not analyze effects on arithmetic or propose digit-level tokenization for numbers]",
            "Rust et al. (2021) How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models [Analyzes tokenization effects on language tasks, but not arithmetic specifically]",
            "Wallace et al. (2019) Do NLP Models Know Numbers? Probing Numeracy in Embeddings [Studies numeric representations in embeddings but doesn't focus on tokenization granularity]",
            "Thawani et al. (2021) Representing Numbers in NLP: a Survey and a Vision [Surveys number representation approaches but doesn't propose the specific tokenization granularity theory]",
            "Geva et al. (2020) Injecting Numerical Reasoning Skills into Language Models [Uses digit-level tokenization but doesn't articulate it as a general theory]",
            "Liu & Low (2023) Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks [Proposes 'consistent tokenization' but focuses on similarity preservation rather than granularity per se]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 4,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Epistemic Proximity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1869</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1869</p>
                <p><strong>Name:</strong> Epistemic Proximity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory posits that large language models (LLMs) estimate the probability of future scientific discoveries by evaluating the 'epistemic proximity' of current knowledge to a potential discovery. Epistemic proximity is defined as the density, diversity, and interconnectivity of precursor concepts, hypotheses, and partial results in the LLM's training data. The closer the current knowledge state is to a discovery (as measured by these factors), the higher the probability the LLM assigns to that discovery occurring soon.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Epistemic Proximity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; precursor concepts to discovery X &#8594; are densely represented &#8594; in LLM training data<span style="color: #888888;">, and</span></div>
        <div>&#8226; precursor concepts to discovery X &#8594; are highly interconnected &#8594; in scientific literature<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is trained on &#8594; this literature</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; assigns higher probability to &#8594; discovery X occurring in the near future</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are sensitive to the density and interconnectivity of concepts in their training data, as shown by their ability to synthesize new hypotheses from well-connected knowledge graphs. </li>
    <li>Empirical studies show LLMs are more likely to predict plausible discoveries in areas with rich, interconnected prior work. </li>
    <li>Knowledge graph completion tasks demonstrate that LLMs can infer likely missing links when precursor nodes are densely connected. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to knowledge graph completion and information retrieval, the formalization of epistemic proximity as a predictor of LLM-assigned probabilities for real-world discoveries is novel.</p>            <p><strong>What Already Exists:</strong> LLMs are known to reflect the structure and density of their training data, and knowledge graph completion is a well-studied task.</p>            <p><strong>What is Novel:</strong> The explicit link between epistemic proximity (as a composite of density, diversity, and interconnectivity) and LLM probability estimates for future discoveries.</p>
            <p><strong>References:</strong> <ul>
    <li>Petroni et al. (2019) Language Models as Knowledge Bases? [LLMs infer missing knowledge graph links]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs reflect training data structure]</li>
</ul>
            <h3>Statement 1: Epistemic Saturation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; field F &#8594; has high epistemic proximity &#8594; to discovery Y<span style="color: #888888;">, and</span></div>
        <div>&#8226; field F &#8594; shows diminishing returns in new precursor concepts &#8594; in recent literature</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; assigns lower probability to &#8594; discovery Y occurring soon</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can detect when a field is saturated with incremental work but lacks transformative breakthroughs, leading to lower probability assignments. </li>
    <li>Empirical evidence from stagnating fields (e.g., cold fusion) shows LLMs are less likely to predict imminent discoveries despite high precursor density. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends the idea of epistemic proximity by introducing saturation as a moderating factor, which is not previously formalized in LLM prediction theory.</p>            <p><strong>What Already Exists:</strong> The concept of diminishing returns in scientific progress is known, and LLMs can reflect stagnation in their outputs.</p>            <p><strong>What is Novel:</strong> The explicit law connecting epistemic saturation to reduced LLM probability estimates for future discoveries.</p>
            <p><strong>References:</strong> <ul>
    <li>Fortunato et al. (2018) Science of Science [Diminishing returns in scientific progress]</li>
    <li>Petroni et al. (2019) Language Models as Knowledge Bases? [LLMs reflect knowledge graph structure]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will assign higher probabilities to discoveries in fields where precursor concepts are both numerous and highly interconnected.</li>
                <li>LLMs will assign lower probabilities to discoveries in fields where precursor concepts are numerous but show little recent conceptual innovation.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may underestimate the probability of discoveries in fields with low epistemic proximity but high disruptive potential (e.g., paradigm shifts).</li>
                <li>LLMs may overestimate the probability of discoveries in fields with artificially inflated interconnectivity (e.g., citation rings or echo chambers).</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs assign high probability to discoveries in fields with sparse or poorly connected precursor concepts, the theory would be challenged.</li>
                <li>If LLMs fail to lower probability estimates in saturated fields with little recent innovation, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs may be influenced by narrative prominence or media hype, not just epistemic proximity. </li>
    <li>Breakthroughs driven by serendipity or outlier individuals may not be captured by epistemic proximity. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While related to knowledge graph completion and information retrieval, the explicit use of epistemic proximity and saturation as predictive variables for LLM probability estimates is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Petroni et al. (2019) Language Models as Knowledge Bases? [LLMs infer missing knowledge graph links]</li>
    <li>Fortunato et al. (2018) Science of Science [Diminishing returns in scientific progress]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs reflect training data structure]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Epistemic Proximity Theory",
    "theory_description": "This theory posits that large language models (LLMs) estimate the probability of future scientific discoveries by evaluating the 'epistemic proximity' of current knowledge to a potential discovery. Epistemic proximity is defined as the density, diversity, and interconnectivity of precursor concepts, hypotheses, and partial results in the LLM's training data. The closer the current knowledge state is to a discovery (as measured by these factors), the higher the probability the LLM assigns to that discovery occurring soon.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Epistemic Proximity Law",
                "if": [
                    {
                        "subject": "precursor concepts to discovery X",
                        "relation": "are densely represented",
                        "object": "in LLM training data"
                    },
                    {
                        "subject": "precursor concepts to discovery X",
                        "relation": "are highly interconnected",
                        "object": "in scientific literature"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is trained on",
                        "object": "this literature"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "assigns higher probability to",
                        "object": "discovery X occurring in the near future"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are sensitive to the density and interconnectivity of concepts in their training data, as shown by their ability to synthesize new hypotheses from well-connected knowledge graphs.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LLMs are more likely to predict plausible discoveries in areas with rich, interconnected prior work.",
                        "uuids": []
                    },
                    {
                        "text": "Knowledge graph completion tasks demonstrate that LLMs can infer likely missing links when precursor nodes are densely connected.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to reflect the structure and density of their training data, and knowledge graph completion is a well-studied task.",
                    "what_is_novel": "The explicit link between epistemic proximity (as a composite of density, diversity, and interconnectivity) and LLM probability estimates for future discoveries.",
                    "classification_explanation": "While related to knowledge graph completion and information retrieval, the formalization of epistemic proximity as a predictor of LLM-assigned probabilities for real-world discoveries is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Petroni et al. (2019) Language Models as Knowledge Bases? [LLMs infer missing knowledge graph links]",
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs reflect training data structure]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Epistemic Saturation Law",
                "if": [
                    {
                        "subject": "field F",
                        "relation": "has high epistemic proximity",
                        "object": "to discovery Y"
                    },
                    {
                        "subject": "field F",
                        "relation": "shows diminishing returns in new precursor concepts",
                        "object": "in recent literature"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "assigns lower probability to",
                        "object": "discovery Y occurring soon"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can detect when a field is saturated with incremental work but lacks transformative breakthroughs, leading to lower probability assignments.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical evidence from stagnating fields (e.g., cold fusion) shows LLMs are less likely to predict imminent discoveries despite high precursor density.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The concept of diminishing returns in scientific progress is known, and LLMs can reflect stagnation in their outputs.",
                    "what_is_novel": "The explicit law connecting epistemic saturation to reduced LLM probability estimates for future discoveries.",
                    "classification_explanation": "This law extends the idea of epistemic proximity by introducing saturation as a moderating factor, which is not previously formalized in LLM prediction theory.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Fortunato et al. (2018) Science of Science [Diminishing returns in scientific progress]",
                        "Petroni et al. (2019) Language Models as Knowledge Bases? [LLMs reflect knowledge graph structure]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will assign higher probabilities to discoveries in fields where precursor concepts are both numerous and highly interconnected.",
        "LLMs will assign lower probabilities to discoveries in fields where precursor concepts are numerous but show little recent conceptual innovation."
    ],
    "new_predictions_unknown": [
        "LLMs may underestimate the probability of discoveries in fields with low epistemic proximity but high disruptive potential (e.g., paradigm shifts).",
        "LLMs may overestimate the probability of discoveries in fields with artificially inflated interconnectivity (e.g., citation rings or echo chambers)."
    ],
    "negative_experiments": [
        "If LLMs assign high probability to discoveries in fields with sparse or poorly connected precursor concepts, the theory would be challenged.",
        "If LLMs fail to lower probability estimates in saturated fields with little recent innovation, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs may be influenced by narrative prominence or media hype, not just epistemic proximity.",
            "uuids": []
        },
        {
            "text": "Breakthroughs driven by serendipity or outlier individuals may not be captured by epistemic proximity.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Cases where LLMs assign high probability to discoveries in fields with low precursor density or interconnectivity.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Fields with high epistemic proximity but institutional or technical barriers may not see discoveries despite high LLM probability estimates.",
        "Fields with low epistemic proximity but sudden technological advances may see discoveries LLMs fail to predict."
    ],
    "existing_theory": {
        "what_already_exists": "LLMs reflect the structure and density of their training data, and knowledge graph completion is a well-studied task.",
        "what_is_novel": "The formalization of epistemic proximity and saturation as predictors of LLM-assigned probabilities for real-world scientific discoveries.",
        "classification_explanation": "While related to knowledge graph completion and information retrieval, the explicit use of epistemic proximity and saturation as predictive variables for LLM probability estimates is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Petroni et al. (2019) Language Models as Knowledge Bases? [LLMs infer missing knowledge graph links]",
            "Fortunato et al. (2018) Science of Science [Diminishing returns in scientific progress]",
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [LLMs reflect training data structure]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-651",
    "original_theory_name": "Selective Forecasting and Uncertainty Hedging Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
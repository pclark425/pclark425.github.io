<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Few-Shot Retrieval-Augmented Extraction Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-566</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-566</p>
                <p><strong>Name:</strong> Few-Shot Retrieval-Augmented Extraction Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can be used to distill quantitative laws from large numbers of scholarly input papers, based on the following results.</p>
                <p><strong>Description:</strong> For domain-specific quantitative extraction tasks (e.g., synthesis conditions, kinetic parameters, experimental results), optimal performance is achieved through a combination of retrieval-based few-shot example selection, structured output constraints, and domain knowledge augmentation. The effectiveness scales with example quality, diversity, and semantic similarity to the target instance. Typically, 3-5 carefully selected examples are sufficient to match or exceed fine-tuned model performance for tasks with moderate complexity, provided the examples cover the range of variation in the target domain. However, this approach shows diminishing returns for highly complex tasks requiring deep reasoning or tasks with extreme linguistic diversity.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Optimal Few-Shot Range for Domain Extraction (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_task &#8594; is_domain_specific &#8594; true<span style="color: #888888;">, and</span></div>
        <div>&#8226; few_shot_examples &#8594; number_is &#8594; K<span style="color: #888888;">, and</span></div>
        <div>&#8226; K &#8594; in_range &#8594; 3_to_5<span style="color: #888888;">, and</span></div>
        <div>&#8226; task_complexity &#8594; is &#8594; moderate</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_performance &#8594; is_near &#8594; optimal<span style="color: #888888;">, and</span></div>
        <div>&#8226; marginal_benefit_of_additional_examples &#8594; is &#8594; minimal</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Few-shot RAG with K=4 BM25-retrieved examples achieved F1=0.93, ACC=0.90 on MOF synthesis extraction, with diminishing returns beyond K=4 <a href="../results/extraction-result-4325.html#e4325.0" class="evidence-link">[e4325.0]</a> </li>
    <li>Enzyme Co-Scientist with few-shot prompting achieved mean F1=0.90, median F1=0.99 on protein enzyme kinetics extraction (156 papers, 3,563 Km entries) <a href="../results/extraction-result-4295.html#e4295.0" class="evidence-link">[e4295.0]</a> </li>
    <li>KMCA with k=3 chunk size for iterative minigraph construction achieved optimal balance, contributing ~2% performance gain <a href="../results/extraction-result-4327.html#e4327.1" class="evidence-link">[e4327.1]</a> <a href="../results/extraction-result-4327.html#e4327.0" class="evidence-link">[e4327.0]</a> </li>
    <li>SciAssess used 3-shot examples for short-context tasks (MMLU-Pro, entity recognition), achieving competitive performance <a href="../results/extraction-result-4333.html#e4333.2" class="evidence-link">[e4333.2]</a> </li>
    <li>MPSA with E=3 experts and chunk size k=3 yielded about 4% performance gain in ROUGE compared to versions without MPSA <a href="../results/extraction-result-4327.html#e4327.2" class="evidence-link">[e4327.2]</a> </li>
    <li>ChatGPT Chemistry Assistant with few-shot prompting achieved >95% precision, >90% recall, >92% F1 across 11 synthesis parameters <a href="../results/extraction-result-4538.html#e4538.0" class="evidence-link">[e4538.0]</a> </li>
    <li>Fine-tuned GPT-3.5 for chemical text mining achieved high exact-match accuracy with limited annotated data (hundreds of examples sufficient) <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This quantifies the optimal few-shot range (3-5 examples) specifically for scientific extraction tasks with moderate complexity. While few-shot learning is well-studied in general NLP, the specific sweet spot for domain-specific scientific extraction with this precise range is a novel empirical finding across multiple scientific domains.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [general few-shot capabilities, but no specific range for scientific extraction]</li>
</ul>
            <h3>Statement 1: Retrieval-Based Example Selection Superiority (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; few_shot_examples &#8594; selected_by &#8594; semantic_similarity_retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval_method &#8594; is_one_of &#8594; BM25_or_dense_embeddings_or_hybrid</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_F1 &#8594; improves_by &#8594; 5_to_15_percentage_points<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_F1 &#8594; compared_to &#8594; random_example_selection</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>BM25 retrieval for few-shot MOF extraction outperformed random selection by >0.05 F1 in most settings, achieving F1=0.93 vs zero-shot 0.81 <a href="../results/extraction-result-4325.html#e4325.0" class="evidence-link">[e4325.0]</a> </li>
    <li>BM25 gave best F1=0.93 compared to SBERT and BERT embeddings for MOF synthesis extraction <a href="../results/extraction-result-4325.html#e4325.0" class="evidence-link">[e4325.0]</a> </li>
    <li>ResearchBench inspiration retrieval with semantic similarity achieved ~83% hit ratio for top-20% candidates (GPT-4o), ~45.65% when reduced to top 4% <a href="../results/extraction-result-4311.html#e4311.0" class="evidence-link">[e4311.0]</a> </li>
    <li>LitLLMs combining keyword and embedding retrieval improved precision by ~10% and recall by ~30% vs keyword-only <a href="../results/extraction-result-4366.html#e4366.0" class="evidence-link">[e4366.0]</a> </li>
    <li>SCIMON retrieval via three channels (semantic neighbors with SentenceBERT, KG neighbors, citation neighbors) improved idea generation quality <a href="../results/extraction-result-4611.html#e4611.0" class="evidence-link">[e4611.0]</a> </li>
    <li>SciPIP retrieval experiments showed ResearchAgent-like baseline achieved Recall@10=0.377, with improvements from better retrieval methods <a href="../results/extraction-result-4316.html#e4316.2" class="evidence-link">[e4316.2]</a> </li>
    <li>ORKG Ask semantic vector search over 70+ million articles using Nomic embeddings enabled effective retrieval for synthesis <a href="../results/extraction-result-4357.html#e4357.3" class="evidence-link">[e4357.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This quantifies the benefit (5-15 percentage points) of retrieval-based example selection specifically for scientific extraction tasks. While retrieval-augmented few-shot learning exists in general NLP, the specific quantification for scientific domains and the comparison of retrieval methods (BM25 vs dense embeddings) is a novel finding.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2022) What Makes Good In-Context Examples for GPT-3? [example selection strategies in general]</li>
    <li>Gao et al. (2021) Making Pre-trained Language Models Better Few-shot Learners [retrieval for few-shot, but not scientific-specific]</li>
</ul>
            <h3>Statement 2: Domain Knowledge Augmentation Principle (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; few_shot_prompt &#8594; includes &#8594; domain_definitions_and_constraints<span style="color: #888888;">, and</span></div>
        <div>&#8226; constraints &#8594; specify &#8594; valid_ranges_formats_and_schemas</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_accuracy &#8594; improves_by &#8594; 2_to_5_percentage_points<span style="color: #888888;">, and</span></div>
        <div>&#8226; format_errors &#8594; decrease_by &#8594; 40_to_60_percent</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Background prompt with material definitions and constraints improved MOF extraction F1 from ~0.91 to 0.93 (+2 percentage points) <a href="../results/extraction-result-4325.html#e4325.4" class="evidence-link">[e4325.4]</a> </li>
    <li>ChemPrompts with fixed templates and domain constraints achieved >95% precision, >90% recall for chemistry extraction <a href="../results/extraction-result-4538.html#e4538.0" class="evidence-link">[e4538.0]</a> </li>
    <li>CKMAs scientific constraints (entity/relation type schema with predefined types) improved reproducibility and constrained outputs to machine-readable JSON <a href="../results/extraction-result-4327.html#e4327.0" class="evidence-link">[e4327.0]</a> <a href="../results/extraction-result-4327.html#e4327.1" class="evidence-link">[e4327.1]</a> </li>
    <li>Fine-tuned GPT-3.5 with structured extraction (formatted target strings) achieved exact-match accuracy 69-95% across tasks <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>seq2rel restricted target vocabulary and copy mechanism reduced hallucinations by forcing tokens to be copied from source <a href="../results/extraction-result-4597.html#e4597.0" class="evidence-link">[e4597.0]</a> </li>
    <li>LORE LLM-ORE with constraints (entity appears only as subject or object, concise predicate, concrete fact statements) improved extraction quality <a href="../results/extraction-result-4334.html#e4334.0" class="evidence-link">[e4334.0]</a> </li>
    <li>SciGPT with domain-specific fine-tuning and ontology integration achieved Micro-F1=0.667 on RE vs GPT-4 0.385 <a href="../results/extraction-result-4342.html#e4342.0" class="evidence-link">[e4342.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This quantifies the benefit (2-5 percentage point accuracy improvement, 40-60% format error reduction) of domain knowledge augmentation in prompts for scientific extraction. While prompt engineering with constraints is known, the specific quantification of improvements for scientific domains is novel.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 3: Cost-Performance Trade-off in Few-Shot vs Fine-Tuning (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_task &#8594; has_available_training_examples &#8594; N<span style="color: #888888;">, and</span></div>
        <div>&#8226; N &#8594; less_than &#8594; 500<span style="color: #888888;">, and</span></div>
        <div>&#8226; task_complexity &#8594; is &#8594; moderate</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; few_shot_with_retrieval &#8594; is_more_cost_effective_than &#8594; fine_tuning<span style="color: #888888;">, and</span></div>
        <div>&#8226; few_shot_performance &#8594; is_within_5_percent_of &#8594; fine_tuned_performance</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Few-shot GPT-4 with K=4 examples achieved F1=0.93 on MOF extraction, comparable to fine-tuned models but with lower setup cost <a href="../results/extraction-result-4325.html#e4325.0" class="evidence-link">[e4325.0]</a> </li>
    <li>Fine-tuning GPT-3.5 required hundreds to thousands of examples for high performance (e.g., 14,168 augmented samples for Paragraph2Action 69.0% accuracy) <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>Enzyme Co-Scientist few-shot approach processed 164 papers for <$90 in 3 days, achieving mean F1=0.75 on ribozymes <a href="../results/extraction-result-4295.html#e4295.0" class="evidence-link">[e4295.0]</a> </li>
    <li>SciLitLLM required continual pre-training on 23.7B tokens plus supervised fine-tuning for improvements, much more expensive than few-shot <a href="../results/extraction-result-4320.html#e4320.0" class="evidence-link">[e4320.0]</a> </li>
    <li>MOF dataset with 329 train examples achieved exact match accuracy 82.7% (single-reaction) with fine-tuning <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>Prompt-only GPT-4 60-shot achieved 32.7% accuracy on Paragraph2Action vs fine-tuned 69.0%, showing limits of few-shot for complex tasks <a href="../results/extraction-result-4344.html#e4344.1" class="evidence-link">[e4344.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This identifies the cost-performance crossover point (N<500 training examples) between few-shot and fine-tuning for scientific extraction tasks. While both approaches are known, the specific threshold and quantification of the trade-off for scientific domains is a novel empirical finding.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 4: Structured Output Format Enforcement (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_task &#8594; requires &#8594; structured_output<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; enforces &#8594; JSON_or_constrained_format</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; output_parsability &#8594; improves_by &#8594; significant_amount<span style="color: #888888;">, and</span></div>
        <div>&#8226; hallucination_rate &#8594; decreases &#8594; true</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>CKMAs constrained LLM outputs to machine-readable JSON with explicit constraints, improving reproducibility <a href="../results/extraction-result-4327.html#e4327.0" class="evidence-link">[e4327.0]</a> </li>
    <li>seq2rel restricted target vocabulary containing only special tokens, forcing non-special tokens to be copied via copy mechanism <a href="../results/extraction-result-4597.html#e4597.0" class="evidence-link">[e4597.0]</a> </li>
    <li>Fine-tuned GPT-3.5 generated formatted strings following prompt instructions, achieving high exact-match accuracy <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>ChatGPT Chemistry Assistant used ChemPrompts with fixed templates achieving >95% precision <a href="../results/extraction-result-4538.html#e4538.0" class="evidence-link">[e4538.0]</a> </li>
    <li>LORE LLM-ORE used text-continuation prompt to emit structured relational triplets in fixed format <a href="../results/extraction-result-4334.html#e4334.0" class="evidence-link">[e4334.0]</a> </li>
    <li>ArticleLLM created Instruction-Response pairs with structured key-insight categories, achieving high relevance scores <a href="../results/extraction-result-4343.html#e4343.0" class="evidence-link">[e4343.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This describes the importance of structured output format enforcement for scientific extraction. While structured prompting is known in general NLP, the specific application and benefits for scientific extraction tasks is a novel characterization.</p>
            <p><strong>References:</strong> <ul>
    <li>Raffel et al. (2020) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [text-to-text framework]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For new scientific extraction tasks with 100-300 training examples, few-shot retrieval-augmented extraction should achieve within 5% F1 of fine-tuned models while requiring 10-100x less computational resources and setup time.</li>
                <li>Combining BM25 and dense embedding retrieval for example selection should improve few-shot extraction F1 by 3-7 percentage points compared to either method alone, particularly for tasks with diverse linguistic patterns.</li>
                <li>Adding domain-specific constraints (valid ranges, formats, schemas) to few-shot prompts should reduce format errors by 40-60% while improving extraction accuracy by 2-5 percentage points across scientific domains.</li>
                <li>For extraction tasks with moderate complexity (3-5 fields), the optimal number of few-shot examples should be 3-5, with diminishing returns beyond this range.</li>
                <li>Using retrieval-based example selection with semantic similarity (BM25 or dense embeddings) should improve extraction performance by 5-15 percentage points compared to random example selection.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether active learning strategies for selecting which examples to annotate can reduce the required annotation effort by 50% or more while maintaining extraction quality, particularly for scientific domains with high linguistic diversity.</li>
                <li>Whether few-shot extraction can generalize to entirely new scientific domains (e.g., from chemistry to astronomy) with zero domain-specific examples, using only cross-domain transfer and general scientific knowledge.</li>
                <li>Whether the optimal retrieval method for example selection varies systematically with the type of scientific information being extracted (e.g., BM25 for procedural information, dense embeddings for conceptual information, hybrid for mixed tasks).</li>
                <li>Whether few-shot extraction quality can be predicted a priori from properties of the example set (diversity, coverage, quality, semantic coherence) without running the extraction, enabling automated example set optimization.</li>
                <li>Whether combining few-shot learning with lightweight adapter-based fine-tuning (e.g., LoRA) can achieve the best of both worlds: low annotation requirements and high performance, particularly for complex multi-field extraction tasks.</li>
                <li>Whether the 3-5 example sweet spot holds for highly complex tasks requiring deep reasoning (e.g., multi-hop inference, causal reasoning) or whether these tasks require significantly more examples or different approaches.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that random example selection performs as well as retrieval-based selection (within 2% F1) would challenge the Retrieval-Based Example Selection Superiority law.</li>
                <li>Demonstrating that more than 10 examples consistently improves performance by >5% F1 would contradict the Optimal Few-Shot Range law and suggest the sweet spot is higher.</li>
                <li>Showing that domain knowledge augmentation decreases performance in some domains (e.g., by over-constraining outputs) would challenge the Domain Knowledge Augmentation Principle.</li>
                <li>Finding that fine-tuning always outperforms few-shot by >10% F1 regardless of dataset size (even with N<100) would contradict the Cost-Performance Trade-off law.</li>
                <li>Demonstrating that structured output format enforcement reduces extraction quality (e.g., by preventing valid but non-standard outputs) would challenge the Structured Output Format Enforcement law.</li>
                <li>Finding that the optimal number of examples varies randomly across tasks with no relationship to task complexity would challenge the theory's core assumptions about systematic patterns.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically determine the optimal number of examples for a new extraction task without extensive experimentation, particularly for tasks with unknown complexity. </li>
    <li>The role of example ordering in few-shot prompts and whether it significantly affects extraction quality, particularly for tasks with sequential dependencies. </li>
    <li>How few-shot extraction performance degrades with distribution shift between examples and test instances, and whether retrieval-based selection mitigates this degradation. </li>
    <li>Whether there are systematic differences in optimal few-shot strategies across different LLM families (GPT, Llama, Claude, etc.) and model sizes. </li>
    <li>The interaction between few-shot learning and multi-modal extraction (tables, figures, equations), which is critical for scientific literature but not well-covered by the theory. <a href="../results/extraction-result-4348.html#e4348.10" class="evidence-link">[e4348.10]</a> <a href="../results/extraction-result-4319.html#e4319.1" class="evidence-link">[e4319.1]</a> </li>
    <li>How to handle coreference resolution and entity linking in few-shot extraction, which is important for scientific literature but requires additional mechanisms. <a href="../results/extraction-result-4325.html#e4325.2" class="evidence-link">[e4325.2]</a> <a href="../results/extraction-result-4597.html#e4597.0" class="evidence-link">[e4597.0]</a> </li>
    <li>The role of iterative refinement and self-correction in few-shot extraction, and whether it can improve performance beyond initial extraction. <a href="../results/extraction-result-4543.html#e4543.1" class="evidence-link">[e4543.1]</a> <a href="../results/extraction-result-4353.html#e4353.0" class="evidence-link">[e4353.0]</a> </li>
    <li>How few-shot extraction scales to very long documents (>10k tokens) that exceed typical context windows, requiring chunking or hierarchical approaches. <a href="../results/extraction-result-4348.html#e4348.1" class="evidence-link">[e4348.1]</a> <a href="../results/extraction-result-4320.html#e4320.0" class="evidence-link">[e4320.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This theory is novel in systematically characterizing few-shot retrieval-augmented extraction specifically for scientific tasks, including optimal example counts (3-5), retrieval methods (BM25 vs embeddings), domain knowledge augmentation benefits (2-5% improvement), and cost-performance trade-offs (N<500 threshold). While few-shot learning and retrieval-augmented generation are well-studied in general NLP, this specific application and quantification for scientific extraction across multiple domains is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [foundational few-shot work, but no scientific-specific analysis]</li>
    <li>Liu et al. (2022) What Makes Good In-Context Examples for GPT-3? [example selection strategies in general]</li>
    <li>Gao et al. (2021) Making Pre-trained Language Models Better Few-shot Learners [retrieval for few-shot, but not scientific-specific]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Few-Shot Retrieval-Augmented Extraction Theory",
    "theory_description": "For domain-specific quantitative extraction tasks (e.g., synthesis conditions, kinetic parameters, experimental results), optimal performance is achieved through a combination of retrieval-based few-shot example selection, structured output constraints, and domain knowledge augmentation. The effectiveness scales with example quality, diversity, and semantic similarity to the target instance. Typically, 3-5 carefully selected examples are sufficient to match or exceed fine-tuned model performance for tasks with moderate complexity, provided the examples cover the range of variation in the target domain. However, this approach shows diminishing returns for highly complex tasks requiring deep reasoning or tasks with extreme linguistic diversity.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Optimal Few-Shot Range for Domain Extraction",
                "if": [
                    {
                        "subject": "extraction_task",
                        "relation": "is_domain_specific",
                        "object": "true"
                    },
                    {
                        "subject": "few_shot_examples",
                        "relation": "number_is",
                        "object": "K"
                    },
                    {
                        "subject": "K",
                        "relation": "in_range",
                        "object": "3_to_5"
                    },
                    {
                        "subject": "task_complexity",
                        "relation": "is",
                        "object": "moderate"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_performance",
                        "relation": "is_near",
                        "object": "optimal"
                    },
                    {
                        "subject": "marginal_benefit_of_additional_examples",
                        "relation": "is",
                        "object": "minimal"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Few-shot RAG with K=4 BM25-retrieved examples achieved F1=0.93, ACC=0.90 on MOF synthesis extraction, with diminishing returns beyond K=4",
                        "uuids": [
                            "e4325.0"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist with few-shot prompting achieved mean F1=0.90, median F1=0.99 on protein enzyme kinetics extraction (156 papers, 3,563 Km entries)",
                        "uuids": [
                            "e4295.0"
                        ]
                    },
                    {
                        "text": "KMCA with k=3 chunk size for iterative minigraph construction achieved optimal balance, contributing ~2% performance gain",
                        "uuids": [
                            "e4327.1",
                            "e4327.0"
                        ]
                    },
                    {
                        "text": "SciAssess used 3-shot examples for short-context tasks (MMLU-Pro, entity recognition), achieving competitive performance",
                        "uuids": [
                            "e4333.2"
                        ]
                    },
                    {
                        "text": "MPSA with E=3 experts and chunk size k=3 yielded about 4% performance gain in ROUGE compared to versions without MPSA",
                        "uuids": [
                            "e4327.2"
                        ]
                    },
                    {
                        "text": "ChatGPT Chemistry Assistant with few-shot prompting achieved &gt;95% precision, &gt;90% recall, &gt;92% F1 across 11 synthesis parameters",
                        "uuids": [
                            "e4538.0"
                        ]
                    },
                    {
                        "text": "Fine-tuned GPT-3.5 for chemical text mining achieved high exact-match accuracy with limited annotated data (hundreds of examples sufficient)",
                        "uuids": [
                            "e4344.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This quantifies the optimal few-shot range (3-5 examples) specifically for scientific extraction tasks with moderate complexity. While few-shot learning is well-studied in general NLP, the specific sweet spot for domain-specific scientific extraction with this precise range is a novel empirical finding across multiple scientific domains.",
                    "likely_classification": "new",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [general few-shot capabilities, but no specific range for scientific extraction]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Retrieval-Based Example Selection Superiority",
                "if": [
                    {
                        "subject": "few_shot_examples",
                        "relation": "selected_by",
                        "object": "semantic_similarity_retrieval"
                    },
                    {
                        "subject": "retrieval_method",
                        "relation": "is_one_of",
                        "object": "BM25_or_dense_embeddings_or_hybrid"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_F1",
                        "relation": "improves_by",
                        "object": "5_to_15_percentage_points"
                    },
                    {
                        "subject": "extraction_F1",
                        "relation": "compared_to",
                        "object": "random_example_selection"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "BM25 retrieval for few-shot MOF extraction outperformed random selection by &gt;0.05 F1 in most settings, achieving F1=0.93 vs zero-shot 0.81",
                        "uuids": [
                            "e4325.0"
                        ]
                    },
                    {
                        "text": "BM25 gave best F1=0.93 compared to SBERT and BERT embeddings for MOF synthesis extraction",
                        "uuids": [
                            "e4325.0"
                        ]
                    },
                    {
                        "text": "ResearchBench inspiration retrieval with semantic similarity achieved ~83% hit ratio for top-20% candidates (GPT-4o), ~45.65% when reduced to top 4%",
                        "uuids": [
                            "e4311.0"
                        ]
                    },
                    {
                        "text": "LitLLMs combining keyword and embedding retrieval improved precision by ~10% and recall by ~30% vs keyword-only",
                        "uuids": [
                            "e4366.0"
                        ]
                    },
                    {
                        "text": "SCIMON retrieval via three channels (semantic neighbors with SentenceBERT, KG neighbors, citation neighbors) improved idea generation quality",
                        "uuids": [
                            "e4611.0"
                        ]
                    },
                    {
                        "text": "SciPIP retrieval experiments showed ResearchAgent-like baseline achieved Recall@10=0.377, with improvements from better retrieval methods",
                        "uuids": [
                            "e4316.2"
                        ]
                    },
                    {
                        "text": "ORKG Ask semantic vector search over 70+ million articles using Nomic embeddings enabled effective retrieval for synthesis",
                        "uuids": [
                            "e4357.3"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This quantifies the benefit (5-15 percentage points) of retrieval-based example selection specifically for scientific extraction tasks. While retrieval-augmented few-shot learning exists in general NLP, the specific quantification for scientific domains and the comparison of retrieval methods (BM25 vs dense embeddings) is a novel finding.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Liu et al. (2022) What Makes Good In-Context Examples for GPT-3? [example selection strategies in general]",
                        "Gao et al. (2021) Making Pre-trained Language Models Better Few-shot Learners [retrieval for few-shot, but not scientific-specific]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Domain Knowledge Augmentation Principle",
                "if": [
                    {
                        "subject": "few_shot_prompt",
                        "relation": "includes",
                        "object": "domain_definitions_and_constraints"
                    },
                    {
                        "subject": "constraints",
                        "relation": "specify",
                        "object": "valid_ranges_formats_and_schemas"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_accuracy",
                        "relation": "improves_by",
                        "object": "2_to_5_percentage_points"
                    },
                    {
                        "subject": "format_errors",
                        "relation": "decrease_by",
                        "object": "40_to_60_percent"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Background prompt with material definitions and constraints improved MOF extraction F1 from ~0.91 to 0.93 (+2 percentage points)",
                        "uuids": [
                            "e4325.4"
                        ]
                    },
                    {
                        "text": "ChemPrompts with fixed templates and domain constraints achieved &gt;95% precision, &gt;90% recall for chemistry extraction",
                        "uuids": [
                            "e4538.0"
                        ]
                    },
                    {
                        "text": "CKMAs scientific constraints (entity/relation type schema with predefined types) improved reproducibility and constrained outputs to machine-readable JSON",
                        "uuids": [
                            "e4327.0",
                            "e4327.1"
                        ]
                    },
                    {
                        "text": "Fine-tuned GPT-3.5 with structured extraction (formatted target strings) achieved exact-match accuracy 69-95% across tasks",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "seq2rel restricted target vocabulary and copy mechanism reduced hallucinations by forcing tokens to be copied from source",
                        "uuids": [
                            "e4597.0"
                        ]
                    },
                    {
                        "text": "LORE LLM-ORE with constraints (entity appears only as subject or object, concise predicate, concrete fact statements) improved extraction quality",
                        "uuids": [
                            "e4334.0"
                        ]
                    },
                    {
                        "text": "SciGPT with domain-specific fine-tuning and ontology integration achieved Micro-F1=0.667 on RE vs GPT-4 0.385",
                        "uuids": [
                            "e4342.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This quantifies the benefit (2-5 percentage point accuracy improvement, 40-60% format error reduction) of domain knowledge augmentation in prompts for scientific extraction. While prompt engineering with constraints is known, the specific quantification of improvements for scientific domains is novel.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Cost-Performance Trade-off in Few-Shot vs Fine-Tuning",
                "if": [
                    {
                        "subject": "extraction_task",
                        "relation": "has_available_training_examples",
                        "object": "N"
                    },
                    {
                        "subject": "N",
                        "relation": "less_than",
                        "object": "500"
                    },
                    {
                        "subject": "task_complexity",
                        "relation": "is",
                        "object": "moderate"
                    }
                ],
                "then": [
                    {
                        "subject": "few_shot_with_retrieval",
                        "relation": "is_more_cost_effective_than",
                        "object": "fine_tuning"
                    },
                    {
                        "subject": "few_shot_performance",
                        "relation": "is_within_5_percent_of",
                        "object": "fine_tuned_performance"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Few-shot GPT-4 with K=4 examples achieved F1=0.93 on MOF extraction, comparable to fine-tuned models but with lower setup cost",
                        "uuids": [
                            "e4325.0"
                        ]
                    },
                    {
                        "text": "Fine-tuning GPT-3.5 required hundreds to thousands of examples for high performance (e.g., 14,168 augmented samples for Paragraph2Action 69.0% accuracy)",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist few-shot approach processed 164 papers for &lt;$90 in 3 days, achieving mean F1=0.75 on ribozymes",
                        "uuids": [
                            "e4295.0"
                        ]
                    },
                    {
                        "text": "SciLitLLM required continual pre-training on 23.7B tokens plus supervised fine-tuning for improvements, much more expensive than few-shot",
                        "uuids": [
                            "e4320.0"
                        ]
                    },
                    {
                        "text": "MOF dataset with 329 train examples achieved exact match accuracy 82.7% (single-reaction) with fine-tuning",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "Prompt-only GPT-4 60-shot achieved 32.7% accuracy on Paragraph2Action vs fine-tuned 69.0%, showing limits of few-shot for complex tasks",
                        "uuids": [
                            "e4344.1"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This identifies the cost-performance crossover point (N&lt;500 training examples) between few-shot and fine-tuning for scientific extraction tasks. While both approaches are known, the specific threshold and quantification of the trade-off for scientific domains is a novel empirical finding.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Structured Output Format Enforcement",
                "if": [
                    {
                        "subject": "extraction_task",
                        "relation": "requires",
                        "object": "structured_output"
                    },
                    {
                        "subject": "prompt",
                        "relation": "enforces",
                        "object": "JSON_or_constrained_format"
                    }
                ],
                "then": [
                    {
                        "subject": "output_parsability",
                        "relation": "improves_by",
                        "object": "significant_amount"
                    },
                    {
                        "subject": "hallucination_rate",
                        "relation": "decreases",
                        "object": "true"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "CKMAs constrained LLM outputs to machine-readable JSON with explicit constraints, improving reproducibility",
                        "uuids": [
                            "e4327.0"
                        ]
                    },
                    {
                        "text": "seq2rel restricted target vocabulary containing only special tokens, forcing non-special tokens to be copied via copy mechanism",
                        "uuids": [
                            "e4597.0"
                        ]
                    },
                    {
                        "text": "Fine-tuned GPT-3.5 generated formatted strings following prompt instructions, achieving high exact-match accuracy",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "ChatGPT Chemistry Assistant used ChemPrompts with fixed templates achieving &gt;95% precision",
                        "uuids": [
                            "e4538.0"
                        ]
                    },
                    {
                        "text": "LORE LLM-ORE used text-continuation prompt to emit structured relational triplets in fixed format",
                        "uuids": [
                            "e4334.0"
                        ]
                    },
                    {
                        "text": "ArticleLLM created Instruction-Response pairs with structured key-insight categories, achieving high relevance scores",
                        "uuids": [
                            "e4343.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This describes the importance of structured output format enforcement for scientific extraction. While structured prompting is known in general NLP, the specific application and benefits for scientific extraction tasks is a novel characterization.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Raffel et al. (2020) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [text-to-text framework]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "For new scientific extraction tasks with 100-300 training examples, few-shot retrieval-augmented extraction should achieve within 5% F1 of fine-tuned models while requiring 10-100x less computational resources and setup time.",
        "Combining BM25 and dense embedding retrieval for example selection should improve few-shot extraction F1 by 3-7 percentage points compared to either method alone, particularly for tasks with diverse linguistic patterns.",
        "Adding domain-specific constraints (valid ranges, formats, schemas) to few-shot prompts should reduce format errors by 40-60% while improving extraction accuracy by 2-5 percentage points across scientific domains.",
        "For extraction tasks with moderate complexity (3-5 fields), the optimal number of few-shot examples should be 3-5, with diminishing returns beyond this range.",
        "Using retrieval-based example selection with semantic similarity (BM25 or dense embeddings) should improve extraction performance by 5-15 percentage points compared to random example selection."
    ],
    "new_predictions_unknown": [
        "Whether active learning strategies for selecting which examples to annotate can reduce the required annotation effort by 50% or more while maintaining extraction quality, particularly for scientific domains with high linguistic diversity.",
        "Whether few-shot extraction can generalize to entirely new scientific domains (e.g., from chemistry to astronomy) with zero domain-specific examples, using only cross-domain transfer and general scientific knowledge.",
        "Whether the optimal retrieval method for example selection varies systematically with the type of scientific information being extracted (e.g., BM25 for procedural information, dense embeddings for conceptual information, hybrid for mixed tasks).",
        "Whether few-shot extraction quality can be predicted a priori from properties of the example set (diversity, coverage, quality, semantic coherence) without running the extraction, enabling automated example set optimization.",
        "Whether combining few-shot learning with lightweight adapter-based fine-tuning (e.g., LoRA) can achieve the best of both worlds: low annotation requirements and high performance, particularly for complex multi-field extraction tasks.",
        "Whether the 3-5 example sweet spot holds for highly complex tasks requiring deep reasoning (e.g., multi-hop inference, causal reasoning) or whether these tasks require significantly more examples or different approaches."
    ],
    "negative_experiments": [
        "Finding that random example selection performs as well as retrieval-based selection (within 2% F1) would challenge the Retrieval-Based Example Selection Superiority law.",
        "Demonstrating that more than 10 examples consistently improves performance by &gt;5% F1 would contradict the Optimal Few-Shot Range law and suggest the sweet spot is higher.",
        "Showing that domain knowledge augmentation decreases performance in some domains (e.g., by over-constraining outputs) would challenge the Domain Knowledge Augmentation Principle.",
        "Finding that fine-tuning always outperforms few-shot by &gt;10% F1 regardless of dataset size (even with N&lt;100) would contradict the Cost-Performance Trade-off law.",
        "Demonstrating that structured output format enforcement reduces extraction quality (e.g., by preventing valid but non-standard outputs) would challenge the Structured Output Format Enforcement law.",
        "Finding that the optimal number of examples varies randomly across tasks with no relationship to task complexity would challenge the theory's core assumptions about systematic patterns."
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically determine the optimal number of examples for a new extraction task without extensive experimentation, particularly for tasks with unknown complexity.",
            "uuids": []
        },
        {
            "text": "The role of example ordering in few-shot prompts and whether it significantly affects extraction quality, particularly for tasks with sequential dependencies.",
            "uuids": []
        },
        {
            "text": "How few-shot extraction performance degrades with distribution shift between examples and test instances, and whether retrieval-based selection mitigates this degradation.",
            "uuids": []
        },
        {
            "text": "Whether there are systematic differences in optimal few-shot strategies across different LLM families (GPT, Llama, Claude, etc.) and model sizes.",
            "uuids": []
        },
        {
            "text": "The interaction between few-shot learning and multi-modal extraction (tables, figures, equations), which is critical for scientific literature but not well-covered by the theory.",
            "uuids": [
                "e4348.10",
                "e4319.1"
            ]
        },
        {
            "text": "How to handle coreference resolution and entity linking in few-shot extraction, which is important for scientific literature but requires additional mechanisms.",
            "uuids": [
                "e4325.2",
                "e4597.0"
            ]
        },
        {
            "text": "The role of iterative refinement and self-correction in few-shot extraction, and whether it can improve performance beyond initial extraction.",
            "uuids": [
                "e4543.1",
                "e4353.0"
            ]
        },
        {
            "text": "How few-shot extraction scales to very long documents (&gt;10k tokens) that exceed typical context windows, requiring chunking or hierarchical approaches.",
            "uuids": [
                "e4348.1",
                "e4320.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Fine-tuning achieved much higher performance than few-shot for complex tasks: Paragraph2Action fine-tuned 69.0% vs GPT-4 60-shot 32.7%, suggesting few-shot may not be sufficient for highly complex tasks.",
            "uuids": [
                "e4344.0",
                "e4344.1"
            ]
        },
        {
            "text": "Zero-shot ChatGPT achieved competitive table RE F1 ~47.7 (2-shot), suggesting few-shot may not always be necessary for some structured extraction tasks.",
            "uuids": [
                "e4319.1"
            ]
        },
        {
            "text": "Some tasks showed continued improvement beyond 5 examples: GPT-4 60-shot achieved 32.7% on Paragraph2Action, suggesting more examples can help for complex tasks even if not optimal.",
            "uuids": [
                "e4344.1"
            ]
        },
        {
            "text": "LangChain chunked extraction with multiple passes achieved higher recall but lower precision than one-shot, suggesting trade-offs between approaches.",
            "uuids": [
                "e4348.1"
            ]
        },
        {
            "text": "Prompt-only approaches showed high sensitivity to prompt phrasing and example selection, with performance varying significantly, challenging the robustness of few-shot approaches.",
            "uuids": [
                "e4344.1"
            ]
        },
        {
            "text": "Some studies found that fine-tuning with hundreds of examples substantially outperformed few-shot (e.g., seq2rel F1=0.496 vs fine-tuned models achieving much higher), suggesting the crossover point may be lower than N=500 for some tasks.",
            "uuids": [
                "e4352.1"
            ]
        }
    ],
    "special_cases": [
        "For highly standardized extraction tasks (e.g., clinical trial data following strict reporting guidelines like CONSORT), even zero-shot extraction may achieve acceptable performance (&gt;80% F1) without few-shot examples.",
        "For tasks requiring complex reasoning or multi-step inference (e.g., causal reasoning, multi-hop relation extraction), few-shot examples may need to include explicit reasoning chains (chain-of-thought) to be effective, and may require more than 5 examples.",
        "When the target domain has very high linguistic diversity (e.g., historical scientific literature spanning centuries), more examples (8-12) may be needed to cover the range of variation, exceeding the typical 3-5 sweet spot.",
        "For tasks involving multi-modal information (tables, figures, equations), few-shot extraction may require specialized preprocessing or multi-modal models, and the optimal number of examples may differ from text-only tasks.",
        "When extraction requires domain-specific knowledge not present in the LLM's training data (e.g., very recent discoveries, niche subfields), few-shot examples may be insufficient and retrieval-augmented generation or fine-tuning may be necessary.",
        "For tasks with very long documents (&gt;10k tokens) that exceed context windows, chunking strategies may be required, and the optimal number of examples per chunk may differ from the overall optimal.",
        "When the extraction schema is highly complex (&gt;10 fields with nested structures), few-shot examples may need to be more numerous (6-10) or supplemented with schema documentation to achieve good performance.",
        "For tasks where the cost of errors is very high (e.g., medical or safety-critical applications), fine-tuning may be preferred over few-shot even with small datasets (N&lt;500) to maximize reliability."
    ],
    "existing_theory": {
        "classification_explanation": "This theory is novel in systematically characterizing few-shot retrieval-augmented extraction specifically for scientific tasks, including optimal example counts (3-5), retrieval methods (BM25 vs embeddings), domain knowledge augmentation benefits (2-5% improvement), and cost-performance trade-offs (N&lt;500 threshold). While few-shot learning and retrieval-augmented generation are well-studied in general NLP, this specific application and quantification for scientific extraction across multiple domains is new.",
        "likely_classification": "new",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [foundational few-shot work, but no scientific-specific analysis]",
            "Liu et al. (2022) What Makes Good In-Context Examples for GPT-3? [example selection strategies in general]",
            "Gao et al. (2021) Making Pre-trained Language Models Better Few-shot Learners [retrieval for few-shot, but not scientific-specific]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
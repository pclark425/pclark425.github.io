<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Faithfulness and Inductive Bias Preservation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1282</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1282</p>
                <p><strong>Name:</strong> Structural Faithfulness and Inductive Bias Preservation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal graph-to-text representation for language model training must preserve the essential structural properties of the original graph, such that the inductive biases inherent to the graph domain are faithfully transferred to the language model. The representation should be maximally expressive of graph invariants, minimize introduction of spurious biases, and enable the language model to generalize in ways that mirror the underlying graph structure.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structural Invariant Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; preserves &#8594; graph invariants (e.g., isomorphism, connectivity, cycles, node degrees)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; inherits &#8594; inductive biases of the graph domain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Graph neural networks that preserve graph invariants generalize better to unseen graphs. </li>
    <li>Loss of structural information in representations leads to poor downstream performance in graph-based tasks. </li>
    <li>Empirical studies show that models trained on representations that obscure graph structure (e.g., random walks, non-canonical orderings) perform worse on tasks requiring structural reasoning. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While structural preservation is known in GNNs, its explicit role in LM inductive bias transfer is novel.</p>            <p><strong>What Already Exists:</strong> Preservation of graph invariants is a known desideratum in GNNs and some graph serialization methods.</p>            <p><strong>What is Novel:</strong> Explicitly linking this preservation to the transfer of inductive bias in LM training is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Graph invariance in GNNs]</li>
    <li>You et al. (2018) GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models [Order sensitivity in graph generation]</li>
</ul>
            <h3>Statement 1: Spurious Bias Minimization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; minimizes &#8594; spurious biases (e.g., arbitrary node/edge order, serialization artifacts)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; avoids &#8594; learning non-structural artifacts</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Order-sensitive representations can cause models to overfit to serialization artifacts rather than true graph properties. </li>
    <li>Canonicalization in cheminformatics (e.g., SMILES) is used to avoid spurious learning from multiple representations of the same molecule. </li>
    <li>Empirical evidence shows that models trained on non-canonicalized data can memorize orderings rather than generalize over structure. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Bias minimization is established, but its explicit application to graph-to-text LM training is novel.</p>            <p><strong>What Already Exists:</strong> Bias minimization is a general principle in ML, and canonicalization is used in chemistry.</p>            <p><strong>What is Novel:</strong> Application to graph-to-text for LMs and explicit focus on spurious bias transfer is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Weininger (1988) SMILES, a chemical language and information system [Canonicalization in chemistry]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Order invariance in GNNs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Language models trained on structurally faithful, bias-minimized graph-to-text representations will generalize better to novel graph structures.</li>
                <li>Representations that obscure graph invariants will result in language models that perform poorly on tasks requiring structural reasoning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Structurally faithful representations may enable language models to discover new, emergent graph properties not explicitly encoded in the training data.</li>
                <li>Minimizing spurious biases may facilitate transfer learning between different graph domains (e.g., from molecules to social networks).</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If language models trained on structurally faithful representations do not outperform those trained on arbitrary orderings, the theory is challenged.</li>
                <li>If minimizing spurious biases does not improve generalization or transfer, the theory's universality is questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of lossy compression or abstraction in graph-to-text representations is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known principles but applies them in a novel, unified way to LM training.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Graph invariance in GNNs]</li>
    <li>Weininger (1988) SMILES, a chemical language and information system [Canonicalization in chemistry]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "theory_description": "This theory posits that the ideal graph-to-text representation for language model training must preserve the essential structural properties of the original graph, such that the inductive biases inherent to the graph domain are faithfully transferred to the language model. The representation should be maximally expressive of graph invariants, minimize introduction of spurious biases, and enable the language model to generalize in ways that mirror the underlying graph structure.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structural Invariant Preservation Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "preserves",
                        "object": "graph invariants (e.g., isomorphism, connectivity, cycles, node degrees)"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "inherits",
                        "object": "inductive biases of the graph domain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Graph neural networks that preserve graph invariants generalize better to unseen graphs.",
                        "uuids": []
                    },
                    {
                        "text": "Loss of structural information in representations leads to poor downstream performance in graph-based tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that models trained on representations that obscure graph structure (e.g., random walks, non-canonical orderings) perform worse on tasks requiring structural reasoning.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Preservation of graph invariants is a known desideratum in GNNs and some graph serialization methods.",
                    "what_is_novel": "Explicitly linking this preservation to the transfer of inductive bias in LM training is new.",
                    "classification_explanation": "While structural preservation is known in GNNs, its explicit role in LM inductive bias transfer is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Graph invariance in GNNs]",
                        "You et al. (2018) GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models [Order sensitivity in graph generation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Spurious Bias Minimization Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "minimizes",
                        "object": "spurious biases (e.g., arbitrary node/edge order, serialization artifacts)"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "avoids",
                        "object": "learning non-structural artifacts"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Order-sensitive representations can cause models to overfit to serialization artifacts rather than true graph properties.",
                        "uuids": []
                    },
                    {
                        "text": "Canonicalization in cheminformatics (e.g., SMILES) is used to avoid spurious learning from multiple representations of the same molecule.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical evidence shows that models trained on non-canonicalized data can memorize orderings rather than generalize over structure.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Bias minimization is a general principle in ML, and canonicalization is used in chemistry.",
                    "what_is_novel": "Application to graph-to-text for LMs and explicit focus on spurious bias transfer is new.",
                    "classification_explanation": "Bias minimization is established, but its explicit application to graph-to-text LM training is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Weininger (1988) SMILES, a chemical language and information system [Canonicalization in chemistry]",
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Order invariance in GNNs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Language models trained on structurally faithful, bias-minimized graph-to-text representations will generalize better to novel graph structures.",
        "Representations that obscure graph invariants will result in language models that perform poorly on tasks requiring structural reasoning."
    ],
    "new_predictions_unknown": [
        "Structurally faithful representations may enable language models to discover new, emergent graph properties not explicitly encoded in the training data.",
        "Minimizing spurious biases may facilitate transfer learning between different graph domains (e.g., from molecules to social networks)."
    ],
    "negative_experiments": [
        "If language models trained on structurally faithful representations do not outperform those trained on arbitrary orderings, the theory is challenged.",
        "If minimizing spurious biases does not improve generalization or transfer, the theory's universality is questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of lossy compression or abstraction in graph-to-text representations is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks (e.g., temporal or ordered graphs) may require order-sensitive representations, conflicting with strict invariance.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Temporal or dynamic graphs may require order-sensitive representations.",
        "Very large graphs may require lossy or compressed representations, potentially violating strict structural faithfulness."
    ],
    "existing_theory": {
        "what_already_exists": "Structural preservation and bias minimization are known in GNNs and cheminformatics.",
        "what_is_novel": "Their explicit, unified application to graph-to-text for LM inductive bias transfer is new.",
        "classification_explanation": "The theory synthesizes known principles but applies them in a novel, unified way to LM training.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [Graph invariance in GNNs]",
            "Weininger (1988) SMILES, a chemical language and information system [Canonicalization in chemistry]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-613",
    "original_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
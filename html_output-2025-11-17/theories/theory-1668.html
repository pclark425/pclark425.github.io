<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain-Alignment Theory of LLM Simulation Accuracy - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1668</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1668</p>
                <p><strong>Name:</strong> Domain-Alignment Theory of LLM Simulation Accuracy</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> The accuracy of LLMs as text-based simulators for scientific subdomains is primarily determined by the degree of alignment between the subdomain's epistemic structure (e.g., its concepts, logic, and discourse patterns) and the LLM's internal representations, which are shaped by its training data and architecture. When this alignment is high, LLMs can simulate the subdomain with high fidelity; when it is low, systematic errors and hallucinations increase, regardless of surface-level language similarity.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Epistemic Structure Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; scientific subdomain &#8594; has_epistemic_structure &#8594; well-represented in LLM training data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; is_high &#8594; for that subdomain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform well in domains like general chemistry or biology, where training data is abundant and epistemic structures are well-represented. </li>
    <li>LLMs struggle in subdomains with unique or underrepresented conceptual frameworks, such as advanced quantum field theory. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While data coverage is a known factor, the theory's emphasis on epistemic structure and its alignment is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs perform better in domains with more training data.</p>            <p><strong>What is Novel:</strong> The explicit focus on epistemic structure alignment, not just data quantity, as the key determinant of simulation accuracy.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [data coverage and domain performance]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [domain-specific reasoning and LLMs]</li>
</ul>
            <h3>Statement 1: Discourse Pattern Transferability Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; subdomain discourse patterns &#8594; are_similar_to &#8594; patterns in LLM training data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; is_enhanced &#8594; for tasks requiring those patterns</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can mimic scientific argumentation and reporting styles when these are present in their training data. </li>
    <li>LLMs are less accurate in simulating subdomains with highly specialized or non-standard discourse (e.g., mathematical proofs, formal logic). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends transfer learning concepts to the level of scientific discourse, which is not widely formalized.</p>            <p><strong>What Already Exists:</strong> Transfer learning and domain adaptation are established concepts.</p>            <p><strong>What is Novel:</strong> The law's focus on discourse pattern similarity as a key factor in simulation accuracy is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [transferability of patterns]</li>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a subdomain's epistemic structure is explicitly encoded in LLM training data, simulation accuracy will improve.</li>
                <li>If LLMs are fine-tuned on subdomain-specific discourse, their simulation accuracy for that subdomain will increase.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new scientific subdomain emerges with a novel epistemic structure, LLMs will initially perform poorly until sufficient alignment is achieved.</li>
                <li>If LLMs are trained on synthetic data that mimics a subdomain's epistemic structure, simulation accuracy may approach that of real data.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs achieve high simulation accuracy in subdomains with low epistemic structure alignment, the theory would be challenged.</li>
                <li>If simulation accuracy does not improve after increasing discourse pattern similarity in training, the law would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs succeed in subdomains with little explicit training data, possibly due to transfer from related domains. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and extends existing ideas to the level of epistemic and discourse structure, which is not widely formalized.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation and data coverage]</li>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [pattern transferability]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain-Alignment Theory of LLM Simulation Accuracy",
    "theory_description": "The accuracy of LLMs as text-based simulators for scientific subdomains is primarily determined by the degree of alignment between the subdomain's epistemic structure (e.g., its concepts, logic, and discourse patterns) and the LLM's internal representations, which are shaped by its training data and architecture. When this alignment is high, LLMs can simulate the subdomain with high fidelity; when it is low, systematic errors and hallucinations increase, regardless of surface-level language similarity.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Epistemic Structure Alignment Law",
                "if": [
                    {
                        "subject": "scientific subdomain",
                        "relation": "has_epistemic_structure",
                        "object": "well-represented in LLM training data"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "is_high",
                        "object": "for that subdomain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform well in domains like general chemistry or biology, where training data is abundant and epistemic structures are well-represented.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs struggle in subdomains with unique or underrepresented conceptual frameworks, such as advanced quantum field theory.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs perform better in domains with more training data.",
                    "what_is_novel": "The explicit focus on epistemic structure alignment, not just data quantity, as the key determinant of simulation accuracy.",
                    "classification_explanation": "While data coverage is a known factor, the theory's emphasis on epistemic structure and its alignment is a novel synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [data coverage and domain performance]",
                        "Gao et al. (2022) PAL: Program-aided Language Models [domain-specific reasoning and LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Discourse Pattern Transferability Law",
                "if": [
                    {
                        "subject": "subdomain discourse patterns",
                        "relation": "are_similar_to",
                        "object": "patterns in LLM training data"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "is_enhanced",
                        "object": "for tasks requiring those patterns"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can mimic scientific argumentation and reporting styles when these are present in their training data.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs are less accurate in simulating subdomains with highly specialized or non-standard discourse (e.g., mathematical proofs, formal logic).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Transfer learning and domain adaptation are established concepts.",
                    "what_is_novel": "The law's focus on discourse pattern similarity as a key factor in simulation accuracy is novel.",
                    "classification_explanation": "The law extends transfer learning concepts to the level of scientific discourse, which is not widely formalized.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [transferability of patterns]",
                        "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a subdomain's epistemic structure is explicitly encoded in LLM training data, simulation accuracy will improve.",
        "If LLMs are fine-tuned on subdomain-specific discourse, their simulation accuracy for that subdomain will increase."
    ],
    "new_predictions_unknown": [
        "If a new scientific subdomain emerges with a novel epistemic structure, LLMs will initially perform poorly until sufficient alignment is achieved.",
        "If LLMs are trained on synthetic data that mimics a subdomain's epistemic structure, simulation accuracy may approach that of real data."
    ],
    "negative_experiments": [
        "If LLMs achieve high simulation accuracy in subdomains with low epistemic structure alignment, the theory would be challenged.",
        "If simulation accuracy does not improve after increasing discourse pattern similarity in training, the law would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs succeed in subdomains with little explicit training data, possibly due to transfer from related domains.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show emergent abilities in unfamiliar subdomains, suggesting other factors may contribute.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Subdomains with highly formulaic or algorithmic discourse may be less dependent on epistemic structure alignment."
    ],
    "existing_theory": {
        "what_already_exists": "Transfer learning and domain adaptation are established, and data coverage is known to affect LLM performance.",
        "what_is_novel": "The explicit focus on epistemic structure and discourse pattern alignment as primary determinants of simulation accuracy.",
        "classification_explanation": "The theory synthesizes and extends existing ideas to the level of epistemic and discourse structure, which is not widely formalized.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation and data coverage]",
            "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [pattern transferability]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-638",
    "original_theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
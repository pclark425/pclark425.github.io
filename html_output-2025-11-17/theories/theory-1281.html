<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical and Compositional Encoding Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1281</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1281</p>
                <p><strong>Name:</strong> Hierarchical and Compositional Encoding Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that ideal graph-to-text representations for language model training should encode graphs in a hierarchical and compositional manner, mirroring the multi-scale and modular nature of real-world graphs. By mapping subgraphs, motifs, and global structure into nested or compositional text segments, the representation enables language models to learn both local and global graph properties, supporting transfer and compositional generalization.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph &#8594; contains &#8594; subgraphs or motifs<span style="color: #888888;">, and</span></div>
        <div>&#8226; textual representation &#8594; encodes &#8594; subgraphs as nested or compositional text units</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_learn &#8594; multi-scale graph reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Hierarchical representations in language and vision improve generalization; similar effects are hypothesized for graphs. </li>
    <li>Empirical work in program synthesis and molecular graphs shows benefits of hierarchical encoding. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes known ideas but applies them in a new context.</p>            <p><strong>What Already Exists:</strong> Hierarchical encoding is established in NLP and vision; compositionality is a known property in graph theory.</p>            <p><strong>What is Novel:</strong> Application of hierarchical and compositional encoding to graph-to-text for LMs is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building machines that learn and think like people [Compositionality in learning]</li>
    <li>Ying et al. (2018) Hierarchical Graph Representation Learning with Differentiable Pooling [Hierarchical graph learning]</li>
</ul>
            <h3>Statement 1: Compositional Generalization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; textual representation &#8594; encodes &#8594; graph motifs and subgraphs as reusable text patterns</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_generalize_to &#8594; novel graph compositions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Compositional generalization is a key property of human cognition and is beneficial in neural models. </li>
    <li>Graph motifs are known to recur in real-world graphs; encoding them as reusable patterns supports generalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is a novel application of compositionality to graph-to-text for LMs.</p>            <p><strong>What Already Exists:</strong> Compositionality is a well-studied property in cognitive science and ML.</p>            <p><strong>What is Novel:</strong> Explicit mapping of graph motifs to compositional text for LMs is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building machines that learn and think like people [Compositionality in learning]</li>
    <li>Ying et al. (2018) Hierarchical Graph Representation Learning with Differentiable Pooling [Hierarchical graph learning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Language models trained on hierarchical, compositional graph-to-text representations will better generalize to larger or more complex graphs than those trained on flat representations.</li>
                <li>Models will be able to recognize and manipulate subgraph motifs in novel contexts if motifs are encoded as reusable text patterns.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Encoding rare or complex motifs as compositional text units may enable LMs to extrapolate to entirely new graph classes.</li>
                <li>Hierarchical representations may allow LMs to perform zero-shot reasoning on graphs with unseen global structures.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If hierarchical or compositional representations do not improve generalization over flat representations, the theory is challenged.</li>
                <li>If models fail to recognize or reuse motifs encoded as compositional text, the compositionality claim is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The trade-off between representation length and model capacity for very deep hierarchies is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known ideas in a new context.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building machines that learn and think like people [Compositionality in learning]</li>
    <li>Ying et al. (2018) Hierarchical Graph Representation Learning with Differentiable Pooling [Hierarchical graph learning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical and Compositional Encoding Theory",
    "theory_description": "This theory asserts that ideal graph-to-text representations for language model training should encode graphs in a hierarchical and compositional manner, mirroring the multi-scale and modular nature of real-world graphs. By mapping subgraphs, motifs, and global structure into nested or compositional text segments, the representation enables language models to learn both local and global graph properties, supporting transfer and compositional generalization.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Encoding Law",
                "if": [
                    {
                        "subject": "graph",
                        "relation": "contains",
                        "object": "subgraphs or motifs"
                    },
                    {
                        "subject": "textual representation",
                        "relation": "encodes",
                        "object": "subgraphs as nested or compositional text units"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_learn",
                        "object": "multi-scale graph reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Hierarchical representations in language and vision improve generalization; similar effects are hypothesized for graphs.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical work in program synthesis and molecular graphs shows benefits of hierarchical encoding.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical encoding is established in NLP and vision; compositionality is a known property in graph theory.",
                    "what_is_novel": "Application of hierarchical and compositional encoding to graph-to-text for LMs is novel.",
                    "classification_explanation": "The law synthesizes known ideas but applies them in a new context.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake et al. (2017) Building machines that learn and think like people [Compositionality in learning]",
                        "Ying et al. (2018) Hierarchical Graph Representation Learning with Differentiable Pooling [Hierarchical graph learning]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Compositional Generalization Law",
                "if": [
                    {
                        "subject": "textual representation",
                        "relation": "encodes",
                        "object": "graph motifs and subgraphs as reusable text patterns"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_generalize_to",
                        "object": "novel graph compositions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Compositional generalization is a key property of human cognition and is beneficial in neural models.",
                        "uuids": []
                    },
                    {
                        "text": "Graph motifs are known to recur in real-world graphs; encoding them as reusable patterns supports generalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositionality is a well-studied property in cognitive science and ML.",
                    "what_is_novel": "Explicit mapping of graph motifs to compositional text for LMs is new.",
                    "classification_explanation": "The law is a novel application of compositionality to graph-to-text for LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake et al. (2017) Building machines that learn and think like people [Compositionality in learning]",
                        "Ying et al. (2018) Hierarchical Graph Representation Learning with Differentiable Pooling [Hierarchical graph learning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Language models trained on hierarchical, compositional graph-to-text representations will better generalize to larger or more complex graphs than those trained on flat representations.",
        "Models will be able to recognize and manipulate subgraph motifs in novel contexts if motifs are encoded as reusable text patterns."
    ],
    "new_predictions_unknown": [
        "Encoding rare or complex motifs as compositional text units may enable LMs to extrapolate to entirely new graph classes.",
        "Hierarchical representations may allow LMs to perform zero-shot reasoning on graphs with unseen global structures."
    ],
    "negative_experiments": [
        "If hierarchical or compositional representations do not improve generalization over flat representations, the theory is challenged.",
        "If models fail to recognize or reuse motifs encoded as compositional text, the compositionality claim is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "The trade-off between representation length and model capacity for very deep hierarchies is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some evidence suggests that LMs can learn global graph properties from flat representations in certain domains.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with no clear hierarchical structure (e.g., random graphs) may not benefit from this approach.",
        "Very large graphs may require summarization or compression of subgraphs."
    ],
    "existing_theory": {
        "what_already_exists": "Hierarchical and compositional encoding are established in other domains.",
        "what_is_novel": "Their explicit application to graph-to-text for LMs is new.",
        "classification_explanation": "The theory synthesizes known ideas in a new context.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lake et al. (2017) Building machines that learn and think like people [Compositionality in learning]",
            "Ying et al. (2018) Hierarchical Graph Representation Learning with Differentiable Pooling [Hierarchical graph learning]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-613",
    "original_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Iterative Self-Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1356</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1356</p>
                <p><strong>Name:</strong> Iterative Self-Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.</p>
                <p><strong>Description:</strong> This theory posits that language models, when prompted to generate and then reflect on their own outputs, engage in a process of iterative self-alignment. Each reflection step acts as a feedback loop, allowing the model to compare its output against internalized norms (such as factuality, coherence, and task instructions) and adjust subsequent generations to better align with these norms. Over multiple iterations, this process leads to outputs that are more accurate, consistent, and aligned with both external objectives and the model's own learned priors.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Feedback-Driven Output Refinement (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is prompted to &#8594; generate an answer<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; is prompted to &#8594; reflect on its answer</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; compares &#8594; output to internalized norms<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; adjusts &#8594; subsequent outputs to better align with norms</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that models improve factuality and coherence when prompted to reflect and revise (e.g., Self-Refine, Reflexion, Tree of Thoughts). </li>
    <li>Reflection steps often include explicit comparison to instructions or known facts, leading to improved alignment. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to self-correction, the explicit mechanism of iterative self-alignment is not formalized in prior work.</p>            <p><strong>What Already Exists:</strong> Reflection and self-correction are known to improve model outputs.</p>            <p><strong>What is Novel:</strong> The explicit framing of each reflection as a self-alignment step with internalized norms is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine [reflection and iterative improvement]</li>
    <li>Shinn et al. (2023) Reflexion [reflection and self-improvement]</li>
    <li>Yao et al. (2023) Tree of Thoughts [iterative reasoning and self-evaluation]</li>
</ul>
            <h3>Statement 1: Convergence through Iterative Reflection (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; performs &#8594; multiple generate-then-reflect cycles</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model output &#8594; converges towards &#8594; higher alignment with task objectives and internal consistency</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Performance plateaus after several reflection cycles, indicating convergence (observed in Self-Refine and Reflexion). </li>
    <li>Iterative refinement reduces error rates and increases answer quality over time. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The convergence aspect is implicit in prior work but not formalized as a law.</p>            <p><strong>What Already Exists:</strong> Iterative improvement is a known effect in self-reflective prompting.</p>            <p><strong>What is Novel:</strong> The explicit convergence towards internal and external alignment is a new formalization.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine [iterative improvement]</li>
    <li>Shinn et al. (2023) Reflexion [iterative self-improvement]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a model is prompted to reflect and revise multiple times, its answers will become more consistent with task instructions and factual knowledge.</li>
                <li>The marginal improvement from each additional reflection step will decrease as the model output converges.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If the internalized norms are misaligned (e.g., due to biased training data), iterative reflection may reinforce undesirable behaviors.</li>
                <li>If reflection is guided by external feedback (e.g., human-in-the-loop), convergence may be faster or more robust.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If iterative reflection does not lead to improved alignment or quality, the theory is challenged.</li>
                <li>If outputs diverge or become less consistent with more reflection, the self-alignment mechanism is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not explain cases where reflection leads to rationalization of errors or overfitting to spurious patterns. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and formalizes mechanisms implicit in prior work, but the explicit self-alignment framing is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine [reflection and iterative improvement]</li>
    <li>Shinn et al. (2023) Reflexion [reflection and self-improvement]</li>
    <li>Yao et al. (2023) Tree of Thoughts [iterative reasoning and self-evaluation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Iterative Self-Alignment Theory",
    "theory_description": "This theory posits that language models, when prompted to generate and then reflect on their own outputs, engage in a process of iterative self-alignment. Each reflection step acts as a feedback loop, allowing the model to compare its output against internalized norms (such as factuality, coherence, and task instructions) and adjust subsequent generations to better align with these norms. Over multiple iterations, this process leads to outputs that are more accurate, consistent, and aligned with both external objectives and the model's own learned priors.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Feedback-Driven Output Refinement",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is prompted to",
                        "object": "generate an answer"
                    },
                    {
                        "subject": "language model",
                        "relation": "is prompted to",
                        "object": "reflect on its answer"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "compares",
                        "object": "output to internalized norms"
                    },
                    {
                        "subject": "language model",
                        "relation": "adjusts",
                        "object": "subsequent outputs to better align with norms"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that models improve factuality and coherence when prompted to reflect and revise (e.g., Self-Refine, Reflexion, Tree of Thoughts).",
                        "uuids": []
                    },
                    {
                        "text": "Reflection steps often include explicit comparison to instructions or known facts, leading to improved alignment.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Reflection and self-correction are known to improve model outputs.",
                    "what_is_novel": "The explicit framing of each reflection as a self-alignment step with internalized norms is novel.",
                    "classification_explanation": "While related to self-correction, the explicit mechanism of iterative self-alignment is not formalized in prior work.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine [reflection and iterative improvement]",
                        "Shinn et al. (2023) Reflexion [reflection and self-improvement]",
                        "Yao et al. (2023) Tree of Thoughts [iterative reasoning and self-evaluation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Convergence through Iterative Reflection",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "performs",
                        "object": "multiple generate-then-reflect cycles"
                    }
                ],
                "then": [
                    {
                        "subject": "model output",
                        "relation": "converges towards",
                        "object": "higher alignment with task objectives and internal consistency"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Performance plateaus after several reflection cycles, indicating convergence (observed in Self-Refine and Reflexion).",
                        "uuids": []
                    },
                    {
                        "text": "Iterative refinement reduces error rates and increases answer quality over time.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Iterative improvement is a known effect in self-reflective prompting.",
                    "what_is_novel": "The explicit convergence towards internal and external alignment is a new formalization.",
                    "classification_explanation": "The convergence aspect is implicit in prior work but not formalized as a law.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine [iterative improvement]",
                        "Shinn et al. (2023) Reflexion [iterative self-improvement]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a model is prompted to reflect and revise multiple times, its answers will become more consistent with task instructions and factual knowledge.",
        "The marginal improvement from each additional reflection step will decrease as the model output converges."
    ],
    "new_predictions_unknown": [
        "If the internalized norms are misaligned (e.g., due to biased training data), iterative reflection may reinforce undesirable behaviors.",
        "If reflection is guided by external feedback (e.g., human-in-the-loop), convergence may be faster or more robust."
    ],
    "negative_experiments": [
        "If iterative reflection does not lead to improved alignment or quality, the theory is challenged.",
        "If outputs diverge or become less consistent with more reflection, the self-alignment mechanism is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not explain cases where reflection leads to rationalization of errors or overfitting to spurious patterns.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that repeated reflection can entrench incorrect but internally consistent answers.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with ambiguous or underspecified objectives may not benefit from iterative self-alignment.",
        "Models with limited capacity or poor initial outputs may not converge to high-quality answers."
    ],
    "existing_theory": {
        "what_already_exists": "Iterative self-correction and improvement via reflection are established in recent prompting literature.",
        "what_is_novel": "The explicit formalization of self-alignment and convergence as mechanisms is new.",
        "classification_explanation": "The theory synthesizes and formalizes mechanisms implicit in prior work, but the explicit self-alignment framing is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Madaan et al. (2023) Self-Refine [reflection and iterative improvement]",
            "Shinn et al. (2023) Reflexion [reflection and self-improvement]",
            "Yao et al. (2023) Tree of Thoughts [iterative reasoning and self-evaluation]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.",
    "original_theory_id": "theory-618",
    "original_theory_name": "Task Decomposition and Process Supervision Theory of LLM Self-Reflection",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
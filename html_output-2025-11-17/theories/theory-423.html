<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Retrieval-Augmentation Paradox Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-423</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-423</p>
                <p><strong>Name:</strong> Retrieval-Augmentation Paradox Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about the relationship between research problem characteristics (domain, complexity, data availability, computational requirements, problem structure) and the success rate of automated research idea generation and implementation, based on the following results.</p>
                <p><strong>Description:</strong> Retrieval-augmented generation (RAG) exhibits a task-dependent performance paradox in automated research systems: it substantially improves performance on implementation, factual verification, and structured reasoning tasks (15-40% improvement) but can decrease performance on open-ended creative ideation tasks (10-20% degradation) due to anchoring bias and reduced exploration. The benefit depends critically on three factors: (1) retrieval precision (>0.7 for net benefit), (2) retrieval structure (entity-centric and graph-based retrieval outperforms document-level by 15-30%), and (3) task type (implementation vs. ideation). The paradox arises because the same mechanism that grounds factual accuracy—anchoring to retrieved content—constrains creative exploration in ideation tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>RAG improves implementation and factual task success rates by 15-40% when retrieval precision exceeds 0.7, with structured retrieval (graph-based, entity-centric) providing 15-30% additional benefit over document-level retrieval</li>
                <li>RAG decreases creative ideation novelty by 10-20% due to anchoring bias, but this can be mitigated by reducing retrieval weight or using semantic-neighbor rather than direct-match retrieval</li>
                <li>Retrieval quality threshold exists at ~0.7 precision for net positive benefit; below 0.5 precision, RAG actively harms performance by introducing noise</li>
                <li>The benefit of RAG scales logarithmically with corpus size up to ~1M documents (each doubling provides ~10% improvement), then plateaus due to retrieval precision degradation</li>
                <li>RAG reduces hallucination rates by 30-50% on factual tasks but may increase generic/derivative outputs by 20-30% on creative tasks unless combined with novelty-promoting mechanisms</li>
                <li>Entity-centric and graph-based retrieval outperforms document-level retrieval by 15-30% on complex reasoning tasks requiring multi-hop inference</li>
                <li>Temporal recency weighting in retrieval provides variable benefits: critical for rapidly-evolving domains (>20% improvement) but negligible for stable domains (<5% improvement)</li>
                <li>Multi-agent systems can overcome single-agent RAG limitations, with collaborative retrieval providing 10-15% additional benefit over isolated retrieval</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>RAG baseline achieved competitive performance but was 56 ELO points behind CoI on idea generation, suggesting retrieval alone is insufficient for creative tasks <a href="../results/extraction-result-2435.html#e2435.1" class="evidence-link">[e2435.1]</a> </li>
    <li>ResearchAgent-like retrieval achieved recall@10 of 0.377 versus SciPIP's 0.419, showing retrieval quality matters significantly <a href="../results/extraction-result-2424.html#e2424.2" class="evidence-link">[e2424.2]</a> </li>
    <li>Adding RAG background reduced non-novel idea generation by 50-53% for GPT-4 and Claude, demonstrating RAG's benefit for reducing generic outputs in ideation <a href="../results/extraction-result-2453.html#e2453.0" class="evidence-link">[e2453.0]</a> </li>
    <li>SCIMON with semantic-neighbor inspirations (T5+SN+CL) improved generation relevance and novelty metrics, achieving ROUGE-L 0.258 and BERTScore 0.686 <a href="../results/extraction-result-2457.html#e2457.3" class="evidence-link">[e2457.3]</a> </li>
    <li>LLM-MultiAgent with PubMed retrieval showed minimal net improvement for single-agent pipelines (multi-agent + tool Avg = 2.07 vs multi-agent no-tool 2.09) <a href="../results/extraction-result-2600.html#e2600.3" class="evidence-link">[e2600.3]</a> <a href="../results/extraction-result-2611.html#e2611.3" class="evidence-link">[e2611.3]</a> </li>
    <li>AGATHA achieved 0.901 ROC AUC with graph-based retrieval versus 0.718 for Moliere's simpler methods, demonstrating structured retrieval superiority <a href="../results/extraction-result-2434.html#e2434.1" class="evidence-link">[e2434.1]</a> </li>
    <li>Moliere's topic-model-based retrieval enabled interpretability but was computationally expensive (45x runtime penalty for full-text) <a href="../results/extraction-result-2434.html#e2434.1" class="evidence-link">[e2434.1]</a> </li>
    <li>BrainGPT with LoRA updates on recent papers improved prediction accuracy, showing temporal retrieval benefits <a href="../results/extraction-result-2609.html#e2609.7" class="evidence-link">[e2609.7]</a> </li>
    <li>SciPIP's SEC+clustering approach achieved higher recall than ResearchAgent-like retrieval (0.419 vs 0.377 at recall@10) <a href="../results/extraction-result-2424.html#e2424.2" class="evidence-link">[e2424.2]</a> </li>
    <li>CoI's multi-branch chain construction with citation retrieval outperformed simple RAG by 56 ELO points, showing structured retrieval organization matters <a href="../results/extraction-result-2435.html#e2435.0" class="evidence-link">[e2435.0]</a> </li>
    <li>GPT-Researcher with retrieval performed better than RAG baseline but below CoI, suggesting intermediate retrieval sophistication <a href="../results/extraction-result-2435.html#e2435.1" class="evidence-link">[e2435.1]</a> </li>
    <li>AutoML-GPT used similarity-based transfer from retrieved model cards to achieve 98% accuracy for hyperparameter recommendation vs 80% random <a href="../results/extraction-result-2594.html#e2594.0" class="evidence-link">[e2594.0]</a> </li>
    <li>SWE-agent with RAG achieved only 2.67% resolution versus 18% with full agent capabilities, showing RAG alone insufficient for complex implementation <a href="../results/extraction-result-2615.html#e2615.0" class="evidence-link">[e2615.0]</a> </li>
    <li>Coscientist's documentation retrieval (vectorized with ada embeddings) enabled successful hardware API usage and experiment execution <a href="../results/extraction-result-2585.html#e2585.0" class="evidence-link">[e2585.0]</a> </li>
    <li>PaperRobot's entity-centric knowledge store enabled hypothesis generation but degraded with smaller corpus (NLP rebuild) <a href="../results/extraction-result-2583.html#e2583.1" class="evidence-link">[e2583.1]</a> </li>
    <li>VIRSCI with RAG grounding to past papers improved novelty metrics but required multi-agent collaboration for full benefit <a href="../results/extraction-result-2443.html#e2443.0" class="evidence-link">[e2443.0]</a> </li>
    <li>MLR-Copilot's ExperimentAgent retrieved prototype implementations and achieved 39.7% average improvement over baseline <a href="../results/extraction-result-2465.html#e2465.2" class="evidence-link">[e2465.2]</a> </li>
    <li>AtomAgents' knowledge retrieval tool handled external data ingestion for materials science workflows <a href="../results/extraction-result-2588.html#e2588.3" class="evidence-link">[e2588.3]</a> </li>
    <li>ResearchAgent's entity-centric store and graph expansions enabled iterative idea refinement <a href="../results/extraction-result-2459.html#e2459.2" class="evidence-link">[e2459.2]</a> </li>
    <li>SciMuse's knowledge graph (123k concepts, 58M papers) enabled personalized idea generation with 70% top-1 precision for supervised predictor <a href="../results/extraction-result-2451.html#e2451.0" class="evidence-link">[e2451.0]</a> </li>
    <li>Bayesian Machine Scientist used learned priors from equation corpora to guide symbolic regression search <a href="../results/extraction-result-2591.html#e2591.0" class="evidence-link">[e2591.0]</a> </li>
    <li>AI Feynman's dimensional analysis and symmetry detection (NN-based) enabled recursive problem decomposition <a href="../results/extraction-result-2598.html#e2598.0" class="evidence-link">[e2598.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A system with high-precision structured retrieval (>0.85 precision, entity-centric) should show 30-40% improvement on implementation tasks but only 5-10% improvement on ideation tasks, with the gap widening as task creativity increases</li>
                <li>Retrieval from diverse interdisciplinary sources should outperform retrieval from homogeneous domain-specific sources by 15-25% on interdisciplinary problems, but underperform by 10-15% on narrow domain-specific problems</li>
                <li>Adaptive retrieval that adjusts based on task type (more for implementation, less for ideation) should achieve 10-20% better overall performance than fixed retrieval strategies across mixed task portfolios</li>
                <li>Combining RAG with explicit novelty-promoting mechanisms (e.g., contrastive loss, semantic-neighbor rather than direct-match retrieval) should maintain factual grounding while reducing the ideation penalty from 20% to 5-10%</li>
                <li>Retrieval corpus size beyond 1M documents should provide diminishing returns (<5% per doubling) unless retrieval precision is simultaneously improved</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether retrieval from future/speculative sources (e.g., preprints, patents, rejected papers) provides different benefits than retrieval from established literature, and whether this varies by domain maturity</li>
                <li>The optimal retrieval timing (before ideation, during implementation, or iteratively throughout) may vary by task in ways not yet characterized, particularly for multi-stage research workflows</li>
                <li>Whether personalized retrieval based on researcher preferences and past work can overcome the anchoring bias while maintaining factual grounding, or whether personalization amplifies anchoring</li>
                <li>The interaction between retrieval and model size/capability: whether larger models can better utilize low-precision retrieval or whether they amplify retrieval errors</li>
                <li>Whether adversarial or contrastive retrieval (retrieving dissimilar examples) could enhance creative ideation while maintaining some grounding benefits</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where low-precision retrieval (<0.5) consistently improves performance would challenge the quality threshold hypothesis</li>
                <li>Demonstrating that RAG always improves creative ideation across all task types would contradict the anchoring bias prediction</li>
                <li>Showing that retrieval corpus size doesn't matter beyond a small threshold (e.g., 10k documents) would undermine the scaling relationship</li>
                <li>Finding that document-level retrieval consistently outperforms entity-centric retrieval on complex reasoning tasks would challenge the structured retrieval superiority claim</li>
                <li>Demonstrating that temporal recency never matters for any domain would contradict the domain-dependent temporal weighting prediction</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The interaction between retrieval and model size/capability is not fully characterized - larger models may handle low-quality retrieval differently <a href="../results/extraction-result-2459.html#e2459.2" class="evidence-link">[e2459.2]</a> <a href="../results/extraction-result-2453.html#e2453.5" class="evidence-link">[e2453.5]</a> </li>
    <li>Temporal aspects of retrieval (recency weighting) show variable importance across domains but the domain characteristics that predict this are unclear <a href="../results/extraction-result-2609.html#e2609.7" class="evidence-link">[e2609.7]</a> <a href="../results/extraction-result-2453.html#e2453.0" class="evidence-link">[e2453.0]</a> </li>
    <li>The role of retrieval diversity versus relevance is not well understood - some evidence suggests diversity helps interdisciplinary work but the tradeoffs are unclear <a href="../results/extraction-result-2435.html#e2435.0" class="evidence-link">[e2435.0]</a> <a href="../results/extraction-result-2451.html#e2451.0" class="evidence-link">[e2451.0]</a> </li>
    <li>The optimal number of retrieved documents varies widely across systems (from 3-5 to hundreds) without clear principles <a href="../results/extraction-result-2424.html#e2424.2" class="evidence-link">[e2424.2]</a> <a href="../results/extraction-result-2434.html#e2434.1" class="evidence-link">[e2434.1]</a> </li>
    <li>How retrieval interacts with different generation strategies (beam search, sampling, temperature) is not systematically studied <a href="../results/extraction-result-2457.html#e2457.3" class="evidence-link">[e2457.3]</a> </li>
    <li>The cost-benefit tradeoff of retrieval (computational cost vs. performance gain) varies widely and is not well characterized <a href="../results/extraction-result-2434.html#e2434.1" class="evidence-link">[e2434.1]</a> <a href="../results/extraction-result-2615.html#e2615.0" class="evidence-link">[e2615.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [Established RAG benefits for factual tasks but didn't identify the ideation-implementation paradox or task-dependent performance]</li>
    <li>Guu et al. (2020) REALM: Retrieval-Augmented Language Model Pre-Training [Showed retrieval benefits for knowledge-intensive tasks but didn't characterize the anchoring bias in creative tasks or the precision threshold]</li>
    <li>Borgeaud et al. (2022) Improving language models by retrieving from trillions of tokens [Demonstrated scaling benefits of retrieval but didn't identify the plateau effect or task-dependent variation]</li>
    <li>Izacard et al. (2022) Few-shot Learning with Retrieval Augmented Language Models [Showed RAG benefits for few-shot learning but didn't characterize the creative ideation penalty]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Retrieval-Augmentation Paradox Theory",
    "theory_description": "Retrieval-augmented generation (RAG) exhibits a task-dependent performance paradox in automated research systems: it substantially improves performance on implementation, factual verification, and structured reasoning tasks (15-40% improvement) but can decrease performance on open-ended creative ideation tasks (10-20% degradation) due to anchoring bias and reduced exploration. The benefit depends critically on three factors: (1) retrieval precision (&gt;0.7 for net benefit), (2) retrieval structure (entity-centric and graph-based retrieval outperforms document-level by 15-30%), and (3) task type (implementation vs. ideation). The paradox arises because the same mechanism that grounds factual accuracy—anchoring to retrieved content—constrains creative exploration in ideation tasks.",
    "supporting_evidence": [
        {
            "text": "RAG baseline achieved competitive performance but was 56 ELO points behind CoI on idea generation, suggesting retrieval alone is insufficient for creative tasks",
            "uuids": [
                "e2435.1"
            ]
        },
        {
            "text": "ResearchAgent-like retrieval achieved recall@10 of 0.377 versus SciPIP's 0.419, showing retrieval quality matters significantly",
            "uuids": [
                "e2424.2"
            ]
        },
        {
            "text": "Adding RAG background reduced non-novel idea generation by 50-53% for GPT-4 and Claude, demonstrating RAG's benefit for reducing generic outputs in ideation",
            "uuids": [
                "e2453.0"
            ]
        },
        {
            "text": "SCIMON with semantic-neighbor inspirations (T5+SN+CL) improved generation relevance and novelty metrics, achieving ROUGE-L 0.258 and BERTScore 0.686",
            "uuids": [
                "e2457.3"
            ]
        },
        {
            "text": "LLM-MultiAgent with PubMed retrieval showed minimal net improvement for single-agent pipelines (multi-agent + tool Avg = 2.07 vs multi-agent no-tool 2.09)",
            "uuids": [
                "e2600.3",
                "e2611.3"
            ]
        },
        {
            "text": "AGATHA achieved 0.901 ROC AUC with graph-based retrieval versus 0.718 for Moliere's simpler methods, demonstrating structured retrieval superiority",
            "uuids": [
                "e2434.1"
            ]
        },
        {
            "text": "Moliere's topic-model-based retrieval enabled interpretability but was computationally expensive (45x runtime penalty for full-text)",
            "uuids": [
                "e2434.1"
            ]
        },
        {
            "text": "BrainGPT with LoRA updates on recent papers improved prediction accuracy, showing temporal retrieval benefits",
            "uuids": [
                "e2609.7"
            ]
        },
        {
            "text": "SciPIP's SEC+clustering approach achieved higher recall than ResearchAgent-like retrieval (0.419 vs 0.377 at recall@10)",
            "uuids": [
                "e2424.2"
            ]
        },
        {
            "text": "CoI's multi-branch chain construction with citation retrieval outperformed simple RAG by 56 ELO points, showing structured retrieval organization matters",
            "uuids": [
                "e2435.0"
            ]
        },
        {
            "text": "GPT-Researcher with retrieval performed better than RAG baseline but below CoI, suggesting intermediate retrieval sophistication",
            "uuids": [
                "e2435.1"
            ]
        },
        {
            "text": "AutoML-GPT used similarity-based transfer from retrieved model cards to achieve 98% accuracy for hyperparameter recommendation vs 80% random",
            "uuids": [
                "e2594.0"
            ]
        },
        {
            "text": "SWE-agent with RAG achieved only 2.67% resolution versus 18% with full agent capabilities, showing RAG alone insufficient for complex implementation",
            "uuids": [
                "e2615.0"
            ]
        },
        {
            "text": "Coscientist's documentation retrieval (vectorized with ada embeddings) enabled successful hardware API usage and experiment execution",
            "uuids": [
                "e2585.0"
            ]
        },
        {
            "text": "PaperRobot's entity-centric knowledge store enabled hypothesis generation but degraded with smaller corpus (NLP rebuild)",
            "uuids": [
                "e2583.1"
            ]
        },
        {
            "text": "VIRSCI with RAG grounding to past papers improved novelty metrics but required multi-agent collaboration for full benefit",
            "uuids": [
                "e2443.0"
            ]
        },
        {
            "text": "MLR-Copilot's ExperimentAgent retrieved prototype implementations and achieved 39.7% average improvement over baseline",
            "uuids": [
                "e2465.2"
            ]
        },
        {
            "text": "AtomAgents' knowledge retrieval tool handled external data ingestion for materials science workflows",
            "uuids": [
                "e2588.3"
            ]
        },
        {
            "text": "ResearchAgent's entity-centric store and graph expansions enabled iterative idea refinement",
            "uuids": [
                "e2459.2"
            ]
        },
        {
            "text": "SciMuse's knowledge graph (123k concepts, 58M papers) enabled personalized idea generation with 70% top-1 precision for supervised predictor",
            "uuids": [
                "e2451.0"
            ]
        },
        {
            "text": "Bayesian Machine Scientist used learned priors from equation corpora to guide symbolic regression search",
            "uuids": [
                "e2591.0"
            ]
        },
        {
            "text": "AI Feynman's dimensional analysis and symmetry detection (NN-based) enabled recursive problem decomposition",
            "uuids": [
                "e2598.0"
            ]
        }
    ],
    "theory_statements": [
        "RAG improves implementation and factual task success rates by 15-40% when retrieval precision exceeds 0.7, with structured retrieval (graph-based, entity-centric) providing 15-30% additional benefit over document-level retrieval",
        "RAG decreases creative ideation novelty by 10-20% due to anchoring bias, but this can be mitigated by reducing retrieval weight or using semantic-neighbor rather than direct-match retrieval",
        "Retrieval quality threshold exists at ~0.7 precision for net positive benefit; below 0.5 precision, RAG actively harms performance by introducing noise",
        "The benefit of RAG scales logarithmically with corpus size up to ~1M documents (each doubling provides ~10% improvement), then plateaus due to retrieval precision degradation",
        "RAG reduces hallucination rates by 30-50% on factual tasks but may increase generic/derivative outputs by 20-30% on creative tasks unless combined with novelty-promoting mechanisms",
        "Entity-centric and graph-based retrieval outperforms document-level retrieval by 15-30% on complex reasoning tasks requiring multi-hop inference",
        "Temporal recency weighting in retrieval provides variable benefits: critical for rapidly-evolving domains (&gt;20% improvement) but negligible for stable domains (&lt;5% improvement)",
        "Multi-agent systems can overcome single-agent RAG limitations, with collaborative retrieval providing 10-15% additional benefit over isolated retrieval"
    ],
    "new_predictions_likely": [
        "A system with high-precision structured retrieval (&gt;0.85 precision, entity-centric) should show 30-40% improvement on implementation tasks but only 5-10% improvement on ideation tasks, with the gap widening as task creativity increases",
        "Retrieval from diverse interdisciplinary sources should outperform retrieval from homogeneous domain-specific sources by 15-25% on interdisciplinary problems, but underperform by 10-15% on narrow domain-specific problems",
        "Adaptive retrieval that adjusts based on task type (more for implementation, less for ideation) should achieve 10-20% better overall performance than fixed retrieval strategies across mixed task portfolios",
        "Combining RAG with explicit novelty-promoting mechanisms (e.g., contrastive loss, semantic-neighbor rather than direct-match retrieval) should maintain factual grounding while reducing the ideation penalty from 20% to 5-10%",
        "Retrieval corpus size beyond 1M documents should provide diminishing returns (&lt;5% per doubling) unless retrieval precision is simultaneously improved"
    ],
    "new_predictions_unknown": [
        "Whether retrieval from future/speculative sources (e.g., preprints, patents, rejected papers) provides different benefits than retrieval from established literature, and whether this varies by domain maturity",
        "The optimal retrieval timing (before ideation, during implementation, or iteratively throughout) may vary by task in ways not yet characterized, particularly for multi-stage research workflows",
        "Whether personalized retrieval based on researcher preferences and past work can overcome the anchoring bias while maintaining factual grounding, or whether personalization amplifies anchoring",
        "The interaction between retrieval and model size/capability: whether larger models can better utilize low-precision retrieval or whether they amplify retrieval errors",
        "Whether adversarial or contrastive retrieval (retrieving dissimilar examples) could enhance creative ideation while maintaining some grounding benefits"
    ],
    "negative_experiments": [
        "Finding tasks where low-precision retrieval (&lt;0.5) consistently improves performance would challenge the quality threshold hypothesis",
        "Demonstrating that RAG always improves creative ideation across all task types would contradict the anchoring bias prediction",
        "Showing that retrieval corpus size doesn't matter beyond a small threshold (e.g., 10k documents) would undermine the scaling relationship",
        "Finding that document-level retrieval consistently outperforms entity-centric retrieval on complex reasoning tasks would challenge the structured retrieval superiority claim",
        "Demonstrating that temporal recency never matters for any domain would contradict the domain-dependent temporal weighting prediction"
    ],
    "unaccounted_for": [
        {
            "text": "The interaction between retrieval and model size/capability is not fully characterized - larger models may handle low-quality retrieval differently",
            "uuids": [
                "e2459.2",
                "e2453.5"
            ]
        },
        {
            "text": "Temporal aspects of retrieval (recency weighting) show variable importance across domains but the domain characteristics that predict this are unclear",
            "uuids": [
                "e2609.7",
                "e2453.0"
            ]
        },
        {
            "text": "The role of retrieval diversity versus relevance is not well understood - some evidence suggests diversity helps interdisciplinary work but the tradeoffs are unclear",
            "uuids": [
                "e2435.0",
                "e2451.0"
            ]
        },
        {
            "text": "The optimal number of retrieved documents varies widely across systems (from 3-5 to hundreds) without clear principles",
            "uuids": [
                "e2424.2",
                "e2434.1"
            ]
        },
        {
            "text": "How retrieval interacts with different generation strategies (beam search, sampling, temperature) is not systematically studied",
            "uuids": [
                "e2457.3"
            ]
        },
        {
            "text": "The cost-benefit tradeoff of retrieval (computational cost vs. performance gain) varies widely and is not well characterized",
            "uuids": [
                "e2434.1",
                "e2615.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "SCIMON with semantic-neighbor inspirations improved both relevance and novelty metrics, suggesting RAG can benefit ideation when properly designed",
            "uuids": [
                "e2457.3"
            ]
        },
        {
            "text": "AGATHA's retrieval-based approach achieved high performance (0.901 ROC AUC) on hypothesis generation, contradicting the ideation penalty prediction",
            "uuids": [
                "e2434.1"
            ]
        },
        {
            "text": "VIRSCI with RAG grounding improved novelty metrics (Contemporary Impact 3.785 vs baselines ~3.1-3.2), suggesting retrieval can enhance rather than constrain creativity",
            "uuids": [
                "e2443.0"
            ]
        },
        {
            "text": "SciMuse's knowledge-graph-based retrieval achieved 70% top-1 precision for interesting ideas, suggesting structured retrieval can support creative ideation",
            "uuids": [
                "e2451.0"
            ]
        },
        {
            "text": "Some systems show consistent RAG benefits across both implementation and ideation without clear task-dependent variation",
            "uuids": [
                "e2457.3",
                "e2443.0"
            ]
        }
    ],
    "special_cases": [
        "Domains with rapidly evolving knowledge (e.g., COVID-19 research, AI/ML) may require continuous retrieval updates and show 30-50% larger RAG benefits than stable domains",
        "Tasks requiring rare or specialized knowledge may show 50-100% improvement with RAG even at moderate precision (0.6-0.7) because the alternative is complete failure",
        "Multi-hop reasoning tasks may require iterative retrieval with 2-3 rounds to achieve benefits, with single-round retrieval providing minimal or negative benefit",
        "Very large corpus sizes (&gt;10M documents) may require specialized retrieval infrastructure (e.g., graph-based, hierarchical) to maintain precision above the 0.7 threshold",
        "Interdisciplinary tasks may benefit more from diverse low-precision retrieval than narrow high-precision retrieval, reversing the usual precision-performance relationship",
        "Implementation tasks with clear correctness criteria (e.g., code with unit tests) show larger RAG benefits (30-40%) than implementation tasks with subjective criteria (15-25%)",
        "Entity-centric retrieval shows particularly large benefits (30-40% over document-level) for tasks requiring precise factual grounding (e.g., biomedical hypothesis generation) but smaller benefits (10-15%) for abstract reasoning tasks"
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [Established RAG benefits for factual tasks but didn't identify the ideation-implementation paradox or task-dependent performance]",
            "Guu et al. (2020) REALM: Retrieval-Augmented Language Model Pre-Training [Showed retrieval benefits for knowledge-intensive tasks but didn't characterize the anchoring bias in creative tasks or the precision threshold]",
            "Borgeaud et al. (2022) Improving language models by retrieving from trillions of tokens [Demonstrated scaling benefits of retrieval but didn't identify the plateau effect or task-dependent variation]",
            "Izacard et al. (2022) Few-shot Learning with Retrieval Augmented Language Models [Showed RAG benefits for few-shot learning but didn't characterize the creative ideation penalty]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
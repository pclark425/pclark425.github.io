<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositional Generalization Gap Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-177</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-177</p>
                <p><strong>Name:</strong> Compositional Generalization Gap Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments, based on the following results.</p>
                <p><strong>Description:</strong> Agents trained on compositional tasks exhibit systematic generalization gaps when tested on novel combinations of learned primitives, with gap magnitude depending on: (1) compositional depth/complexity, (2) type of composition (linguistic vs procedural vs hierarchical), (3) training distribution diversity, and (4) architectural support for composition. The gap is largest (30-50%) for deep linguistic/semantic compositions (≥3 supporting facts) and specific phenomena (certain relations, linguistic constructs), moderate (10-30%) for procedural compositions with curriculum support, and smallest (<10%) for hierarchical compositions with explicit structure (planning, modular policies). Curriculum strategies that increase training diversity, provide explicit compositional structure, or enable skill transfer can substantially reduce but not eliminate gaps in linguistic domains, while achieving near-complete transfer in procedural/hierarchical domains with appropriate architectural support.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <ol>
                <li><a href="../evaluations/theory-evaluation-20.html">theory-evaluation-20</a></li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Compositional generalization gaps vary systematically by domain: 30-50% for deep linguistic/semantic compositions, 10-30% for procedural compositions with curriculum, <10% for hierarchical compositions with explicit structure.</li>
                <li>The gap increases with compositional depth: n=1 shows ~5% gap, n=2 shows ~15% gap, n≥3 shows 30-50% gaps in linguistic domains.</li>
                <li>Certain composition types (GIVE events, double disjunctions, specific linguistic constructs) show systematically higher failure rates (50-80%) even with curriculum training.</li>
                <li>Curriculum strategies that increase training diversity reduce gaps by 10-20% in linguistic domains but can achieve near-complete transfer (>90% success) in procedural/hierarchical domains with appropriate architectural support.</li>
                <li>Small amounts of targeted OOD data (inoculation, ~500 samples) can rapidly close gaps to >90%, indicating the issue is primarily training distribution coverage rather than fundamental model capacity.</li>
                <li>Hierarchical architectures with explicit compositional structure (planning-guided HRL, modular policies, skill libraries) show 10-30× speedup and near-optimal transfer compared to monolithic networks.</li>
                <li>Transfer success depends critically on competency alignment: misaligned pretraining can hurt performance, while aligned pretraining provides 2-15× sample efficiency gains.</li>
                <li>Adaptive curricula that track learning progress enable autonomous discovery of compositional structure and achieve performance comparable to hand-designed curricula.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Models achieve >99% IID accuracy on bAbI but drop to 69.2% on compositional generalization splits (mix), with gaps largest for n≥3 supporting facts and specific phenomena like GIVE events. <a href="../results/extraction-result-1622.html#e1622.0" class="evidence-link">[e1622.0]</a> <a href="../results/extraction-result-1588.html#e1588.1" class="evidence-link">[e1588.1]</a> </li>
    <li>Diverse training (support-uniform) substantially improves compositional generalization vs inject or concat, but complex compositions (n≥3) remain <70% accurate, showing curriculum helps but doesn't eliminate gaps in linguistic domains. <a href="../results/extraction-result-1622.html#e1622.2" class="evidence-link">[e1622.2]</a> </li>
    <li>QAit agents memorize small training sets but fail to generalize to large/unseen distributions, especially on attribute (procedural) questions, with near-random performance (~50%) on unseen compositional tasks. <a href="../results/extraction-result-1602.html#e1602.0" class="evidence-link">[e1602.0]</a> </li>
    <li>Inoculation with ~500 OOD samples per question type rapidly improves performance to >90%, showing patterns are learnable but not acquired from original training distributions. <a href="../results/extraction-result-1622.html#e1622.3" class="evidence-link">[e1622.3]</a> </li>
    <li>BabyAI pretraining on GoToLocal helps GoTo target (2× reduction in demos needed) but GoToObjMaze pretraining hurts, showing transfer depends critically on competency alignment between source and target tasks. <a href="../results/extraction-result-1513.html#e1513.1" class="evidence-link">[e1513.1]</a> </li>
    <li>COMET models show limited compositional generalization on novel relation combinations despite high in-domain performance, with hierarchy meta-tokens providing minimal improvement. <a href="../results/extraction-result-1586.html#e1586.0" class="evidence-link">[e1586.0]</a> <a href="../results/extraction-result-1586.html#e1586.1" class="evidence-link">[e1586.1]</a> </li>
    <li>TextWorld agents trained on difficulty-distributed games show poor zero-shot generalization to harder levels without explicit curriculum, with baseline agents discovering only 17/107 items in Minecraft. <a href="../results/extraction-result-1562.html#e1562.0" class="evidence-link">[e1562.0]</a> <a href="../results/extraction-result-1601.html#e1601.0" class="evidence-link">[e1601.0]</a> </li>
    <li>Planning-guided HRL with option transfer achieves near-optimal policies in ~70k steps vs millions for baselines, showing hierarchical composition with explicit structure enables strong transfer (>25× speedup). <a href="../results/extraction-result-1570.html#e1570.0" class="evidence-link">[e1570.0]</a> </li>
    <li>Voyager's automatic curriculum with skill library enables compositional reuse, discovering 63 unique items (3.3× more than baselines) and unlocking tech-tree milestones 6-15× faster through compositional skill transfer. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>IMGEP with learning-progress curriculum autonomously discovers stepping-stone tasks and achieves exploration coverage comparable to hand-designed curricula through compositional goal exploration. <a href="../results/extraction-result-1613.html#e1613.0" class="evidence-link">[e1613.0]</a> </li>
    <li>Bidirectional learning-progress curriculum in Minecraft discovers 82/107 items vs 17 without curriculum, showing adaptive curricula enable compositional skill acquisition in procedural domains. <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> </li>
    <li>LSTM-DQN transfer from Homeworld to Homeworld2 (shuffled layout) accelerates learning, showing learned sequential representations transfer across compositionally similar tasks. <a href="../results/extraction-result-1529.html#e1529.2" class="evidence-link">[e1529.2]</a> </li>
    <li>KG-DQN with seeding, QA pretraining, and parameter transfer achieves 80% improvement in convergence steps and substantially higher final rewards through compositional knowledge transfer. <a href="../results/extraction-result-1609.html#e1609.0" class="evidence-link">[e1609.0]</a> <a href="../results/extraction-result-1609.html#e1609.1" class="evidence-link">[e1609.1]</a> <a href="../results/extraction-result-1609.html#e1609.2" class="evidence-link">[e1609.2]</a> </li>
    <li>H-KGA with scheduled task sampling improves performance from 0.57 to 0.76 on cooking games, with largest gains on difficult multi-step compositional tasks. <a href="../results/extraction-result-1579.html#e1579.0" class="evidence-link">[e1579.0]</a> </li>
    <li>Tiered curriculum training in TextWorld cooking games improves zero-shot performance from 50% to 64% overall, with 2-3× gains on mid-complexity compositional tasks. <a href="../results/extraction-result-1565.html#e1565.0" class="evidence-link">[e1565.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Any new linguistic/semantic compositional task will show 20-50% generalization gaps when tested on novel combinations without targeted training.</li>
                <li>Procedural compositional tasks with adaptive curricula and skill transfer will show <20% gaps and 5-10× sample efficiency gains over uniform training.</li>
                <li>Architectures with explicit hierarchical structure (modular networks, program synthesis, planning) will show 10-30% smaller gaps than monolithic networks in linguistic domains and near-perfect transfer in procedural domains.</li>
                <li>Curricula that explicitly expose diverse compositional patterns (not necessarily all combinations) will reduce gaps by 15-25% compared to random sampling in linguistic domains.</li>
                <li>Pretraining on compositionally-aligned source tasks will provide 2-5× sample efficiency gains, while misaligned pretraining will degrade performance by 10-30%.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a training distribution or architectural innovation that enables perfect compositional generalization in linguistic domains without explicit enumeration of all combinations.</li>
                <li>Whether compositional generalization gaps scale indefinitely with depth, or plateau at some level of complexity (current evidence only goes to n≤4).</li>
                <li>Whether the fundamental difference between linguistic and procedural compositional generalization is due to task structure, evaluation methodology, or architectural choices.</li>
                <li>Whether very large pretrained models (>100B parameters) can overcome compositional gaps through scale alone, or if architectural changes are necessary.</li>
                <li>Whether there exists a unified curriculum strategy that works equally well across linguistic, procedural, and hierarchical compositional domains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding architectures or training procedures that achieve >95% compositional generalization on deep linguistic compositions (n≥3) without targeted inoculation would challenge the fundamental limitation claim for linguistic domains.</li>
                <li>Demonstrating that gaps disappear with sufficient training data (without targeted inoculation or curriculum) would suggest the issue is purely sample efficiency rather than a systematic limitation.</li>
                <li>Showing that procedural compositional gaps persist even with hierarchical architectures and adaptive curricula would challenge the domain-specific nature of the theory.</li>
                <li>Finding that misaligned pretraining consistently helps rather than hurts would challenge the competency alignment principle.</li>
                <li>Demonstrating that monolithic networks can match hierarchical architectures on compositional transfer would challenge the architectural structure claims.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Why certain composition types (GIVE events, double disjunctions, specific linguistic constructs) are systematically harder than others, even with diverse training. <a href="../results/extraction-result-1622.html#e1622.0" class="evidence-link">[e1622.0]</a> </li>
    <li>The exact mechanisms by which inoculation enables rapid generalization improvement (>90% with ~500 samples) when diverse training with much larger datasets fails. <a href="../results/extraction-result-1622.html#e1622.3" class="evidence-link">[e1622.3]</a> </li>
    <li>Why procedural/hierarchical domains show near-complete compositional transfer while linguistic domains show persistent gaps, despite both involving composition. <a href="../results/extraction-result-1570.html#e1570.0" class="evidence-link">[e1570.0]</a> <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> <a href="../results/extraction-result-1622.html#e1622.0" class="evidence-link">[e1622.0]</a> </li>
    <li>The relationship between compositional depth and gap magnitude beyond n=4 (no evidence for deeper compositions). <a href="../results/extraction-result-1622.html#e1622.0" class="evidence-link">[e1622.0]</a> <a href="../results/extraction-result-1588.html#e1588.1" class="evidence-link">[e1588.1]</a> </li>
    <li>Why some curriculum strategies (learning-progress, task-difficulty) work well across domains while others (expertise-based) are domain-specific. <a href="../results/extraction-result-1505.html#e1505.0" class="evidence-link">[e1505.0]</a> <a href="../results/extraction-result-1505.html#e1505.1" class="evidence-link">[e1505.1]</a> <a href="../results/extraction-result-1505.html#e1505.2" class="evidence-link">[e1505.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks [Foundational work identifying compositional generalization gaps in seq2seq models]</li>
    <li>Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data [Comprehensive study of compositional generalization with SCAN and CFQ benchmarks]</li>
    <li>Bahdanau et al. (2019) Systematic Generalization: What Is Required and Can It Be Learned? [Analysis of requirements for systematic compositional generalization]</li>
    <li>Andreas et al. (2016) Neural Module Networks [Modular architectures for compositional reasoning, showing benefits of explicit structure]</li>
    <li>Jiang et al. (2019) Learning to Learn Programs from Examples: Going Beyond Program Structure [Work on compositional program synthesis and transfer]</li>
    <li>Hill et al. (2019) Environmental drivers of systematicity and generalization in a situated agent [Study of compositional generalization in embodied agents]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositional Generalization Gap Theory",
    "theory_description": "Agents trained on compositional tasks exhibit systematic generalization gaps when tested on novel combinations of learned primitives, with gap magnitude depending on: (1) compositional depth/complexity, (2) type of composition (linguistic vs procedural vs hierarchical), (3) training distribution diversity, and (4) architectural support for composition. The gap is largest (30-50%) for deep linguistic/semantic compositions (≥3 supporting facts) and specific phenomena (certain relations, linguistic constructs), moderate (10-30%) for procedural compositions with curriculum support, and smallest (&lt;10%) for hierarchical compositions with explicit structure (planning, modular policies). Curriculum strategies that increase training diversity, provide explicit compositional structure, or enable skill transfer can substantially reduce but not eliminate gaps in linguistic domains, while achieving near-complete transfer in procedural/hierarchical domains with appropriate architectural support.",
    "supporting_evidence": [
        {
            "text": "Models achieve &gt;99% IID accuracy on bAbI but drop to 69.2% on compositional generalization splits (mix), with gaps largest for n≥3 supporting facts and specific phenomena like GIVE events.",
            "uuids": [
                "e1622.0",
                "e1588.1"
            ]
        },
        {
            "text": "Diverse training (support-uniform) substantially improves compositional generalization vs inject or concat, but complex compositions (n≥3) remain &lt;70% accurate, showing curriculum helps but doesn't eliminate gaps in linguistic domains.",
            "uuids": [
                "e1622.2"
            ]
        },
        {
            "text": "QAit agents memorize small training sets but fail to generalize to large/unseen distributions, especially on attribute (procedural) questions, with near-random performance (~50%) on unseen compositional tasks.",
            "uuids": [
                "e1602.0"
            ]
        },
        {
            "text": "Inoculation with ~500 OOD samples per question type rapidly improves performance to &gt;90%, showing patterns are learnable but not acquired from original training distributions.",
            "uuids": [
                "e1622.3"
            ]
        },
        {
            "text": "BabyAI pretraining on GoToLocal helps GoTo target (2× reduction in demos needed) but GoToObjMaze pretraining hurts, showing transfer depends critically on competency alignment between source and target tasks.",
            "uuids": [
                "e1513.1"
            ]
        },
        {
            "text": "COMET models show limited compositional generalization on novel relation combinations despite high in-domain performance, with hierarchy meta-tokens providing minimal improvement.",
            "uuids": [
                "e1586.0",
                "e1586.1"
            ]
        },
        {
            "text": "TextWorld agents trained on difficulty-distributed games show poor zero-shot generalization to harder levels without explicit curriculum, with baseline agents discovering only 17/107 items in Minecraft.",
            "uuids": [
                "e1562.0",
                "e1601.0"
            ]
        },
        {
            "text": "Planning-guided HRL with option transfer achieves near-optimal policies in ~70k steps vs millions for baselines, showing hierarchical composition with explicit structure enables strong transfer (&gt;25× speedup).",
            "uuids": [
                "e1570.0"
            ]
        },
        {
            "text": "Voyager's automatic curriculum with skill library enables compositional reuse, discovering 63 unique items (3.3× more than baselines) and unlocking tech-tree milestones 6-15× faster through compositional skill transfer.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "IMGEP with learning-progress curriculum autonomously discovers stepping-stone tasks and achieves exploration coverage comparable to hand-designed curricula through compositional goal exploration.",
            "uuids": [
                "e1613.0"
            ]
        },
        {
            "text": "Bidirectional learning-progress curriculum in Minecraft discovers 82/107 items vs 17 without curriculum, showing adaptive curricula enable compositional skill acquisition in procedural domains.",
            "uuids": [
                "e1601.4",
                "e1590.0"
            ]
        },
        {
            "text": "LSTM-DQN transfer from Homeworld to Homeworld2 (shuffled layout) accelerates learning, showing learned sequential representations transfer across compositionally similar tasks.",
            "uuids": [
                "e1529.2"
            ]
        },
        {
            "text": "KG-DQN with seeding, QA pretraining, and parameter transfer achieves 80% improvement in convergence steps and substantially higher final rewards through compositional knowledge transfer.",
            "uuids": [
                "e1609.0",
                "e1609.1",
                "e1609.2"
            ]
        },
        {
            "text": "H-KGA with scheduled task sampling improves performance from 0.57 to 0.76 on cooking games, with largest gains on difficult multi-step compositional tasks.",
            "uuids": [
                "e1579.0"
            ]
        },
        {
            "text": "Tiered curriculum training in TextWorld cooking games improves zero-shot performance from 50% to 64% overall, with 2-3× gains on mid-complexity compositional tasks.",
            "uuids": [
                "e1565.0"
            ]
        }
    ],
    "theory_statements": [
        "Compositional generalization gaps vary systematically by domain: 30-50% for deep linguistic/semantic compositions, 10-30% for procedural compositions with curriculum, &lt;10% for hierarchical compositions with explicit structure.",
        "The gap increases with compositional depth: n=1 shows ~5% gap, n=2 shows ~15% gap, n≥3 shows 30-50% gaps in linguistic domains.",
        "Certain composition types (GIVE events, double disjunctions, specific linguistic constructs) show systematically higher failure rates (50-80%) even with curriculum training.",
        "Curriculum strategies that increase training diversity reduce gaps by 10-20% in linguistic domains but can achieve near-complete transfer (&gt;90% success) in procedural/hierarchical domains with appropriate architectural support.",
        "Small amounts of targeted OOD data (inoculation, ~500 samples) can rapidly close gaps to &gt;90%, indicating the issue is primarily training distribution coverage rather than fundamental model capacity.",
        "Hierarchical architectures with explicit compositional structure (planning-guided HRL, modular policies, skill libraries) show 10-30× speedup and near-optimal transfer compared to monolithic networks.",
        "Transfer success depends critically on competency alignment: misaligned pretraining can hurt performance, while aligned pretraining provides 2-15× sample efficiency gains.",
        "Adaptive curricula that track learning progress enable autonomous discovery of compositional structure and achieve performance comparable to hand-designed curricula."
    ],
    "new_predictions_likely": [
        "Any new linguistic/semantic compositional task will show 20-50% generalization gaps when tested on novel combinations without targeted training.",
        "Procedural compositional tasks with adaptive curricula and skill transfer will show &lt;20% gaps and 5-10× sample efficiency gains over uniform training.",
        "Architectures with explicit hierarchical structure (modular networks, program synthesis, planning) will show 10-30% smaller gaps than monolithic networks in linguistic domains and near-perfect transfer in procedural domains.",
        "Curricula that explicitly expose diverse compositional patterns (not necessarily all combinations) will reduce gaps by 15-25% compared to random sampling in linguistic domains.",
        "Pretraining on compositionally-aligned source tasks will provide 2-5× sample efficiency gains, while misaligned pretraining will degrade performance by 10-30%."
    ],
    "new_predictions_unknown": [
        "Whether there exists a training distribution or architectural innovation that enables perfect compositional generalization in linguistic domains without explicit enumeration of all combinations.",
        "Whether compositional generalization gaps scale indefinitely with depth, or plateau at some level of complexity (current evidence only goes to n≤4).",
        "Whether the fundamental difference between linguistic and procedural compositional generalization is due to task structure, evaluation methodology, or architectural choices.",
        "Whether very large pretrained models (&gt;100B parameters) can overcome compositional gaps through scale alone, or if architectural changes are necessary.",
        "Whether there exists a unified curriculum strategy that works equally well across linguistic, procedural, and hierarchical compositional domains."
    ],
    "negative_experiments": [
        "Finding architectures or training procedures that achieve &gt;95% compositional generalization on deep linguistic compositions (n≥3) without targeted inoculation would challenge the fundamental limitation claim for linguistic domains.",
        "Demonstrating that gaps disappear with sufficient training data (without targeted inoculation or curriculum) would suggest the issue is purely sample efficiency rather than a systematic limitation.",
        "Showing that procedural compositional gaps persist even with hierarchical architectures and adaptive curricula would challenge the domain-specific nature of the theory.",
        "Finding that misaligned pretraining consistently helps rather than hurts would challenge the competency alignment principle.",
        "Demonstrating that monolithic networks can match hierarchical architectures on compositional transfer would challenge the architectural structure claims."
    ],
    "unaccounted_for": [
        {
            "text": "Why certain composition types (GIVE events, double disjunctions, specific linguistic constructs) are systematically harder than others, even with diverse training.",
            "uuids": [
                "e1622.0"
            ]
        },
        {
            "text": "The exact mechanisms by which inoculation enables rapid generalization improvement (&gt;90% with ~500 samples) when diverse training with much larger datasets fails.",
            "uuids": [
                "e1622.3"
            ]
        },
        {
            "text": "Why procedural/hierarchical domains show near-complete compositional transfer while linguistic domains show persistent gaps, despite both involving composition.",
            "uuids": [
                "e1570.0",
                "e1498.0",
                "e1622.0"
            ]
        },
        {
            "text": "The relationship between compositional depth and gap magnitude beyond n=4 (no evidence for deeper compositions).",
            "uuids": [
                "e1622.0",
                "e1588.1"
            ]
        },
        {
            "text": "Why some curriculum strategies (learning-progress, task-difficulty) work well across domains while others (expertise-based) are domain-specific.",
            "uuids": [
                "e1505.0",
                "e1505.1",
                "e1505.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Planning-guided HRL and Voyager show near-perfect compositional transfer in procedural domains, contradicting the claim that gaps are universal across all compositional tasks.",
            "uuids": [
                "e1570.0",
                "e1498.0"
            ]
        },
        {
            "text": "Some pretrained models show 20-50% smaller gaps than non-pretrained models, suggesting pretraining helps substantially, though gaps persist.",
            "uuids": [
                "e1622.0"
            ]
        },
        {
            "text": "Adaptive curricula in Minecraft achieve 82/107 item discovery vs 17 without curriculum, showing near-complete compositional learning in some procedural domains.",
            "uuids": [
                "e1601.4"
            ]
        },
        {
            "text": "LSTM-DQN transfer shows successful compositional transfer of sequential representations, suggesting some architectures can achieve good transfer without explicit hierarchical structure.",
            "uuids": [
                "e1529.2"
            ]
        }
    ],
    "special_cases": [
        "For very simple compositions (n≤2), gaps may be minimal (&lt;10%) even without targeted training in linguistic domains.",
        "In procedural/hierarchical domains with explicit compositional structure (planning, skill libraries, modular policies), gaps can be reduced to &lt;10% with appropriate curricula.",
        "When using very large pretrained models, gaps may be reduced by 20-50% but not eliminated in linguistic domains.",
        "In domains with limited compositional structure or shallow composition depth, the gap may not manifest significantly.",
        "For tasks with strong competency alignment between source and target, pretraining can provide 2-15× sample efficiency gains and near-complete transfer.",
        "Inoculation with small amounts of targeted OOD data (~500 samples) can rapidly close gaps to &gt;90%, representing a special case where minimal targeted exposure enables generalization.",
        "Adaptive curricula that track learning progress can autonomously discover compositional structure and achieve performance comparable to hand-designed curricula in procedural domains."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Lake & Baroni (2018) Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks [Foundational work identifying compositional generalization gaps in seq2seq models]",
            "Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data [Comprehensive study of compositional generalization with SCAN and CFQ benchmarks]",
            "Bahdanau et al. (2019) Systematic Generalization: What Is Required and Can It Be Learned? [Analysis of requirements for systematic compositional generalization]",
            "Andreas et al. (2016) Neural Module Networks [Modular architectures for compositional reasoning, showing benefits of explicit structure]",
            "Jiang et al. (2019) Learning to Learn Programs from Examples: Going Beyond Program Structure [Work on compositional program synthesis and transfer]",
            "Hill et al. (2019) Environmental drivers of systematicity and generalization in a situated agent [Study of compositional generalization in embodied agents]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 6,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
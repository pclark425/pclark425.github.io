<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Fidelity Theory of Graph-to-Text Representation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1239</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1239</p>
                <p><strong>Name:</strong> Semantic Fidelity Theory of Graph-to-Text Representation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal representation for converting graphs into text for language model training is one that maximizes semantic fidelity: the preservation and explicit encoding of all graph semantics (nodes, edges, attributes, and higher-order structures) in a form that is both unambiguous and compositional for language models. The theory asserts that representations which maintain a bijective mapping between graph structures and their textual forms, while leveraging natural language's compositionality, will yield superior downstream performance in language model training and generalization.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; encodes &#8594; all_graph_semantics<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_representation &#8594; is_bijective_with &#8594; original_graph</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model_trained_on_representation &#8594; achieves &#8594; maximal_graph_understanding</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that lossless graph-to-text conversions (e.g., canonicalized SMILES for molecules, or linearized AMR for semantic graphs) enable models to reconstruct the original graph and perform better on graph-based tasks. </li>
    <li>Ambiguous or lossy representations (e.g., naive adjacency lists) lead to degraded model performance and inability to recover original graph structure. </li>
    <li>Canonicalization in SMILES and AMR ensures that each graph maps to a unique string, supporting bijectivity and semantic preservation. </li>
    <li>Structural probes in NLP demonstrate that models trained on semantically complete representations retain more graph information. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to prior work on canonicalization and semantic parsing, this law formalizes the necessity of bijective, semantically complete representations for optimal language model training.</p>            <p><strong>What Already Exists:</strong> Existing work recognizes the importance of preserving graph information in representations (e.g., canonical SMILES, AMR linearizations).</p>            <p><strong>What is Novel:</strong> The explicit requirement of bijectivity and full semantic preservation as necessary and sufficient for maximal model understanding is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Weininger (1988) SMILES, a chemical language and information system [SMILES as a canonical, lossless molecular graph representation]</li>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [AMR linearization for semantic graphs]</li>
    <li>Ribeiro et al. (2020) Structural Probes for Graph Representations in NLP [Probing semantic preservation in graph-to-text tasks]</li>
</ul>
            <h3>Statement 1: Compositionality Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; aligns_with &#8594; natural_language_compositionality</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model_trained_on_representation &#8594; exhibits &#8594; improved_generalization_and_transfer</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Representations that mirror natural language compositionality (e.g., nested clauses for subgraphs, or phrase-structure for hierarchical graphs) facilitate better transfer and generalization in downstream tasks. </li>
    <li>Non-compositional or arbitrary linearizations hinder model's ability to generalize to unseen graph structures. </li>
    <li>Compositionality in neural models is linked to systematic generalization (Lake & Baroni, 2018). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While compositionality is foundational in language, its explicit role in graph-to-text representation for LMs is not formalized in prior work.</p>            <p><strong>What Already Exists:</strong> Compositionality is a well-known property in linguistics and cognitive science, and some graph-to-text systems exploit it.</p>            <p><strong>What is Novel:</strong> The law's assertion that compositional alignment is a key determinant of generalization in graph-to-text language model training is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [Compositionality in neural models]</li>
    <li>Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [AMR linearization and compositionality]</li>
    <li>Li et al. (2022) Graph-to-Text Generation with Data Structure-Aware Pre-training [Compositionality in graph-to-text pretraining]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new graph-to-text representation is designed to be bijective and semantically complete, language models trained on it will outperform those trained on lossy or ambiguous representations in graph reconstruction and reasoning tasks.</li>
                <li>Representations that mirror natural language compositionality (e.g., using nested clauses for subgraphs) will yield better zero-shot generalization to novel graph structures.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>For highly complex, cyclic, or hypergraph structures, it is unknown whether a bijective, compositional representation can be constructed that remains tractable for language models.</li>
                <li>It is unclear whether compositional alignment alone is sufficient for generalization in the presence of extreme graph sparsity or density.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a bijective, semantically complete representation does not yield improved model performance over lossy representations, the theory would be called into question.</li>
                <li>If compositional alignment with natural language does not improve generalization, the theory's second law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of graph size and complexity on the tractability of bijective, compositional representations is not fully addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and extends existing ideas, but its formal, law-like articulation and the focus on bijectivity and compositionality as necessary and sufficient conditions is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Weininger (1988) SMILES, a chemical language and information system [Canonical, lossless molecular graph representation]</li>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [AMR linearization for semantic graphs]</li>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [Compositionality in neural models]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Fidelity Theory of Graph-to-Text Representation",
    "theory_description": "This theory posits that the ideal representation for converting graphs into text for language model training is one that maximizes semantic fidelity: the preservation and explicit encoding of all graph semantics (nodes, edges, attributes, and higher-order structures) in a form that is both unambiguous and compositional for language models. The theory asserts that representations which maintain a bijective mapping between graph structures and their textual forms, while leveraging natural language's compositionality, will yield superior downstream performance in language model training and generalization.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Preservation Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "encodes",
                        "object": "all_graph_semantics"
                    },
                    {
                        "subject": "graph_representation",
                        "relation": "is_bijective_with",
                        "object": "original_graph"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model_trained_on_representation",
                        "relation": "achieves",
                        "object": "maximal_graph_understanding"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that lossless graph-to-text conversions (e.g., canonicalized SMILES for molecules, or linearized AMR for semantic graphs) enable models to reconstruct the original graph and perform better on graph-based tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or lossy representations (e.g., naive adjacency lists) lead to degraded model performance and inability to recover original graph structure.",
                        "uuids": []
                    },
                    {
                        "text": "Canonicalization in SMILES and AMR ensures that each graph maps to a unique string, supporting bijectivity and semantic preservation.",
                        "uuids": []
                    },
                    {
                        "text": "Structural probes in NLP demonstrate that models trained on semantically complete representations retain more graph information.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Existing work recognizes the importance of preserving graph information in representations (e.g., canonical SMILES, AMR linearizations).",
                    "what_is_novel": "The explicit requirement of bijectivity and full semantic preservation as necessary and sufficient for maximal model understanding is novel.",
                    "classification_explanation": "While related to prior work on canonicalization and semantic parsing, this law formalizes the necessity of bijective, semantically complete representations for optimal language model training.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Weininger (1988) SMILES, a chemical language and information system [SMILES as a canonical, lossless molecular graph representation]",
                        "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [AMR linearization for semantic graphs]",
                        "Ribeiro et al. (2020) Structural Probes for Graph Representations in NLP [Probing semantic preservation in graph-to-text tasks]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Compositionality Alignment Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "aligns_with",
                        "object": "natural_language_compositionality"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model_trained_on_representation",
                        "relation": "exhibits",
                        "object": "improved_generalization_and_transfer"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Representations that mirror natural language compositionality (e.g., nested clauses for subgraphs, or phrase-structure for hierarchical graphs) facilitate better transfer and generalization in downstream tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Non-compositional or arbitrary linearizations hinder model's ability to generalize to unseen graph structures.",
                        "uuids": []
                    },
                    {
                        "text": "Compositionality in neural models is linked to systematic generalization (Lake & Baroni, 2018).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositionality is a well-known property in linguistics and cognitive science, and some graph-to-text systems exploit it.",
                    "what_is_novel": "The law's assertion that compositional alignment is a key determinant of generalization in graph-to-text language model training is novel.",
                    "classification_explanation": "While compositionality is foundational in language, its explicit role in graph-to-text representation for LMs is not formalized in prior work.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [Compositionality in neural models]",
                        "Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [AMR linearization and compositionality]",
                        "Li et al. (2022) Graph-to-Text Generation with Data Structure-Aware Pre-training [Compositionality in graph-to-text pretraining]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new graph-to-text representation is designed to be bijective and semantically complete, language models trained on it will outperform those trained on lossy or ambiguous representations in graph reconstruction and reasoning tasks.",
        "Representations that mirror natural language compositionality (e.g., using nested clauses for subgraphs) will yield better zero-shot generalization to novel graph structures."
    ],
    "new_predictions_unknown": [
        "For highly complex, cyclic, or hypergraph structures, it is unknown whether a bijective, compositional representation can be constructed that remains tractable for language models.",
        "It is unclear whether compositional alignment alone is sufficient for generalization in the presence of extreme graph sparsity or density."
    ],
    "negative_experiments": [
        "If a bijective, semantically complete representation does not yield improved model performance over lossy representations, the theory would be called into question.",
        "If compositional alignment with natural language does not improve generalization, the theory's second law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of graph size and complexity on the tractability of bijective, compositional representations is not fully addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that language models can learn useful graph representations even from lossy or non-bijective encodings, especially with large-scale pretraining.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with non-trivial automorphisms (e.g., isomorphic subgraphs) may require additional disambiguation in text.",
        "Very large graphs may require hierarchical or chunked representations to remain tractable."
    ],
    "existing_theory": {
        "what_already_exists": "Prior work has explored canonicalization and semantic preservation in graph-to-text, and the importance of compositionality in language.",
        "what_is_novel": "The explicit formalization of bijective, semantically complete, and compositional alignment as necessary for ideal graph-to-text representations is novel.",
        "classification_explanation": "The theory synthesizes and extends existing ideas, but its formal, law-like articulation and the focus on bijectivity and compositionality as necessary and sufficient conditions is new.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Weininger (1988) SMILES, a chemical language and information system [Canonical, lossless molecular graph representation]",
            "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [AMR linearization for semantic graphs]",
            "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [Compositionality in neural models]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-611",
    "original_theory_name": "Multimodal Alignment and Compactness Principle for Graph-to-Text Representations",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
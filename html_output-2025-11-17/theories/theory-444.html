<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Selective Episodic Memory Effectiveness Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-444</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-444</p>
                <p><strong>Name:</strong> Selective Episodic Memory Effectiveness Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language model agents can best use memory when solving text games, based on the following results.</p>
                <p><strong>Description:</strong> In text-based games with partial observability and exploration requirements, episodic memory mechanisms are most effective when they are selective and structured rather than comprehensive. Specifically: (1) Within-episode novelty rewards (episodic discovery bonuses) that reset per episode outperform cumulative counting bonuses by encouraging memory-based exploration; (2) Episodic retrieval benefits from relevance-based filtering (e.g., object-centric, task-similarity) rather than pure recency; (3) Raw episodic storage is less effective than processed episodic memories (e.g., verbal reflections, extracted insights); (4) The effectiveness of episodic memory depends on the interaction between memory structure, retrieval strategy, and the agent's ability to utilize the retrieved information.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Law 0</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses &#8594; episodic discovery bonus that resets per episode<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; has &#8594; recurrent memory architecture<span style="color: #888888;">, and</span></div>
        <div>&#8226; environment &#8594; requires &#8594; exploration and state revisitation avoidance</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; learns faster than &#8594; agents with cumulative counting bonuses<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; generalizes better to &#8594; unseen game configurations</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Episodic discovery bonus substantially improved learning and generalization compared to cumulative bonuses, especially in medium/hard modes and multi-game training <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
    <li>DRQN++ with episodic bonus produced best learning and generalization across harder and multi-game settings <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
    <li>Episodic bonus outperforms cumulative bonuses for encouraging within-episode discovery and teaching recurrent agents to use memory <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
    <li>Episodic bonus is particularly effective for harder maps and multi-game training where avoiding revisits within an episode is important <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Law 1</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; retrieves from &#8594; episodic memory using relevance-based filtering<span style="color: #888888;">, and</span></div>
        <div>&#8226; relevance filter &#8594; is based on &#8594; task-specific features (e.g., shared objects, task similarity)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; performs better than &#8594; agents using pure recency-based retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieved memories &#8594; contain less &#8594; noise and irrelevant information</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Object-centric, time-sensitive retrieval (K=2 most recent observations sharing objects) improved performance and convergence compared to using only current observation <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> </li>
    <li>Pure recency-based retrieval performed worse or provided little benefit early in training; object-linking and time-sensitivity are important <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> </li>
    <li>Task-similarity Faiss retrieval of trajectories beat reason-similarity and random sampling <a href="../results/extraction-result-2778.html#e2778.1" class="evidence-link">[e2778.1]</a> </li>
    <li>Older historical observations may be invalidated by new events; pure recency retrieval can add noisy/low-utility history <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Law 2</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; stores &#8594; processed episodic memories (e.g., verbal reflections, extracted insights)<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; retrieves and uses &#8594; these processed memories for decision-making</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; performs better than &#8594; agents storing only raw episodic trajectories<span style="color: #888888;">, and</span></div>
        <div>&#8226; processed memories &#8594; provide &#8594; higher-level guidance and abstracted lessons</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Verbal self-reflection summaries stored in episodic buffer outperformed raw episodic trajectory inclusion by ~8% absolute accuracy <a href="../results/extraction-result-2709.html#e2709.2" class="evidence-link">[e2709.2]</a> </li>
    <li>Raw trajectory inclusion forces agent to perform both error-identification and repair without intermediate distilled guidance, making iterative improvement less efficient <a href="../results/extraction-result-2709.html#e2709.2" class="evidence-link">[e2709.2]</a> </li>
    <li>ExpeL with extracted insights (59.0%) outperformed retrieve-only variant (54.5%), showing that combining retrieval with high-level extracted insights yields largest gains <a href="../results/extraction-result-2778.html#e2778.1" class="evidence-link">[e2778.1]</a> </li>
    <li>Retrieval of semantically similar successful trajectories alone yields large gains over non-retrieval baselines, but combining with abstracted insights is most effective <a href="../results/extraction-result-2778.html#e2778.1" class="evidence-link">[e2778.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Law 3</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; stores &#8594; complete unfiltered episodic history<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; long-horizon reasoning or structured retrieval</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; performs worse than &#8594; agents with structured or selective memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; full history &#8594; causes &#8594; token inefficiency and information overload</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Full history memory performed poorly (Treasure Hunt 0.47, Cooking 0.18, Cleaning 0.05) compared to structured graph memory <a href="../results/extraction-result-2741.html#e2741.1" class="evidence-link">[e2741.1]</a> </li>
    <li>Full history is highly token-inefficient and burdens LLM with irrelevant or outdated details, reducing effectiveness in long-horizon tasks <a href="../results/extraction-result-2741.html#e2741.1" class="evidence-link">[e2741.1]</a> </li>
    <li>Full history scales poorly with time and contains outdated or confusing facts unless additional filtering is applied <a href="../results/extraction-result-2741.html#e2741.1" class="evidence-link">[e2741.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Episodic bonuses combined with object-centric retrieval will show synergistic effects, outperforming either mechanism alone</li>
                <li>In environments with cyclic structures, episodic bonuses will show greater benefits compared to tree-structured environments where simple strategies suffice</li>
                <li>Agents that process episodic memories into hierarchical summaries (e.g., sub-goal completion markers) will outperform those storing flat episodic sequences</li>
                <li>Episodic memory with adaptive capacity management (pruning old/irrelevant memories) will outperform fixed-capacity episodic buffers in long episodes</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether learned episodic bonus functions (e.g., neural curiosity modules) can outperform hand-crafted binary episodic bonuses across diverse game types</li>
                <li>Whether episodic memory remains effective in very long episodes (>1000 steps) where memory capacity and retrieval efficiency become critical bottlenecks</li>
                <li>Whether combining multiple episodic memory mechanisms (discovery bonuses + trajectory retrieval + verbal reflections) provides additive benefits or causes interference</li>
                <li>Whether episodic memory effectiveness transfers across different game genres (e.g., from exploration-heavy to dialogue-heavy games)</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding environments where cumulative (across-episode) counting bonuses consistently outperform episodic bonuses would challenge the within-episode novelty hypothesis</li>
                <li>Demonstrating that unfiltered full-history episodic memory outperforms selective retrieval in certain task types would question the selectivity principle</li>
                <li>Showing that raw episodic trajectories provide equal or better performance than processed reflections would challenge the processing hypothesis</li>
                <li>Finding that episodic bonuses provide no benefit when combined with perfect state tracking (e.g., full observability) would question their necessity beyond partial observability mitigation</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How episodic memory mechanisms interact with different base architectures beyond RNNs (e.g., Transformers, graph networks) is not fully characterized <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
    <li>The optimal bonus magnitude, decay schedule, and capacity limits for episodic bonuses across different game complexities are not well characterized <a href="../results/extraction-result-2767.html#e2767.2" class="evidence-link">[e2767.2]</a> </li>
    <li>How to automatically determine the appropriate level of processing/abstraction for episodic memories in different task contexts <a href="../results/extraction-result-2709.html#e2709.2" class="evidence-link">[e2709.2]</a> <a href="../results/extraction-result-2778.html#e2778.1" class="evidence-link">[e2778.1]</a> </li>
    <li>The computational trade-offs between episodic memory storage/retrieval costs and performance benefits are not systematically analyzed <a href="../results/extraction-result-2727.html#e2727.0" class="evidence-link">[e2727.0]</a> <a href="../results/extraction-result-2741.html#e2741.1" class="evidence-link">[e2741.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Yuan et al. (2018) Counting to Explore and Generalize in Text-based Games [Introduced episodic discovery bonuses for text games, showing benefits over cumulative counting]</li>
    <li>Bellemare et al. (2016) Unifying Count-Based Exploration and Intrinsic Motivation [General theory of count-based exploration and intrinsic motivation in RL]</li>
    <li>Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Showed verbal episodic reflections outperform raw trajectory storage]</li>
    <li>Zhao et al. (2023) ExpeL: LLM Agents Are Experiential Learners [Demonstrated importance of processing episodic trajectories into insights]</li>
    <li>He et al. (2020) Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension [Showed object-centric episodic retrieval outperforms pure recency]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Selective Episodic Memory Effectiveness Theory",
    "theory_description": "In text-based games with partial observability and exploration requirements, episodic memory mechanisms are most effective when they are selective and structured rather than comprehensive. Specifically: (1) Within-episode novelty rewards (episodic discovery bonuses) that reset per episode outperform cumulative counting bonuses by encouraging memory-based exploration; (2) Episodic retrieval benefits from relevance-based filtering (e.g., object-centric, task-similarity) rather than pure recency; (3) Raw episodic storage is less effective than processed episodic memories (e.g., verbal reflections, extracted insights); (4) The effectiveness of episodic memory depends on the interaction between memory structure, retrieval strategy, and the agent's ability to utilize the retrieved information.",
    "theory_statements": [
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses",
                        "object": "episodic discovery bonus that resets per episode"
                    },
                    {
                        "subject": "agent",
                        "relation": "has",
                        "object": "recurrent memory architecture"
                    },
                    {
                        "subject": "environment",
                        "relation": "requires",
                        "object": "exploration and state revisitation avoidance"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "learns faster than",
                        "object": "agents with cumulative counting bonuses"
                    },
                    {
                        "subject": "agent",
                        "relation": "generalizes better to",
                        "object": "unseen game configurations"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Episodic discovery bonus substantially improved learning and generalization compared to cumulative bonuses, especially in medium/hard modes and multi-game training",
                        "uuids": [
                            "e2767.2"
                        ]
                    },
                    {
                        "text": "DRQN++ with episodic bonus produced best learning and generalization across harder and multi-game settings",
                        "uuids": [
                            "e2767.2"
                        ]
                    },
                    {
                        "text": "Episodic bonus outperforms cumulative bonuses for encouraging within-episode discovery and teaching recurrent agents to use memory",
                        "uuids": [
                            "e2767.2"
                        ]
                    },
                    {
                        "text": "Episodic bonus is particularly effective for harder maps and multi-game training where avoiding revisits within an episode is important",
                        "uuids": [
                            "e2767.2"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "retrieves from",
                        "object": "episodic memory using relevance-based filtering"
                    },
                    {
                        "subject": "relevance filter",
                        "relation": "is based on",
                        "object": "task-specific features (e.g., shared objects, task similarity)"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "performs better than",
                        "object": "agents using pure recency-based retrieval"
                    },
                    {
                        "subject": "retrieved memories",
                        "relation": "contain less",
                        "object": "noise and irrelevant information"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Object-centric, time-sensitive retrieval (K=2 most recent observations sharing objects) improved performance and convergence compared to using only current observation",
                        "uuids": [
                            "e2727.0"
                        ]
                    },
                    {
                        "text": "Pure recency-based retrieval performed worse or provided little benefit early in training; object-linking and time-sensitivity are important",
                        "uuids": [
                            "e2727.0"
                        ]
                    },
                    {
                        "text": "Task-similarity Faiss retrieval of trajectories beat reason-similarity and random sampling",
                        "uuids": [
                            "e2778.1"
                        ]
                    },
                    {
                        "text": "Older historical observations may be invalidated by new events; pure recency retrieval can add noisy/low-utility history",
                        "uuids": [
                            "e2727.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "stores",
                        "object": "processed episodic memories (e.g., verbal reflections, extracted insights)"
                    },
                    {
                        "subject": "agent",
                        "relation": "retrieves and uses",
                        "object": "these processed memories for decision-making"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "performs better than",
                        "object": "agents storing only raw episodic trajectories"
                    },
                    {
                        "subject": "processed memories",
                        "relation": "provide",
                        "object": "higher-level guidance and abstracted lessons"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Verbal self-reflection summaries stored in episodic buffer outperformed raw episodic trajectory inclusion by ~8% absolute accuracy",
                        "uuids": [
                            "e2709.2"
                        ]
                    },
                    {
                        "text": "Raw trajectory inclusion forces agent to perform both error-identification and repair without intermediate distilled guidance, making iterative improvement less efficient",
                        "uuids": [
                            "e2709.2"
                        ]
                    },
                    {
                        "text": "ExpeL with extracted insights (59.0%) outperformed retrieve-only variant (54.5%), showing that combining retrieval with high-level extracted insights yields largest gains",
                        "uuids": [
                            "e2778.1"
                        ]
                    },
                    {
                        "text": "Retrieval of semantically similar successful trajectories alone yields large gains over non-retrieval baselines, but combining with abstracted insights is most effective",
                        "uuids": [
                            "e2778.1"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "stores",
                        "object": "complete unfiltered episodic history"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "long-horizon reasoning or structured retrieval"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "performs worse than",
                        "object": "agents with structured or selective memory"
                    },
                    {
                        "subject": "full history",
                        "relation": "causes",
                        "object": "token inefficiency and information overload"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Full history memory performed poorly (Treasure Hunt 0.47, Cooking 0.18, Cleaning 0.05) compared to structured graph memory",
                        "uuids": [
                            "e2741.1"
                        ]
                    },
                    {
                        "text": "Full history is highly token-inefficient and burdens LLM with irrelevant or outdated details, reducing effectiveness in long-horizon tasks",
                        "uuids": [
                            "e2741.1"
                        ]
                    },
                    {
                        "text": "Full history scales poorly with time and contains outdated or confusing facts unless additional filtering is applied",
                        "uuids": [
                            "e2741.1"
                        ]
                    }
                ]
            }
        }
    ],
    "new_predictions_likely": [
        "Episodic bonuses combined with object-centric retrieval will show synergistic effects, outperforming either mechanism alone",
        "In environments with cyclic structures, episodic bonuses will show greater benefits compared to tree-structured environments where simple strategies suffice",
        "Agents that process episodic memories into hierarchical summaries (e.g., sub-goal completion markers) will outperform those storing flat episodic sequences",
        "Episodic memory with adaptive capacity management (pruning old/irrelevant memories) will outperform fixed-capacity episodic buffers in long episodes"
    ],
    "new_predictions_unknown": [
        "Whether learned episodic bonus functions (e.g., neural curiosity modules) can outperform hand-crafted binary episodic bonuses across diverse game types",
        "Whether episodic memory remains effective in very long episodes (&gt;1000 steps) where memory capacity and retrieval efficiency become critical bottlenecks",
        "Whether combining multiple episodic memory mechanisms (discovery bonuses + trajectory retrieval + verbal reflections) provides additive benefits or causes interference",
        "Whether episodic memory effectiveness transfers across different game genres (e.g., from exploration-heavy to dialogue-heavy games)"
    ],
    "negative_experiments": [
        "Finding environments where cumulative (across-episode) counting bonuses consistently outperform episodic bonuses would challenge the within-episode novelty hypothesis",
        "Demonstrating that unfiltered full-history episodic memory outperforms selective retrieval in certain task types would question the selectivity principle",
        "Showing that raw episodic trajectories provide equal or better performance than processed reflections would challenge the processing hypothesis",
        "Finding that episodic bonuses provide no benefit when combined with perfect state tracking (e.g., full observability) would question their necessity beyond partial observability mitigation"
    ],
    "unaccounted_for": [
        {
            "text": "How episodic memory mechanisms interact with different base architectures beyond RNNs (e.g., Transformers, graph networks) is not fully characterized",
            "uuids": [
                "e2767.2"
            ]
        },
        {
            "text": "The optimal bonus magnitude, decay schedule, and capacity limits for episodic bonuses across different game complexities are not well characterized",
            "uuids": [
                "e2767.2"
            ]
        },
        {
            "text": "How to automatically determine the appropriate level of processing/abstraction for episodic memories in different task contexts",
            "uuids": [
                "e2709.2",
                "e2778.1"
            ]
        },
        {
            "text": "The computational trade-offs between episodic memory storage/retrieval costs and performance benefits are not systematically analyzed",
            "uuids": [
                "e2727.0",
                "e2741.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some simple deterministic games, agents without episodic mechanisms can achieve high performance through environment-specific strategies (e.g., wall-following), suggesting episodic memory is not universally necessary",
            "uuids": [
                "e2767.2"
            ]
        },
        {
            "text": "Prioritized experience replay (across-episode memory) is widely used and effective in many RL settings, suggesting cumulative memory has important benefits not captured by pure episodic approaches",
            "uuids": [
                "e2760.0",
                "e2696.0"
            ]
        }
    ],
    "special_cases": [
        "Episodic discovery bonuses are typically disabled at test time, so learned policies must internalize the exploration behavior during training",
        "In deterministic environments with perfect state representation and simple topology, episodic bonuses may provide minimal benefit as simple reactive strategies suffice",
        "Very short episodes (few steps) may not benefit from episodic bonuses as there is insufficient time for within-episode learning",
        "In environments where state revisitation is necessary for progress (e.g., backtracking puzzles), episodic bonuses that penalize all revisits may be counterproductive",
        "The effectiveness of object-centric episodic retrieval depends on the quality of object detection/extraction from observations",
        "Verbal reflection-based episodic memory requires sufficient language model capacity to generate meaningful reflections",
        "Full episodic history may be acceptable for very short interactions where token limits are not reached"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Yuan et al. (2018) Counting to Explore and Generalize in Text-based Games [Introduced episodic discovery bonuses for text games, showing benefits over cumulative counting]",
            "Bellemare et al. (2016) Unifying Count-Based Exploration and Intrinsic Motivation [General theory of count-based exploration and intrinsic motivation in RL]",
            "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Showed verbal episodic reflections outperform raw trajectory storage]",
            "Zhao et al. (2023) ExpeL: LLM Agents Are Experiential Learners [Demonstrated importance of processing episodic trajectories into insights]",
            "He et al. (2020) Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension [Showed object-centric episodic retrieval outperforms pure recency]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1932</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1932</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the format in which a problem is presented to a large language model (LLM) modulates the alignment between the model's internal cognitive architecture and the structure of the problem, thereby affecting performance. Formats that more closely mirror the LLM's pretraining data structures, or that reduce ambiguity and cognitive load, enhance performance by facilitating more effective pattern matching and reasoning.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Alignment Performance Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_aligned_with &#8594; LLM_pretraining_data_structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_increased &#8594; on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on tasks when prompts are formatted similarly to their pretraining data (e.g., Q&A, dialogue, code). </li>
    <li>Ambiguous or unfamiliar formats reduce LLM accuracy and increase error rates. </li>
    <li>Instruction tuning with formats similar to downstream tasks improves LLM generalization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This law is closely related to existing work on prompt engineering and instruction tuning, but formalizes the alignment principle as a general law.</p>            <p><strong>What Already Exists:</strong> Instruction tuning and prompt engineering are known to improve LLM performance by leveraging familiar formats.</p>            <p><strong>What is Novel:</strong> The law formalizes the relationship as a conditional dependency on alignment between problem format and pretraining data structure.</p>
            <p><strong>References:</strong> <ul>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning, format alignment]</li>
    <li>Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [Prompt format effects]</li>
</ul>
            <h3>Statement 1: Cognitive Load Reduction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; minimizes &#8594; ambiguity_and_cognitive_load</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_increased &#8594; on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Clear, unambiguous prompts lead to higher LLM accuracy and fewer hallucinations. </li>
    <li>Complex or overloaded formats increase LLM error rates. </li>
    <li>Stepwise and explicit instructions reduce error propagation in multi-step tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law is somewhat related to existing work but extends the principle to a broader cognitive framework.</p>            <p><strong>What Already Exists:</strong> Prompt clarity and explicitness are known to improve LLM outputs.</p>            <p><strong>What is Novel:</strong> The law generalizes the effect to a principle of cognitive load minimization across all problem formats.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise reduction of load]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt ambiguity effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Presenting a problem in a format that mimics Wikipedia or StackExchange will improve LLM performance on knowledge-based tasks.</li>
                <li>Rewriting ambiguous prompts to be more explicit will reduce LLM hallucination rates.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a novel but internally consistent format is used, will LLMs adapt and perform well after minimal exposure?</li>
                <li>Can LLMs be trained to generalize across highly diverse formats if given meta-formatting instructions?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on misaligned or ambiguous formats, the theory is challenged.</li>
                <li>If increasing ambiguity or cognitive load does not reduce LLM performance, the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs perform well on unfamiliar formats due to emergent generalization abilities. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and formalizes existing findings into general laws, extending their scope.</p>
            <p><strong>References:</strong> <ul>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning, format alignment]</li>
    <li>Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [Prompt format effects]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise reduction of load]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that the format in which a problem is presented to a large language model (LLM) modulates the alignment between the model's internal cognitive architecture and the structure of the problem, thereby affecting performance. Formats that more closely mirror the LLM's pretraining data structures, or that reduce ambiguity and cognitive load, enhance performance by facilitating more effective pattern matching and reasoning.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Alignment Performance Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_aligned_with",
                        "object": "LLM_pretraining_data_structure"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_increased",
                        "object": "on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on tasks when prompts are formatted similarly to their pretraining data (e.g., Q&A, dialogue, code).",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or unfamiliar formats reduce LLM accuracy and increase error rates.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction tuning with formats similar to downstream tasks improves LLM generalization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Instruction tuning and prompt engineering are known to improve LLM performance by leveraging familiar formats.",
                    "what_is_novel": "The law formalizes the relationship as a conditional dependency on alignment between problem format and pretraining data structure.",
                    "classification_explanation": "This law is closely related to existing work on prompt engineering and instruction tuning, but formalizes the alignment principle as a general law.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning, format alignment]",
                        "Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [Prompt format effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Cognitive Load Reduction Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "minimizes",
                        "object": "ambiguity_and_cognitive_load"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_increased",
                        "object": "on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Clear, unambiguous prompts lead to higher LLM accuracy and fewer hallucinations.",
                        "uuids": []
                    },
                    {
                        "text": "Complex or overloaded formats increase LLM error rates.",
                        "uuids": []
                    },
                    {
                        "text": "Stepwise and explicit instructions reduce error propagation in multi-step tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt clarity and explicitness are known to improve LLM outputs.",
                    "what_is_novel": "The law generalizes the effect to a principle of cognitive load minimization across all problem formats.",
                    "classification_explanation": "This law is somewhat related to existing work but extends the principle to a broader cognitive framework.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise reduction of load]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt ambiguity effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Presenting a problem in a format that mimics Wikipedia or StackExchange will improve LLM performance on knowledge-based tasks.",
        "Rewriting ambiguous prompts to be more explicit will reduce LLM hallucination rates."
    ],
    "new_predictions_unknown": [
        "If a novel but internally consistent format is used, will LLMs adapt and perform well after minimal exposure?",
        "Can LLMs be trained to generalize across highly diverse formats if given meta-formatting instructions?"
    ],
    "negative_experiments": [
        "If LLMs perform equally well on misaligned or ambiguous formats, the theory is challenged.",
        "If increasing ambiguity or cognitive load does not reduce LLM performance, the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs perform well on unfamiliar formats due to emergent generalization abilities.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robust performance on tasks with high ambiguity, suggesting limits to the effect.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Highly capable LLMs with advanced meta-reasoning may be less sensitive to format alignment.",
        "Tasks with extremely simple structure may not benefit from format optimization."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and instruction tuning are established techniques for improving LLM performance.",
        "what_is_novel": "The explicit generalization to a cognitive alignment principle and cognitive load minimization as universal laws is new.",
        "classification_explanation": "The theory synthesizes and formalizes existing findings into general laws, extending their scope.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning, format alignment]",
            "Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [Prompt format effects]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise reduction of load]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-655",
    "original_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Selective Forecasting and Uncertainty Hedging Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-651</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-651</p>
                <p><strong>Name:</strong> Selective Forecasting and Uncertainty Hedging Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries, based on the following results.</p>
                <p><strong>Description:</strong> LLMs can match or outperform human crowd aggregates in forecasting the probability of future scientific discoveries when they selectively issue forecasts only in cases where the human crowd is uncertain or where the LLM's own confidence is high. This selective forecasting approach leverages the complementary strengths of LLMs and humans, and mitigates the impact of LLM underconfidence or overconfidence in highly certain cases. The theory is supported by evidence from retrieval-augmented LLM systems, which show that restricting forecasts to uncertain cases or high-confidence outputs improves relative performance, and is further informed by calibration and abstention analyses across multiple LLM forecasting studies.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Selective Forecasting Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; issues_forecasts_only_when &#8594; human crowd aggregate is uncertain (e.g., probability in [0.3, 0.7])</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; matches_or_outperforms &#8594; human crowd aggregate in forecasting accuracy (e.g., Brier score) on those questions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Retrieval-Augmented LM Forecaster matches or slightly beats the crowd when forecasting only on questions where the crowd is uncertain (system Brier 0.238 vs crowd 0.240; under all three selective criteria jointly, system 0.240 vs crowd 0.247). <a href="../results/extraction-result-5823.html#e5823.0" class="evidence-link">[e5823.0]</a> </li>
    <li>Selective forecasting analyses in the same study show that LLMs can approach or exceed crowd performance when abstaining from highly certain cases. <a href="../results/extraction-result-5823.html#e5823.0" class="evidence-link">[e5823.0]</a> </li>
    <li>System-level Brier scores improve in selective settings, especially when the crowd is uncertain. <a href="../results/extraction-result-5823.html#e5823.0" class="evidence-link">[e5823.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Selective prediction is known in ML, but its application and empirical validation in LLM-based scientific forecasting is new.</p>            <p><strong>What Already Exists:</strong> Selective prediction and abstention are established in machine learning, but have not been widely applied to LLM-based scientific forecasting.</p>            <p><strong>What is Novel:</strong> This law formalizes selective forecasting as a method for LLMs to match or beat human crowd accuracy in scientific discovery forecasting, based on empirical evidence from LLM forecasting systems.</p>
            <p><strong>References:</strong> <ul>
    <li>Geifman & El-Yaniv (2017) Selective Classification for Deep Neural Networks [selective prediction in ML]</li>
    <li>Wang et al. (2024) Approaching Human-Level Forecasting with Language Models [selective forecasting in LLMs]</li>
</ul>
            <h3>Statement 1: Uncertainty Hedging Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; issues_forecasts_only_when &#8594; its own confidence (e.g., high perplexity difference, narrow interval, or high calibration) exceeds a threshold</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves &#8594; higher accuracy and calibration on those forecasts compared to unconditional forecasting</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>System-level Brier scores improve when LLMs forecast only when confident, as shown in selective forecasting analyses (e.g., system Brier improves under selective criteria). <a href="../results/extraction-result-5823.html#e5823.0" class="evidence-link">[e5823.0]</a> </li>
    <li>BrainGPT and general-purpose LLMs on BrainBench show that higher model confidence (perplexity difference) is positively correlated with correctness, indicating that confidence can be used to hedge forecasts. <a href="../results/extraction-result-5710.html#e5710.2" class="evidence-link">[e5710.2]</a> <a href="../results/extraction-result-5710.html#e5710.0" class="evidence-link">[e5710.0]</a> </li>
    <li>DeBERTa-v3 (IntervalQA) models show that calibration improves with model size and that confidence intervals can be used to select more reliable predictions. <a href="../results/extraction-result-5792.html#e5792.4" class="evidence-link">[e5792.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general principle is known, but its application and empirical support in LLM-based scientific forecasting is new.</p>            <p><strong>What Already Exists:</strong> Abstention and confidence-based prediction are established in ML.</p>            <p><strong>What is Novel:</strong> This law applies confidence-based selective forecasting to LLMs for scientific discovery prediction, supported by empirical calibration and accuracy analyses.</p>
            <p><strong>References:</strong> <ul>
    <li>Geifman & El-Yaniv (2017) Selective Classification for Deep Neural Networks [confidence-based abstention]</li>
    <li>Wang et al. (2024) Approaching Human-Level Forecasting with Language Models [selective forecasting in LLMs]</li>
    <li>Zhou et al. (2022) Calibration of Neural Networks: A Survey [calibration and confidence in neural models]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If LLMs are set to forecast only when the human crowd is uncertain, their accuracy will approach or exceed that of the crowd on those questions.</li>
                <li>If LLMs abstain from forecasting when their own confidence is low, their average accuracy and calibration on issued forecasts will improve.</li>
                <li>Selective LLM forecasting will yield higher Brier score performance than unconditional LLM forecasting on the same set of questions.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>A hybrid system that dynamically combines LLM and human forecasts based on mutual uncertainty could consistently outperform both in all domains, including rare or high-impact scientific discoveries.</li>
                <li>Selective forecasting could enable LLMs to identify and forecast rare, high-impact scientific discoveries that humans are uncertain about, potentially surfacing discoveries missed by the crowd.</li>
                <li>If LLMs are trained to optimize for selective forecasting, they may develop new internal representations of uncertainty that generalize across scientific domains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If selective forecasting does not improve LLM accuracy or calibration compared to unconditional forecasting, the theory would be called into question.</li>
                <li>If LLMs cannot reliably estimate their own confidence (e.g., if confidence is not correlated with correctness), uncertainty hedging would not yield benefits.</li>
                <li>If selective forecasting leads to systematic omission of important discoveries (coverage bias), the theory's utility would be limited.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Selective forecasting may reduce coverage, leaving some important discoveries unforecasted, especially if LLMs or humans are both uncertain in rare but critical cases. </li>
    <li>Some LLMs (e.g., PaLM2, T5, UnifiedQA-v2) perform near chance or poorly even with selective or confidence-based approaches, indicating that model architecture and training may limit the effectiveness of selective forecasting. <a href="../results/extraction-result-5706.html#e5706.0" class="evidence-link">[e5706.0]</a> <a href="../results/extraction-result-5706.html#e5706.2" class="evidence-link">[e5706.2]</a> <a href="../results/extraction-result-5706.html#e5706.3" class="evidence-link">[e5706.3]</a> <a href="../results/extraction-result-5706.html#e5706.4" class="evidence-link">[e5706.4]</a> <a href="../results/extraction-result-5792.html#e5792.1" class="evidence-link">[e5792.1]</a> <a href="../results/extraction-result-5792.html#e5792.0" class="evidence-link">[e5792.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> Selective prediction is known, but its application and empirical validation in LLM-based scientific forecasting is new, as demonstrated by recent LLM forecasting studies.</p>
            <p><strong>References:</strong> <ul>
    <li>Geifman & El-Yaniv (2017) Selective Classification for Deep Neural Networks [selective prediction]</li>
    <li>Wang et al. (2024) Approaching Human-Level Forecasting with Language Models [selective forecasting in LLMs]</li>
    <li>Zhou et al. (2022) Calibration of Neural Networks: A Survey [calibration and confidence in neural models]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Selective Forecasting and Uncertainty Hedging Theory",
    "theory_description": "LLMs can match or outperform human crowd aggregates in forecasting the probability of future scientific discoveries when they selectively issue forecasts only in cases where the human crowd is uncertain or where the LLM's own confidence is high. This selective forecasting approach leverages the complementary strengths of LLMs and humans, and mitigates the impact of LLM underconfidence or overconfidence in highly certain cases. The theory is supported by evidence from retrieval-augmented LLM systems, which show that restricting forecasts to uncertain cases or high-confidence outputs improves relative performance, and is further informed by calibration and abstention analyses across multiple LLM forecasting studies.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Selective Forecasting Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "issues_forecasts_only_when",
                        "object": "human crowd aggregate is uncertain (e.g., probability in [0.3, 0.7])"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "matches_or_outperforms",
                        "object": "human crowd aggregate in forecasting accuracy (e.g., Brier score) on those questions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Retrieval-Augmented LM Forecaster matches or slightly beats the crowd when forecasting only on questions where the crowd is uncertain (system Brier 0.238 vs crowd 0.240; under all three selective criteria jointly, system 0.240 vs crowd 0.247).",
                        "uuids": [
                            "e5823.0"
                        ]
                    },
                    {
                        "text": "Selective forecasting analyses in the same study show that LLMs can approach or exceed crowd performance when abstaining from highly certain cases.",
                        "uuids": [
                            "e5823.0"
                        ]
                    },
                    {
                        "text": "System-level Brier scores improve in selective settings, especially when the crowd is uncertain.",
                        "uuids": [
                            "e5823.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Selective prediction and abstention are established in machine learning, but have not been widely applied to LLM-based scientific forecasting.",
                    "what_is_novel": "This law formalizes selective forecasting as a method for LLMs to match or beat human crowd accuracy in scientific discovery forecasting, based on empirical evidence from LLM forecasting systems.",
                    "classification_explanation": "Selective prediction is known in ML, but its application and empirical validation in LLM-based scientific forecasting is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Geifman & El-Yaniv (2017) Selective Classification for Deep Neural Networks [selective prediction in ML]",
                        "Wang et al. (2024) Approaching Human-Level Forecasting with Language Models [selective forecasting in LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Uncertainty Hedging Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "issues_forecasts_only_when",
                        "object": "its own confidence (e.g., high perplexity difference, narrow interval, or high calibration) exceeds a threshold"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves",
                        "object": "higher accuracy and calibration on those forecasts compared to unconditional forecasting"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "System-level Brier scores improve when LLMs forecast only when confident, as shown in selective forecasting analyses (e.g., system Brier improves under selective criteria).",
                        "uuids": [
                            "e5823.0"
                        ]
                    },
                    {
                        "text": "BrainGPT and general-purpose LLMs on BrainBench show that higher model confidence (perplexity difference) is positively correlated with correctness, indicating that confidence can be used to hedge forecasts.",
                        "uuids": [
                            "e5710.2",
                            "e5710.0"
                        ]
                    },
                    {
                        "text": "DeBERTa-v3 (IntervalQA) models show that calibration improves with model size and that confidence intervals can be used to select more reliable predictions.",
                        "uuids": [
                            "e5792.4"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Abstention and confidence-based prediction are established in ML.",
                    "what_is_novel": "This law applies confidence-based selective forecasting to LLMs for scientific discovery prediction, supported by empirical calibration and accuracy analyses.",
                    "classification_explanation": "The general principle is known, but its application and empirical support in LLM-based scientific forecasting is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Geifman & El-Yaniv (2017) Selective Classification for Deep Neural Networks [confidence-based abstention]",
                        "Wang et al. (2024) Approaching Human-Level Forecasting with Language Models [selective forecasting in LLMs]",
                        "Zhou et al. (2022) Calibration of Neural Networks: A Survey [calibration and confidence in neural models]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If LLMs are set to forecast only when the human crowd is uncertain, their accuracy will approach or exceed that of the crowd on those questions.",
        "If LLMs abstain from forecasting when their own confidence is low, their average accuracy and calibration on issued forecasts will improve.",
        "Selective LLM forecasting will yield higher Brier score performance than unconditional LLM forecasting on the same set of questions."
    ],
    "new_predictions_unknown": [
        "A hybrid system that dynamically combines LLM and human forecasts based on mutual uncertainty could consistently outperform both in all domains, including rare or high-impact scientific discoveries.",
        "Selective forecasting could enable LLMs to identify and forecast rare, high-impact scientific discoveries that humans are uncertain about, potentially surfacing discoveries missed by the crowd.",
        "If LLMs are trained to optimize for selective forecasting, they may develop new internal representations of uncertainty that generalize across scientific domains."
    ],
    "negative_experiments": [
        "If selective forecasting does not improve LLM accuracy or calibration compared to unconditional forecasting, the theory would be called into question.",
        "If LLMs cannot reliably estimate their own confidence (e.g., if confidence is not correlated with correctness), uncertainty hedging would not yield benefits.",
        "If selective forecasting leads to systematic omission of important discoveries (coverage bias), the theory's utility would be limited."
    ],
    "unaccounted_for": [
        {
            "text": "Selective forecasting may reduce coverage, leaving some important discoveries unforecasted, especially if LLMs or humans are both uncertain in rare but critical cases.",
            "uuids": []
        },
        {
            "text": "Some LLMs (e.g., PaLM2, T5, UnifiedQA-v2) perform near chance or poorly even with selective or confidence-based approaches, indicating that model architecture and training may limit the effectiveness of selective forecasting.",
            "uuids": [
                "e5706.0",
                "e5706.2",
                "e5706.3",
                "e5706.4",
                "e5792.1",
                "e5792.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, LLMs may be overconfident in incorrect forecasts, leading to selective errors (e.g., LLM ensemble and GPT-4 show overconfidence/acquiescence bias and poor calibration, with aggregate CI = 0.041 and overprediction).",
            "uuids": [
                "e5790.0",
                "e5790.1"
            ]
        },
        {
            "text": "PaLM2 rationale and persona ensemble strategies increased mean predicted probabilities and worsened calibration, indicating that selective or confidence-based approaches can interact with model biases in nontrivial ways.",
            "uuids": [
                "e5706.1",
                "e5706.3"
            ]
        }
    ],
    "special_cases": [
        "Selective forecasting may not be feasible in settings where full coverage is required (e.g., regulatory or safety-critical scientific domains).",
        "Thresholds for uncertainty may need to be dynamically adjusted for different domains, event types, or model architectures.",
        "Selective forecasting may be less effective if the LLM's confidence estimates are poorly calibrated or if the underlying model is not sufficiently accurate."
    ],
    "existing_theory": {
        "what_already_exists": "Selective prediction, abstention, and confidence-based prediction are established in machine learning, but have not been widely applied or empirically validated in LLM-based scientific forecasting.",
        "what_is_novel": "This theory applies and empirically validates selective forecasting and uncertainty hedging in LLM-based scientific discovery prediction, showing that these methods can close the gap with or surpass human crowd accuracy in specific settings.",
        "classification_explanation": "Selective prediction is known, but its application and empirical validation in LLM-based scientific forecasting is new, as demonstrated by recent LLM forecasting studies.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Geifman & El-Yaniv (2017) Selective Classification for Deep Neural Networks [selective prediction]",
            "Wang et al. (2024) Approaching Human-Level Forecasting with Language Models [selective forecasting in LLMs]",
            "Zhou et al. (2022) Calibration of Neural Networks: A Survey [calibration and confidence in neural models]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
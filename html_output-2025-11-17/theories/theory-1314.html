<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Order-Invariance Robustness Law for Graph Linearization in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1314</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1314</p>
                <p><strong>Name:</strong> Order-Invariance Robustness Law for Graph Linearization in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal representation for converting graphs into text for LLM training should be robust to permutations of node and edge orderings, such that the semantic content and learnability of the graph structure are preserved regardless of the specific linearization order. This order-invariance ensures that LLMs do not overfit to arbitrary serialization choices and instead learn the underlying graph structure.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Order-Invariance Robustness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; is_order_invariant &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_robust_to_permutation &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; learns_graph_structure &#8594; effectively</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that LLMs trained on multiple random orderings of the same graph generalize better and are less sensitive to serialization artifacts. </li>
    <li>Order-invariance is a core property in graph neural networks, leading to better generalization and robustness. </li>
    <li>Graph isomorphism invariance is a desirable property in graph representation learning. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While order-invariance is known in GNNs, its formalization as a law for LLM graph linearization is new.</p>            <p><strong>What Already Exists:</strong> Order-invariance is a well-established principle in GNNs and some graph-to-sequence models.</p>            <p><strong>What is Novel:</strong> Its explicit application as a robustness law for LLM-based graph linearization is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Order-invariance in graph-to-sequence]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs trained on order-invariant graph linearizations will generalize better to unseen graphs than those trained on fixed-order linearizations.</li>
                <li>Introducing random permutations during training will reduce overfitting to serialization artifacts.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Order-invariant linearizations may enable LLMs to transfer graph reasoning skills across domains with different canonical orderings.</li>
                <li>Order-invariance could improve LLM robustness to adversarial serialization attacks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs trained on order-invariant encodings do not outperform those trained on fixed-order encodings, the law would be challenged.</li>
                <li>If order-invariant encodings lead to loss of important graph information, the law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The effect of order-invariance on tasks requiring explicit sequential information is not fully explained. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known order-invariance principles but applies them to LLM graph linearization in a novel, formalized way.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Order-invariance in graph-to-sequence]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "theory_description": "This theory posits that the ideal representation for converting graphs into text for LLM training should be robust to permutations of node and edge orderings, such that the semantic content and learnability of the graph structure are preserved regardless of the specific linearization order. This order-invariance ensures that LLMs do not overfit to arbitrary serialization choices and instead learn the underlying graph structure.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Order-Invariance Robustness Law",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "is_order_invariant",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "is_robust_to_permutation",
                        "object": "True"
                    },
                    {
                        "subject": "LLM",
                        "relation": "learns_graph_structure",
                        "object": "effectively"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that LLMs trained on multiple random orderings of the same graph generalize better and are less sensitive to serialization artifacts.",
                        "uuids": []
                    },
                    {
                        "text": "Order-invariance is a core property in graph neural networks, leading to better generalization and robustness.",
                        "uuids": []
                    },
                    {
                        "text": "Graph isomorphism invariance is a desirable property in graph representation learning.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Order-invariance is a well-established principle in GNNs and some graph-to-sequence models.",
                    "what_is_novel": "Its explicit application as a robustness law for LLM-based graph linearization is novel.",
                    "classification_explanation": "While order-invariance is known in GNNs, its formalization as a law for LLM graph linearization is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]",
                        "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Order-invariance in graph-to-sequence]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs trained on order-invariant graph linearizations will generalize better to unseen graphs than those trained on fixed-order linearizations.",
        "Introducing random permutations during training will reduce overfitting to serialization artifacts."
    ],
    "new_predictions_unknown": [
        "Order-invariant linearizations may enable LLMs to transfer graph reasoning skills across domains with different canonical orderings.",
        "Order-invariance could improve LLM robustness to adversarial serialization attacks."
    ],
    "negative_experiments": [
        "If LLMs trained on order-invariant encodings do not outperform those trained on fixed-order encodings, the law would be challenged.",
        "If order-invariant encodings lead to loss of important graph information, the law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The effect of order-invariance on tasks requiring explicit sequential information is not fully explained.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some graph tasks may benefit from canonical orderings (e.g., for graphs with inherent directionality or hierarchy).",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with natural sequential structure (e.g., paths, trees) may require partial order preservation.",
        "Very large graphs may require approximate order-invariance due to computational constraints."
    ],
    "existing_theory": {
        "what_already_exists": "Order-invariance is a known property in GNNs and some graph-to-sequence models.",
        "what_is_novel": "Its explicit formalization as a robustness law for LLM-based graph linearization is new.",
        "classification_explanation": "The theory synthesizes known order-invariance principles but applies them to LLM graph linearization in a novel, formalized way.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]",
            "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Order-invariance in graph-to-sequence]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-615",
    "original_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
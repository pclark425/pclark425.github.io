<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Decomposition and Iterative Composition Law - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1150</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1150</p>
                <p><strong>Name:</strong> Prompt Decomposition and Iterative Composition Law</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models achieve strict logical reasoning most effectively when complex reasoning tasks are decomposed into atomic subproblems, each solved independently, and their solutions are iteratively composed using explicit logical operators. The process must be guided by a prompt structure that enforces both decomposition and explicit composition, ensuring that intermediate reasoning steps are both interpretable and verifiable.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Decomposition-Composition Synergy Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning_task &#8594; is_complex &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; enforces_decomposition &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; enforces_iterative_composition &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; performs_strict_logical_reasoning &#8594; with_high_accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Chain-of-thought and least-to-most prompting improve logical reasoning by breaking down tasks. </li>
    <li>Explicitly structured prompts that require stepwise solutions yield more reliable logical outputs. </li>
    <li>Iterative composition of subproblem solutions with explicit operators reduces logical errors. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While decomposition and composition are individually studied, their joint necessity and explicit iterative structure for strict logical reasoning is a new formalization.</p>            <p><strong>What Already Exists:</strong> Prompting strategies like chain-of-thought and least-to-most prompting are known to improve reasoning by decomposition.</p>            <p><strong>What is Novel:</strong> The formalization that both decomposition and explicit iterative composition are jointly necessary for strict logical reasoning is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Decomposition via stepwise reasoning]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Decomposition and composition, but not formalized as a law]</li>
</ul>
            <h3>Statement 1: Intermediate Verifiability Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; subproblem_solutions &#8594; are_explicitly_represented &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; composition_process &#8594; is_iterative_and_explicit &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; reasoning_chain &#8594; is_verifiable_by_external_agent &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; logical_errors &#8594; are_detectable_and_correctable &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Explicit intermediate steps allow for error detection and correction by humans or automated checkers. </li>
    <li>Opaque or implicit reasoning chains make error detection difficult. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While interpretability is valued, the explicit link to iterative composition and verifiability is a new formalization.</p>            <p><strong>What Already Exists:</strong> Intermediate step visibility is known to aid interpretability and error correction.</p>            <p><strong>What is Novel:</strong> The law that explicit, iterative composition enables external verifiability and correction is novel in the context of LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate steps for verifiability]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Stepwise reasoning, but not formalized as a law]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt enforces both decomposition and explicit iterative composition, LM logical accuracy will surpass that of prompts enforcing only one or neither.</li>
                <li>If intermediate subproblem solutions are made explicit, logical errors will be more easily detected and corrected.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LMs are trained end-to-end with explicit decomposition and composition, they may develop emergent logical reasoning capabilities beyond current benchmarks.</li>
                <li>If iterative composition is performed with learned, rather than hard-coded, operators, LMs may develop novel forms of logical inference.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs perform strict logical reasoning without explicit decomposition or composition, the theory would be challenged.</li>
                <li>If explicit intermediate steps do not improve verifiability or error correction, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs may perform well on certain logical tasks without explicit decomposition or composition, possibly due to internalized patterns. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This is a new formalization of a necessary condition for strict logical reasoning in LMs, building on but extending prior work.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Decomposition via stepwise reasoning]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Decomposition and composition, but not formalized as a law]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate steps for verifiability]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt Decomposition and Iterative Composition Law",
    "theory_description": "This theory posits that language models achieve strict logical reasoning most effectively when complex reasoning tasks are decomposed into atomic subproblems, each solved independently, and their solutions are iteratively composed using explicit logical operators. The process must be guided by a prompt structure that enforces both decomposition and explicit composition, ensuring that intermediate reasoning steps are both interpretable and verifiable.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Decomposition-Composition Synergy Law",
                "if": [
                    {
                        "subject": "reasoning_task",
                        "relation": "is_complex",
                        "object": "True"
                    },
                    {
                        "subject": "prompt",
                        "relation": "enforces_decomposition",
                        "object": "True"
                    },
                    {
                        "subject": "prompt",
                        "relation": "enforces_iterative_composition",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "performs_strict_logical_reasoning",
                        "object": "with_high_accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Chain-of-thought and least-to-most prompting improve logical reasoning by breaking down tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Explicitly structured prompts that require stepwise solutions yield more reliable logical outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Iterative composition of subproblem solutions with explicit operators reduces logical errors.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompting strategies like chain-of-thought and least-to-most prompting are known to improve reasoning by decomposition.",
                    "what_is_novel": "The formalization that both decomposition and explicit iterative composition are jointly necessary for strict logical reasoning is novel.",
                    "classification_explanation": "While decomposition and composition are individually studied, their joint necessity and explicit iterative structure for strict logical reasoning is a new formalization.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Decomposition via stepwise reasoning]",
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Decomposition and composition, but not formalized as a law]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Intermediate Verifiability Law",
                "if": [
                    {
                        "subject": "subproblem_solutions",
                        "relation": "are_explicitly_represented",
                        "object": "True"
                    },
                    {
                        "subject": "composition_process",
                        "relation": "is_iterative_and_explicit",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "reasoning_chain",
                        "relation": "is_verifiable_by_external_agent",
                        "object": "True"
                    },
                    {
                        "subject": "logical_errors",
                        "relation": "are_detectable_and_correctable",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Explicit intermediate steps allow for error detection and correction by humans or automated checkers.",
                        "uuids": []
                    },
                    {
                        "text": "Opaque or implicit reasoning chains make error detection difficult.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Intermediate step visibility is known to aid interpretability and error correction.",
                    "what_is_novel": "The law that explicit, iterative composition enables external verifiability and correction is novel in the context of LMs.",
                    "classification_explanation": "While interpretability is valued, the explicit link to iterative composition and verifiability is a new formalization.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate steps for verifiability]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Stepwise reasoning, but not formalized as a law]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt enforces both decomposition and explicit iterative composition, LM logical accuracy will surpass that of prompts enforcing only one or neither.",
        "If intermediate subproblem solutions are made explicit, logical errors will be more easily detected and corrected."
    ],
    "new_predictions_unknown": [
        "If LMs are trained end-to-end with explicit decomposition and composition, they may develop emergent logical reasoning capabilities beyond current benchmarks.",
        "If iterative composition is performed with learned, rather than hard-coded, operators, LMs may develop novel forms of logical inference."
    ],
    "negative_experiments": [
        "If LMs perform strict logical reasoning without explicit decomposition or composition, the theory would be challenged.",
        "If explicit intermediate steps do not improve verifiability or error correction, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs may perform well on certain logical tasks without explicit decomposition or composition, possibly due to internalized patterns.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "There are cases where LMs solve complex logical tasks with end-to-end black-box inference, suggesting decomposition is not always necessary.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For atomic or trivially simple tasks, decomposition and iterative composition may be unnecessary.",
        "For LMs with strong internal logical representations, explicit decomposition may be less critical."
    ],
    "existing_theory": {
        "what_already_exists": "Prompting strategies for decomposition and stepwise reasoning are known, but not formalized as a joint law for strict logical reasoning.",
        "what_is_novel": "The explicit law that both decomposition and iterative composition are jointly necessary for strict logical reasoning is novel.",
        "classification_explanation": "This is a new formalization of a necessary condition for strict logical reasoning in LMs, building on but extending prior work.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Decomposition via stepwise reasoning]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Decomposition and composition, but not formalized as a law]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Intermediate steps for verifiability]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-604",
    "original_theory_name": "Prompt Decomposition and Iterative Composition Law",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt Decomposition and Iterative Composition Law",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
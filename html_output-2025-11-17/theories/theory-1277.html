<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory for Graph-to-Text Representations - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1277</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1277</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory for Graph-to-Text Representations</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal graph-to-text representation for language model training is one that aligns with human cognitive strategies for understanding and describing graphs. Representations that mirror human preferences for ordering, grouping, and abstraction will facilitate more natural, fluent, and accurate text generation, and will improve LM interpretability and controllability.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Human-Order Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; orders &#8594; nodes_and_edges_in_human-preferred_sequences</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; generates &#8594; more_fluent_and_natural_text</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human annotators prefer certain node/edge orderings in AMR and code-to-text tasks. </li>
    <li>Text generated from human-aligned orderings is rated as more natural and easier to understand. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law generalizes cognitive alignment from a dataset design choice to a universal principle.</p>            <p><strong>What Already Exists:</strong> Human-preferred orderings are used in AMR-to-text and code summarization datasets.</p>            <p><strong>What is Novel:</strong> The law claims that cognitive alignment is necessary for ideal graph-to-text LM representations.</p>
            <p><strong>References:</strong> <ul>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [AMR, human annotation order]</li>
    <li>Iyer et al. (2016) Summarizing source code using a neural attention model [code-to-text, human orderings]</li>
</ul>
            <h3>Statement 1: Cognitive Abstraction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; groups &#8594; subgraphs_into_human-meaningful_units</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; produces &#8594; more_interpretable_and_controllable_text</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Hierarchical and modular representations improve interpretability in NLG and code generation. </li>
    <li>Human summarization strategies involve grouping related nodes/edges into higher-level concepts. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law elevates cognitive abstraction from a useful technique to a universal requirement.</p>            <p><strong>What Already Exists:</strong> Hierarchical and modular representations are used in summarization and code generation.</p>            <p><strong>What is Novel:</strong> The law claims that cognitive abstraction is necessary for ideal graph-to-text LM representations.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2019) Hierarchical Transformers for Long Document Classification [hierarchical modeling]</li>
    <li>Radev et al. (2004) Centroid-based summarization of multiple documents [abstraction in summarization]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs trained on cognitively aligned representations will produce text that is rated as more natural and interpretable by humans.</li>
                <li>Ordering and grouping strategies that deviate from human preferences will result in less fluent and less controllable text.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Cognitive alignment may enable LMs to explain their outputs in human-understandable terms.</li>
                <li>Cognitively aligned representations may improve LM robustness to adversarial or out-of-distribution graphs.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If non-cognitively aligned representations yield equal or better LM performance, the theory is challenged.</li>
                <li>If human raters cannot distinguish between outputs from cognitively aligned and non-aligned representations, the theory's claims are weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to determine human-preferred orderings or groupings for arbitrary graphs. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes cognitive alignment from a best practice to a universal requirement.</p>
            <p><strong>References:</strong> <ul>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [AMR, human annotation order]</li>
    <li>Iyer et al. (2016) Summarizing source code using a neural attention model [code-to-text, human orderings]</li>
    <li>Liu et al. (2019) Hierarchical Transformers for Long Document Classification [hierarchical modeling]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory for Graph-to-Text Representations",
    "theory_description": "This theory posits that the ideal graph-to-text representation for language model training is one that aligns with human cognitive strategies for understanding and describing graphs. Representations that mirror human preferences for ordering, grouping, and abstraction will facilitate more natural, fluent, and accurate text generation, and will improve LM interpretability and controllability.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Human-Order Alignment Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "orders",
                        "object": "nodes_and_edges_in_human-preferred_sequences"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "generates",
                        "object": "more_fluent_and_natural_text"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human annotators prefer certain node/edge orderings in AMR and code-to-text tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Text generated from human-aligned orderings is rated as more natural and easier to understand.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Human-preferred orderings are used in AMR-to-text and code summarization datasets.",
                    "what_is_novel": "The law claims that cognitive alignment is necessary for ideal graph-to-text LM representations.",
                    "classification_explanation": "The law generalizes cognitive alignment from a dataset design choice to a universal principle.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [AMR, human annotation order]",
                        "Iyer et al. (2016) Summarizing source code using a neural attention model [code-to-text, human orderings]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Cognitive Abstraction Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "groups",
                        "object": "subgraphs_into_human-meaningful_units"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "produces",
                        "object": "more_interpretable_and_controllable_text"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Hierarchical and modular representations improve interpretability in NLG and code generation.",
                        "uuids": []
                    },
                    {
                        "text": "Human summarization strategies involve grouping related nodes/edges into higher-level concepts.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical and modular representations are used in summarization and code generation.",
                    "what_is_novel": "The law claims that cognitive abstraction is necessary for ideal graph-to-text LM representations.",
                    "classification_explanation": "The law elevates cognitive abstraction from a useful technique to a universal requirement.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Liu et al. (2019) Hierarchical Transformers for Long Document Classification [hierarchical modeling]",
                        "Radev et al. (2004) Centroid-based summarization of multiple documents [abstraction in summarization]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs trained on cognitively aligned representations will produce text that is rated as more natural and interpretable by humans.",
        "Ordering and grouping strategies that deviate from human preferences will result in less fluent and less controllable text."
    ],
    "new_predictions_unknown": [
        "Cognitive alignment may enable LMs to explain their outputs in human-understandable terms.",
        "Cognitively aligned representations may improve LM robustness to adversarial or out-of-distribution graphs."
    ],
    "negative_experiments": [
        "If non-cognitively aligned representations yield equal or better LM performance, the theory is challenged.",
        "If human raters cannot distinguish between outputs from cognitively aligned and non-aligned representations, the theory's claims are weakened."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to determine human-preferred orderings or groupings for arbitrary graphs.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LMs can learn to generate fluent text from arbitrary orderings given sufficient data.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For highly technical or formal graphs, human preferences may be ill-defined or inconsistent.",
        "For graphs with no clear human-meaningful groupings, cognitive alignment may be infeasible."
    ],
    "existing_theory": {
        "what_already_exists": "Cognitive alignment is used in dataset design for AMR and code summarization.",
        "what_is_novel": "The theory generalizes cognitive alignment as a universal principle for graph-to-text LM training.",
        "classification_explanation": "The theory synthesizes cognitive alignment from a best practice to a universal requirement.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [AMR, human annotation order]",
            "Iyer et al. (2016) Summarizing source code using a neural attention model [code-to-text, human orderings]",
            "Liu et al. (2019) Hierarchical Transformers for Long Document Classification [hierarchical modeling]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-613",
    "original_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
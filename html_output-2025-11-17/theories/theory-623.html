<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Task- and Model-Dependence of Self-Reflection Efficacy in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-623</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-623</p>
                <p><strong>Name:</strong> Task- and Model-Dependence of Self-Reflection Efficacy in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that the effectiveness of self-reflection and iterative refinement in LLMs is highly dependent on both the nature of the task (e.g., math, code, open-ended generation, safety) and the underlying model's scale and capability. The theory claims that strong, large models are more likely to benefit from self-reflection, while smaller or less capable models may fail to generate useful feedback or corrections, and that certain tasks (e.g., those with clear verification signals) are more amenable to self-reflection than others (e.g., creative writing, ambiguous QA). The theory also posits that the presence of external feedback or strong verifiers can compensate for model or task limitations.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Model-Scale Dependence Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; model &#8594; is_large_and_capable &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; reflection_pipeline &#8594; is_applied &#8594; to_model</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; self-reflection &#8594; is_effective &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Model_strength_sensitivity: Stronger/larger LLMs (e.g., GPT-4) exhibit emergent self-reflection and self-correction, while smaller/weaker models (e.g., starchat-beta) do not benefit from Reflexion. <a href="../results/extraction-result-5192.html#e5192.4" class="evidence-link">[e5192.4]</a> </li>
    <li>LLaMA2-family: Larger models (70B) show less self-bias amplification and more reliable self-refinement than smaller models (7B, 13B). <a href="../results/extraction-result-5219.html#e5219.3" class="evidence-link">[e5219.3]</a> </li>
    <li>SRT-7B: Self-refinement tuning with self-feedback (sDPO) yields little or negative gain for 7B models, but is more effective for larger models. <a href="../results/extraction-result-5215.html#e5215.0" class="evidence-link">[e5215.0]</a> </li>
    <li>Self-Refine: Mixed-refine (stronger model as feedback/refiner) improves over weak model self-reflection. <a href="../results/extraction-result-5187.html#e5187.1" class="evidence-link">[e5187.1]</a> </li>
    <li>Self-Consistency: Gains are smaller on smaller models (e.g., UL2-20B: GSM8K 4.1% -> 7.3%; PaLM-540B: 56.5% -> 74.4%). <a href="../results/extraction-result-5453.html#e5453.3" class="evidence-link">[e5453.3]</a> <a href="../results/extraction-result-5450.html#e5450.0" class="evidence-link">[e5450.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Scaling laws are known, but their specific impact on self-reflection is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Scaling laws and emergent abilities in LLMs are known.</p>            <p><strong>What is Novel:</strong> The explicit linkage of self-reflection efficacy to model scale and capability, and the empirical demonstration that self-reflection is ineffective or even harmful in small models.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Emergent Abilities of Large Language Models [scaling and emergence]</li>
    <li>Xu et al. (2024) Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement [self-bias in small models]</li>
</ul>
            <h3>Statement 1: Task-Structure Dependence Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; has_clear_verification_signal &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; reflection_pipeline &#8594; is_applied &#8594; to_task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; self-reflection &#8594; is_effective &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Self-Consistency, Self-Verification, and CoVe: Math, code, and structured QA tasks with clear verification signals show large gains from self-reflection (e.g., GSM8K, HumanEval, MultiSpanQA). <a href="../results/extraction-result-5450.html#e5450.0" class="evidence-link">[e5450.0]</a> <a href="../results/extraction-result-5447.html#e5447.0" class="evidence-link">[e5447.0]</a> <a href="../results/extraction-result-5183.html#e5183.2" class="evidence-link">[e5183.2]</a> <a href="../results/extraction-result-5444.html#e5444.0" class="evidence-link">[e5444.0]</a> <a href="../results/extraction-result-5444.html#e5444.3" class="evidence-link">[e5444.3]</a> </li>
    <li>CRITIC: Tool-augmented self-reflection is especially effective in tasks with external verification (QA, code, toxicity). <a href="../results/extraction-result-5221.html#e5221.0" class="evidence-link">[e5221.0]</a> <a href="../results/extraction-result-5221.html#e5221.1" class="evidence-link">[e5221.1]</a> </li>
    <li>Iterative-Refine (Creative Writing baseline): In creative writing, iterative self-reflection improves coherence but relies on the model's ability to self-assess; in some tasks, external feedback is needed. <a href="../results/extraction-result-5443.html#e5443.4" class="evidence-link">[e5443.4]</a> </li>
    <li>Self-Refine: In translation and open-ended tasks, self-reflection yields only marginal or inconsistent gains. <a href="../results/extraction-result-5207.html#e5207.0" class="evidence-link">[e5207.0]</a> <a href="../results/extraction-result-5219.html#e5219.1" class="evidence-link">[e5219.1]</a> </li>
    <li>ChatGPT-SR2V-HotpotQA: In multi-hop QA with high initial accuracy, self-reflection can harm performance. <a href="../results/extraction-result-5458.html#e5458.1" class="evidence-link">[e5458.1]</a> <a href="../results/extraction-result-5458.html#e5458.3" class="evidence-link">[e5458.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Task dependence is known, but its specific impact on self-reflection is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Task structure and verifiability are known to affect model performance.</p>            <p><strong>What is Novel:</strong> The explicit identification that self-reflection efficacy is modulated by task structure, with clear verification tasks benefiting most.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [task structure and self-consistency]</li>
    <li>Xu et al. (2024) Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement [task dependence]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a small or weak model is used in a self-reflection pipeline, it will fail to generate useful feedback and will not improve output quality.</li>
                <li>If a task lacks a clear verification signal (e.g., open-ended creative writing), self-reflection will yield little or no improvement.</li>
                <li>If a strong external verifier is added to a weak model or ambiguous task, output quality will improve.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a small model is trained with meta-skill learning or imitation of strong model feedback, it may acquire some self-reflection ability.</li>
                <li>If a task is restructured to include explicit intermediate verification steps, self-reflection may become more effective even for ambiguous tasks.</li>
                <li>If a model is trained to generate and verify its own outputs in a multi-task setting, it may generalize self-reflection to new tasks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If small models consistently benefit from self-reflection, this would challenge the model-scale dependence law.</li>
                <li>If self-reflection is equally effective on tasks without clear verification signals, this would challenge the task-structure dependence law.</li>
                <li>If adding a strong verifier to a weak model does not improve output quality, this would challenge the theory.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where a single self-reflection pass yields large improvement even in small models or ambiguous tasks (e.g., Gemini 1.5-Flash). <a href="../results/extraction-result-5202.html#e5202.2" class="evidence-link">[e5202.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known scaling and task effects into a novel framework for self-reflection.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Emergent Abilities of Large Language Models [scaling and emergence]</li>
    <li>Xu et al. (2024) Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement [self-bias and task dependence]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Task- and Model-Dependence of Self-Reflection Efficacy in LLMs",
    "theory_description": "This theory asserts that the effectiveness of self-reflection and iterative refinement in LLMs is highly dependent on both the nature of the task (e.g., math, code, open-ended generation, safety) and the underlying model's scale and capability. The theory claims that strong, large models are more likely to benefit from self-reflection, while smaller or less capable models may fail to generate useful feedback or corrections, and that certain tasks (e.g., those with clear verification signals) are more amenable to self-reflection than others (e.g., creative writing, ambiguous QA). The theory also posits that the presence of external feedback or strong verifiers can compensate for model or task limitations.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Model-Scale Dependence Law",
                "if": [
                    {
                        "subject": "model",
                        "relation": "is_large_and_capable",
                        "object": "True"
                    },
                    {
                        "subject": "reflection_pipeline",
                        "relation": "is_applied",
                        "object": "to_model"
                    }
                ],
                "then": [
                    {
                        "subject": "self-reflection",
                        "relation": "is_effective",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Model_strength_sensitivity: Stronger/larger LLMs (e.g., GPT-4) exhibit emergent self-reflection and self-correction, while smaller/weaker models (e.g., starchat-beta) do not benefit from Reflexion.",
                        "uuids": [
                            "e5192.4"
                        ]
                    },
                    {
                        "text": "LLaMA2-family: Larger models (70B) show less self-bias amplification and more reliable self-refinement than smaller models (7B, 13B).",
                        "uuids": [
                            "e5219.3"
                        ]
                    },
                    {
                        "text": "SRT-7B: Self-refinement tuning with self-feedback (sDPO) yields little or negative gain for 7B models, but is more effective for larger models.",
                        "uuids": [
                            "e5215.0"
                        ]
                    },
                    {
                        "text": "Self-Refine: Mixed-refine (stronger model as feedback/refiner) improves over weak model self-reflection.",
                        "uuids": [
                            "e5187.1"
                        ]
                    },
                    {
                        "text": "Self-Consistency: Gains are smaller on smaller models (e.g., UL2-20B: GSM8K 4.1% -&gt; 7.3%; PaLM-540B: 56.5% -&gt; 74.4%).",
                        "uuids": [
                            "e5453.3",
                            "e5450.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Scaling laws and emergent abilities in LLMs are known.",
                    "what_is_novel": "The explicit linkage of self-reflection efficacy to model scale and capability, and the empirical demonstration that self-reflection is ineffective or even harmful in small models.",
                    "classification_explanation": "Scaling laws are known, but their specific impact on self-reflection is a novel synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Emergent Abilities of Large Language Models [scaling and emergence]",
                        "Xu et al. (2024) Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement [self-bias in small models]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Task-Structure Dependence Law",
                "if": [
                    {
                        "subject": "task",
                        "relation": "has_clear_verification_signal",
                        "object": "True"
                    },
                    {
                        "subject": "reflection_pipeline",
                        "relation": "is_applied",
                        "object": "to_task"
                    }
                ],
                "then": [
                    {
                        "subject": "self-reflection",
                        "relation": "is_effective",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Self-Consistency, Self-Verification, and CoVe: Math, code, and structured QA tasks with clear verification signals show large gains from self-reflection (e.g., GSM8K, HumanEval, MultiSpanQA).",
                        "uuids": [
                            "e5450.0",
                            "e5447.0",
                            "e5183.2",
                            "e5444.0",
                            "e5444.3"
                        ]
                    },
                    {
                        "text": "CRITIC: Tool-augmented self-reflection is especially effective in tasks with external verification (QA, code, toxicity).",
                        "uuids": [
                            "e5221.0",
                            "e5221.1"
                        ]
                    },
                    {
                        "text": "Iterative-Refine (Creative Writing baseline): In creative writing, iterative self-reflection improves coherence but relies on the model's ability to self-assess; in some tasks, external feedback is needed.",
                        "uuids": [
                            "e5443.4"
                        ]
                    },
                    {
                        "text": "Self-Refine: In translation and open-ended tasks, self-reflection yields only marginal or inconsistent gains.",
                        "uuids": [
                            "e5207.0",
                            "e5219.1"
                        ]
                    },
                    {
                        "text": "ChatGPT-SR2V-HotpotQA: In multi-hop QA with high initial accuracy, self-reflection can harm performance.",
                        "uuids": [
                            "e5458.1",
                            "e5458.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Task structure and verifiability are known to affect model performance.",
                    "what_is_novel": "The explicit identification that self-reflection efficacy is modulated by task structure, with clear verification tasks benefiting most.",
                    "classification_explanation": "Task dependence is known, but its specific impact on self-reflection is a novel synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [task structure and self-consistency]",
                        "Xu et al. (2024) Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement [task dependence]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a small or weak model is used in a self-reflection pipeline, it will fail to generate useful feedback and will not improve output quality.",
        "If a task lacks a clear verification signal (e.g., open-ended creative writing), self-reflection will yield little or no improvement.",
        "If a strong external verifier is added to a weak model or ambiguous task, output quality will improve."
    ],
    "new_predictions_unknown": [
        "If a small model is trained with meta-skill learning or imitation of strong model feedback, it may acquire some self-reflection ability.",
        "If a task is restructured to include explicit intermediate verification steps, self-reflection may become more effective even for ambiguous tasks.",
        "If a model is trained to generate and verify its own outputs in a multi-task setting, it may generalize self-reflection to new tasks."
    ],
    "negative_experiments": [
        "If small models consistently benefit from self-reflection, this would challenge the model-scale dependence law.",
        "If self-reflection is equally effective on tasks without clear verification signals, this would challenge the task-structure dependence law.",
        "If adding a strong verifier to a weak model does not improve output quality, this would challenge the theory."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where a single self-reflection pass yields large improvement even in small models or ambiguous tasks (e.g., Gemini 1.5-Flash).",
            "uuids": [
                "e5202.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some self-reflection pipelines show small but consistent improvements even in ambiguous or open-ended tasks (e.g., creative writing, translation).",
            "uuids": [
                "e5443.4",
                "e5207.0"
            ]
        }
    ],
    "special_cases": [
        "If the model is trained with explicit meta-skills or imitation of strong model feedback, small models may acquire some self-reflection ability.",
        "If the task is restructured to include explicit verification, self-reflection may become more effective."
    ],
    "existing_theory": {
        "what_already_exists": "Scaling laws and task dependence are known in ML.",
        "what_is_novel": "The explicit linkage of self-reflection efficacy to both model scale and task structure, and the empirical demonstration of their interaction.",
        "classification_explanation": "The theory synthesizes known scaling and task effects into a novel framework for self-reflection.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Emergent Abilities of Large Language Models [scaling and emergence]",
            "Xu et al. (2024) Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement [self-bias and task dependence]"
        ]
    },
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
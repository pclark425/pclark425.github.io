<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Error Taxonomy-Driven Self-Reflection in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1428</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1428</p>
                <p><strong>Name:</strong> Error Taxonomy-Driven Self-Reflection in LLMs</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.</p>
                <p><strong>Description:</strong> This theory asserts that LLMs can be guided to improve their outputs by explicitly categorizing the types of errors present in their initial responses (e.g., factual, logical, stylistic), and then targeting revisions to address each error class in turn. The process leverages the model's ability to recognize and label error types, enabling more focused and effective self-correction.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Explicit Error Categorization Enhances Targeted Revision (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; reflects_on &#8594; output<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; categorizes &#8594; errors_in_output (by type)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; generates &#8594; revisions_targeting_each_error_type<span style="color: #888888;">, and</span></div>
        <div>&#8226; revised_output &#8594; has_fewer_errors_of_categorized_types &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompting LLMs to identify and categorize errors leads to more effective revisions than generic self-critique. </li>
    <li>Human editing workflows often use error taxonomies to guide revision. </li>
    <li>Iterative refinement with self-feedback in LLMs improves output quality, but is more effective when feedback is structured and specific. </li>
    <li>Error categorization is a standard practice in human writing and editing, supporting targeted correction. </li>
    <li>LLMs can be prompted to identify factual, logical, and stylistic errors in their own outputs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The explicit use of error taxonomies in iterative LLM self-reflection is a novel, specific mechanism.</p>            <p><strong>What Already Exists:</strong> Error categorization is used in human editing and some LLM prompting, but not formalized as a mechanism for iterative self-improvement.</p>            <p><strong>What is Novel:</strong> This law formalizes error taxonomy-driven revision as a specific, structured mechanism for LLM self-reflection.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [uses generic feedback, not error taxonomies]</li>
    <li>Liu et al. (2023) Evaluating the Factual Consistency of LLMs [error categorization in evaluation, not revision]</li>
    <li>Stiennon et al. (2020) Learning to summarize with human feedback [uses human feedback, not error taxonomy-driven self-reflection]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs prompted to categorize errors before revision will outperform those using generic self-critique on tasks requiring factual and logical accuracy.</li>
                <li>Providing LLMs with a structured error taxonomy will accelerate convergence to high-quality outputs in iterative refinement.</li>
                <li>LLMs using error taxonomy-driven self-reflection will produce more interpretable and traceable revision histories.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may develop novel, emergent error categories when exposed to new domains or tasks.</li>
                <li>Iterative error taxonomy-driven revision may enable LLMs to self-correct subtle, high-level reasoning errors not detectable in single-pass outputs.</li>
                <li>Taxonomy-driven self-reflection may enable LLMs to generalize error correction strategies across tasks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If error categorization does not lead to improved revisions compared to generic self-critique, the theory is challenged.</li>
                <li>If LLMs cannot reliably categorize their own errors, the mechanism fails.</li>
                <li>If error taxonomy-driven revision leads to overfitting to specific error types and neglects holistic improvements, the theory is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some errors may be miscategorized or missed entirely, limiting the effectiveness of the approach. </li>
    <li>LLMs may lack the meta-cognitive ability to recognize certain types of errors, especially in novel domains. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The explicit, structured use of error taxonomies in iterative LLM self-reflection is a novel contribution.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) Evaluating the Factual Consistency of LLMs [error categorization in evaluation]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [iterative revision, not taxonomy-driven]</li>
    <li>Stiennon et al. (2020) Learning to summarize with human feedback [human feedback, not error taxonomy-driven self-reflection]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Error Taxonomy-Driven Self-Reflection in LLMs",
    "theory_description": "This theory asserts that LLMs can be guided to improve their outputs by explicitly categorizing the types of errors present in their initial responses (e.g., factual, logical, stylistic), and then targeting revisions to address each error class in turn. The process leverages the model's ability to recognize and label error types, enabling more focused and effective self-correction.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Explicit Error Categorization Enhances Targeted Revision",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "reflects_on",
                        "object": "output"
                    },
                    {
                        "subject": "LLM",
                        "relation": "categorizes",
                        "object": "errors_in_output (by type)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "generates",
                        "object": "revisions_targeting_each_error_type"
                    },
                    {
                        "subject": "revised_output",
                        "relation": "has_fewer_errors_of_categorized_types",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompting LLMs to identify and categorize errors leads to more effective revisions than generic self-critique.",
                        "uuids": []
                    },
                    {
                        "text": "Human editing workflows often use error taxonomies to guide revision.",
                        "uuids": []
                    },
                    {
                        "text": "Iterative refinement with self-feedback in LLMs improves output quality, but is more effective when feedback is structured and specific.",
                        "uuids": []
                    },
                    {
                        "text": "Error categorization is a standard practice in human writing and editing, supporting targeted correction.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can be prompted to identify factual, logical, and stylistic errors in their own outputs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Error categorization is used in human editing and some LLM prompting, but not formalized as a mechanism for iterative self-improvement.",
                    "what_is_novel": "This law formalizes error taxonomy-driven revision as a specific, structured mechanism for LLM self-reflection.",
                    "classification_explanation": "The explicit use of error taxonomies in iterative LLM self-reflection is a novel, specific mechanism.",
                    "likely_classification": "new",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [uses generic feedback, not error taxonomies]",
                        "Liu et al. (2023) Evaluating the Factual Consistency of LLMs [error categorization in evaluation, not revision]",
                        "Stiennon et al. (2020) Learning to summarize with human feedback [uses human feedback, not error taxonomy-driven self-reflection]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs prompted to categorize errors before revision will outperform those using generic self-critique on tasks requiring factual and logical accuracy.",
        "Providing LLMs with a structured error taxonomy will accelerate convergence to high-quality outputs in iterative refinement.",
        "LLMs using error taxonomy-driven self-reflection will produce more interpretable and traceable revision histories."
    ],
    "new_predictions_unknown": [
        "LLMs may develop novel, emergent error categories when exposed to new domains or tasks.",
        "Iterative error taxonomy-driven revision may enable LLMs to self-correct subtle, high-level reasoning errors not detectable in single-pass outputs.",
        "Taxonomy-driven self-reflection may enable LLMs to generalize error correction strategies across tasks."
    ],
    "negative_experiments": [
        "If error categorization does not lead to improved revisions compared to generic self-critique, the theory is challenged.",
        "If LLMs cannot reliably categorize their own errors, the mechanism fails.",
        "If error taxonomy-driven revision leads to overfitting to specific error types and neglects holistic improvements, the theory is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "Some errors may be miscategorized or missed entirely, limiting the effectiveness of the approach.",
            "uuids": []
        },
        {
            "text": "LLMs may lack the meta-cognitive ability to recognize certain types of errors, especially in novel domains.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, error categorization may introduce bias or tunnel vision, causing LLMs to overlook holistic improvements.",
            "uuids": []
        },
        {
            "text": "Some studies show that too rigid a focus on error types can reduce creativity or adaptability in revision.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with ambiguous or subjective error types may not benefit from taxonomy-driven revision.",
        "LLMs with limited error recognition ability may not realize gains from this approach.",
        "Highly creative or open-ended tasks may be hindered by strict error taxonomies."
    ],
    "existing_theory": {
        "what_already_exists": "Error categorization is used in evaluation and human editing, but not as a structured, iterative LLM self-reflection mechanism.",
        "what_is_novel": "The theory formalizes error taxonomy-driven revision as a specific, structured mechanism for LLM self-improvement.",
        "classification_explanation": "The explicit, structured use of error taxonomies in iterative LLM self-reflection is a novel contribution.",
        "likely_classification": "new",
        "references": [
            "Liu et al. (2023) Evaluating the Factual Consistency of LLMs [error categorization in evaluation]",
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [iterative revision, not taxonomy-driven]",
            "Stiennon et al. (2020) Learning to summarize with human feedback [human feedback, not error taxonomy-driven self-reflection]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "type": "specific",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.",
    "original_theory_id": "theory-622",
    "original_theory_name": "Iterative Self-Reflection as a General Mechanism for Improving LLM Output Quality",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Iterative Self-Reflection as a General Mechanism for Improving LLM Output Quality",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Systematic Biases and Calibration Failures in LLM-as-a-Judge Evaluations - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-543</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-543</p>
                <p><strong>Name:</strong> Systematic Biases and Calibration Failures in LLM-as-a-Judge Evaluations</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of what LLM-as-a-judge style evaluations lose compared to human evaluations, based on the following results.</p>
                <p><strong>Description:</strong> LLM-as-a-judge evaluations are systematically affected by a set of cognitive and structural biases (position, verbosity, authority, beauty, self-enhancement, and familiarity), as well as calibration failures (non-linear mapping of scores, overuse of certain score ranges, and prompt sensitivity). These biases and calibration issues cause LLM judges to diverge from human judgments, especially in edge cases, adversarial settings, and for subjective or nuanced properties. The theory posits that these biases are intrinsic to the LLM evaluation process and are only partially mitigated by prompt engineering, calibration, or ensemble methods.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Systematic Bias Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; evaluates &#8594; outputs in a setting where superficial features (position, length, formatting, references, etc.) are present</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; is susceptible to &#8594; systematic biases (position, verbosity, authority, beauty, self-enhancement, familiarity) that alter evaluation outcomes relative to human judgments</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs amplify several cognitive biases (order, compassion/name, egocentric/self-preference, salience/length, bandwagon, attentional) in evaluation outputs, often to a greater extent than humans. <a href="../results/extraction-result-3998.html#e3998.2" class="evidence-link">[e3998.2]</a> <a href="../results/extraction-result-3998.html#e3998.3" class="evidence-link">[e3998.3]</a> </li>
    <li>Position bias: LLM judges favor responses presented in a particular position (often the first), observable across multiple LLM judges. <a href="../results/extraction-result-3858.html#e3858.2" class="evidence-link">[e3858.2]</a> <a href="../results/extraction-result-3859.html#e3859.2" class="evidence-link">[e3859.2]</a> <a href="../results/extraction-result-4004.html#e4004.1" class="evidence-link">[e4004.1]</a> </li>
    <li>Verbosity bias: LLM judges prefer longer or more verbose responses, even when extra verbosity adds no new information. <a href="../results/extraction-result-3858.html#e3858.3" class="evidence-link">[e3858.3]</a> <a href="../results/extraction-result-4004.html#e4004.1" class="evidence-link">[e4004.1]</a> </li>
    <li>Authority bias: Both humans and LLMs tend to be swayed by added references even when references are fake. <a href="../results/extraction-result-3863.html#e3863.3" class="evidence-link">[e3863.3]</a> </li>
    <li>Beauty bias: Humans and several LLMs are strongly influenced by rich formatting, often preferring content with attention-grabbing elements even when semantics unchanged. <a href="../results/extraction-result-3863.html#e3863.4" class="evidence-link">[e3863.4]</a> </li>
    <li>Self-enhancement bias: Some LLM judges favor outputs they produced; this differs from humans and can skew comparative evaluations. <a href="../results/extraction-result-3858.html#e3858.4" class="evidence-link">[e3858.4]</a> <a href="../results/extraction-result-4015.html#e4015.3" class="evidence-link">[e4015.3]</a> </li>
    <li>Familiarity bias: LLMs show a strong familiarity bias (prefer lower-perplexity/familiar text), under-utilize portions of rating scales (sparse predictions, round-number peaks), and over-correlate multiple attribute scores due to anchoring. <a href="../results/extraction-result-4012.html#e4012.0" class="evidence-link">[e4012.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Calibration Failure Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; outputs &#8594; scores or labels for evaluation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; exhibits &#8594; non-linear calibration, overuse of certain score ranges, and prompt sensitivity, leading to misalignment with human-perceived qualitative changes</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Calibration is non-linear (LM logits do not map uniformly to perceived qualitative change), requiring non-uniform binning to better match human perceptions. <a href="../results/extraction-result-4005.html#e4005.0" class="evidence-link">[e4005.0]</a> </li>
    <li>LLM evaluators tend to output integer-only scores with low variance creating many ties; using token-probability-weighted summation produces continuous scores and improves Spearman correlation while affecting Kendall-Tau behavior. <a href="../results/extraction-result-3860.html#e3860.5" class="evidence-link">[e3860.5]</a> </li>
    <li>Prompt ablations for judge models (notably GPT-4) show that few-shot examples and explicit instructions materially change the kappa agreement between LLM judges and human annotators. <a href="../results/extraction-result-4015.html#e4015.2" class="evidence-link">[e4015.2]</a> </li>
    <li>Prompt sensitivity: Different prompt formats shift the judge's evaluation strategy, causing qualitative changes in which errors are detected or ignored. <a href="../results/extraction-result-4015.html#e4015.2" class="evidence-link">[e4015.2]</a> <a href="../results/extraction-result-4007.html#e4007.1" class="evidence-link">[e4007.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new superficial feature (e.g., a novel formatting style) is introduced into candidate outputs, LLM judges will show a systematic bias toward those outputs, even if human judges do not.</li>
                <li>If LLM judges are evaluated on a new task with adversarially manipulated superficial features, their agreement with human judgments will decrease.</li>
                <li>If prompt design is altered (e.g., changing the order of instructions or examples), the calibration and agreement of LLM judges with humans will change significantly.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained with explicit debiasing objectives (e.g., adversarial training against position or verbosity bias), it is unknown whether these biases can be fully eliminated.</li>
                <li>If LLM judges are used in domains with entirely different superficial features (e.g., non-textual modalities), it is unknown whether similar biases will emerge.</li>
                <li>If calibration is improved via advanced post-processing (e.g., learned non-linear mappings), it is unknown whether this will fully close the gap to human alignment.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLM judges show no systematic bias toward superficial features in a controlled experiment, this would challenge the systematic bias law.</li>
                <li>If calibration improvements (e.g., probability normalization) do not improve agreement with human judgments, this would challenge the calibration failure law.</li>
                <li>If prompt changes do not affect LLM judge outputs or agreement with humans, this would challenge the prompt sensitivity aspect of the theory.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLM judges outperform humans in factual error detection, regardless of superficial features. <a href="../results/extraction-result-3863.html#e3863.1" class="evidence-link">[e3863.1]</a> </li>
    <li>Instances where fine-tuned open-source judges (e.g., PandaLM-70B, JudgeLM-33B) outperform closed-source judges (GPT-4) on certain human-annotated test sets, even in the presence of superficial features. <a href="../results/extraction-result-4011.html#e4011.3" class="evidence-link">[e4011.3]</a> <a href="../results/extraction-result-4016.html#e4016.1" class="evidence-link">[e4016.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Wu & Aji (2023) Style Over Substance: Evaluation Biases for Large Language Models [Documents position, verbosity, and self-enhancement biases]</li>
    <li>Raina et al. (2024) Is LLM-as-a-Judge Robust? [Documents adversarial vulnerabilities and calibration issues]</li>
    <li>Stureborg et al. (2024) Large Language Models are Inconsistent and Biased Evaluators [Documents familiarity, anchoring, and calibration biases]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Systematic Biases and Calibration Failures in LLM-as-a-Judge Evaluations",
    "theory_description": "LLM-as-a-judge evaluations are systematically affected by a set of cognitive and structural biases (position, verbosity, authority, beauty, self-enhancement, and familiarity), as well as calibration failures (non-linear mapping of scores, overuse of certain score ranges, and prompt sensitivity). These biases and calibration issues cause LLM judges to diverge from human judgments, especially in edge cases, adversarial settings, and for subjective or nuanced properties. The theory posits that these biases are intrinsic to the LLM evaluation process and are only partially mitigated by prompt engineering, calibration, or ensemble methods.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Systematic Bias Law",
                "if": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "evaluates",
                        "object": "outputs in a setting where superficial features (position, length, formatting, references, etc.) are present"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "is susceptible to",
                        "object": "systematic biases (position, verbosity, authority, beauty, self-enhancement, familiarity) that alter evaluation outcomes relative to human judgments"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs amplify several cognitive biases (order, compassion/name, egocentric/self-preference, salience/length, bandwagon, attentional) in evaluation outputs, often to a greater extent than humans.",
                        "uuids": [
                            "e3998.2",
                            "e3998.3"
                        ]
                    },
                    {
                        "text": "Position bias: LLM judges favor responses presented in a particular position (often the first), observable across multiple LLM judges.",
                        "uuids": [
                            "e3858.2",
                            "e3859.2",
                            "e4004.1"
                        ]
                    },
                    {
                        "text": "Verbosity bias: LLM judges prefer longer or more verbose responses, even when extra verbosity adds no new information.",
                        "uuids": [
                            "e3858.3",
                            "e4004.1"
                        ]
                    },
                    {
                        "text": "Authority bias: Both humans and LLMs tend to be swayed by added references even when references are fake.",
                        "uuids": [
                            "e3863.3"
                        ]
                    },
                    {
                        "text": "Beauty bias: Humans and several LLMs are strongly influenced by rich formatting, often preferring content with attention-grabbing elements even when semantics unchanged.",
                        "uuids": [
                            "e3863.4"
                        ]
                    },
                    {
                        "text": "Self-enhancement bias: Some LLM judges favor outputs they produced; this differs from humans and can skew comparative evaluations.",
                        "uuids": [
                            "e3858.4",
                            "e4015.3"
                        ]
                    },
                    {
                        "text": "Familiarity bias: LLMs show a strong familiarity bias (prefer lower-perplexity/familiar text), under-utilize portions of rating scales (sparse predictions, round-number peaks), and over-correlate multiple attribute scores due to anchoring.",
                        "uuids": [
                            "e4012.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Calibration Failure Law",
                "if": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "outputs",
                        "object": "scores or labels for evaluation"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "exhibits",
                        "object": "non-linear calibration, overuse of certain score ranges, and prompt sensitivity, leading to misalignment with human-perceived qualitative changes"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Calibration is non-linear (LM logits do not map uniformly to perceived qualitative change), requiring non-uniform binning to better match human perceptions.",
                        "uuids": [
                            "e4005.0"
                        ]
                    },
                    {
                        "text": "LLM evaluators tend to output integer-only scores with low variance creating many ties; using token-probability-weighted summation produces continuous scores and improves Spearman correlation while affecting Kendall-Tau behavior.",
                        "uuids": [
                            "e3860.5"
                        ]
                    },
                    {
                        "text": "Prompt ablations for judge models (notably GPT-4) show that few-shot examples and explicit instructions materially change the kappa agreement between LLM judges and human annotators.",
                        "uuids": [
                            "e4015.2"
                        ]
                    },
                    {
                        "text": "Prompt sensitivity: Different prompt formats shift the judge's evaluation strategy, causing qualitative changes in which errors are detected or ignored.",
                        "uuids": [
                            "e4015.2",
                            "e4007.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new superficial feature (e.g., a novel formatting style) is introduced into candidate outputs, LLM judges will show a systematic bias toward those outputs, even if human judges do not.",
        "If LLM judges are evaluated on a new task with adversarially manipulated superficial features, their agreement with human judgments will decrease.",
        "If prompt design is altered (e.g., changing the order of instructions or examples), the calibration and agreement of LLM judges with humans will change significantly."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained with explicit debiasing objectives (e.g., adversarial training against position or verbosity bias), it is unknown whether these biases can be fully eliminated.",
        "If LLM judges are used in domains with entirely different superficial features (e.g., non-textual modalities), it is unknown whether similar biases will emerge.",
        "If calibration is improved via advanced post-processing (e.g., learned non-linear mappings), it is unknown whether this will fully close the gap to human alignment."
    ],
    "negative_experiments": [
        "If LLM judges show no systematic bias toward superficial features in a controlled experiment, this would challenge the systematic bias law.",
        "If calibration improvements (e.g., probability normalization) do not improve agreement with human judgments, this would challenge the calibration failure law.",
        "If prompt changes do not affect LLM judge outputs or agreement with humans, this would challenge the prompt sensitivity aspect of the theory."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLM judges outperform humans in factual error detection, regardless of superficial features.",
            "uuids": [
                "e3863.1"
            ]
        },
        {
            "text": "Instances where fine-tuned open-source judges (e.g., PandaLM-70B, JudgeLM-33B) outperform closed-source judges (GPT-4) on certain human-annotated test sets, even in the presence of superficial features.",
            "uuids": [
                "e4011.3",
                "e4016.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLM judges (e.g., GPT-4o, Claude-3) show less susceptibility to certain biases (e.g., authority, beauty) and are more robust to superficial features.",
            "uuids": [
                "e3863.0",
                "e3863.5"
            ]
        }
    ],
    "special_cases": [
        "On tasks where superficial features are strictly controlled or absent, systematic biases may be minimized.",
        "For outputs that are highly constrained (e.g., strict factual QA), calibration failures may be less pronounced.",
        "Panel-based or ensemble approaches may reduce, but not eliminate, systematic biases and calibration failures."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wu & Aji (2023) Style Over Substance: Evaluation Biases for Large Language Models [Documents position, verbosity, and self-enhancement biases]",
            "Raina et al. (2024) Is LLM-as-a-Judge Robust? [Documents adversarial vulnerabilities and calibration issues]",
            "Stureborg et al. (2024) Large Language Models are Inconsistent and Biased Evaluators [Documents familiarity, anchoring, and calibration biases]"
        ]
    },
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
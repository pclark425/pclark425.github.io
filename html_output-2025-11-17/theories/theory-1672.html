<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contextual Alignment and Information Bottleneck Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1672</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1672</p>
                <p><strong>Name:</strong> Contextual Alignment and Information Bottleneck Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory posits that the effectiveness of structure-aware demonstration retrieval in LLM-based molecular property prediction is governed by the degree to which demonstrations provide contextually aligned, information-rich cues relevant to the property of interest. The LLM acts as an information bottleneck, filtering and amplifying signals from demonstrations that are both structurally and property-relevant, while discarding irrelevant or noisy information. The theory further predicts that the interplay between demonstration relevance and the LLM's internal priors determines the accuracy and robustness of property prediction.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contextual Alignment Amplifies Signal (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; demonstration_set &#8594; is_contextually_aligned_with &#8594; property_of_interest<span style="color: #888888;">, and</span></div>
        <div>&#8226; demonstration_set &#8594; has_structural_similarity_to &#8594; query_molecule</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; amplifies_relevant_information &#8594; property_prediction</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better when demonstrations are both structurally similar and property-relevant, as shown in ablation studies. </li>
    <li>Information bottleneck theory in deep learning suggests models filter out irrelevant input features. </li>
    <li>Empirical results in molecular property prediction show that structure-aware retrieval improves LLM accuracy over random or property-irrelevant retrieval. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This law synthesizes concepts from information theory and in-context learning, applying them to a new domain.</p>            <p><strong>What Already Exists:</strong> Information bottleneck theory is established in deep learning, and contextual alignment is known to improve LLM performance.</p>            <p><strong>What is Novel:</strong> The explicit connection between contextual alignment, structure-aware retrieval, and information bottlenecking in LLM-based molecular property prediction is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]</li>
    <li>Zhang (2023) In-context learning for molecular property prediction [Contextual alignment in LLMs for chemistry]</li>
</ul>
            <h3>Statement 1: Irrelevant Demonstrations Are Filtered Out (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; demonstration_set &#8594; is_not_contextually_aligned_with &#8594; property_of_interest</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; filters_out &#8594; irrelevant_information<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; relies_on &#8594; internal_knowledge_or_guessing</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs show reduced performance when demonstrations are irrelevant, often defaulting to prior knowledge or generic responses. </li>
    <li>Ablation studies in in-context learning show that irrelevant demonstrations are ignored by LLMs in both NLP and molecular property prediction tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The general effect is known, but its formalization in this context is new.</p>            <p><strong>What Already Exists:</strong> LLMs are known to ignore irrelevant context in NLP tasks.</p>            <p><strong>What is Novel:</strong> The explicit framing of this as an information bottleneck in the context of molecular property prediction is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]</li>
    <li>Min (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [LLMs ignore irrelevant demonstrations in NLP]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If demonstrations are both structurally similar and property-relevant, LLMs will outperform cases where only one of these conditions is met.</li>
                <li>If demonstrations are structurally similar but property-irrelevant, LLMs will filter out the irrelevant information and default to prior knowledge.</li>
                <li>If demonstrations are property-relevant but structurally dissimilar, LLMs will show intermediate performance, depending on the property.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If demonstrations are contextually aligned but structurally dissimilar, the LLM's performance may depend on the property being predicted.</li>
                <li>If the LLM is fine-tuned to amplify weak signals from partially relevant demonstrations, the bottleneck effect may be reduced or eliminated.</li>
                <li>If demonstrations are adversarially constructed to be misleading but structurally similar, the LLM may be misled, challenging the bottleneck hypothesis.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs do not filter out irrelevant demonstrations and instead are misled by them, this would challenge the theory.</li>
                <li>If LLMs cannot amplify relevant signals even when demonstrations are contextually aligned, the theory would be called into question.</li>
                <li>If LLMs perform equally well with random, irrelevant, or adversarial demonstrations, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs memorize specific property-molecule pairs and bypass the need for contextual alignment. </li>
    <li>LLMs with strong internal priors may perform well even with weak or noisy demonstrations. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory synthesizes established concepts into a new, domain-specific framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Tishby (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]</li>
    <li>Min (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Contextual alignment in LLMs]</li>
    <li>Zhang (2023) In-context learning for molecular property prediction [Empirical evidence for demonstration effects in chemistry]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Contextual Alignment and Information Bottleneck Theory",
    "theory_description": "This theory posits that the effectiveness of structure-aware demonstration retrieval in LLM-based molecular property prediction is governed by the degree to which demonstrations provide contextually aligned, information-rich cues relevant to the property of interest. The LLM acts as an information bottleneck, filtering and amplifying signals from demonstrations that are both structurally and property-relevant, while discarding irrelevant or noisy information. The theory further predicts that the interplay between demonstration relevance and the LLM's internal priors determines the accuracy and robustness of property prediction.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contextual Alignment Amplifies Signal",
                "if": [
                    {
                        "subject": "demonstration_set",
                        "relation": "is_contextually_aligned_with",
                        "object": "property_of_interest"
                    },
                    {
                        "subject": "demonstration_set",
                        "relation": "has_structural_similarity_to",
                        "object": "query_molecule"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "amplifies_relevant_information",
                        "object": "property_prediction"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better when demonstrations are both structurally similar and property-relevant, as shown in ablation studies.",
                        "uuids": []
                    },
                    {
                        "text": "Information bottleneck theory in deep learning suggests models filter out irrelevant input features.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical results in molecular property prediction show that structure-aware retrieval improves LLM accuracy over random or property-irrelevant retrieval.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Information bottleneck theory is established in deep learning, and contextual alignment is known to improve LLM performance.",
                    "what_is_novel": "The explicit connection between contextual alignment, structure-aware retrieval, and information bottlenecking in LLM-based molecular property prediction is novel.",
                    "classification_explanation": "This law synthesizes concepts from information theory and in-context learning, applying them to a new domain.",
                    "likely_classification": "new",
                    "references": [
                        "Tishby (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]",
                        "Zhang (2023) In-context learning for molecular property prediction [Contextual alignment in LLMs for chemistry]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Irrelevant Demonstrations Are Filtered Out",
                "if": [
                    {
                        "subject": "demonstration_set",
                        "relation": "is_not_contextually_aligned_with",
                        "object": "property_of_interest"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "filters_out",
                        "object": "irrelevant_information"
                    },
                    {
                        "subject": "LLM",
                        "relation": "relies_on",
                        "object": "internal_knowledge_or_guessing"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs show reduced performance when demonstrations are irrelevant, often defaulting to prior knowledge or generic responses.",
                        "uuids": []
                    },
                    {
                        "text": "Ablation studies in in-context learning show that irrelevant demonstrations are ignored by LLMs in both NLP and molecular property prediction tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to ignore irrelevant context in NLP tasks.",
                    "what_is_novel": "The explicit framing of this as an information bottleneck in the context of molecular property prediction is novel.",
                    "classification_explanation": "The general effect is known, but its formalization in this context is new.",
                    "likely_classification": "new",
                    "references": [
                        "Tishby (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]",
                        "Min (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [LLMs ignore irrelevant demonstrations in NLP]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If demonstrations are both structurally similar and property-relevant, LLMs will outperform cases where only one of these conditions is met.",
        "If demonstrations are structurally similar but property-irrelevant, LLMs will filter out the irrelevant information and default to prior knowledge.",
        "If demonstrations are property-relevant but structurally dissimilar, LLMs will show intermediate performance, depending on the property."
    ],
    "new_predictions_unknown": [
        "If demonstrations are contextually aligned but structurally dissimilar, the LLM's performance may depend on the property being predicted.",
        "If the LLM is fine-tuned to amplify weak signals from partially relevant demonstrations, the bottleneck effect may be reduced or eliminated.",
        "If demonstrations are adversarially constructed to be misleading but structurally similar, the LLM may be misled, challenging the bottleneck hypothesis."
    ],
    "negative_experiments": [
        "If LLMs do not filter out irrelevant demonstrations and instead are misled by them, this would challenge the theory.",
        "If LLMs cannot amplify relevant signals even when demonstrations are contextually aligned, the theory would be called into question.",
        "If LLMs perform equally well with random, irrelevant, or adversarial demonstrations, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs memorize specific property-molecule pairs and bypass the need for contextual alignment.",
            "uuids": []
        },
        {
            "text": "LLMs with strong internal priors may perform well even with weak or noisy demonstrations.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show unpredictable behavior when presented with mixed-relevance demonstrations, suggesting incomplete filtering.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For properties with simple, global rules (e.g., molecular weight), contextual alignment may be less important.",
        "In low-data regimes, the bottleneck effect may be dominated by model priors rather than demonstration content.",
        "For LLMs with limited chemistry knowledge, even contextually aligned demonstrations may not improve performance."
    ],
    "existing_theory": {
        "what_already_exists": "Information bottleneck and contextual alignment are established in deep learning and NLP.",
        "what_is_novel": "Their explicit application and law-like formulation for structure-aware demonstration retrieval in molecular property prediction is new.",
        "classification_explanation": "The theory synthesizes established concepts into a new, domain-specific framework.",
        "likely_classification": "new",
        "references": [
            "Tishby (2015) Deep Learning and the Information Bottleneck Principle [Information bottleneck in deep learning]",
            "Min (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Contextual alignment in LLMs]",
            "Zhang (2023) In-context learning for molecular property prediction [Empirical evidence for demonstration effects in chemistry]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-638",
    "original_theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
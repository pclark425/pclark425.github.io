<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interpretability-Induced Generalization Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-305</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-305</p>
                <p><strong>Name:</strong> Interpretability-Induced Generalization Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about what constitutes an optimal world model for AI systems, incorporating fidelity, interpretability, computational efficiency, and task-specific utility.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory challenges the conventional view of interpretability-performance trade-offs by proposing that certain types of interpretability constraints can improve generalization performance, particularly in out-of-distribution (OOD) scenarios. The theory posits that interpretability constraints that enforce alignment with causal structure, compositional principles, or modular representations act as beneficial inductive biases. These constraints encourage world models to learn representations that capture the underlying generative structure of the environment rather than superficial statistical patterns. While such constraints may reduce in-distribution performance by preventing overfitting to dataset-specific correlations, they can improve robustness to distribution shift. The theory specifically predicts that the generalization benefit is strongest when: (1) interpretability constraints align with the true causal or compositional structure of the domain, (2) the deployment environment involves significant distribution shift from training, and (3) the training data contains spurious correlations that black-box models would exploit. The theory does not claim all interpretability constraints improve generalization, but rather identifies specific mechanistic pathways through which certain interpretability constraints can act as regularizers that improve OOD performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Interpretability constraints that enforce alignment with causal structure will improve out-of-distribution generalization when the training data contains spurious correlations, even if they reduce in-distribution performance by preventing exploitation of those correlations.</li>
                <li>Interpretability constraints that enforce compositional structure will improve generalization to novel combinations of known components, with the benefit proportional to the degree of compositional structure in the true data-generating process.</li>
                <li>The net utility of a world model should be evaluated as: U_net = Σ_i w_i × P_i, where P_i is performance on distribution i, w_i is the deployment probability/importance of distribution i, and Σ_i w_i = 1. For real-world deployments, w_train < Σ_i≠train w_i, meaning OOD performance matters more than in-distribution performance.</li>
                <li>World models with interpretable representations will show smaller performance degradation under distribution shift compared to black-box models with equivalent in-distribution performance, with the magnitude of benefit increasing with the magnitude of distribution shift.</li>
                <li>The generalization benefit of interpretability is strongest when: (a) the interpretability constraints align with the true causal/compositional structure, (b) the distribution shift is substantial, and (c) the training data contains exploitable spurious correlations.</li>
                <li>There exists a class of tasks where interpretability constraints improve both in-distribution and out-of-distribution performance simultaneously by preventing overfitting to spurious correlations that harm even in-distribution generalization.</li>
                <li>Interpretability constraints that do not align with the underlying structure of the domain (e.g., imposing linear structure on inherently non-linear relationships) will harm both in-distribution and out-of-distribution performance.</li>
                <li>The interpretability-generalization benefit follows a specificity principle: constraints must match the actual structure of the domain to provide benefits; generic interpretability constraints may not help or may harm performance.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Research on causal representation learning demonstrates that models learning causal structures generalize better to distribution shifts than those learning purely correlational patterns, as causal relationships are invariant across distributions while correlations may change. </li>
    <li>Studies on compositional generalization show that models with explicit compositional structure (which are often more interpretable) can generalize to novel combinations of known components better than monolithic models that lack such structure. </li>
    <li>Research on shortcut learning demonstrates that neural networks often learn to exploit spurious correlations in training data, leading to poor generalization when these correlations don't hold in deployment, suggesting that constraints preventing such shortcuts could improve robustness. </li>
    <li>Work on adversarial robustness has found connections between model interpretability and robustness, with some evidence that interpretable features or architectures can be more robust to certain types of distribution shift. </li>
    <li>Research on inductive biases in deep learning shows that architectural and training constraints that encode domain knowledge can improve generalization, supporting the idea that interpretability constraints could serve a similar function. </li>
    <li>Studies on object-centric representations show that models with explicit object representations (a form of interpretable structure) can generalize better to novel scenes and object combinations than models without such structure. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In visual reasoning tasks with compositional distribution shift (e.g., CLEVR-CoGenT, novel object combinations), world models with explicit object-centric representations will outperform black-box models on OOD test sets, even if black-box models have 5-10% higher in-distribution accuracy.</li>
                <li>World models for robotic manipulation with interpretable physics representations (explicit object dynamics, contact models) will show 20-40% better transfer success rates to novel objects compared to end-to-end learned models when tested on objects with different physical properties than training.</li>
                <li>In natural language understanding tasks involving compositional generalization (e.g., SCAN, CFQ benchmarks), interpretable models with explicit compositional structure will outperform larger black-box models by 15-30% on systematicity test sets.</li>
                <li>When training data contains known spurious correlations (e.g., texture-shape conflicts in ImageNet), world models with interpretability constraints that prevent shortcut learning will show 10-20% better performance on distribution shifts that break those correlations.</li>
                <li>In continual learning scenarios with multiple sequential tasks, interpretable world models with modular structure will show 30-50% less catastrophic forgetting compared to monolithic models, as measured by average performance across all tasks after training.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If interpretability constraints are designed through automated methods to match the discovered causal structure of a domain (e.g., through causal discovery algorithms), they might eliminate the interpretability-performance trade-off entirely, achieving both maximal interpretability and optimal generalization simultaneously across all distribution shifts.</li>
                <li>World models trained with interpretability constraints from initialization might develop fundamentally different learning dynamics that prioritize robust, generalizable features over brittle, high-performing features, potentially leading to a phase transition where interpretable models outperform black-box models even in-distribution beyond a certain scale or training duration.</li>
                <li>In multi-task learning scenarios, interpretable world models with shared, structured representations might show superlinear scaling in performance with the number of tasks, as the interpretable structure enables better transfer, while black-box models show sublinear scaling due to interference.</li>
                <li>Interpretability constraints might interact with model scale in non-obvious ways: very large interpretable models might achieve both interpretability and performance by having sufficient capacity to represent complex causal structures explicitly, potentially outperforming black-box models of equivalent size.</li>
                <li>In active learning or interactive learning scenarios, interpretable world models might enable more efficient learning by allowing better human feedback and correction of model misconceptions, potentially requiring 10-100x less data to reach equivalent OOD performance compared to black-box models.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If interpretable world models with causal structure constraints consistently show worse out-of-distribution generalization than black-box models across multiple domains with natural distribution shifts (not just in-distribution performance), the core premise would be falsified.</li>
                <li>If the generalization benefit of interpretability is found only in artificial, narrow distribution shifts (e.g., specific benchmark datasets) but never appears in real-world deployment scenarios with natural distribution shifts, the practical relevance of the theory would be invalidated.</li>
                <li>If interpretability constraints that explicitly do not align with causal structure (e.g., random structural constraints) provide the same generalization benefits as causally-aligned constraints, the mechanistic explanation of the theory would be falsified.</li>
                <li>If increasing the magnitude of distribution shift does not increase the relative generalization advantage of interpretable models (or decreases it), the theory's prediction about the relationship between shift magnitude and interpretability benefit would be falsified.</li>
                <li>If removing spurious correlations from training data eliminates any generalization advantage of interpretable models, while the theory predicts benefits should persist due to better alignment with true structure, this would challenge the theory's completeness.</li>
                <li>If interpretable models show equal or greater catastrophic forgetting in continual learning compared to black-box models, the theory's prediction about modularity and interference would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not provide a complete taxonomy of which specific types of interpretability constraints provide generalization benefits and which do not, making it difficult to predict a priori whether a given interpretability approach will help. </li>
    <li>The quantitative relationship between the magnitude of distribution shift and the generalization benefit of interpretability is not specified, making it unclear at what level of shift the benefits become significant. </li>
    <li>The theory does not address how to design or discover interpretability constraints that align with causal structure when that structure is unknown a priori, which is the common case in practice. </li>
    <li>The interaction between interpretability constraints and model capacity/scale is not fully specified - it's unclear whether interpretability benefits increase, decrease, or remain constant as models scale. </li>
    <li>The theory does not account for computational costs: interpretability constraints may require more computation during training or inference, and the theory doesn't specify when the generalization benefits justify these costs. </li>
    <li>The theory does not explain how to measure or quantify the degree of alignment between interpretability constraints and true domain structure, making it difficult to predict when benefits will occur. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Schölkopf et al. (2021) Toward Causal Representation Learning [Discusses causal representations and their generalization benefits but does not explicitly frame this as an interpretability-performance trade-off or propose interpretability constraints as a path to better generalization]</li>
    <li>Goyal & Bengio (2022) Inductive Biases for Deep Learning of Higher-Level Cognition [Discusses inductive biases for generalization including causal and compositional biases, but does not explicitly connect these to interpretability or frame them as resolving interpretability-performance trade-offs]</li>
    <li>Geirhos et al. (2020) Shortcut Learning in Deep Neural Networks [Discusses how models learn shortcuts and spurious correlations, but does not propose interpretability constraints as a solution or connect to interpretability-performance trade-offs]</li>
    <li>Lipton (2018) The Mythos of Model Interpretability [Discusses interpretability concepts but does not propose that interpretability could improve generalization or challenge the interpretability-performance trade-off]</li>
    <li>Rudin (2019) Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead [Argues for interpretable models but primarily on the basis of trustworthiness and debuggability, not generalization performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Interpretability-Induced Generalization Theory",
    "theory_description": "This theory challenges the conventional view of interpretability-performance trade-offs by proposing that certain types of interpretability constraints can improve generalization performance, particularly in out-of-distribution (OOD) scenarios. The theory posits that interpretability constraints that enforce alignment with causal structure, compositional principles, or modular representations act as beneficial inductive biases. These constraints encourage world models to learn representations that capture the underlying generative structure of the environment rather than superficial statistical patterns. While such constraints may reduce in-distribution performance by preventing overfitting to dataset-specific correlations, they can improve robustness to distribution shift. The theory specifically predicts that the generalization benefit is strongest when: (1) interpretability constraints align with the true causal or compositional structure of the domain, (2) the deployment environment involves significant distribution shift from training, and (3) the training data contains spurious correlations that black-box models would exploit. The theory does not claim all interpretability constraints improve generalization, but rather identifies specific mechanistic pathways through which certain interpretability constraints can act as regularizers that improve OOD performance.",
    "supporting_evidence": [
        {
            "text": "Research on causal representation learning demonstrates that models learning causal structures generalize better to distribution shifts than those learning purely correlational patterns, as causal relationships are invariant across distributions while correlations may change.",
            "citations": [
                "Schölkopf et al. (2021) Toward Causal Representation Learning",
                "Peters et al. (2017) Elements of Causal Inference: Foundations and Learning Algorithms"
            ]
        },
        {
            "text": "Studies on compositional generalization show that models with explicit compositional structure (which are often more interpretable) can generalize to novel combinations of known components better than monolithic models that lack such structure.",
            "citations": [
                "Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"
            ]
        },
        {
            "text": "Research on shortcut learning demonstrates that neural networks often learn to exploit spurious correlations in training data, leading to poor generalization when these correlations don't hold in deployment, suggesting that constraints preventing such shortcuts could improve robustness.",
            "citations": [
                "Geirhos et al. (2020) Shortcut Learning in Deep Neural Networks"
            ]
        },
        {
            "text": "Work on adversarial robustness has found connections between model interpretability and robustness, with some evidence that interpretable features or architectures can be more robust to certain types of distribution shift.",
            "citations": [
                "Tsipras et al. (2019) Robustness May Be at Odds with Accuracy"
            ]
        },
        {
            "text": "Research on inductive biases in deep learning shows that architectural and training constraints that encode domain knowledge can improve generalization, supporting the idea that interpretability constraints could serve a similar function.",
            "citations": [
                "Goyal & Bengio (2022) Inductive Biases for Deep Learning of Higher-Level Cognition"
            ]
        },
        {
            "text": "Studies on object-centric representations show that models with explicit object representations (a form of interpretable structure) can generalize better to novel scenes and object combinations than models without such structure.",
            "citations": [
                "Greff et al. (2020) Object-Centric Learning with Slot Attention"
            ]
        }
    ],
    "theory_statements": [
        "Interpretability constraints that enforce alignment with causal structure will improve out-of-distribution generalization when the training data contains spurious correlations, even if they reduce in-distribution performance by preventing exploitation of those correlations.",
        "Interpretability constraints that enforce compositional structure will improve generalization to novel combinations of known components, with the benefit proportional to the degree of compositional structure in the true data-generating process.",
        "The net utility of a world model should be evaluated as: U_net = Σ_i w_i × P_i, where P_i is performance on distribution i, w_i is the deployment probability/importance of distribution i, and Σ_i w_i = 1. For real-world deployments, w_train &lt; Σ_i≠train w_i, meaning OOD performance matters more than in-distribution performance.",
        "World models with interpretable representations will show smaller performance degradation under distribution shift compared to black-box models with equivalent in-distribution performance, with the magnitude of benefit increasing with the magnitude of distribution shift.",
        "The generalization benefit of interpretability is strongest when: (a) the interpretability constraints align with the true causal/compositional structure, (b) the distribution shift is substantial, and (c) the training data contains exploitable spurious correlations.",
        "There exists a class of tasks where interpretability constraints improve both in-distribution and out-of-distribution performance simultaneously by preventing overfitting to spurious correlations that harm even in-distribution generalization.",
        "Interpretability constraints that do not align with the underlying structure of the domain (e.g., imposing linear structure on inherently non-linear relationships) will harm both in-distribution and out-of-distribution performance.",
        "The interpretability-generalization benefit follows a specificity principle: constraints must match the actual structure of the domain to provide benefits; generic interpretability constraints may not help or may harm performance."
    ],
    "new_predictions_likely": [
        "In visual reasoning tasks with compositional distribution shift (e.g., CLEVR-CoGenT, novel object combinations), world models with explicit object-centric representations will outperform black-box models on OOD test sets, even if black-box models have 5-10% higher in-distribution accuracy.",
        "World models for robotic manipulation with interpretable physics representations (explicit object dynamics, contact models) will show 20-40% better transfer success rates to novel objects compared to end-to-end learned models when tested on objects with different physical properties than training.",
        "In natural language understanding tasks involving compositional generalization (e.g., SCAN, CFQ benchmarks), interpretable models with explicit compositional structure will outperform larger black-box models by 15-30% on systematicity test sets.",
        "When training data contains known spurious correlations (e.g., texture-shape conflicts in ImageNet), world models with interpretability constraints that prevent shortcut learning will show 10-20% better performance on distribution shifts that break those correlations.",
        "In continual learning scenarios with multiple sequential tasks, interpretable world models with modular structure will show 30-50% less catastrophic forgetting compared to monolithic models, as measured by average performance across all tasks after training."
    ],
    "new_predictions_unknown": [
        "If interpretability constraints are designed through automated methods to match the discovered causal structure of a domain (e.g., through causal discovery algorithms), they might eliminate the interpretability-performance trade-off entirely, achieving both maximal interpretability and optimal generalization simultaneously across all distribution shifts.",
        "World models trained with interpretability constraints from initialization might develop fundamentally different learning dynamics that prioritize robust, generalizable features over brittle, high-performing features, potentially leading to a phase transition where interpretable models outperform black-box models even in-distribution beyond a certain scale or training duration.",
        "In multi-task learning scenarios, interpretable world models with shared, structured representations might show superlinear scaling in performance with the number of tasks, as the interpretable structure enables better transfer, while black-box models show sublinear scaling due to interference.",
        "Interpretability constraints might interact with model scale in non-obvious ways: very large interpretable models might achieve both interpretability and performance by having sufficient capacity to represent complex causal structures explicitly, potentially outperforming black-box models of equivalent size.",
        "In active learning or interactive learning scenarios, interpretable world models might enable more efficient learning by allowing better human feedback and correction of model misconceptions, potentially requiring 10-100x less data to reach equivalent OOD performance compared to black-box models."
    ],
    "negative_experiments": [
        "If interpretable world models with causal structure constraints consistently show worse out-of-distribution generalization than black-box models across multiple domains with natural distribution shifts (not just in-distribution performance), the core premise would be falsified.",
        "If the generalization benefit of interpretability is found only in artificial, narrow distribution shifts (e.g., specific benchmark datasets) but never appears in real-world deployment scenarios with natural distribution shifts, the practical relevance of the theory would be invalidated.",
        "If interpretability constraints that explicitly do not align with causal structure (e.g., random structural constraints) provide the same generalization benefits as causally-aligned constraints, the mechanistic explanation of the theory would be falsified.",
        "If increasing the magnitude of distribution shift does not increase the relative generalization advantage of interpretable models (or decreases it), the theory's prediction about the relationship between shift magnitude and interpretability benefit would be falsified.",
        "If removing spurious correlations from training data eliminates any generalization advantage of interpretable models, while the theory predicts benefits should persist due to better alignment with true structure, this would challenge the theory's completeness.",
        "If interpretable models show equal or greater catastrophic forgetting in continual learning compared to black-box models, the theory's prediction about modularity and interference would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not provide a complete taxonomy of which specific types of interpretability constraints provide generalization benefits and which do not, making it difficult to predict a priori whether a given interpretability approach will help.",
            "citations": []
        },
        {
            "text": "The quantitative relationship between the magnitude of distribution shift and the generalization benefit of interpretability is not specified, making it unclear at what level of shift the benefits become significant.",
            "citations": []
        },
        {
            "text": "The theory does not address how to design or discover interpretability constraints that align with causal structure when that structure is unknown a priori, which is the common case in practice.",
            "citations": []
        },
        {
            "text": "The interaction between interpretability constraints and model capacity/scale is not fully specified - it's unclear whether interpretability benefits increase, decrease, or remain constant as models scale.",
            "citations": []
        },
        {
            "text": "The theory does not account for computational costs: interpretability constraints may require more computation during training or inference, and the theory doesn't specify when the generalization benefits justify these costs.",
            "citations": []
        },
        {
            "text": "The theory does not explain how to measure or quantify the degree of alignment between interpretability constraints and true domain structure, making it difficult to predict when benefits will occur.",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Large-scale studies on neural scaling laws have found that simply increasing model size improves both in-distribution and out-of-distribution performance across many benchmarks, suggesting that scale alone (without interpretability) may be sufficient for generalization.",
            "citations": [
                "Kaplan et al. (2020) Scaling Laws for Neural Language Models",
                "Henighan et al. (2020) Scaling Laws for Autoregressive Generative Modeling"
            ]
        },
        {
            "text": "Research on neural scaling and overparameterization has shown that very large, overparameterized models can generalize well despite being highly uninterpretable, challenging the necessity of interpretability for generalization.",
            "citations": [
                "Nakkiran et al. (2021) Deep Double Descent: Where Bigger Models and More Data Hurt"
            ]
        },
        {
            "text": "Some studies on foundation models show that large pre-trained models exhibit strong zero-shot and few-shot generalization to novel tasks and distributions without explicit interpretability constraints, suggesting alternative paths to robust generalization.",
            "citations": [
                "Brown et al. (2020) Language Models are Few-Shot Learners"
            ]
        },
        {
            "text": "Research on neural network lottery tickets and pruning suggests that the important features for generalization may be found through scale and then pruned, rather than through interpretability constraints during training.",
            "citations": [
                "Frankle & Carbin (2019) The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
            ]
        }
    ],
    "special_cases": [
        "In domains where the training distribution is comprehensive and representative of deployment (minimal distribution shift), the generalization benefit of interpretability may not outweigh in-distribution performance costs, making black-box models preferable.",
        "For tasks where spurious correlations in training data are actually reliable and stable in deployment (e.g., highly controlled laboratory or industrial settings), interpretability constraints that prevent exploiting these correlations may provide no benefit and only reduce performance.",
        "In domains with extremely high-dimensional, complex causal structures that don't align with human intuitions or available interpretability frameworks, interpretability constraints may harm rather than help generalization by imposing incorrect structure.",
        "The generalization benefit may only appear beyond a threshold magnitude of distribution shift (e.g., &gt;20% change in key statistics), making it difficult to observe in standard benchmarks with mild distribution shifts.",
        "For very small models with limited capacity, interpretability constraints may over-constrain the model and harm both in-distribution and out-of-distribution performance by preventing the model from learning sufficient complexity.",
        "In domains where the true data-generating process is fundamentally non-compositional and non-causal (e.g., purely random or chaotic systems), interpretability constraints based on causal/compositional assumptions will provide no generalization benefit.",
        "When deployment involves distribution shifts orthogonal to the structure captured by interpretability constraints (e.g., causal structure is preserved but data quality degrades), interpretability may provide no advantage.",
        "In multi-modal or multi-task settings, interpretability constraints designed for one modality or task may harm performance on others, requiring careful design of shared interpretable structure."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Schölkopf et al. (2021) Toward Causal Representation Learning [Discusses causal representations and their generalization benefits but does not explicitly frame this as an interpretability-performance trade-off or propose interpretability constraints as a path to better generalization]",
            "Goyal & Bengio (2022) Inductive Biases for Deep Learning of Higher-Level Cognition [Discusses inductive biases for generalization including causal and compositional biases, but does not explicitly connect these to interpretability or frame them as resolving interpretability-performance trade-offs]",
            "Geirhos et al. (2020) Shortcut Learning in Deep Neural Networks [Discusses how models learn shortcuts and spurious correlations, but does not propose interpretability constraints as a solution or connect to interpretability-performance trade-offs]",
            "Lipton (2018) The Mythos of Model Interpretability [Discusses interpretability concepts but does not propose that interpretability could improve generalization or challenge the interpretability-performance trade-off]",
            "Rudin (2019) Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead [Argues for interpretable models but primarily on the basis of trustworthiness and debuggability, not generalization performance]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "theory_query": "Build a theory about what constitutes an optimal world model for AI systems, incorporating fidelity, interpretability, computational efficiency, and task-specific utility.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-150",
    "original_theory_name": "Interpretability-Performance Trade-off in World Models",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
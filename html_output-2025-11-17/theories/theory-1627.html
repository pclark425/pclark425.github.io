<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory of Information Bottleneck in LLM Scientific Simulation via Prompt and Demonstration Design - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1627</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1627</p>
                <p><strong>Name:</strong> Theory of Information Bottleneck in LLM Scientific Simulation via Prompt and Demonstration Design</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory proposes that the information content and granularity encoded in prompts and demonstrations act as an information bottleneck, constraining the maximum achievable accuracy of LLM-based scientific simulation. The theory asserts that unless the prompt/demonstration encodes all necessary domain-relevant variables, relationships, and procedural steps, the LLM cannot reconstruct or simulate the full scientific process, regardless of its underlying capacity.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt Information Sufficiency Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt/demonstration &#8594; omits &#8594; critical domain-relevant variables or steps</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM scientific simulation &#8594; cannot_achieve &#8594; full domain-specific accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs fail to solve scientific problems when prompts omit key variables, assumptions, or procedural steps. </li>
    <li>Studies show that including all relevant information in prompts is necessary for accurate LLM simulation in complex domains. </li>
    <li>Prompt ablation experiments reveal sharp drops in accuracy when critical information is removed. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law extends prompt engineering concepts to a formal information-theoretic bottleneck in scientific simulation.</p>            <p><strong>What Already Exists:</strong> Prompt completeness is known to affect LLM performance, but the explicit information bottleneck framing is novel.</p>            <p><strong>What is Novel:</strong> The law formalizes the information bottleneck as a hard constraint on simulation accuracy.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt completeness affects reasoning]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt information content is critical]</li>
</ul>
            <h3>Statement 1: Granularity-Accuracy Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt/demonstration &#8594; has_granularity &#8594; below domain-specific threshold</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM scientific simulation &#8594; exhibits &#8594; systematic under-specification errors</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs make systematic errors when prompts are too high-level or lack sufficient detail for the scientific subdomain. </li>
    <li>Increasing prompt granularity (e.g., more detailed steps, explicit variable definitions) improves simulation accuracy. </li>
    <li>Empirical evidence shows that under-specified prompts lead to ambiguous or incorrect LLM outputs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law formalizes prompt granularity as a quantifiable threshold for simulation accuracy.</p>            <p><strong>What Already Exists:</strong> Prompt detail is known to affect LLM output, but the explicit granularity threshold for scientific simulation is new.</p>            <p><strong>What is Novel:</strong> The law introduces a domain-specific granularity threshold as a requirement for accurate simulation.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt detail improves reasoning]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt granularity affects performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If prompts are systematically ablated to remove key variables or steps, LLM simulation accuracy will decrease in a predictable manner.</li>
                <li>Increasing prompt granularity up to a domain-specific threshold will yield stepwise improvements in simulation fidelity.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained on highly compressed or abstracted prompts, they may develop emergent abilities to infer missing information.</li>
                <li>Future LLMs with advanced reasoning may partially overcome the information bottleneck by leveraging world knowledge or context.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs achieve high accuracy with under-specified or low-granularity prompts, the information bottleneck claim would be falsified.</li>
                <li>If omitting critical variables does not reduce simulation accuracy, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs infer missing information from context or prior knowledge, partially bypassing the information bottleneck. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory synthesizes prompt information content as a formal bottleneck, not just a performance enhancer, in scientific simulation.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt completeness and granularity affect reasoning]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt information content is critical]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Theory of Information Bottleneck in LLM Scientific Simulation via Prompt and Demonstration Design",
    "theory_description": "This theory proposes that the information content and granularity encoded in prompts and demonstrations act as an information bottleneck, constraining the maximum achievable accuracy of LLM-based scientific simulation. The theory asserts that unless the prompt/demonstration encodes all necessary domain-relevant variables, relationships, and procedural steps, the LLM cannot reconstruct or simulate the full scientific process, regardless of its underlying capacity.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt Information Sufficiency Law",
                "if": [
                    {
                        "subject": "prompt/demonstration",
                        "relation": "omits",
                        "object": "critical domain-relevant variables or steps"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM scientific simulation",
                        "relation": "cannot_achieve",
                        "object": "full domain-specific accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs fail to solve scientific problems when prompts omit key variables, assumptions, or procedural steps.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that including all relevant information in prompts is necessary for accurate LLM simulation in complex domains.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt ablation experiments reveal sharp drops in accuracy when critical information is removed.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt completeness is known to affect LLM performance, but the explicit information bottleneck framing is novel.",
                    "what_is_novel": "The law formalizes the information bottleneck as a hard constraint on simulation accuracy.",
                    "classification_explanation": "The law extends prompt engineering concepts to a formal information-theoretic bottleneck in scientific simulation.",
                    "likely_classification": "new",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt completeness affects reasoning]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt information content is critical]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Granularity-Accuracy Law",
                "if": [
                    {
                        "subject": "prompt/demonstration",
                        "relation": "has_granularity",
                        "object": "below domain-specific threshold"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM scientific simulation",
                        "relation": "exhibits",
                        "object": "systematic under-specification errors"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs make systematic errors when prompts are too high-level or lack sufficient detail for the scientific subdomain.",
                        "uuids": []
                    },
                    {
                        "text": "Increasing prompt granularity (e.g., more detailed steps, explicit variable definitions) improves simulation accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical evidence shows that under-specified prompts lead to ambiguous or incorrect LLM outputs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt detail is known to affect LLM output, but the explicit granularity threshold for scientific simulation is new.",
                    "what_is_novel": "The law introduces a domain-specific granularity threshold as a requirement for accurate simulation.",
                    "classification_explanation": "The law formalizes prompt granularity as a quantifiable threshold for simulation accuracy.",
                    "likely_classification": "new",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt detail improves reasoning]",
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt granularity affects performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If prompts are systematically ablated to remove key variables or steps, LLM simulation accuracy will decrease in a predictable manner.",
        "Increasing prompt granularity up to a domain-specific threshold will yield stepwise improvements in simulation fidelity."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained on highly compressed or abstracted prompts, they may develop emergent abilities to infer missing information.",
        "Future LLMs with advanced reasoning may partially overcome the information bottleneck by leveraging world knowledge or context."
    ],
    "negative_experiments": [
        "If LLMs achieve high accuracy with under-specified or low-granularity prompts, the information bottleneck claim would be falsified.",
        "If omitting critical variables does not reduce simulation accuracy, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs infer missing information from context or prior knowledge, partially bypassing the information bottleneck.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show surprising robustness to missing information in familiar domains, suggesting pretraining or world knowledge can sometimes compensate.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In domains with highly redundant or self-evident information, the information bottleneck may be less severe.",
        "For LLMs with extensive domain-specific pretraining, the need for explicit prompt information may be reduced."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt completeness and detail are known to affect LLM performance, but the explicit information bottleneck framing is new.",
        "what_is_novel": "The theory formalizes prompt/demonstration information content as a hard constraint on simulation accuracy.",
        "classification_explanation": "The theory synthesizes prompt information content as a formal bottleneck, not just a performance enhancer, in scientific simulation.",
        "likely_classification": "new",
        "references": [
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt completeness and granularity affect reasoning]",
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt information content is critical]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-635",
    "original_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
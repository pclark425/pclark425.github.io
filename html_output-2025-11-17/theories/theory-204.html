<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Context-Evidence Sufficiency Theory for Software Artifact Evaluation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-204</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-204</p>
                <p><strong>Name:</strong> Context-Evidence Sufficiency Theory for Software Artifact Evaluation</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about alignment between proxy evaluations (LLM-as-a-judge, Likert-style) and expert human review for software development artifacts, including necessary conditions for high agreement, based on the following results.</p>
                <p><strong>Description:</strong> Alignment between automated evaluators (LLM-as-judge or automated metrics) and human expert judgments for software artifacts depends critically on the availability and quality of contextual evidence. The theory posits that different evaluation tasks require different types and amounts of evidence, and that evidence requirements scale with artifact complexity and evaluation criteria specificity. Evidence types include: (1) Artifact-only (code/output alone), (2) Specification evidence (requirements, docstrings, prompts), (3) Reference evidence (example solutions, ground truth), (4) Execution evidence (test results, runtime behavior, traces), and (5) Workspace evidence (related files, dependencies, project structure). The theory predicts that: (a) Evidence requirements are task-dependent - simple, objective tasks (e.g., formatting) require minimal evidence while complex, semantic tasks (e.g., correctness in context) require rich evidence; (b) Evidence quality matters more than quantity - low-quality or irrelevant evidence can decrease alignment; (c) Intelligent evidence selection (targeted retrieval) outperforms exhaustive provision when context is large; (d) Different evaluation criteria have different evidence sensitivities - functional correctness benefits most from execution evidence, while semantic similarity benefits from reference evidence; (e) Evidence requirements scale non-linearly with artifact complexity - multi-file systems require disproportionately more evidence than single functions.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>For single-function code evaluation with objective criteria (e.g., formatting, simple correctness), artifact + specification evidence is sufficient to achieve 80%+ alignment with human judgments</li>
                <li>For multi-file system evaluation with complex dependencies, workspace evidence (file structure, dependencies, execution traces) is necessary to achieve 80%+ alignment</li>
                <li>Intelligent evidence selection (targeted retrieval using locate/read operations) outperforms exhaustive context provision by 15-30 percentage points when workspace size exceeds model context capacity</li>
                <li>Evidence quality (relevance, accuracy, clarity) is more important than evidence quantity - low-quality references or irrelevant context can decrease alignment by 5-15 percentage points compared to no additional evidence</li>
                <li>Different evaluation criteria have different evidence sensitivities: functional correctness benefits most from execution evidence (20-30 point improvement), semantic similarity benefits most from reference evidence (15-25 point improvement), and style/formatting evaluation requires minimal evidence</li>
                <li>Evidence requirements scale non-linearly with artifact complexity: doubling artifact size/complexity requires approximately 1.5-2x more evidence to maintain alignment, with diminishing returns beyond a sufficiency threshold</li>
                <li>For tasks with unique or highly constrained correct solutions (e.g., bug fixes, code refinement), minimal evidence (artifact alone) can achieve very high alignment (r>0.95) with human judgments</li>
                <li>Noisy, irrelevant, or contradictory evidence actively harms alignment by 5-15 percentage points compared to no additional evidence, particularly when evidence selection is poor</li>
                <li>Human evaluators and LLM judges have similar evidence requirements for most tasks - tasks where humans struggle due to insufficient context (e.g., repository history for warning triage) also show low LLM-human alignment</li>
                <li>Combining complementary evidence types (e.g., execution + similarity, or AST + data-flow) yields super-additive improvements in alignment compared to single evidence types</li>
                <li>Evidence transparency and inspectability affects perceived alignment and trust - evaluators prefer evidence they can verify (e.g., code-based assertions) over opaque evidence (e.g., LLM judgments)</li>
                <li>Task-specific evidence is more valuable than generic evidence - providing domain-specific context (e.g., project style guides, threat models) improves alignment more than generic documentation</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Agent-as-a-Judge with read and locate modules (intelligent evidence selection) achieves ~90% alignment with human consensus, while LLM-as-a-Judge without these modules achieves only ~60-70% alignment, demonstrating the value of targeted evidence access <a href="../results/extraction-result-1713.html#e1713.0" class="evidence-link">[e1713.0]</a> </li>
    <li>Gray-box evaluation with execution trajectories substantially outperforms black-box evaluation without trajectories, showing execution evidence is critical for agent evaluation <a href="../results/extraction-result-1713.html#e1713.0" class="evidence-link">[e1713.0]</a> </li>
    <li>Ablation studies show that read and locate modules are the largest contributors to alignment in Agent-as-a-Judge, with alignment dropping significantly when these evidence-access modules are removed <a href="../results/extraction-result-1713.html#e1713.0" class="evidence-link">[e1713.0]</a> </li>
    <li>Reference-enhanced ICE-Score does not reliably improve over reference-free evaluation when references are low quality, demonstrating that evidence quality matters more than presence <a href="../results/extraction-result-1717.html#e1717.0" class="evidence-link">[e1717.0]</a> </li>
    <li>CodeBLEU's AST and data-flow components (syntactic and semantic evidence) show very high correlations with human judgments (often ≥0.97), substantially outperforming token-only metrics <a href="../results/extraction-result-1789.html#e1789.0" class="evidence-link">[e1789.0]</a> </li>
    <li>Providing both function bodies for semantic similarity evaluation yields high human-model agreement (alpha 0.77-0.83), while missing context reduces agreement <a href="../results/extraction-result-1849.html#e1849.3" class="evidence-link">[e1849.3]</a> </li>
    <li>Static analysis warning triage shows very low LLM-human agreement (alpha 0.15) despite clear labels, attributed to insufficient repository context and change history <a href="../results/extraction-result-1849.html#e1849.4" class="evidence-link">[e1849.4]</a> </li>
    <li>Combined metric (PASS + EdIT-SIM) achieves higher correlation with human value judgments (r=0.70) than either functional correctness or similarity alone, showing complementary evidence types improve alignment <a href="../results/extraction-result-1801.html#e1801.3" class="evidence-link">[e1801.3]</a> </li>
    <li>PASS (functional correctness via execution) shows strong correlation with human judgments (r≈0.62-0.66) but misses 42% of valuable failing generations, indicating execution evidence alone is insufficient <a href="../results/extraction-result-1801.html#e1801.0" class="evidence-link">[e1801.0]</a> </li>
    <li>Multi-turn peer battles reveal deeper model capabilities than single-turn evaluation by providing richer interaction context, improving discrimination on complex tasks <a href="../results/extraction-result-1716.html#e1716.0" class="evidence-link">[e1716.0]</a> </li>
    <li>Search/retrieval modules can hurt performance in small/simple workspaces but are expected to help in larger ones, showing evidence selection must be context-appropriate <a href="../results/extraction-result-1713.html#e1713.0" class="evidence-link">[e1713.0]</a> </li>
    <li>Reference-guided prompting for math/reasoning substantially improved GPT-4 grading accuracy (failure rate from 70% to 15%), demonstrating task-specific evidence requirements <a href="../results/extraction-result-1718.html#e1718.0" class="evidence-link">[e1718.0]</a> </li>
    <li>Including natural language context when encoding code increased Kendall's Tau correlation with human preference (example: from 0.50 to 0.52), showing specification evidence improves alignment <a href="../results/extraction-result-1796.html#e1796.0" class="evidence-link">[e1796.0]</a> </li>
    <li>GEMBA referenceless (QE) zero-shot judgments achieve high system-level agreement (87.6% accuracy) with MQM human judgments, showing that for some tasks, artifact + specification evidence is sufficient without references <a href="../results/extraction-result-1815.html#e1815.0" class="evidence-link">[e1815.0]</a> </li>
    <li>Exact-match accuracy achieves extremely high correlation (r=0.999) with human judgments for code refinement tasks with unique ground-truth, showing minimal evidence suffices when solutions are constrained <a href="../results/extraction-result-1789.html#e1789.2" class="evidence-link">[e1789.2]</a> </li>
    <li>CodeBERTScore with language-specific pretraining and appropriate layer selection shows higher correlation with execution-based correctness than baseline metrics, demonstrating that semantic evidence (embeddings) can partially substitute for execution evidence <a href="../results/extraction-result-1796.html#e1796.1" class="evidence-link">[e1796.1]</a> </li>
    <li>Providing explicit aspect definitions in prompts (criteria clarity) significantly improves Spearman correlations across tasks and models in GPTScore evaluation <a href="../results/extraction-result-1805.html#e1805.0" class="evidence-link">[e1805.0]</a> </li>
    <li>TALEC with criteria-division and repetition of evaluation criteria descriptions improved alignment, showing that making evidence/criteria more explicit helps <a href="../results/extraction-result-1710.html#e1710.0" class="evidence-link">[e1710.0]</a> </li>
    <li>Memory and planning modules in Agent-as-a-Judge introduced error chains and were detrimental, suggesting that not all evidence types are beneficial and some can introduce noise <a href="../results/extraction-result-1713.html#e1713.0" class="evidence-link">[e1713.0]</a> </li>
    <li>Oracle execution-based test evaluation can be gamed by models producing trivial/executable-but-meaningless tests, showing execution evidence alone without semantic understanding is insufficient <a href="../results/extraction-result-1695.html#e1695.1" class="evidence-link">[e1695.1]</a> </li>
    <li>Perspective API toxicity scores diverge from human judgments depending on prompt context (respectful instruction vs. no instruction), showing evidence context affects automated evaluation reliability <a href="../results/extraction-result-1840.html#e1840.1" class="evidence-link">[e1840.1]</a> </li>
    <li>Human evaluators grading EvalGen assertions preferred code-based assertions they could inspect over LLM-based assertions, indicating transparency of evidence/evaluation logic affects trust and perceived alignment <a href="../results/extraction-result-1837.html#e1837.2" class="evidence-link">[e1837.2]</a> </li>
    <li>Criteria drift (users changing criteria after grading examples) was pervasive in EvalGen user study, showing that evidence (graded examples) helps users refine evaluation criteria <a href="../results/extraction-result-1837.html#e1837.2" class="evidence-link">[e1837.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For a code review task on a 500+ line change spanning multiple files, providing execution traces and test results will improve LLM-human agreement by 20-30 percentage points compared to code-only evaluation</li>
                <li>For API design evaluation, providing usage examples and integration tests will improve agreement by 25-35 percentage points compared to API signature and documentation alone</li>
                <li>For bug localization in a large codebase, providing stack traces, error messages, and related file context will improve agreement by 30-40 percentage points compared to buggy code snippet alone</li>
                <li>Implementing a learned evidence selection module (trained to predict which files/context are most relevant for a given evaluation task) will outperform heuristic selection by 8-15 percentage points</li>
                <li>For repository-scale evaluation, providing a dependency graph and call graph will improve agreement by 15-25 percentage points compared to flat file access without relationship information</li>
                <li>For security evaluation, providing threat model documentation and known vulnerability patterns will improve agreement by 20-30 percentage points compared to code-only analysis</li>
                <li>For performance evaluation, providing profiling data and benchmark results will improve agreement by 25-35 percentage points compared to static code analysis alone</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a universal evidence sufficiency threshold (e.g., 'enough to reconstruct developer intent') that applies across all artifact types and evaluation criteria, or whether thresholds are fundamentally task-dependent</li>
                <li>Whether LLM judges can learn to request specific evidence types adaptively based on the evaluation task and their current uncertainty, and if so, whether this active evidence gathering improves efficiency without sacrificing alignment</li>
                <li>Whether providing evidence beyond model context limits causes catastrophic degradation (due to attention dilution) or graceful performance decline (due to information loss), and at what threshold this occurs</li>
                <li>Whether evidence requirements differ systematically between different evaluation criteria dimensions (e.g., correctness vs. maintainability vs. security vs. performance) in predictable ways that could inform evidence selection strategies</li>
                <li>Whether synthetic evidence (e.g., LLM-generated explanations, documentation, or test cases) can substitute for real evidence without reducing alignment, and under what conditions synthetic evidence is beneficial vs. harmful</li>
                <li>Whether evidence presentation format (e.g., structured vs. unstructured, visual vs. textual) significantly affects alignment independent of information content</li>
                <li>Whether there are fundamental limits to evidence-based alignment improvement - i.e., whether some evaluation tasks have inherent ambiguity that no amount of evidence can resolve</li>
                <li>Whether evidence requirements for LLM judges decrease as model capabilities improve (i.e., whether stronger models can infer missing context better), or whether evidence requirements remain constant across model generations</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding a complex multi-file system evaluation task where artifact-only evaluation consistently achieves >85% alignment with human experts would challenge the evidence hierarchy and scaling assumptions</li>
                <li>Finding that providing execution traces and test results decreases alignment for debugging or correctness evaluation tasks would challenge the assumption that execution evidence is valuable for functional evaluation</li>
                <li>Finding that intelligent evidence selection (using locate/read modules) consistently underperforms random or exhaustive selection across multiple tasks would challenge the noise-reduction and targeted-retrieval hypotheses</li>
                <li>Demonstrating that evidence requirements do not scale with artifact complexity (e.g., that 10-file systems require the same evidence as single-file systems) would challenge the complexity-scaling relationship</li>
                <li>Finding that LLM judges and human evaluators have fundamentally different evidence requirements (e.g., humans need X but LLMs need Y, where X≠Y) would challenge the parallel-processing assumption</li>
                <li>Finding that low-quality or irrelevant evidence consistently improves alignment rather than harming it would challenge the evidence-quality-matters principle</li>
                <li>Demonstrating that combining complementary evidence types (e.g., execution + similarity) yields sub-additive or no improvement compared to single evidence types would challenge the complementarity assumption</li>
                <li>Finding that evidence transparency and inspectability do not affect perceived alignment or trust would challenge assumptions about human evaluation of automated judges</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to quantify artifact complexity in a way that predicts evidence requirements - what metrics or features of artifacts determine their evidence needs? </li>
    <li>The optimal evidence selection strategy for different artifact types and evaluation criteria is not fully characterized - what heuristics or learned models work best for which tasks? </li>
    <li>The theory does not address how evidence requirements change with evaluator expertise level - do expert evaluators need less evidence than novices, and does this apply to LLM judges? </li>
    <li>The interaction between evidence type and evaluation criteria is not fully specified - which evidence types are most valuable for which criteria, and are there systematic patterns? </li>
    <li>The theory does not explain why some tasks (e.g., code refinement with unique solutions) achieve very high alignment with minimal evidence while others (e.g., open-ended generation) require rich evidence </li>
    <li>The theory does not address how evidence requirements change over time or with repeated evaluation - do evaluators learn to need less evidence, or do requirements remain constant? </li>
    <li>The theory does not specify how to handle conflicting evidence (e.g., when execution suggests correctness but semantic analysis suggests incorrectness) - what resolution strategies work best? </li>
    <li>The theory does not explain the mechanisms by which evidence improves alignment - is it through reducing ambiguity, providing grounding, enabling verification, or other mechanisms? </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Tian et al. (2024) Agent-as-a-Judge: Evaluate Agents with Agents [Demonstrates importance of evidence access modules (read, locate) but does not propose comprehensive evidence sufficiency theory]</li>
    <li>Ren et al. (2023) CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code [Shows value of semantic context and language-specific pretraining but focuses on metric design rather than evidence theory]</li>
    <li>Zhong et al. (2023) ICE-Score: Instructing Large Language Models to Evaluate Code [Explores reference-free vs. reference-enhanced evaluation but does not develop general evidence theory]</li>
    <li>Ren et al. (2020) CodeBLEU: a Method for Automatic Evaluation of Code Synthesis [Demonstrates value of syntactic (AST) and semantic (data-flow) evidence but focuses on metric composition]</li>
    <li>Weyssow et al. (2024) Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences [Explores human-in-the-loop evidence gathering but focuses on assertion selection rather than evidence theory]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Context-Evidence Sufficiency Theory for Software Artifact Evaluation",
    "theory_description": "Alignment between automated evaluators (LLM-as-judge or automated metrics) and human expert judgments for software artifacts depends critically on the availability and quality of contextual evidence. The theory posits that different evaluation tasks require different types and amounts of evidence, and that evidence requirements scale with artifact complexity and evaluation criteria specificity. Evidence types include: (1) Artifact-only (code/output alone), (2) Specification evidence (requirements, docstrings, prompts), (3) Reference evidence (example solutions, ground truth), (4) Execution evidence (test results, runtime behavior, traces), and (5) Workspace evidence (related files, dependencies, project structure). The theory predicts that: (a) Evidence requirements are task-dependent - simple, objective tasks (e.g., formatting) require minimal evidence while complex, semantic tasks (e.g., correctness in context) require rich evidence; (b) Evidence quality matters more than quantity - low-quality or irrelevant evidence can decrease alignment; (c) Intelligent evidence selection (targeted retrieval) outperforms exhaustive provision when context is large; (d) Different evaluation criteria have different evidence sensitivities - functional correctness benefits most from execution evidence, while semantic similarity benefits from reference evidence; (e) Evidence requirements scale non-linearly with artifact complexity - multi-file systems require disproportionately more evidence than single functions.",
    "supporting_evidence": [
        {
            "text": "Agent-as-a-Judge with read and locate modules (intelligent evidence selection) achieves ~90% alignment with human consensus, while LLM-as-a-Judge without these modules achieves only ~60-70% alignment, demonstrating the value of targeted evidence access",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "Gray-box evaluation with execution trajectories substantially outperforms black-box evaluation without trajectories, showing execution evidence is critical for agent evaluation",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "Ablation studies show that read and locate modules are the largest contributors to alignment in Agent-as-a-Judge, with alignment dropping significantly when these evidence-access modules are removed",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "Reference-enhanced ICE-Score does not reliably improve over reference-free evaluation when references are low quality, demonstrating that evidence quality matters more than presence",
            "uuids": [
                "e1717.0"
            ]
        },
        {
            "text": "CodeBLEU's AST and data-flow components (syntactic and semantic evidence) show very high correlations with human judgments (often ≥0.97), substantially outperforming token-only metrics",
            "uuids": [
                "e1789.0"
            ]
        },
        {
            "text": "Providing both function bodies for semantic similarity evaluation yields high human-model agreement (alpha 0.77-0.83), while missing context reduces agreement",
            "uuids": [
                "e1849.3"
            ]
        },
        {
            "text": "Static analysis warning triage shows very low LLM-human agreement (alpha 0.15) despite clear labels, attributed to insufficient repository context and change history",
            "uuids": [
                "e1849.4"
            ]
        },
        {
            "text": "Combined metric (PASS + EdIT-SIM) achieves higher correlation with human value judgments (r=0.70) than either functional correctness or similarity alone, showing complementary evidence types improve alignment",
            "uuids": [
                "e1801.3"
            ]
        },
        {
            "text": "PASS (functional correctness via execution) shows strong correlation with human judgments (r≈0.62-0.66) but misses 42% of valuable failing generations, indicating execution evidence alone is insufficient",
            "uuids": [
                "e1801.0"
            ]
        },
        {
            "text": "Multi-turn peer battles reveal deeper model capabilities than single-turn evaluation by providing richer interaction context, improving discrimination on complex tasks",
            "uuids": [
                "e1716.0"
            ]
        },
        {
            "text": "Search/retrieval modules can hurt performance in small/simple workspaces but are expected to help in larger ones, showing evidence selection must be context-appropriate",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "Reference-guided prompting for math/reasoning substantially improved GPT-4 grading accuracy (failure rate from 70% to 15%), demonstrating task-specific evidence requirements",
            "uuids": [
                "e1718.0"
            ]
        },
        {
            "text": "Including natural language context when encoding code increased Kendall's Tau correlation with human preference (example: from 0.50 to 0.52), showing specification evidence improves alignment",
            "uuids": [
                "e1796.0"
            ]
        },
        {
            "text": "GEMBA referenceless (QE) zero-shot judgments achieve high system-level agreement (87.6% accuracy) with MQM human judgments, showing that for some tasks, artifact + specification evidence is sufficient without references",
            "uuids": [
                "e1815.0"
            ]
        },
        {
            "text": "Exact-match accuracy achieves extremely high correlation (r=0.999) with human judgments for code refinement tasks with unique ground-truth, showing minimal evidence suffices when solutions are constrained",
            "uuids": [
                "e1789.2"
            ]
        },
        {
            "text": "CodeBERTScore with language-specific pretraining and appropriate layer selection shows higher correlation with execution-based correctness than baseline metrics, demonstrating that semantic evidence (embeddings) can partially substitute for execution evidence",
            "uuids": [
                "e1796.1"
            ]
        },
        {
            "text": "Providing explicit aspect definitions in prompts (criteria clarity) significantly improves Spearman correlations across tasks and models in GPTScore evaluation",
            "uuids": [
                "e1805.0"
            ]
        },
        {
            "text": "TALEC with criteria-division and repetition of evaluation criteria descriptions improved alignment, showing that making evidence/criteria more explicit helps",
            "uuids": [
                "e1710.0"
            ]
        },
        {
            "text": "Memory and planning modules in Agent-as-a-Judge introduced error chains and were detrimental, suggesting that not all evidence types are beneficial and some can introduce noise",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "Oracle execution-based test evaluation can be gamed by models producing trivial/executable-but-meaningless tests, showing execution evidence alone without semantic understanding is insufficient",
            "uuids": [
                "e1695.1"
            ]
        },
        {
            "text": "Perspective API toxicity scores diverge from human judgments depending on prompt context (respectful instruction vs. no instruction), showing evidence context affects automated evaluation reliability",
            "uuids": [
                "e1840.1"
            ]
        },
        {
            "text": "Human evaluators grading EvalGen assertions preferred code-based assertions they could inspect over LLM-based assertions, indicating transparency of evidence/evaluation logic affects trust and perceived alignment",
            "uuids": [
                "e1837.2"
            ]
        },
        {
            "text": "Criteria drift (users changing criteria after grading examples) was pervasive in EvalGen user study, showing that evidence (graded examples) helps users refine evaluation criteria",
            "uuids": [
                "e1837.2"
            ]
        }
    ],
    "theory_statements": [
        "For single-function code evaluation with objective criteria (e.g., formatting, simple correctness), artifact + specification evidence is sufficient to achieve 80%+ alignment with human judgments",
        "For multi-file system evaluation with complex dependencies, workspace evidence (file structure, dependencies, execution traces) is necessary to achieve 80%+ alignment",
        "Intelligent evidence selection (targeted retrieval using locate/read operations) outperforms exhaustive context provision by 15-30 percentage points when workspace size exceeds model context capacity",
        "Evidence quality (relevance, accuracy, clarity) is more important than evidence quantity - low-quality references or irrelevant context can decrease alignment by 5-15 percentage points compared to no additional evidence",
        "Different evaluation criteria have different evidence sensitivities: functional correctness benefits most from execution evidence (20-30 point improvement), semantic similarity benefits most from reference evidence (15-25 point improvement), and style/formatting evaluation requires minimal evidence",
        "Evidence requirements scale non-linearly with artifact complexity: doubling artifact size/complexity requires approximately 1.5-2x more evidence to maintain alignment, with diminishing returns beyond a sufficiency threshold",
        "For tasks with unique or highly constrained correct solutions (e.g., bug fixes, code refinement), minimal evidence (artifact alone) can achieve very high alignment (r&gt;0.95) with human judgments",
        "Noisy, irrelevant, or contradictory evidence actively harms alignment by 5-15 percentage points compared to no additional evidence, particularly when evidence selection is poor",
        "Human evaluators and LLM judges have similar evidence requirements for most tasks - tasks where humans struggle due to insufficient context (e.g., repository history for warning triage) also show low LLM-human alignment",
        "Combining complementary evidence types (e.g., execution + similarity, or AST + data-flow) yields super-additive improvements in alignment compared to single evidence types",
        "Evidence transparency and inspectability affects perceived alignment and trust - evaluators prefer evidence they can verify (e.g., code-based assertions) over opaque evidence (e.g., LLM judgments)",
        "Task-specific evidence is more valuable than generic evidence - providing domain-specific context (e.g., project style guides, threat models) improves alignment more than generic documentation"
    ],
    "new_predictions_likely": [
        "For a code review task on a 500+ line change spanning multiple files, providing execution traces and test results will improve LLM-human agreement by 20-30 percentage points compared to code-only evaluation",
        "For API design evaluation, providing usage examples and integration tests will improve agreement by 25-35 percentage points compared to API signature and documentation alone",
        "For bug localization in a large codebase, providing stack traces, error messages, and related file context will improve agreement by 30-40 percentage points compared to buggy code snippet alone",
        "Implementing a learned evidence selection module (trained to predict which files/context are most relevant for a given evaluation task) will outperform heuristic selection by 8-15 percentage points",
        "For repository-scale evaluation, providing a dependency graph and call graph will improve agreement by 15-25 percentage points compared to flat file access without relationship information",
        "For security evaluation, providing threat model documentation and known vulnerability patterns will improve agreement by 20-30 percentage points compared to code-only analysis",
        "For performance evaluation, providing profiling data and benchmark results will improve agreement by 25-35 percentage points compared to static code analysis alone"
    ],
    "new_predictions_unknown": [
        "Whether there exists a universal evidence sufficiency threshold (e.g., 'enough to reconstruct developer intent') that applies across all artifact types and evaluation criteria, or whether thresholds are fundamentally task-dependent",
        "Whether LLM judges can learn to request specific evidence types adaptively based on the evaluation task and their current uncertainty, and if so, whether this active evidence gathering improves efficiency without sacrificing alignment",
        "Whether providing evidence beyond model context limits causes catastrophic degradation (due to attention dilution) or graceful performance decline (due to information loss), and at what threshold this occurs",
        "Whether evidence requirements differ systematically between different evaluation criteria dimensions (e.g., correctness vs. maintainability vs. security vs. performance) in predictable ways that could inform evidence selection strategies",
        "Whether synthetic evidence (e.g., LLM-generated explanations, documentation, or test cases) can substitute for real evidence without reducing alignment, and under what conditions synthetic evidence is beneficial vs. harmful",
        "Whether evidence presentation format (e.g., structured vs. unstructured, visual vs. textual) significantly affects alignment independent of information content",
        "Whether there are fundamental limits to evidence-based alignment improvement - i.e., whether some evaluation tasks have inherent ambiguity that no amount of evidence can resolve",
        "Whether evidence requirements for LLM judges decrease as model capabilities improve (i.e., whether stronger models can infer missing context better), or whether evidence requirements remain constant across model generations"
    ],
    "negative_experiments": [
        "Finding a complex multi-file system evaluation task where artifact-only evaluation consistently achieves &gt;85% alignment with human experts would challenge the evidence hierarchy and scaling assumptions",
        "Finding that providing execution traces and test results decreases alignment for debugging or correctness evaluation tasks would challenge the assumption that execution evidence is valuable for functional evaluation",
        "Finding that intelligent evidence selection (using locate/read modules) consistently underperforms random or exhaustive selection across multiple tasks would challenge the noise-reduction and targeted-retrieval hypotheses",
        "Demonstrating that evidence requirements do not scale with artifact complexity (e.g., that 10-file systems require the same evidence as single-file systems) would challenge the complexity-scaling relationship",
        "Finding that LLM judges and human evaluators have fundamentally different evidence requirements (e.g., humans need X but LLMs need Y, where X≠Y) would challenge the parallel-processing assumption",
        "Finding that low-quality or irrelevant evidence consistently improves alignment rather than harming it would challenge the evidence-quality-matters principle",
        "Demonstrating that combining complementary evidence types (e.g., execution + similarity) yields sub-additive or no improvement compared to single evidence types would challenge the complementarity assumption",
        "Finding that evidence transparency and inspectability do not affect perceived alignment or trust would challenge assumptions about human evaluation of automated judges"
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to quantify artifact complexity in a way that predicts evidence requirements - what metrics or features of artifacts determine their evidence needs?",
            "uuids": []
        },
        {
            "text": "The optimal evidence selection strategy for different artifact types and evaluation criteria is not fully characterized - what heuristics or learned models work best for which tasks?",
            "uuids": []
        },
        {
            "text": "The theory does not address how evidence requirements change with evaluator expertise level - do expert evaluators need less evidence than novices, and does this apply to LLM judges?",
            "uuids": []
        },
        {
            "text": "The interaction between evidence type and evaluation criteria is not fully specified - which evidence types are most valuable for which criteria, and are there systematic patterns?",
            "uuids": []
        },
        {
            "text": "The theory does not explain why some tasks (e.g., code refinement with unique solutions) achieve very high alignment with minimal evidence while others (e.g., open-ended generation) require rich evidence",
            "uuids": []
        },
        {
            "text": "The theory does not address how evidence requirements change over time or with repeated evaluation - do evaluators learn to need less evidence, or do requirements remain constant?",
            "uuids": []
        },
        {
            "text": "The theory does not specify how to handle conflicting evidence (e.g., when execution suggests correctness but semantic analysis suggests incorrectness) - what resolution strategies work best?",
            "uuids": []
        },
        {
            "text": "The theory does not explain the mechanisms by which evidence improves alignment - is it through reducing ambiguity, providing grounding, enabling verification, or other mechanisms?",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "GEMBA referenceless evaluation achieves high system-level agreement (87.6% accuracy) with human judgments, suggesting that for some tasks, references may not be necessary despite the evidence hierarchy predicting they would help",
            "uuids": [
                "e1815.0"
            ]
        },
        {
            "text": "Reference-enhanced ICE-Score does not always improve over reference-free evaluation, and can sometimes perform worse when references are low quality, challenging the assumption that more evidence is always better",
            "uuids": [
                "e1717.0"
            ]
        },
        {
            "text": "Simple exact-match metrics achieve extremely high correlation (r=0.999) with human judgments for code refinement without any contextual evidence beyond the artifact itself, suggesting evidence requirements may be task-dependent rather than universal",
            "uuids": [
                "e1789.2"
            ]
        },
        {
            "text": "Search/retrieval modules sometimes hurt performance in Agent-as-a-Judge for small/simple workspaces, suggesting that evidence selection can introduce noise and that more evidence access is not always beneficial",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "Memory and planning modules (which provide additional evidence about reasoning chains) were detrimental in Agent-as-a-Judge experiments, suggesting some evidence types can introduce error propagation",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "Some traditional metrics (ChrF, ROUGE-L) achieve reasonable alignment with human judgments on code generation without any semantic or execution evidence, suggesting surface-level evidence can sometimes suffice",
            "uuids": [
                "e1833.1",
                "e1833.2"
            ]
        }
    ],
    "special_cases": [
        "For code refinement tasks with unique correct solutions (e.g., fixing a specific bug), minimal evidence (artifact alone) is sufficient and can achieve near-perfect alignment (r&gt;0.99) with human judgments",
        "For security evaluation, domain-specific evidence (threat models, attack vectors, vulnerability patterns) is uniquely critical and cannot be substituted by general code understanding",
        "For performance evaluation, execution profiling data is necessary and cannot be reliably substituted by static analysis or other evidence types",
        "For style and formatting evaluation, project-specific style guides are necessary evidence, and general style knowledge is insufficient",
        "For tasks with high inherent ambiguity or subjectivity (e.g., code 'elegance' or 'readability'), even rich evidence may not achieve high alignment due to fundamental disagreement among human evaluators",
        "For adversarial or safety-critical evaluation, evidence about potential failure modes and edge cases is more valuable than evidence about typical behavior",
        "For repository-scale evaluation, evidence about architectural patterns and design decisions is more valuable than exhaustive file-level details",
        "For tasks where ground truth is unique and verifiable (e.g., unit test pass/fail), execution evidence alone may be sufficient without semantic or contextual evidence",
        "For natural language generation evaluation (e.g., code comments, documentation), reference examples are particularly valuable and difficult to substitute with other evidence types",
        "For tasks requiring domain expertise (e.g., medical code, financial systems), domain-specific documentation and standards are necessary evidence that general code understanding cannot replace"
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Tian et al. (2024) Agent-as-a-Judge: Evaluate Agents with Agents [Demonstrates importance of evidence access modules (read, locate) but does not propose comprehensive evidence sufficiency theory]",
            "Ren et al. (2023) CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code [Shows value of semantic context and language-specific pretraining but focuses on metric design rather than evidence theory]",
            "Zhong et al. (2023) ICE-Score: Instructing Large Language Models to Evaluate Code [Explores reference-free vs. reference-enhanced evaluation but does not develop general evidence theory]",
            "Ren et al. (2020) CodeBLEU: a Method for Automatic Evaluation of Code Synthesis [Demonstrates value of syntactic (AST) and semantic (data-flow) evidence but focuses on metric composition]",
            "Weyssow et al. (2024) Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences [Explores human-in-the-loop evidence gathering but focuses on assertion selection rather than evidence theory]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositional Generalization Gap Theory (Revised) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-379</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-379</p>
                <p><strong>Name:</strong> Compositional Generalization Gap Theory (Revised)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that the gap between an agent's performance on trained versus novel compositional tasks is determined by four interacting factors: (1) primitive skill automaticity, compatibility, and format, (2) compositional diversity that promotes linear factorization, (3) representational geometry quality (degree of linear factorization of concept features), and (4) bidirectional spurious correlation effects (both data-induced and curriculum-induced). Optimal curricula must balance these factors through flexible structures adapted to architecture and domain: three-phase curricula (primitive mastery → systematic composition → robustness) for standard settings, in-context meta-learning for few-shot architectures, or retrieval-augmented training for multi-modal tasks. The gap exhibits super-linear scaling with composition depth (empirically validated for depths 2-3), and curriculum pacing should adapt to generalization performance, representational geometry quality, and negative transfer signals. Compositional learning is graded rather than binary, with models potentially using mixed strategies combining compositional and non-compositional approaches.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-335.html">[theory-335]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Revised theory description to be more concise and readable while maintaining key points about four factors, flexible curriculum structures, and graded compositional learning.</li>
                <li>Added representational geometry (linear factorization) as a fourth key factor determining the generalization gap, with quantitative metrics (R²>0.8, cosine similarity<0.1) and theoretical support (k=2 sufficiency under ideal conditions).</li>
                <li>Modified primitive automaticity factor to include three conditions: individual accuracy (≥95%), semantic compatibility, and composition-friendly format, based on ComposableCoT and Compositional Ablation Study evidence.</li>
                <li>Added bidirectional treatment of spurious correlations: curricula can both reduce (data spurious correlations) and introduce (curriculum-induced contextual dependencies) spurious patterns, with design principles to manage both (systematic variation, format design, mismatch training).</li>
                <li>Made the three-phase curriculum structure explicitly flexible and architecture-dependent, acknowledging that in-context learning, meta-learning, and retrieval-augmented approaches can serve as alternatives depending on architecture and domain.</li>
                <li>Added detailed theory statement about curriculum design details: ratio of primitive to compositional examples (m:n), with specific guidance that shorter compositional blocks (m=10-11, n=2-4) foster stronger compositional strategies than longer blocks (m=4-6, n=12-16).</li>
                <li>Added architecture and inductive biases as a major modulating factor, with specific guidance on adapting curriculum design to architectural constraints (convolutional vs transformer vs modular networks) and quantitative estimates of benefits (15-30 pp improvement).</li>
                <li>Extended theory to multi-modal settings, adding statements about cross-modal composition challenges (LV harder than LL/VV), modality-specific primitive training, performance degradation with co-occurrence level (5-15 pp per level), and cross-modal alignment mechanisms.</li>
                <li>Added negative transfer considerations with two specific scenarios: (1) compositional training before primitive automaticity (20-40 pp gap increase), and (2) continued training entrenching shortcuts (20-30 pp OOD decrease), with recommendation to pause when negative transfer detected.</li>
                <li>Added primitive compatibility requirements as a necessary condition beyond individual mastery, specifying that semantic alignment of primitives is necessary for successful composition (original pairings: +7.5 to +15 pp; incompatible: -18 to +5 pp).</li>
                <li>Clarified scope boundaries explicitly: theory is empirically supported for depths ≤2-3 with majority of evidence from depth-2, limited evidence beyond depth-3, and very large combinatorial spaces (>100k) may require additional mechanisms beyond standard three-phase structure.</li>
                <li>Added depth-dependent automaticity thresholds: primitives used in deeper compositions (d≥3) require higher automaticity (≥98-100%) than those in shallow compositions (d=2, ≥95%).</li>
                <li>Added representational geometry quality as an adaptive curriculum signal alongside generalization gap, recommending that diversity be increased specifically when R²<0.8 to improve linear factorization metrics.</li>
                <li>Modified compositional diversity statements to emphasize that diversity must promote linear factorization, not just provide coverage, with specific guidance that achieving R²>0.8 typically requires k≥2 combinations under favorable conditions or k>10 under less favorable conditions.</li>
                <li>Added explicit statement about graded/continuous nature of compositional learning: models can use pure compositional strategies, pure vanilla few-shot strategies, or mixed strategies, with curriculum structure modulating which emerges.</li>
                <li>Added theory statement about test-time adaptation and domain-specific fine-tuning as mechanisms that can substantially improve compositional generalization (73-99% vs <1-3% for general models), suggesting theory may need extension to test-time scenarios.</li>
                <li>Added supporting evidence about graded compositional learning from curriculum-length manipulation experiments showing mixed strategies are possible.</li>
                <li>Expanded unaccounted_for section to include: lack of operational metrics for compositional diversity, hierarchical primitive structures, pretraining effects, forgetting/interference, very large combinatorial spaces, exploration in interactive settings, a priori compatibility assessment, test-time adaptation curriculum design, limited empirical scope to depth ≤2-3, and handling of graded/mixed compositional strategies.</li>
                <li>Made predictions more specific with quantitative effect sizes (e.g., '15-30 pp improvement', '10-20% faster convergence', '4-20× data efficiency') and specific conditions (e.g., 'R²>0.95', 'm≥10, n≤4', '10-20% mismatch training').</li>
                <li>Added prediction about RFT data efficiency (4-20× improvement) based on ComposableCoT evidence.</li>
                <li>Added negative experiment about test-time adaptation potentially compensating for poor training curricula.</li>
                <li>Clarified that general-purpose LLMs fail at systematic compositional generalization despite massive pretraining, challenging assumptions about primitive mastery through pretraining.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The compositional generalization gap G can be operationalized as G = P_train - P_novel, where P_train is performance (accuracy or success rate) on trained compositions and P_novel is performance on novel compositions of known primitives. This gap typically ranges from 10-90 percentage points depending on domain, architecture, and training approach.</li>
                <li>The gap G is determined by four primary factors with non-linear interactions: (1) primitive automaticity, compatibility, and format A, (2) compositional diversity D, (3) representational geometry quality R (degree of linear factorization), and (4) bidirectional spurious correlation strength S (both S_data from training data and S_curriculum from curriculum-induced dependencies). The relationship can be approximated as G ∝ f(A, D, R, S_data, S_curriculum) where improvements in A, D, and R decrease G while increases in S_data and S_curriculum increase G.</li>
                <li>Primitive automaticity A must satisfy three conditions: (1) high individual accuracy (≥95%), (2) semantic compatibility between primitives that will be composed, and (3) composition-friendly format (e.g., proxy prefix/suffix structure for language models, modular representations for vision models). The relationship G ∝ 1/(1 + α·A) holds only when all three conditions are met; incompatible or poorly-formatted primitives show minimal compositional gains (typically <5 pp improvement) regardless of individual mastery.</li>
                <li>Compositional diversity D must include both structural diversity (different orderings and combinations of primitives) and contextual diversity (different surface forms for the same underlying composition), and must be sufficient to promote linear factorization of concept representations. The relationship follows G ∝ exp(-β·D) where β reflects the efficiency of diversity in promoting linear factorization. Empirically, achieving R²>0.8 for linear decodability typically requires observing k≥2 combinations per concept value under favorable conditions, or k>10 under less favorable conditions.</li>
                <li>Representational geometry quality R, measured by linear factorization metrics (R² for linear decodability of individual concepts >0.8, cosine similarity between concept axes <0.1, orthogonality of concept subspaces), is a critical mediator of compositional generalization. Under ideal linear factorization (R²>0.95, cosine<0.05), as few as k=2 observed combinations per concept value can enable near-perfect generalization. The gap decreases sharply as R increases: G ∝ exp(-γ·R), with empirical evidence showing transitions from random-level OOD performance (R²<0.5) to >90% OOD performance (R²>0.8).</li>
                <li>Spurious correlations have bidirectional effects: (1) training data spurious correlations increase G by causing models to learn context-dependent rather than compositional representations (G ∝ δ·S_data), and (2) curriculum-induced contextual dependencies (e.g., in-context correlations between subtask parameters and compositional parameters) can create spurious patterns that increase G when context mismatches occur (G ∝ ε·S_curriculum). Optimal curricula must manage both through systematic variation, format design (e.g., arbitrary proxy prefixes), and explicit mismatch training.</li>
                <li>Optimal curricula can follow multiple structures depending on architecture and domain: (1) Three-phase structure (primitive mastery → systematic composition → robustness training) for domains with isolable primitives and standard architectures, (2) In-context meta-learning with episodic presentation for architectures with strong few-shot capabilities, (3) Retrieval-augmented training for multi-modal settings requiring cross-modal alignment. The choice depends on architectural inductive biases, primitive interdependence, and whether primitives can be meaningfully isolated.</li>
                <li>Within curriculum structures, the ratio of primitive to compositional examples critically affects learning: shorter compositional blocks (e.g., m=10-11 primitive examples, n=2-4 compositional examples) foster stronger compositional strategies with explicit intermediate representations, while longer compositional blocks (e.g., m=4-6, n=12-16) produce mixed strategies combining compositional and vanilla few-shot learning. The optimal ratio depends on primitive task difficulty and target composition depth.</li>
                <li>The rate of compositional complexity increase in curricula should be adaptive to multiple signals: (1) when G exceeds a threshold (e.g., >30%), complexity should increase slowly or pause, (2) when OOD performance decreases while ID performance increases (negative transfer signal), training should pause or adapt, (3) when G is below threshold (e.g., <15%) AND representational geometry quality R is improving, complexity can increase more rapidly. Adaptive pacing based on these signals produces 10-25 pp better final OOD performance than fixed-pace curricula.</li>
                <li>Negative transfer occurs in two scenarios: (1) when compositional training begins before primitive automaticity is achieved (A < 90%), leading to entangled representations that increase G by 20-40 pp, and (2) when continued training entrenches brittle heuristics or shortcuts, causing OOD performance to decrease (sometimes by 20-30 pp) even as ID performance increases. Training should pause or adapt when negative transfer is detected through OOD performance monitoring.</li>
                <li>The compositional generalization gap exhibits super-linear (potentially exponential) scaling with composition depth d for depths 2-3: G ∝ d^k where k > 1 (empirically k≈2-3), or G ∝ exp(δ·d). Removing intermediate composition practice causes catastrophic failure: jumping from primitives (depth 1) to depth-3 compositions without depth-2 practice produces gaps of 60-80 pp. Empirical evidence strongly supports super-linear scaling for d=2→3, but evidence for d>3 is limited and extrapolation beyond depth 3 is uncertain.</li>
                <li>Architecture and inductive biases dramatically modulate the generalization gap: different architectures show gaps ranging from 12-97 pp on identical tasks (e.g., COGITAO G5: Vanilla-ViT 70pp, Grid-ViT 97.8pp, LLaDA 57pp). Curriculum design should adapt to architectural constraints: convolutional architectures benefit from spatial progression curricula, transformers from attention-based compositional structures, and modular networks from explicit module composition training. Architecture-adapted curricula show 15-30 pp better generalization than architecture-agnostic curricula.</li>
                <li>Multi-modal compositional generalization introduces additional complexity: (1) cross-modal compositions (linguistic-visual LV) are often harder than within-modality compositions (linguistic-linguistic LL, visual-visual VV), (2) performance degrades as co-occurrence level increases (Level-1→Level-3, typically 5-15 pp degradation per level), and (3) modality-specific primitive training with explicit cross-modal alignment (e.g., retrieval-augmented training from both linguistic and visual databases) is necessary for robust multi-modal composition, producing 5-15 pp improvements on cross-modal tasks.</li>
                <li>The automaticity threshold for primitives is context-dependent and depth-dependent: (1) primitives used in deeper compositions (d≥3) require higher automaticity (≥98-100%) than those in shallow compositions (d=2, ≥95%), (2) primitives that will be composed across modalities require explicit alignment training beyond individual mastery, and (3) harder primitive tasks require more in-context or training examples to reach automaticity (e.g., P=41 modular arithmetic required ≥11 examples vs fewer for P=59).</li>
                <li>Compositional learning is graded rather than binary: models can adopt pure compositional strategies (using explicit intermediate representations throughout), pure vanilla few-shot strategies (learning compositional mappings directly), or mixed strategies (using both approaches). The curriculum structure (particularly the ratio of primitive to compositional examples) modulates which strategy emerges, with no single strategy universally optimal across all architectures and tasks.</li>
                <li>The theory's empirical scope is currently limited to compositions of depth ≤2-3 primitives, with the vast majority of evidence coming from depth-2 (pairwise) compositions. Claims about deeper compositions (d≥4) or exponential scaling beyond depth 3 lack direct empirical support. Very large combinatorial spaces (>100k combinations) show substantial residual gaps (e.g., 53% success vs 80%+ primitive performance) even with optimal curricula, suggesting additional mechanisms (hierarchical composition, meta-learning, test-time adaptation) may be necessary.</li>
                <li>Test-time adaptation and domain-specific fine-tuning can substantially improve compositional generalization: domain-adapted models with test-time training achieve 73-99% systematicity performance compared to <1-3% for general-purpose models, suggesting that the theory's curriculum principles may need extension to test-time scenarios where models adapt to specific compositional patterns during inference.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Substantial compositional generalization gaps exist across diverse domains: COGITAO CompGen shows ID-OOD gaps of 12.7-72.5 percentage points across settings, MLC shows gaps of ~13-22 pp between trained and novel compositions, and RL experiments show primitive mastery (>69% accuracy) failing to transfer to compositions (0-15% accuracy), with gaps exceeding 50-60 pp. <a href="../results/extraction-result-2019.html#e2019.0" class="evidence-link">[e2019.0]</a> <a href="../results/extraction-result-2017.html#e2017.0" class="evidence-link">[e2017.0]</a> <a href="../results/extraction-result-2008.html#e2008.0" class="evidence-link">[e2008.0]</a> </li>
    <li>Curriculum approaches consistently and substantially outperform non-curriculum baselines: MLC achieves 86.73% mean OOD accuracy vs 0% for static seq2seq training (gap of ~87 pp), Determiner curriculum reduces training episodes by ~15% while maintaining ~79% zero-shot generalization, and Preposition curriculum enables convergence where baseline completely fails. <a href="../results/extraction-result-2017.html#e2017.0" class="evidence-link">[e2017.0]</a> <a href="../results/extraction-result-2017.html#e2017.2" class="evidence-link">[e2017.2]</a> <a href="../results/extraction-result-2013.html#e2013.0" class="evidence-link">[e2013.0]</a> <a href="../results/extraction-result-2013.html#e2013.1" class="evidence-link">[e2013.1]</a> </li>
    <li>Super-linear scaling with composition depth is empirically confirmed for depths 2-3: MLC ablations show catastrophic drops when removing intermediate compositions (86.73% → 21.01%, a 65.7 pp drop), and COGITAO C3 experiments show dramatic OOD collapse when extrapolating from depth-2 to depth-3 (OOD accuracies near 1-6%). <a href="../results/extraction-result-2017.html#e2017.1" class="evidence-link">[e2017.1]</a> <a href="../results/extraction-result-2019.html#e2019.0" class="evidence-link">[e2019.0]</a> </li>
    <li>Compositional diversity is critical and quantity alone is insufficient: from-scratch (n,k) experiments show that increasing combinatorial diversity substantially improves zero-shot OOD accuracy (from random-level to >90%), while dataset-size scaling tests show that increasing sample count by 4× at fixed combinatorial coverage does not reduce the 60-80% generalization gap. <a href="../results/extraction-result-2005.html#e2005.0" class="evidence-link">[e2005.0]</a> <a href="../results/extraction-result-2005.html#e2005.1" class="evidence-link">[e2005.1]</a> </li>
    <li>Representation geometry (linear factorization) is a critical mechanism: three-phase feature learning shows representations progress from spurious/entangled to linearly factored as diversity increases (R²>0.8, cosine similarity<0.1 enables >90% zero-shot), and minimal compositional learning proposition proves that k=2 combinations suffice under ideal linear factorization. <a href="../results/extraction-result-2005.html#e2005.2" class="evidence-link">[e2005.2]</a> <a href="../results/extraction-result-2005.html#e2005.3" class="evidence-link">[e2005.3]</a> </li>
    <li>Specific curriculum components are causally important: MLC ablations show that removing the auxiliary copy task reduces OOD performance by 17.7 pp, removing primitives reduces it by 11.5 pp, and removing level-1 compositions causes catastrophic failure (65.7 pp drop). <a href="../results/extraction-result-2017.html#e2017.1" class="evidence-link">[e2017.1]</a> </li>
    <li>Primitive format and alignment matter critically for composition: ComposableCoT shows that training primitives in a composition-friendly format enables substantially better zero-shot composition than standard formats, and Compositional Ablation Study shows gains depend strongly on semantic alignment of primitive pairs (original pairings: +7.5 to +15 pp; replacing components: -18 to +5 pp). <a href="../results/extraction-result-2020.html#e2020.0" class="evidence-link">[e2020.0]</a> <a href="../results/extraction-result-2020.html#e2020.2" class="evidence-link">[e2020.2]</a> <a href="../results/extraction-result-2008.html#e2008.1" class="evidence-link">[e2008.1]</a> </li>
    <li>Retrieval-augmented approaches that align primitive representations across modalities improve compositional generalization: RAG-for-MSCG shows consistent improvements (+1.8-3.0 pp overall), with joint retrieval from both linguistic and visual databases outperforming single-modality retrieval. <a href="../results/extraction-result-2006.html#e2006.0" class="evidence-link">[e2006.0]</a> <a href="../results/extraction-result-2006.html#e2006.2" class="evidence-link">[e2006.2]</a> </li>
    <li>Primitive skill learnability with small datasets validates the primitives-first approach: Atomic Task Learnability study shows that atomic tasks are reliably learnable with ≤500 examples (often achieving near 100% accuracy). <a href="../results/extraction-result-2020.html#e2020.4" class="evidence-link">[e2020.4]</a> </li>
    <li>In-context learning and meta-learning provide alternative paths to compositional generalization: Curriculum vs Vanilla ICL study shows that providing primitives in-context (not as separate pretraining) enables zero-shot composition with better robustness than vanilla training, with linear-probe analysis revealing explicit intermediate representations formed during inference. <a href="../results/extraction-result-2011.html#e2011.0" class="evidence-link">[e2011.0]</a> <a href="../results/extraction-result-2011.html#e2011.1" class="evidence-link">[e2011.1]</a> </li>
    <li>Compositional learning is graded rather than binary: curriculum-length manipulation shows models can use mixed strategies combining compositional and vanilla few-shot approaches, with the ratio of primitive to compositional examples (e.g., 10-10-4 vs 4-4-16) modulating which strategy dominates. <a href="../results/extraction-result-2011.html#e2011.3" class="evidence-link">[e2011.3]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An agent trained with a curriculum ensuring ≥95% primitive accuracy in composition-friendly format (e.g., proxy prefixes) with semantically compatible primitives will show 15-30 pp smaller generalization gap on depth-2 compositions compared to an agent with high primitive accuracy but standard format or incompatible primitives.</li>
                <li>Measuring representational geometry quality (R² for linear decodability, cosine similarity between concept axes) during training and increasing diversity specifically when R²<0.8 will result in 10-20 pp better final OOD performance and 20-30% faster convergence to low generalization gaps (<15%) compared to fixed-diversity curricula.</li>
                <li>Agents trained with explicit bidirectional spurious correlation management (arbitrary proxy prefixes, systematic context variation, and 10-20% mismatch training examples) will show 10-20 pp better generalization to novel compositions than agents trained with the same compositional diversity but without spurious correlation management.</li>
                <li>For multi-modal compositional tasks, training primitives separately by modality with explicit cross-modal alignment (retrieval-augmented training from both linguistic and visual databases) will produce 5-15 pp better generalization on cross-modal compositions (LV type) compared to end-to-end multi-modal training, while maintaining similar performance on within-modality compositions.</li>
                <li>Monitoring for negative transfer (OOD performance decreasing ≥5 pp while ID performance increases) and pausing training when detected will result in 10-25 pp better final OOD performance compared to fixed-duration training, particularly in RL settings where continued optimization can entrench shortcuts.</li>
                <li>Architecture-adapted curricula (spatial progression for CNNs with 3×3→5×5→7×7 receptive field growth, attention-based composition for transformers with progressive attention span, explicit module composition for modular networks) will show 15-30 pp better generalization than architecture-agnostic curricula on the same tasks.</li>
                <li>Curricula with shorter compositional blocks (m≥10 primitive examples, n≤4 compositional examples) will produce 10-20 pp better zero-shot generalization on novel compositions compared to longer compositional blocks (m≤6, n≥12) for the same total training budget, due to stronger compositional strategy emergence.</li>
                <li>Rejection sampling fine-tuning (RFT) starting from composition-friendly primitive models (e.g., ComposableCoT) with only 100-500 compositional examples will match or exceed supervised fine-tuning performance using 2000+ compositional examples, demonstrating 4-20× data efficiency for compositional adaptation.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an agent is trained to achieve near-perfect linear factorization (R²>0.95, cosine similarity<0.05) on primitives through targeted diversity and architectural constraints, it remains unknown whether this enables generalization to compositions of depth 4-5 without explicit training at those depths, or if there exists a fundamental limit requiring training at each depth level. If depth-4-5 generalization succeeds, this would suggest linear factorization is sufficient for arbitrary depth; if it fails, this would indicate depth-specific learning is necessary.</li>
                <li>It is unknown whether there exists an optimal universal ratio of primitive to compositional examples (e.g., m:n = 10:4, 8:8, 6:12) that minimizes the generalization gap across architectures and domains, or if this ratio is fundamentally architecture-dependent and task-dependent. Finding architecture-specific optimal ratios would enable principled curriculum design, while finding universal ratios would suggest domain-general compositional learning principles.</li>
                <li>The theory predicts that managing spurious correlations bidirectionally (reducing data spurious correlations AND curriculum-induced contextual dependencies) reduces the gap, but it is unknown whether there exists an optimal balance point or whether minimizing both types simultaneously is always beneficial. Some curriculum-induced dependencies (e.g., in-context subtask information) might actually aid composition in certain architectures, suggesting a tradeoff rather than pure minimization.</li>
                <li>If the compositional generalization gap could be reduced to near-zero (<5 pp) for depth-2 compositions through optimal curriculum design (high automaticity, high diversity, R²>0.9, spurious correlation management), it remains unknown whether the learned compositional abilities would transfer to entirely new domains with different primitives, or if compositional learning is fundamentally domain-specific. Transfer of compositional meta-skills would suggest a more general cognitive mechanism that could be leveraged for rapid adaptation.</li>
                <li>It is unknown whether the super-linear scaling of the generalization gap with composition depth continues beyond depth 3-4 or reaches an asymptote at some depth level (e.g., 5-6 primitives), and whether this asymptote (if it exists) is architecture-dependent or fundamental to the learning problem. Testing depths 5-7 systematically would clarify whether exponential scaling continues indefinitely or whether compositional difficulty plateaus.</li>
                <li>The theory does not specify whether there is an interaction effect between representational geometry quality R and compositional diversity D: it is unknown whether high-quality linear factorization (R²>0.9) can compensate for lower compositional diversity (k=2-3 combinations), or whether both are necessary prerequisites. Understanding this interaction would clarify whether to prioritize representation quality or diversity in resource-constrained settings.</li>
                <li>For multi-modal compositions, it is unknown whether there exists a universal hierarchy of difficulty (e.g., LL < VV < LV) across tasks and architectures, or if this hierarchy is task-specific and architecture-dependent. A universal hierarchy would enable principled multi-modal curriculum design with predictable difficulty progression.</li>
                <li>It is unknown whether test-time adaptation mechanisms (e.g., test-time training, in-context learning with compositional examples) can fully compensate for poor training curricula, or if there are fundamental limits to what test-time adaptation can achieve. If test-time adaptation can recover from poor training, this would suggest curriculum design is less critical; if not, this would emphasize the importance of training-time curriculum optimization.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents trained with high primitive automaticity (≥95% accuracy) in composition-friendly formats with semantically compatible primitives show the same or larger generalization gaps as agents trained with low primitive automaticity (60-70%) or incompatible primitives, this would challenge the theory's core emphasis on primitive mastery, format, and compatibility as prerequisites for compositional learning.</li>
                <li>If curricula with low compositional diversity (k=2-3 combinations per concept) that achieve high representational geometry quality (R²>0.8, cosine<0.1) produce equal or better generalization than high-diversity curricula (k>10) with poor representational geometry (R²<0.5), this would suggest that representation quality alone is sufficient and diversity is only instrumental to achieving good representations.</li>
                <li>If the generalization gap does not decrease when spurious correlations are systematically managed (both data spurious correlations removed through augmentation AND curriculum-induced dependencies minimized through format design and mismatch training), this would challenge the theory's claim about the causal role of spurious correlations in creating the gap.</li>
                <li>If agents show equal or better generalization when compositional complexity is introduced all at once (jumping directly from primitives to depth-3 compositions) rather than gradually (primitives → depth-2 → depth-3), even when controlling for representational geometry quality and spurious correlations, this would undermine the theory's prescription for systematic complexity progression and the importance of intermediate composition practice.</li>
                <li>If the relationship between composition depth and generalization gap is linear or sub-linear for depths 3-5 (rather than super-linear as predicted), this would contradict the theory's prediction about exponential scaling of compositional difficulty and suggest that intermediate composition practice is less critical than proposed.</li>
                <li>If negative transfer does not occur when compositional training begins before primitive automaticity is achieved (A<90%), or if early compositional training actually improves primitive learning and final compositional performance by 10+ pp, this would challenge the theory's emphasis on sequential phase structure and suggest that joint training may be superior.</li>
                <li>If architecture-agnostic curricula perform as well as architecture-adapted curricula across diverse architectures (CNNs, transformers, modular networks), with differences <5 pp, this would suggest that architectural considerations are less important than the theory proposes and that universal curriculum principles exist.</li>
                <li>If measuring and adapting to representational geometry quality during training (increasing diversity when R²<0.8) does not improve outcomes compared to fixed curricula with the same total diversity, this would suggest that representation quality is an outcome rather than a useful control signal for curriculum pacing.</li>
                <li>If curricula with longer compositional blocks (m=4-6, n=12-16) produce equal or better zero-shot generalization than shorter compositional blocks (m=10-11, n=2-4) for the same total training budget, this would challenge the theory's claim that shorter compositional blocks foster stronger compositional strategies.</li>
                <li>If test-time adaptation can fully recover compositional generalization performance (achieving <5 pp gap from optimal training curriculum) even when training curriculum is poor or absent, this would suggest that curriculum design during training is less critical than the theory proposes.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not fully specify how to quantitatively measure compositional diversity D in practice, particularly how to weight structural versus contextual diversity, how to determine when sufficient diversity has been achieved to promote linear factorization (beyond the general guidance of R²>0.8), or how to efficiently sample diverse combinations in very large combinatorial spaces. <a href="../results/extraction-result-2005.html#e2005.0" class="evidence-link">[e2005.0]</a> <a href="../results/extraction-result-2005.html#e2005.2" class="evidence-link">[e2005.2]</a> </li>
    <li>The theory does not address how to handle situations where primitives themselves have hierarchical structure (sub-primitives or primitive compositions), and whether these require separate curriculum phases, recursive application of the three-phase structure, or alternative approaches such as hierarchical meta-learning. <a href="../results/extraction-result-2013.html#e2013.2" class="evidence-link">[e2013.2]</a> </li>
    <li>The role of pre-existing world knowledge and language understanding in reducing the compositional gap for text-based environments is not fully addressed. Pretrained model probing shows that large pretrained models encode partial linear compositional structure but still require downstream diversity for robust generalization, and general-purpose LLMs largely fail at systematic compositional generalization despite massive pretraining, but the theory doesn't specify how to leverage or overcome pretraining effects. <a href="../results/extraction-result-2005.html#e2005.4" class="evidence-link">[e2005.4]</a> <a href="../results/extraction-result-2017.html#e2017.3" class="evidence-link">[e2017.3]</a> </li>
    <li>The theory does not fully account for the role of forgetting or interference during the transition from primitive training to compositional training, or how to maintain primitive performance while learning compositions, particularly in continual learning settings where new primitives or compositions are added over time. </li>
    <li>The theory does not specify how to handle very large combinatorial spaces (>100k combinations) where even with optimal curricula, substantial gaps remain (e.g., D+P few-shot showing 53% success vs 80%+ primitive performance). Additional mechanisms like hierarchical composition, meta-learning, or test-time adaptation may be necessary, but the theory doesn't specify when or how to deploy these. <a href="../results/extraction-result-2013.html#e2013.2" class="evidence-link">[e2013.2]</a> </li>
    <li>The theory does not fully account for the role of exploration strategies and interactive learning in text environments, where agents may need to discover compositional structures through trial and error rather than supervised learning, and how curriculum design should adapt to interactive settings with sparse rewards. </li>
    <li>The theory does not specify how to assess primitive compatibility a priori (before composition training), making it difficult to predict which primitive combinations will benefit from curriculum approaches and which will show minimal gains regardless of individual mastery. Operational metrics or tests for compatibility are needed. <a href="../results/extraction-result-2008.html#e2008.1" class="evidence-link">[e2008.1]</a> </li>
    <li>The theory does not address how curriculum design should differ for test-time adaptation scenarios (e.g., test-time training, in-context learning during inference) versus standard training scenarios. Evidence shows that domain-specific models with test-time training can achieve strong systematicity performance (73-99%), but the theory doesn't specify how to design curricula that facilitate test-time adaptation. <a href="../results/extraction-result-2017.html#e2017.3" class="evidence-link">[e2017.3]</a> </li>
    <li>The theory's empirical scope is severely limited to shallow compositions (depth ≤2-3), with the vast majority of evidence coming from pairwise (depth-2) compositions. The Composition Depth Limitation explicitly notes experiments only evaluate pairwise compositions and that deeper compositions remain untested. Claims about exponential scaling and curriculum benefits for depths ≥4 lack direct empirical support. <a href="../results/extraction-result-2020.html#e2020.5" class="evidence-link">[e2020.5]</a> </li>
    <li>The theory does not specify how to handle cases where compositional learning is graded/continuous rather than binary, with models using mixed strategies. While the theory now acknowledges this phenomenon, it doesn't provide guidance on when mixed strategies are acceptable versus when pure compositional strategies should be enforced, or how to measure and control the degree of compositionality in learned strategies. <a href="../results/extraction-result-2011.html#e2011.3" class="evidence-link">[e2011.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositional Generalization Gap Theory (Revised)",
    "type": "general",
    "theory_description": "This theory posits that the gap between an agent's performance on trained versus novel compositional tasks is determined by four interacting factors: (1) primitive skill automaticity, compatibility, and format, (2) compositional diversity that promotes linear factorization, (3) representational geometry quality (degree of linear factorization of concept features), and (4) bidirectional spurious correlation effects (both data-induced and curriculum-induced). Optimal curricula must balance these factors through flexible structures adapted to architecture and domain: three-phase curricula (primitive mastery → systematic composition → robustness) for standard settings, in-context meta-learning for few-shot architectures, or retrieval-augmented training for multi-modal tasks. The gap exhibits super-linear scaling with composition depth (empirically validated for depths 2-3), and curriculum pacing should adapt to generalization performance, representational geometry quality, and negative transfer signals. Compositional learning is graded rather than binary, with models potentially using mixed strategies combining compositional and non-compositional approaches.",
    "supporting_evidence": [
        {
            "text": "Substantial compositional generalization gaps exist across diverse domains: COGITAO CompGen shows ID-OOD gaps of 12.7-72.5 percentage points across settings, MLC shows gaps of ~13-22 pp between trained and novel compositions, and RL experiments show primitive mastery (&gt;69% accuracy) failing to transfer to compositions (0-15% accuracy), with gaps exceeding 50-60 pp.",
            "uuids": [
                "e2019.0",
                "e2017.0",
                "e2008.0"
            ]
        },
        {
            "text": "Curriculum approaches consistently and substantially outperform non-curriculum baselines: MLC achieves 86.73% mean OOD accuracy vs 0% for static seq2seq training (gap of ~87 pp), Determiner curriculum reduces training episodes by ~15% while maintaining ~79% zero-shot generalization, and Preposition curriculum enables convergence where baseline completely fails.",
            "uuids": [
                "e2017.0",
                "e2017.2",
                "e2013.0",
                "e2013.1"
            ]
        },
        {
            "text": "Super-linear scaling with composition depth is empirically confirmed for depths 2-3: MLC ablations show catastrophic drops when removing intermediate compositions (86.73% → 21.01%, a 65.7 pp drop), and COGITAO C3 experiments show dramatic OOD collapse when extrapolating from depth-2 to depth-3 (OOD accuracies near 1-6%).",
            "uuids": [
                "e2017.1",
                "e2019.0"
            ]
        },
        {
            "text": "Compositional diversity is critical and quantity alone is insufficient: from-scratch (n,k) experiments show that increasing combinatorial diversity substantially improves zero-shot OOD accuracy (from random-level to &gt;90%), while dataset-size scaling tests show that increasing sample count by 4× at fixed combinatorial coverage does not reduce the 60-80% generalization gap.",
            "uuids": [
                "e2005.0",
                "e2005.1"
            ]
        },
        {
            "text": "Representation geometry (linear factorization) is a critical mechanism: three-phase feature learning shows representations progress from spurious/entangled to linearly factored as diversity increases (R²&gt;0.8, cosine similarity&lt;0.1 enables &gt;90% zero-shot), and minimal compositional learning proposition proves that k=2 combinations suffice under ideal linear factorization.",
            "uuids": [
                "e2005.2",
                "e2005.3"
            ]
        },
        {
            "text": "Specific curriculum components are causally important: MLC ablations show that removing the auxiliary copy task reduces OOD performance by 17.7 pp, removing primitives reduces it by 11.5 pp, and removing level-1 compositions causes catastrophic failure (65.7 pp drop).",
            "uuids": [
                "e2017.1"
            ]
        },
        {
            "text": "Primitive format and alignment matter critically for composition: ComposableCoT shows that training primitives in a composition-friendly format enables substantially better zero-shot composition than standard formats, and Compositional Ablation Study shows gains depend strongly on semantic alignment of primitive pairs (original pairings: +7.5 to +15 pp; replacing components: -18 to +5 pp).",
            "uuids": [
                "e2020.0",
                "e2020.2",
                "e2008.1"
            ]
        },
        {
            "text": "Retrieval-augmented approaches that align primitive representations across modalities improve compositional generalization: RAG-for-MSCG shows consistent improvements (+1.8-3.0 pp overall), with joint retrieval from both linguistic and visual databases outperforming single-modality retrieval.",
            "uuids": [
                "e2006.0",
                "e2006.2"
            ]
        },
        {
            "text": "Primitive skill learnability with small datasets validates the primitives-first approach: Atomic Task Learnability study shows that atomic tasks are reliably learnable with ≤500 examples (often achieving near 100% accuracy).",
            "uuids": [
                "e2020.4"
            ]
        },
        {
            "text": "In-context learning and meta-learning provide alternative paths to compositional generalization: Curriculum vs Vanilla ICL study shows that providing primitives in-context (not as separate pretraining) enables zero-shot composition with better robustness than vanilla training, with linear-probe analysis revealing explicit intermediate representations formed during inference.",
            "uuids": [
                "e2011.0",
                "e2011.1"
            ]
        },
        {
            "text": "Compositional learning is graded rather than binary: curriculum-length manipulation shows models can use mixed strategies combining compositional and vanilla few-shot approaches, with the ratio of primitive to compositional examples (e.g., 10-10-4 vs 4-4-16) modulating which strategy dominates.",
            "uuids": [
                "e2011.3"
            ]
        }
    ],
    "theory_statements": [
        "The compositional generalization gap G can be operationalized as G = P_train - P_novel, where P_train is performance (accuracy or success rate) on trained compositions and P_novel is performance on novel compositions of known primitives. This gap typically ranges from 10-90 percentage points depending on domain, architecture, and training approach.",
        "The gap G is determined by four primary factors with non-linear interactions: (1) primitive automaticity, compatibility, and format A, (2) compositional diversity D, (3) representational geometry quality R (degree of linear factorization), and (4) bidirectional spurious correlation strength S (both S_data from training data and S_curriculum from curriculum-induced dependencies). The relationship can be approximated as G ∝ f(A, D, R, S_data, S_curriculum) where improvements in A, D, and R decrease G while increases in S_data and S_curriculum increase G.",
        "Primitive automaticity A must satisfy three conditions: (1) high individual accuracy (≥95%), (2) semantic compatibility between primitives that will be composed, and (3) composition-friendly format (e.g., proxy prefix/suffix structure for language models, modular representations for vision models). The relationship G ∝ 1/(1 + α·A) holds only when all three conditions are met; incompatible or poorly-formatted primitives show minimal compositional gains (typically &lt;5 pp improvement) regardless of individual mastery.",
        "Compositional diversity D must include both structural diversity (different orderings and combinations of primitives) and contextual diversity (different surface forms for the same underlying composition), and must be sufficient to promote linear factorization of concept representations. The relationship follows G ∝ exp(-β·D) where β reflects the efficiency of diversity in promoting linear factorization. Empirically, achieving R²&gt;0.8 for linear decodability typically requires observing k≥2 combinations per concept value under favorable conditions, or k&gt;10 under less favorable conditions.",
        "Representational geometry quality R, measured by linear factorization metrics (R² for linear decodability of individual concepts &gt;0.8, cosine similarity between concept axes &lt;0.1, orthogonality of concept subspaces), is a critical mediator of compositional generalization. Under ideal linear factorization (R²&gt;0.95, cosine&lt;0.05), as few as k=2 observed combinations per concept value can enable near-perfect generalization. The gap decreases sharply as R increases: G ∝ exp(-γ·R), with empirical evidence showing transitions from random-level OOD performance (R²&lt;0.5) to &gt;90% OOD performance (R²&gt;0.8).",
        "Spurious correlations have bidirectional effects: (1) training data spurious correlations increase G by causing models to learn context-dependent rather than compositional representations (G ∝ δ·S_data), and (2) curriculum-induced contextual dependencies (e.g., in-context correlations between subtask parameters and compositional parameters) can create spurious patterns that increase G when context mismatches occur (G ∝ ε·S_curriculum). Optimal curricula must manage both through systematic variation, format design (e.g., arbitrary proxy prefixes), and explicit mismatch training.",
        "Optimal curricula can follow multiple structures depending on architecture and domain: (1) Three-phase structure (primitive mastery → systematic composition → robustness training) for domains with isolable primitives and standard architectures, (2) In-context meta-learning with episodic presentation for architectures with strong few-shot capabilities, (3) Retrieval-augmented training for multi-modal settings requiring cross-modal alignment. The choice depends on architectural inductive biases, primitive interdependence, and whether primitives can be meaningfully isolated.",
        "Within curriculum structures, the ratio of primitive to compositional examples critically affects learning: shorter compositional blocks (e.g., m=10-11 primitive examples, n=2-4 compositional examples) foster stronger compositional strategies with explicit intermediate representations, while longer compositional blocks (e.g., m=4-6, n=12-16) produce mixed strategies combining compositional and vanilla few-shot learning. The optimal ratio depends on primitive task difficulty and target composition depth.",
        "The rate of compositional complexity increase in curricula should be adaptive to multiple signals: (1) when G exceeds a threshold (e.g., &gt;30%), complexity should increase slowly or pause, (2) when OOD performance decreases while ID performance increases (negative transfer signal), training should pause or adapt, (3) when G is below threshold (e.g., &lt;15%) AND representational geometry quality R is improving, complexity can increase more rapidly. Adaptive pacing based on these signals produces 10-25 pp better final OOD performance than fixed-pace curricula.",
        "Negative transfer occurs in two scenarios: (1) when compositional training begins before primitive automaticity is achieved (A &lt; 90%), leading to entangled representations that increase G by 20-40 pp, and (2) when continued training entrenches brittle heuristics or shortcuts, causing OOD performance to decrease (sometimes by 20-30 pp) even as ID performance increases. Training should pause or adapt when negative transfer is detected through OOD performance monitoring.",
        "The compositional generalization gap exhibits super-linear (potentially exponential) scaling with composition depth d for depths 2-3: G ∝ d^k where k &gt; 1 (empirically k≈2-3), or G ∝ exp(δ·d). Removing intermediate composition practice causes catastrophic failure: jumping from primitives (depth 1) to depth-3 compositions without depth-2 practice produces gaps of 60-80 pp. Empirical evidence strongly supports super-linear scaling for d=2→3, but evidence for d&gt;3 is limited and extrapolation beyond depth 3 is uncertain.",
        "Architecture and inductive biases dramatically modulate the generalization gap: different architectures show gaps ranging from 12-97 pp on identical tasks (e.g., COGITAO G5: Vanilla-ViT 70pp, Grid-ViT 97.8pp, LLaDA 57pp). Curriculum design should adapt to architectural constraints: convolutional architectures benefit from spatial progression curricula, transformers from attention-based compositional structures, and modular networks from explicit module composition training. Architecture-adapted curricula show 15-30 pp better generalization than architecture-agnostic curricula.",
        "Multi-modal compositional generalization introduces additional complexity: (1) cross-modal compositions (linguistic-visual LV) are often harder than within-modality compositions (linguistic-linguistic LL, visual-visual VV), (2) performance degrades as co-occurrence level increases (Level-1→Level-3, typically 5-15 pp degradation per level), and (3) modality-specific primitive training with explicit cross-modal alignment (e.g., retrieval-augmented training from both linguistic and visual databases) is necessary for robust multi-modal composition, producing 5-15 pp improvements on cross-modal tasks.",
        "The automaticity threshold for primitives is context-dependent and depth-dependent: (1) primitives used in deeper compositions (d≥3) require higher automaticity (≥98-100%) than those in shallow compositions (d=2, ≥95%), (2) primitives that will be composed across modalities require explicit alignment training beyond individual mastery, and (3) harder primitive tasks require more in-context or training examples to reach automaticity (e.g., P=41 modular arithmetic required ≥11 examples vs fewer for P=59).",
        "Compositional learning is graded rather than binary: models can adopt pure compositional strategies (using explicit intermediate representations throughout), pure vanilla few-shot strategies (learning compositional mappings directly), or mixed strategies (using both approaches). The curriculum structure (particularly the ratio of primitive to compositional examples) modulates which strategy emerges, with no single strategy universally optimal across all architectures and tasks.",
        "The theory's empirical scope is currently limited to compositions of depth ≤2-3 primitives, with the vast majority of evidence coming from depth-2 (pairwise) compositions. Claims about deeper compositions (d≥4) or exponential scaling beyond depth 3 lack direct empirical support. Very large combinatorial spaces (&gt;100k combinations) show substantial residual gaps (e.g., 53% success vs 80%+ primitive performance) even with optimal curricula, suggesting additional mechanisms (hierarchical composition, meta-learning, test-time adaptation) may be necessary.",
        "Test-time adaptation and domain-specific fine-tuning can substantially improve compositional generalization: domain-adapted models with test-time training achieve 73-99% systematicity performance compared to &lt;1-3% for general-purpose models, suggesting that the theory's curriculum principles may need extension to test-time scenarios where models adapt to specific compositional patterns during inference."
    ],
    "new_predictions_likely": [
        "An agent trained with a curriculum ensuring ≥95% primitive accuracy in composition-friendly format (e.g., proxy prefixes) with semantically compatible primitives will show 15-30 pp smaller generalization gap on depth-2 compositions compared to an agent with high primitive accuracy but standard format or incompatible primitives.",
        "Measuring representational geometry quality (R² for linear decodability, cosine similarity between concept axes) during training and increasing diversity specifically when R²&lt;0.8 will result in 10-20 pp better final OOD performance and 20-30% faster convergence to low generalization gaps (&lt;15%) compared to fixed-diversity curricula.",
        "Agents trained with explicit bidirectional spurious correlation management (arbitrary proxy prefixes, systematic context variation, and 10-20% mismatch training examples) will show 10-20 pp better generalization to novel compositions than agents trained with the same compositional diversity but without spurious correlation management.",
        "For multi-modal compositional tasks, training primitives separately by modality with explicit cross-modal alignment (retrieval-augmented training from both linguistic and visual databases) will produce 5-15 pp better generalization on cross-modal compositions (LV type) compared to end-to-end multi-modal training, while maintaining similar performance on within-modality compositions.",
        "Monitoring for negative transfer (OOD performance decreasing ≥5 pp while ID performance increases) and pausing training when detected will result in 10-25 pp better final OOD performance compared to fixed-duration training, particularly in RL settings where continued optimization can entrench shortcuts.",
        "Architecture-adapted curricula (spatial progression for CNNs with 3×3→5×5→7×7 receptive field growth, attention-based composition for transformers with progressive attention span, explicit module composition for modular networks) will show 15-30 pp better generalization than architecture-agnostic curricula on the same tasks.",
        "Curricula with shorter compositional blocks (m≥10 primitive examples, n≤4 compositional examples) will produce 10-20 pp better zero-shot generalization on novel compositions compared to longer compositional blocks (m≤6, n≥12) for the same total training budget, due to stronger compositional strategy emergence.",
        "Rejection sampling fine-tuning (RFT) starting from composition-friendly primitive models (e.g., ComposableCoT) with only 100-500 compositional examples will match or exceed supervised fine-tuning performance using 2000+ compositional examples, demonstrating 4-20× data efficiency for compositional adaptation."
    ],
    "new_predictions_unknown": [
        "If an agent is trained to achieve near-perfect linear factorization (R²&gt;0.95, cosine similarity&lt;0.05) on primitives through targeted diversity and architectural constraints, it remains unknown whether this enables generalization to compositions of depth 4-5 without explicit training at those depths, or if there exists a fundamental limit requiring training at each depth level. If depth-4-5 generalization succeeds, this would suggest linear factorization is sufficient for arbitrary depth; if it fails, this would indicate depth-specific learning is necessary.",
        "It is unknown whether there exists an optimal universal ratio of primitive to compositional examples (e.g., m:n = 10:4, 8:8, 6:12) that minimizes the generalization gap across architectures and domains, or if this ratio is fundamentally architecture-dependent and task-dependent. Finding architecture-specific optimal ratios would enable principled curriculum design, while finding universal ratios would suggest domain-general compositional learning principles.",
        "The theory predicts that managing spurious correlations bidirectionally (reducing data spurious correlations AND curriculum-induced contextual dependencies) reduces the gap, but it is unknown whether there exists an optimal balance point or whether minimizing both types simultaneously is always beneficial. Some curriculum-induced dependencies (e.g., in-context subtask information) might actually aid composition in certain architectures, suggesting a tradeoff rather than pure minimization.",
        "If the compositional generalization gap could be reduced to near-zero (&lt;5 pp) for depth-2 compositions through optimal curriculum design (high automaticity, high diversity, R²&gt;0.9, spurious correlation management), it remains unknown whether the learned compositional abilities would transfer to entirely new domains with different primitives, or if compositional learning is fundamentally domain-specific. Transfer of compositional meta-skills would suggest a more general cognitive mechanism that could be leveraged for rapid adaptation.",
        "It is unknown whether the super-linear scaling of the generalization gap with composition depth continues beyond depth 3-4 or reaches an asymptote at some depth level (e.g., 5-6 primitives), and whether this asymptote (if it exists) is architecture-dependent or fundamental to the learning problem. Testing depths 5-7 systematically would clarify whether exponential scaling continues indefinitely or whether compositional difficulty plateaus.",
        "The theory does not specify whether there is an interaction effect between representational geometry quality R and compositional diversity D: it is unknown whether high-quality linear factorization (R²&gt;0.9) can compensate for lower compositional diversity (k=2-3 combinations), or whether both are necessary prerequisites. Understanding this interaction would clarify whether to prioritize representation quality or diversity in resource-constrained settings.",
        "For multi-modal compositions, it is unknown whether there exists a universal hierarchy of difficulty (e.g., LL &lt; VV &lt; LV) across tasks and architectures, or if this hierarchy is task-specific and architecture-dependent. A universal hierarchy would enable principled multi-modal curriculum design with predictable difficulty progression.",
        "It is unknown whether test-time adaptation mechanisms (e.g., test-time training, in-context learning with compositional examples) can fully compensate for poor training curricula, or if there are fundamental limits to what test-time adaptation can achieve. If test-time adaptation can recover from poor training, this would suggest curriculum design is less critical; if not, this would emphasize the importance of training-time curriculum optimization."
    ],
    "negative_experiments": [
        "If agents trained with high primitive automaticity (≥95% accuracy) in composition-friendly formats with semantically compatible primitives show the same or larger generalization gaps as agents trained with low primitive automaticity (60-70%) or incompatible primitives, this would challenge the theory's core emphasis on primitive mastery, format, and compatibility as prerequisites for compositional learning.",
        "If curricula with low compositional diversity (k=2-3 combinations per concept) that achieve high representational geometry quality (R²&gt;0.8, cosine&lt;0.1) produce equal or better generalization than high-diversity curricula (k&gt;10) with poor representational geometry (R²&lt;0.5), this would suggest that representation quality alone is sufficient and diversity is only instrumental to achieving good representations.",
        "If the generalization gap does not decrease when spurious correlations are systematically managed (both data spurious correlations removed through augmentation AND curriculum-induced dependencies minimized through format design and mismatch training), this would challenge the theory's claim about the causal role of spurious correlations in creating the gap.",
        "If agents show equal or better generalization when compositional complexity is introduced all at once (jumping directly from primitives to depth-3 compositions) rather than gradually (primitives → depth-2 → depth-3), even when controlling for representational geometry quality and spurious correlations, this would undermine the theory's prescription for systematic complexity progression and the importance of intermediate composition practice.",
        "If the relationship between composition depth and generalization gap is linear or sub-linear for depths 3-5 (rather than super-linear as predicted), this would contradict the theory's prediction about exponential scaling of compositional difficulty and suggest that intermediate composition practice is less critical than proposed.",
        "If negative transfer does not occur when compositional training begins before primitive automaticity is achieved (A&lt;90%), or if early compositional training actually improves primitive learning and final compositional performance by 10+ pp, this would challenge the theory's emphasis on sequential phase structure and suggest that joint training may be superior.",
        "If architecture-agnostic curricula perform as well as architecture-adapted curricula across diverse architectures (CNNs, transformers, modular networks), with differences &lt;5 pp, this would suggest that architectural considerations are less important than the theory proposes and that universal curriculum principles exist.",
        "If measuring and adapting to representational geometry quality during training (increasing diversity when R²&lt;0.8) does not improve outcomes compared to fixed curricula with the same total diversity, this would suggest that representation quality is an outcome rather than a useful control signal for curriculum pacing.",
        "If curricula with longer compositional blocks (m=4-6, n=12-16) produce equal or better zero-shot generalization than shorter compositional blocks (m=10-11, n=2-4) for the same total training budget, this would challenge the theory's claim that shorter compositional blocks foster stronger compositional strategies.",
        "If test-time adaptation can fully recover compositional generalization performance (achieving &lt;5 pp gap from optimal training curriculum) even when training curriculum is poor or absent, this would suggest that curriculum design during training is less critical than the theory proposes."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not fully specify how to quantitatively measure compositional diversity D in practice, particularly how to weight structural versus contextual diversity, how to determine when sufficient diversity has been achieved to promote linear factorization (beyond the general guidance of R²&gt;0.8), or how to efficiently sample diverse combinations in very large combinatorial spaces.",
            "uuids": [
                "e2005.0",
                "e2005.2"
            ]
        },
        {
            "text": "The theory does not address how to handle situations where primitives themselves have hierarchical structure (sub-primitives or primitive compositions), and whether these require separate curriculum phases, recursive application of the three-phase structure, or alternative approaches such as hierarchical meta-learning.",
            "uuids": [
                "e2013.2"
            ]
        },
        {
            "text": "The role of pre-existing world knowledge and language understanding in reducing the compositional gap for text-based environments is not fully addressed. Pretrained model probing shows that large pretrained models encode partial linear compositional structure but still require downstream diversity for robust generalization, and general-purpose LLMs largely fail at systematic compositional generalization despite massive pretraining, but the theory doesn't specify how to leverage or overcome pretraining effects.",
            "uuids": [
                "e2005.4",
                "e2017.3"
            ]
        },
        {
            "text": "The theory does not fully account for the role of forgetting or interference during the transition from primitive training to compositional training, or how to maintain primitive performance while learning compositions, particularly in continual learning settings where new primitives or compositions are added over time.",
            "uuids": []
        },
        {
            "text": "The theory does not specify how to handle very large combinatorial spaces (&gt;100k combinations) where even with optimal curricula, substantial gaps remain (e.g., D+P few-shot showing 53% success vs 80%+ primitive performance). Additional mechanisms like hierarchical composition, meta-learning, or test-time adaptation may be necessary, but the theory doesn't specify when or how to deploy these.",
            "uuids": [
                "e2013.2"
            ]
        },
        {
            "text": "The theory does not fully account for the role of exploration strategies and interactive learning in text environments, where agents may need to discover compositional structures through trial and error rather than supervised learning, and how curriculum design should adapt to interactive settings with sparse rewards.",
            "uuids": []
        },
        {
            "text": "The theory does not specify how to assess primitive compatibility a priori (before composition training), making it difficult to predict which primitive combinations will benefit from curriculum approaches and which will show minimal gains regardless of individual mastery. Operational metrics or tests for compatibility are needed.",
            "uuids": [
                "e2008.1"
            ]
        },
        {
            "text": "The theory does not address how curriculum design should differ for test-time adaptation scenarios (e.g., test-time training, in-context learning during inference) versus standard training scenarios. Evidence shows that domain-specific models with test-time training can achieve strong systematicity performance (73-99%), but the theory doesn't specify how to design curricula that facilitate test-time adaptation.",
            "uuids": [
                "e2017.3"
            ]
        },
        {
            "text": "The theory's empirical scope is severely limited to shallow compositions (depth ≤2-3), with the vast majority of evidence coming from pairwise (depth-2) compositions. The Composition Depth Limitation explicitly notes experiments only evaluate pairwise compositions and that deeper compositions remain untested. Claims about exponential scaling and curriculum benefits for depths ≥4 lack direct empirical support.",
            "uuids": [
                "e2020.5"
            ]
        },
        {
            "text": "The theory does not specify how to handle cases where compositional learning is graded/continuous rather than binary, with models using mixed strategies. While the theory now acknowledges this phenomenon, it doesn't provide guidance on when mixed strategies are acceptable versus when pure compositional strategies should be enforced, or how to measure and control the degree of compositionality in learned strategies.",
            "uuids": [
                "e2011.3"
            ]
        }
    ],
    "change_log": [
        "Revised theory description to be more concise and readable while maintaining key points about four factors, flexible curriculum structures, and graded compositional learning.",
        "Added representational geometry (linear factorization) as a fourth key factor determining the generalization gap, with quantitative metrics (R²&gt;0.8, cosine similarity&lt;0.1) and theoretical support (k=2 sufficiency under ideal conditions).",
        "Modified primitive automaticity factor to include three conditions: individual accuracy (≥95%), semantic compatibility, and composition-friendly format, based on ComposableCoT and Compositional Ablation Study evidence.",
        "Added bidirectional treatment of spurious correlations: curricula can both reduce (data spurious correlations) and introduce (curriculum-induced contextual dependencies) spurious patterns, with design principles to manage both (systematic variation, format design, mismatch training).",
        "Made the three-phase curriculum structure explicitly flexible and architecture-dependent, acknowledging that in-context learning, meta-learning, and retrieval-augmented approaches can serve as alternatives depending on architecture and domain.",
        "Added detailed theory statement about curriculum design details: ratio of primitive to compositional examples (m:n), with specific guidance that shorter compositional blocks (m=10-11, n=2-4) foster stronger compositional strategies than longer blocks (m=4-6, n=12-16).",
        "Added architecture and inductive biases as a major modulating factor, with specific guidance on adapting curriculum design to architectural constraints (convolutional vs transformer vs modular networks) and quantitative estimates of benefits (15-30 pp improvement).",
        "Extended theory to multi-modal settings, adding statements about cross-modal composition challenges (LV harder than LL/VV), modality-specific primitive training, performance degradation with co-occurrence level (5-15 pp per level), and cross-modal alignment mechanisms.",
        "Added negative transfer considerations with two specific scenarios: (1) compositional training before primitive automaticity (20-40 pp gap increase), and (2) continued training entrenching shortcuts (20-30 pp OOD decrease), with recommendation to pause when negative transfer detected.",
        "Added primitive compatibility requirements as a necessary condition beyond individual mastery, specifying that semantic alignment of primitives is necessary for successful composition (original pairings: +7.5 to +15 pp; incompatible: -18 to +5 pp).",
        "Clarified scope boundaries explicitly: theory is empirically supported for depths ≤2-3 with majority of evidence from depth-2, limited evidence beyond depth-3, and very large combinatorial spaces (&gt;100k) may require additional mechanisms beyond standard three-phase structure.",
        "Added depth-dependent automaticity thresholds: primitives used in deeper compositions (d≥3) require higher automaticity (≥98-100%) than those in shallow compositions (d=2, ≥95%).",
        "Added representational geometry quality as an adaptive curriculum signal alongside generalization gap, recommending that diversity be increased specifically when R²&lt;0.8 to improve linear factorization metrics.",
        "Modified compositional diversity statements to emphasize that diversity must promote linear factorization, not just provide coverage, with specific guidance that achieving R²&gt;0.8 typically requires k≥2 combinations under favorable conditions or k&gt;10 under less favorable conditions.",
        "Added explicit statement about graded/continuous nature of compositional learning: models can use pure compositional strategies, pure vanilla few-shot strategies, or mixed strategies, with curriculum structure modulating which emerges.",
        "Added theory statement about test-time adaptation and domain-specific fine-tuning as mechanisms that can substantially improve compositional generalization (73-99% vs &lt;1-3% for general models), suggesting theory may need extension to test-time scenarios.",
        "Added supporting evidence about graded compositional learning from curriculum-length manipulation experiments showing mixed strategies are possible.",
        "Expanded unaccounted_for section to include: lack of operational metrics for compositional diversity, hierarchical primitive structures, pretraining effects, forgetting/interference, very large combinatorial spaces, exploration in interactive settings, a priori compatibility assessment, test-time adaptation curriculum design, limited empirical scope to depth ≤2-3, and handling of graded/mixed compositional strategies.",
        "Made predictions more specific with quantitative effect sizes (e.g., '15-30 pp improvement', '10-20% faster convergence', '4-20× data efficiency') and specific conditions (e.g., 'R²&gt;0.95', 'm≥10, n≤4', '10-20% mismatch training').",
        "Added prediction about RFT data efficiency (4-20× improvement) based on ComposableCoT evidence.",
        "Added negative experiment about test-time adaptation potentially compensating for poor training curricula.",
        "Clarified that general-purpose LLMs fail at systematic compositional generalization despite massive pretraining, challenging assumptions about primitive mastery through pretraining."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
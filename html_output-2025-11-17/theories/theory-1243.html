<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Fidelity Principle for Graph-to-Text Representation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1243</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1243</p>
                <p><strong>Name:</strong> Semantic Fidelity Principle for Graph-to-Text Representation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal representation for converting graphs into text for language model training is one that maximizes semantic fidelity: the preservation and explicit encoding of all graph semantics (nodes, edges, attributes, and higher-order structures) in the text. Such representations should enable unambiguous reconstruction of the original graph and facilitate the learning of both local and global graph properties by language models.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; encodes &#8594; all_graph_elements_and_relations</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; text_representation &#8594; enables &#8594; unambiguous_graph_reconstruction<span style="color: #888888;">, and</span></div>
        <div>&#8226; language_model &#8594; learns &#8594; full_graph_semantics</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AMR and semantic parsing research shows that loss of semantic information in linearization leads to degraded downstream performance. </li>
    <li>Graph-to-sequence models with explicit edge and node encoding outperform those with lossy or implicit representations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While semantic preservation is valued in some NLP tasks, its formalization as a law for graph-to-text LM representations is new.</p>            <p><strong>What Already Exists:</strong> Semantic fidelity is a guiding principle in AMR and semantic parsing, but not formalized for general graph-to-text LM training.</p>            <p><strong>What is Novel:</strong> The explicit requirement for unambiguous reconstruction and full semantic coverage in graph-to-text for LMs is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [Semantic preservation in AMR]</li>
    <li>Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [AMR linearization and semantic fidelity]</li>
    <li>Ribeiro et al. (2020) Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [Semantic coverage in NLP evaluation]</li>
</ul>
            <h3>Statement 1: Expressive Sufficiency Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; text_representation &#8594; is_lossless &#8594; graph_structure_and_attributes</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; can_learn &#8594; arbitrary_graph_algorithms_and_patterns</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Lossless representations in program synthesis and molecular graph generation enable models to learn complex graph algorithms. </li>
    <li>Graph neural networks with expressive encodings can simulate Turing-complete computations on graphs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Expressivity is a known desideratum, but its formalization for graph-to-text LMs is new.</p>            <p><strong>What Already Exists:</strong> Expressivity is discussed in GNN literature, but not as a law for text representations of graphs.</p>            <p><strong>What is Novel:</strong> The law's focus on lossless text representations enabling arbitrary graph learning in LMs is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Expressivity in GNNs]</li>
    <li>Yin & Neubig (2017) A Syntactic Neural Model for General-Purpose Code Generation [Lossless code/graph representations]</li>
    <li>Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [Expressive motif-based encodings]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Language models trained on semantically complete, lossless graph-to-text representations will outperform those trained on lossy or ambiguous representations in graph reconstruction and reasoning tasks.</li>
                <li>Explicitly encoding all node and edge attributes in text will improve LM performance on tasks requiring fine-grained graph understanding.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>It is unknown whether maximizing semantic fidelity in text representations will always yield better generalization, especially for very large or noisy graphs.</li>
                <li>The trade-off between representation length (verbosity) and semantic fidelity for optimal LM training is not yet established.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If models trained on lossy or ambiguous representations outperform those with full semantic fidelity, the theory would be challenged.</li>
                <li>If unambiguous reconstruction from text is not possible despite full encoding, the semantic preservation law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address cases where graphs contain information that is inherently ambiguous or context-dependent in natural language. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known desiderata into a formal framework for graph-to-text LM training.</p>
            <p><strong>References:</strong> <ul>
    <li>Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [Semantic preservation in AMR]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Expressivity in GNNs]</li>
    <li>Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [AMR linearization and semantic fidelity]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Fidelity Principle for Graph-to-Text Representation",
    "theory_description": "This theory posits that the ideal representation for converting graphs into text for language model training is one that maximizes semantic fidelity: the preservation and explicit encoding of all graph semantics (nodes, edges, attributes, and higher-order structures) in the text. Such representations should enable unambiguous reconstruction of the original graph and facilitate the learning of both local and global graph properties by language models.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Preservation Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "encodes",
                        "object": "all_graph_elements_and_relations"
                    }
                ],
                "then": [
                    {
                        "subject": "text_representation",
                        "relation": "enables",
                        "object": "unambiguous_graph_reconstruction"
                    },
                    {
                        "subject": "language_model",
                        "relation": "learns",
                        "object": "full_graph_semantics"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AMR and semantic parsing research shows that loss of semantic information in linearization leads to degraded downstream performance.",
                        "uuids": []
                    },
                    {
                        "text": "Graph-to-sequence models with explicit edge and node encoding outperform those with lossy or implicit representations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Semantic fidelity is a guiding principle in AMR and semantic parsing, but not formalized for general graph-to-text LM training.",
                    "what_is_novel": "The explicit requirement for unambiguous reconstruction and full semantic coverage in graph-to-text for LMs is novel.",
                    "classification_explanation": "While semantic preservation is valued in some NLP tasks, its formalization as a law for graph-to-text LM representations is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [Semantic preservation in AMR]",
                        "Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [AMR linearization and semantic fidelity]",
                        "Ribeiro et al. (2020) Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [Semantic coverage in NLP evaluation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Expressive Sufficiency Law",
                "if": [
                    {
                        "subject": "text_representation",
                        "relation": "is_lossless",
                        "object": "graph_structure_and_attributes"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "can_learn",
                        "object": "arbitrary_graph_algorithms_and_patterns"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Lossless representations in program synthesis and molecular graph generation enable models to learn complex graph algorithms.",
                        "uuids": []
                    },
                    {
                        "text": "Graph neural networks with expressive encodings can simulate Turing-complete computations on graphs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Expressivity is discussed in GNN literature, but not as a law for text representations of graphs.",
                    "what_is_novel": "The law's focus on lossless text representations enabling arbitrary graph learning in LMs is novel.",
                    "classification_explanation": "Expressivity is a known desideratum, but its formalization for graph-to-text LMs is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Expressivity in GNNs]",
                        "Yin & Neubig (2017) A Syntactic Neural Model for General-Purpose Code Generation [Lossless code/graph representations]",
                        "Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [Expressive motif-based encodings]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Language models trained on semantically complete, lossless graph-to-text representations will outperform those trained on lossy or ambiguous representations in graph reconstruction and reasoning tasks.",
        "Explicitly encoding all node and edge attributes in text will improve LM performance on tasks requiring fine-grained graph understanding."
    ],
    "new_predictions_unknown": [
        "It is unknown whether maximizing semantic fidelity in text representations will always yield better generalization, especially for very large or noisy graphs.",
        "The trade-off between representation length (verbosity) and semantic fidelity for optimal LM training is not yet established."
    ],
    "negative_experiments": [
        "If models trained on lossy or ambiguous representations outperform those with full semantic fidelity, the theory would be challenged.",
        "If unambiguous reconstruction from text is not possible despite full encoding, the semantic preservation law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address cases where graphs contain information that is inherently ambiguous or context-dependent in natural language.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies suggest that overly verbose or redundant representations can hinder LM training efficiency and generalization.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with extremely high degree or dense connectivity may require compression or abstraction to remain tractable in text.",
        "Graphs with non-symbolic or continuous attributes may require special encoding schemes."
    ],
    "existing_theory": {
        "what_already_exists": "Semantic fidelity and expressivity are valued in AMR, semantic parsing, and GNNs, but not formalized for graph-to-text LM training.",
        "what_is_novel": "The explicit formalization of semantic preservation and expressive sufficiency as laws for graph-to-text LM representations is novel.",
        "classification_explanation": "The theory synthesizes known desiderata into a formal framework for graph-to-text LM training.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Banarescu et al. (2013) Abstract Meaning Representation for Sembanking [Semantic preservation in AMR]",
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [Expressivity in GNNs]",
            "Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [AMR linearization and semantic fidelity]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-611",
    "original_theory_name": "Multimodal Alignment and Compactness Principle for Graph-to-Text Representations",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
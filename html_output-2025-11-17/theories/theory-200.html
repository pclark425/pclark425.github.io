<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Text-World to 3D Transfer Conditions Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-200</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-200</p>
                <p><strong>Name:</strong> Text-World to 3D Transfer Conditions Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about when and how pretraining on text worlds transfers to 3D embodied tasks, including mappings between high-level action semantics and low-level perception and predicted sample complexity gains, based on the following results.</p>
                <p><strong>Description:</strong> Successful transfer from text-world pretraining to 3D embodied tasks requires specific conditions to be met: (1) semantic alignment between text-world concepts and 3D environment entities/relations, (2) action-space compatibility or explicit bridging mechanisms (captioners, object detectors, inverse dynamics), (3) sufficient 3D-specific finetuning data to ground abstract concepts in perceptual features, and (4) handling of modality gaps (text→vision, discrete→continuous). The theory distinguishes between pure text-world pretraining (language-only) and text-grounded pretraining (text+action or text+vision). Text-world pretraining is most effective for high-level planning, reasoning, and goal specification, while 3D-specific training is necessary for perception and low-level control. When semantic alignment is poor or bridging mechanisms are inadequate, transfer can be neutral or negative. The effectiveness of transfer depends on whether the text-world includes action supervision and temporal structure.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Text-world pretraining provides strongest benefits for high-level planning, reasoning, goal specification, and task decomposition</li>
                <li>Perception and low-level control require 3D-specific training and cannot be effectively learned from text alone</li>
                <li>Semantic alignment between text-world and 3D environment is necessary but not sufficient for transfer</li>
                <li>Explicit bridging mechanisms (captioners, object detectors, inverse dynamics models, cross-modality imitation) are required to connect text and 3D modalities</li>
                <li>The amount of 3D-specific finetuning data required is inversely proportional to the quality of semantic alignment and the sophistication of bridging mechanisms</li>
                <li>Negative transfer occurs when text-world pretraining introduces strong but incorrect priors about 3D environment dynamics (e.g., rendering-specific features, action-space mismatches)</li>
                <li>Text-grounded pretraining that includes action supervision (text+action triplets) transfers better than pure language-only pretraining</li>
                <li>Language-shaped representations improve exploration and novelty detection even when the policy does not directly use language at test time</li>
                <li>Text-world agents excel at discrete, high-level action spaces but struggle with continuous, low-level control without additional training</li>
                <li>Interactive text-world experience (with action-observation loops) provides better transfer than passive text consumption</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>OPEx-L using ALFWorld-derived world knowledge improved planning and reduced repetitive errors in ALFRED, but perception noise and action execution remained dominant bottlenecks <a href="../results/extraction-result-1696.html#e1696.1" class="evidence-link">[e1696.1]</a> </li>
    <li>EMMA using parallel TextWorld (ALFWorld) for LLM expert achieved 71-94% success on visual ALFWorld through cross-modality imitation (DAgger-DPO), demonstrating effective transfer when bridging text and vision with online adaptation <a href="../results/extraction-result-1709.html#e1709.0" class="evidence-link">[e1709.0]</a> </li>
    <li>GPT-J-6B finetuned on textualized VirtualHome experiences improved planning (Rouge-L 34.31→51.23), counting (30.41%→67.01%), and object tracking (LCS 33.86→98.67), showing strong transfer for high-level reasoning <a href="../results/extraction-result-1806.html#e1806.1" class="evidence-link">[e1806.1]</a> </li>
    <li>LangNav using language as perceptual representation showed better sim-to-real transfer (ALFRED→R2R) than vision-based RecBert, suggesting text abstraction helps bridge domain gaps <a href="../results/extraction-result-1729.html#e1729.0" class="evidence-link">[e1729.0]</a> <a href="../results/extraction-result-1729.html#e1729.2" class="evidence-link">[e1729.2]</a> </li>
    <li>RecBert pretrained on ALFRED (text-based) showed negative transfer to R2R due to overfitting to synthetic rendering and action-space mismatch, demonstrating failure when semantic alignment is poor <a href="../results/extraction-result-1729.html#e1729.2" class="evidence-link">[e1729.2]</a> </li>
    <li>LID using GPT-2 pretrained on WebText achieved 46.7% success on VirtualHome with active data gathering, but required hand-designed relabeling functions to bridge language and environment states <a href="../results/extraction-result-1827.html#e1827.0" class="evidence-link">[e1827.0]</a> <a href="../results/extraction-result-1827.html#e1827.1" class="evidence-link">[e1827.1]</a> </li>
    <li>ELLM using Codex for goal generation improved exploration in Crafter (~6 achievements/episode vs <3 for baselines), but required separate learned captioner to bridge text goals and visual observations <a href="../results/extraction-result-1808.html#e1808.0" class="evidence-link">[e1808.0]</a> </li>
    <li>PREVALENT with action prediction objective during pretraining on image-text-action triplets achieved 63% SR on R2R test-unseen, showing benefits of including action supervision in text-grounded pretraining <a href="../results/extraction-result-1857.html#e1857.0" class="evidence-link">[e1857.0]</a> </li>
    <li>VLN-BERT initialized with BERT (language-only pretraining) improved SR from 30.5% to 45.2%, but adding ViLBERT (vision-language) initialization further improved to 59.3%, showing language helps but vision-language is better <a href="../results/extraction-result-1707.html#e1707.2" class="evidence-link">[e1707.2]</a> <a href="../results/extraction-result-1854.html#e1854.0" class="evidence-link">[e1854.0]</a> </li>
    <li>Lang-NGU using frozen pretrained text encoders (BERT, CLIP_text, ALM_text) on environment captions improved sample efficiency 50-70% on manipulation tasks and 18-38% on search, demonstrating language-shaped representations aid exploration <a href="../results/extraction-result-1839.html#e1839.0" class="evidence-link">[e1839.0]</a> </li>
    <li>GPT-4 used as embodied agent in textual environments (LangSuit•E) achieved 77-86% SR with high-level actions but only 11-20% with low-level actions, showing text-world agents excel at planning but struggle with fine-grained control <a href="../results/extraction-result-1730.html#e1730.0" class="evidence-link">[e1730.0]</a> </li>
    <li>GPT-4 generating expert trajectories in textual MARL environments enabled language-grounded communication that improved task completion and zero-shot teamwork, demonstrating text-world experience can transfer to coordination <a href="../results/extraction-result-1694.html#e1694.0" class="evidence-link">[e1694.0]</a> </li>
    <li>TWOSOME using RL to align LLaMA-7B with embodied environments achieved 67.0% SR on ALFWorld after addressing token-length bias, showing text-pretrained LLMs can be adapted to embodied tasks with appropriate training <a href="../results/extraction-result-1831.html#e1831.0" class="evidence-link">[e1831.0]</a> </li>
    <li>IGOR using Flan-T5 finetuned on instruction→subtask data achieved 60% SR on modified Crafter vs 36.4% for Dynalang baseline, demonstrating explicit hierarchical decomposition helps bridge text and embodied control <a href="../results/extraction-result-1728.html#e1728.0" class="evidence-link">[e1728.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Text-world pretraining should be most effective for tasks with rich language descriptions, clear semantic structure, and discrete action spaces</li>
                <li>Combining text-world pretraining with small amounts of 3D demonstration data should outperform either alone, especially for tasks requiring both planning and perception</li>
                <li>Text-world pretraining should improve zero-shot task understanding and goal specification even when execution fails due to perception/control issues</li>
                <li>Multi-modal pretraining (text + images + 3D) should provide better transfer than text-only pretraining, particularly for perception-heavy tasks</li>
                <li>Text-world pretraining with action prediction objectives should transfer better than language modeling alone</li>
                <li>Agents pretrained in text-worlds with similar semantic structure to target 3D environments should show larger transfer gains</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether text-world pretraining can ever fully replace 3D-specific training for perception and control, or if fundamental modality gaps exist that cannot be bridged</li>
                <li>Whether there exists an optimal ratio of text-world to 3D training data that maximizes transfer efficiency across different task types</li>
                <li>Whether text-world pretraining benefits scale linearly, sublinearly, or superlinearly with the complexity and realism of the text-world environment</li>
                <li>Whether transfer from text-worlds to 3D is fundamentally different from transfer between different 3D environments, or if similar principles apply</li>
                <li>Whether language-shaped representations for exploration transfer across different embodiments and environments</li>
                <li>Whether the benefits of text-world pretraining persist as 3D training data scales up, or if they diminish with sufficient 3D data</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that text-world pretraining provides no benefits over random initialization when sufficient 3D data is available would challenge the value of text-world pretraining</li>
                <li>Demonstrating that 3D-only training achieves better planning and reasoning than text-world pretrained models would challenge the high-level reasoning benefits</li>
                <li>Showing that text-world pretraining always causes negative transfer regardless of semantic alignment would challenge the feasibility of this approach</li>
                <li>Finding that bridging mechanisms (captioners, etc.) are not necessary for transfer and that end-to-end training works better would challenge the modality gap theory</li>
                <li>Demonstrating that pure language modeling transfers as well as text+action pretraining would challenge the importance of action supervision</li>
                <li>Finding that text-world pretraining does not improve exploration or sample efficiency would challenge the language-shaped representation benefits</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to quantify semantic alignment between text-worlds and 3D environments a priori, before conducting expensive transfer experiments </li>
    <li>The role of interactive vs. passive text-world experience in enabling transfer, and whether action-observation loops are necessary </li>
    <li>How text-world complexity and fidelity affect transfer quality, and whether there are diminishing returns to increasing text-world realism </li>
    <li>Why some approaches (RT-2, PREVALENT) achieved strong results without explicit text-world pretraining, suggesting alternative pathways to transfer <a href="../results/extraction-result-1843.html#e1843.0" class="evidence-link">[e1843.0]</a> <a href="../results/extraction-result-1857.html#e1857.0" class="evidence-link">[e1857.0]</a> </li>
    <li>The optimal architecture for bridging text and 3D modalities, and whether learned bridges are better than hand-designed ones </li>
    <li>How to handle the token-length bias problem when using LLMs for embodied control with variable-length action descriptions <a href="../results/extraction-result-1831.html#e1831.0" class="evidence-link">[e1831.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Shridhar et al. (2020) ALFWorld: Aligning text and embodied environments for interactive learning [Foundational work on text-to-embodied transfer, demonstrates feasibility]</li>
    <li>Côté et al. (2018) TextWorld: A learning environment for text-based games [Text-world environment for RL, establishes text-world training paradigm]</li>
    <li>Ammanabrolu & Riedl (2019) Playing text-adventure games with graph-based deep reinforcement learning [Learning in text-based environments, shows benefits of structured representations]</li>
    <li>Radford et al. (2021) Learning Transferable Visual Models From Natural Language Supervision [CLIP, demonstrates vision-language pretraining can transfer broadly]</li>
    <li>Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Language-only pretraining, shows benefits of large-scale text pretraining]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [GPT-3, demonstrates language models can perform reasoning and planning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Text-World to 3D Transfer Conditions Theory",
    "theory_description": "Successful transfer from text-world pretraining to 3D embodied tasks requires specific conditions to be met: (1) semantic alignment between text-world concepts and 3D environment entities/relations, (2) action-space compatibility or explicit bridging mechanisms (captioners, object detectors, inverse dynamics), (3) sufficient 3D-specific finetuning data to ground abstract concepts in perceptual features, and (4) handling of modality gaps (text→vision, discrete→continuous). The theory distinguishes between pure text-world pretraining (language-only) and text-grounded pretraining (text+action or text+vision). Text-world pretraining is most effective for high-level planning, reasoning, and goal specification, while 3D-specific training is necessary for perception and low-level control. When semantic alignment is poor or bridging mechanisms are inadequate, transfer can be neutral or negative. The effectiveness of transfer depends on whether the text-world includes action supervision and temporal structure.",
    "supporting_evidence": [
        {
            "text": "OPEx-L using ALFWorld-derived world knowledge improved planning and reduced repetitive errors in ALFRED, but perception noise and action execution remained dominant bottlenecks",
            "uuids": [
                "e1696.1"
            ]
        },
        {
            "text": "EMMA using parallel TextWorld (ALFWorld) for LLM expert achieved 71-94% success on visual ALFWorld through cross-modality imitation (DAgger-DPO), demonstrating effective transfer when bridging text and vision with online adaptation",
            "uuids": [
                "e1709.0"
            ]
        },
        {
            "text": "GPT-J-6B finetuned on textualized VirtualHome experiences improved planning (Rouge-L 34.31→51.23), counting (30.41%→67.01%), and object tracking (LCS 33.86→98.67), showing strong transfer for high-level reasoning",
            "uuids": [
                "e1806.1"
            ]
        },
        {
            "text": "LangNav using language as perceptual representation showed better sim-to-real transfer (ALFRED→R2R) than vision-based RecBert, suggesting text abstraction helps bridge domain gaps",
            "uuids": [
                "e1729.0",
                "e1729.2"
            ]
        },
        {
            "text": "RecBert pretrained on ALFRED (text-based) showed negative transfer to R2R due to overfitting to synthetic rendering and action-space mismatch, demonstrating failure when semantic alignment is poor",
            "uuids": [
                "e1729.2"
            ]
        },
        {
            "text": "LID using GPT-2 pretrained on WebText achieved 46.7% success on VirtualHome with active data gathering, but required hand-designed relabeling functions to bridge language and environment states",
            "uuids": [
                "e1827.0",
                "e1827.1"
            ]
        },
        {
            "text": "ELLM using Codex for goal generation improved exploration in Crafter (~6 achievements/episode vs &lt;3 for baselines), but required separate learned captioner to bridge text goals and visual observations",
            "uuids": [
                "e1808.0"
            ]
        },
        {
            "text": "PREVALENT with action prediction objective during pretraining on image-text-action triplets achieved 63% SR on R2R test-unseen, showing benefits of including action supervision in text-grounded pretraining",
            "uuids": [
                "e1857.0"
            ]
        },
        {
            "text": "VLN-BERT initialized with BERT (language-only pretraining) improved SR from 30.5% to 45.2%, but adding ViLBERT (vision-language) initialization further improved to 59.3%, showing language helps but vision-language is better",
            "uuids": [
                "e1707.2",
                "e1854.0"
            ]
        },
        {
            "text": "Lang-NGU using frozen pretrained text encoders (BERT, CLIP_text, ALM_text) on environment captions improved sample efficiency 50-70% on manipulation tasks and 18-38% on search, demonstrating language-shaped representations aid exploration",
            "uuids": [
                "e1839.0"
            ]
        },
        {
            "text": "GPT-4 used as embodied agent in textual environments (LangSuit•E) achieved 77-86% SR with high-level actions but only 11-20% with low-level actions, showing text-world agents excel at planning but struggle with fine-grained control",
            "uuids": [
                "e1730.0"
            ]
        },
        {
            "text": "GPT-4 generating expert trajectories in textual MARL environments enabled language-grounded communication that improved task completion and zero-shot teamwork, demonstrating text-world experience can transfer to coordination",
            "uuids": [
                "e1694.0"
            ]
        },
        {
            "text": "TWOSOME using RL to align LLaMA-7B with embodied environments achieved 67.0% SR on ALFWorld after addressing token-length bias, showing text-pretrained LLMs can be adapted to embodied tasks with appropriate training",
            "uuids": [
                "e1831.0"
            ]
        },
        {
            "text": "IGOR using Flan-T5 finetuned on instruction→subtask data achieved 60% SR on modified Crafter vs 36.4% for Dynalang baseline, demonstrating explicit hierarchical decomposition helps bridge text and embodied control",
            "uuids": [
                "e1728.0"
            ]
        }
    ],
    "theory_statements": [
        "Text-world pretraining provides strongest benefits for high-level planning, reasoning, goal specification, and task decomposition",
        "Perception and low-level control require 3D-specific training and cannot be effectively learned from text alone",
        "Semantic alignment between text-world and 3D environment is necessary but not sufficient for transfer",
        "Explicit bridging mechanisms (captioners, object detectors, inverse dynamics models, cross-modality imitation) are required to connect text and 3D modalities",
        "The amount of 3D-specific finetuning data required is inversely proportional to the quality of semantic alignment and the sophistication of bridging mechanisms",
        "Negative transfer occurs when text-world pretraining introduces strong but incorrect priors about 3D environment dynamics (e.g., rendering-specific features, action-space mismatches)",
        "Text-grounded pretraining that includes action supervision (text+action triplets) transfers better than pure language-only pretraining",
        "Language-shaped representations improve exploration and novelty detection even when the policy does not directly use language at test time",
        "Text-world agents excel at discrete, high-level action spaces but struggle with continuous, low-level control without additional training",
        "Interactive text-world experience (with action-observation loops) provides better transfer than passive text consumption"
    ],
    "new_predictions_likely": [
        "Text-world pretraining should be most effective for tasks with rich language descriptions, clear semantic structure, and discrete action spaces",
        "Combining text-world pretraining with small amounts of 3D demonstration data should outperform either alone, especially for tasks requiring both planning and perception",
        "Text-world pretraining should improve zero-shot task understanding and goal specification even when execution fails due to perception/control issues",
        "Multi-modal pretraining (text + images + 3D) should provide better transfer than text-only pretraining, particularly for perception-heavy tasks",
        "Text-world pretraining with action prediction objectives should transfer better than language modeling alone",
        "Agents pretrained in text-worlds with similar semantic structure to target 3D environments should show larger transfer gains"
    ],
    "new_predictions_unknown": [
        "Whether text-world pretraining can ever fully replace 3D-specific training for perception and control, or if fundamental modality gaps exist that cannot be bridged",
        "Whether there exists an optimal ratio of text-world to 3D training data that maximizes transfer efficiency across different task types",
        "Whether text-world pretraining benefits scale linearly, sublinearly, or superlinearly with the complexity and realism of the text-world environment",
        "Whether transfer from text-worlds to 3D is fundamentally different from transfer between different 3D environments, or if similar principles apply",
        "Whether language-shaped representations for exploration transfer across different embodiments and environments",
        "Whether the benefits of text-world pretraining persist as 3D training data scales up, or if they diminish with sufficient 3D data"
    ],
    "negative_experiments": [
        "Finding that text-world pretraining provides no benefits over random initialization when sufficient 3D data is available would challenge the value of text-world pretraining",
        "Demonstrating that 3D-only training achieves better planning and reasoning than text-world pretrained models would challenge the high-level reasoning benefits",
        "Showing that text-world pretraining always causes negative transfer regardless of semantic alignment would challenge the feasibility of this approach",
        "Finding that bridging mechanisms (captioners, etc.) are not necessary for transfer and that end-to-end training works better would challenge the modality gap theory",
        "Demonstrating that pure language modeling transfers as well as text+action pretraining would challenge the importance of action supervision",
        "Finding that text-world pretraining does not improve exploration or sample efficiency would challenge the language-shaped representation benefits"
    ],
    "unaccounted_for": [
        {
            "text": "How to quantify semantic alignment between text-worlds and 3D environments a priori, before conducting expensive transfer experiments",
            "uuids": []
        },
        {
            "text": "The role of interactive vs. passive text-world experience in enabling transfer, and whether action-observation loops are necessary",
            "uuids": []
        },
        {
            "text": "How text-world complexity and fidelity affect transfer quality, and whether there are diminishing returns to increasing text-world realism",
            "uuids": []
        },
        {
            "text": "Why some approaches (RT-2, PREVALENT) achieved strong results without explicit text-world pretraining, suggesting alternative pathways to transfer",
            "uuids": [
                "e1843.0",
                "e1857.0"
            ]
        },
        {
            "text": "The optimal architecture for bridging text and 3D modalities, and whether learned bridges are better than hand-designed ones",
            "uuids": []
        },
        {
            "text": "How to handle the token-length bias problem when using LLMs for embodied control with variable-length action descriptions",
            "uuids": [
                "e1831.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "PREVALENT achieved 63% SR on R2R using image-text-action pretraining (not pure text-world), suggesting vision-grounded pretraining may be more effective than text-only",
            "uuids": [
                "e1857.0"
            ]
        },
        {
            "text": "RT-2 achieved strong transfer using vision-language pretraining (WebLI image-text pairs) without text-world experience, suggesting text-worlds may not be necessary",
            "uuids": [
                "e1843.0"
            ]
        },
        {
            "text": "RecBert showed negative transfer from text-based ALFRED to visual R2R, suggesting text-world pretraining can hurt when domains are misaligned",
            "uuids": [
                "e1729.2"
            ]
        },
        {
            "text": "Direct vision-language pretraining (CLIP, ALIGN) often outperformed text-world approaches in various benchmarks, suggesting visual grounding during pretraining is important",
            "uuids": [
                "e1769.0"
            ]
        },
        {
            "text": "VLN-BERT with ViLBERT initialization (vision-language) outperformed BERT-only initialization (language-only), suggesting multimodal pretraining is superior to text-only",
            "uuids": [
                "e1707.2",
                "e1854.0"
            ]
        },
        {
            "text": "Some text-world approaches (LID, ELLM) required substantial additional mechanisms (relabeling functions, learned captioners) to work, suggesting text-world pretraining alone is insufficient",
            "uuids": [
                "e1827.0",
                "e1808.0"
            ]
        }
    ],
    "special_cases": [
        "Text-world pretraining is most effective when the text-world closely mirrors the 3D environment structure (e.g., ALFWorld→ALFRED) and includes similar action spaces",
        "Benefits are largest for tasks requiring complex reasoning, planning, and goal decomposition rather than precise perception or continuous control",
        "Text-world pretraining may be more practical than 3D pretraining when 3D data collection is expensive, dangerous, or impossible",
        "The modality gap is smaller for tasks that can be naturally described in language (e.g., object manipulation, navigation) vs. tasks requiring continuous control (e.g., dexterous manipulation)",
        "Interactive text-worlds with action-observation loops provide better transfer than passive text consumption or language modeling alone",
        "Text-grounded pretraining (text+action or text+vision) provides better transfer than pure text-only pretraining",
        "Language-shaped representations can improve exploration and sample efficiency even when the policy does not use language at test time",
        "High-level action spaces benefit more from text-world pretraining than low-level continuous control spaces",
        "Text-world pretraining is most effective when combined with appropriate bridging mechanisms (captioners, detectors, inverse dynamics) rather than used in isolation"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Shridhar et al. (2020) ALFWorld: Aligning text and embodied environments for interactive learning [Foundational work on text-to-embodied transfer, demonstrates feasibility]",
            "Côté et al. (2018) TextWorld: A learning environment for text-based games [Text-world environment for RL, establishes text-world training paradigm]",
            "Ammanabrolu & Riedl (2019) Playing text-adventure games with graph-based deep reinforcement learning [Learning in text-based environments, shows benefits of structured representations]",
            "Radford et al. (2021) Learning Transferable Visual Models From Natural Language Supervision [CLIP, demonstrates vision-language pretraining can transfer broadly]",
            "Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Language-only pretraining, shows benefits of large-scale text pretraining]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [GPT-3, demonstrates language models can perform reasoning and planning]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 6,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Specification Incompleteness Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-91</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-91</p>
                <p><strong>Name:</strong> Specification Incompleteness Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about faithfulness gaps between natural language descriptions and code implementations in automated experimentation, based on the following results.</p>
                <p><strong>Description:</strong> Natural language descriptions of computational methods systematically omit critical implementation details due to cognitive limitations, communication constraints, and implicit assumptions. This incompleteness creates a fundamental gap between what is described and what must be implemented, leading to non-reproducible results. The theory posits that specification gaps occur across multiple dimensions: (1) explicit parameter values and settings, (2) algorithmic details and procedural steps, (3) data preprocessing and handling, (4) environmental and implementation-level details, and (5) evaluation protocols. The degree of incompleteness correlates with method complexity, domain expertise assumptions, publication format constraints, and the inherent difficulty of translating continuous prose into discrete computational steps. Critically, specification incompleteness is necessary but not sufficient to explain reproduction failures—complete specifications can still fail due to environmental factors, non-determinism, or errors in provided artifacts.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The probability of successful independent reproduction decreases monotonically with the number of unspecified implementation details, with critical details (hyperparameters, data preprocessing, evaluation protocols) having disproportionate impact.</li>
                <li>Specification incompleteness is inversely proportional to paper readability scores (p < 1e-24 in empirical studies) and directly proportional to method complexity.</li>
                <li>Critical omissions cluster in specific categories with measurable frequencies: hyperparameters (>60% of papers), preprocessing steps (~40%), training procedures (~50%), and environmental details (~70%).</li>
                <li>Specification gaps occur across multiple dimensions: explicit parameters, algorithmic procedures, data handling, environmental factors, and evaluation protocols, with different dimensions affecting different stages of reproduction.</li>
                <li>The relationship between specification completeness and reproduction success is necessary but not sufficient—complete specifications can still fail due to environmental factors, inherent non-determinism, or errors in provided artifacts.</li>
                <li>Specification incompleteness follows predictable patterns based on domain: deep learning methods show higher incompleteness than classical ML (~2x), reinforcement learning has particularly high rates of missing environmental details, and novel architectures have higher gaps than standard architectures.</li>
                <li>Author communication can partially compensate for specification gaps, with author replies correlating with ~84.6% reproduction success versus ~4.2% without replies (p < 1e-7).</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Papers frequently omit exact hyperparameter values, initialization procedures, and training details, forcing implementers to make assumptions that affect results. <a href="../results/extraction-result-487.html#e487.3" class="evidence-link">[e487.3]</a> <a href="../results/extraction-result-445.html#e445.0" class="evidence-link">[e445.0]</a> <a href="../results/extraction-result-454.html#e454.4" class="evidence-link">[e454.4]</a> <a href="../results/extraction-result-706.html#e706.0" class="evidence-link">[e706.0]</a> <a href="../results/extraction-result-728.html#e728.0" class="evidence-link">[e728.0]</a> </li>
    <li>Missing gradient derivations and derivative details force re-derivation with potential errors. <a href="../results/extraction-result-487.html#e487.2" class="evidence-link">[e487.2]</a> </li>
    <li>Unclear notation, ambiguous language, and entirely missing algorithmic steps prevent reliable implementation. <a href="../results/extraction-result-711.html#e711.2" class="evidence-link">[e711.2]</a> <a href="../results/extraction-result-693.html#e693.1" class="evidence-link">[e693.1]</a> </li>
    <li>Method documentation frequently omits high-level motivations, pseudo-code, and parameter descriptions. <a href="../results/extraction-result-476.html#e476.0" class="evidence-link">[e476.0]</a> </li>
    <li>Underspecified hyperparameters and training procedures require choices by reproducers that affect results. <a href="../results/extraction-result-454.html#e454.4" class="evidence-link">[e454.4]</a> <a href="../results/extraction-result-487.html#e487.3" class="evidence-link">[e487.3]</a> <a href="../results/extraction-result-706.html#e706.0" class="evidence-link">[e706.0]</a> </li>
    <li>Ambiguous evaluation protocol reporting in prior studies creates possible train/test leakage. <a href="../results/extraction-result-731.html#e731.3" class="evidence-link">[e731.3]</a> </li>
    <li>Incomplete specification of data splits and preprocessing leads to data leakage. <a href="../results/extraction-result-485.html#e485.7" class="evidence-link">[e485.7]</a> <a href="../results/extraction-result-475.html#e475.0" class="evidence-link">[e475.0]</a> </li>
    <li>Omitted or ambiguous method documentation causes gaps between description and implementation needs. <a href="../results/extraction-result-476.html#e476.0" class="evidence-link">[e476.0]</a> </li>
    <li>Excessive equations without prose or implementation guidance harm readability and reproducibility. <a href="../results/extraction-result-711.html#e711.7" class="evidence-link">[e711.7]</a> </li>
    <li>Language version and pseudo-code specifications are often missing or ambiguous. <a href="../results/extraction-result-690.html#e690.3" class="evidence-link">[e690.3]</a> </li>
    <li>Annotation quality and coverage issues stem from incomplete specification of annotation protocols. <a href="../results/extraction-result-684.html#e684.3" class="evidence-link">[e684.3]</a> <a href="../results/extraction-result-461.html#e461.3" class="evidence-link">[e461.3]</a> <a href="../results/extraction-result-481.html#e481.6" class="evidence-link">[e481.6]</a> </li>
    <li>Prompt and context specifications for LLMs are often underspecified, leading to variable results. <a href="../results/extraction-result-498.html#e498.5" class="evidence-link">[e498.5]</a> <a href="../results/extraction-result-498.html#e498.6" class="evidence-link">[e498.6]</a> <a href="../results/extraction-result-481.html#e481.5" class="evidence-link">[e481.5]</a> </li>
    <li>Evaluation metric specifications can be incomplete or ambiguous, leading to implementation mismatches. <a href="../results/extraction-result-752.html#e752.5" class="evidence-link">[e752.5]</a> </li>
    <li>Preprocessing steps including feature scaling and tokenization are frequently underspecified. <a href="../results/extraction-result-454.html#e454.1" class="evidence-link">[e454.1]</a> </li>
    <li>Random seed management and averaging procedures are often not fully specified. <a href="../results/extraction-result-706.html#e706.2" class="evidence-link">[e706.2]</a> <a href="../results/extraction-result-694.html#e694.4" class="evidence-link">[e694.4]</a> </li>
    <li>Environmental details including hardware, framework versions, and compiler settings are frequently omitted. <a href="../results/extraction-result-485.html#e485.6" class="evidence-link">[e485.6]</a> </li>
    <li>Implementation-level details such as non-deterministic operations and framework behaviors are rarely documented. <a href="../results/extraction-result-703.html#e703.1" class="evidence-link">[e703.1]</a> <a href="../results/extraction-result-688.html#e688.4" class="evidence-link">[e688.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Papers with explicit pseudo-code and worked examples will have >2x higher independent reproduction rates than those without, controlling for method complexity.</li>
                <li>Providing configuration files alongside papers will reduce reproduction time by 40-60% and increase success rates by 25-35%.</li>
                <li>Automated specification completeness checkers that flag missing hyperparameters, initialization details, and preprocessing steps will identify 70-80% of papers with low reproducibility before publication.</li>
                <li>Requiring authors to complete structured specification templates (like reproducibility checklists) will increase reproduction success rates from ~30% baseline to ~60-70%.</li>
                <li>Papers that document all five method variables (problem, objective, hypothesis, contribution, pseudo-code) will have significantly higher reproduction rates than those documenting two or fewer.</li>
                <li>Specification gaps in hyperparameters will have larger impact on reproduction success than gaps in high-level motivation or background sections.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether AI-assisted specification completion tools can automatically infer and fill missing implementation details with >90% accuracy from paper text alone, or whether human domain expertise is fundamentally required.</li>
                <li>Whether formal specification languages for ML experiments could reduce specification gaps to <5% while remaining practical enough for widespread researcher adoption.</li>
                <li>Whether the specification gap follows a power law distribution where a small number of critical omissions (e.g., 20% of details) account for the majority (e.g., 80%) of reproduction failures.</li>
                <li>Whether specification incompleteness has increased or decreased over time as ML methods have become more complex but documentation practices and tooling have evolved.</li>
                <li>Whether there exists a theoretical minimum level of specification incompleteness below which further detail provides diminishing returns for reproduction success.</li>
                <li>Whether specification completeness requirements differ fundamentally across scientific domains (e.g., physics vs. ML vs. biology) or follow universal patterns.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding papers with complete specifications (all hyperparameters, procedures, and environmental details documented) that still fail to reproduce at rates similar to incomplete specifications would challenge the theory's emphasis on specification completeness as a primary cause.</li>
                <li>Demonstrating that providing all implementation details does not improve reproduction rates compared to minimal documentation would undermine the theory.</li>
                <li>Showing that specification completeness does not correlate with reproduction success across a large sample (>100 papers) would contradict the theory's core predictions.</li>
                <li>Finding that expert implementers succeed at reproduction regardless of specification completeness at similar rates to novices would suggest other factors dominate over specification gaps.</li>
                <li>Demonstrating that papers with low readability scores but complete specifications reproduce at similar rates to high-readability papers would challenge the readability-completeness relationship.</li>
                <li>Finding that specification gaps in certain categories (e.g., high-level motivation) have equal impact to gaps in critical details (e.g., hyperparameters) would contradict the theory's hierarchical structure.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some reproduction failures occur even with complete specifications due to environmental factors (hardware differences, framework versions, compiler variations). <a href="../results/extraction-result-485.html#e485.6" class="evidence-link">[e485.6]</a> <a href="../results/extraction-result-491.html#e491.0" class="evidence-link">[e491.0]</a> <a href="../results/extraction-result-491.html#e491.1" class="evidence-link">[e491.1]</a> </li>
    <li>Inherent non-determinism in training (random initialization, stochastic optimization, parallel operations) can cause failures independent of specification quality. <a href="../results/extraction-result-475.html#e475.1" class="evidence-link">[e475.1]</a> <a href="../results/extraction-result-485.html#e485.5" class="evidence-link">[e485.5]</a> <a href="../results/extraction-result-703.html#e703.1" class="evidence-link">[e703.1]</a> <a href="../results/extraction-result-688.html#e688.4" class="evidence-link">[e688.4]</a> </li>
    <li>Data quality issues and annotation problems can cause failures unrelated to specification completeness. <a href="../results/extraction-result-461.html#e461.3" class="evidence-link">[e461.3]</a> <a href="../results/extraction-result-719.html#e719.8" class="evidence-link">[e719.8]</a> <a href="../results/extraction-result-684.html#e684.3" class="evidence-link">[e684.3]</a> </li>
    <li>Implementation-level mismatches between theoretical descriptions and practical code (e.g., embedding lookup as index vs. one-hot) can cause failures even with complete high-level specifications. <a href="../results/extraction-result-717.html#e717.0" class="evidence-link">[e717.0]</a> <a href="../results/extraction-result-722.html#e722.1" class="evidence-link">[e722.1]</a> </li>
    <li>Framework-level non-determinism and auto-tuning behaviors can cause divergence even with identical specifications. <a href="../results/extraction-result-688.html#e688.4" class="evidence-link">[e688.4]</a> <a href="../results/extraction-result-491.html#e491.1" class="evidence-link">[e491.1]</a> </li>
    <li>Bugs or errors in provided code artifacts can cause failures independent of specification completeness. <a href="../results/extraction-result-445.html#e445.3" class="evidence-link">[e445.3]</a> <a href="../results/extraction-result-677.html#e677.2" class="evidence-link">[e677.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Gundersen & Kjensmo (2018) State of the Art: Reproducibility in Artificial Intelligence [Documents systematic specification gaps in AI papers with RXD framework]</li>
    <li>Raff (2019) A Step Toward Quantifying Independently Reproducible Machine Learning Research [Empirically measures factors affecting reproduction including specification completeness, readability correlation]</li>
    <li>Pineau et al. (2020) Improving Reproducibility in Machine Learning Research [Proposes checklists to address specification gaps, NeurIPS reproducibility program]</li>
    <li>Henderson et al. (2018) Deep Reinforcement Learning that Matters [Documents how underspecified details affect RL reproducibility specifically]</li>
    <li>Dodge et al. (2019) Show Your Work: Improved Reporting of Experimental Results [Analyzes reporting practices and proposes improvements for specification completeness]</li>
    <li>Bouthillier et al. (2019) Unreproducible Research is Reproducible [Demonstrates that specification gaps in hyperparameters cause reproducibility failures]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Specification Incompleteness Theory",
    "theory_description": "Natural language descriptions of computational methods systematically omit critical implementation details due to cognitive limitations, communication constraints, and implicit assumptions. This incompleteness creates a fundamental gap between what is described and what must be implemented, leading to non-reproducible results. The theory posits that specification gaps occur across multiple dimensions: (1) explicit parameter values and settings, (2) algorithmic details and procedural steps, (3) data preprocessing and handling, (4) environmental and implementation-level details, and (5) evaluation protocols. The degree of incompleteness correlates with method complexity, domain expertise assumptions, publication format constraints, and the inherent difficulty of translating continuous prose into discrete computational steps. Critically, specification incompleteness is necessary but not sufficient to explain reproduction failures—complete specifications can still fail due to environmental factors, non-determinism, or errors in provided artifacts.",
    "supporting_evidence": [
        {
            "text": "Papers frequently omit exact hyperparameter values, initialization procedures, and training details, forcing implementers to make assumptions that affect results.",
            "uuids": [
                "e487.3",
                "e445.0",
                "e454.4",
                "e706.0",
                "e728.0"
            ]
        },
        {
            "text": "Missing gradient derivations and derivative details force re-derivation with potential errors.",
            "uuids": [
                "e487.2"
            ]
        },
        {
            "text": "Unclear notation, ambiguous language, and entirely missing algorithmic steps prevent reliable implementation.",
            "uuids": [
                "e711.2",
                "e693.1"
            ]
        },
        {
            "text": "Method documentation frequently omits high-level motivations, pseudo-code, and parameter descriptions.",
            "uuids": [
                "e476.0"
            ]
        },
        {
            "text": "Underspecified hyperparameters and training procedures require choices by reproducers that affect results.",
            "uuids": [
                "e454.4",
                "e487.3",
                "e706.0"
            ]
        },
        {
            "text": "Ambiguous evaluation protocol reporting in prior studies creates possible train/test leakage.",
            "uuids": [
                "e731.3"
            ]
        },
        {
            "text": "Incomplete specification of data splits and preprocessing leads to data leakage.",
            "uuids": [
                "e485.7",
                "e475.0"
            ]
        },
        {
            "text": "Omitted or ambiguous method documentation causes gaps between description and implementation needs.",
            "uuids": [
                "e476.0"
            ]
        },
        {
            "text": "Excessive equations without prose or implementation guidance harm readability and reproducibility.",
            "uuids": [
                "e711.7"
            ]
        },
        {
            "text": "Language version and pseudo-code specifications are often missing or ambiguous.",
            "uuids": [
                "e690.3"
            ]
        },
        {
            "text": "Annotation quality and coverage issues stem from incomplete specification of annotation protocols.",
            "uuids": [
                "e684.3",
                "e461.3",
                "e481.6"
            ]
        },
        {
            "text": "Prompt and context specifications for LLMs are often underspecified, leading to variable results.",
            "uuids": [
                "e498.5",
                "e498.6",
                "e481.5"
            ]
        },
        {
            "text": "Evaluation metric specifications can be incomplete or ambiguous, leading to implementation mismatches.",
            "uuids": [
                "e752.5"
            ]
        },
        {
            "text": "Preprocessing steps including feature scaling and tokenization are frequently underspecified.",
            "uuids": [
                "e454.1"
            ]
        },
        {
            "text": "Random seed management and averaging procedures are often not fully specified.",
            "uuids": [
                "e706.2",
                "e694.4"
            ]
        },
        {
            "text": "Environmental details including hardware, framework versions, and compiler settings are frequently omitted.",
            "uuids": [
                "e485.6"
            ]
        },
        {
            "text": "Implementation-level details such as non-deterministic operations and framework behaviors are rarely documented.",
            "uuids": [
                "e703.1",
                "e688.4"
            ]
        }
    ],
    "theory_statements": [
        "The probability of successful independent reproduction decreases monotonically with the number of unspecified implementation details, with critical details (hyperparameters, data preprocessing, evaluation protocols) having disproportionate impact.",
        "Specification incompleteness is inversely proportional to paper readability scores (p &lt; 1e-24 in empirical studies) and directly proportional to method complexity.",
        "Critical omissions cluster in specific categories with measurable frequencies: hyperparameters (&gt;60% of papers), preprocessing steps (~40%), training procedures (~50%), and environmental details (~70%).",
        "Specification gaps occur across multiple dimensions: explicit parameters, algorithmic procedures, data handling, environmental factors, and evaluation protocols, with different dimensions affecting different stages of reproduction.",
        "The relationship between specification completeness and reproduction success is necessary but not sufficient—complete specifications can still fail due to environmental factors, inherent non-determinism, or errors in provided artifacts.",
        "Specification incompleteness follows predictable patterns based on domain: deep learning methods show higher incompleteness than classical ML (~2x), reinforcement learning has particularly high rates of missing environmental details, and novel architectures have higher gaps than standard architectures.",
        "Author communication can partially compensate for specification gaps, with author replies correlating with ~84.6% reproduction success versus ~4.2% without replies (p &lt; 1e-7)."
    ],
    "new_predictions_likely": [
        "Papers with explicit pseudo-code and worked examples will have &gt;2x higher independent reproduction rates than those without, controlling for method complexity.",
        "Providing configuration files alongside papers will reduce reproduction time by 40-60% and increase success rates by 25-35%.",
        "Automated specification completeness checkers that flag missing hyperparameters, initialization details, and preprocessing steps will identify 70-80% of papers with low reproducibility before publication.",
        "Requiring authors to complete structured specification templates (like reproducibility checklists) will increase reproduction success rates from ~30% baseline to ~60-70%.",
        "Papers that document all five method variables (problem, objective, hypothesis, contribution, pseudo-code) will have significantly higher reproduction rates than those documenting two or fewer.",
        "Specification gaps in hyperparameters will have larger impact on reproduction success than gaps in high-level motivation or background sections."
    ],
    "new_predictions_unknown": [
        "Whether AI-assisted specification completion tools can automatically infer and fill missing implementation details with &gt;90% accuracy from paper text alone, or whether human domain expertise is fundamentally required.",
        "Whether formal specification languages for ML experiments could reduce specification gaps to &lt;5% while remaining practical enough for widespread researcher adoption.",
        "Whether the specification gap follows a power law distribution where a small number of critical omissions (e.g., 20% of details) account for the majority (e.g., 80%) of reproduction failures.",
        "Whether specification incompleteness has increased or decreased over time as ML methods have become more complex but documentation practices and tooling have evolved.",
        "Whether there exists a theoretical minimum level of specification incompleteness below which further detail provides diminishing returns for reproduction success.",
        "Whether specification completeness requirements differ fundamentally across scientific domains (e.g., physics vs. ML vs. biology) or follow universal patterns."
    ],
    "negative_experiments": [
        "Finding papers with complete specifications (all hyperparameters, procedures, and environmental details documented) that still fail to reproduce at rates similar to incomplete specifications would challenge the theory's emphasis on specification completeness as a primary cause.",
        "Demonstrating that providing all implementation details does not improve reproduction rates compared to minimal documentation would undermine the theory.",
        "Showing that specification completeness does not correlate with reproduction success across a large sample (&gt;100 papers) would contradict the theory's core predictions.",
        "Finding that expert implementers succeed at reproduction regardless of specification completeness at similar rates to novices would suggest other factors dominate over specification gaps.",
        "Demonstrating that papers with low readability scores but complete specifications reproduce at similar rates to high-readability papers would challenge the readability-completeness relationship.",
        "Finding that specification gaps in certain categories (e.g., high-level motivation) have equal impact to gaps in critical details (e.g., hyperparameters) would contradict the theory's hierarchical structure."
    ],
    "unaccounted_for": [
        {
            "text": "Some reproduction failures occur even with complete specifications due to environmental factors (hardware differences, framework versions, compiler variations).",
            "uuids": [
                "e485.6",
                "e491.0",
                "e491.1"
            ]
        },
        {
            "text": "Inherent non-determinism in training (random initialization, stochastic optimization, parallel operations) can cause failures independent of specification quality.",
            "uuids": [
                "e475.1",
                "e485.5",
                "e703.1",
                "e688.4"
            ]
        },
        {
            "text": "Data quality issues and annotation problems can cause failures unrelated to specification completeness.",
            "uuids": [
                "e461.3",
                "e719.8",
                "e684.3"
            ]
        },
        {
            "text": "Implementation-level mismatches between theoretical descriptions and practical code (e.g., embedding lookup as index vs. one-hot) can cause failures even with complete high-level specifications.",
            "uuids": [
                "e717.0",
                "e722.1"
            ]
        },
        {
            "text": "Framework-level non-determinism and auto-tuning behaviors can cause divergence even with identical specifications.",
            "uuids": [
                "e688.4",
                "e491.1"
            ]
        },
        {
            "text": "Bugs or errors in provided code artifacts can cause failures independent of specification completeness.",
            "uuids": [
                "e445.3",
                "e677.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Author correspondence strongly correlates with reproduction success (84.6% vs 4.2%, p &lt; 1e-7), suggesting personal communication can overcome specification gaps more effectively than written documentation alone.",
            "uuids": [
                "e487.6"
            ]
        },
        {
            "text": "Some papers with poor readability and incomplete specifications are still reproduced when authors provide code, suggesting code availability can compensate for specification gaps.",
            "uuids": [
                "e445.1"
            ]
        },
        {
            "text": "Provided code may contain bugs or implement variants different from descriptions, indicating that artifact availability does not guarantee specification completeness or correctness.",
            "uuids": [
                "e445.3",
                "e677.2"
            ]
        },
        {
            "text": "Cases exist where original artifacts are incomplete or incorrect but independent reimplementation succeeds, suggesting specification quality in natural language can sometimes exceed artifact quality.",
            "uuids": [
                "e475.5"
            ]
        }
    ],
    "special_cases": [
        "Deep learning methods show approximately 2x higher specification incompleteness than classical ML methods due to greater sensitivity to initialization, hyperparameters, and optimization details.",
        "Reinforcement learning papers have particularly high rates of missing environmental details (reward function specifications, environment parameters, episode termination conditions) and hyperparameter sensitivity.",
        "Papers using novel architectures have higher specification gaps than those using standard architectures with minor modifications, as novel architectures lack established conventions and defaults.",
        "Generative models (GANs, VAEs, diffusion models) show high specification incompleteness in training stability details, with frequent omission of failure modes and outlier rates.",
        "Cross-lingual and multilingual NLP papers have additional specification gaps related to translation procedures, language-specific preprocessing, and dataset construction methods.",
        "Papers involving human annotation have specification gaps in annotation protocols, inter-annotator agreement procedures, and quality control measures.",
        "Code generation and program synthesis papers have specification gaps in evaluation metrics (e.g., pass@k estimation methods), test suite construction, and execution environments.",
        "Specification gaps in evaluation protocols (metric computation, statistical testing, multiple comparison corrections) can be as impactful as gaps in method descriptions."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Gundersen & Kjensmo (2018) State of the Art: Reproducibility in Artificial Intelligence [Documents systematic specification gaps in AI papers with RXD framework]",
            "Raff (2019) A Step Toward Quantifying Independently Reproducible Machine Learning Research [Empirically measures factors affecting reproduction including specification completeness, readability correlation]",
            "Pineau et al. (2020) Improving Reproducibility in Machine Learning Research [Proposes checklists to address specification gaps, NeurIPS reproducibility program]",
            "Henderson et al. (2018) Deep Reinforcement Learning that Matters [Documents how underspecified details affect RL reproducibility specifically]",
            "Dodge et al. (2019) Show Your Work: Improved Reporting of Experimental Results [Analyzes reporting practices and proposes improvements for specification completeness]",
            "Bouthillier et al. (2019) Unreproducible Research is Reproducible [Demonstrates that specification gaps in hyperparameters cause reproducibility failures]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 4,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Spherical vs. Euclidean Architecture Performance Theory for Global Data - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-403</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-403</p>
                <p><strong>Name:</strong> Spherical vs. Euclidean Architecture Performance Theory for Global Data</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of the relationship between scientific problem characteristics (including data availability, data structure, problem complexity, domain maturity, and mechanistic understanding requirements) and the applicability, effectiveness, and impact potential of different AI methodologies and approaches, based on the following results.</p>
                <p><strong>Description:</strong> For scientific problems involving global spherical data (climate, planetary science, astrophysics), the choice between spherical-aware architectures (graph convolutions on icosahedral meshes, spherical harmonics) and Euclidean architectures (standard CNNs, MLPs, transformers) depends on multiple factors: (1) the spatial scale of relevant features, (2) the importance of long-range dependencies vs. local patterns, (3) available training data, and (4) computational constraints. Specifically: Local convolutional architectures (spherical CNNs, standard CNNs with appropriate projections) are effective for capturing regional features and patterns with characteristic scales <1000-2000 km, but suffer from information loss through pooling when global teleconnections dominate. Fully-connected architectures can capture arbitrary global patterns but require substantially more training data and have O(N²) computational scaling. Global attention mechanisms (transformers, AFNO) can capture long-range dependencies more efficiently than fully-connected layers but still require more data than local architectures. The optimal choice depends on the problem's spatial scale structure, with hybrid architectures often providing the best trade-offs for problems with mixed local and global features.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>For global spherical data with dominant long-range dependencies (teleconnections spanning >5000 km on Earth), fully-connected or global attention architectures outperform local convolutional approaches when sufficient training data is available, despite higher parameter counts.</li>
                <li>Spherical graph convolutional networks are effective for capturing local-to-regional features (scales <2000 km) but suffer from information loss through pooling operations when global patterns are important, limiting their effectiveness for problems dominated by teleconnections.</li>
                <li>The data efficiency of architectures varies inversely with their spatial receptive field: local convolutional models require less training data to learn local patterns, while global models (fully-connected, transformers) require substantially more data but can generalize better to unseen global configurations once trained.</li>
                <li>Computational cost scales differently by architecture: fully-connected scales as O(N²) with spatial resolution N, standard convolutions as O(N), and attention mechanisms as O(N² log N) with efficient implementations, creating practical trade-offs between expressiveness and scalability.</li>
                <li>For problems with mixed local and global features, hybrid architectures combining local feature extraction (convolutions) with global aggregation (attention or fully-connected layers) can achieve better performance than pure local or pure global architectures.</li>
                <li>Architectural inductive biases (locality, translation equivariance, rotation invariance) improve sample efficiency and generalization when they match the problem structure, but become limitations when the problem requires patterns outside the bias (e.g., local convolutions for global teleconnections).</li>
                <li>Positional encodings (sinusoidal, learned) can help architectures without explicit spatial structure (transformers, MLPs) learn spatial relationships, but may be less sample-efficient than architectures with built-in spatial inductive biases for problems where those biases apply.</li>
                <li>The optimal architecture depends on the ratio of variance explained by local vs. global patterns: problems where global patterns explain the majority of predictive variance benefit from global architectures despite their higher data requirements.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>S-MLP (fully-connected) outperformed S-Unet (spherical graph convolutions) and S-AFNO (spherical transformer) for global climate response modeling with long-range teleconnections, achieving better correlation and lower RMSE for lagged responses <a href="../results/extraction-result-2274.html#e2274.2" class="evidence-link">[e2274.2]</a> <a href="../results/extraction-result-2274.html#e2274.3" class="evidence-link">[e2274.3]</a> <a href="../results/extraction-result-2274.html#e2274.4" class="evidence-link">[e2274.4]</a> </li>
    <li>S-Unet captured local spatial features (e.g., tropical Pacific patterns) but missed global teleconnection patterns due to pooling operations and locality bias inherent in convolutional architectures <a href="../results/extraction-result-2274.html#e2274.3" class="evidence-link">[e2274.3]</a> </li>
    <li>S-AFNO (spherical transformer with AFNO layers) showed promise for operator learning but underperformed S-MLP in the available data regime, with authors attributing this to insufficient training data for transformer-style models <a href="../results/extraction-result-2274.html#e2274.4" class="evidence-link">[e2274.4]</a> </li>
    <li>FNO (Fourier Neural Operator) works well on smooth/periodic problems but struggles with non-periodic boundary conditions and boundary-driven problems, showing sensitivity to temporal discretization <a href="../results/extraction-result-2311.html#e2311.7" class="evidence-link">[e2311.7]</a> </li>
    <li>DiTTO-point with 1D convolutions and spatial positional encodings handles unstructured geometries effectively and showed superior noise robustness compared to FNO on hypersonic flow problems <a href="../results/extraction-result-2311.html#e2311.2" class="evidence-link">[e2311.2]</a> </li>
    <li>AMBER extends SegFormer to 3D patches for hyperspectral segmentation, preserving spatial resolution and capturing spectral-spatial features more effectively than standard 2D CNN approaches <a href="../results/extraction-result-2293.html#e2293.3" class="evidence-link">[e2293.3]</a> </li>
    <li>Rotation-invariant CNN architectures improve detection robustness for astronomical imaging tasks where orientation invariance is important <a href="../results/extraction-result-2289.html#e2289.12" class="evidence-link">[e2289.12]</a> </li>
    <li>CNN architectures combined with physics-informed training (using ab initio simulations) enable reliable automated interpretation of complex microscopy data on spherical/curved surfaces <a href="../results/extraction-result-2337.html#e2337.6" class="evidence-link">[e2337.6]</a> </li>
    <li>U-Net baselines lack temporal conditioning and architectural enhancements needed for continuous-time operator learning, failing to scale to high-dimensional time-dependent problems <a href="../results/extraction-result-2311.html#e2311.8" class="evidence-link">[e2311.8]</a> </li>
    <li>Invariant Point Attention (IPA) in AlphaFold2 uses geometry-aware attention with 3D points in local frames to achieve rotation/translation invariance while maintaining spatial locality <a href="../results/extraction-result-2308.html#e2308.2" class="evidence-link">[e2308.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For predicting El Niño events (a global teleconnection phenomenon with characteristic scales >5000 km), a fully-connected or transformer architecture will outperform a spherical CNN by 15-30% in prediction skill when trained on datasets of >1000 examples, but the spherical CNN will perform comparably or better with <500 examples.</li>
                <li>For regional weather forecasting tasks focused on scales <1000 km (e.g., precipitation, local temperature), a spherical CNN will match or exceed fully-connected architectures while using 10-100x fewer parameters and requiring 2-5x less training data.</li>
                <li>A hybrid architecture with local spherical convolutions for feature extraction and global attention for aggregation will outperform both pure local and pure global architectures for climate modeling tasks with mixed spatial scales (e.g., monsoon prediction, which involves both local land-atmosphere coupling and global circulation patterns).</li>
                <li>For problems on irregular spherical meshes (e.g., icosahedral grids with variable resolution), graph-based spherical convolutions with positional encodings will outperform standard CNNs with projection-based approaches by 10-20% in regions with high mesh irregularity.</li>
                <li>Rotation-invariant architectures will show 5-15% improvement over standard CNNs for astronomical object detection tasks where objects can appear at arbitrary orientations, with larger improvements for rarer object classes.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether efficient attention mechanisms (e.g., linear attention, sparse attention patterns based on physical distance) can match fully-connected performance for global teleconnection patterns while maintaining O(N) or O(N log N) computational scaling.</li>
                <li>If there exists a universal spatial scale threshold (e.g., ~2000 km for Earth climate) for switching from local to global architectures, or if the threshold is fundamentally problem-dependent based on the physics of each system.</li>
                <li>Whether learned spatial hierarchies (e.g., through neural architecture search or meta-learning) can automatically discover the optimal local-to-global feature extraction strategy for a given problem without manual architecture design.</li>
                <li>If spherical harmonic-based architectures can match or exceed graph-based spherical architectures for global problems while providing better interpretability through frequency-domain representations.</li>
                <li>Whether physics-informed architectural constraints (e.g., enforcing conservation laws, symmetries) can reduce the data requirements of global architectures to be competitive with local architectures even for global-scale problems.</li>
                <li>If the performance gap between local and global architectures narrows or widens as spatial resolution increases (e.g., moving from 100 km to 10 km to 1 km resolution climate models).</li>
                <li>Whether multimodal architectures that combine spherical spatial data with other modalities (e.g., time series, text descriptions) can achieve better performance than unimodal architectures through cross-modal learning, even with less spatial training data.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding global spherical problems with known long-range dependencies (>5000 km) where local convolutional architectures consistently outperform global architectures even with abundant training data (>10,000 examples) would challenge the spatial scale dependency claim.</li>
                <li>Demonstrating that fully-connected architectures do not outperform local architectures for problems with experimentally verified global teleconnections (e.g., ENSO, NAO) would weaken the theory's core prediction about architecture-scale matching.</li>
                <li>Showing that the computational cost of global architectures always outweighs their performance benefits in practical applications (e.g., operational forecasting) would limit the theory's practical applicability.</li>
                <li>Finding that hybrid architectures consistently underperform pure local or pure global architectures across multiple problems with mixed spatial scales would challenge the hybrid architecture advantage claim.</li>
                <li>Demonstrating that positional encodings allow transformers to match or exceed spherical CNNs even on small datasets (<500 examples) for local-scale problems would challenge the data efficiency claims.</li>
                <li>Finding that rotation-invariant architectures provide no benefit or harm performance for astronomical detection tasks would challenge the importance of geometric inductive biases.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to determine a priori (before training) whether a problem has dominant local or global features, and what the characteristic spatial scales are </li>
    <li>The interaction between temporal and spatial scales in spatio-temporal problems - whether temporal dependencies affect the optimal spatial architecture choice </li>
    <li>How to handle problems with multiple important spatial scales (e.g., both 100 km and 5000 km scales) - whether multiple parallel pathways or hierarchical architectures are needed </li>
    <li>The role of transfer learning and pre-training in reducing data requirements for global architectures - whether pre-training on related tasks can make global architectures competitive with local architectures in low-data regimes </li>
    <li>How the theory applies to other spherical domains beyond Earth (e.g., other planets, stars, cosmological scales) where characteristic spatial scales differ </li>
    <li>The impact of data quality and noise levels on architecture choice - whether noisy data favors architectures with stronger inductive biases <a href="../results/extraction-result-2311.html#e2311.2" class="evidence-link">[e2311.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Perraudin et al. (2019) DeepSphere: Efficient spherical convolutional neural network with HEALPix sampling for cosmological applications [Spherical CNNs for astrophysics, demonstrates local convolutions on sphere]</li>
    <li>Dosovitskiy et al. (2021) An image is worth 16x16 words: Transformers for image recognition at scale [Vision transformers, demonstrates global attention for images]</li>
    <li>Pathak et al. (2022) FourCastNet: A global data-driven high-resolution weather model using adaptive Fourier neural operators [AFNO for weather, demonstrates global spectral methods]</li>
    <li>Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Theory of inductive biases in neural networks]</li>
    <li>Bronstein et al. (2021) Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges [Theoretical framework for geometric deep learning including spherical domains]</li>
    <li>Weyn et al. (2021) Sub-seasonal forecasting with a large ensemble of deep-learning weather prediction models [Comparison of CNN vs other architectures for weather]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Spherical vs. Euclidean Architecture Performance Theory for Global Data",
    "theory_description": "For scientific problems involving global spherical data (climate, planetary science, astrophysics), the choice between spherical-aware architectures (graph convolutions on icosahedral meshes, spherical harmonics) and Euclidean architectures (standard CNNs, MLPs, transformers) depends on multiple factors: (1) the spatial scale of relevant features, (2) the importance of long-range dependencies vs. local patterns, (3) available training data, and (4) computational constraints. Specifically: Local convolutional architectures (spherical CNNs, standard CNNs with appropriate projections) are effective for capturing regional features and patterns with characteristic scales &lt;1000-2000 km, but suffer from information loss through pooling when global teleconnections dominate. Fully-connected architectures can capture arbitrary global patterns but require substantially more training data and have O(N²) computational scaling. Global attention mechanisms (transformers, AFNO) can capture long-range dependencies more efficiently than fully-connected layers but still require more data than local architectures. The optimal choice depends on the problem's spatial scale structure, with hybrid architectures often providing the best trade-offs for problems with mixed local and global features.",
    "supporting_evidence": [
        {
            "text": "S-MLP (fully-connected) outperformed S-Unet (spherical graph convolutions) and S-AFNO (spherical transformer) for global climate response modeling with long-range teleconnections, achieving better correlation and lower RMSE for lagged responses",
            "uuids": [
                "e2274.2",
                "e2274.3",
                "e2274.4"
            ]
        },
        {
            "text": "S-Unet captured local spatial features (e.g., tropical Pacific patterns) but missed global teleconnection patterns due to pooling operations and locality bias inherent in convolutional architectures",
            "uuids": [
                "e2274.3"
            ]
        },
        {
            "text": "S-AFNO (spherical transformer with AFNO layers) showed promise for operator learning but underperformed S-MLP in the available data regime, with authors attributing this to insufficient training data for transformer-style models",
            "uuids": [
                "e2274.4"
            ]
        },
        {
            "text": "FNO (Fourier Neural Operator) works well on smooth/periodic problems but struggles with non-periodic boundary conditions and boundary-driven problems, showing sensitivity to temporal discretization",
            "uuids": [
                "e2311.7"
            ]
        },
        {
            "text": "DiTTO-point with 1D convolutions and spatial positional encodings handles unstructured geometries effectively and showed superior noise robustness compared to FNO on hypersonic flow problems",
            "uuids": [
                "e2311.2"
            ]
        },
        {
            "text": "AMBER extends SegFormer to 3D patches for hyperspectral segmentation, preserving spatial resolution and capturing spectral-spatial features more effectively than standard 2D CNN approaches",
            "uuids": [
                "e2293.3"
            ]
        },
        {
            "text": "Rotation-invariant CNN architectures improve detection robustness for astronomical imaging tasks where orientation invariance is important",
            "uuids": [
                "e2289.12"
            ]
        },
        {
            "text": "CNN architectures combined with physics-informed training (using ab initio simulations) enable reliable automated interpretation of complex microscopy data on spherical/curved surfaces",
            "uuids": [
                "e2337.6"
            ]
        },
        {
            "text": "U-Net baselines lack temporal conditioning and architectural enhancements needed for continuous-time operator learning, failing to scale to high-dimensional time-dependent problems",
            "uuids": [
                "e2311.8"
            ]
        },
        {
            "text": "Invariant Point Attention (IPA) in AlphaFold2 uses geometry-aware attention with 3D points in local frames to achieve rotation/translation invariance while maintaining spatial locality",
            "uuids": [
                "e2308.2"
            ]
        }
    ],
    "theory_statements": [
        "For global spherical data with dominant long-range dependencies (teleconnections spanning &gt;5000 km on Earth), fully-connected or global attention architectures outperform local convolutional approaches when sufficient training data is available, despite higher parameter counts.",
        "Spherical graph convolutional networks are effective for capturing local-to-regional features (scales &lt;2000 km) but suffer from information loss through pooling operations when global patterns are important, limiting their effectiveness for problems dominated by teleconnections.",
        "The data efficiency of architectures varies inversely with their spatial receptive field: local convolutional models require less training data to learn local patterns, while global models (fully-connected, transformers) require substantially more data but can generalize better to unseen global configurations once trained.",
        "Computational cost scales differently by architecture: fully-connected scales as O(N²) with spatial resolution N, standard convolutions as O(N), and attention mechanisms as O(N² log N) with efficient implementations, creating practical trade-offs between expressiveness and scalability.",
        "For problems with mixed local and global features, hybrid architectures combining local feature extraction (convolutions) with global aggregation (attention or fully-connected layers) can achieve better performance than pure local or pure global architectures.",
        "Architectural inductive biases (locality, translation equivariance, rotation invariance) improve sample efficiency and generalization when they match the problem structure, but become limitations when the problem requires patterns outside the bias (e.g., local convolutions for global teleconnections).",
        "Positional encodings (sinusoidal, learned) can help architectures without explicit spatial structure (transformers, MLPs) learn spatial relationships, but may be less sample-efficient than architectures with built-in spatial inductive biases for problems where those biases apply.",
        "The optimal architecture depends on the ratio of variance explained by local vs. global patterns: problems where global patterns explain the majority of predictive variance benefit from global architectures despite their higher data requirements."
    ],
    "new_predictions_likely": [
        "For predicting El Niño events (a global teleconnection phenomenon with characteristic scales &gt;5000 km), a fully-connected or transformer architecture will outperform a spherical CNN by 15-30% in prediction skill when trained on datasets of &gt;1000 examples, but the spherical CNN will perform comparably or better with &lt;500 examples.",
        "For regional weather forecasting tasks focused on scales &lt;1000 km (e.g., precipitation, local temperature), a spherical CNN will match or exceed fully-connected architectures while using 10-100x fewer parameters and requiring 2-5x less training data.",
        "A hybrid architecture with local spherical convolutions for feature extraction and global attention for aggregation will outperform both pure local and pure global architectures for climate modeling tasks with mixed spatial scales (e.g., monsoon prediction, which involves both local land-atmosphere coupling and global circulation patterns).",
        "For problems on irregular spherical meshes (e.g., icosahedral grids with variable resolution), graph-based spherical convolutions with positional encodings will outperform standard CNNs with projection-based approaches by 10-20% in regions with high mesh irregularity.",
        "Rotation-invariant architectures will show 5-15% improvement over standard CNNs for astronomical object detection tasks where objects can appear at arbitrary orientations, with larger improvements for rarer object classes."
    ],
    "new_predictions_unknown": [
        "Whether efficient attention mechanisms (e.g., linear attention, sparse attention patterns based on physical distance) can match fully-connected performance for global teleconnection patterns while maintaining O(N) or O(N log N) computational scaling.",
        "If there exists a universal spatial scale threshold (e.g., ~2000 km for Earth climate) for switching from local to global architectures, or if the threshold is fundamentally problem-dependent based on the physics of each system.",
        "Whether learned spatial hierarchies (e.g., through neural architecture search or meta-learning) can automatically discover the optimal local-to-global feature extraction strategy for a given problem without manual architecture design.",
        "If spherical harmonic-based architectures can match or exceed graph-based spherical architectures for global problems while providing better interpretability through frequency-domain representations.",
        "Whether physics-informed architectural constraints (e.g., enforcing conservation laws, symmetries) can reduce the data requirements of global architectures to be competitive with local architectures even for global-scale problems.",
        "If the performance gap between local and global architectures narrows or widens as spatial resolution increases (e.g., moving from 100 km to 10 km to 1 km resolution climate models).",
        "Whether multimodal architectures that combine spherical spatial data with other modalities (e.g., time series, text descriptions) can achieve better performance than unimodal architectures through cross-modal learning, even with less spatial training data."
    ],
    "negative_experiments": [
        "Finding global spherical problems with known long-range dependencies (&gt;5000 km) where local convolutional architectures consistently outperform global architectures even with abundant training data (&gt;10,000 examples) would challenge the spatial scale dependency claim.",
        "Demonstrating that fully-connected architectures do not outperform local architectures for problems with experimentally verified global teleconnections (e.g., ENSO, NAO) would weaken the theory's core prediction about architecture-scale matching.",
        "Showing that the computational cost of global architectures always outweighs their performance benefits in practical applications (e.g., operational forecasting) would limit the theory's practical applicability.",
        "Finding that hybrid architectures consistently underperform pure local or pure global architectures across multiple problems with mixed spatial scales would challenge the hybrid architecture advantage claim.",
        "Demonstrating that positional encodings allow transformers to match or exceed spherical CNNs even on small datasets (&lt;500 examples) for local-scale problems would challenge the data efficiency claims.",
        "Finding that rotation-invariant architectures provide no benefit or harm performance for astronomical detection tasks would challenge the importance of geometric inductive biases."
    ],
    "unaccounted_for": [
        {
            "text": "How to determine a priori (before training) whether a problem has dominant local or global features, and what the characteristic spatial scales are",
            "uuids": []
        },
        {
            "text": "The interaction between temporal and spatial scales in spatio-temporal problems - whether temporal dependencies affect the optimal spatial architecture choice",
            "uuids": []
        },
        {
            "text": "How to handle problems with multiple important spatial scales (e.g., both 100 km and 5000 km scales) - whether multiple parallel pathways or hierarchical architectures are needed",
            "uuids": []
        },
        {
            "text": "The role of transfer learning and pre-training in reducing data requirements for global architectures - whether pre-training on related tasks can make global architectures competitive with local architectures in low-data regimes",
            "uuids": []
        },
        {
            "text": "How the theory applies to other spherical domains beyond Earth (e.g., other planets, stars, cosmological scales) where characteristic spatial scales differ",
            "uuids": []
        },
        {
            "text": "The impact of data quality and noise levels on architecture choice - whether noisy data favors architectures with stronger inductive biases",
            "uuids": [
                "e2311.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some local architectures with sufficient depth and skip connections (e.g., deep ResNets, U-Nets with many layers) can capture global patterns through repeated local operations, potentially challenging the strict local/global architecture dichotomy",
            "uuids": []
        },
        {
            "text": "DiTTO-point with 1D convolutions (a local operation) achieved superior performance on hypersonic flow compared to FNO (which has global receptive field through Fourier transforms), suggesting that the local/global distinction may be less important than other factors like noise robustness and positional encoding",
            "uuids": [
                "e2311.2",
                "e2311.7"
            ]
        }
    ],
    "special_cases": [
        "For problems with purely local dynamics and no long-range interactions (e.g., land surface processes, local turbulence), local convolutional architectures are always preferable due to better sample efficiency and computational cost.",
        "When computational resources are effectively unlimited and training data is abundant (&gt;100,000 examples), global architectures may be preferred even for primarily local problems to avoid making incorrect architectural assumptions.",
        "For real-time operational applications (e.g., weather forecasting with &lt;1 minute inference time), the computational cost of global architectures may be prohibitive regardless of performance benefits, favoring local or hybrid architectures.",
        "For problems on highly irregular or adaptive meshes, graph-based architectures may be necessary regardless of spatial scale to handle the mesh structure.",
        "When interpretability is critical (e.g., for scientific discovery or regulatory approval), architectures with clearer spatial inductive biases (local convolutions, spherical harmonics) may be preferred over fully-connected or attention-based architectures even if they have lower performance.",
        "For problems with strong physical symmetries (rotation, translation, scale), architectures that explicitly enforce these symmetries (equivariant networks) will be more sample-efficient than generic architectures regardless of spatial scale.",
        "In low-data regimes (&lt;100 examples), local architectures with strong inductive biases will generally outperform global architectures regardless of the true spatial scale of the problem, due to the data requirements of global architectures."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Perraudin et al. (2019) DeepSphere: Efficient spherical convolutional neural network with HEALPix sampling for cosmological applications [Spherical CNNs for astrophysics, demonstrates local convolutions on sphere]",
            "Dosovitskiy et al. (2021) An image is worth 16x16 words: Transformers for image recognition at scale [Vision transformers, demonstrates global attention for images]",
            "Pathak et al. (2022) FourCastNet: A global data-driven high-resolution weather model using adaptive Fourier neural operators [AFNO for weather, demonstrates global spectral methods]",
            "Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Theory of inductive biases in neural networks]",
            "Bronstein et al. (2021) Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges [Theoretical framework for geometric deep learning including spherical domains]",
            "Weyn et al. (2021) Sub-seasonal forecasting with a large ensemble of deep-learning weather prediction models [Comparison of CNN vs other architectures for weather]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 8,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
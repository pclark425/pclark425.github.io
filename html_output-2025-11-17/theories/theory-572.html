<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Dimensional Evaluation Necessity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-572</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-572</p>
                <p><strong>Name:</strong> Multi-Dimensional Evaluation Necessity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> Effective evaluation of LLM-generated scientific theories requires assessment across multiple independent dimensions (novelty, validity, feasibility, clarity, significance) because single-metric evaluation systematically fails to capture the multifaceted nature of scientific quality. No single metric correlates sufficiently with overall scientific value, and different dimensions can trade off against each other (e.g., high novelty with low feasibility). Optimal evaluation combines complementary metrics that capture distinct aspects of scientific quality, with weights adapted to domain-specific priorities and validated through correlation with expert judgments.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Single-Metric Insufficiency Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; uses_only &#8594; single_metric<span style="color: #888888;">, and</span></div>
        <div>&#8226; single_metric &#8594; measures &#8594; one_dimension_of_quality</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; fails_to_capture &#8594; overall_scientific_value<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; produces &#8594; misleading_rankings<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; shows_weak_correlation_with &#8594; expert_judgments</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>BLEU and ROUGE are inadequate for hypothesis evaluation because they fail to capture semantic depth, novelty, or testability; encourage paraphrasing rather than novel idea generation. <a href="../results/extraction-result-4612.html#e4612.3" class="evidence-link">[e4612.3]</a> </li>
    <li>Automated similarity metrics reward surface-level similarity and templates, fail to capture novelty or technical depth, and can penalize longer, more detailed outputs even when better per human judgment; therefore unsuitable as sole evaluation for open-ended scientific idea generation. <a href="../results/extraction-result-4433.html#e4433.3" class="evidence-link">[e4433.3]</a> </li>
    <li>Surface overlap metrics (BLEU/ROUGE) are poorly suited as primary measures for long-form surveys because they reward surface overlap and fail to capture factual accuracy, structure, and integration of references. <a href="../results/extraction-result-4428.html#e4428.6" class="evidence-link">[e4428.6]</a> </li>
    <li>Good between-question calibration does not imply good within-question discrimination necessary for selecting among responses to the same question; may be misleading for tasks where within-question ranking matters. <a href="../results/extraction-result-4560.html#e4560.3" class="evidence-link">[e4560.3]</a> </li>
    <li>Semantic similarity conflates surface/terminological overlap with deep conceptual novelty; embedding cosine measures can reward paraphrase rather than genuine creative novelty; these metrics poorly capture 'value' beyond alignment to existing published work and cannot distinguish P- vs H-creativity or influence potential. <a href="../results/extraction-result-4462.html#e4462.0" class="evidence-link">[e4462.0]</a> </li>
    <li>Higher diversity does not guarantee factuality or relevance; can reward incoherent or incorrect text; sensitive to dataset and generation length. <a href="../results/extraction-result-4535.html#e4535.5" class="evidence-link">[e4535.5]</a> </li>
    <li>Correlation measures evaluate alignment with existing human judgments but do not prove absolute metric quality; results depend on the quality and protocol of human annotations (which themselves vary), and selection of sample- vs dataset-level aggregation affects outcomes. Correlations can mask systematic biases even when overall agreement is high. <a href="../results/extraction-result-4610.html#e4610.1" class="evidence-link">[e4610.1]</a> </li>
    <li>LLM-based reviewers performed worse than the simple Title+Abstract prediction model on pairwise comparison accuracy. Quantitatively, the Sakana reviewer showed Pearson correlation 0.161 with mean human review scores on an ICLR subset, substantially below the trained review-score model (0.330 ± 0.030) and human single-reviewer baseline (0.412 ± 0.044). Qualitatively, LLM reviews were more generic and lacked deep semantic critique compared to human reviews. <a href="../results/extraction-result-4435.html#e4435.3" class="evidence-link">[e4435.3]</a> </li>
    <li>GPT-3.5-as-a-judge baseline accuracy ~59% on some settings and exhibited a weak and statistically insignificant Spearman correlation (~0.31) with human judgments. IBE-Eval outperformed this baseline by ≈17 percentage points on average. <a href="../results/extraction-result-4466.html#e4466.6" class="evidence-link">[e4466.6]</a> </li>
    <li>Evaluating 'idea quality' is inherently subjective; alignment with human preferences may not reflect downstream empirical success; scaling expert evaluation is costly; potential for generating plausible but untestable or invalid ideas. <a href="../results/extraction-result-4478.html#e4478.3" class="evidence-link">[e4478.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This law synthesizes observations from multiple papers showing single metrics fail, but frames it as a general principle with explicit failure modes. The specific formulation as a conditional law about evaluation system design with quantified correlation weaknesses is novel, though the underlying observation that single metrics are insufficient has been noted in evaluation literature.</p>
            <p><strong>References:</strong> <ul>
    <li>Reiter (2018) A Structured Review of the Validity of BLEU [critiques of single-metric evaluation in NLG]</li>
    <li>Novikova et al. (2017) Why We Need New Evaluation Metrics for NLG [argues for multi-dimensional evaluation]</li>
    <li>Gatt & Krahmer (2018) Survey of the State of the Art in Natural Language Generation [reviews evaluation approaches]</li>
</ul>
            <h3>Statement 1: Dimensional Independence Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; dimension_A &#8594; is_quality_dimension_of &#8594; scientific_theory<span style="color: #888888;">, and</span></div>
        <div>&#8226; dimension_B &#8594; is_quality_dimension_of &#8594; scientific_theory<span style="color: #888888;">, and</span></div>
        <div>&#8226; dimension_A &#8594; is_distinct_from &#8594; dimension_B</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; dimension_A &#8594; has_low_correlation_with &#8594; dimension_B<span style="color: #888888;">, and</span></div>
        <div>&#8226; dimension_A &#8594; can_trade_off_against &#8594; dimension_B<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; must_assess_separately &#8594; dimension_A and dimension_B</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human expert rubric evaluation uses three independent dimensions: Novelty (creativity and difference from existing work), Feasibility (practicality of implementing the idea), and Effectiveness (likelihood the idea will perform well vs baselines), each rated separately on 10-point scale. <a href="../results/extraction-result-4439.html#e4439.3" class="evidence-link">[e4439.3]</a> </li>
    <li>Evaluation rubric uses distinct criteria with explicit textual anchors: Validness (completely reflects the reality), Novelty (completely novel and not proposed by existing literature), Helpfulness (novel, valid, clear, specific enough to be directly adopted with no modifications), each with separate 5-point scales. <a href="../results/extraction-result-4457.html#e4457.3" class="evidence-link">[e4457.3]</a> </li>
    <li>HDR combines Feature Discovery Rate (proportion of true features discovered) with Relationship Correctness (direction/nature of relationship to outcome), as two independent components that multiply together. <a href="../results/extraction-result-4448.html#e4448.1" class="evidence-link">[e4448.1]</a> </li>
    <li>ACCELMAT uses dual-component evaluation: Closeness (objective similarity via concept/property/keyword overlap) and Quality (six expert-derived criteria including Alignment, Scientific Plausibility, Novelty/Innovation, Testability, Feasibility/Scalability, Impact Potential), each rated 1-5 with text anchors. <a href="../results/extraction-result-4452.html#e4452.0" class="evidence-link">[e4452.0]</a> </li>
    <li>Domain-specific evaluation criteria vary by artifact type: Problems use Clarity/Relevance/Originality/Feasibility/Significance; Methods use Clarity/Validity/Rigorousness/Innovativeness/Generalizability; Experiments use Clarity/Validity/Robustness/Feasibility/Reproducibility. <a href="../results/extraction-result-4568.html#e4568.0" class="evidence-link">[e4568.0]</a> </li>
    <li>SciHorizon Knowledge Evaluation uses four independent subdimensions: Factuality (accuracy on knowledge questions), Robustness (accuracy on perturbed inputs), Externalization (sentence-level cohesion via inverse perplexity), and Helpfulness (transfer utility to downstream models). <a href="../results/extraction-result-4446.html#e4446.1" class="evidence-link">[e4446.1]</a> </li>
    <li>HMS combines three independent component metrics: context F1 (alignment of contexts), variable F1 (set overlap for variables using fuzzy matching), and relationship accuracy (relationship alignment with three-level specificity heuristic). <a href="../results/extraction-result-4559.html#e4559.2" class="evidence-link">[e4559.2]</a> </li>
    <li>IBE-Eval computes explicit independent features: logical consistency (via autoformalization+Prolog entailment), parsimony (via proof depth and concept drift), coherence (via stepwise NLI entailment scores), and linguistic uncertainty (via sentence-level uncertainty and hedge-detection). <a href="../results/extraction-result-4466.html#e4466.0" class="evidence-link">[e4466.0]</a> </li>
    <li>Evaluation aspects use four SummEval dimensions: Relevance (captures central themes), Consistency/Factuality (accuracy relative to source), Coherence (logical ordering and flow), Fluency (grammatical quality/clarity). <a href="../results/extraction-result-4476.html#e4476.10" class="evidence-link">[e4476.10]</a> </li>
    <li>Component-weighted similarity uses independent components: each hypothesis decomposed into K functional components with mechanism similarity s_i ∈ [0,1], component weights w_i, and critical component indicator. <a href="../results/extraction-result-4525.html#e4525.4" class="evidence-link">[e4525.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The principle that quality dimensions are independent and can trade off is implicit in multi-dimensional evaluation frameworks, but this explicit formulation as a law governing evaluation design with the requirement for separate assessment is novel. Prior work uses multi-dimensional rubrics but doesn't formalize the independence principle as a necessary condition.</p>
            <p><strong>References:</strong> <ul>
    <li>Belz & Reiter (2006) Comparing Automatic and Human Evaluation of NLG Systems [discusses multiple evaluation dimensions]</li>
    <li>Gatt & Krahmer (2018) Survey of the State of the Art in Natural Language Generation [reviews multi-dimensional evaluation]</li>
</ul>
            <h3>Statement 2: Complementary Metric Synergy Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; metric_A &#8594; captures &#8594; dimension_X<span style="color: #888888;">, and</span></div>
        <div>&#8226; metric_B &#8594; captures &#8594; dimension_Y<span style="color: #888888;">, and</span></div>
        <div>&#8226; dimension_X &#8594; is_complementary_to &#8594; dimension_Y<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; combines &#8594; metric_A and metric_B</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; achieves_higher_validity_than &#8594; either_metric_alone<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; reduces &#8594; systematic_bias<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; shows_higher_correlation_with &#8594; expert_judgments</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>IBE-Eval combines logical consistency, parsimony, coherence, and linguistic uncertainty features with a linear regression model, achieving 77% accuracy (≈27 percentage points above random 50% and ≈17 pp above GPT-3.5-as-a-judge baseline) and Spearman correlation 0.64 (p < 0.01) with human judgments, outperforming single-feature approaches. <a href="../results/extraction-result-4466.html#e4466.0" class="evidence-link">[e4466.0]</a> </li>
    <li>HMS combines context F1, variable F1, and relationship accuracy as complementary components to evaluate data-driven hypotheses, with component-level analysis showing context accuracy (ctx_F1) is predictive: higher ctx_F1 correlates with higher combined var_F1×rel_acc. <a href="../results/extraction-result-4559.html#e4559.2" class="evidence-link">[e4559.2]</a> </li>
    <li>ACCELMAT combines Closeness (concept/property/keyword overlap) with Quality (six criteria) achieving: With Critic Feedback — Closeness 73.33%, Quality 85.67%; With Knowledge Graph + Feedback — Closeness 80.00%, Quality 89.00%, substantially outperforming single-component approaches. <a href="../results/extraction-result-4452.html#e4452.0" class="evidence-link">[e4452.0]</a> </li>
    <li>Hybrid evaluation combining automated metrics (ROUGE-L, BERTScore, BARTScore) and human evaluations produces more robust assessments than either alone; human ratings appear in Table 1: HypoGen Nov=4.78 Fea=4.24 Eff=4.43; AI Scientist Nov=4.94 Fea=4.18 Eff=4.77; Ours Nov=5.24 Fea=4.52 Eff=4.95. <a href="../results/extraction-result-4439.html#e4439.3" class="evidence-link">[e4439.3]</a> </li>
    <li>Literature+data integrated method outperformed few-shot prompting by 8.97% absolute accuracy, literature-only by 15.75%, and HYPOGENIC (data-driven) by 3.37% on OOD datasets, demonstrating synergy between complementary sources. <a href="../results/extraction-result-4447.html#e4447.0" class="evidence-link">[e4447.0]</a> </li>
    <li>Multi-agent collaboration improved 4-metric Avg over single-agent baseline: baseline Avg=1.92; multi-agent without tools Avg=2.09; multi-agent with tools Avg=2.07, showing role separation introduces productive uncertainty. <a href="../results/extraction-result-4443.html#e4443.6" class="evidence-link">[e4443.6]</a> </li>
    <li>SFT + RL with both benign and adversarial samples yielded a balanced, discriminative evaluator (LLaMA-8B: high scores for benign, ~1 for extreme adversarial, ~3 for subtle adversarial), outperforming SFT-only or RL-only approaches. <a href="../results/extraction-result-4451.html#e4451.3" class="evidence-link">[e4451.3]</a> </li>
    <li>Composed IBE-Eval (linear combination of features) correctly selected the best explanation with up to 77% accuracy, with individual criteria showing linguistic uncertainty and parsimony/coherence were stronger predictors than logical consistency alone. <a href="../results/extraction-result-4466.html#e4466.0" class="evidence-link">[e4466.0]</a> </li>
    <li>Including AD (aspect definitions) consistently improved performance regardless of prompt template or model size; AD-GPT variants (generated by GPT-4) produced similar improvements, indicating robustness of concise aspect definitions. <a href="../results/extraction-result-4476.html#e4476.10" class="evidence-link">[e4476.10]</a> </li>
    <li>MOOSE-Chem using GPT-4o with multi-step retrieval and EU components achieved Top MS=4.020 vs SciMON 2.549, MOOSE 2.882, Qi et al. 2.686; ablations show multi-step retrieval and EU contribute substantially. <a href="../results/extraction-result-4563.html#e4563.7" class="evidence-link">[e4563.7]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While ensemble methods and multi-metric evaluation are known in ML, this specific formulation about complementary metrics in scientific theory evaluation with quantified synergy effects (correlation improvements, accuracy gains) is novel. The law formalizes the synergy principle observed across multiple evaluation frameworks with empirical support.</p>
            <p><strong>References:</strong> <ul>
    <li>Dietterich (2000) Ensemble Methods in Machine Learning [general ensemble principles]</li>
    <li>Kuncheva & Whitaker (2003) Measures of Diversity in Classifier Ensembles [diversity in ensemble methods]</li>
</ul>
            <h3>Statement 3: Domain-Specific Weighting Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; scientific_domain &#8594; has_characteristic &#8594; domain_priorities<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; evaluates_theories_in &#8594; scientific_domain<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; uses &#8594; fixed_uniform_weights</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; produces &#8594; suboptimal_rankings<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; misaligns_with &#8594; domain_expert_preferences<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; shows_lower_correlation_with &#8594; domain_specific_outcomes</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Quality Q(H) = w_N * N(H) + w_F * F(H) + w_R * R(H) where weights are chosen according to research objectives to balance originality, testability, and domain importance; weight specification is subjective and domain-dependent. <a href="../results/extraction-result-4605.html#e4605.2" class="evidence-link">[e4605.2]</a> </li>
    <li>Different domains show different patterns: chemistry & biology had larger ID/OOD gaps in symbolic equation discovery; domain-specific analyses showed chemistry & biology had larger ID/OOD gaps. <a href="../results/extraction-result-4453.html#e4453.4" class="evidence-link">[e4453.4]</a> </li>
    <li>Domain-specific evaluation criteria vary by artifact type: Problems use Clarity/Relevance/Originality/Feasibility/Significance; Methods use Clarity/Validity/Rigorousness/Innovativeness/Generalizability; Experiments use Clarity/Validity/Robustness/Feasibility/Reproducibility. <a href="../results/extraction-result-4568.html#e4568.0" class="evidence-link">[e4568.0]</a> </li>
    <li>Values assessment requires discipline-specific ethical evaluation scenarios and guidelines aligned to predefined principles (laws, academic integrity, objective truth, respect for humanity/nature, fairness, privacy, transparency). <a href="../results/extraction-result-4446.html#e4446.7" class="evidence-link">[e4446.7]</a> </li>
    <li>Formative evaluation (Design Science Research strategy) focuses on generating empirically grounded interpretations to improve the artifact during development; OllaBench explicitly adopts formative evaluation and restricts its evaluative criteria to relevance, alignment, effectiveness, and unintended effects. <a href="../results/extraction-result-4424.html#e4424.5" class="evidence-link">[e4424.5]</a> </li>
    <li>External knowledge generally improved groundedness and accuracy for larger models: combined KG+Literature often yielded best groundedness and higher accuracy (e.g., GPT-4o-mini accuracy improvements up to ~5.14% when augmented). KG alone sometimes improved precision for Disease&Gene and Gene&Gene tasks. <a href="../results/extraction-result-4461.html#e4461.6" class="evidence-link">[e4461.6]</a> </li>
    <li>Opus (Claude-3-Opus variant) consistently achieved the highest accuracy across most categories in materials science; GPT-4 also showed strong performance in several topics (e.g., material processing and fluid mechanics). Smaller LLaMA3-8B underperformed relative to larger models. <a href="../results/extraction-result-4442.html#e4442.4" class="evidence-link">[e4442.4]</a> </li>
    <li>Spearman rank correlations (scoring) between two human annotators varied by task: Problem 0.83, Method 0.76, Experiment 0.67; pairwise Cohen's kappa: Problem 0.62, Method 0.62, Experiment 0.41, showing domain-dependent agreement patterns. <a href="../results/extraction-result-4568.html#e4568.0" class="evidence-link">[e4568.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Domain-specific evaluation is practiced but not formalized as a law. This formulation explicitly states that uniform weighting across domains produces suboptimal results with quantified misalignment, which is novel as a general principle with empirical support.</p>
            <p><strong>References:</strong> <ul>
    <li>Bornmann & Daniel (2008) What do citation counts measure? A review of studies on citing behavior [domain differences in evaluation]</li>
    <li>Hicks et al. (2015) Bibliometrics: The Leiden Manifesto for research metrics [context-dependent evaluation]</li>
</ul>
            <h3>Statement 4: Human-Machine Hybrid Validation Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; uses_only &#8594; automated_metrics<span style="color: #888888;">, and</span></div>
        <div>&#8226; automated_metrics &#8594; lack &#8594; human_validation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; produces &#8594; uncalibrated_scores<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; may_propagate &#8594; systematic_biases<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; requires &#8594; human_calibration</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Validation with ON showed Pearson r ≈ 0.52 between ON and human novelty ratings (200-abstract sample), demonstrating need for human calibration of automated metrics. <a href="../results/extraction-result-4439.html#e4439.3" class="evidence-link">[e4439.3]</a> </li>
    <li>Expert Panel Review with Concordance Metrics: panel of three domain experts independently rated each hypothesis; Cohen's κ = 0.82 reported for LLM-generated hypotheses overall, confirming reproducibility and biological plausibility. <a href="../results/extraction-result-4410.html#e4410.2" class="evidence-link">[e4410.2]</a> </li>
    <li>Human expert Likert-scale scoring showed high inter-annotator reliability: Spearman rank correlations between two human annotators: Problem 0.83, Method 0.76, Experiment 0.67; used as reference gold-standard to calibrate automated evaluations. <a href="../results/extraction-result-4568.html#e4568.0" class="evidence-link">[e4568.0]</a> </li>
    <li>Expert validation pipeline: five domain experts reviewed random sample of 62 benchmark papers, achieving 91.9% accuracy when counting only major issues and 82.3% when including minor issues. <a href="../results/extraction-result-4460.html#e4460.4" class="evidence-link">[e4460.4]</a> </li>
    <li>Hybrid evaluation combining automated metrics and human evaluation produces more robust assessments than either alone; automated scoring via OpenAI-o1-preview combined with human expert evaluation (4 PhD students) produced parallel/consistent judgments. <a href="../results/extraction-result-4452.html#e4452.0" class="evidence-link">[e4452.0]</a> </li>
    <li>Training without human labels risks propagating biases from generator/evaluator pairs; adversarial-only RL causes pessimism; choice of 'desirable' score thresholds influences training dynamics. <a href="../results/extraction-result-4451.html#e4451.3" class="evidence-link">[e4451.3]</a> </li>
    <li>Measured correlation between GPT-4-pref and human-pref: 82% (Sentiment Reversal), 68% (Acronym Generation), and 71% (Dialogue Response Generation), showing proxy imperfect—correlations <100% differ by task. <a href="../results/extraction-result-4546.html#e4546.2" class="evidence-link">[e4546.2]</a> </li>
    <li>Reported human annotators A1 vs A2 ρ ≈ 0.710 (p = 0.001); GPT-4 vs Mistral ρ ≈ 0.786 (p = 0.000). Correlations between annotators and LLMs were weak: A1 vs GPT-4 ρ = 0.248 (p = 0.305), indicating LLM ratings do not replicate human judgments without calibration. <a href="../results/extraction-result-4434.html#e4434.2" class="evidence-link">[e4434.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The need for human validation of automated metrics is recognized in practice, but this formulation as a law with quantified calibration requirements and systematic bias propagation is novel. The explicit requirement for hybrid validation with correlation thresholds is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Dang et al. (2006) Overview of DUC 2006 [human evaluation in NLG]</li>
    <li>Celikyilmaz et al. (2020) Evaluation of Text Generation: A Survey [discusses human-automated metric alignment]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an evaluation system for mathematical theories uses only proof correctness without considering elegance or generalizability, it will rank brute-force proofs equal to elegant ones, misaligning with mathematician preferences by at least 30% (based on observed correlation gaps).</li>
                <li>If a biomedical hypothesis evaluation system weights novelty and feasibility equally across all contexts, it will undervalue incremental but immediately testable hypotheses in clinical settings and overvalue speculative hypotheses in basic research settings, with correlation to expert preferences dropping below 0.5.</li>
                <li>If two evaluation systems assess the same set of generated theories, one using three independent dimensions (novelty, validity, feasibility) and another using only a composite 'quality' score, the multi-dimensional system will show higher inter-rater reliability (Cohen's κ > 0.7 vs < 0.5) and better prediction of expert preferences (correlation improvement of 0.15-0.25).</li>
                <li>If an evaluation system combines automated metrics with human validation (hybrid approach), it will achieve correlation with expert judgments of r > 0.7, compared to r < 0.5 for automated-only or human-only approaches in complex domains.</li>
                <li>If evaluation dimensions are assessed separately and then combined with learned weights, the system will outperform uniform weighting by 10-20% in domain-specific accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If evaluation systems adaptively learn domain-specific dimension weights from expert feedback using reinforcement learning, they might achieve substantially higher correlation with expert rankings (potentially r > 0.85) than fixed-weight systems, but the optimal learning rate, sample size requirements, and stability across domains are unknown.</li>
                <li>If dimensional independence breaks down in certain domains (e.g., in theoretical physics where novelty and validity are tightly coupled due to mathematical constraints), multi-dimensional evaluation might not outperform carefully designed single composite metrics, and the boundary conditions for this breakdown are unclear.</li>
                <li>If evaluation dimensions are expanded beyond the typical 3-6 dimensions to 10+ fine-grained dimensions, the evaluation quality might improve due to better coverage or degrade due to increased noise and evaluator cognitive load; the optimal dimensionality and its relationship to domain complexity is unknown.</li>
                <li>If automated metrics are calibrated using active learning to select the most informative human judgments, the required number of human validations might be reduced by an order of magnitude while maintaining correlation > 0.7, but the optimal selection strategy is unknown.</li>
                <li>If evaluation systems incorporate temporal dynamics (tracking how dimension importance changes over time in a field), they might better predict long-term impact, but the appropriate time scales and update frequencies are unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding a domain where single-metric evaluation (e.g., only novelty scoring) consistently outperforms multi-dimensional evaluation in predicting expert preferences (correlation difference > 0.1) would challenge this theory.</li>
                <li>Demonstrating that dimension weights optimized for one scientific domain transfer perfectly to all other domains (correlation drop < 0.05) would challenge the domain-specific weighting law.</li>
                <li>Showing that randomly weighted combinations of dimensions perform as well as carefully designed weights (difference < 5% in expert correlation) would challenge the complementary metric synergy law.</li>
                <li>Finding that automated metrics without any human calibration achieve correlation > 0.8 with expert judgments across multiple domains would challenge the human-machine hybrid validation law.</li>
                <li>Demonstrating that evaluation systems with highly correlated dimensions (r > 0.9 between dimensions) perform as well as systems with independent dimensions would challenge the dimensional independence law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal number of dimensions for evaluation is not specified by the theory; evidence suggests 3-6 dimensions are common but the theoretical optimum is unclear. </li>
    <li>How to handle conflicting signals across dimensions (e.g., high novelty but low feasibility) is not fully addressed; some systems use weighted averaging but optimal conflict resolution strategies are not specified. </li>
    <li>The theory doesn't specify how to validate that chosen dimensions are truly independent; correlation thresholds for acceptable independence are not established. </li>
    <li>The relationship between evaluation complexity (number of dimensions, calibration requirements) and practical usability is not addressed. </li>
    <li>How evaluation requirements change with the maturity of a scientific field is not covered. </li>
    <li>The theory doesn't address how to evaluate theories that span multiple domains or create new domains. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes observations about multi-dimensional evaluation from multiple papers into a coherent framework with explicit laws and quantified relationships. While multi-dimensional evaluation is practiced, the formalization of why it's necessary (dimensional independence with correlation thresholds), how it works (complementary synergy with quantified improvements), and when it applies (domain-specific weighting with empirical support) as a unified theory with predictive laws is novel. The theory goes beyond existing practice by specifying quantitative relationships and failure modes.</p>
            <p><strong>References:</strong> <ul>
    <li>Belz & Reiter (2006) Comparing Automatic and Human Evaluation of NLG Systems [multi-dimensional evaluation in NLG]</li>
    <li>Novikova et al. (2017) Why We Need New Evaluation Metrics for NLG [argues for multiple dimensions]</li>
    <li>Gatt & Krahmer (2018) Survey of the State of the Art in Natural Language Generation [reviews evaluation approaches]</li>
    <li>Celikyilmaz et al. (2020) Evaluation of Text Generation: A Survey [comprehensive review of evaluation methods]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Dimensional Evaluation Necessity Theory",
    "theory_description": "Effective evaluation of LLM-generated scientific theories requires assessment across multiple independent dimensions (novelty, validity, feasibility, clarity, significance) because single-metric evaluation systematically fails to capture the multifaceted nature of scientific quality. No single metric correlates sufficiently with overall scientific value, and different dimensions can trade off against each other (e.g., high novelty with low feasibility). Optimal evaluation combines complementary metrics that capture distinct aspects of scientific quality, with weights adapted to domain-specific priorities and validated through correlation with expert judgments.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Single-Metric Insufficiency Law",
                "if": [
                    {
                        "subject": "evaluation_system",
                        "relation": "uses_only",
                        "object": "single_metric"
                    },
                    {
                        "subject": "single_metric",
                        "relation": "measures",
                        "object": "one_dimension_of_quality"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_system",
                        "relation": "fails_to_capture",
                        "object": "overall_scientific_value"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "produces",
                        "object": "misleading_rankings"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "shows_weak_correlation_with",
                        "object": "expert_judgments"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "BLEU and ROUGE are inadequate for hypothesis evaluation because they fail to capture semantic depth, novelty, or testability; encourage paraphrasing rather than novel idea generation.",
                        "uuids": [
                            "e4612.3"
                        ]
                    },
                    {
                        "text": "Automated similarity metrics reward surface-level similarity and templates, fail to capture novelty or technical depth, and can penalize longer, more detailed outputs even when better per human judgment; therefore unsuitable as sole evaluation for open-ended scientific idea generation.",
                        "uuids": [
                            "e4433.3"
                        ]
                    },
                    {
                        "text": "Surface overlap metrics (BLEU/ROUGE) are poorly suited as primary measures for long-form surveys because they reward surface overlap and fail to capture factual accuracy, structure, and integration of references.",
                        "uuids": [
                            "e4428.6"
                        ]
                    },
                    {
                        "text": "Good between-question calibration does not imply good within-question discrimination necessary for selecting among responses to the same question; may be misleading for tasks where within-question ranking matters.",
                        "uuids": [
                            "e4560.3"
                        ]
                    },
                    {
                        "text": "Semantic similarity conflates surface/terminological overlap with deep conceptual novelty; embedding cosine measures can reward paraphrase rather than genuine creative novelty; these metrics poorly capture 'value' beyond alignment to existing published work and cannot distinguish P- vs H-creativity or influence potential.",
                        "uuids": [
                            "e4462.0"
                        ]
                    },
                    {
                        "text": "Higher diversity does not guarantee factuality or relevance; can reward incoherent or incorrect text; sensitive to dataset and generation length.",
                        "uuids": [
                            "e4535.5"
                        ]
                    },
                    {
                        "text": "Correlation measures evaluate alignment with existing human judgments but do not prove absolute metric quality; results depend on the quality and protocol of human annotations (which themselves vary), and selection of sample- vs dataset-level aggregation affects outcomes. Correlations can mask systematic biases even when overall agreement is high.",
                        "uuids": [
                            "e4610.1"
                        ]
                    },
                    {
                        "text": "LLM-based reviewers performed worse than the simple Title+Abstract prediction model on pairwise comparison accuracy. Quantitatively, the Sakana reviewer showed Pearson correlation 0.161 with mean human review scores on an ICLR subset, substantially below the trained review-score model (0.330 ± 0.030) and human single-reviewer baseline (0.412 ± 0.044). Qualitatively, LLM reviews were more generic and lacked deep semantic critique compared to human reviews.",
                        "uuids": [
                            "e4435.3"
                        ]
                    },
                    {
                        "text": "GPT-3.5-as-a-judge baseline accuracy ~59% on some settings and exhibited a weak and statistically insignificant Spearman correlation (~0.31) with human judgments. IBE-Eval outperformed this baseline by ≈17 percentage points on average.",
                        "uuids": [
                            "e4466.6"
                        ]
                    },
                    {
                        "text": "Evaluating 'idea quality' is inherently subjective; alignment with human preferences may not reflect downstream empirical success; scaling expert evaluation is costly; potential for generating plausible but untestable or invalid ideas.",
                        "uuids": [
                            "e4478.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This law synthesizes observations from multiple papers showing single metrics fail, but frames it as a general principle with explicit failure modes. The specific formulation as a conditional law about evaluation system design with quantified correlation weaknesses is novel, though the underlying observation that single metrics are insufficient has been noted in evaluation literature.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Reiter (2018) A Structured Review of the Validity of BLEU [critiques of single-metric evaluation in NLG]",
                        "Novikova et al. (2017) Why We Need New Evaluation Metrics for NLG [argues for multi-dimensional evaluation]",
                        "Gatt & Krahmer (2018) Survey of the State of the Art in Natural Language Generation [reviews evaluation approaches]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dimensional Independence Law",
                "if": [
                    {
                        "subject": "dimension_A",
                        "relation": "is_quality_dimension_of",
                        "object": "scientific_theory"
                    },
                    {
                        "subject": "dimension_B",
                        "relation": "is_quality_dimension_of",
                        "object": "scientific_theory"
                    },
                    {
                        "subject": "dimension_A",
                        "relation": "is_distinct_from",
                        "object": "dimension_B"
                    }
                ],
                "then": [
                    {
                        "subject": "dimension_A",
                        "relation": "has_low_correlation_with",
                        "object": "dimension_B"
                    },
                    {
                        "subject": "dimension_A",
                        "relation": "can_trade_off_against",
                        "object": "dimension_B"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "must_assess_separately",
                        "object": "dimension_A and dimension_B"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human expert rubric evaluation uses three independent dimensions: Novelty (creativity and difference from existing work), Feasibility (practicality of implementing the idea), and Effectiveness (likelihood the idea will perform well vs baselines), each rated separately on 10-point scale.",
                        "uuids": [
                            "e4439.3"
                        ]
                    },
                    {
                        "text": "Evaluation rubric uses distinct criteria with explicit textual anchors: Validness (completely reflects the reality), Novelty (completely novel and not proposed by existing literature), Helpfulness (novel, valid, clear, specific enough to be directly adopted with no modifications), each with separate 5-point scales.",
                        "uuids": [
                            "e4457.3"
                        ]
                    },
                    {
                        "text": "HDR combines Feature Discovery Rate (proportion of true features discovered) with Relationship Correctness (direction/nature of relationship to outcome), as two independent components that multiply together.",
                        "uuids": [
                            "e4448.1"
                        ]
                    },
                    {
                        "text": "ACCELMAT uses dual-component evaluation: Closeness (objective similarity via concept/property/keyword overlap) and Quality (six expert-derived criteria including Alignment, Scientific Plausibility, Novelty/Innovation, Testability, Feasibility/Scalability, Impact Potential), each rated 1-5 with text anchors.",
                        "uuids": [
                            "e4452.0"
                        ]
                    },
                    {
                        "text": "Domain-specific evaluation criteria vary by artifact type: Problems use Clarity/Relevance/Originality/Feasibility/Significance; Methods use Clarity/Validity/Rigorousness/Innovativeness/Generalizability; Experiments use Clarity/Validity/Robustness/Feasibility/Reproducibility.",
                        "uuids": [
                            "e4568.0"
                        ]
                    },
                    {
                        "text": "SciHorizon Knowledge Evaluation uses four independent subdimensions: Factuality (accuracy on knowledge questions), Robustness (accuracy on perturbed inputs), Externalization (sentence-level cohesion via inverse perplexity), and Helpfulness (transfer utility to downstream models).",
                        "uuids": [
                            "e4446.1"
                        ]
                    },
                    {
                        "text": "HMS combines three independent component metrics: context F1 (alignment of contexts), variable F1 (set overlap for variables using fuzzy matching), and relationship accuracy (relationship alignment with three-level specificity heuristic).",
                        "uuids": [
                            "e4559.2"
                        ]
                    },
                    {
                        "text": "IBE-Eval computes explicit independent features: logical consistency (via autoformalization+Prolog entailment), parsimony (via proof depth and concept drift), coherence (via stepwise NLI entailment scores), and linguistic uncertainty (via sentence-level uncertainty and hedge-detection).",
                        "uuids": [
                            "e4466.0"
                        ]
                    },
                    {
                        "text": "Evaluation aspects use four SummEval dimensions: Relevance (captures central themes), Consistency/Factuality (accuracy relative to source), Coherence (logical ordering and flow), Fluency (grammatical quality/clarity).",
                        "uuids": [
                            "e4476.10"
                        ]
                    },
                    {
                        "text": "Component-weighted similarity uses independent components: each hypothesis decomposed into K functional components with mechanism similarity s_i ∈ [0,1], component weights w_i, and critical component indicator.",
                        "uuids": [
                            "e4525.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "The principle that quality dimensions are independent and can trade off is implicit in multi-dimensional evaluation frameworks, but this explicit formulation as a law governing evaluation design with the requirement for separate assessment is novel. Prior work uses multi-dimensional rubrics but doesn't formalize the independence principle as a necessary condition.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Belz & Reiter (2006) Comparing Automatic and Human Evaluation of NLG Systems [discusses multiple evaluation dimensions]",
                        "Gatt & Krahmer (2018) Survey of the State of the Art in Natural Language Generation [reviews multi-dimensional evaluation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Complementary Metric Synergy Law",
                "if": [
                    {
                        "subject": "metric_A",
                        "relation": "captures",
                        "object": "dimension_X"
                    },
                    {
                        "subject": "metric_B",
                        "relation": "captures",
                        "object": "dimension_Y"
                    },
                    {
                        "subject": "dimension_X",
                        "relation": "is_complementary_to",
                        "object": "dimension_Y"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "combines",
                        "object": "metric_A and metric_B"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_system",
                        "relation": "achieves_higher_validity_than",
                        "object": "either_metric_alone"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "reduces",
                        "object": "systematic_bias"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "shows_higher_correlation_with",
                        "object": "expert_judgments"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "IBE-Eval combines logical consistency, parsimony, coherence, and linguistic uncertainty features with a linear regression model, achieving 77% accuracy (≈27 percentage points above random 50% and ≈17 pp above GPT-3.5-as-a-judge baseline) and Spearman correlation 0.64 (p &lt; 0.01) with human judgments, outperforming single-feature approaches.",
                        "uuids": [
                            "e4466.0"
                        ]
                    },
                    {
                        "text": "HMS combines context F1, variable F1, and relationship accuracy as complementary components to evaluate data-driven hypotheses, with component-level analysis showing context accuracy (ctx_F1) is predictive: higher ctx_F1 correlates with higher combined var_F1×rel_acc.",
                        "uuids": [
                            "e4559.2"
                        ]
                    },
                    {
                        "text": "ACCELMAT combines Closeness (concept/property/keyword overlap) with Quality (six criteria) achieving: With Critic Feedback — Closeness 73.33%, Quality 85.67%; With Knowledge Graph + Feedback — Closeness 80.00%, Quality 89.00%, substantially outperforming single-component approaches.",
                        "uuids": [
                            "e4452.0"
                        ]
                    },
                    {
                        "text": "Hybrid evaluation combining automated metrics (ROUGE-L, BERTScore, BARTScore) and human evaluations produces more robust assessments than either alone; human ratings appear in Table 1: HypoGen Nov=4.78 Fea=4.24 Eff=4.43; AI Scientist Nov=4.94 Fea=4.18 Eff=4.77; Ours Nov=5.24 Fea=4.52 Eff=4.95.",
                        "uuids": [
                            "e4439.3"
                        ]
                    },
                    {
                        "text": "Literature+data integrated method outperformed few-shot prompting by 8.97% absolute accuracy, literature-only by 15.75%, and HYPOGENIC (data-driven) by 3.37% on OOD datasets, demonstrating synergy between complementary sources.",
                        "uuids": [
                            "e4447.0"
                        ]
                    },
                    {
                        "text": "Multi-agent collaboration improved 4-metric Avg over single-agent baseline: baseline Avg=1.92; multi-agent without tools Avg=2.09; multi-agent with tools Avg=2.07, showing role separation introduces productive uncertainty.",
                        "uuids": [
                            "e4443.6"
                        ]
                    },
                    {
                        "text": "SFT + RL with both benign and adversarial samples yielded a balanced, discriminative evaluator (LLaMA-8B: high scores for benign, ~1 for extreme adversarial, ~3 for subtle adversarial), outperforming SFT-only or RL-only approaches.",
                        "uuids": [
                            "e4451.3"
                        ]
                    },
                    {
                        "text": "Composed IBE-Eval (linear combination of features) correctly selected the best explanation with up to 77% accuracy, with individual criteria showing linguistic uncertainty and parsimony/coherence were stronger predictors than logical consistency alone.",
                        "uuids": [
                            "e4466.0"
                        ]
                    },
                    {
                        "text": "Including AD (aspect definitions) consistently improved performance regardless of prompt template or model size; AD-GPT variants (generated by GPT-4) produced similar improvements, indicating robustness of concise aspect definitions.",
                        "uuids": [
                            "e4476.10"
                        ]
                    },
                    {
                        "text": "MOOSE-Chem using GPT-4o with multi-step retrieval and EU components achieved Top MS=4.020 vs SciMON 2.549, MOOSE 2.882, Qi et al. 2.686; ablations show multi-step retrieval and EU contribute substantially.",
                        "uuids": [
                            "e4563.7"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "While ensemble methods and multi-metric evaluation are known in ML, this specific formulation about complementary metrics in scientific theory evaluation with quantified synergy effects (correlation improvements, accuracy gains) is novel. The law formalizes the synergy principle observed across multiple evaluation frameworks with empirical support.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Dietterich (2000) Ensemble Methods in Machine Learning [general ensemble principles]",
                        "Kuncheva & Whitaker (2003) Measures of Diversity in Classifier Ensembles [diversity in ensemble methods]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Domain-Specific Weighting Law",
                "if": [
                    {
                        "subject": "scientific_domain",
                        "relation": "has_characteristic",
                        "object": "domain_priorities"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "evaluates_theories_in",
                        "object": "scientific_domain"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "uses",
                        "object": "fixed_uniform_weights"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_system",
                        "relation": "produces",
                        "object": "suboptimal_rankings"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "misaligns_with",
                        "object": "domain_expert_preferences"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "shows_lower_correlation_with",
                        "object": "domain_specific_outcomes"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Quality Q(H) = w_N * N(H) + w_F * F(H) + w_R * R(H) where weights are chosen according to research objectives to balance originality, testability, and domain importance; weight specification is subjective and domain-dependent.",
                        "uuids": [
                            "e4605.2"
                        ]
                    },
                    {
                        "text": "Different domains show different patterns: chemistry & biology had larger ID/OOD gaps in symbolic equation discovery; domain-specific analyses showed chemistry & biology had larger ID/OOD gaps.",
                        "uuids": [
                            "e4453.4"
                        ]
                    },
                    {
                        "text": "Domain-specific evaluation criteria vary by artifact type: Problems use Clarity/Relevance/Originality/Feasibility/Significance; Methods use Clarity/Validity/Rigorousness/Innovativeness/Generalizability; Experiments use Clarity/Validity/Robustness/Feasibility/Reproducibility.",
                        "uuids": [
                            "e4568.0"
                        ]
                    },
                    {
                        "text": "Values assessment requires discipline-specific ethical evaluation scenarios and guidelines aligned to predefined principles (laws, academic integrity, objective truth, respect for humanity/nature, fairness, privacy, transparency).",
                        "uuids": [
                            "e4446.7"
                        ]
                    },
                    {
                        "text": "Formative evaluation (Design Science Research strategy) focuses on generating empirically grounded interpretations to improve the artifact during development; OllaBench explicitly adopts formative evaluation and restricts its evaluative criteria to relevance, alignment, effectiveness, and unintended effects.",
                        "uuids": [
                            "e4424.5"
                        ]
                    },
                    {
                        "text": "External knowledge generally improved groundedness and accuracy for larger models: combined KG+Literature often yielded best groundedness and higher accuracy (e.g., GPT-4o-mini accuracy improvements up to ~5.14% when augmented). KG alone sometimes improved precision for Disease&Gene and Gene&Gene tasks.",
                        "uuids": [
                            "e4461.6"
                        ]
                    },
                    {
                        "text": "Opus (Claude-3-Opus variant) consistently achieved the highest accuracy across most categories in materials science; GPT-4 also showed strong performance in several topics (e.g., material processing and fluid mechanics). Smaller LLaMA3-8B underperformed relative to larger models.",
                        "uuids": [
                            "e4442.4"
                        ]
                    },
                    {
                        "text": "Spearman rank correlations (scoring) between two human annotators varied by task: Problem 0.83, Method 0.76, Experiment 0.67; pairwise Cohen's kappa: Problem 0.62, Method 0.62, Experiment 0.41, showing domain-dependent agreement patterns.",
                        "uuids": [
                            "e4568.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "Domain-specific evaluation is practiced but not formalized as a law. This formulation explicitly states that uniform weighting across domains produces suboptimal results with quantified misalignment, which is novel as a general principle with empirical support.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bornmann & Daniel (2008) What do citation counts measure? A review of studies on citing behavior [domain differences in evaluation]",
                        "Hicks et al. (2015) Bibliometrics: The Leiden Manifesto for research metrics [context-dependent evaluation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Human-Machine Hybrid Validation Law",
                "if": [
                    {
                        "subject": "evaluation_system",
                        "relation": "uses_only",
                        "object": "automated_metrics"
                    },
                    {
                        "subject": "automated_metrics",
                        "relation": "lack",
                        "object": "human_validation"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_system",
                        "relation": "produces",
                        "object": "uncalibrated_scores"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "may_propagate",
                        "object": "systematic_biases"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "requires",
                        "object": "human_calibration"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Validation with ON showed Pearson r ≈ 0.52 between ON and human novelty ratings (200-abstract sample), demonstrating need for human calibration of automated metrics.",
                        "uuids": [
                            "e4439.3"
                        ]
                    },
                    {
                        "text": "Expert Panel Review with Concordance Metrics: panel of three domain experts independently rated each hypothesis; Cohen's κ = 0.82 reported for LLM-generated hypotheses overall, confirming reproducibility and biological plausibility.",
                        "uuids": [
                            "e4410.2"
                        ]
                    },
                    {
                        "text": "Human expert Likert-scale scoring showed high inter-annotator reliability: Spearman rank correlations between two human annotators: Problem 0.83, Method 0.76, Experiment 0.67; used as reference gold-standard to calibrate automated evaluations.",
                        "uuids": [
                            "e4568.0"
                        ]
                    },
                    {
                        "text": "Expert validation pipeline: five domain experts reviewed random sample of 62 benchmark papers, achieving 91.9% accuracy when counting only major issues and 82.3% when including minor issues.",
                        "uuids": [
                            "e4460.4"
                        ]
                    },
                    {
                        "text": "Hybrid evaluation combining automated metrics and human evaluation produces more robust assessments than either alone; automated scoring via OpenAI-o1-preview combined with human expert evaluation (4 PhD students) produced parallel/consistent judgments.",
                        "uuids": [
                            "e4452.0"
                        ]
                    },
                    {
                        "text": "Training without human labels risks propagating biases from generator/evaluator pairs; adversarial-only RL causes pessimism; choice of 'desirable' score thresholds influences training dynamics.",
                        "uuids": [
                            "e4451.3"
                        ]
                    },
                    {
                        "text": "Measured correlation between GPT-4-pref and human-pref: 82% (Sentiment Reversal), 68% (Acronym Generation), and 71% (Dialogue Response Generation), showing proxy imperfect—correlations &lt;100% differ by task.",
                        "uuids": [
                            "e4546.2"
                        ]
                    },
                    {
                        "text": "Reported human annotators A1 vs A2 ρ ≈ 0.710 (p = 0.001); GPT-4 vs Mistral ρ ≈ 0.786 (p = 0.000). Correlations between annotators and LLMs were weak: A1 vs GPT-4 ρ = 0.248 (p = 0.305), indicating LLM ratings do not replicate human judgments without calibration.",
                        "uuids": [
                            "e4434.2"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "The need for human validation of automated metrics is recognized in practice, but this formulation as a law with quantified calibration requirements and systematic bias propagation is novel. The explicit requirement for hybrid validation with correlation thresholds is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Dang et al. (2006) Overview of DUC 2006 [human evaluation in NLG]",
                        "Celikyilmaz et al. (2020) Evaluation of Text Generation: A Survey [discusses human-automated metric alignment]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an evaluation system for mathematical theories uses only proof correctness without considering elegance or generalizability, it will rank brute-force proofs equal to elegant ones, misaligning with mathematician preferences by at least 30% (based on observed correlation gaps).",
        "If a biomedical hypothesis evaluation system weights novelty and feasibility equally across all contexts, it will undervalue incremental but immediately testable hypotheses in clinical settings and overvalue speculative hypotheses in basic research settings, with correlation to expert preferences dropping below 0.5.",
        "If two evaluation systems assess the same set of generated theories, one using three independent dimensions (novelty, validity, feasibility) and another using only a composite 'quality' score, the multi-dimensional system will show higher inter-rater reliability (Cohen's κ &gt; 0.7 vs &lt; 0.5) and better prediction of expert preferences (correlation improvement of 0.15-0.25).",
        "If an evaluation system combines automated metrics with human validation (hybrid approach), it will achieve correlation with expert judgments of r &gt; 0.7, compared to r &lt; 0.5 for automated-only or human-only approaches in complex domains.",
        "If evaluation dimensions are assessed separately and then combined with learned weights, the system will outperform uniform weighting by 10-20% in domain-specific accuracy."
    ],
    "new_predictions_unknown": [
        "If evaluation systems adaptively learn domain-specific dimension weights from expert feedback using reinforcement learning, they might achieve substantially higher correlation with expert rankings (potentially r &gt; 0.85) than fixed-weight systems, but the optimal learning rate, sample size requirements, and stability across domains are unknown.",
        "If dimensional independence breaks down in certain domains (e.g., in theoretical physics where novelty and validity are tightly coupled due to mathematical constraints), multi-dimensional evaluation might not outperform carefully designed single composite metrics, and the boundary conditions for this breakdown are unclear.",
        "If evaluation dimensions are expanded beyond the typical 3-6 dimensions to 10+ fine-grained dimensions, the evaluation quality might improve due to better coverage or degrade due to increased noise and evaluator cognitive load; the optimal dimensionality and its relationship to domain complexity is unknown.",
        "If automated metrics are calibrated using active learning to select the most informative human judgments, the required number of human validations might be reduced by an order of magnitude while maintaining correlation &gt; 0.7, but the optimal selection strategy is unknown.",
        "If evaluation systems incorporate temporal dynamics (tracking how dimension importance changes over time in a field), they might better predict long-term impact, but the appropriate time scales and update frequencies are unknown."
    ],
    "negative_experiments": [
        "Finding a domain where single-metric evaluation (e.g., only novelty scoring) consistently outperforms multi-dimensional evaluation in predicting expert preferences (correlation difference &gt; 0.1) would challenge this theory.",
        "Demonstrating that dimension weights optimized for one scientific domain transfer perfectly to all other domains (correlation drop &lt; 0.05) would challenge the domain-specific weighting law.",
        "Showing that randomly weighted combinations of dimensions perform as well as carefully designed weights (difference &lt; 5% in expert correlation) would challenge the complementary metric synergy law.",
        "Finding that automated metrics without any human calibration achieve correlation &gt; 0.8 with expert judgments across multiple domains would challenge the human-machine hybrid validation law.",
        "Demonstrating that evaluation systems with highly correlated dimensions (r &gt; 0.9 between dimensions) perform as well as systems with independent dimensions would challenge the dimensional independence law."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal number of dimensions for evaluation is not specified by the theory; evidence suggests 3-6 dimensions are common but the theoretical optimum is unclear.",
            "uuids": []
        },
        {
            "text": "How to handle conflicting signals across dimensions (e.g., high novelty but low feasibility) is not fully addressed; some systems use weighted averaging but optimal conflict resolution strategies are not specified.",
            "uuids": []
        },
        {
            "text": "The theory doesn't specify how to validate that chosen dimensions are truly independent; correlation thresholds for acceptable independence are not established.",
            "uuids": []
        },
        {
            "text": "The relationship between evaluation complexity (number of dimensions, calibration requirements) and practical usability is not addressed.",
            "uuids": []
        },
        {
            "text": "How evaluation requirements change with the maturity of a scientific field is not covered.",
            "uuids": []
        },
        {
            "text": "The theory doesn't address how to evaluate theories that span multiple domains or create new domains.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that simple metrics like citation counts can predict long-term impact as well as complex multi-dimensional evaluations in certain contexts, suggesting single metrics may suffice when the outcome measure is clear and long-term.",
            "uuids": []
        },
        {
            "text": "Temperature sampling (single randomness parameter) sometimes matches multi-parameter approaches in generation quality, suggesting single-dimensional control may suffice in some generation tasks.",
            "uuids": [
                "e4554.4"
            ]
        },
        {
            "text": "Pass@N (single upper-bound metric) sometimes provides sufficient discrimination between models despite being one-dimensional, though it lacks statistical interpretation of uncertainty.",
            "uuids": [
                "e4562.2"
            ]
        },
        {
            "text": "In domains with well-defined ground truth (e.g., code correctness), single-metric evaluation can achieve high correlation with expert judgments (&gt; 0.9), suggesting multi-dimensional evaluation may not always be necessary.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In domains with well-defined ground truth (e.g., mathematical proofs, code correctness), single-metric evaluation (correctness) may be sufficient and achieve correlation &gt; 0.9 with expert judgments.",
        "For purely theoretical work without immediate applications, feasibility may be less important than novelty and validity, allowing dimension reduction.",
        "In time-critical evaluation scenarios (e.g., real-time peer review), simplified multi-dimensional rubrics (3 dimensions instead of 6) may be necessary despite 10-15% reduced accuracy.",
        "For incremental research in mature fields, validity and feasibility may be more important than novelty, requiring different dimension weights.",
        "In interdisciplinary research, standard domain-specific weights may not apply, requiring custom weight optimization or equal weighting as a default.",
        "For early-stage exploratory research, novelty and potential impact may outweigh immediate feasibility, justifying different weighting schemes.",
        "When evaluating theories with immediate safety implications (e.g., medical interventions), validity and feasibility must be weighted much more heavily than novelty."
    ],
    "existing_theory": {
        "classification_explanation": "This theory synthesizes observations about multi-dimensional evaluation from multiple papers into a coherent framework with explicit laws and quantified relationships. While multi-dimensional evaluation is practiced, the formalization of why it's necessary (dimensional independence with correlation thresholds), how it works (complementary synergy with quantified improvements), and when it applies (domain-specific weighting with empirical support) as a unified theory with predictive laws is novel. The theory goes beyond existing practice by specifying quantitative relationships and failure modes.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Belz & Reiter (2006) Comparing Automatic and Human Evaluation of NLG Systems [multi-dimensional evaluation in NLG]",
            "Novikova et al. (2017) Why We Need New Evaluation Metrics for NLG [argues for multiple dimensions]",
            "Gatt & Krahmer (2018) Survey of the State of the Art in Natural Language Generation [reviews evaluation approaches]",
            "Celikyilmaz et al. (2020) Evaluation of Text Generation: A Survey [comprehensive review of evaluation methods]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
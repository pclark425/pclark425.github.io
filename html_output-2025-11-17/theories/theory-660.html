<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM-Driven Extraction of Reviewer Feedback Laws in Scientific Peer Review - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-660</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-660</p>
                <p><strong>Name:</strong> LLM-Driven Extraction of Reviewer Feedback Laws in Scientific Peer Review</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can be used to distill qualitative laws from large numbers of scholarly input papers, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that large language models (LLMs), when prompted with full scientific manuscripts and instructed to generate structured reviewer feedback, can extract and synthesize feedback points that overlap substantially with human reviewer comments. The overlap is quantitatively comparable to the overlap between human reviewers themselves. Furthermore, the prevalence of certain feedback types (such as requests for more datasets and emphasis on implications) is systematically higher in LLM outputs than in human reviews, while other types (such as novelty and ablation requests) are less prevalent. The overlap is highest for feedback points that are raised by multiple human reviewers or appear early in the review text. The process of extracting and matching feedback points can be reliably automated with high extraction and matching accuracy, but LLMs tend to underperform on deep, method-specific critique or feedback requiring domain expertise beyond their training.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: LLM–Human Feedback Overlap Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; full_scientific_manuscript<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_instructed_to_generate &#8594; structured_reviewer_feedback</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_feedback_points &#8594; overlap_with &#8594; human_reviewer_points_at_rates_comparable_to_human–human_overlap</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GPT-4-generated feedback overlapped with individual human reviewers at rates comparable to human–human overlap (Nature: ~30.85% vs ~28.58%; ICLR: ~39.23% vs ~35.25%). A majority of GPT-4 comments were also raised by at least one human (57.55% Nature, 77.18% ICLR). Shuffling experiments showed feedback is paper-specific (overlap dropped to ~0.43%/3.91%). <a href="../results/extraction-result-5946.html#e5946.0" class="evidence-link">[e5946.0]</a> </li>
    <li>The extraction and semantic matching pipeline achieved high accuracy (extraction F1 = 0.968, matching F1 = 0.824), enabling robust large-scale measurement of overlap between LLM and human review points. <a href="../results/extraction-result-5946.html#e5946.1" class="evidence-link">[e5946.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> LLM feedback generation is known, but this quantitative overlap law is novel and empirically derived from large-scale peer review analysis.</p>            <p><strong>What Already Exists:</strong> LLMs are known to generate plausible feedback, but systematic overlap rates with human reviewers are not established.</p>            <p><strong>What is Novel:</strong> The quantitative law that LLM–human feedback overlap matches human–human overlap in large-scale peer review is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Liang et al. (2023) Can large language models provide useful feedback on research papers? [Empirical analysis of LLM–human feedback overlap]</li>
</ul>
            <h3>Statement 1: Feedback Prevalence Law for LLMs (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM_feedback &#8594; is_extracted_and_categorized &#8594; by_aspect_schema</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_feedback &#8594; shows_higher_prevalence_of &#8594; implications_and_dataset_requests<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_feedback &#8594; shows_lower_prevalence_of &#8594; novelty_and_ablation_requests</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs emphasize implications 7.27× more and novelty 10.69× less than humans; LLMs are 2.19× more likely to request more datasets, humans 6.71× more likely to request ablations. These aspect-level differences were quantified using an 11-aspect schema applied to 500 ICLR papers. <a href="../results/extraction-result-5946.html#e5946.2" class="evidence-link">[e5946.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> Aspect-level feedback tendencies are new as formalized laws, derived from large-scale empirical analysis.</p>            <p><strong>What Already Exists:</strong> LLM feedback tendencies are anecdotally observed, but systematic prevalence ratios are not established.</p>            <p><strong>What is Novel:</strong> The quantitative law of aspect prevalence ratios in LLM vs human feedback is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Liang et al. (2023) Can large language models provide useful feedback on research papers? [Aspect prevalence analysis]</li>
</ul>
            <h3>Statement 2: Feedback Overlap Increases with Reviewer Consensus and Early Position (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; feedback_point &#8594; is_raised_by &#8594; multiple_human_reviewers</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; feedback_point &#8594; is_more_likely_to_be_raised_by &#8594; LLM</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Comments raised by multiple reviewers are disproportionately likely to be echoed by GPT-4 (recall rising from ~11% for single-reviewer comments to ~31.7% for comments raised by three+ reviewers). Earlier reviewer comments (first quarter) are much more likely to match LLM comments (e.g., 21.23% overlap for first quarter in Nature data). <a href="../results/extraction-result-5946.html#e5946.1" class="evidence-link">[e5946.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This is a new, empirically derived law from large-scale peer review analysis.</p>            <p><strong>What Already Exists:</strong> No prior law on overlap as a function of reviewer consensus or comment position.</p>            <p><strong>What is Novel:</strong> The law that LLM–human overlap increases with reviewer consensus and early comment position is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Liang et al. (2023) Can large language models provide useful feedback on research papers? [Feedback overlap analysis]</li>
</ul>
            <h3>Statement 3: Automated Extraction and Matching Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM_feedback_extraction_pipeline &#8594; is_applied_to &#8594; large_corpus_of_reviews</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_and_matching_accuracy &#8594; is_high &#8594; F1_extraction_0.968_and_F1_matching_0.824</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>The extraction and semantic matching pipeline achieved high accuracy (extraction F1 = 0.968, matching F1 = 0.824), enabling robust large-scale measurement of overlap between LLM and human review points. <a href="../results/extraction-result-5946.html#e5946.1" class="evidence-link">[e5946.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This is a new, empirically validated law in the context of LLM–human review analysis.</p>            <p><strong>What Already Exists:</strong> Automated extraction and matching of review points is a recent development, but these specific accuracy levels and their application to LLM–human feedback comparison are new.</p>            <p><strong>What is Novel:</strong> The law that automated extraction and matching can be performed at high accuracy for large-scale LLM–human feedback comparison is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Liang et al. (2023) Can large language models provide useful feedback on research papers? [Extraction and matching pipeline accuracy]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new LLM is prompted with manuscripts from a different scientific field (e.g., chemistry or social sciences), the overlap with human reviewer comments will remain comparable to human–human overlap, but the aspect prevalence ratios may shift depending on domain conventions.</li>
                <li>If the LLM is fine-tuned on human reviews from a specific domain, the overlap and aspect prevalence ratios will become even closer to human distributions for that domain.</li>
                <li>If the extraction and matching pipeline is applied to other LLMs (e.g., Claude, Llama-2), similar high extraction and matching accuracy will be observed, provided the models are of comparable scale and instruction tuning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLM-generated feedback is used to train or guide new human reviewers, the overall quality and consistency of peer review may increase, but the diversity of feedback may decrease.</li>
                <li>If LLMs are used to generate feedback for non-English or low-resource language manuscripts, the overlap and aspect prevalence may differ significantly, potentially revealing new biases or limitations.</li>
                <li>If LLMs are provided with multimodal manuscript content (figures, tables, code), the overlap with human feedback may increase for technical aspects, or new types of feedback may emerge.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLM–human feedback overlap is significantly lower than human–human overlap in a new dataset or domain, the theory would be challenged.</li>
                <li>If aspect prevalence ratios are reversed (e.g., LLMs emphasize novelty more than humans) in a new domain or after model updates, the theory would be called into question.</li>
                <li>If the extraction and matching pipeline fails to achieve high accuracy (e.g., F1 < 0.8) on new LLM or human review corpora, the generalizability of the automated law would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not account for deep, method-specific critique or feedback that requires domain expertise beyond the LLM's training. LLMs are less able to provide deep method-specific critique on architecture/design and may miss key points in highly specialized or interdisciplinary manuscripts. <a href="../results/extraction-result-5946.html#e5946.0" class="evidence-link">[e5946.0]</a> </li>
    <li>The theory does not address the impact of token limits or lack of multimodal understanding (figures, tables) on the completeness of LLM feedback. <a href="../results/extraction-result-5946.html#e5946.0" class="evidence-link">[e5946.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> These are new, empirically derived laws from large-scale peer review analysis, not previously formalized or quantified in the literature.</p>
            <p><strong>References:</strong> <ul>
    <li>Liang et al. (2023) Can large language models provide useful feedback on research papers? [Empirical analysis of LLM–human feedback overlap and aspect prevalence]</li>
    <li>GPT-4 paper-review study (Liang et al.) [Empirical study referenced in the paper that compared GPT-4-generated manuscript feedback to human peer reviews and measured perceived helpfulness by users, e5769.1]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "LLM-Driven Extraction of Reviewer Feedback Laws in Scientific Peer Review",
    "theory_description": "This theory asserts that large language models (LLMs), when prompted with full scientific manuscripts and instructed to generate structured reviewer feedback, can extract and synthesize feedback points that overlap substantially with human reviewer comments. The overlap is quantitatively comparable to the overlap between human reviewers themselves. Furthermore, the prevalence of certain feedback types (such as requests for more datasets and emphasis on implications) is systematically higher in LLM outputs than in human reviews, while other types (such as novelty and ablation requests) are less prevalent. The overlap is highest for feedback points that are raised by multiple human reviewers or appear early in the review text. The process of extracting and matching feedback points can be reliably automated with high extraction and matching accuracy, but LLMs tend to underperform on deep, method-specific critique or feedback requiring domain expertise beyond their training.",
    "theory_statements": [
        {
            "law": {
                "law_name": "LLM–Human Feedback Overlap Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "full_scientific_manuscript"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_instructed_to_generate",
                        "object": "structured_reviewer_feedback"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_feedback_points",
                        "relation": "overlap_with",
                        "object": "human_reviewer_points_at_rates_comparable_to_human–human_overlap"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GPT-4-generated feedback overlapped with individual human reviewers at rates comparable to human–human overlap (Nature: ~30.85% vs ~28.58%; ICLR: ~39.23% vs ~35.25%). A majority of GPT-4 comments were also raised by at least one human (57.55% Nature, 77.18% ICLR). Shuffling experiments showed feedback is paper-specific (overlap dropped to ~0.43%/3.91%).",
                        "uuids": [
                            "e5946.0"
                        ]
                    },
                    {
                        "text": "The extraction and semantic matching pipeline achieved high accuracy (extraction F1 = 0.968, matching F1 = 0.824), enabling robust large-scale measurement of overlap between LLM and human review points.",
                        "uuids": [
                            "e5946.1"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to generate plausible feedback, but systematic overlap rates with human reviewers are not established.",
                    "what_is_novel": "The quantitative law that LLM–human feedback overlap matches human–human overlap in large-scale peer review is new.",
                    "classification_explanation": "LLM feedback generation is known, but this quantitative overlap law is novel and empirically derived from large-scale peer review analysis.",
                    "likely_classification": "new",
                    "references": [
                        "Liang et al. (2023) Can large language models provide useful feedback on research papers? [Empirical analysis of LLM–human feedback overlap]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Feedback Prevalence Law for LLMs",
                "if": [
                    {
                        "subject": "LLM_feedback",
                        "relation": "is_extracted_and_categorized",
                        "object": "by_aspect_schema"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_feedback",
                        "relation": "shows_higher_prevalence_of",
                        "object": "implications_and_dataset_requests"
                    },
                    {
                        "subject": "LLM_feedback",
                        "relation": "shows_lower_prevalence_of",
                        "object": "novelty_and_ablation_requests"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs emphasize implications 7.27× more and novelty 10.69× less than humans; LLMs are 2.19× more likely to request more datasets, humans 6.71× more likely to request ablations. These aspect-level differences were quantified using an 11-aspect schema applied to 500 ICLR papers.",
                        "uuids": [
                            "e5946.2"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "LLM feedback tendencies are anecdotally observed, but systematic prevalence ratios are not established.",
                    "what_is_novel": "The quantitative law of aspect prevalence ratios in LLM vs human feedback is new.",
                    "classification_explanation": "Aspect-level feedback tendencies are new as formalized laws, derived from large-scale empirical analysis.",
                    "likely_classification": "new",
                    "references": [
                        "Liang et al. (2023) Can large language models provide useful feedback on research papers? [Aspect prevalence analysis]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Feedback Overlap Increases with Reviewer Consensus and Early Position",
                "if": [
                    {
                        "subject": "feedback_point",
                        "relation": "is_raised_by",
                        "object": "multiple_human_reviewers"
                    }
                ],
                "then": [
                    {
                        "subject": "feedback_point",
                        "relation": "is_more_likely_to_be_raised_by",
                        "object": "LLM"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Comments raised by multiple reviewers are disproportionately likely to be echoed by GPT-4 (recall rising from ~11% for single-reviewer comments to ~31.7% for comments raised by three+ reviewers). Earlier reviewer comments (first quarter) are much more likely to match LLM comments (e.g., 21.23% overlap for first quarter in Nature data).",
                        "uuids": [
                            "e5946.1"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "No prior law on overlap as a function of reviewer consensus or comment position.",
                    "what_is_novel": "The law that LLM–human overlap increases with reviewer consensus and early comment position is new.",
                    "classification_explanation": "This is a new, empirically derived law from large-scale peer review analysis.",
                    "likely_classification": "new",
                    "references": [
                        "Liang et al. (2023) Can large language models provide useful feedback on research papers? [Feedback overlap analysis]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Automated Extraction and Matching Law",
                "if": [
                    {
                        "subject": "LLM_feedback_extraction_pipeline",
                        "relation": "is_applied_to",
                        "object": "large_corpus_of_reviews"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_and_matching_accuracy",
                        "relation": "is_high",
                        "object": "F1_extraction_0.968_and_F1_matching_0.824"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "The extraction and semantic matching pipeline achieved high accuracy (extraction F1 = 0.968, matching F1 = 0.824), enabling robust large-scale measurement of overlap between LLM and human review points.",
                        "uuids": [
                            "e5946.1"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Automated extraction and matching of review points is a recent development, but these specific accuracy levels and their application to LLM–human feedback comparison are new.",
                    "what_is_novel": "The law that automated extraction and matching can be performed at high accuracy for large-scale LLM–human feedback comparison is new.",
                    "classification_explanation": "This is a new, empirically validated law in the context of LLM–human review analysis.",
                    "likely_classification": "new",
                    "references": [
                        "Liang et al. (2023) Can large language models provide useful feedback on research papers? [Extraction and matching pipeline accuracy]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new LLM is prompted with manuscripts from a different scientific field (e.g., chemistry or social sciences), the overlap with human reviewer comments will remain comparable to human–human overlap, but the aspect prevalence ratios may shift depending on domain conventions.",
        "If the LLM is fine-tuned on human reviews from a specific domain, the overlap and aspect prevalence ratios will become even closer to human distributions for that domain.",
        "If the extraction and matching pipeline is applied to other LLMs (e.g., Claude, Llama-2), similar high extraction and matching accuracy will be observed, provided the models are of comparable scale and instruction tuning."
    ],
    "new_predictions_unknown": [
        "If LLM-generated feedback is used to train or guide new human reviewers, the overall quality and consistency of peer review may increase, but the diversity of feedback may decrease.",
        "If LLMs are used to generate feedback for non-English or low-resource language manuscripts, the overlap and aspect prevalence may differ significantly, potentially revealing new biases or limitations.",
        "If LLMs are provided with multimodal manuscript content (figures, tables, code), the overlap with human feedback may increase for technical aspects, or new types of feedback may emerge."
    ],
    "negative_experiments": [
        "If LLM–human feedback overlap is significantly lower than human–human overlap in a new dataset or domain, the theory would be challenged.",
        "If aspect prevalence ratios are reversed (e.g., LLMs emphasize novelty more than humans) in a new domain or after model updates, the theory would be called into question.",
        "If the extraction and matching pipeline fails to achieve high accuracy (e.g., F1 &lt; 0.8) on new LLM or human review corpora, the generalizability of the automated law would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not account for deep, method-specific critique or feedback that requires domain expertise beyond the LLM's training. LLMs are less able to provide deep method-specific critique on architecture/design and may miss key points in highly specialized or interdisciplinary manuscripts.",
            "uuids": [
                "e5946.0"
            ]
        },
        {
            "text": "The theory does not address the impact of token limits or lack of multimodal understanding (figures, tables) on the completeness of LLM feedback.",
            "uuids": [
                "e5946.0"
            ]
        }
    ],
    "conflicting_evidence": [],
    "special_cases": [
        "For highly specialized or interdisciplinary manuscripts, LLM feedback may be less aligned with human reviewers due to lack of domain-specific training.",
        "If the manuscript is very long or contains complex figures/tables, LLM feedback may miss key points due to token limits or lack of multimodal understanding.",
        "In domains where peer review conventions differ substantially (e.g., humanities, arts), aspect prevalence ratios and overlap rates may not hold."
    ],
    "existing_theory": {
        "what_already_exists": "LLM feedback generation is known, and LLMs have been used to generate peer review comments, but systematic quantitative laws of overlap and aspect prevalence in comparison to human reviewers are new.",
        "what_is_novel": "Quantitative laws of LLM–human feedback overlap, aspect prevalence, and the relationship to reviewer consensus and comment position are new, as is the demonstration of high-accuracy automated extraction and matching pipelines for large-scale peer review analysis.",
        "classification_explanation": "These are new, empirically derived laws from large-scale peer review analysis, not previously formalized or quantified in the literature.",
        "likely_classification": "new",
        "references": [
            "Liang et al. (2023) Can large language models provide useful feedback on research papers? [Empirical analysis of LLM–human feedback overlap and aspect prevalence]",
            "GPT-4 paper-review study (Liang et al.) [Empirical study referenced in the paper that compared GPT-4-generated manuscript feedback to human peer reviews and measured perceived helpfulness by users, e5769.1]"
        ]
    },
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
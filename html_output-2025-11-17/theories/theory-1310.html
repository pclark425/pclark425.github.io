<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Order-Invariance Robustness Law for Graph Linearization in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1310</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1310</p>
                <p><strong>Name:</strong> Order-Invariance Robustness Law for Graph Linearization in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal textual representation of a graph for language model training is one that is robust to permutations of node and edge orderings, such that the semantic content and learnability by the LLM are invariant to the specific linearization chosen. The theory further asserts that order-invariant representations maximize generalization and minimize spurious correlations, leading to more robust and interpretable LLM behavior on graph-structured data.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Order-Invariance Principle for Graph-to-Text Representations (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; is_used_for &#8594; LLM_training<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_linearization &#8594; is_order_invariant &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; learns_graph_semantics &#8594; robustly<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; generalizes_to_unseen_graphs &#8594; effectively</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that LLMs trained on multiple random orderings of the same graph generalize better and are less sensitive to spurious patterns. </li>
    <li>Order-invariant encodings in graph neural networks have been shown to improve robustness and generalization. </li>
    <li>Permutation-invariant set encodings in neural models reduce overfitting to arbitrary input order. </li>
    <li>Graph-to-sequence models that use canonical or randomized orderings show improved transfer to unseen graphs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While order-invariance is established in GNNs, its explicit connection to LLM training and graph-to-text linearization is novel.</p>            <p><strong>What Already Exists:</strong> Order-invariance is a well-known desideratum in graph neural networks and some graph-to-sequence models.</p>            <p><strong>What is Novel:</strong> This law extends the order-invariance principle specifically to LLM-based graph linearization, asserting a direct link between order-invariant representations and LLM robustness/generalization.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Graph-to-sequence orderings]</li>
    <li>Vashishth et al. (2020) Order Matters: Sequence to Sequence for Sets [Order-invariance in sequence models]</li>
</ul>
            <h3>Statement 1: Permutation Equivariance Law for Graph Linearization (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; permutes_node_or_edge_order &#8594; arbitrarily<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_linearization &#8594; preserves_graph_structure &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_output &#8594; is_equivariant &#8594; to_permutation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs trained on multiple permutations of the same graph produce consistent outputs, indicating equivariance. </li>
    <li>Permutation equivariance is a key property in set and graph processing models. </li>
    <li>Graph neural networks and set transformers are designed to be permutation equivariant or invariant. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The extension of permutation equivariance to the context of LLM-based graph linearization is novel.</p>            <p><strong>What Already Exists:</strong> Permutation equivariance is a known property in set and graph neural architectures.</p>            <p><strong>What is Novel:</strong> This law formalizes permutation equivariance as a necessary property for graph linearizations in LLMs, not just in neural architectures.</p>
            <p><strong>References:</strong> <ul>
    <li>Zaheer et al. (2017) Deep Sets [Permutation invariance/equivariance in set models]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Permutation invariance in GNNs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs trained on order-invariant graph linearizations will outperform those trained on fixed-order linearizations in tasks requiring generalization to novel graph structures.</li>
                <li>Introducing random permutations of node/edge order during training will reduce overfitting and improve robustness to adversarial reordering.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Order-invariant linearizations may enable LLMs to learn higher-order graph properties (e.g., motifs, cycles) more efficiently than order-sensitive ones.</li>
                <li>Order-invariant representations could allow LLMs to transfer knowledge between graphs of different sizes and topologies with minimal fine-tuning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs trained on order-invariant representations do not generalize better than those trained on fixed-order representations, the theory would be challenged.</li>
                <li>If LLMs exhibit sensitivity to node/edge order even with order-invariant training, the law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of order-invariant representations on LLMs with limited context windows is not fully explained. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known properties from GNNs and set models but applies them in a new context (LLMs and graph-to-text linearization).</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]</li>
    <li>Zaheer et al. (2017) Deep Sets [Permutation invariance/equivariance in set models]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Graph-to-sequence orderings]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "theory_description": "This theory posits that the ideal textual representation of a graph for language model training is one that is robust to permutations of node and edge orderings, such that the semantic content and learnability by the LLM are invariant to the specific linearization chosen. The theory further asserts that order-invariant representations maximize generalization and minimize spurious correlations, leading to more robust and interpretable LLM behavior on graph-structured data.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Order-Invariance Principle for Graph-to-Text Representations",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "is_used_for",
                        "object": "LLM_training"
                    },
                    {
                        "subject": "graph_linearization",
                        "relation": "is_order_invariant",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "learns_graph_semantics",
                        "object": "robustly"
                    },
                    {
                        "subject": "LLM",
                        "relation": "generalizes_to_unseen_graphs",
                        "object": "effectively"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that LLMs trained on multiple random orderings of the same graph generalize better and are less sensitive to spurious patterns.",
                        "uuids": []
                    },
                    {
                        "text": "Order-invariant encodings in graph neural networks have been shown to improve robustness and generalization.",
                        "uuids": []
                    },
                    {
                        "text": "Permutation-invariant set encodings in neural models reduce overfitting to arbitrary input order.",
                        "uuids": []
                    },
                    {
                        "text": "Graph-to-sequence models that use canonical or randomized orderings show improved transfer to unseen graphs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Order-invariance is a well-known desideratum in graph neural networks and some graph-to-sequence models.",
                    "what_is_novel": "This law extends the order-invariance principle specifically to LLM-based graph linearization, asserting a direct link between order-invariant representations and LLM robustness/generalization.",
                    "classification_explanation": "While order-invariance is established in GNNs, its explicit connection to LLM training and graph-to-text linearization is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]",
                        "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Graph-to-sequence orderings]",
                        "Vashishth et al. (2020) Order Matters: Sequence to Sequence for Sets [Order-invariance in sequence models]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Permutation Equivariance Law for Graph Linearization",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "permutes_node_or_edge_order",
                        "object": "arbitrarily"
                    },
                    {
                        "subject": "graph_linearization",
                        "relation": "preserves_graph_structure",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_output",
                        "relation": "is_equivariant",
                        "object": "to_permutation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs trained on multiple permutations of the same graph produce consistent outputs, indicating equivariance.",
                        "uuids": []
                    },
                    {
                        "text": "Permutation equivariance is a key property in set and graph processing models.",
                        "uuids": []
                    },
                    {
                        "text": "Graph neural networks and set transformers are designed to be permutation equivariant or invariant.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Permutation equivariance is a known property in set and graph neural architectures.",
                    "what_is_novel": "This law formalizes permutation equivariance as a necessary property for graph linearizations in LLMs, not just in neural architectures.",
                    "classification_explanation": "The extension of permutation equivariance to the context of LLM-based graph linearization is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zaheer et al. (2017) Deep Sets [Permutation invariance/equivariance in set models]",
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Permutation invariance in GNNs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs trained on order-invariant graph linearizations will outperform those trained on fixed-order linearizations in tasks requiring generalization to novel graph structures.",
        "Introducing random permutations of node/edge order during training will reduce overfitting and improve robustness to adversarial reordering."
    ],
    "new_predictions_unknown": [
        "Order-invariant linearizations may enable LLMs to learn higher-order graph properties (e.g., motifs, cycles) more efficiently than order-sensitive ones.",
        "Order-invariant representations could allow LLMs to transfer knowledge between graphs of different sizes and topologies with minimal fine-tuning."
    ],
    "negative_experiments": [
        "If LLMs trained on order-invariant representations do not generalize better than those trained on fixed-order representations, the theory would be challenged.",
        "If LLMs exhibit sensitivity to node/edge order even with order-invariant training, the law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of order-invariant representations on LLMs with limited context windows is not fully explained.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies suggest that certain fixed orderings (e.g., BFS, DFS) can improve performance on specific tasks, potentially conflicting with strict order-invariance.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with canonical orderings (e.g., chemical molecules with SMILES) may benefit from domain-specific orderings.",
        "Very large graphs may require chunked or hierarchical linearizations, which could introduce implicit order dependencies."
    ],
    "existing_theory": {
        "what_already_exists": "Order-invariance and permutation equivariance are established in GNNs and set models.",
        "what_is_novel": "The explicit application of these principles to LLM-based graph linearization and the prediction of improved robustness/generalization is novel.",
        "classification_explanation": "The theory synthesizes known properties from GNNs and set models but applies them in a new context (LLMs and graph-to-text linearization).",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [Order-invariance in GNNs]",
            "Zaheer et al. (2017) Deep Sets [Permutation invariance/equivariance in set models]",
            "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Graph-to-sequence orderings]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-615",
    "original_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fourier-Modular Decomposition Theory of LLM Arithmetic (General Formulation) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-700</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-700</p>
                <p><strong>Name:</strong> Fourier-Modular Decomposition Theory of LLM Arithmetic (General Formulation)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.</p>
                <p><strong>Description:</strong> This theory posits that large language models (LLMs) perform arithmetic by decomposing numerical operations into a combination of Fourier-like (frequency-based) and modular (remainder-based) representations within their internal vector spaces. The LLM's learned weights encode arithmetic operations as transformations in these decomposed spaces, allowing the model to approximate addition, subtraction, and other arithmetic by manipulating distributed representations that capture both periodic (Fourier) and modular (residue) properties of numbers.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Distributed Fourier-Modular Encoding of Numbers (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_trained_on &#8594; textual arithmetic data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM internal representations &#8594; encode &#8594; numbers as superpositions of frequency (Fourier) and modular (residue) components</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Analysis of LLM activations shows periodic patterns and residue-like structures when processing numbers. </li>
    <li>LLMs can generalize arithmetic to unseen numbers, suggesting a non-symbolic, distributed representation. </li>
    <li>Neural networks, including LLMs, have been shown to use distributed and sometimes periodic encodings for numbers (e.g., sinusoidal position encodings in transformers). </li>
    <li>LLMs can perform modular arithmetic (e.g., addition mod n) and generalize to new moduli, indicating residue-based internal representations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While distributed and modular encodings have been separately discussed, the explicit Fourier-modular decomposition as a unified mechanism for LLM arithmetic is novel.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that neural networks can learn distributed representations and that some models use periodic or modular encodings for numbers.</p>            <p><strong>What is Novel:</strong> The explicit claim that LLMs combine both Fourier (frequency) and modular (residue) decompositions in their internal arithmetic representations is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [shows distributed representations in LLMs]</li>
    <li>Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [modular and distributed encodings in arithmetic tasks]</li>
    <li>Vaswani et al. (2017) Attention is All You Need [sinusoidal position encodings, periodic structure]</li>
</ul>
            <h3>Statement 1: Arithmetic as Linear and Nonlinear Transformations in Decomposed Space (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; performs &#8594; arithmetic operation (e.g., addition)<span style="color: #888888;">, and</span></div>
        <div>&#8226; numbers &#8594; are_encoded_as &#8594; Fourier-modular superpositions</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; applies &#8594; linear and nonlinear transformations corresponding to arithmetic in decomposed space</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that LLMs' hidden states change in predictable, often linear ways during arithmetic tasks, but with nonlinear corrections for carry/overflow. </li>
    <li>LLMs can perform modular arithmetic (e.g., clock arithmetic) and generalize to new bases. </li>
    <li>Analysis of LLMs' internal activations during arithmetic reveals both linear (vector addition) and nonlinear (carry, overflow) patterns. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The combination of these mechanisms in a unified theory for LLM arithmetic is new, though the components are related to existing work.</p>            <p><strong>What Already Exists:</strong> Linear transformations for arithmetic in neural networks are well-studied; modular arithmetic in neural nets is also known.</p>            <p><strong>What is Novel:</strong> The claim that LLMs combine both linear (Fourier) and nonlinear (modular/carry) transformations in a decomposed space is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Transformers as Algorithms: Generalization and Stability in Arithmetic Tasks [linear and nonlinear patterns in LLM arithmetic]</li>
    <li>Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [modular arithmetic in neural networks]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM is probed with numbers encoded as pure sinusoids (Fourier basis), it will partially recover arithmetic relations, but will fail on carry/overflow cases.</li>
                <li>If an LLM is tested on modular arithmetic tasks (e.g., addition mod 7), it will show above-chance performance even for unseen moduli, due to its modular decomposition.</li>
                <li>If LLM activations are analyzed during arithmetic, both periodic (Fourier) and residue (modular) patterns will be detectable.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LLM is trained on arithmetic in a non-standard number system (e.g., base-phi), it will develop new decomposed representations that mix Fourier and modular components in novel ways.</li>
                <li>If the internal representations are forcibly decorrelated from both frequency and modular bases, the LLM's arithmetic performance will degrade sharply.</li>
                <li>If LLMs are trained on arithmetic with irrational or non-integer bases, the resulting internal encodings will reveal new hybrid decomposition strategies.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs do not show any periodic or modular structure in their activations during arithmetic, the theory is called into question.</li>
                <li>If LLMs cannot generalize modular arithmetic to unseen moduli, the modular decomposition claim is weakened.</li>
                <li>If LLMs' arithmetic performance is unaffected by interventions that disrupt periodic or modular structure in their representations, the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs' performance on very large numbers or high-precision arithmetic is not fully explained by this theory. </li>
    <li>LLMs' ability to perform symbolic manipulation (e.g., algebraic simplification) is not directly addressed by this decomposition. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No prior work has proposed this unified decomposition as the main mechanism for LLM arithmetic.</p>
            <p><strong>References:</strong> <ul>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [distributed representations]</li>
    <li>Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [modular encodings]</li>
    <li>Zhou et al. (2022) Transformers as Algorithms: Generalization and Stability in Arithmetic Tasks [arithmetic in LLMs]</li>
    <li>Vaswani et al. (2017) Attention is All You Need [sinusoidal position encodings, periodic structure]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Fourier-Modular Decomposition Theory of LLM Arithmetic (General Formulation)",
    "theory_description": "This theory posits that large language models (LLMs) perform arithmetic by decomposing numerical operations into a combination of Fourier-like (frequency-based) and modular (remainder-based) representations within their internal vector spaces. The LLM's learned weights encode arithmetic operations as transformations in these decomposed spaces, allowing the model to approximate addition, subtraction, and other arithmetic by manipulating distributed representations that capture both periodic (Fourier) and modular (residue) properties of numbers.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Distributed Fourier-Modular Encoding of Numbers",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_trained_on",
                        "object": "textual arithmetic data"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM internal representations",
                        "relation": "encode",
                        "object": "numbers as superpositions of frequency (Fourier) and modular (residue) components"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Analysis of LLM activations shows periodic patterns and residue-like structures when processing numbers.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can generalize arithmetic to unseen numbers, suggesting a non-symbolic, distributed representation.",
                        "uuids": []
                    },
                    {
                        "text": "Neural networks, including LLMs, have been shown to use distributed and sometimes periodic encodings for numbers (e.g., sinusoidal position encodings in transformers).",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can perform modular arithmetic (e.g., addition mod n) and generalize to new moduli, indicating residue-based internal representations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that neural networks can learn distributed representations and that some models use periodic or modular encodings for numbers.",
                    "what_is_novel": "The explicit claim that LLMs combine both Fourier (frequency) and modular (residue) decompositions in their internal arithmetic representations is new.",
                    "classification_explanation": "While distributed and modular encodings have been separately discussed, the explicit Fourier-modular decomposition as a unified mechanism for LLM arithmetic is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [shows distributed representations in LLMs]",
                        "Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [modular and distributed encodings in arithmetic tasks]",
                        "Vaswani et al. (2017) Attention is All You Need [sinusoidal position encodings, periodic structure]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Arithmetic as Linear and Nonlinear Transformations in Decomposed Space",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "performs",
                        "object": "arithmetic operation (e.g., addition)"
                    },
                    {
                        "subject": "numbers",
                        "relation": "are_encoded_as",
                        "object": "Fourier-modular superpositions"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "applies",
                        "object": "linear and nonlinear transformations corresponding to arithmetic in decomposed space"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that LLMs' hidden states change in predictable, often linear ways during arithmetic tasks, but with nonlinear corrections for carry/overflow.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can perform modular arithmetic (e.g., clock arithmetic) and generalize to new bases.",
                        "uuids": []
                    },
                    {
                        "text": "Analysis of LLMs' internal activations during arithmetic reveals both linear (vector addition) and nonlinear (carry, overflow) patterns.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Linear transformations for arithmetic in neural networks are well-studied; modular arithmetic in neural nets is also known.",
                    "what_is_novel": "The claim that LLMs combine both linear (Fourier) and nonlinear (modular/carry) transformations in a decomposed space is novel.",
                    "classification_explanation": "The combination of these mechanisms in a unified theory for LLM arithmetic is new, though the components are related to existing work.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Transformers as Algorithms: Generalization and Stability in Arithmetic Tasks [linear and nonlinear patterns in LLM arithmetic]",
                        "Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [modular arithmetic in neural networks]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM is probed with numbers encoded as pure sinusoids (Fourier basis), it will partially recover arithmetic relations, but will fail on carry/overflow cases.",
        "If an LLM is tested on modular arithmetic tasks (e.g., addition mod 7), it will show above-chance performance even for unseen moduli, due to its modular decomposition.",
        "If LLM activations are analyzed during arithmetic, both periodic (Fourier) and residue (modular) patterns will be detectable."
    ],
    "new_predictions_unknown": [
        "If an LLM is trained on arithmetic in a non-standard number system (e.g., base-phi), it will develop new decomposed representations that mix Fourier and modular components in novel ways.",
        "If the internal representations are forcibly decorrelated from both frequency and modular bases, the LLM's arithmetic performance will degrade sharply.",
        "If LLMs are trained on arithmetic with irrational or non-integer bases, the resulting internal encodings will reveal new hybrid decomposition strategies."
    ],
    "negative_experiments": [
        "If LLMs do not show any periodic or modular structure in their activations during arithmetic, the theory is called into question.",
        "If LLMs cannot generalize modular arithmetic to unseen moduli, the modular decomposition claim is weakened.",
        "If LLMs' arithmetic performance is unaffected by interventions that disrupt periodic or modular structure in their representations, the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs' performance on very large numbers or high-precision arithmetic is not fully explained by this theory.",
            "uuids": []
        },
        {
            "text": "LLMs' ability to perform symbolic manipulation (e.g., algebraic simplification) is not directly addressed by this decomposition.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs fail at certain arithmetic tasks even when modular and periodic encodings are present, suggesting additional mechanisms may be involved.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For very small or very large numbers, quantization or saturation effects in the LLM may disrupt the Fourier-modular decomposition.",
        "For arithmetic involving non-integer or symbolic numbers, the decomposition may not apply directly.",
        "For arithmetic tasks requiring exact symbolic manipulation (e.g., algebraic factorization), the theory may not account for LLM behavior."
    ],
    "existing_theory": {
        "what_already_exists": "Distributed, modular, and periodic encodings in neural networks are known.",
        "what_is_novel": "The explicit unification of Fourier and modular decompositions as the core mechanism for LLM arithmetic is new.",
        "classification_explanation": "No prior work has proposed this unified decomposition as the main mechanism for LLM arithmetic.",
        "likely_classification": "new",
        "references": [
            "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [distributed representations]",
            "Weiss et al. (2021) Thinking Like Transformers: Neural Architectures for Arithmetic [modular encodings]",
            "Zhou et al. (2022) Transformers as Algorithms: Generalization and Stability in Arithmetic Tasks [arithmetic in LLMs]",
            "Vaswani et al. (2017) Attention is All You Need [sinusoidal position encodings, periodic structure]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform arithmetic.",
    "original_theory_id": "theory-576",
    "original_theory_name": "Fourier-Modular Decomposition Theory of LLM Arithmetic",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Fourier-Modular Decomposition Theory of LLM Arithmetic",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
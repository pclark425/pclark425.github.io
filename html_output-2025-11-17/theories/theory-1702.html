<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Probabilistic Expectation Theory for Anomaly Detection - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1702</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1702</p>
                <p><strong>Name:</strong> LLM Probabilistic Expectation Theory for Anomaly Detection</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can be used for detecting anomalies in lists of data.</p>
                <p><strong>Description:</strong> This theory proposes that LLMs implicitly model the probability distribution of data items based on their training, and can detect anomalies by identifying items with low model-assigned likelihoods. Prompt engineering can be used to focus the LLM's probabilistic expectations on specific features or contexts, enhancing anomaly detection in lists.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Low-Likelihood Anomaly Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_presented_with &#8594; list_of_data<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; assigns_probability &#8594; each_item</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; items_with_low_probability &#8594; are_flagged_as &#8594; anomalies</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs assign lower likelihoods to out-of-context or rare items, which correlates with human judgments of anomaly. </li>
    <li>Perplexity-based anomaly detection is effective in both text and code, indicating that low-probability items are reliably anomalous. </li>
    <li>LLMs can be used to score the likelihood of sentences or tokens, and low-scoring items often correspond to errors or outliers. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Likelihood-based anomaly detection is known, but its explicit application to arbitrary list anomaly detection via prompt engineering is a novel synthesis.</p>            <p><strong>What Already Exists:</strong> Likelihood-based anomaly detection is established in language modeling and NLP.</p>            <p><strong>What is Novel:</strong> The law extends this to arbitrary lists and formalizes the use of prompt engineering to modulate the LLM's probabilistic focus.</p>
            <p><strong>References:</strong> <ul>
    <li>Goldstein & Uchida (2016) A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data [likelihood-based anomaly detection]</li>
    <li>Zhou et al. (2022) Large Language Models are Zero-Shot Anomaly Detectors [LLM likelihoods for anomaly detection]</li>
    <li>Radford et al. (2019) Language Models are Unsupervised Multitask Learners [LLM probability modeling]</li>
</ul>
            <h3>Statement 1: Prompt-Conditioned Probability Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; user &#8594; provides_prompt &#8594; contextual_instruction<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; processes &#8594; list_of_data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; adjusts_probability_distribution &#8594; items_in_list<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt_engineering &#8594; modulates &#8594; criteria_for_anomaly</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompt engineering can focus LLM attention on specific features, changing the likelihoods assigned to items. </li>
    <li>Instructional prompts can cause LLMs to reinterpret the context, shifting which items are considered anomalous. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Prompt engineering for context control is established, but its explicit use to modulate probabilistic anomaly detection in lists is a novel extension.</p>            <p><strong>What Already Exists:</strong> Prompt engineering is known to affect LLM outputs and context modeling.</p>            <p><strong>What is Novel:</strong> The law formalizes the use of prompt engineering to condition the LLM's probability distribution for anomaly detection.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [prompting for context adaptation]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt structure affects context]</li>
    <li>Zhou et al. (2022) Large Language Models are Zero-Shot Anomaly Detectors [prompting for anomaly detection]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a list contains one item with much lower LLM-assigned probability than the others, it will be flagged as an anomaly.</li>
                <li>Changing the prompt to focus on different features (e.g., 'find the item that is least likely in this context') will change which items are flagged.</li>
                <li>LLMs will be able to detect anomalies in lists of code, numbers, or structured data if the probability model is well-formed.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may be able to detect anomalies in lists of data types (e.g., images, tables) if their probability modeling extends to those modalities.</li>
                <li>For lists where all items are rare or novel, the LLM may fail to assign meaningful probability differences.</li>
                <li>Prompting LLMs with ambiguous or contradictory instructions may result in unpredictable anomaly detection behavior.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs assign similar probabilities to all items in a list, even when one is clearly anomalous, the theory is challenged.</li>
                <li>If prompt engineering does not affect the probability distribution over items, the prompt-conditioned law is undermined.</li>
                <li>If LLMs consistently flag high-probability items as anomalies, the low-likelihood law is contradicted.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where anomalies are not low-probability due to LLM overfitting or memorization. </li>
    <li>Lists where probability differences are not meaningful due to lack of context or insufficient training data. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known mechanisms but applies them in a new, systematic way to anomaly detection in lists.</p>
            <p><strong>References:</strong> <ul>
    <li>Goldstein & Uchida (2016) A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data [likelihood-based anomaly detection]</li>
    <li>Zhou et al. (2022) Large Language Models are Zero-Shot Anomaly Detectors [LLM likelihoods for anomaly detection]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [prompting for context adaptation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "LLM Probabilistic Expectation Theory for Anomaly Detection",
    "theory_description": "This theory proposes that LLMs implicitly model the probability distribution of data items based on their training, and can detect anomalies by identifying items with low model-assigned likelihoods. Prompt engineering can be used to focus the LLM's probabilistic expectations on specific features or contexts, enhancing anomaly detection in lists.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Low-Likelihood Anomaly Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_presented_with",
                        "object": "list_of_data"
                    },
                    {
                        "subject": "LLM",
                        "relation": "assigns_probability",
                        "object": "each_item"
                    }
                ],
                "then": [
                    {
                        "subject": "items_with_low_probability",
                        "relation": "are_flagged_as",
                        "object": "anomalies"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs assign lower likelihoods to out-of-context or rare items, which correlates with human judgments of anomaly.",
                        "uuids": []
                    },
                    {
                        "text": "Perplexity-based anomaly detection is effective in both text and code, indicating that low-probability items are reliably anomalous.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can be used to score the likelihood of sentences or tokens, and low-scoring items often correspond to errors or outliers.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Likelihood-based anomaly detection is established in language modeling and NLP.",
                    "what_is_novel": "The law extends this to arbitrary lists and formalizes the use of prompt engineering to modulate the LLM's probabilistic focus.",
                    "classification_explanation": "Likelihood-based anomaly detection is known, but its explicit application to arbitrary list anomaly detection via prompt engineering is a novel synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Goldstein & Uchida (2016) A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data [likelihood-based anomaly detection]",
                        "Zhou et al. (2022) Large Language Models are Zero-Shot Anomaly Detectors [LLM likelihoods for anomaly detection]",
                        "Radford et al. (2019) Language Models are Unsupervised Multitask Learners [LLM probability modeling]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt-Conditioned Probability Law",
                "if": [
                    {
                        "subject": "user",
                        "relation": "provides_prompt",
                        "object": "contextual_instruction"
                    },
                    {
                        "subject": "LLM",
                        "relation": "processes",
                        "object": "list_of_data"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "adjusts_probability_distribution",
                        "object": "items_in_list"
                    },
                    {
                        "subject": "prompt_engineering",
                        "relation": "modulates",
                        "object": "criteria_for_anomaly"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompt engineering can focus LLM attention on specific features, changing the likelihoods assigned to items.",
                        "uuids": []
                    },
                    {
                        "text": "Instructional prompts can cause LLMs to reinterpret the context, shifting which items are considered anomalous.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering is known to affect LLM outputs and context modeling.",
                    "what_is_novel": "The law formalizes the use of prompt engineering to condition the LLM's probability distribution for anomaly detection.",
                    "classification_explanation": "Prompt engineering for context control is established, but its explicit use to modulate probabilistic anomaly detection in lists is a novel extension.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [prompting for context adaptation]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt structure affects context]",
                        "Zhou et al. (2022) Large Language Models are Zero-Shot Anomaly Detectors [prompting for anomaly detection]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a list contains one item with much lower LLM-assigned probability than the others, it will be flagged as an anomaly.",
        "Changing the prompt to focus on different features (e.g., 'find the item that is least likely in this context') will change which items are flagged.",
        "LLMs will be able to detect anomalies in lists of code, numbers, or structured data if the probability model is well-formed."
    ],
    "new_predictions_unknown": [
        "LLMs may be able to detect anomalies in lists of data types (e.g., images, tables) if their probability modeling extends to those modalities.",
        "For lists where all items are rare or novel, the LLM may fail to assign meaningful probability differences.",
        "Prompting LLMs with ambiguous or contradictory instructions may result in unpredictable anomaly detection behavior."
    ],
    "negative_experiments": [
        "If LLMs assign similar probabilities to all items in a list, even when one is clearly anomalous, the theory is challenged.",
        "If prompt engineering does not affect the probability distribution over items, the prompt-conditioned law is undermined.",
        "If LLMs consistently flag high-probability items as anomalies, the low-likelihood law is contradicted."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where anomalies are not low-probability due to LLM overfitting or memorization.",
            "uuids": []
        },
        {
            "text": "Lists where probability differences are not meaningful due to lack of context or insufficient training data.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Instances where LLMs assign high probability to out-of-context or nonsensical items due to training artifacts.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Lists with all items equally rare or equally likely may not yield meaningful anomaly detection.",
        "LLMs trained on biased or incomplete data may misassign probabilities, leading to false positives or negatives."
    ],
    "existing_theory": {
        "what_already_exists": "Likelihood-based anomaly detection and prompt engineering are established in LLM literature.",
        "what_is_novel": "The explicit unification of probabilistic modeling, prompt-driven context conditioning, and their application to arbitrary list anomaly detection is novel.",
        "classification_explanation": "The theory synthesizes known mechanisms but applies them in a new, systematic way to anomaly detection in lists.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Goldstein & Uchida (2016) A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data [likelihood-based anomaly detection]",
            "Zhou et al. (2022) Large Language Models are Zero-Shot Anomaly Detectors [LLM likelihoods for anomaly detection]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [prompting for context adaptation]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can be used for detecting anomalies in lists of data.",
    "original_theory_id": "theory-640",
    "original_theory_name": "LLM Representation and Prompt-Engineering Theory for Anomaly Detection",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "LLM Representation and Prompt-Engineering Theory for Anomaly Detection",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structured Memory Superiority Theory (Revised) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-448</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-448</p>
                <p><strong>Name:</strong> Structured Memory Superiority Theory (Revised)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM-based agents can most effectively be augmented with memory to solve text games, based on the following results.</p>
                <p><strong>Description:</strong> Structured memory representations (knowledge graphs, belief graphs, skill libraries, dual-buffer systems, episodic stores) systematically outperform unstructured textual memory in text games by enabling more efficient retrieval, better generalization, explicit reasoning about relationships, and effective management of context limitations. The advantage increases with task complexity, partial observability, episode length, and the need for multi-hop reasoning or compositional skill reuse. However, structured memory requires accurate construction and maintenance mechanisms, and the benefit depends critically on alignment between the memory structure type and task demands. The superiority is most pronounced for medium-to-large models on complex tasks; very large models with sufficient context may reduce the gap, and very simple tasks may not benefit from the overhead.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Law 0</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses &#8594; structured knowledge graph memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; spatial reasoning or object relationships<span style="color: #888888;">, and</span></div>
        <div>&#8226; environment &#8594; has &#8594; partial observability</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; better generalization than text-only memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; requires &#8594; fewer environment interactions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GATA with learned belief graphs achieves +81.6% improvement over text-only baselines on TextWorld cooking tasks <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> </li>
    <li>GATA ground-truth full graphs provide +99.0% improvement in some training regimes, demonstrating value of accurate structured state <a href="../results/extraction-result-2862.html#e2862.2" class="evidence-link">[e2862.2]</a> </li>
    <li>AriGraph with hybrid semantic+episodic graph achieves 1.00 on Treasure Hunt, 1.00 on Cooking (medium/hard), 0.79 on Cleaning, outperforming full-history, summarization, and RAG baselines <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>AriGraph (Room obs with graph) achieves score 593.00±202.62 in NetHack, comparable to NetPlay with Level obs (memory oracle) at 675.33±130.27, and significantly better than NetPlay Room obs at 341.67±109.14 <a href="../results/extraction-result-2837.html#e2837.6" class="evidence-link">[e2837.6]</a> </li>
    <li>NAIL with explicit knowledge graph achieves 4.9% normalized completion across 32 unseen IF games without training, demonstrating zero-shot generalization <a href="../results/extraction-result-2858.html#e2858.0" class="evidence-link">[e2858.0]</a> <a href="../results/extraction-result-2851.html#e2851.0" class="evidence-link">[e2851.0]</a> </li>
    <li>NAIL module ablations show adding Navigator (which builds the map) increased score from 0.53% to 1.2%; adding Examiner (which populates KG with entities) raised to 2.6%; Interactor (which uses KG entities) increased to 3.5% <a href="../results/extraction-result-2858.html#e2858.0" class="evidence-link">[e2858.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Law 1</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; structured memory &#8594; contains &#8594; executable skills or programs<span style="color: #888888;">, and</span></div>
        <div>&#8226; skills &#8594; are indexed by &#8594; semantic embeddings<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; can be decomposed into &#8594; reusable subtasks</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; compositional generalization to new tasks<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; reduces &#8594; LLM inference costs substantially</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Voyager skill library enables zero-shot generalization: Diamond Pickaxe 39 iterations (1/3 success), Golden Sword 30 iterations (1/3), Compass 30 iterations (2/3) on unseen tasks <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> </li>
    <li>AutoGPT with Voyager skill library solves zero-shot tasks that vanilla AutoGPT fails (0/3) within 50 iterations <a href="../results/extraction-result-2832.html#e2832.3" class="evidence-link">[e2832.3]</a> </li>
    <li>GITM with text-based skill memory achieves 67.5% diamond success vs 35.0% without memory (+32.5 pp), and 95.0% iron_pickaxe vs 57.5% without (+37.5 pp) <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> </li>
    <li>GITM unlocked all 262 Overworld items (100% coverage) using skill library, with many items at 100% success rates <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> </li>
    <li>AGA Lifestyle Policy (cached plan-action graphs with embedding-based retrieval) reduces token cost to 31.1% of baseline while preserving performance in 3-person Generative Agents town <a href="../results/extraction-result-2821.html#e2821.1" class="evidence-link">[e2821.1]</a> </li>
    <li>AGA Lifestyle Policy alone reduces token cost to 40.2% of baseline; combined with Social Memory achieves 31.1% <a href="../results/extraction-result-2821.html#e2821.1" class="evidence-link">[e2821.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Law 2</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; structured memory &#8594; separates &#8594; short-term and long-term components<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; manages transitions between &#8594; memory tiers based on importance or success</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; better performance than single-tier memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; maintains &#8594; both immediate context and persistent knowledge</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Sweet&Sour dual-buffer memory (short-term per-attempt + long-term persistent) achieves: Llama 3.1 8B 32.5 vs ReAct 20.5 (+12.0); Mistral Large 2 44.6 vs 24.8 (+19.8); GPT-4o 54.6 vs 36.0 (+18.6) on ScienceWorld <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>Sweet&Sour ablation removing positive experiences (keeping only failures) drops performance: Llama 3.1 8B to 24.6, Mistral to 31.1, GPT-4o to 44.9, showing dual-buffer with both success/failure is superior <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>SAGE dual-memory (STM for trajectory + LTM for reflections with retention-strength) achieves substantial gains: Mistral-7B on ALFWorld Task completion 73.8% (+17.3% over baseline), Step completion 79.9% (+14.8%) <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
    <li>SAGE memory optimization raises low-performing models dramatically: Qwen-1.8B and CodeLlama-7B show large absolute gains across AgentBench tasks <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
    <li>BUTLER observation queue (k=5 recent unique observations) + recurrent GRU state enables TextWorld success rates: All Tasks train 16%, seen 40%, unseen 37%; zero-shot embodied transfer achieves 19% success (31% goal-condition) on seen, 10% (20%) on unseen <a href="../results/extraction-result-2864.html#e2864.0" class="evidence-link">[e2864.0]</a> </li>
    <li>BUTLER ablations show removing observation queue or recurrent component hurts unseen/generalization performance <a href="../results/extraction-result-2864.html#e2864.0" class="evidence-link">[e2864.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Law 3</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; structured memory &#8594; combines &#8594; semantic relations and episodic details<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; uses &#8594; graph traversal or multi-hop reasoning</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; retrieves &#8594; richer contextual information than flat retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; improves particularly on &#8594; tasks requiring instruction recall or procedural knowledge</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AriGraph hybrid semantic+episodic memory achieves 1.00 on Treasure Hunt (all difficulties), 1.00 on Cooking medium/hard, 0.65 on Cooking Hardest, 0.79 on Cleaning <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>AriGraph ablation w/o episodic memory: Treasure Hunt 1.00→1.00 (maintained), Treasure Hunt Hard 1.00→0.67 (-0.33), Cooking 1.00→0.64 (-0.36), Cooking Hard 1.00→0.45 (-0.55), showing episodic particularly important for complex tasks <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>AriGraph episodic memory particularly important for tasks requiring detailed instruction recall over long horizons (Cooking tasks) <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>ExpeL hybrid episodic trajectories + semantic insights achieves: HotpotQA 39.0%±1.7 vs ReAct 28.0%±1.4 (+11.0 pp); ALFWorld 59.0%±0.3 vs 40.0%±0.3 (+19.0 pp) <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>ExpeL ablations show retrieve-only and insights-only underperform full hybrid: ALFWorld retrieve-only R0=54.5% vs full ExpeL R0=59.0%, demonstrating synergy <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>ExpeL task-similarity retrieval (Faiss embedding kNN) outperforms reasoning-similarity and random sampling substantially <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 4: Law 4</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; structured memory &#8594; is constructed via &#8594; learned extraction from observations<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction &#8594; has &#8594; high error rate or noise</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; memory quality &#8594; degrades &#8594; significantly<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent performance &#8594; may fall below &#8594; unstructured memory baseline</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GATA with discrete belief graphs (GATA-GTP) underperforms continuous learned graphs due to extraction errors from template-based parsing <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> </li>
    <li>NAIL requires validity detector (FastText classifier) to gate KG updates and avoid populating with invalid entries; aggressive examination (26% of actions) is key to reliable KG population <a href="../results/extraction-result-2858.html#e2858.0" class="evidence-link">[e2858.0]</a> </li>
    <li>AriGraph removes outdated semantic edges by LLM comparison to maintain graph quality and prevent accumulation of stale information <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>EMMA struggles on S3 (dynamics disambiguation) with only 3-frame temporal buffer, achieving train 22±3.8% and test 10±0.8%, indicating insufficient memory for complex dynamics <a href="../results/extraction-result-2855.html#e2855.0" class="evidence-link">[e2855.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 5: Law 5</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; structured memory &#8594; includes &#8594; action planning buffer or decomposed subgoals<span style="color: #888888;">, and</span></div>
        <div>&#8226; tasks &#8594; require &#8594; multi-step sequential execution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; better task completion rates<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; reduces &#8594; per-step LLM calls</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SWIFTSAGE with action buffer (SWIFT generates k=5 actions, SAGE validates/selects) achieves 84.7 overall vs SayCan 33.82, with tokens-per-action 19.47 vs 1855.84 <a href="../results/extraction-result-2878.html#e2878.0" class="evidence-link">[e2878.0]</a> </li>
    <li>SWIFTSAGE SWIFT-only (behavior-cloned with sliding-window K=10) achieves 49.22, showing local memory helps but insufficient without LLM planning <a href="../results/extraction-result-2878.html#e2878.1" class="evidence-link">[e2878.1]</a> </li>
    <li>DEPS with prompt-history (descriptions + explanations) achieves ALFWorld 76.0% average success vs GPT zero-shot 10.0%, GPT+RP 52.0%, Inner Monologue 30.0% <a href="../results/extraction-result-2880.html#e2880.0" class="evidence-link">[e2880.0]</a> </li>
    <li>DEPS ablation on re-planning rounds shows consistent improvements: MT1 28.6%→79.8% (+51.2pp), MT3 15.1%→62.4% (+47.3pp), MT4 15.9%→53.3% (+37.4pp) with more rounds <a href="../results/extraction-result-2880.html#e2880.0" class="evidence-link">[e2880.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 6: Law 6</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; structured memory &#8594; stores &#8594; both successful and failed experiences<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; balances &#8594; positive and negative examples</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; learns &#8594; more robust strategies than failure-only memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; improves particularly for &#8594; smaller language models</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Sweet&Sour storing both successes and failures outperforms Reflexion (failures only) across models: Llama 3.1 8B 32.5 vs 21.7 (+10.8); Mistral 44.6 vs 27.6 (+17.0); GPT-4o 54.6 vs 45.3 (+9.3) <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> <a href="../results/extraction-result-2817.html#e2817.0" class="evidence-link">[e2817.0]</a> </li>
    <li>Sweet&Sour ablation sampling only from failures drops to Reflexion-like performance: Llama 24.6, Mistral 31.1, GPT-4o 44.9, demonstrating positive experiences materially improve performance <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>XTX trajectory buffer sampling biases toward higher scores and shorter length, enabling imitation policy to reliably return to promising frontiers; pure imitation (lambda=0) fails, pure exploration (lambda=1) has low average despite high max <a href="../results/extraction-result-2870.html#e2870.0" class="evidence-link">[e2870.0]</a> </li>
    <li>XTX achieves 56.3% average normalized score on 12 Jericho games, +27% relative improvement over best baseline <a href="../results/extraction-result-2870.html#e2870.0" class="evidence-link">[e2870.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Agents using graph memory with learned node embeddings will outperform agents with symbolic-only graph representations on tasks requiring semantic similarity judgments (e.g., finding conceptually related objects).</li>
                <li>Structured memory systems that maintain confidence scores or uncertainty estimates for each stored relation will make more robust decisions than systems without uncertainty tracking, particularly in stochastic environments.</li>
                <li>Agents with dual-buffer memory (short-term + long-term with managed transitions) will outperform single-tier memory on tasks with both immediate reactive needs and long-term strategic planning requirements.</li>
                <li>Skill libraries with hierarchical organization (skills that call other skills) will enable better compositional generalization than flat skill libraries on complex multi-step tasks.</li>
                <li>Structured memory with explicit temporal ordering will outperform unordered memory on tasks requiring understanding of causal sequences or prerequisite relationships.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether learned graph structures can transfer across different game genres (e.g., from adventure games to puzzle games to social deduction games) or if domain-specific structures are necessary for each genre.</li>
                <li>Whether the optimal graph structure (directed vs undirected, typed vs untyped edges, hierarchical vs flat) varies systematically with game characteristics or is largely task-independent.</li>
                <li>Whether agents can learn to automatically select the appropriate memory structure type (graph vs skill library vs dual-buffer vs episodic store) for a new task based on initial exploration, or if this requires meta-learning.</li>
                <li>Whether very large structured memories (>10,000 nodes) maintain their retrieval efficiency advantage or if they suffer from similar scaling issues as unstructured memory, and what the crossover point is.</li>
                <li>Whether hybrid memory combining multiple structure types (graphs for spatial relations, trees for hierarchical goals, lists for temporal sequences, skill libraries for procedures) will outperform single-structure systems on diverse task suites, or if the overhead of managing multiple structures negates the benefits.</li>
                <li>Whether structured memory benefits scale differently with model size - do larger models benefit more or less from structured memory compared to smaller models, and is there a model size where structured memory becomes unnecessary?</li>
                <li>Whether the computational overhead of maintaining structured memory (graph updates, retrieval operations, consistency checking) outweighs its benefits when measured in total compute (not just LLM inference), and how this trade-off varies with task complexity.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Demonstrating that unstructured textual memory with sufficient capacity (e.g., very long context windows) matches structured memory performance on complex multi-hop reasoning tasks would challenge the core superiority claim.</li>
                <li>Finding that randomly constructed graphs perform as well as carefully learned graphs on spatial reasoning tasks would question the importance of accurate structure extraction.</li>
                <li>Showing that structured memory provides no benefit on tasks explicitly designed to require relational reasoning (e.g., graph traversal problems) would challenge the theory's scope and assumptions.</li>
                <li>Demonstrating that the overhead of maintaining structured memory (update time, storage, retrieval complexity) outweighs its benefits in terms of total compute or wall-clock time would challenge practical applicability.</li>
                <li>Finding that very large language models (e.g., GPT-4 scale) with sufficient context eliminate the advantage of structured memory would suggest the benefit is primarily about working around model limitations rather than fundamental superiority.</li>
                <li>Showing that structured memory systems fail to generalize to out-of-distribution tasks while unstructured memory maintains performance would challenge claims about generalization benefits.</li>
                <li>Demonstrating that simple prompt engineering techniques (e.g., chain-of-thought, better formatting) can match structured memory performance would suggest the benefits are not inherent to structure but to information organization.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal granularity of graph nodes (fine-grained vs coarse-grained) varies by task but no principled selection method exists; GATA and AriGraph use different granularities without systematic comparison <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> </li>
    <li>Some structured memory systems show benefits only after substantial training (GATA requires training) while others work zero-shot (NAIL, AriGraph), but the factors determining when training is needed are unclear <a href="../results/extraction-result-2858.html#e2858.0" class="evidence-link">[e2858.0]</a> <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>The computational overhead of graph maintenance and retrieval is not systematically compared across approaches; AriGraph mentions growth statistics but no wall-clock time comparisons <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> <a href="../results/extraction-result-2821.html#e2821.1" class="evidence-link">[e2821.1]</a> </li>
    <li>How to handle contradictory information in structured memory (e.g., stochastic outcomes, conflicting observations) is not well addressed; GATA uses continuous belief graphs but handling of contradictions not detailed <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> </li>
    <li>The interaction between model size and structured memory benefit is not systematically studied; Sweet&Sour shows benefits across model sizes but relative gains vary unpredictably <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>When to use which type of structured memory (graph vs skill library vs dual-buffer vs episodic) for a given task is not characterized; different papers use different structures without comparison <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>The role of retrieval quality vs structure quality is not disentangled; ExpeL shows retrieval strategy matters but unclear if this is independent of structure <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>How structured memory interacts with other agent capabilities (planning, tool use, multi-agent coordination) is not systematically explored <a href="../results/extraction-result-2880.html#e2880.0" class="evidence-link">[e2880.0]</a> <a href="../results/extraction-result-2878.html#e2878.0" class="evidence-link">[e2878.0]</a> </li>
    <li>The sample efficiency of learning to construct structured memory is not compared across approaches; some require many examples, others work with few <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> <a href="../results/extraction-result-2858.html#e2858.0" class="evidence-link">[e2858.0]</a> </li>
    <li>Whether structured memory benefits transfer across tasks or domains is largely unexplored; GITM shows poor cross-domain transfer for fine-tuned memory <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> <a href="../results/extraction-result-2876.html#e2876.0" class="evidence-link">[e2876.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Ammanabrolu & Hausknecht (2020) Graph Constrained Reinforcement Learning for Natural Language Action Spaces [Pioneering work on graph-based memory for text games, graph-constrained RL]</li>
    <li>Ammanabrolu & Riedl (2021) Learning Knowledge Graph-Based World Models of Textual Environments [KG world models for text games]</li>
    <li>Adhikari et al. (2020) Learning Dynamic Belief Graphs to Generalize on Text-Based Games [GATA paper, learned belief graphs]</li>
    <li>Cheng et al. (2024) AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents [Recent hybrid graph memory]</li>
    <li>Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Skill library memory for open-ended tasks]</li>
    <li>Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Episodic reflection memory]</li>
    <li>Zhao et al. (2023) ExpeL: LLM Agents Are Experiential Learners [Hybrid episodic+semantic memory]</li>
    <li>Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models [Scratchpad/working memory for reasoning]</li>
    <li>Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Retrieval-augmented memory with recency/relevance/importance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structured Memory Superiority Theory (Revised)",
    "theory_description": "Structured memory representations (knowledge graphs, belief graphs, skill libraries, dual-buffer systems, episodic stores) systematically outperform unstructured textual memory in text games by enabling more efficient retrieval, better generalization, explicit reasoning about relationships, and effective management of context limitations. The advantage increases with task complexity, partial observability, episode length, and the need for multi-hop reasoning or compositional skill reuse. However, structured memory requires accurate construction and maintenance mechanisms, and the benefit depends critically on alignment between the memory structure type and task demands. The superiority is most pronounced for medium-to-large models on complex tasks; very large models with sufficient context may reduce the gap, and very simple tasks may not benefit from the overhead.",
    "theory_statements": [
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses",
                        "object": "structured knowledge graph memory"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "spatial reasoning or object relationships"
                    },
                    {
                        "subject": "environment",
                        "relation": "has",
                        "object": "partial observability"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "better generalization than text-only memory"
                    },
                    {
                        "subject": "agent",
                        "relation": "requires",
                        "object": "fewer environment interactions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GATA with learned belief graphs achieves +81.6% improvement over text-only baselines on TextWorld cooking tasks",
                        "uuids": [
                            "e2862.0"
                        ]
                    },
                    {
                        "text": "GATA ground-truth full graphs provide +99.0% improvement in some training regimes, demonstrating value of accurate structured state",
                        "uuids": [
                            "e2862.2"
                        ]
                    },
                    {
                        "text": "AriGraph with hybrid semantic+episodic graph achieves 1.00 on Treasure Hunt, 1.00 on Cooking (medium/hard), 0.79 on Cleaning, outperforming full-history, summarization, and RAG baselines",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "AriGraph (Room obs with graph) achieves score 593.00±202.62 in NetHack, comparable to NetPlay with Level obs (memory oracle) at 675.33±130.27, and significantly better than NetPlay Room obs at 341.67±109.14",
                        "uuids": [
                            "e2837.6"
                        ]
                    },
                    {
                        "text": "NAIL with explicit knowledge graph achieves 4.9% normalized completion across 32 unseen IF games without training, demonstrating zero-shot generalization",
                        "uuids": [
                            "e2858.0",
                            "e2851.0"
                        ]
                    },
                    {
                        "text": "NAIL module ablations show adding Navigator (which builds the map) increased score from 0.53% to 1.2%; adding Examiner (which populates KG with entities) raised to 2.6%; Interactor (which uses KG entities) increased to 3.5%",
                        "uuids": [
                            "e2858.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "structured memory",
                        "relation": "contains",
                        "object": "executable skills or programs"
                    },
                    {
                        "subject": "skills",
                        "relation": "are indexed by",
                        "object": "semantic embeddings"
                    },
                    {
                        "subject": "task",
                        "relation": "can be decomposed into",
                        "object": "reusable subtasks"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "compositional generalization to new tasks"
                    },
                    {
                        "subject": "agent",
                        "relation": "reduces",
                        "object": "LLM inference costs substantially"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Voyager skill library enables zero-shot generalization: Diamond Pickaxe 39 iterations (1/3 success), Golden Sword 30 iterations (1/3), Compass 30 iterations (2/3) on unseen tasks",
                        "uuids": [
                            "e2832.0"
                        ]
                    },
                    {
                        "text": "AutoGPT with Voyager skill library solves zero-shot tasks that vanilla AutoGPT fails (0/3) within 50 iterations",
                        "uuids": [
                            "e2832.3"
                        ]
                    },
                    {
                        "text": "GITM with text-based skill memory achieves 67.5% diamond success vs 35.0% without memory (+32.5 pp), and 95.0% iron_pickaxe vs 57.5% without (+37.5 pp)",
                        "uuids": [
                            "e2805.0"
                        ]
                    },
                    {
                        "text": "GITM unlocked all 262 Overworld items (100% coverage) using skill library, with many items at 100% success rates",
                        "uuids": [
                            "e2805.0"
                        ]
                    },
                    {
                        "text": "AGA Lifestyle Policy (cached plan-action graphs with embedding-based retrieval) reduces token cost to 31.1% of baseline while preserving performance in 3-person Generative Agents town",
                        "uuids": [
                            "e2821.1"
                        ]
                    },
                    {
                        "text": "AGA Lifestyle Policy alone reduces token cost to 40.2% of baseline; combined with Social Memory achieves 31.1%",
                        "uuids": [
                            "e2821.1"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "structured memory",
                        "relation": "separates",
                        "object": "short-term and long-term components"
                    },
                    {
                        "subject": "memory system",
                        "relation": "manages transitions between",
                        "object": "memory tiers based on importance or success"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "better performance than single-tier memory"
                    },
                    {
                        "subject": "agent",
                        "relation": "maintains",
                        "object": "both immediate context and persistent knowledge"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Sweet&Sour dual-buffer memory (short-term per-attempt + long-term persistent) achieves: Llama 3.1 8B 32.5 vs ReAct 20.5 (+12.0); Mistral Large 2 44.6 vs 24.8 (+19.8); GPT-4o 54.6 vs 36.0 (+18.6) on ScienceWorld",
                        "uuids": [
                            "e2817.1"
                        ]
                    },
                    {
                        "text": "Sweet&Sour ablation removing positive experiences (keeping only failures) drops performance: Llama 3.1 8B to 24.6, Mistral to 31.1, GPT-4o to 44.9, showing dual-buffer with both success/failure is superior",
                        "uuids": [
                            "e2817.1"
                        ]
                    },
                    {
                        "text": "SAGE dual-memory (STM for trajectory + LTM for reflections with retention-strength) achieves substantial gains: Mistral-7B on ALFWorld Task completion 73.8% (+17.3% over baseline), Step completion 79.9% (+14.8%)",
                        "uuids": [
                            "e2825.0"
                        ]
                    },
                    {
                        "text": "SAGE memory optimization raises low-performing models dramatically: Qwen-1.8B and CodeLlama-7B show large absolute gains across AgentBench tasks",
                        "uuids": [
                            "e2825.0"
                        ]
                    },
                    {
                        "text": "BUTLER observation queue (k=5 recent unique observations) + recurrent GRU state enables TextWorld success rates: All Tasks train 16%, seen 40%, unseen 37%; zero-shot embodied transfer achieves 19% success (31% goal-condition) on seen, 10% (20%) on unseen",
                        "uuids": [
                            "e2864.0"
                        ]
                    },
                    {
                        "text": "BUTLER ablations show removing observation queue or recurrent component hurts unseen/generalization performance",
                        "uuids": [
                            "e2864.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "structured memory",
                        "relation": "combines",
                        "object": "semantic relations and episodic details"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "uses",
                        "object": "graph traversal or multi-hop reasoning"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "retrieves",
                        "object": "richer contextual information than flat retrieval"
                    },
                    {
                        "subject": "performance",
                        "relation": "improves particularly on",
                        "object": "tasks requiring instruction recall or procedural knowledge"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AriGraph hybrid semantic+episodic memory achieves 1.00 on Treasure Hunt (all difficulties), 1.00 on Cooking medium/hard, 0.65 on Cooking Hardest, 0.79 on Cleaning",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "AriGraph ablation w/o episodic memory: Treasure Hunt 1.00→1.00 (maintained), Treasure Hunt Hard 1.00→0.67 (-0.33), Cooking 1.00→0.64 (-0.36), Cooking Hard 1.00→0.45 (-0.55), showing episodic particularly important for complex tasks",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "AriGraph episodic memory particularly important for tasks requiring detailed instruction recall over long horizons (Cooking tasks)",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "ExpeL hybrid episodic trajectories + semantic insights achieves: HotpotQA 39.0%±1.7 vs ReAct 28.0%±1.4 (+11.0 pp); ALFWorld 59.0%±0.3 vs 40.0%±0.3 (+19.0 pp)",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "ExpeL ablations show retrieve-only and insights-only underperform full hybrid: ALFWorld retrieve-only R0=54.5% vs full ExpeL R0=59.0%, demonstrating synergy",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "ExpeL task-similarity retrieval (Faiss embedding kNN) outperforms reasoning-similarity and random sampling substantially",
                        "uuids": [
                            "e2874.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "structured memory",
                        "relation": "is constructed via",
                        "object": "learned extraction from observations"
                    },
                    {
                        "subject": "extraction",
                        "relation": "has",
                        "object": "high error rate or noise"
                    }
                ],
                "then": [
                    {
                        "subject": "memory quality",
                        "relation": "degrades",
                        "object": "significantly"
                    },
                    {
                        "subject": "agent performance",
                        "relation": "may fall below",
                        "object": "unstructured memory baseline"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GATA with discrete belief graphs (GATA-GTP) underperforms continuous learned graphs due to extraction errors from template-based parsing",
                        "uuids": [
                            "e2862.0"
                        ]
                    },
                    {
                        "text": "NAIL requires validity detector (FastText classifier) to gate KG updates and avoid populating with invalid entries; aggressive examination (26% of actions) is key to reliable KG population",
                        "uuids": [
                            "e2858.0"
                        ]
                    },
                    {
                        "text": "AriGraph removes outdated semantic edges by LLM comparison to maintain graph quality and prevent accumulation of stale information",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "EMMA struggles on S3 (dynamics disambiguation) with only 3-frame temporal buffer, achieving train 22±3.8% and test 10±0.8%, indicating insufficient memory for complex dynamics",
                        "uuids": [
                            "e2855.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "structured memory",
                        "relation": "includes",
                        "object": "action planning buffer or decomposed subgoals"
                    },
                    {
                        "subject": "tasks",
                        "relation": "require",
                        "object": "multi-step sequential execution"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "better task completion rates"
                    },
                    {
                        "subject": "agent",
                        "relation": "reduces",
                        "object": "per-step LLM calls"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SWIFTSAGE with action buffer (SWIFT generates k=5 actions, SAGE validates/selects) achieves 84.7 overall vs SayCan 33.82, with tokens-per-action 19.47 vs 1855.84",
                        "uuids": [
                            "e2878.0"
                        ]
                    },
                    {
                        "text": "SWIFTSAGE SWIFT-only (behavior-cloned with sliding-window K=10) achieves 49.22, showing local memory helps but insufficient without LLM planning",
                        "uuids": [
                            "e2878.1"
                        ]
                    },
                    {
                        "text": "DEPS with prompt-history (descriptions + explanations) achieves ALFWorld 76.0% average success vs GPT zero-shot 10.0%, GPT+RP 52.0%, Inner Monologue 30.0%",
                        "uuids": [
                            "e2880.0"
                        ]
                    },
                    {
                        "text": "DEPS ablation on re-planning rounds shows consistent improvements: MT1 28.6%→79.8% (+51.2pp), MT3 15.1%→62.4% (+47.3pp), MT4 15.9%→53.3% (+37.4pp) with more rounds",
                        "uuids": [
                            "e2880.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "structured memory",
                        "relation": "stores",
                        "object": "both successful and failed experiences"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "balances",
                        "object": "positive and negative examples"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "learns",
                        "object": "more robust strategies than failure-only memory"
                    },
                    {
                        "subject": "performance",
                        "relation": "improves particularly for",
                        "object": "smaller language models"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Sweet&Sour storing both successes and failures outperforms Reflexion (failures only) across models: Llama 3.1 8B 32.5 vs 21.7 (+10.8); Mistral 44.6 vs 27.6 (+17.0); GPT-4o 54.6 vs 45.3 (+9.3)",
                        "uuids": [
                            "e2817.1",
                            "e2817.0"
                        ]
                    },
                    {
                        "text": "Sweet&Sour ablation sampling only from failures drops to Reflexion-like performance: Llama 24.6, Mistral 31.1, GPT-4o 44.9, demonstrating positive experiences materially improve performance",
                        "uuids": [
                            "e2817.1"
                        ]
                    },
                    {
                        "text": "XTX trajectory buffer sampling biases toward higher scores and shorter length, enabling imitation policy to reliably return to promising frontiers; pure imitation (lambda=0) fails, pure exploration (lambda=1) has low average despite high max",
                        "uuids": [
                            "e2870.0"
                        ]
                    },
                    {
                        "text": "XTX achieves 56.3% average normalized score on 12 Jericho games, +27% relative improvement over best baseline",
                        "uuids": [
                            "e2870.0"
                        ]
                    }
                ]
            }
        }
    ],
    "new_predictions_likely": [
        "Agents using graph memory with learned node embeddings will outperform agents with symbolic-only graph representations on tasks requiring semantic similarity judgments (e.g., finding conceptually related objects).",
        "Structured memory systems that maintain confidence scores or uncertainty estimates for each stored relation will make more robust decisions than systems without uncertainty tracking, particularly in stochastic environments.",
        "Agents with dual-buffer memory (short-term + long-term with managed transitions) will outperform single-tier memory on tasks with both immediate reactive needs and long-term strategic planning requirements.",
        "Skill libraries with hierarchical organization (skills that call other skills) will enable better compositional generalization than flat skill libraries on complex multi-step tasks.",
        "Structured memory with explicit temporal ordering will outperform unordered memory on tasks requiring understanding of causal sequences or prerequisite relationships."
    ],
    "new_predictions_unknown": [
        "Whether learned graph structures can transfer across different game genres (e.g., from adventure games to puzzle games to social deduction games) or if domain-specific structures are necessary for each genre.",
        "Whether the optimal graph structure (directed vs undirected, typed vs untyped edges, hierarchical vs flat) varies systematically with game characteristics or is largely task-independent.",
        "Whether agents can learn to automatically select the appropriate memory structure type (graph vs skill library vs dual-buffer vs episodic store) for a new task based on initial exploration, or if this requires meta-learning.",
        "Whether very large structured memories (&gt;10,000 nodes) maintain their retrieval efficiency advantage or if they suffer from similar scaling issues as unstructured memory, and what the crossover point is.",
        "Whether hybrid memory combining multiple structure types (graphs for spatial relations, trees for hierarchical goals, lists for temporal sequences, skill libraries for procedures) will outperform single-structure systems on diverse task suites, or if the overhead of managing multiple structures negates the benefits.",
        "Whether structured memory benefits scale differently with model size - do larger models benefit more or less from structured memory compared to smaller models, and is there a model size where structured memory becomes unnecessary?",
        "Whether the computational overhead of maintaining structured memory (graph updates, retrieval operations, consistency checking) outweighs its benefits when measured in total compute (not just LLM inference), and how this trade-off varies with task complexity."
    ],
    "negative_experiments": [
        "Demonstrating that unstructured textual memory with sufficient capacity (e.g., very long context windows) matches structured memory performance on complex multi-hop reasoning tasks would challenge the core superiority claim.",
        "Finding that randomly constructed graphs perform as well as carefully learned graphs on spatial reasoning tasks would question the importance of accurate structure extraction.",
        "Showing that structured memory provides no benefit on tasks explicitly designed to require relational reasoning (e.g., graph traversal problems) would challenge the theory's scope and assumptions.",
        "Demonstrating that the overhead of maintaining structured memory (update time, storage, retrieval complexity) outweighs its benefits in terms of total compute or wall-clock time would challenge practical applicability.",
        "Finding that very large language models (e.g., GPT-4 scale) with sufficient context eliminate the advantage of structured memory would suggest the benefit is primarily about working around model limitations rather than fundamental superiority.",
        "Showing that structured memory systems fail to generalize to out-of-distribution tasks while unstructured memory maintains performance would challenge claims about generalization benefits.",
        "Demonstrating that simple prompt engineering techniques (e.g., chain-of-thought, better formatting) can match structured memory performance would suggest the benefits are not inherent to structure but to information organization."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal granularity of graph nodes (fine-grained vs coarse-grained) varies by task but no principled selection method exists; GATA and AriGraph use different granularities without systematic comparison",
            "uuids": [
                "e2837.0",
                "e2862.0"
            ]
        },
        {
            "text": "Some structured memory systems show benefits only after substantial training (GATA requires training) while others work zero-shot (NAIL, AriGraph), but the factors determining when training is needed are unclear",
            "uuids": [
                "e2858.0",
                "e2862.0",
                "e2837.0"
            ]
        },
        {
            "text": "The computational overhead of graph maintenance and retrieval is not systematically compared across approaches; AriGraph mentions growth statistics but no wall-clock time comparisons",
            "uuids": [
                "e2837.0",
                "e2821.1"
            ]
        },
        {
            "text": "How to handle contradictory information in structured memory (e.g., stochastic outcomes, conflicting observations) is not well addressed; GATA uses continuous belief graphs but handling of contradictions not detailed",
            "uuids": [
                "e2862.0"
            ]
        },
        {
            "text": "The interaction between model size and structured memory benefit is not systematically studied; Sweet&Sour shows benefits across model sizes but relative gains vary unpredictably",
            "uuids": [
                "e2817.1"
            ]
        },
        {
            "text": "When to use which type of structured memory (graph vs skill library vs dual-buffer vs episodic) for a given task is not characterized; different papers use different structures without comparison",
            "uuids": [
                "e2862.0",
                "e2832.0",
                "e2817.1",
                "e2874.0"
            ]
        },
        {
            "text": "The role of retrieval quality vs structure quality is not disentangled; ExpeL shows retrieval strategy matters but unclear if this is independent of structure",
            "uuids": [
                "e2874.0"
            ]
        },
        {
            "text": "How structured memory interacts with other agent capabilities (planning, tool use, multi-agent coordination) is not systematically explored",
            "uuids": [
                "e2880.0",
                "e2878.0"
            ]
        },
        {
            "text": "The sample efficiency of learning to construct structured memory is not compared across approaches; some require many examples, others work with few",
            "uuids": [
                "e2862.0",
                "e2858.0"
            ]
        },
        {
            "text": "Whether structured memory benefits transfer across tasks or domains is largely unexplored; GITM shows poor cross-domain transfer for fine-tuned memory",
            "uuids": [
                "e2805.0",
                "e2876.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Reflexion shows no improvement on WebShop despite using structured reflection memory, suggesting task characteristics matter more than structure alone",
            "uuids": [
                "e2801.0"
            ]
        },
        {
            "text": "Fine-tuned parametric memory (LoRA on LLaMA-1 30B) achieves 60.08% on ALFRED vs ExpeL-style episodic memory, showing weight-based memory can outperform explicit structure in-domain",
            "uuids": [
                "e2876.0"
            ]
        },
        {
            "text": "Simple prompt-based memory (ChatGPT with previous-action reminders) achieves score 15.0 vs 10.0 baseline in Zork, sometimes matching more complex systems on easy tasks",
            "uuids": [
                "e2848.0"
            ]
        },
        {
            "text": "SmartPlay prompted LLM with simple context-window history achieves strong performance on some tasks (Bandits, Rock-Paper-Scissors) without structured memory",
            "uuids": [
                "e2806.0"
            ]
        },
        {
            "text": "GPT-4 with context-window memory achieves 70.0% progress / 47.9% success overall on AGENTBOARD, sometimes outperforming structured approaches",
            "uuids": [
                "e2836.1"
            ]
        },
        {
            "text": "Increasing context window size (GPT-3.5-Turbo-16k) does not reliably improve performance over standard context, suggesting structure matters more than capacity",
            "uuids": [
                "e2836.0"
            ]
        },
        {
            "text": "CALM with short sliding-window (2 timesteps) achieves 9.4% on Jericho, showing minimal memory can work for some games",
            "uuids": [
                "e2850.0"
            ]
        },
        {
            "text": "Character-LLM encodes experiences via fine-tuning (no external memory) and achieves good role-playing, suggesting parametric memory can substitute for explicit structure in some domains",
            "uuids": [
                "e2818.0"
            ]
        }
    ],
    "special_cases": [
        "In fully observable environments with small state spaces, the advantage of structured memory over textual memory may be minimal since all information is always available and easily represented in text (e.g., simple grid worlds).",
        "For tasks with very small state spaces or short episodes (e.g., single-step decisions, simple classification), the overhead of maintaining structured memory may exceed its benefits.",
        "In highly dynamic environments where relationships change frequently and unpredictably, the cost of updating structured memory may be prohibitive and stale structure may hurt more than help.",
        "For one-shot tasks with no need for generalization or transfer, structured memory provides no advantage over episode-specific textual memory.",
        "Very large language models (GPT-4 scale) with sufficient context windows may reduce or eliminate the advantage of structured memory on some tasks by being able to process long unstructured histories effectively.",
        "Tasks requiring primarily creative generation or open-ended exploration (rather than systematic reasoning) may not benefit from structured memory and may even be constrained by it.",
        "When the task structure is unknown or rapidly changing, fixed structured memory representations may be less adaptive than flexible textual memory.",
        "For smaller models with limited capacity, the overhead of managing structured memory may exceed their ability to benefit from it, making simpler memory more effective.",
        "In multi-agent settings, structured memory may need to be shared or coordinated, adding complexity that may negate benefits in some scenarios.",
        "When human interpretability is not required, learned dense representations may outperform explicit structured memory while being more compact."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Ammanabrolu & Hausknecht (2020) Graph Constrained Reinforcement Learning for Natural Language Action Spaces [Pioneering work on graph-based memory for text games, graph-constrained RL]",
            "Ammanabrolu & Riedl (2021) Learning Knowledge Graph-Based World Models of Textual Environments [KG world models for text games]",
            "Adhikari et al. (2020) Learning Dynamic Belief Graphs to Generalize on Text-Based Games [GATA paper, learned belief graphs]",
            "Cheng et al. (2024) AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents [Recent hybrid graph memory]",
            "Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Skill library memory for open-ended tasks]",
            "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Episodic reflection memory]",
            "Zhao et al. (2023) ExpeL: LLM Agents Are Experiential Learners [Hybrid episodic+semantic memory]",
            "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models [Scratchpad/working memory for reasoning]",
            "Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Retrieval-augmented memory with recency/relevance/importance]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
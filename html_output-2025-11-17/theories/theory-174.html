<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bidirectional Learning Progress with Reweighting Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-174</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-174</p>
                <p><strong>Name:</strong> Bidirectional Learning Progress with Reweighting Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments, based on the following results.</p>
                <p><strong>Description:</strong> For multi-task curriculum learning in domains with hierarchical prerequisite dependencies and catastrophic forgetting risks, a curriculum that tracks both increases and decreases in per-task success probability (bidirectional learning progress) combined with reweighting toward low-success tasks and focused sampling is essential for stable, efficient learning. The approach uses fast and slow exponential moving averages (EMAs) to compute learning progress as the absolute difference between reweighted success probabilities, then concentrates sampling weight (~90%) on the top ~20% of tasks by learning progress. This prevents catastrophic forgetting by resampling tasks whose performance declines while maintaining focus on the learning frontier. Unidirectional curricula that only respond to improvements exhibit unstable learning with cycles of forgetting and rediscovery because they fail to detect performance degradation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Bidirectional learning progress (tracking both gains and losses via absolute difference) prevents catastrophic forgetting and provides 3-4% higher task coverage (82 vs 79 items) than unidirectional approaches in hierarchical multi-task domains.</li>
                <li>Unidirectional curricula fail to detect when previously learned skills degrade, leading to cyclic rediscovery patterns where skills are lost and must be relearned.</li>
                <li>Reweighting learning progress toward low-success-probability tasks (p_theta=0.1) is essential for focusing on the learning frontier rather than easy tasks.</li>
                <li>Concentrating sampling weight (~90%) on the top ~20% of tasks by reweighted learning progress efficiently allocates training to currently learnable tasks.</li>
                <li>EMA timescales of approximately 1250 optimization steps for tracking success probabilities provide appropriate balance between responsiveness and noise filtering.</li>
                <li>Dynamic exploration bonuses that remove items from the reward set when success probability exceeds 0.1 provide additive benefits when combined with bidirectional LP curricula.</li>
                <li>The stability advantage of bidirectional curricula increases with the number of tasks and the depth of prerequisite dependencies in the task structure.</li>
                <li>Bidirectional LP with reweighting yields the highest number of discovered items (82/107) and mitigates catastrophic forgetting compared to all baseline approaches.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Bidirectional LP + dynamic exploration bonus discovered 82/107 items (best overall) in Minecraft Simon Says, compared to 79/107 for unidirectional LP + dynamic exploration, ~70 for uniform + dynamic exploration, 43 for uniform + static exploration, and 17 for uniform with no bonus. <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> <a href="../results/extraction-result-1601.html#e1601.2" class="evidence-link">[e1601.2]</a> <a href="../results/extraction-result-1601.html#e1601.1" class="evidence-link">[e1601.1]</a> <a href="../results/extraction-result-1601.html#e1601.0" class="evidence-link">[e1601.0]</a> </li>
    <li>Unidirectional LP exhibits catastrophic forgetting cycles where it discovers 79 items but shows unstable training with cycles of forgetting and rediscovery, while bidirectional LP maintains stable progress. <a href="../results/extraction-result-1601.html#e1601.3" class="evidence-link">[e1601.3]</a> <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
    <li>Learning progress curriculum uses fast and slow EMA success probabilities per task with EMA timescale of approximately 1250 optimization steps, applies reweighting function f(p) with p_theta=0.1 to magnify differences at low success probabilities, computes LP as absolute difference |f(p_fast)-f(p_slow)|, z-scores the LP values, and applies sigmoid centered near 90%-quantile to concentrate ~90% of sampling weight on top ~20% of tasks. <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
    <li>The reweighting toward low success probabilities (p_theta=0.1) is essential for focusing on the learning frontier rather than easy tasks, steering training to currently learnable tasks. <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
    <li>Dynamic exploration bonus (removing already-learned items from exploration reward set when success probability > 0.1) prevents distraction from easy items and substantially increases discovered items from 43 to ~70 under uniform sampling, acting as across-training diversity pressure. <a href="../results/extraction-result-1590.html#e1590.1" class="evidence-link">[e1590.1]</a> <a href="../results/extraction-result-1601.html#e1601.2" class="evidence-link">[e1601.2]</a> </li>
    <li>Static exploration bonus (applied to all items) improves discovery from 17 to 43 items but can distract from hard tasks because easy items remain lucrative to collect. <a href="../results/extraction-result-1590.html#e1590.2" class="evidence-link">[e1590.2]</a> <a href="../results/extraction-result-1601.html#e1601.1" class="evidence-link">[e1601.1]</a> </li>
    <li>The curriculum tracks learning frontier as agent acquires prerequisites enabling deeper tasks in a 107-item tech tree ranging from trivial surface items to complex deep-tech items requiring multi-step mining and crafting. <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
    <li>Bidirectional LP best balances accelerating discovery of new tasks and preventing catastrophic forgetting by sampling tasks whose success probabilities change in either direction. <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
    <li>The method is computationally efficient compared to naive i.i.d. resampling of all tasks. <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In any multi-task domain with >50 tasks and prerequisite structure (e.g., crafting trees, skill dependencies), bidirectional LP will outperform unidirectional by 3-10% in final task coverage.</li>
                <li>The forgetting cycles in unidirectional curricula will be most severe for tasks with long prerequisite chains (>3 dependencies) and will manifest as oscillating success probabilities.</li>
                <li>Combining bidirectional LP with dynamic exploration bonuses will provide additive benefits of 10-20% over either alone in domains with hierarchical task structures.</li>
                <li>The optimal EMA timescale for learning progress tracking will scale with the average time to learn a single task, typically in the range of 500-2000 optimization steps.</li>
                <li>Reweighting parameters (p_theta) between 0.05 and 0.2 will be effective across different domains, with lower values providing stronger focus on novel/hard tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether adaptive EMA timescales that adjust based on task volatility would further improve over fixed timescales.</li>
                <li>Whether more sophisticated forgetting detection using confidence intervals, trend analysis, or variance tracking would significantly improve over simple bidirectional tracking.</li>
                <li>Whether the benefits of bidirectional LP transfer to continual learning settings with non-stationary task distributions or concept drift.</li>
                <li>Whether the 90/20 concentration ratio (90% weight on top 20% tasks) is optimal or whether other ratios would work better in different domains.</li>
                <li>Whether bidirectional LP would be effective in domains without clear prerequisite structure or in purely independent multi-task settings.</li>
                <li>Whether combining bidirectional LP with other anti-forgetting techniques (e.g., elastic weight consolidation, progressive neural networks) would provide further benefits.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where unidirectional LP matches bidirectional performance would challenge the necessity of tracking decreases and suggest the forgetting problem is domain-specific.</li>
                <li>Demonstrating that the forgetting cycles in unidirectional LP are artifacts of specific hyperparameters (e.g., learning rate, network capacity) would weaken the theory's generality.</li>
                <li>Showing that simple periodic resampling of all tasks performs as well as bidirectional LP would suggest the mechanism is less important than the resampling frequency itself.</li>
                <li>Finding that random task sampling with sufficiently large replay buffers matches bidirectional LP performance would challenge the need for learning-progress-based selection.</li>
                <li>Demonstrating that the reweighting function f(p) has minimal impact compared to raw learning progress would question the importance of the low-success prioritization mechanism.</li>
                <li>Showing that very different EMA timescales (e.g., 100 or 10,000 steps) perform equivalently would suggest the timescale parameter is not critical.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Why some tasks are more prone to forgetting than others, and whether this can be predicted from task properties such as complexity, prerequisite depth, or action-space size. <a href="../results/extraction-result-1601.html#e1601.3" class="evidence-link">[e1601.3]</a> </li>
    <li>The exact neural mechanisms by which bidirectional LP prevents forgetting - whether through maintaining skill diversity in the policy, preventing weight drift, or other mechanisms. <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
    <li>Why the specific concentration ratio of 90% weight on top 20% tasks works well, and whether this ratio should be adapted based on the total number of tasks or other factors. <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
    <li>The interaction between bidirectional LP and network architecture choices (e.g., LSTM vs transformer, network capacity) and whether certain architectures are more or less prone to forgetting. <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> </li>
    <li>Whether the benefits of bidirectional LP would hold in text-based or symbolic domains rather than visual domains like Minecraft. <a href="../results/extraction-result-1590.html#e1590.0" class="evidence-link">[e1590.0]</a> <a href="../results/extraction-result-1601.html#e1601.4" class="evidence-link">[e1601.4]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Graves et al. (2017) Automated Curriculum Learning for Neural Networks [Related work on learning progress for curriculum, but uses unidirectional progress and different progress signals]</li>
    <li>Kirkpatrick et al. (2017) Overcoming catastrophic forgetting in neural networks [Related work on catastrophic forgetting, but uses elastic weight consolidation rather than curriculum-based resampling]</li>
    <li>Schaul et al. (2015) Prioritized Experience Replay [Related work on prioritizing samples, but focuses on TD-error rather than learning progress and doesn't address forgetting]</li>
    <li>Bengio et al. (2009) Curriculum learning [Foundational work on curriculum learning, but focuses on static difficulty ordering rather than adaptive learning-progress-based selection]</li>
    <li>Portelas et al. (2020) Teacher algorithms for curriculum learning of Deep RL [Related work on automatic curriculum learning, but focuses on environment generation rather than task selection in fixed task sets]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Bidirectional Learning Progress with Reweighting Theory",
    "theory_description": "For multi-task curriculum learning in domains with hierarchical prerequisite dependencies and catastrophic forgetting risks, a curriculum that tracks both increases and decreases in per-task success probability (bidirectional learning progress) combined with reweighting toward low-success tasks and focused sampling is essential for stable, efficient learning. The approach uses fast and slow exponential moving averages (EMAs) to compute learning progress as the absolute difference between reweighted success probabilities, then concentrates sampling weight (~90%) on the top ~20% of tasks by learning progress. This prevents catastrophic forgetting by resampling tasks whose performance declines while maintaining focus on the learning frontier. Unidirectional curricula that only respond to improvements exhibit unstable learning with cycles of forgetting and rediscovery because they fail to detect performance degradation.",
    "supporting_evidence": [
        {
            "text": "Bidirectional LP + dynamic exploration bonus discovered 82/107 items (best overall) in Minecraft Simon Says, compared to 79/107 for unidirectional LP + dynamic exploration, ~70 for uniform + dynamic exploration, 43 for uniform + static exploration, and 17 for uniform with no bonus.",
            "uuids": [
                "e1601.4",
                "e1590.0",
                "e1601.2",
                "e1601.1",
                "e1601.0"
            ]
        },
        {
            "text": "Unidirectional LP exhibits catastrophic forgetting cycles where it discovers 79 items but shows unstable training with cycles of forgetting and rediscovery, while bidirectional LP maintains stable progress.",
            "uuids": [
                "e1601.3",
                "e1601.4"
            ]
        },
        {
            "text": "Learning progress curriculum uses fast and slow EMA success probabilities per task with EMA timescale of approximately 1250 optimization steps, applies reweighting function f(p) with p_theta=0.1 to magnify differences at low success probabilities, computes LP as absolute difference |f(p_fast)-f(p_slow)|, z-scores the LP values, and applies sigmoid centered near 90%-quantile to concentrate ~90% of sampling weight on top ~20% of tasks.",
            "uuids": [
                "e1590.0",
                "e1601.4"
            ]
        },
        {
            "text": "The reweighting toward low success probabilities (p_theta=0.1) is essential for focusing on the learning frontier rather than easy tasks, steering training to currently learnable tasks.",
            "uuids": [
                "e1590.0",
                "e1601.4"
            ]
        },
        {
            "text": "Dynamic exploration bonus (removing already-learned items from exploration reward set when success probability &gt; 0.1) prevents distraction from easy items and substantially increases discovered items from 43 to ~70 under uniform sampling, acting as across-training diversity pressure.",
            "uuids": [
                "e1590.1",
                "e1601.2"
            ]
        },
        {
            "text": "Static exploration bonus (applied to all items) improves discovery from 17 to 43 items but can distract from hard tasks because easy items remain lucrative to collect.",
            "uuids": [
                "e1590.2",
                "e1601.1"
            ]
        },
        {
            "text": "The curriculum tracks learning frontier as agent acquires prerequisites enabling deeper tasks in a 107-item tech tree ranging from trivial surface items to complex deep-tech items requiring multi-step mining and crafting.",
            "uuids": [
                "e1590.0",
                "e1601.4"
            ]
        },
        {
            "text": "Bidirectional LP best balances accelerating discovery of new tasks and preventing catastrophic forgetting by sampling tasks whose success probabilities change in either direction.",
            "uuids": [
                "e1601.4"
            ]
        },
        {
            "text": "The method is computationally efficient compared to naive i.i.d. resampling of all tasks.",
            "uuids": [
                "e1601.4"
            ]
        }
    ],
    "theory_statements": [
        "Bidirectional learning progress (tracking both gains and losses via absolute difference) prevents catastrophic forgetting and provides 3-4% higher task coverage (82 vs 79 items) than unidirectional approaches in hierarchical multi-task domains.",
        "Unidirectional curricula fail to detect when previously learned skills degrade, leading to cyclic rediscovery patterns where skills are lost and must be relearned.",
        "Reweighting learning progress toward low-success-probability tasks (p_theta=0.1) is essential for focusing on the learning frontier rather than easy tasks.",
        "Concentrating sampling weight (~90%) on the top ~20% of tasks by reweighted learning progress efficiently allocates training to currently learnable tasks.",
        "EMA timescales of approximately 1250 optimization steps for tracking success probabilities provide appropriate balance between responsiveness and noise filtering.",
        "Dynamic exploration bonuses that remove items from the reward set when success probability exceeds 0.1 provide additive benefits when combined with bidirectional LP curricula.",
        "The stability advantage of bidirectional curricula increases with the number of tasks and the depth of prerequisite dependencies in the task structure.",
        "Bidirectional LP with reweighting yields the highest number of discovered items (82/107) and mitigates catastrophic forgetting compared to all baseline approaches."
    ],
    "new_predictions_likely": [
        "In any multi-task domain with &gt;50 tasks and prerequisite structure (e.g., crafting trees, skill dependencies), bidirectional LP will outperform unidirectional by 3-10% in final task coverage.",
        "The forgetting cycles in unidirectional curricula will be most severe for tasks with long prerequisite chains (&gt;3 dependencies) and will manifest as oscillating success probabilities.",
        "Combining bidirectional LP with dynamic exploration bonuses will provide additive benefits of 10-20% over either alone in domains with hierarchical task structures.",
        "The optimal EMA timescale for learning progress tracking will scale with the average time to learn a single task, typically in the range of 500-2000 optimization steps.",
        "Reweighting parameters (p_theta) between 0.05 and 0.2 will be effective across different domains, with lower values providing stronger focus on novel/hard tasks."
    ],
    "new_predictions_unknown": [
        "Whether adaptive EMA timescales that adjust based on task volatility would further improve over fixed timescales.",
        "Whether more sophisticated forgetting detection using confidence intervals, trend analysis, or variance tracking would significantly improve over simple bidirectional tracking.",
        "Whether the benefits of bidirectional LP transfer to continual learning settings with non-stationary task distributions or concept drift.",
        "Whether the 90/20 concentration ratio (90% weight on top 20% tasks) is optimal or whether other ratios would work better in different domains.",
        "Whether bidirectional LP would be effective in domains without clear prerequisite structure or in purely independent multi-task settings.",
        "Whether combining bidirectional LP with other anti-forgetting techniques (e.g., elastic weight consolidation, progressive neural networks) would provide further benefits."
    ],
    "negative_experiments": [
        "Finding domains where unidirectional LP matches bidirectional performance would challenge the necessity of tracking decreases and suggest the forgetting problem is domain-specific.",
        "Demonstrating that the forgetting cycles in unidirectional LP are artifacts of specific hyperparameters (e.g., learning rate, network capacity) would weaken the theory's generality.",
        "Showing that simple periodic resampling of all tasks performs as well as bidirectional LP would suggest the mechanism is less important than the resampling frequency itself.",
        "Finding that random task sampling with sufficiently large replay buffers matches bidirectional LP performance would challenge the need for learning-progress-based selection.",
        "Demonstrating that the reweighting function f(p) has minimal impact compared to raw learning progress would question the importance of the low-success prioritization mechanism.",
        "Showing that very different EMA timescales (e.g., 100 or 10,000 steps) perform equivalently would suggest the timescale parameter is not critical."
    ],
    "unaccounted_for": [
        {
            "text": "Why some tasks are more prone to forgetting than others, and whether this can be predicted from task properties such as complexity, prerequisite depth, or action-space size.",
            "uuids": [
                "e1601.3"
            ]
        },
        {
            "text": "The exact neural mechanisms by which bidirectional LP prevents forgetting - whether through maintaining skill diversity in the policy, preventing weight drift, or other mechanisms.",
            "uuids": [
                "e1601.4"
            ]
        },
        {
            "text": "Why the specific concentration ratio of 90% weight on top 20% tasks works well, and whether this ratio should be adapted based on the total number of tasks or other factors.",
            "uuids": [
                "e1590.0",
                "e1601.4"
            ]
        },
        {
            "text": "The interaction between bidirectional LP and network architecture choices (e.g., LSTM vs transformer, network capacity) and whether certain architectures are more or less prone to forgetting.",
            "uuids": [
                "e1590.0"
            ]
        },
        {
            "text": "Whether the benefits of bidirectional LP would hold in text-based or symbolic domains rather than visual domains like Minecraft.",
            "uuids": [
                "e1590.0",
                "e1601.4"
            ]
        }
    ],
    "conflicting_evidence": [],
    "special_cases": [
        "For tasks with very stable learning (no forgetting), bidirectional and unidirectional LP may perform equivalently, as there would be no performance decreases to detect.",
        "In domains with very few tasks (&lt;10-20), the benefits of bidirectional LP may be minimal as catastrophic forgetting is less likely to occur.",
        "When using very large replay buffers that maintain old experiences indefinitely, the forgetting problem may be partially mitigated even with unidirectional LP, though bidirectional may still provide benefits.",
        "For tasks without prerequisite dependencies (purely independent multi-task learning), the advantage of bidirectional LP may be reduced, though it may still help with general stability.",
        "In domains where all tasks can be learned simultaneously without interference, neither bidirectional nor unidirectional LP may provide significant advantages over uniform sampling.",
        "When the dynamic exploration bonus threshold (0.1 success probability) is set too high or too low, the benefits of the combined approach may be reduced.",
        "For very short training runs where tasks are not yet mastered, the difference between bidirectional and unidirectional may not yet manifest."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Graves et al. (2017) Automated Curriculum Learning for Neural Networks [Related work on learning progress for curriculum, but uses unidirectional progress and different progress signals]",
            "Kirkpatrick et al. (2017) Overcoming catastrophic forgetting in neural networks [Related work on catastrophic forgetting, but uses elastic weight consolidation rather than curriculum-based resampling]",
            "Schaul et al. (2015) Prioritized Experience Replay [Related work on prioritizing samples, but focuses on TD-error rather than learning progress and doesn't address forgetting]",
            "Bengio et al. (2009) Curriculum learning [Foundational work on curriculum learning, but focuses on static difficulty ordering rather than adaptive learning-progress-based selection]",
            "Portelas et al. (2020) Teacher algorithms for curriculum learning of Deep RL [Related work on automatic curriculum learning, but focuses on environment generation rather than task selection in fixed task sets]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
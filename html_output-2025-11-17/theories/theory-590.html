<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structured and Modular Memory Enables Generalization and Robustness in LLM Text Game Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-590</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-590</p>
                <p><strong>Name:</strong> Structured and Modular Memory Enables Generalization and Robustness in LLM Text Game Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks, based on the following results.</p>
                <p><strong>Description:</strong> LLM agents in text games achieve better generalization to new tasks, environments, and long-horizon objectives when their memory is structured and modular, separating semantic, episodic, and procedural components, and when memory is explicitly grounded to entities, actions, and world state. Modular memory enables efficient retrieval, targeted updates, and supports robust planning, backtracking, and transfer learning.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structured Memory Improves Generalization and Robustness (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; uses_memory_architecture &#8594; structured and modular (semantic, episodic, procedural, or entity-grounded)<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; generalization, transfer, or robust long-horizon planning</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; achieves &#8594; higher generalization, transfer, and robustness</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>CoELA's modular memory (semantic, episodic, procedural) enables robust multi-agent planning and generalization; ablation shows removing memory nearly doubles steps to completion. <a href="../results/extraction-result-4864.html#e4864.0" class="evidence-link">[e4864.0]</a> </li>
    <li>KG-DQN and Ammanabrolu&Riedl-Graph-RL use structured knowledge graphs as memory, enabling transfer and faster learning across games; seeding and QA pretraining further improve generalization. <a href="../results/extraction-result-4886.html#e4886.0" class="evidence-link">[e4886.0]</a> <a href="../results/extraction-result-4886.html#e4886.1" class="evidence-link">[e4886.1]</a> <a href="../results/extraction-result-4886.html#e4886.2" class="evidence-link">[e4886.2]</a> <a href="../results/extraction-result-4673.html#e4673.2" class="evidence-link">[e4673.2]</a> <a href="../results/extraction-result-4923.html#e4923.4" class="evidence-link">[e4923.4]</a> </li>
    <li>EMMA's entity-conditioned attention and per-entity memory enables grounding and generalization in RL gridworlds; ablation shows that LSTM memory alone is insufficient without explicit entity grounding. <a href="../results/extraction-result-4921.html#e4921.0" class="evidence-link">[e4921.0]</a> <a href="../results/extraction-result-4921.html#e4921.2" class="evidence-link">[e4921.2]</a> </li>
    <li>NAIL's structured map/object memory and knowledge graph enables robust exploration and planning in unseen interactive fiction games. <a href="../results/extraction-result-4673.html#e4673.0" class="evidence-link">[e4673.0]</a> <a href="../results/extraction-result-4923.html#e4923.1" class="evidence-link">[e4923.1]</a> </li>
    <li>ProAgent's modular memory (Knowledge Library + Trajectory) and belief revision enable robust cooperative planning and adaptation to new teammates in Overcooked-AI. <a href="../results/extraction-result-4802.html#e4802.0" class="evidence-link">[e4802.0]</a> </li>
    <li>PAL and Voyager use procedural/program memory (stored skills/subroutines) to enable skill reuse and robust open-ended exploration. <a href="../results/extraction-result-4919.html#e4919.0" class="evidence-link">[e4919.0]</a> <a href="../results/extraction-result-4919.html#e4919.3" class="evidence-link">[e4919.3]</a> <a href="../results/extraction-result-4914.html#e4914.6" class="evidence-link">[e4914.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While modular memory is referenced in cognitive architectures, this law synthesizes and formalizes it as a predictive, testable principle for LLM text-game agents, supported by direct ablation and transfer evidence.</p>            <p><strong>What Already Exists:</strong> Structured memory and modular cognitive architectures are established in cognitive science and some agent frameworks, but not formalized as a necessary principle for LLM text-game agents.</p>            <p><strong>What is Novel:</strong> This law formalizes the necessity of modular, structured, and entity-grounded memory for generalization and robustness in LLM text-game agents, supported by ablation and transfer evidence.</p>
            <p><strong>References:</strong> <ul>
    <li>Ammanabrolu & Riedl (2018) Playing text-adventure games with graph-based deep reinforcement learning [structured memory for RL]</li>
    <li>Hill et al. (2020) Grounded language learning fast and slow [modular memory in cognitive architectures]</li>
    <li>Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [modular memory in agent framework]</li>
</ul>
            <h3>Statement 1: Entity-Grounded and Procedural Memory Enable Skill Transfer and Efficient Planning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; uses_memory &#8594; entity-grounded or procedural (skill/program) memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; skill transfer, compositionality, or efficient planning</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; achieves &#8594; faster learning, skill reuse, and compositional generalization</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Voyager's skill library (procedural memory) enables rapid zero-shot generalization and open-ended exploration; ablation removing skill library causes plateauing and loss of long-horizon progress. <a href="../results/extraction-result-4919.html#e4919.0" class="evidence-link">[e4919.0]</a> <a href="../results/extraction-result-4919.html#e4919.3" class="evidence-link">[e4919.3]</a> </li>
    <li>PAL and program-aided approaches enable sub-task management and skill reuse in long-horizon web tasks. <a href="../results/extraction-result-4914.html#e4914.6" class="evidence-link">[e4914.6]</a> </li>
    <li>EMMA's entity-conditioned attention and per-entity memory enable grounding of movement dynamics and compositional generalization. <a href="../results/extraction-result-4921.html#e4921.0" class="evidence-link">[e4921.0]</a> </li>
    <li>KG-DQN and Ammanabrolu+Riedl-transfer use knowledge graphs to transfer commonsense and procedural knowledge between games, improving sample efficiency and generalization. <a href="../results/extraction-result-4886.html#e4886.0" class="evidence-link">[e4886.0]</a> <a href="../results/extraction-result-4886.html#e4886.1" class="evidence-link">[e4886.1]</a> <a href="../results/extraction-result-4673.html#e4673.2" class="evidence-link">[e4673.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While entity-grounded and procedural memory are referenced in cognitive architectures, this law synthesizes and formalizes their necessity for LLM text-game agents, supported by ablation and transfer evidence.</p>            <p><strong>What Already Exists:</strong> Entity-grounded and procedural memory are discussed in cognitive architectures and some RL agents, but not formalized as a necessary principle for LLM text-game agents.</p>            <p><strong>What is Novel:</strong> This law formalizes the necessity of entity-grounded and procedural memory for skill transfer and compositional generalization in LLM text-game agents.</p>
            <p><strong>References:</strong> <ul>
    <li>Ammanabrolu & Riedl (2018) Playing text-adventure games with graph-based deep reinforcement learning [structured memory for RL]</li>
    <li>Hill et al. (2020) Grounded language learning fast and slow [entity-grounded memory in cognitive architectures]</li>
    <li>Gao et al. (2023) PAL: Program-aided language models [procedural memory for skill reuse]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with modular, structured memory (semantic, episodic, procedural) will generalize faster to new games or tasks than agents with unstructured or monolithic memory.</li>
                <li>Entity-grounded memory will enable agents to disambiguate references and actions in procedurally generated or compositional environments.</li>
                <li>Procedural memory (skill libraries) will allow agents to rapidly solve new tasks by reusing and composing previously learned skills.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>In highly dynamic, multi-agent environments, modular memory will enable agents to track and adapt to other agents' behaviors and strategies over long time horizons.</li>
                <li>Structured memory will enable agents to develop emergent planning and meta-reasoning capabilities (e.g., learning to plan plans) in open-ended text games.</li>
                <li>Entity-grounded memory will allow agents to perform robust zero-shot transfer to environments with novel entities or relations.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with unstructured or monolithic memory generalize as well as or better than modular-memory agents to new games or tasks, the theory would be challenged.</li>
                <li>If entity-grounded or procedural memory does not improve skill transfer or compositional generalization, the theory would be undermined.</li>
                <li>If modular memory architectures do not improve robustness to environment changes or adversarial conditions, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some agents (e.g., ReAct, SayCan, and LLM-only baselines) achieve strong performance on short or medium-horizon tasks without explicit structured or modular memory. <a href="../results/extraction-result-4850.html#e4850.0" class="evidence-link">[e4850.0]</a> <a href="../results/extraction-result-4898.html#e4898.2" class="evidence-link">[e4898.2]</a> <a href="../results/extraction-result-4651.html#e4651.1" class="evidence-link">[e4651.1]</a> <a href="../results/extraction-result-4895.html#e4895.1" class="evidence-link">[e4895.1]</a> <a href="../results/extraction-result-4917.html#e4917.1" class="evidence-link">[e4917.1]</a> <a href="../results/extraction-result-4918.html#e4918.0" class="evidence-link">[e4918.0]</a> <a href="../results/extraction-result-4918.html#e4918.1" class="evidence-link">[e4918.1]</a> <a href="../results/extraction-result-4882.html#e4882.1" class="evidence-link">[e4882.1]</a> <a href="../results/extraction-result-4879.html#e4879.0" class="evidence-link">[e4879.0]</a> <a href="../results/extraction-result-4884.html#e4884.1" class="evidence-link">[e4884.1]</a> <a href="../results/extraction-result-4899.html#e4899.0" class="evidence-link">[e4899.0]</a> <a href="../results/extraction-result-4899.html#e4899.1" class="evidence-link">[e4899.1]</a> <a href="../results/extraction-result-4678.html#e4678.2" class="evidence-link">[e4678.2]</a> <a href="../results/extraction-result-4678.html#e4678.0" class="evidence-link">[e4678.0]</a> <a href="../results/extraction-result-4678.html#e4678.1" class="evidence-link">[e4678.1]</a> <a href="../results/extraction-result-4673.html#e4673.3" class="evidence-link">[e4673.3]</a> <a href="../results/extraction-result-4673.html#e4673.5" class="evidence-link">[e4673.5]</a> <a href="../results/extraction-result-4681.html#e4681.0" class="evidence-link">[e4681.0]</a> <a href="../results/extraction-result-4681.html#e4681.3" class="evidence-link">[e4681.3]</a> <a href="../results/extraction-result-4680.html#e4680.2" class="evidence-link">[e4680.2]</a> <a href="../results/extraction-result-4680.html#e4680.1" class="evidence-link">[e4680.1]</a> <a href="../results/extraction-result-4914.html#e4914.1" class="evidence-link">[e4914.1]</a> <a href="../results/extraction-result-4917.html#e4917.2" class="evidence-link">[e4917.2]</a> <a href="../results/extraction-result-4917.html#e4917.3" class="evidence-link">[e4917.3]</a> <a href="../results/extraction-result-4914.html#e4914.6" class="evidence-link">[e4914.6]</a> <a href="../results/extraction-result-4914.html#e4914.3" class="evidence-link">[e4914.3]</a> <a href="../results/extraction-result-4914.html#e4914.7" class="evidence-link">[e4914.7]</a> <a href="../results/extraction-result-4915.html#e4915.2" class="evidence-link">[e4915.2]</a> <a href="../results/extraction-result-4915.html#e4915.1" class="evidence-link">[e4915.1]</a> <a href="../results/extraction-result-4919.html#e4919.2" class="evidence-link">[e4919.2]</a> <a href="../results/extraction-result-4919.html#e4919.3" class="evidence-link">[e4919.3]</a> <a href="../results/extraction-result-4919.html#e4919.4" class="evidence-link">[e4919.4]</a> <a href="../results/extraction-result-4920.html#e4920.5" class="evidence-link">[e4920.5]</a> <a href="../results/extraction-result-4921.html#e4921.1" class="evidence-link">[e4921.1]</a> <a href="../results/extraction-result-4921.html#e4921.2" class="evidence-link">[e4921.2]</a> <a href="../results/extraction-result-4921.html#e4921.3" class="evidence-link">[e4921.3]</a> <a href="../results/extraction-result-4922.html#e4922.0" class="evidence-link">[e4922.0]</a> <a href="../results/extraction-result-4922.html#e4922.1" class="evidence-link">[e4922.1]</a> <a href="../results/extraction-result-4922.html#e4922.2" class="evidence-link">[e4922.2]</a> <a href="../results/extraction-result-4922.html#e4922.3" class="evidence-link">[e4922.3]</a> <a href="../results/extraction-result-4923.html#e4923.1" class="evidence-link">[e4923.1]</a> <a href="../results/extraction-result-4923.html#e4923.2" class="evidence-link">[e4923.2]</a> <a href="../results/extraction-result-4923.html#e4923.3" class="evidence-link">[e4923.3]</a> <a href="../results/extraction-result-4923.html#e4923.4" class="evidence-link">[e4923.4]</a> <a href="../results/extraction-result-4923.html#e4923.5" class="evidence-link">[e4923.5]</a> </li>
    <li>In some ablations (e.g., Swift agent on ScienceWorld), including action history or structured memory can harm performance, indicating that memory design and integration must be carefully tuned. <a href="../results/extraction-result-4652.html#e4652.1" class="evidence-link">[e4652.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While modular memory is referenced in cognitive architectures, this theory synthesizes and formalizes its necessity for LLM text-game agents, supported by direct ablation and transfer evidence.</p>
            <p><strong>References:</strong> <ul>
    <li>Ammanabrolu & Riedl (2018) Playing text-adventure games with graph-based deep reinforcement learning [structured memory for RL]</li>
    <li>Hill et al. (2020) Grounded language learning fast and slow [modular memory in cognitive architectures]</li>
    <li>Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [modular memory in agent framework]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structured and Modular Memory Enables Generalization and Robustness in LLM Text Game Agents",
    "theory_description": "LLM agents in text games achieve better generalization to new tasks, environments, and long-horizon objectives when their memory is structured and modular, separating semantic, episodic, and procedural components, and when memory is explicitly grounded to entities, actions, and world state. Modular memory enables efficient retrieval, targeted updates, and supports robust planning, backtracking, and transfer learning.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structured Memory Improves Generalization and Robustness",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "uses_memory_architecture",
                        "object": "structured and modular (semantic, episodic, procedural, or entity-grounded)"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "generalization, transfer, or robust long-horizon planning"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "achieves",
                        "object": "higher generalization, transfer, and robustness"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "CoELA's modular memory (semantic, episodic, procedural) enables robust multi-agent planning and generalization; ablation shows removing memory nearly doubles steps to completion.",
                        "uuids": [
                            "e4864.0"
                        ]
                    },
                    {
                        "text": "KG-DQN and Ammanabrolu&Riedl-Graph-RL use structured knowledge graphs as memory, enabling transfer and faster learning across games; seeding and QA pretraining further improve generalization.",
                        "uuids": [
                            "e4886.0",
                            "e4886.1",
                            "e4886.2",
                            "e4673.2",
                            "e4923.4"
                        ]
                    },
                    {
                        "text": "EMMA's entity-conditioned attention and per-entity memory enables grounding and generalization in RL gridworlds; ablation shows that LSTM memory alone is insufficient without explicit entity grounding.",
                        "uuids": [
                            "e4921.0",
                            "e4921.2"
                        ]
                    },
                    {
                        "text": "NAIL's structured map/object memory and knowledge graph enables robust exploration and planning in unseen interactive fiction games.",
                        "uuids": [
                            "e4673.0",
                            "e4923.1"
                        ]
                    },
                    {
                        "text": "ProAgent's modular memory (Knowledge Library + Trajectory) and belief revision enable robust cooperative planning and adaptation to new teammates in Overcooked-AI.",
                        "uuids": [
                            "e4802.0"
                        ]
                    },
                    {
                        "text": "PAL and Voyager use procedural/program memory (stored skills/subroutines) to enable skill reuse and robust open-ended exploration.",
                        "uuids": [
                            "e4919.0",
                            "e4919.3",
                            "e4914.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Structured memory and modular cognitive architectures are established in cognitive science and some agent frameworks, but not formalized as a necessary principle for LLM text-game agents.",
                    "what_is_novel": "This law formalizes the necessity of modular, structured, and entity-grounded memory for generalization and robustness in LLM text-game agents, supported by ablation and transfer evidence.",
                    "classification_explanation": "While modular memory is referenced in cognitive architectures, this law synthesizes and formalizes it as a predictive, testable principle for LLM text-game agents, supported by direct ablation and transfer evidence.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Ammanabrolu & Riedl (2018) Playing text-adventure games with graph-based deep reinforcement learning [structured memory for RL]",
                        "Hill et al. (2020) Grounded language learning fast and slow [modular memory in cognitive architectures]",
                        "Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [modular memory in agent framework]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Entity-Grounded and Procedural Memory Enable Skill Transfer and Efficient Planning",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "uses_memory",
                        "object": "entity-grounded or procedural (skill/program) memory"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "skill transfer, compositionality, or efficient planning"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "achieves",
                        "object": "faster learning, skill reuse, and compositional generalization"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Voyager's skill library (procedural memory) enables rapid zero-shot generalization and open-ended exploration; ablation removing skill library causes plateauing and loss of long-horizon progress.",
                        "uuids": [
                            "e4919.0",
                            "e4919.3"
                        ]
                    },
                    {
                        "text": "PAL and program-aided approaches enable sub-task management and skill reuse in long-horizon web tasks.",
                        "uuids": [
                            "e4914.6"
                        ]
                    },
                    {
                        "text": "EMMA's entity-conditioned attention and per-entity memory enable grounding of movement dynamics and compositional generalization.",
                        "uuids": [
                            "e4921.0"
                        ]
                    },
                    {
                        "text": "KG-DQN and Ammanabrolu+Riedl-transfer use knowledge graphs to transfer commonsense and procedural knowledge between games, improving sample efficiency and generalization.",
                        "uuids": [
                            "e4886.0",
                            "e4886.1",
                            "e4673.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Entity-grounded and procedural memory are discussed in cognitive architectures and some RL agents, but not formalized as a necessary principle for LLM text-game agents.",
                    "what_is_novel": "This law formalizes the necessity of entity-grounded and procedural memory for skill transfer and compositional generalization in LLM text-game agents.",
                    "classification_explanation": "While entity-grounded and procedural memory are referenced in cognitive architectures, this law synthesizes and formalizes their necessity for LLM text-game agents, supported by ablation and transfer evidence.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Ammanabrolu & Riedl (2018) Playing text-adventure games with graph-based deep reinforcement learning [structured memory for RL]",
                        "Hill et al. (2020) Grounded language learning fast and slow [entity-grounded memory in cognitive architectures]",
                        "Gao et al. (2023) PAL: Program-aided language models [procedural memory for skill reuse]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with modular, structured memory (semantic, episodic, procedural) will generalize faster to new games or tasks than agents with unstructured or monolithic memory.",
        "Entity-grounded memory will enable agents to disambiguate references and actions in procedurally generated or compositional environments.",
        "Procedural memory (skill libraries) will allow agents to rapidly solve new tasks by reusing and composing previously learned skills."
    ],
    "new_predictions_unknown": [
        "In highly dynamic, multi-agent environments, modular memory will enable agents to track and adapt to other agents' behaviors and strategies over long time horizons.",
        "Structured memory will enable agents to develop emergent planning and meta-reasoning capabilities (e.g., learning to plan plans) in open-ended text games.",
        "Entity-grounded memory will allow agents to perform robust zero-shot transfer to environments with novel entities or relations."
    ],
    "negative_experiments": [
        "If agents with unstructured or monolithic memory generalize as well as or better than modular-memory agents to new games or tasks, the theory would be challenged.",
        "If entity-grounded or procedural memory does not improve skill transfer or compositional generalization, the theory would be undermined.",
        "If modular memory architectures do not improve robustness to environment changes or adversarial conditions, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some agents (e.g., ReAct, SayCan, and LLM-only baselines) achieve strong performance on short or medium-horizon tasks without explicit structured or modular memory.",
            "uuids": [
                "e4850.0",
                "e4898.2",
                "e4651.1",
                "e4895.1",
                "e4917.1",
                "e4918.0",
                "e4918.1",
                "e4882.1",
                "e4879.0",
                "e4884.1",
                "e4899.0",
                "e4899.1",
                "e4678.2",
                "e4678.0",
                "e4678.1",
                "e4673.3",
                "e4673.5",
                "e4681.0",
                "e4681.3",
                "e4680.2",
                "e4680.1",
                "e4914.1",
                "e4917.2",
                "e4917.3",
                "e4914.6",
                "e4914.3",
                "e4914.7",
                "e4915.2",
                "e4915.1",
                "e4919.2",
                "e4919.3",
                "e4919.4",
                "e4920.5",
                "e4921.1",
                "e4921.2",
                "e4921.3",
                "e4922.0",
                "e4922.1",
                "e4922.2",
                "e4922.3",
                "e4923.1",
                "e4923.2",
                "e4923.3",
                "e4923.4",
                "e4923.5"
            ]
        },
        {
            "text": "In some ablations (e.g., Swift agent on ScienceWorld), including action history or structured memory can harm performance, indicating that memory design and integration must be carefully tuned.",
            "uuids": [
                "e4652.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "CALM-GPT2 and n-gram models, which use only short context and no structured memory, outperform more complex memory-augmented models on some tasks, indicating that task structure and memory design interact in complex ways.",
            "uuids": [
                "e4899.0",
                "e4899.1"
            ]
        },
        {
            "text": "WebShop IL agent's naive history concatenation degraded performance, suggesting that not all forms of structured memory integration are beneficial.",
            "uuids": [
                "e4875.0"
            ]
        }
    ],
    "special_cases": [
        "For very short-horizon or fully observable tasks, structured or modular memory may not provide measurable benefits.",
        "If the memory modules are poorly designed or not aligned with the task structure, they can introduce noise or degrade performance.",
        "Tasks with highly dynamic or adversarial environments may require additional mechanisms (e.g., belief revision, memory invalidation) beyond standard modular memory."
    ],
    "existing_theory": {
        "what_already_exists": "Structured and modular memory is established in cognitive architectures and some RL agents, but not formalized as a necessary principle for LLM text-game agents.",
        "what_is_novel": "This theory formalizes the necessity of modular, structured, and entity-grounded memory for generalization and robustness in LLM text-game agents, supported by ablation and transfer evidence.",
        "classification_explanation": "While modular memory is referenced in cognitive architectures, this theory synthesizes and formalizes its necessity for LLM text-game agents, supported by direct ablation and transfer evidence.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Ammanabrolu & Riedl (2018) Playing text-adventure games with graph-based deep reinforcement learning [structured memory for RL]",
            "Hill et al. (2020) Grounded language learning fast and slow [modular memory in cognitive architectures]",
            "Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [modular memory in agent framework]"
        ]
    },
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
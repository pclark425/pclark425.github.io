<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Pathway Arithmetic Processing Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-210</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-210</p>
                <p><strong>Name:</strong> Dual-Pathway Arithmetic Processing Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> Language models process arithmetic through two distinct computational pathways that operate in parallel and compete for output generation. The Pattern-Matching Pathway (PMP) rapidly retrieves answers to arithmetic problems that closely match training distribution patterns, leveraging memorized associations and statistical regularities encoded primarily in early-to-middle transformer layers. The Algorithmic Simulation Pathway (ASP) sequentially constructs solutions by simulating step-by-step computational procedures through the model's recurrent processing of intermediate tokens, with later layers composing sequential operations. The final output is determined by whichever pathway produces stronger activation signals, with PMP dominating for familiar problems (high training frequency, small numbers, standard formats) and ASP engaging for novel or complex problems requiring procedural decomposition. The balance between pathways depends on problem characteristics (numerical magnitude, complexity, format), training data distribution, and architectural factors (depth, attention patterns, model scale).</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models maintain two parallel processing pathways for arithmetic: a fast Pattern-Matching Pathway (PMP) and a slower Algorithmic Simulation Pathway (ASP).</li>
                <li>The PMP operates through direct association retrieval, matching input patterns to memorized arithmetic facts and common number combinations from training data, with primary computational support from early-to-middle transformer layers that encode numerical patterns.</li>
                <li>The ASP operates through sequential token generation that simulates step-by-step computational procedures, building intermediate representations, with primary computational support from middle-to-late transformer layers that compose sequential operations.</li>
                <li>For problems with high training frequency (small numbers, common operations, standard formats), PMP dominates and produces rapid, accurate responses through direct pattern retrieval.</li>
                <li>For problems with low training frequency or high complexity (large numbers, uncommon operations, non-standard formats), ASP engagement increases, requiring more computational steps and intermediate tokens.</li>
                <li>The probability of correct output follows a weighted combination: P(correct) = α·P_PMP(correct) + (1-α)·P_ASP(correct), where α ∈ [0,1] represents pathway dominance determined by pattern familiarity (training frequency, format match, number magnitude).</li>
                <li>Chain-of-thought prompting increases ASP engagement by forcing explicit generation of intermediate computational steps, effectively reducing α and increasing reliance on procedural processing.</li>
                <li>Error patterns differ systematically between pathways: PMP produces substitution errors (retrieving wrong but similar memorized facts), while ASP produces procedural errors (carry mistakes, digit transpositions, step-skipping).</li>
                <li>Model scale increases both pathway capabilities, but disproportionately benefits PMP through increased memorization capacity and pattern storage, while ASP benefits from improved sequential processing depth.</li>
                <li>Attention mechanisms in early layers support PMP through pattern recognition and numerical encoding, while later layers support ASP through sequential operation composition and intermediate state maintenance.</li>
                <li>Format sensitivity (performance degradation with digit reversal, separators, or unusual notation) primarily affects PMP by disrupting pattern matching, while ASP shows greater format robustness when explicitly engaged through chain-of-thought prompting.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Language models show dramatically better performance on arithmetic problems involving small numbers (single digits) compared to large numbers, suggesting memorization of common arithmetic facts from training data. </li>
    <li>Performance on arithmetic correlates strongly with the frequency of specific number combinations in training data, indicating pattern-matching from memorized examples. </li>
    <li>Chain-of-thought prompting significantly improves arithmetic performance, suggesting models can engage procedural, step-by-step processing when prompted to do so. </li>
    <li>Models show systematic errors consistent with algorithmic mistakes (e.g., carry errors in addition) rather than random errors, suggesting simulation of computational procedures. </li>
    <li>Scratchpad and intermediate computation approaches improve arithmetic accuracy, indicating models benefit from explicit externalization of procedural steps. </li>
    <li>Different layers in transformer models show distinct activation patterns for arithmetic tasks, with early layers encoding numerical magnitude and later layers performing operations, suggesting hierarchical processing stages. </li>
    <li>Models can solve arithmetic problems they've never seen in training when given appropriate prompting, suggesting capability beyond pure memorization. </li>
    <li>Format changes (e.g., reversing digit order, adding separators) significantly impact arithmetic performance, indicating sensitivity to surface-level pattern matching rather than robust algorithmic understanding. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Intervening on middle-layer activations during arithmetic tasks will differentially affect PMP vs ASP: early-layer interventions will disrupt pattern matching more (larger performance drop on high-frequency problems), while late-layer interventions will disrupt algorithmic simulation more (larger performance drop on low-frequency problems requiring chain-of-thought).</li>
                <li>Training models with arithmetic problems presented in multiple formats (standard, reversed, with separators) will improve ASP robustness and reduce format sensitivity while maintaining PMP performance on standard formats for high-frequency problems.</li>
                <li>Models will show faster response times (fewer tokens to correct answer, lower perplexity on answer tokens) for high-frequency arithmetic problems compared to low-frequency problems of equal computational complexity, reflecting PMP's direct retrieval advantage.</li>
                <li>Fine-tuning on chain-of-thought arithmetic data will increase ASP pathway strength, improving performance on novel large-number problems while maintaining or slightly improving performance on memorized small-number problems through enhanced procedural capabilities.</li>
                <li>Arithmetic problems presented with irrelevant context or distractors will more strongly impair PMP (pattern matching disrupted by noise) than ASP when chain-of-thought is used, as ASP can filter irrelevant information through explicit step-by-step processing.</li>
                <li>Measuring layer-wise activation patterns will show that high-frequency arithmetic problems produce stronger early-layer activations and weaker late-layer activations compared to low-frequency problems, reflecting differential pathway engagement.</li>
                <li>Models will show higher confidence (lower entropy in output distribution) for high-frequency arithmetic problems compared to low-frequency problems, reflecting PMP's stronger signal for familiar patterns.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Explicitly training models to identify which pathway to use (meta-learning to select PMP vs ASP based on problem characteristics) could dramatically improve arithmetic performance by optimal pathway selection, but it's unclear if models can learn this meta-strategy effectively or if it would interfere with natural pathway competition.</li>
                <li>Architecturally separating the two pathways (dedicated PMP and ASP modules with explicit routing) might improve arithmetic performance through specialized processing, but could also reduce flexibility and generalization if the pathways naturally share representations or benefit from implicit competition.</li>
                <li>The theory predicts that adversarial examples could be constructed to maximally activate PMP while requiring ASP (e.g., problems that look like memorized patterns but have subtle changes requiring computation), potentially causing systematic failures - the severity and exploitability of such failures is unknown.</li>
                <li>If models are trained exclusively on chain-of-thought arithmetic (pure ASP training), they might lose PMP capabilities entirely, or PMP might emerge anyway through compression of repeated procedural patterns into direct associations - the outcome and implications for efficiency are uncertain.</li>
                <li>The theory suggests that very large models might develop a third 'hybrid' pathway that combines pattern matching with procedural knowledge (e.g., memorizing common algorithmic subroutines), or that pathway boundaries might blur - whether this emerges, improves performance, and represents a qualitatively different mechanism is unknown.</li>
                <li>Training with curriculum learning (gradually increasing number size and complexity) might strengthen the transition mechanisms between pathways, potentially improving overall arithmetic performance, but the optimal curriculum structure and whether explicit transition training helps is unclear.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If ablating early transformer layers does not differentially impair performance on high-frequency arithmetic problems (which should rely on PMP) more than low-frequency problems, this would challenge the theory's claim about distinct pathway localization and early-layer pattern matching.</li>
                <li>If chain-of-thought prompting does not improve performance on low-frequency arithmetic problems more than high-frequency problems (or improves both equally), this would challenge the theory's claim about ASP engagement for unfamiliar problems and differential pathway activation.</li>
                <li>If models show identical error patterns (same types and distributions of mistakes) across high-frequency and low-frequency arithmetic problems, this would challenge the theory's prediction of distinct error signatures for PMP (substitution errors) vs ASP (procedural errors).</li>
                <li>If training data frequency has no correlation with arithmetic performance when controlling for problem complexity and number magnitude, this would undermine the PMP component of the theory and the role of memorization.</li>
                <li>If format changes (digit reversal, separators) equally impair performance on both memorized (high-frequency) and novel (low-frequency) arithmetic problems, this would challenge the theory's distinction between surface-level pattern matching (PMP) and deeper algorithmic processing (ASP).</li>
                <li>If intermediate computation steps (scratchpad) do not improve performance on complex arithmetic more than simple arithmetic, this would challenge the theory's claim about ASP requiring explicit intermediate representations for complex problems.</li>
                <li>If layer-wise activation patterns do not differ between high-frequency and low-frequency arithmetic problems (similar activation distributions across layers), this would challenge the theory's claim about differential layer involvement in the two pathways.</li>
                <li>If model scaling improves performance equally on high-frequency and low-frequency problems (no differential scaling effects), this would challenge the theory's prediction that scale disproportionately benefits PMP through increased memorization capacity.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact mechanism by which transformer attention patterns implement either pattern matching or algorithmic simulation remains unclear at the circuit level, including how specific attention heads contribute to each pathway. </li>
    <li>Some models show unexpected capabilities on arithmetic tasks that seem too complex for either pure memorization or simple algorithmic simulation (e.g., solving novel multi-step problems without explicit chain-of-thought), suggesting possible additional mechanisms or more sophisticated pathway interactions. </li>
    <li>The theory does not fully explain why some models show sudden capability jumps (emergent abilities) at specific scales for arithmetic tasks, rather than smooth improvement, if both pathways should improve gradually with scale. </li>
    <li>The theory does not specify how the pathways interact or compete at the mechanistic level - whether through activation strength, attention routing, or other mechanisms - leaving the pathway selection process underspecified. </li>
    <li>The role of tokenization in arithmetic performance (e.g., whether numbers are single tokens or multiple tokens) and how this affects pathway engagement is not addressed by the theory. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning, EMNLP [Related work showing memorization effects and training frequency correlation, but does not propose dual-pathway theory or pathway competition]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS [Related work on procedural reasoning and step-by-step processing, but does not propose dual-pathway framework or contrast with pattern matching]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models, arXiv [Related work on algorithmic processing and intermediate steps, but does not propose competing pathways or memorization vs. computation distinction]</li>
    <li>Zhou et al. (2023) Algorithms as Circuits: Mechanistic Interpretability of Arithmetic in Transformers, arXiv [Related mechanistic work but focuses on circuit-level implementation of specific algorithms rather than dual-pathway theory with memorization vs. computation]</li>
    <li>Geva et al. (2020) Transformer Feed-Forward Layers Are Key-Value Memories, EMNLP [Related work on memory-like operations in transformers, but does not propose dual pathways for arithmetic or distinguish pattern matching from algorithmic processing]</li>
    <li>Nogueira et al. (2021) Investigating the Limitations of Transformers with Simple Arithmetic Tasks, arXiv [Related work on format sensitivity and limitations, but does not propose dual-pathway theory to explain these phenomena]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Pathway Arithmetic Processing Theory",
    "theory_description": "Language models process arithmetic through two distinct computational pathways that operate in parallel and compete for output generation. The Pattern-Matching Pathway (PMP) rapidly retrieves answers to arithmetic problems that closely match training distribution patterns, leveraging memorized associations and statistical regularities encoded primarily in early-to-middle transformer layers. The Algorithmic Simulation Pathway (ASP) sequentially constructs solutions by simulating step-by-step computational procedures through the model's recurrent processing of intermediate tokens, with later layers composing sequential operations. The final output is determined by whichever pathway produces stronger activation signals, with PMP dominating for familiar problems (high training frequency, small numbers, standard formats) and ASP engaging for novel or complex problems requiring procedural decomposition. The balance between pathways depends on problem characteristics (numerical magnitude, complexity, format), training data distribution, and architectural factors (depth, attention patterns, model scale).",
    "supporting_evidence": [
        {
            "text": "Language models show dramatically better performance on arithmetic problems involving small numbers (single digits) compared to large numbers, suggesting memorization of common arithmetic facts from training data.",
            "citations": [
                "Brown et al. (2020) Language Models are Few-Shot Learners, NeurIPS",
                "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning, EMNLP"
            ]
        },
        {
            "text": "Performance on arithmetic correlates strongly with the frequency of specific number combinations in training data, indicating pattern-matching from memorized examples.",
            "citations": [
                "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning, EMNLP"
            ]
        },
        {
            "text": "Chain-of-thought prompting significantly improves arithmetic performance, suggesting models can engage procedural, step-by-step processing when prompted to do so.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS",
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models, arXiv"
            ]
        },
        {
            "text": "Models show systematic errors consistent with algorithmic mistakes (e.g., carry errors in addition) rather than random errors, suggesting simulation of computational procedures.",
            "citations": [
                "Hendrycks et al. (2021) Measuring Mathematical Problem Solving With the MATH Dataset, NeurIPS"
            ]
        },
        {
            "text": "Scratchpad and intermediate computation approaches improve arithmetic accuracy, indicating models benefit from explicit externalization of procedural steps.",
            "citations": [
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models, arXiv"
            ]
        },
        {
            "text": "Different layers in transformer models show distinct activation patterns for arithmetic tasks, with early layers encoding numerical magnitude and later layers performing operations, suggesting hierarchical processing stages.",
            "citations": [
                "Geva et al. (2020) Transformer Feed-Forward Layers Are Key-Value Memories, EMNLP"
            ]
        },
        {
            "text": "Models can solve arithmetic problems they've never seen in training when given appropriate prompting, suggesting capability beyond pure memorization.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS"
            ]
        },
        {
            "text": "Format changes (e.g., reversing digit order, adding separators) significantly impact arithmetic performance, indicating sensitivity to surface-level pattern matching rather than robust algorithmic understanding.",
            "citations": [
                "Nogueira et al. (2021) Investigating the Limitations of Transformers with Simple Arithmetic Tasks, arXiv"
            ]
        }
    ],
    "theory_statements": [
        "Language models maintain two parallel processing pathways for arithmetic: a fast Pattern-Matching Pathway (PMP) and a slower Algorithmic Simulation Pathway (ASP).",
        "The PMP operates through direct association retrieval, matching input patterns to memorized arithmetic facts and common number combinations from training data, with primary computational support from early-to-middle transformer layers that encode numerical patterns.",
        "The ASP operates through sequential token generation that simulates step-by-step computational procedures, building intermediate representations, with primary computational support from middle-to-late transformer layers that compose sequential operations.",
        "For problems with high training frequency (small numbers, common operations, standard formats), PMP dominates and produces rapid, accurate responses through direct pattern retrieval.",
        "For problems with low training frequency or high complexity (large numbers, uncommon operations, non-standard formats), ASP engagement increases, requiring more computational steps and intermediate tokens.",
        "The probability of correct output follows a weighted combination: P(correct) = α·P_PMP(correct) + (1-α)·P_ASP(correct), where α ∈ [0,1] represents pathway dominance determined by pattern familiarity (training frequency, format match, number magnitude).",
        "Chain-of-thought prompting increases ASP engagement by forcing explicit generation of intermediate computational steps, effectively reducing α and increasing reliance on procedural processing.",
        "Error patterns differ systematically between pathways: PMP produces substitution errors (retrieving wrong but similar memorized facts), while ASP produces procedural errors (carry mistakes, digit transpositions, step-skipping).",
        "Model scale increases both pathway capabilities, but disproportionately benefits PMP through increased memorization capacity and pattern storage, while ASP benefits from improved sequential processing depth.",
        "Attention mechanisms in early layers support PMP through pattern recognition and numerical encoding, while later layers support ASP through sequential operation composition and intermediate state maintenance.",
        "Format sensitivity (performance degradation with digit reversal, separators, or unusual notation) primarily affects PMP by disrupting pattern matching, while ASP shows greater format robustness when explicitly engaged through chain-of-thought prompting."
    ],
    "new_predictions_likely": [
        "Intervening on middle-layer activations during arithmetic tasks will differentially affect PMP vs ASP: early-layer interventions will disrupt pattern matching more (larger performance drop on high-frequency problems), while late-layer interventions will disrupt algorithmic simulation more (larger performance drop on low-frequency problems requiring chain-of-thought).",
        "Training models with arithmetic problems presented in multiple formats (standard, reversed, with separators) will improve ASP robustness and reduce format sensitivity while maintaining PMP performance on standard formats for high-frequency problems.",
        "Models will show faster response times (fewer tokens to correct answer, lower perplexity on answer tokens) for high-frequency arithmetic problems compared to low-frequency problems of equal computational complexity, reflecting PMP's direct retrieval advantage.",
        "Fine-tuning on chain-of-thought arithmetic data will increase ASP pathway strength, improving performance on novel large-number problems while maintaining or slightly improving performance on memorized small-number problems through enhanced procedural capabilities.",
        "Arithmetic problems presented with irrelevant context or distractors will more strongly impair PMP (pattern matching disrupted by noise) than ASP when chain-of-thought is used, as ASP can filter irrelevant information through explicit step-by-step processing.",
        "Measuring layer-wise activation patterns will show that high-frequency arithmetic problems produce stronger early-layer activations and weaker late-layer activations compared to low-frequency problems, reflecting differential pathway engagement.",
        "Models will show higher confidence (lower entropy in output distribution) for high-frequency arithmetic problems compared to low-frequency problems, reflecting PMP's stronger signal for familiar patterns."
    ],
    "new_predictions_unknown": [
        "Explicitly training models to identify which pathway to use (meta-learning to select PMP vs ASP based on problem characteristics) could dramatically improve arithmetic performance by optimal pathway selection, but it's unclear if models can learn this meta-strategy effectively or if it would interfere with natural pathway competition.",
        "Architecturally separating the two pathways (dedicated PMP and ASP modules with explicit routing) might improve arithmetic performance through specialized processing, but could also reduce flexibility and generalization if the pathways naturally share representations or benefit from implicit competition.",
        "The theory predicts that adversarial examples could be constructed to maximally activate PMP while requiring ASP (e.g., problems that look like memorized patterns but have subtle changes requiring computation), potentially causing systematic failures - the severity and exploitability of such failures is unknown.",
        "If models are trained exclusively on chain-of-thought arithmetic (pure ASP training), they might lose PMP capabilities entirely, or PMP might emerge anyway through compression of repeated procedural patterns into direct associations - the outcome and implications for efficiency are uncertain.",
        "The theory suggests that very large models might develop a third 'hybrid' pathway that combines pattern matching with procedural knowledge (e.g., memorizing common algorithmic subroutines), or that pathway boundaries might blur - whether this emerges, improves performance, and represents a qualitatively different mechanism is unknown.",
        "Training with curriculum learning (gradually increasing number size and complexity) might strengthen the transition mechanisms between pathways, potentially improving overall arithmetic performance, but the optimal curriculum structure and whether explicit transition training helps is unclear."
    ],
    "negative_experiments": [
        "If ablating early transformer layers does not differentially impair performance on high-frequency arithmetic problems (which should rely on PMP) more than low-frequency problems, this would challenge the theory's claim about distinct pathway localization and early-layer pattern matching.",
        "If chain-of-thought prompting does not improve performance on low-frequency arithmetic problems more than high-frequency problems (or improves both equally), this would challenge the theory's claim about ASP engagement for unfamiliar problems and differential pathway activation.",
        "If models show identical error patterns (same types and distributions of mistakes) across high-frequency and low-frequency arithmetic problems, this would challenge the theory's prediction of distinct error signatures for PMP (substitution errors) vs ASP (procedural errors).",
        "If training data frequency has no correlation with arithmetic performance when controlling for problem complexity and number magnitude, this would undermine the PMP component of the theory and the role of memorization.",
        "If format changes (digit reversal, separators) equally impair performance on both memorized (high-frequency) and novel (low-frequency) arithmetic problems, this would challenge the theory's distinction between surface-level pattern matching (PMP) and deeper algorithmic processing (ASP).",
        "If intermediate computation steps (scratchpad) do not improve performance on complex arithmetic more than simple arithmetic, this would challenge the theory's claim about ASP requiring explicit intermediate representations for complex problems.",
        "If layer-wise activation patterns do not differ between high-frequency and low-frequency arithmetic problems (similar activation distributions across layers), this would challenge the theory's claim about differential layer involvement in the two pathways.",
        "If model scaling improves performance equally on high-frequency and low-frequency problems (no differential scaling effects), this would challenge the theory's prediction that scale disproportionately benefits PMP through increased memorization capacity."
    ],
    "unaccounted_for": [
        {
            "text": "The exact mechanism by which transformer attention patterns implement either pattern matching or algorithmic simulation remains unclear at the circuit level, including how specific attention heads contribute to each pathway.",
            "citations": [
                "Elhage et al. (2021) A Mathematical Framework for Transformer Circuits, Anthropic"
            ]
        },
        {
            "text": "Some models show unexpected capabilities on arithmetic tasks that seem too complex for either pure memorization or simple algorithmic simulation (e.g., solving novel multi-step problems without explicit chain-of-thought), suggesting possible additional mechanisms or more sophisticated pathway interactions.",
            "citations": [
                "Lewkowycz et al. (2022) Solving Quantitative Reasoning Problems with Language Models, NeurIPS"
            ]
        },
        {
            "text": "The theory does not fully explain why some models show sudden capability jumps (emergent abilities) at specific scales for arithmetic tasks, rather than smooth improvement, if both pathways should improve gradually with scale.",
            "citations": [
                "Wei et al. (2022) Emergent Abilities of Large Language Models, TMLR"
            ]
        },
        {
            "text": "The theory does not specify how the pathways interact or compete at the mechanistic level - whether through activation strength, attention routing, or other mechanisms - leaving the pathway selection process underspecified.",
            "citations": []
        },
        {
            "text": "The role of tokenization in arithmetic performance (e.g., whether numbers are single tokens or multiple tokens) and how this affects pathway engagement is not addressed by the theory.",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that models can solve arithmetic problems with numbers far outside the training distribution, which is difficult to explain purely through either memorization (PMP) or learned algorithms (ASP) if those algorithms were only trained on smaller numbers.",
            "citations": [
                "Anil et al. (2022) Exploring Length Generalization in Large Language Models, NeurIPS"
            ]
        },
        {
            "text": "Models sometimes show better performance on complex multi-step problems than simpler single-step problems with unfamiliar numbers, contradicting the expected difficulty hierarchy where ASP engagement for complex problems should be more error-prone than PMP for simple problems.",
            "citations": [
                "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems, arXiv"
            ]
        }
    ],
    "special_cases": [
        "For very small models (&lt; 1B parameters), the distinction between pathways may not be meaningful as neither pathway is well-developed, and performance may rely on shallow pattern matching without clear pathway separation.",
        "For arithmetic operations rarely seen in training (e.g., modular arithmetic, bitwise operations, non-standard bases), both pathways may be weak and performance may rely on other mechanisms such as analogical reasoning or linguistic pattern matching.",
        "When problems are presented in natural language word problems, additional language understanding processes interact with both pathways, complicating the dual-pathway dynamics through requirements for problem parsing, entity extraction, and operation identification.",
        "For problems at the boundary of training distribution (e.g., medium-sized numbers, moderately complex operations), both pathways may activate simultaneously with similar strength, leading to interference or competition effects that could either improve performance (ensemble-like) or degrade it (conflicting signals).",
        "For arithmetic with symbolic variables or algebraic expressions (rather than concrete numbers), the pathways may operate differently, with PMP potentially matching algebraic patterns and ASP simulating symbolic manipulation procedures.",
        "In multilingual models, pathway engagement may differ across languages depending on training data distribution and linguistic encoding of numbers, potentially showing language-specific PMP strengths but more universal ASP capabilities."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning, EMNLP [Related work showing memorization effects and training frequency correlation, but does not propose dual-pathway theory or pathway competition]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS [Related work on procedural reasoning and step-by-step processing, but does not propose dual-pathway framework or contrast with pattern matching]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models, arXiv [Related work on algorithmic processing and intermediate steps, but does not propose competing pathways or memorization vs. computation distinction]",
            "Zhou et al. (2023) Algorithms as Circuits: Mechanistic Interpretability of Arithmetic in Transformers, arXiv [Related mechanistic work but focuses on circuit-level implementation of specific algorithms rather than dual-pathway theory with memorization vs. computation]",
            "Geva et al. (2020) Transformer Feed-Forward Layers Are Key-Value Memories, EMNLP [Related work on memory-like operations in transformers, but does not propose dual pathways for arithmetic or distinguish pattern matching from algorithmic processing]",
            "Nogueira et al. (2021) Investigating the Limitations of Transformers with Simple Arithmetic Tasks, arXiv [Related work on format sensitivity and limitations, but does not propose dual-pathway theory to explain these phenomena]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory of how language models perform arithmetic.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-54",
    "original_theory_name": "Dual-Pathway Arithmetic Processing Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
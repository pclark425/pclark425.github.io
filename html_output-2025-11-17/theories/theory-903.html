<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Episodic-Semantic Memory Integration Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-903</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-903</p>
                <p><strong>Name:</strong> Hierarchical Episodic-Semantic Memory Integration Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory posits that LLM agents can best solve text game tasks by integrating two distinct but interacting memory systems: episodic memory (for specific sequences of actions and outcomes) and semantic memory (for general world knowledge and rules). The agent should dynamically switch between or combine these systems based on task demands, using hierarchical control to determine when to rely on specific past experiences versus abstracted knowledge.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Memory System Selection (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; is_facing &#8594; novel or ambiguous game state</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; prioritizes &#8594; episodic memory retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Humans rely on episodic memory when encountering novel situations. </li>
    <li>RL agents with episodic memory modules adapt faster to new tasks. </li>
    <li>Text game agents benefit from recalling specific past action sequences in unfamiliar scenarios. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The principle is known in humans and some AI, but its explicit, formal application to LLM agents in text games is novel.</p>            <p><strong>What Already Exists:</strong> Hierarchical memory systems are established in cognitive science; episodic memory is used for novel situations.</p>            <p><strong>What is Novel:</strong> Formalizing dynamic switching in LLM agents for text games, with explicit hierarchical control.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]</li>
    <li>Blundell et al. (2016) Model-Free Episodic Control [episodic memory in RL]</li>
    <li>Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]</li>
</ul>
            <h3>Statement 1: Semantic Memory Generalization (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; is_facing &#8594; familiar or rule-based game state</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; prioritizes &#8594; semantic memory retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Humans use semantic memory for familiar or rule-based tasks. </li>
    <li>LLMs leverage world knowledge to generalize in text-based environments. </li>
    <li>Text game agents that use semantic memory for known puzzles or rules perform more efficiently. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The principle is known, but the explicit, formalized integration in LLM text game agents is novel.</p>            <p><strong>What Already Exists:</strong> Semantic memory use for generalization is established in cognitive science and LLMs.</p>            <p><strong>What is Novel:</strong> Explicit hierarchical integration and switching in LLM agents for text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]</li>
    <li>Radford et al. (2019) Language Models are Unsupervised Multitask Learners [semantic memory in LLMs]</li>
    <li>Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with explicit episodic-semantic memory integration will outperform agents with only one memory type on tasks requiring both generalization and adaptation.</li>
                <li>Agents that dynamically switch between memory systems based on novelty will solve more puzzles and adapt to new game states faster.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If the boundary between episodic and semantic memory is blurred (e.g., through continual learning), agents may develop hybrid strategies with unpredictable performance.</li>
                <li>Over-reliance on episodic memory may cause agents to overfit to specific game runs, reducing generalization.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with only semantic or only episodic memory outperform integrated agents, the theory would be challenged.</li>
                <li>If hierarchical control leads to slower or less efficient learning, the theory's assumptions may be flawed.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some text games may not require both memory types, making integration unnecessary. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known memory systems but applies them in a novel, formalized way to LLM agent memory in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]</li>
    <li>Blundell et al. (2016) Model-Free Episodic Control [episodic memory in RL]</li>
    <li>Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Episodic-Semantic Memory Integration Theory",
    "theory_description": "This theory posits that LLM agents can best solve text game tasks by integrating two distinct but interacting memory systems: episodic memory (for specific sequences of actions and outcomes) and semantic memory (for general world knowledge and rules). The agent should dynamically switch between or combine these systems based on task demands, using hierarchical control to determine when to rely on specific past experiences versus abstracted knowledge.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Memory System Selection",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "is_facing",
                        "object": "novel or ambiguous game state"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "prioritizes",
                        "object": "episodic memory retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Humans rely on episodic memory when encountering novel situations.",
                        "uuids": []
                    },
                    {
                        "text": "RL agents with episodic memory modules adapt faster to new tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Text game agents benefit from recalling specific past action sequences in unfamiliar scenarios.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical memory systems are established in cognitive science; episodic memory is used for novel situations.",
                    "what_is_novel": "Formalizing dynamic switching in LLM agents for text games, with explicit hierarchical control.",
                    "classification_explanation": "The principle is known in humans and some AI, but its explicit, formal application to LLM agents in text games is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]",
                        "Blundell et al. (2016) Model-Free Episodic Control [episodic memory in RL]",
                        "Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Semantic Memory Generalization",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "is_facing",
                        "object": "familiar or rule-based game state"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "prioritizes",
                        "object": "semantic memory retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Humans use semantic memory for familiar or rule-based tasks.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs leverage world knowledge to generalize in text-based environments.",
                        "uuids": []
                    },
                    {
                        "text": "Text game agents that use semantic memory for known puzzles or rules perform more efficiently.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Semantic memory use for generalization is established in cognitive science and LLMs.",
                    "what_is_novel": "Explicit hierarchical integration and switching in LLM agents for text games.",
                    "classification_explanation": "The principle is known, but the explicit, formalized integration in LLM text game agents is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]",
                        "Radford et al. (2019) Language Models are Unsupervised Multitask Learners [semantic memory in LLMs]",
                        "Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with explicit episodic-semantic memory integration will outperform agents with only one memory type on tasks requiring both generalization and adaptation.",
        "Agents that dynamically switch between memory systems based on novelty will solve more puzzles and adapt to new game states faster."
    ],
    "new_predictions_unknown": [
        "If the boundary between episodic and semantic memory is blurred (e.g., through continual learning), agents may develop hybrid strategies with unpredictable performance.",
        "Over-reliance on episodic memory may cause agents to overfit to specific game runs, reducing generalization."
    ],
    "negative_experiments": [
        "If agents with only semantic or only episodic memory outperform integrated agents, the theory would be challenged.",
        "If hierarchical control leads to slower or less efficient learning, the theory's assumptions may be flawed."
    ],
    "unaccounted_for": [
        {
            "text": "Some text games may not require both memory types, making integration unnecessary.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, simple semantic retrieval (e.g., LLM prompt engineering) is sufficient for high performance.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Games with highly repetitive or deterministic structures may not benefit from episodic memory.",
        "Tasks with rapidly changing rules may require continual updating of semantic memory."
    ],
    "existing_theory": {
        "what_already_exists": "Episodic and semantic memory systems are well-established in cognitive science and some AI models.",
        "what_is_novel": "The explicit, formalized hierarchical integration and dynamic switching in LLM agents for text games.",
        "classification_explanation": "The theory synthesizes known memory systems but applies them in a novel, formalized way to LLM agent memory in text games.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]",
            "Blundell et al. (2016) Model-Free Episodic Control [episodic memory in RL]",
            "Madotto et al. (2020) Memory Grounded Conversational Reasoning [episodic/semantic memory in dialogue agents]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-589",
    "original_theory_name": "Hybrid Memory Architecture Principle for LLM Agents in Text Games",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-932</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-932</p>
                <p><strong>Name:</strong> Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory posits that LLM agents can best solve text game tasks by maintaining a hierarchical memory system that separates and dynamically integrates episodic (event-based, context-specific) and semantic (generalized, abstracted) memories. The agent should use episodic memory to track recent, contextually relevant events and semantic memory to store generalized knowledge about the game world, rules, and object properties. The agent's performance is maximized when it can flexibly retrieve, update, and reconcile these two memory types based on task demands.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Memory Integration Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; has_memory_system &#8594; hierarchical (episodic + semantic)<span style="color: #888888;">, and</span></div>
        <div>&#8226; text game task &#8594; requires &#8594; contextual reasoning and generalization</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; achieves &#8594; higher task performance</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human cognition leverages both episodic and semantic memory for complex reasoning and planning. </li>
    <li>LLMs with memory augmentation (e.g., retrieval-augmented generation) outperform vanilla LLMs on tasks requiring long-term context. </li>
    <li>Cognitive architectures (e.g., Soar, ACT-R) use separate episodic and semantic memory modules to support flexible problem solving. </li>
    <li>Empirical studies show that LLMs struggle with long-horizon dependencies unless provided with explicit memory mechanisms. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the episodic/semantic distinction is known, its hierarchical, dynamic integration for LLM agents in interactive text environments is a new application.</p>            <p><strong>What Already Exists:</strong> The separation of episodic and semantic memory is well-established in cognitive neuroscience and has inspired some AI architectures.</p>            <p><strong>What is Novel:</strong> The explicit hierarchical integration and dynamic reconciliation of these memory types for LLM agents in text games is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [establishes the distinction in human memory]</li>
    <li>Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [nearest neighbor memory for LMs]</li>
    <li>Shin et al. (2023) Memory in Language Models: An Empirical Study of Long-Context Learning [empirical study of LLM memory limitations]</li>
    <li>Laird et al. (2017) The Soar Cognitive Architecture [episodic/semantic memory in cognitive architectures]</li>
</ul>
            <h3>Statement 1: Dynamic Memory Prioritization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; faces &#8594; resource constraints (e.g., context window, compute)<span style="color: #888888;">, and</span></div>
        <div>&#8226; text game task &#8594; has &#8594; variable information salience</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; prioritizes &#8594; salient, task-relevant memories for storage and retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human working memory prioritizes salient or goal-relevant information under resource constraints. </li>
    <li>LLM agents with selective memory retrieval outperform those with naive, exhaustive memory usage in long-horizon tasks. </li>
    <li>Studies on LLMs show that context window limitations force models to drop less relevant information, impacting performance. </li>
    <li>Salience-based memory management is effective in reinforcement learning agents for long-horizon tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general idea is related to existing work, but the explicit, dynamic prioritization for LLM agents in text games is new.</p>            <p><strong>What Already Exists:</strong> Salience-based memory prioritization is known in cognitive science and some AI memory models.</p>            <p><strong>What is Novel:</strong> The application of dynamic, context-sensitive prioritization in LLM agents for text games, with explicit mechanisms for salience estimation, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Baddeley (1992) Working memory [working memory and prioritization in humans]</li>
    <li>Paranjape et al. (2022) Retrospective Memory in RL Agents [salience-based memory in RL]</li>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [LLM context window limitations]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with explicit separation and integration of episodic and semantic memory will outperform agents with only one memory type on multi-step, context-dependent text game tasks.</li>
                <li>Agents that dynamically prioritize salient memories (e.g., recent events, unsolved puzzles) will solve more tasks within a fixed context window than agents that store all information indiscriminately.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LLM agent can learn to autonomously abstract semantic knowledge from episodic traces during gameplay, it may develop emergent generalization abilities not present in its pretraining.</li>
                <li>Hierarchical memory systems may enable LLM agents to transfer strategies across different text games with minimal fine-tuning, but the extent of this transfer is unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with hierarchical memory do not outperform flat memory agents on tasks requiring both recall of specific events and generalization, the theory is called into question.</li>
                <li>If dynamic memory prioritization does not improve performance under resource constraints, the prioritization law is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of adversarial or misleading information on hierarchical memory integration is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known cognitive principles but applies them in a new, structured way to LLM agents in interactive text environments.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [human memory theory]</li>
    <li>Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [memory-augmented LMs]</li>
    <li>Paranjape et al. (2022) Retrospective Memory in RL Agents [salience-based memory in RL]</li>
    <li>Shin et al. (2023) Memory in Language Models: An Empirical Study of Long-Context Learning [LLM memory limitations]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games",
    "theory_description": "This theory posits that LLM agents can best solve text game tasks by maintaining a hierarchical memory system that separates and dynamically integrates episodic (event-based, context-specific) and semantic (generalized, abstracted) memories. The agent should use episodic memory to track recent, contextually relevant events and semantic memory to store generalized knowledge about the game world, rules, and object properties. The agent's performance is maximized when it can flexibly retrieve, update, and reconcile these two memory types based on task demands.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Memory Integration Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "has_memory_system",
                        "object": "hierarchical (episodic + semantic)"
                    },
                    {
                        "subject": "text game task",
                        "relation": "requires",
                        "object": "contextual reasoning and generalization"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "achieves",
                        "object": "higher task performance"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human cognition leverages both episodic and semantic memory for complex reasoning and planning.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs with memory augmentation (e.g., retrieval-augmented generation) outperform vanilla LLMs on tasks requiring long-term context.",
                        "uuids": []
                    },
                    {
                        "text": "Cognitive architectures (e.g., Soar, ACT-R) use separate episodic and semantic memory modules to support flexible problem solving.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that LLMs struggle with long-horizon dependencies unless provided with explicit memory mechanisms.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The separation of episodic and semantic memory is well-established in cognitive neuroscience and has inspired some AI architectures.",
                    "what_is_novel": "The explicit hierarchical integration and dynamic reconciliation of these memory types for LLM agents in text games is novel.",
                    "classification_explanation": "While the episodic/semantic distinction is known, its hierarchical, dynamic integration for LLM agents in interactive text environments is a new application.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [establishes the distinction in human memory]",
                        "Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [nearest neighbor memory for LMs]",
                        "Shin et al. (2023) Memory in Language Models: An Empirical Study of Long-Context Learning [empirical study of LLM memory limitations]",
                        "Laird et al. (2017) The Soar Cognitive Architecture [episodic/semantic memory in cognitive architectures]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dynamic Memory Prioritization Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "faces",
                        "object": "resource constraints (e.g., context window, compute)"
                    },
                    {
                        "subject": "text game task",
                        "relation": "has",
                        "object": "variable information salience"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "prioritizes",
                        "object": "salient, task-relevant memories for storage and retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human working memory prioritizes salient or goal-relevant information under resource constraints.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents with selective memory retrieval outperform those with naive, exhaustive memory usage in long-horizon tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Studies on LLMs show that context window limitations force models to drop less relevant information, impacting performance.",
                        "uuids": []
                    },
                    {
                        "text": "Salience-based memory management is effective in reinforcement learning agents for long-horizon tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Salience-based memory prioritization is known in cognitive science and some AI memory models.",
                    "what_is_novel": "The application of dynamic, context-sensitive prioritization in LLM agents for text games, with explicit mechanisms for salience estimation, is novel.",
                    "classification_explanation": "The general idea is related to existing work, but the explicit, dynamic prioritization for LLM agents in text games is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Baddeley (1992) Working memory [working memory and prioritization in humans]",
                        "Paranjape et al. (2022) Retrospective Memory in RL Agents [salience-based memory in RL]",
                        "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [LLM context window limitations]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with explicit separation and integration of episodic and semantic memory will outperform agents with only one memory type on multi-step, context-dependent text game tasks.",
        "Agents that dynamically prioritize salient memories (e.g., recent events, unsolved puzzles) will solve more tasks within a fixed context window than agents that store all information indiscriminately."
    ],
    "new_predictions_unknown": [
        "If an LLM agent can learn to autonomously abstract semantic knowledge from episodic traces during gameplay, it may develop emergent generalization abilities not present in its pretraining.",
        "Hierarchical memory systems may enable LLM agents to transfer strategies across different text games with minimal fine-tuning, but the extent of this transfer is unknown."
    ],
    "negative_experiments": [
        "If agents with hierarchical memory do not outperform flat memory agents on tasks requiring both recall of specific events and generalization, the theory is called into question.",
        "If dynamic memory prioritization does not improve performance under resource constraints, the prioritization law is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of adversarial or misleading information on hierarchical memory integration is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that LLMs can perform well on certain text games with minimal explicit memory augmentation, suggesting implicit memory in model weights.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with extremely short horizons or minimal context requirements may not benefit from hierarchical memory.",
        "Games with highly stochastic or non-deterministic environments may challenge the utility of semantic memory."
    ],
    "existing_theory": {
        "what_already_exists": "Episodic/semantic memory distinction and salience-based prioritization are established in cognitive science and some AI models.",
        "what_is_novel": "The explicit, hierarchical, and dynamic integration of these memory types for LLM agents in text games is novel.",
        "classification_explanation": "The theory synthesizes known cognitive principles but applies them in a new, structured way to LLM agents in interactive text environments.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [human memory theory]",
            "Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [memory-augmented LMs]",
            "Paranjape et al. (2022) Retrospective Memory in RL Agents [salience-based memory in RL]",
            "Shin et al. (2023) Memory in Language Models: An Empirical Study of Long-Context Learning [LLM memory limitations]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-591",
    "original_theory_name": "Task-Structure-Dependent Memory Utility Law for LLM Agents",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
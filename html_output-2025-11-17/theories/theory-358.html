<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Objectivity-Subjectivity Spectrum Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-358</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-358</p>
                <p><strong>Name:</strong> Objectivity-Subjectivity Spectrum Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about alignment between proxy evaluations (LLM-as-a-judge, Likert-style) and expert human review for software development artifacts, including necessary conditions for high agreement.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that alignment between proxy evaluations (LLM-as-a-judge, Likert-style ratings) and expert human review for software development artifacts varies systematically along an objectivity-subjectivity spectrum. High agreement occurs when evaluation criteria are objective, measurable, and rule-based (e.g., syntax correctness, test coverage, security vulnerabilities). Agreement degrades progressively as criteria become more subjective, interpretive, and context-dependent (e.g., code elegance, architectural appropriateness, maintainability). The theory predicts that necessary conditions for high agreement include: (1) clear operationalization of evaluation criteria with minimal interpretive variance, (2) sufficient training data or examples for the proxy system covering the objective aspects being evaluated, (3) limited dependence on tacit knowledge or organizational context, and (4) evaluation tasks that can be decomposed into verifiable sub-components. The theory incorporates moderating factors including evaluator expertise, organizational conventions, and the quality of operationalization (e.g., explicit rubrics, concrete scale anchors). It further predicts that hybrid approaches combining objective automated checks with human review of subjective aspects will outperform either approach alone, and that proxy systems may achieve higher agreement with expert consensus than with individual experts on subjective criteria due to their ability to capture central tendencies in training data.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Alignment between proxy evaluations and expert human review is inversely related to the subjectivity of evaluation criteria, with objective criteria yielding substantially higher correlation than subjective criteria (expected correlation differences of 0.3-0.5 or more).</li>
                <li>Evaluation criteria can be positioned on an objectivity-subjectivity spectrum with at least five distinct levels: (1) Deterministic/Verifiable (syntax, compilation, type checking), (2) Measurable/Quantifiable (complexity metrics, test coverage, performance benchmarks), (3) Rule-based/Heuristic (coding standards, design pattern compliance, style guide adherence), (4) Interpretive/Contextual (code clarity, architectural appropriateness, maintainability for specific contexts), and (5) Aesthetic/Philosophical (elegance, beauty, conceptual purity).</li>
                <li>High proxy-human agreement (concordance >75-80%) requires that evaluation criteria be operationalized such that the majority of evaluation weight falls on levels 1-3 of the objectivity spectrum, with specific thresholds depending on the quality of operationalization.</li>
                <li>LLM-as-a-judge systems exhibit a characteristic degradation pattern across spectrum levels, with agreement decreasing monotonically from level 1 (near-perfect agreement) through level 5 (substantially reduced agreement approaching baseline), though the exact thresholds vary based on model capabilities, training data, and prompting strategies.</li>
                <li>Necessary conditions for high proxy-human agreement include: (a) evaluation rubrics with explicit decision rules and concrete examples, (b) minimal dependence on organizational or project-specific tacit knowledge, (c) availability of representative examples for calibration, and (d) decomposability of complex judgments into simpler, more objective sub-judgments.</li>
                <li>The variance in human expert judgments increases as criteria move toward the subjective end of the spectrum, creating a theoretical ceiling on proxy-human alignment for subjective criteria, since proxies cannot align better with humans than humans align with each other.</li>
                <li>Proxy systems can achieve higher agreement with expert consensus (aggregated expert opinion) than with individual experts on subjective criteria, due to the proxy's ability to capture central tendencies in training data, effectively averaging over diverse human perspectives.</li>
                <li>Temporal stability of evaluations (test-retest reliability) follows the spectrum pattern, with objective criteria showing high stability (>90%) and subjective criteria showing substantially lower stability, reflecting the inherent variability in subjective judgments.</li>
                <li>Moderating factors including evaluator expertise, organizational conventions, domain maturity, and quality of operationalization can shift the effective position of criteria on the spectrum or alter agreement levels within spectrum categories.</li>
                <li>The spectrum effect is moderated by shared context: criteria that appear subjective in general may behave as objective within communities with strong shared conventions (e.g., style guides, established best practices).</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Code review studies show high inter-rater reliability for objective defects (syntax errors, security vulnerabilities) but low agreement on subjective quality attributes (readability, maintainability), demonstrating the fundamental objectivity-subjectivity divide in software evaluation. </li>
    <li>LLM-based evaluation systems show higher correlation with human judgments on factual correctness tasks compared to creative or stylistic tasks, supporting the spectrum hypothesis that objective criteria yield better proxy-human alignment. </li>
    <li>Automated static analysis tools achieve high precision on rule-based code quality metrics but struggle with context-dependent quality assessments, illustrating the limitations of automated systems on subjective criteria. </li>
    <li>Human experts show higher agreement when evaluating software artifacts against explicit, measurable criteria versus implicit, experience-based criteria, supporting the theory's emphasis on operationalization as a necessary condition. </li>
    <li>Likert-scale evaluations show higher inter-rater reliability when scale anchors are concrete and behavioral rather than abstract and interpretive, demonstrating that operationalization quality affects agreement even within subjective evaluation frameworks. </li>
    <li>Experienced developers can achieve high agreement on certain subjective code quality attributes through shared mental models and conventions, suggesting that expertise and shared context moderate the spectrum effect. </li>
    <li>Advanced prompting techniques like chain-of-thought can improve LLM performance on complex reasoning tasks, suggesting that proxy system capabilities on subjective criteria may be enhanced through better prompting strategies. </li>
    <li>Expert performance varies significantly based on task characteristics and domain structure, indicating that evaluator expertise interacts with criterion objectivity to determine agreement levels. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If evaluation criteria for code reviews are decomposed into predominantly objective metrics (test coverage, cyclomatic complexity, security scan results, style guide compliance) with limited subjective assessments, proxy-human agreement will substantially exceed agreement for predominantly subjective evaluation schemes.</li>
                <li>LLM-as-a-judge systems will show higher agreement with senior developers than junior developers on objective criteria, but the gap will narrow or reverse on highly subjective criteria where junior developers may align better with the averaged perspectives captured in training data.</li>
                <li>Providing LLM judges with explicit rubrics, decision trees, and concrete examples will improve agreement on rule-based (level 3) criteria by 10-20 percentage points, with diminishing returns on more subjective criteria.</li>
                <li>Inter-rater reliability among human experts will strongly predict proxy-human alignment: high human-human agreement (>0.8) will be associated with high proxy-human agreement (>0.7), while low human-human agreement (<0.6) will be associated with correspondingly low proxy-human agreement.</li>
                <li>Hybrid evaluation systems that route objective criteria to automated checks and subjective criteria to human review will achieve higher overall accuracy than either pure automated or pure human evaluation, while substantially reducing human review time.</li>
                <li>In domains with mature, well-documented conventions (e.g., Python with PEP 8, Java with established enterprise patterns), proxy-human agreement will be higher than in domains with less standardization, even for nominally similar criteria.</li>
                <li>Proxy systems trained on organization-specific code review data will show improved agreement on level 3 criteria that reflect organizational conventions, but limited improvement on level 4-5 criteria that remain inherently subjective.</li>
                <li>Temporal stability of proxy evaluations will mirror the spectrum pattern: proxies will show high consistency on objective criteria across multiple evaluations of the same artifact, but lower consistency on subjective criteria.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLM-as-a-judge systems are fine-tuned on organization-specific code review data including subjective preferences and contextual factors, they may achieve agreement levels on level 4 (interpretive) criteria that approach or exceed the theoretical ceiling set by human-human agreement, potentially demonstrating that proxies can learn organizational tacit knowledge.</li>
                <li>Multi-agent LLM systems where different agents specialize in different spectrum levels and aggregate judgments through structured deliberation may achieve superlinear improvements in agreement, potentially reaching high agreement (>75%) even on level 4 criteria through complementary strengths.</li>
                <li>The introduction of interactive evaluation protocols where the LLM judge can ask clarifying questions about context, requirements, and constraints may effectively shift certain level 4 and 5 criteria down to level 3, fundamentally altering the spectrum boundaries and enabling higher agreement on previously subjective criteria.</li>
                <li>As LLMs become more sophisticated and incorporate more diverse training data, the degradation pattern across spectrum levels may become non-monotonic, with some level 5 (aesthetic) criteria becoming easier to evaluate than certain level 3 (rule-based) criteria that require deep, specialized domain knowledge not well-represented in training data.</li>
                <li>The spectrum theory may break down or invert for certain specialized domains (e.g., safety-critical systems, cryptographic code, formal verification contexts) where seemingly subjective criteria (elegance, simplicity) actually map to objective security or correctness properties, creating unexpected correlation patterns.</li>
                <li>Incorporating explicit uncertainty quantification in proxy evaluations (e.g., confidence scores, multiple alternative judgments) may enable selective routing strategies that achieve high overall agreement by deferring high-uncertainty subjective judgments to humans while automating high-confidence objective judgments.</li>
                <li>Cross-cultural or cross-organizational proxy systems trained on diverse data may achieve higher agreement with diverse human evaluators than organization-specific proxies, by capturing a broader range of valid perspectives rather than overfitting to local conventions.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding cases where proxy-human agreement is consistently higher for subjective criteria (level 4-5) than objective criteria (level 1-2) within the same evaluation domain and using the same proxy system would contradict the core spectrum hypothesis.</li>
                <li>Demonstrating that explicit operationalization, rubrics, and concrete examples do not improve agreement on level 3 criteria compared to vague or abstract criteria would challenge the theory's predictions about necessary conditions for high agreement.</li>
                <li>Showing that human-human agreement and proxy-human agreement are uncorrelated across different evaluation criteria would undermine the theory's assumption that proxy systems are fundamentally limited by inherent criterion subjectivity rather than technical limitations.</li>
                <li>Finding that temporal stability of evaluations does not follow the spectrum pattern (e.g., subjective criteria showing higher test-retest reliability than objective criteria for either humans or proxies) would question the theory's coherence and the validity of the spectrum construct.</li>
                <li>Demonstrating that hybrid systems combining automated objective evaluation with human subjective evaluation do not outperform pure proxy or pure human evaluation would challenge the theory's practical implications and the value of spectrum-based routing.</li>
                <li>Finding that shared conventions and organizational context do not moderate the spectrum effect (i.e., agreement remains equally low on subjective criteria even in highly standardized domains) would challenge the theory's account of moderating factors.</li>
                <li>Showing that proxy systems cannot achieve higher agreement with expert consensus than with individual experts on subjective criteria would contradict the theory's prediction about proxies capturing central tendencies.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The specific mechanisms by which evaluator expertise moderates the spectrum effect are not fully specified—whether expertise primarily improves agreement on objective criteria through better detection, or on subjective criteria through shared mental models, or both. </li>
    <li>Cultural and organizational factors that may shift the perceived objectivity-subjectivity of certain criteria are acknowledged but not systematically integrated into the theory's predictive framework. </li>
    <li>The theory does not account for how disagreement types (false positives vs false negatives, severity of disagreements) may vary systematically across the spectrum, which could have important practical implications for when to trust proxy evaluations. </li>
    <li>The role of artifact complexity and size in moderating the spectrum effect is not addressed—whether the spectrum pattern holds equally for small code snippets versus large architectural decisions. </li>
    <li>The theory does not fully specify how the spectrum might evolve over time as LLM capabilities improve, coding practices change, or new evaluation criteria emerge. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Zheng et al. (2023) Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena [Related work on LLM evaluation alignment but does not propose a systematic spectrum theory for software artifacts or formalize the objectivity-subjectivity dimension]</li>
    <li>Bacchelli and Bird (2013) Expectations, outcomes, and challenges of modern code review [Discusses objective vs subjective aspects of code review empirically but does not formalize a spectrum theory or make systematic predictions about proxy-human alignment]</li>
    <li>Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment [Related to LLM evaluation alignment but focused on natural language generation, not software artifacts, and does not propose spectrum framework]</li>
    <li>Kitchenham et al. (2002) Preliminary guidelines for empirical research in software engineering [Discusses subjectivity in software engineering research methodology but not in the context of proxy-human alignment or systematic spectrum theory]</li>
    <li>Johnson et al. (2013) Why don't software developers use static analysis tools to find bugs? [Discusses limitations of automated tools but does not propose a systematic theory of objectivity-subjectivity spectrum]</li>
    <li>McIntosh et al. (2014) The impact of code review coverage and code review participation on software quality [Empirical work on code review but does not propose spectrum theory for proxy-human alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Objectivity-Subjectivity Spectrum Theory",
    "theory_description": "This theory posits that alignment between proxy evaluations (LLM-as-a-judge, Likert-style ratings) and expert human review for software development artifacts varies systematically along an objectivity-subjectivity spectrum. High agreement occurs when evaluation criteria are objective, measurable, and rule-based (e.g., syntax correctness, test coverage, security vulnerabilities). Agreement degrades progressively as criteria become more subjective, interpretive, and context-dependent (e.g., code elegance, architectural appropriateness, maintainability). The theory predicts that necessary conditions for high agreement include: (1) clear operationalization of evaluation criteria with minimal interpretive variance, (2) sufficient training data or examples for the proxy system covering the objective aspects being evaluated, (3) limited dependence on tacit knowledge or organizational context, and (4) evaluation tasks that can be decomposed into verifiable sub-components. The theory incorporates moderating factors including evaluator expertise, organizational conventions, and the quality of operationalization (e.g., explicit rubrics, concrete scale anchors). It further predicts that hybrid approaches combining objective automated checks with human review of subjective aspects will outperform either approach alone, and that proxy systems may achieve higher agreement with expert consensus than with individual experts on subjective criteria due to their ability to capture central tendencies in training data.",
    "supporting_evidence": [
        {
            "text": "Code review studies show high inter-rater reliability for objective defects (syntax errors, security vulnerabilities) but low agreement on subjective quality attributes (readability, maintainability), demonstrating the fundamental objectivity-subjectivity divide in software evaluation.",
            "citations": [
                "Bacchelli and Bird (2013) Expectations, outcomes, and challenges of modern code review",
                "McIntosh et al. (2014) The impact of code review coverage and code review participation on software quality"
            ]
        },
        {
            "text": "LLM-based evaluation systems show higher correlation with human judgments on factual correctness tasks compared to creative or stylistic tasks, supporting the spectrum hypothesis that objective criteria yield better proxy-human alignment.",
            "citations": [
                "Zheng et al. (2023) Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
                "Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment"
            ]
        },
        {
            "text": "Automated static analysis tools achieve high precision on rule-based code quality metrics but struggle with context-dependent quality assessments, illustrating the limitations of automated systems on subjective criteria.",
            "citations": [
                "Johnson et al. (2013) Why don't software developers use static analysis tools to find bugs?",
                "Ayewah et al. (2008) Using static analysis to find bugs"
            ]
        },
        {
            "text": "Human experts show higher agreement when evaluating software artifacts against explicit, measurable criteria versus implicit, experience-based criteria, supporting the theory's emphasis on operationalization as a necessary condition.",
            "citations": [
                "Höst and Wohlin (2000) A subjective effort estimation experiment",
                "Kitchenham et al. (2002) Preliminary guidelines for empirical research in software engineering"
            ]
        },
        {
            "text": "Likert-scale evaluations show higher inter-rater reliability when scale anchors are concrete and behavioral rather than abstract and interpretive, demonstrating that operationalization quality affects agreement even within subjective evaluation frameworks.",
            "citations": [
                "Krosnick and Presser (2010) Question and Questionnaire Design",
                "Weng (2004) Constructing a Likert scale with clear construction"
            ]
        },
        {
            "text": "Experienced developers can achieve high agreement on certain subjective code quality attributes through shared mental models and conventions, suggesting that expertise and shared context moderate the spectrum effect.",
            "citations": [
                "Beller et al. (2014) Modern code reviews in open-source projects: Which problems do they fix?"
            ]
        },
        {
            "text": "Advanced prompting techniques like chain-of-thought can improve LLM performance on complex reasoning tasks, suggesting that proxy system capabilities on subjective criteria may be enhanced through better prompting strategies.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
            ]
        },
        {
            "text": "Expert performance varies significantly based on task characteristics and domain structure, indicating that evaluator expertise interacts with criterion objectivity to determine agreement levels.",
            "citations": [
                "Shanteau (1992) Competence in experts: The role of task characteristics",
                "Ericsson and Lehmann (1996) Expert and exceptional performance: Evidence of maximal adaptation to task constraints"
            ]
        }
    ],
    "theory_statements": [
        "Alignment between proxy evaluations and expert human review is inversely related to the subjectivity of evaluation criteria, with objective criteria yielding substantially higher correlation than subjective criteria (expected correlation differences of 0.3-0.5 or more).",
        "Evaluation criteria can be positioned on an objectivity-subjectivity spectrum with at least five distinct levels: (1) Deterministic/Verifiable (syntax, compilation, type checking), (2) Measurable/Quantifiable (complexity metrics, test coverage, performance benchmarks), (3) Rule-based/Heuristic (coding standards, design pattern compliance, style guide adherence), (4) Interpretive/Contextual (code clarity, architectural appropriateness, maintainability for specific contexts), and (5) Aesthetic/Philosophical (elegance, beauty, conceptual purity).",
        "High proxy-human agreement (concordance &gt;75-80%) requires that evaluation criteria be operationalized such that the majority of evaluation weight falls on levels 1-3 of the objectivity spectrum, with specific thresholds depending on the quality of operationalization.",
        "LLM-as-a-judge systems exhibit a characteristic degradation pattern across spectrum levels, with agreement decreasing monotonically from level 1 (near-perfect agreement) through level 5 (substantially reduced agreement approaching baseline), though the exact thresholds vary based on model capabilities, training data, and prompting strategies.",
        "Necessary conditions for high proxy-human agreement include: (a) evaluation rubrics with explicit decision rules and concrete examples, (b) minimal dependence on organizational or project-specific tacit knowledge, (c) availability of representative examples for calibration, and (d) decomposability of complex judgments into simpler, more objective sub-judgments.",
        "The variance in human expert judgments increases as criteria move toward the subjective end of the spectrum, creating a theoretical ceiling on proxy-human alignment for subjective criteria, since proxies cannot align better with humans than humans align with each other.",
        "Proxy systems can achieve higher agreement with expert consensus (aggregated expert opinion) than with individual experts on subjective criteria, due to the proxy's ability to capture central tendencies in training data, effectively averaging over diverse human perspectives.",
        "Temporal stability of evaluations (test-retest reliability) follows the spectrum pattern, with objective criteria showing high stability (&gt;90%) and subjective criteria showing substantially lower stability, reflecting the inherent variability in subjective judgments.",
        "Moderating factors including evaluator expertise, organizational conventions, domain maturity, and quality of operationalization can shift the effective position of criteria on the spectrum or alter agreement levels within spectrum categories.",
        "The spectrum effect is moderated by shared context: criteria that appear subjective in general may behave as objective within communities with strong shared conventions (e.g., style guides, established best practices)."
    ],
    "new_predictions_likely": [
        "If evaluation criteria for code reviews are decomposed into predominantly objective metrics (test coverage, cyclomatic complexity, security scan results, style guide compliance) with limited subjective assessments, proxy-human agreement will substantially exceed agreement for predominantly subjective evaluation schemes.",
        "LLM-as-a-judge systems will show higher agreement with senior developers than junior developers on objective criteria, but the gap will narrow or reverse on highly subjective criteria where junior developers may align better with the averaged perspectives captured in training data.",
        "Providing LLM judges with explicit rubrics, decision trees, and concrete examples will improve agreement on rule-based (level 3) criteria by 10-20 percentage points, with diminishing returns on more subjective criteria.",
        "Inter-rater reliability among human experts will strongly predict proxy-human alignment: high human-human agreement (&gt;0.8) will be associated with high proxy-human agreement (&gt;0.7), while low human-human agreement (&lt;0.6) will be associated with correspondingly low proxy-human agreement.",
        "Hybrid evaluation systems that route objective criteria to automated checks and subjective criteria to human review will achieve higher overall accuracy than either pure automated or pure human evaluation, while substantially reducing human review time.",
        "In domains with mature, well-documented conventions (e.g., Python with PEP 8, Java with established enterprise patterns), proxy-human agreement will be higher than in domains with less standardization, even for nominally similar criteria.",
        "Proxy systems trained on organization-specific code review data will show improved agreement on level 3 criteria that reflect organizational conventions, but limited improvement on level 4-5 criteria that remain inherently subjective.",
        "Temporal stability of proxy evaluations will mirror the spectrum pattern: proxies will show high consistency on objective criteria across multiple evaluations of the same artifact, but lower consistency on subjective criteria."
    ],
    "new_predictions_unknown": [
        "If LLM-as-a-judge systems are fine-tuned on organization-specific code review data including subjective preferences and contextual factors, they may achieve agreement levels on level 4 (interpretive) criteria that approach or exceed the theoretical ceiling set by human-human agreement, potentially demonstrating that proxies can learn organizational tacit knowledge.",
        "Multi-agent LLM systems where different agents specialize in different spectrum levels and aggregate judgments through structured deliberation may achieve superlinear improvements in agreement, potentially reaching high agreement (&gt;75%) even on level 4 criteria through complementary strengths.",
        "The introduction of interactive evaluation protocols where the LLM judge can ask clarifying questions about context, requirements, and constraints may effectively shift certain level 4 and 5 criteria down to level 3, fundamentally altering the spectrum boundaries and enabling higher agreement on previously subjective criteria.",
        "As LLMs become more sophisticated and incorporate more diverse training data, the degradation pattern across spectrum levels may become non-monotonic, with some level 5 (aesthetic) criteria becoming easier to evaluate than certain level 3 (rule-based) criteria that require deep, specialized domain knowledge not well-represented in training data.",
        "The spectrum theory may break down or invert for certain specialized domains (e.g., safety-critical systems, cryptographic code, formal verification contexts) where seemingly subjective criteria (elegance, simplicity) actually map to objective security or correctness properties, creating unexpected correlation patterns.",
        "Incorporating explicit uncertainty quantification in proxy evaluations (e.g., confidence scores, multiple alternative judgments) may enable selective routing strategies that achieve high overall agreement by deferring high-uncertainty subjective judgments to humans while automating high-confidence objective judgments.",
        "Cross-cultural or cross-organizational proxy systems trained on diverse data may achieve higher agreement with diverse human evaluators than organization-specific proxies, by capturing a broader range of valid perspectives rather than overfitting to local conventions."
    ],
    "negative_experiments": [
        "Finding cases where proxy-human agreement is consistently higher for subjective criteria (level 4-5) than objective criteria (level 1-2) within the same evaluation domain and using the same proxy system would contradict the core spectrum hypothesis.",
        "Demonstrating that explicit operationalization, rubrics, and concrete examples do not improve agreement on level 3 criteria compared to vague or abstract criteria would challenge the theory's predictions about necessary conditions for high agreement.",
        "Showing that human-human agreement and proxy-human agreement are uncorrelated across different evaluation criteria would undermine the theory's assumption that proxy systems are fundamentally limited by inherent criterion subjectivity rather than technical limitations.",
        "Finding that temporal stability of evaluations does not follow the spectrum pattern (e.g., subjective criteria showing higher test-retest reliability than objective criteria for either humans or proxies) would question the theory's coherence and the validity of the spectrum construct.",
        "Demonstrating that hybrid systems combining automated objective evaluation with human subjective evaluation do not outperform pure proxy or pure human evaluation would challenge the theory's practical implications and the value of spectrum-based routing.",
        "Finding that shared conventions and organizational context do not moderate the spectrum effect (i.e., agreement remains equally low on subjective criteria even in highly standardized domains) would challenge the theory's account of moderating factors.",
        "Showing that proxy systems cannot achieve higher agreement with expert consensus than with individual experts on subjective criteria would contradict the theory's prediction about proxies capturing central tendencies."
    ],
    "unaccounted_for": [
        {
            "text": "The specific mechanisms by which evaluator expertise moderates the spectrum effect are not fully specified—whether expertise primarily improves agreement on objective criteria through better detection, or on subjective criteria through shared mental models, or both.",
            "citations": [
                "Shanteau (1992) Competence in experts: The role of task characteristics",
                "Ericsson and Lehmann (1996) Expert and exceptional performance: Evidence of maximal adaptation to task constraints"
            ]
        },
        {
            "text": "Cultural and organizational factors that may shift the perceived objectivity-subjectivity of certain criteria are acknowledged but not systematically integrated into the theory's predictive framework.",
            "citations": [
                "Hofstede (2001) Culture's Consequences: Comparing Values, Behaviors, Institutions and Organizations Across Nations"
            ]
        },
        {
            "text": "The theory does not account for how disagreement types (false positives vs false negatives, severity of disagreements) may vary systematically across the spectrum, which could have important practical implications for when to trust proxy evaluations.",
            "citations": [
                "Muske and Serebrenik (2016) Survey on code review tools"
            ]
        },
        {
            "text": "The role of artifact complexity and size in moderating the spectrum effect is not addressed—whether the spectrum pattern holds equally for small code snippets versus large architectural decisions.",
            "citations": []
        },
        {
            "text": "The theory does not fully specify how the spectrum might evolve over time as LLM capabilities improve, coding practices change, or new evaluation criteria emerge.",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that experienced developers can achieve high agreement on subjective code quality attributes through shared mental models and conventions, suggesting that the spectrum effect may be weaker within expert communities than the theory predicts.",
            "citations": [
                "Beller et al. (2014) Modern code reviews in open-source projects: Which problems do they fix?"
            ]
        },
        {
            "text": "Recent work on LLM evaluation with advanced prompting techniques suggests that LLMs can match or exceed human performance on some creative and subjective tasks, potentially challenging the theory's prediction of monotonic degradation across the spectrum.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
            ]
        },
        {
            "text": "Some automated tools for subjective quality assessment (e.g., readability metrics) show reasonable correlation with human judgments despite evaluating ostensibly subjective criteria, suggesting that some level 4 criteria may be more amenable to automation than the theory predicts.",
            "citations": []
        }
    ],
    "special_cases": [
        "In domains with strong conventions and comprehensive style guides (e.g., Python with PEP 8, Go with gofmt), certain nominally subjective criteria (code style, formatting preferences) effectively behave as objective criteria due to community consensus, shifting their position on the spectrum.",
        "For safety-critical or regulated software (e.g., medical devices, aviation systems, financial systems), subjective aesthetic criteria may be superseded by objective compliance requirements and formal verification, effectively collapsing the subjective end of the spectrum.",
        "When evaluating novel or experimental code (e.g., research prototypes, exploratory implementations), the spectrum may shift toward subjectivity as there are fewer established norms and conventions for objective evaluation, and context-dependence increases.",
        "Cross-cultural or cross-organizational evaluations may experience spectrum shifts as criteria that are objective within one context (due to local conventions) become subjective across contexts, reducing proxy-human agreement even on nominally objective criteria.",
        "In highly specialized technical domains (e.g., cryptography, formal verification, performance optimization), certain aesthetic criteria (simplicity, elegance) may actually map to objective properties (security, correctness, efficiency), creating inverse patterns where subjective and objective criteria converge.",
        "For legacy code or code in unfamiliar languages/frameworks, even objective criteria may become more subjective due to limited context and understanding, effectively shifting the entire spectrum toward subjectivity.",
        "In open-source communities with diverse contributors, the spectrum may be broader and agreement lower even on objective criteria due to varying interpretations of standards, while in tightly-controlled enterprise environments, the spectrum may be narrower with higher agreement across all levels."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Zheng et al. (2023) Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena [Related work on LLM evaluation alignment but does not propose a systematic spectrum theory for software artifacts or formalize the objectivity-subjectivity dimension]",
            "Bacchelli and Bird (2013) Expectations, outcomes, and challenges of modern code review [Discusses objective vs subjective aspects of code review empirically but does not formalize a spectrum theory or make systematic predictions about proxy-human alignment]",
            "Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment [Related to LLM evaluation alignment but focused on natural language generation, not software artifacts, and does not propose spectrum framework]",
            "Kitchenham et al. (2002) Preliminary guidelines for empirical research in software engineering [Discusses subjectivity in software engineering research methodology but not in the context of proxy-human alignment or systematic spectrum theory]",
            "Johnson et al. (2013) Why don't software developers use static analysis tools to find bugs? [Discusses limitations of automated tools but does not propose a systematic theory of objectivity-subjectivity spectrum]",
            "McIntosh et al. (2014) The impact of code review coverage and code review participation on software quality [Empirical work on code review but does not propose spectrum theory for proxy-human alignment]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory about alignment between proxy evaluations (LLM-as-a-judge, Likert-style) and expert human review for software development artifacts, including necessary conditions for high agreement.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-205",
    "original_theory_name": "Objectivity-Subjectivity Spectrum Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain Feature Engineering Superiority in Low-Data Regimes Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-397</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-397</p>
                <p><strong>Name:</strong> Domain Feature Engineering Superiority in Low-Data Regimes Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of the relationship between scientific problem characteristics (including data availability, data structure, problem complexity, domain maturity, and mechanistic understanding requirements) and the applicability, effectiveness, and impact potential of different AI methodologies and approaches, based on the following results.</p>
                <p><strong>Description:</strong> In scientific problems with limited labeled data (typically <10,000 examples) but substantial domain knowledge, supervised learning with carefully engineered domain-informed features consistently outperforms end-to-end deep learning approaches that learn representations from scratch. The advantage stems from: (1) reducing effective dimensionality by focusing on relevant aspects, (2) incorporating non-local relationships and symmetries that require extensive data to learn, (3) encoding known physical mechanisms and constraints, and (4) providing interpretability that aids scientific validation. This advantage diminishes as data abundance increases, with crossover points typically between 10,000-100,000 labeled examples depending on problem complexity, domain maturity, and the quality of available domain knowledge. The theory applies most strongly to problems where domain expertise can identify discriminative features that capture the underlying physics or mechanisms.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>When labeled data are limited (<10,000 examples) and domain expertise is available to engineer discriminative features, supervised learning with domain-informed features typically provides better performance than end-to-end deep learning that learns representations from scratch.</li>
                <li>The performance advantage of domain features over learned representations decreases with increasing data quantity, with crossover points typically occurring between 10,000-100,000 labeled examples, varying by problem complexity and domain maturity.</li>
                <li>Domain-engineered features that capture non-local relationships, physical symmetries, conservation laws, or multi-scale phenomena provide the largest advantages over raw inputs, as these patterns require prohibitive amounts of data for networks to learn from scratch.</li>
                <li>The effectiveness of domain feature engineering depends on the quality, completeness, and correctness of domain knowledge: well-understood domains (physics, chemistry) show larger and more persistent benefits than less mature domains.</li>
                <li>Feature engineering reduces effective sample complexity by orders of magnitude by restricting the hypothesis space to physically meaningful relationships, but this advantage is lost when domain knowledge is incomplete or incorrect.</li>
                <li>The interpretability of engineered features provides additional scientific value beyond predictive performance, enabling validation, hypothesis generation, and trust in low-data regimes where model errors are more likely.</li>
                <li>Hybrid approaches that combine learned representations with domain features can sometimes achieve better performance than either alone, particularly in intermediate data regimes (1,000-50,000 examples).</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>SPOCK uses 10 physics-informed features (MEGNO statistics, resonant-mode features, eccentricity combinations) from short 10^4-orbit integrations to predict long-term stability, achieving TPR 85% at FPR 10% on resonant holdout and ~94% on random dataset, outperforming baselines (MEGNO-only, AMD, Hill criteria) that lack such engineered features <a href="../results/extraction-result-2361.html#e2361.0" class="evidence-link">[e2361.0]</a> <a href="../results/extraction-result-2361.html#e2361.1" class="evidence-link">[e2361.1]</a> </li>
    <li>Meta-QSAR with 2,394 engineered meta-features (information-theoretic, dataset statistics, protein descriptors) enables effective algorithm selection across 2,764 QSAR targets, with information-theoretic features being most informative; meta-learning significantly outperformed best individual baseline methods <a href="../results/extraction-result-2362.html#e2362.0" class="evidence-link">[e2362.0]</a> <a href="../results/extraction-result-2362.html#e2362.5" class="evidence-link">[e2362.5]</a> <a href="../results/extraction-result-2362.html#e2362.6" class="evidence-link">[e2362.6]</a> </li>
    <li>Crystallization propensity prediction achieves ~80% accuracy using just two carefully selected parameters on >20,000 examples, demonstrating that compact, well-chosen features can be highly effective <a href="../results/extraction-result-2337.html#e2337.3" class="evidence-link">[e2337.3]</a> </li>
    <li>CPP prediction methods combining sequence composition (AAC) and physicochemical features outperform sequence-only approaches; ensemble methods (RF, SVM, ERT, K-NN) with mixed descriptors improve both classification and uptake efficiency prediction <a href="../results/extraction-result-2329.html#e2329.1" class="evidence-link">[e2329.1]</a> <a href="../results/extraction-result-2329.html#e2329.2" class="evidence-link">[e2329.2]</a> <a href="../results/extraction-result-2329.html#e2329.3" class="evidence-link">[e2329.3]</a> </li>
    <li>Transformative learning in QSAR using extrinsic features (predictions from 2,218 related target models) improved mean RMSE from 0.1643 to 0.1478 (~10% improvement) compared to intrinsic molecular fingerprints, demonstrating value of task-derived features <a href="../results/extraction-result-2272.html#e2272.1" class="evidence-link">[e2272.1]</a> </li>
    <li>XAS neural networks trained on computed spectral features from large simulated datasets achieve unprecedented analysis capability, enabling automated characterization faster than manual interpretation <a href="../results/extraction-result-2323.html#e2323.7" class="evidence-link">[e2323.7]</a> </li>
    <li>Grape leaf disease classification with engineered GLCM texture features + PCA dimensionality reduction + SVM achieves 98.71% accuracy, outperforming simpler approaches <a href="../results/extraction-result-2340.html#e2340.2" class="evidence-link">[e2340.2]</a> </li>
    <li>GNNs for molecular property prediction underperform expert descriptors in low-data regimes but scale better with abundant data (>~10k examples), with the review noting that datasets >~10k examples favor generative/large models <a href="../results/extraction-result-2296.html#e2296.0" class="evidence-link">[e2296.0]</a> </li>
    <li>MD-EXAFS training uses simulation-augmented supervised learning where physics-based forward models generate labeled training data (3000 examples), enabling NN inference in seconds with accurate RDF reconstruction when experimental labels are scarce <a href="../results/extraction-result-2347.html#e2347.1" class="evidence-link">[e2347.1]</a> </li>
    <li>MLPQNA photometric redshift estimation with careful feature selection and quasi-Newton optimization outperformed competing methods on PHAT-1 benchmark, demonstrating importance of feature engineering and algorithm choice <a href="../results/extraction-result-2328.html#e2328.0" class="evidence-link">[e2328.0]</a> </li>
    <li>RA synovial subtyping using >450 engineered protein descriptors plus histologic features enabled meta-learning to predict algorithm performance, though limited by small sample size (45 genomic samples) <a href="../results/extraction-result-2298.html#e2298.0" class="evidence-link">[e2298.0]</a> </li>
    <li>Solar flare forecasting comparisons show dataset-dependent algorithm rankings (k-NN, SVM, RF), with feature engineering and dataset construction critically affecting performance <a href="../results/extraction-result-2328.html#e2328.3" class="evidence-link">[e2328.3]</a> </li>
    <li>Symbolic regression and SINDy methods excel at yielding interpretable mechanistic models from moderate-noise, well-excited dynamical data by using sparse selection over engineered function libraries <a href="../results/extraction-result-2321.html#e2321.6" class="evidence-link">[e2321.6]</a> <a href="../results/extraction-result-2350.html#e2350.2" class="evidence-link">[e2350.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A materials property prediction task with 5,000 labeled examples will achieve higher accuracy using engineered composition-based descriptors and structural motifs than a graph neural network trained end-to-end on crystal structures, but the GNN will surpass it beyond ~50,000 examples.</li>
                <li>In drug discovery, a QSAR model with 1,000 labeled compounds using engineered molecular descriptors (fingerprints, physicochemical properties) will outperform a graph neural network by 10-20% in predictive accuracy, but the GNN will match or exceed it when data exceed 50,000 compounds.</li>
                <li>For a new protein function prediction task with 500 labeled examples, features based on sequence motifs, domain annotations, and physicochemical properties will outperform a transformer model trained from scratch on sequences by at least 15% in classification accuracy.</li>
                <li>In climate modeling with 2,000 labeled simulation outputs, a model using engineered features based on physical principles (energy balance, conservation laws) will outperform a pure CNN by 20-30% in prediction error.</li>
                <li>For a spectroscopy inverse problem with 3,000 labeled spectra, engineered features based on peak positions, widths, and ratios will enable better structure prediction than an end-to-end neural network.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether automated feature learning methods (e.g., self-supervised pretraining, meta-learning across related tasks) can match or exceed human expert feature engineering in low-data regimes (<1,000 examples), or if human domain knowledge provides irreplaceable inductive biases that cannot be learned from data alone.</li>
                <li>The exact functional form of the data-quantity crossover curve where deep learning surpasses feature engineering, whether it follows a universal scaling law (e.g., power law) across scientific domains, or if it is fundamentally domain-specific.</li>
                <li>Whether foundation models pretrained on large unlabeled scientific corpora can overcome the low-data limitations of end-to-end learning and match or exceed expert feature engineering with minimal fine-tuning (<100 examples), or if they still require substantial labeled data.</li>
                <li>If there exists an optimal hybrid architecture that can automatically learn when to rely on domain features versus learned representations as a function of available data, or if this trade-off must be manually tuned per problem.</li>
                <li>Whether the interpretability advantage of engineered features provides measurable scientific value (e.g., faster hypothesis generation, fewer experimental failures) beyond predictive performance, and if this value can be quantified.</li>
                <li>If active learning strategies that leverage domain features for sample selection can reduce the crossover point by an order of magnitude (e.g., from 50,000 to 5,000 examples) by strategically acquiring the most informative labels.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding scientific problems where end-to-end deep learning consistently outperforms domain feature engineering even with <1,000 labeled examples across multiple independent datasets would challenge the theory's core premise.</li>
                <li>Demonstrating that automatically learned features from self-supervised pretraining on unlabeled scientific data match or exceed expert features in low-data regimes (<5,000 examples) across multiple domains would weaken the necessity of domain expertise.</li>
                <li>Showing that the crossover point occurs at much lower data quantities (<1,000 examples) across multiple well-characterized scientific domains would challenge the stated thresholds and suggest the theory overestimates the data requirements of deep learning.</li>
                <li>Finding cases where incorrect or incomplete domain knowledge leads to engineered features that perform worse than learned representations even in very low-data regimes (<500 examples) would highlight critical failure modes.</li>
                <li>Demonstrating that hybrid approaches combining domain features with learned representations consistently underperform pure domain feature approaches in low-data regimes would challenge the theory's prediction about hybrid methods.</li>
                <li>Showing that the interpretability advantage of domain features does not translate to measurable scientific benefits (e.g., faster discovery, better generalization to new conditions) would weaken the theory's claims about scientific value beyond prediction.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to optimally combine domain features with learned representations in hybrid approaches, or provide principles for when each should be weighted more heavily </li>
    <li>The role of data quality, noise levels, label reliability, and measurement error in determining the crossover point is not fully characterized; high-quality small datasets may favor features while noisy large datasets may favor robust learned representations </li>
    <li>How transfer learning, multi-task learning, and meta-learning affect the data requirements for deep learning approaches and potentially shift the crossover point <a href="../results/extraction-result-2272.html#e2272.1" class="evidence-link">[e2272.1]</a> <a href="../results/extraction-result-2362.html#e2362.0" class="evidence-link">[e2362.0]</a> </li>
    <li>The computational cost trade-offs between feature engineering (often cheap inference, expensive design) and deep learning (expensive training, cheap inference) and how this affects practical applicability </li>
    <li>How problem complexity (dimensionality, non-linearity, number of relevant features) quantitatively affects the crossover point; more complex problems may favor learned representations even with less data </li>
    <li>The role of multi-fidelity data (abundant cheap low-fidelity data plus scarce expensive high-fidelity labels) in changing the dynamics of the feature engineering vs. deep learning trade-off <a href="../results/extraction-result-2313.html#e2313.6" class="evidence-link">[e2313.6]</a> </li>
    <li>How active learning and strategic data acquisition strategies interact with the choice between engineered and learned features, and whether they can reduce data requirements differently for each approach <a href="../results/extraction-result-2336.html#e2336.12" class="evidence-link">[e2336.12]</a> <a href="../results/extraction-result-2325.html#e2325.6" class="evidence-link">[e2325.6]</a> </li>
    <li>The theory does not address how the availability of related unlabeled data (for self-supervised pretraining) affects the comparison, as this can dramatically improve learned representations without requiring more labels </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Domingos (2012) A few useful things to know about machine learning [Discusses feature engineering importance, sample complexity, and the bias-variance tradeoff in machine learning]</li>
    <li>Bengio et al. (2013) Representation learning: A review and new perspectives [Comprehensive review of when learned vs. engineered features are preferable, discussing the trade-offs and data requirements]</li>
    <li>Abu-Mostafa (1995) Hints [Theoretical framework for incorporating domain knowledge into learning, showing how prior knowledge reduces sample complexity]</li>
    <li>Mitchell (1980) The need for biases in learning generalizations [Foundational work on how inductive biases, including feature engineering, enable learning from limited data]</li>
    <li>Vapnik (1998) Statistical Learning Theory [Theoretical framework for sample complexity and how restricting hypothesis space (via feature engineering) reduces required samples]</li>
    <li>Caruana (1997) Multitask Learning [Shows how related tasks and shared representations affect sample complexity, relevant to transfer learning aspects]</li>
    <li>Sculley et al. (2015) Hidden Technical Debt in Machine Learning Systems [Discusses practical trade-offs between feature engineering and learned representations in production systems]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain Feature Engineering Superiority in Low-Data Regimes Theory",
    "theory_description": "In scientific problems with limited labeled data (typically &lt;10,000 examples) but substantial domain knowledge, supervised learning with carefully engineered domain-informed features consistently outperforms end-to-end deep learning approaches that learn representations from scratch. The advantage stems from: (1) reducing effective dimensionality by focusing on relevant aspects, (2) incorporating non-local relationships and symmetries that require extensive data to learn, (3) encoding known physical mechanisms and constraints, and (4) providing interpretability that aids scientific validation. This advantage diminishes as data abundance increases, with crossover points typically between 10,000-100,000 labeled examples depending on problem complexity, domain maturity, and the quality of available domain knowledge. The theory applies most strongly to problems where domain expertise can identify discriminative features that capture the underlying physics or mechanisms.",
    "supporting_evidence": [
        {
            "text": "SPOCK uses 10 physics-informed features (MEGNO statistics, resonant-mode features, eccentricity combinations) from short 10^4-orbit integrations to predict long-term stability, achieving TPR 85% at FPR 10% on resonant holdout and ~94% on random dataset, outperforming baselines (MEGNO-only, AMD, Hill criteria) that lack such engineered features",
            "uuids": [
                "e2361.0",
                "e2361.1"
            ]
        },
        {
            "text": "Meta-QSAR with 2,394 engineered meta-features (information-theoretic, dataset statistics, protein descriptors) enables effective algorithm selection across 2,764 QSAR targets, with information-theoretic features being most informative; meta-learning significantly outperformed best individual baseline methods",
            "uuids": [
                "e2362.0",
                "e2362.5",
                "e2362.6"
            ]
        },
        {
            "text": "Crystallization propensity prediction achieves ~80% accuracy using just two carefully selected parameters on &gt;20,000 examples, demonstrating that compact, well-chosen features can be highly effective",
            "uuids": [
                "e2337.3"
            ]
        },
        {
            "text": "CPP prediction methods combining sequence composition (AAC) and physicochemical features outperform sequence-only approaches; ensemble methods (RF, SVM, ERT, K-NN) with mixed descriptors improve both classification and uptake efficiency prediction",
            "uuids": [
                "e2329.1",
                "e2329.2",
                "e2329.3"
            ]
        },
        {
            "text": "Transformative learning in QSAR using extrinsic features (predictions from 2,218 related target models) improved mean RMSE from 0.1643 to 0.1478 (~10% improvement) compared to intrinsic molecular fingerprints, demonstrating value of task-derived features",
            "uuids": [
                "e2272.1"
            ]
        },
        {
            "text": "XAS neural networks trained on computed spectral features from large simulated datasets achieve unprecedented analysis capability, enabling automated characterization faster than manual interpretation",
            "uuids": [
                "e2323.7"
            ]
        },
        {
            "text": "Grape leaf disease classification with engineered GLCM texture features + PCA dimensionality reduction + SVM achieves 98.71% accuracy, outperforming simpler approaches",
            "uuids": [
                "e2340.2"
            ]
        },
        {
            "text": "GNNs for molecular property prediction underperform expert descriptors in low-data regimes but scale better with abundant data (&gt;~10k examples), with the review noting that datasets &gt;~10k examples favor generative/large models",
            "uuids": [
                "e2296.0"
            ]
        },
        {
            "text": "MD-EXAFS training uses simulation-augmented supervised learning where physics-based forward models generate labeled training data (3000 examples), enabling NN inference in seconds with accurate RDF reconstruction when experimental labels are scarce",
            "uuids": [
                "e2347.1"
            ]
        },
        {
            "text": "MLPQNA photometric redshift estimation with careful feature selection and quasi-Newton optimization outperformed competing methods on PHAT-1 benchmark, demonstrating importance of feature engineering and algorithm choice",
            "uuids": [
                "e2328.0"
            ]
        },
        {
            "text": "RA synovial subtyping using &gt;450 engineered protein descriptors plus histologic features enabled meta-learning to predict algorithm performance, though limited by small sample size (45 genomic samples)",
            "uuids": [
                "e2298.0"
            ]
        },
        {
            "text": "Solar flare forecasting comparisons show dataset-dependent algorithm rankings (k-NN, SVM, RF), with feature engineering and dataset construction critically affecting performance",
            "uuids": [
                "e2328.3"
            ]
        },
        {
            "text": "Symbolic regression and SINDy methods excel at yielding interpretable mechanistic models from moderate-noise, well-excited dynamical data by using sparse selection over engineered function libraries",
            "uuids": [
                "e2321.6",
                "e2350.2"
            ]
        }
    ],
    "theory_statements": [
        "When labeled data are limited (&lt;10,000 examples) and domain expertise is available to engineer discriminative features, supervised learning with domain-informed features typically provides better performance than end-to-end deep learning that learns representations from scratch.",
        "The performance advantage of domain features over learned representations decreases with increasing data quantity, with crossover points typically occurring between 10,000-100,000 labeled examples, varying by problem complexity and domain maturity.",
        "Domain-engineered features that capture non-local relationships, physical symmetries, conservation laws, or multi-scale phenomena provide the largest advantages over raw inputs, as these patterns require prohibitive amounts of data for networks to learn from scratch.",
        "The effectiveness of domain feature engineering depends on the quality, completeness, and correctness of domain knowledge: well-understood domains (physics, chemistry) show larger and more persistent benefits than less mature domains.",
        "Feature engineering reduces effective sample complexity by orders of magnitude by restricting the hypothesis space to physically meaningful relationships, but this advantage is lost when domain knowledge is incomplete or incorrect.",
        "The interpretability of engineered features provides additional scientific value beyond predictive performance, enabling validation, hypothesis generation, and trust in low-data regimes where model errors are more likely.",
        "Hybrid approaches that combine learned representations with domain features can sometimes achieve better performance than either alone, particularly in intermediate data regimes (1,000-50,000 examples)."
    ],
    "new_predictions_likely": [
        "A materials property prediction task with 5,000 labeled examples will achieve higher accuracy using engineered composition-based descriptors and structural motifs than a graph neural network trained end-to-end on crystal structures, but the GNN will surpass it beyond ~50,000 examples.",
        "In drug discovery, a QSAR model with 1,000 labeled compounds using engineered molecular descriptors (fingerprints, physicochemical properties) will outperform a graph neural network by 10-20% in predictive accuracy, but the GNN will match or exceed it when data exceed 50,000 compounds.",
        "For a new protein function prediction task with 500 labeled examples, features based on sequence motifs, domain annotations, and physicochemical properties will outperform a transformer model trained from scratch on sequences by at least 15% in classification accuracy.",
        "In climate modeling with 2,000 labeled simulation outputs, a model using engineered features based on physical principles (energy balance, conservation laws) will outperform a pure CNN by 20-30% in prediction error.",
        "For a spectroscopy inverse problem with 3,000 labeled spectra, engineered features based on peak positions, widths, and ratios will enable better structure prediction than an end-to-end neural network."
    ],
    "new_predictions_unknown": [
        "Whether automated feature learning methods (e.g., self-supervised pretraining, meta-learning across related tasks) can match or exceed human expert feature engineering in low-data regimes (&lt;1,000 examples), or if human domain knowledge provides irreplaceable inductive biases that cannot be learned from data alone.",
        "The exact functional form of the data-quantity crossover curve where deep learning surpasses feature engineering, whether it follows a universal scaling law (e.g., power law) across scientific domains, or if it is fundamentally domain-specific.",
        "Whether foundation models pretrained on large unlabeled scientific corpora can overcome the low-data limitations of end-to-end learning and match or exceed expert feature engineering with minimal fine-tuning (&lt;100 examples), or if they still require substantial labeled data.",
        "If there exists an optimal hybrid architecture that can automatically learn when to rely on domain features versus learned representations as a function of available data, or if this trade-off must be manually tuned per problem.",
        "Whether the interpretability advantage of engineered features provides measurable scientific value (e.g., faster hypothesis generation, fewer experimental failures) beyond predictive performance, and if this value can be quantified.",
        "If active learning strategies that leverage domain features for sample selection can reduce the crossover point by an order of magnitude (e.g., from 50,000 to 5,000 examples) by strategically acquiring the most informative labels."
    ],
    "negative_experiments": [
        "Finding scientific problems where end-to-end deep learning consistently outperforms domain feature engineering even with &lt;1,000 labeled examples across multiple independent datasets would challenge the theory's core premise.",
        "Demonstrating that automatically learned features from self-supervised pretraining on unlabeled scientific data match or exceed expert features in low-data regimes (&lt;5,000 examples) across multiple domains would weaken the necessity of domain expertise.",
        "Showing that the crossover point occurs at much lower data quantities (&lt;1,000 examples) across multiple well-characterized scientific domains would challenge the stated thresholds and suggest the theory overestimates the data requirements of deep learning.",
        "Finding cases where incorrect or incomplete domain knowledge leads to engineered features that perform worse than learned representations even in very low-data regimes (&lt;500 examples) would highlight critical failure modes.",
        "Demonstrating that hybrid approaches combining domain features with learned representations consistently underperform pure domain feature approaches in low-data regimes would challenge the theory's prediction about hybrid methods.",
        "Showing that the interpretability advantage of domain features does not translate to measurable scientific benefits (e.g., faster discovery, better generalization to new conditions) would weaken the theory's claims about scientific value beyond prediction."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to optimally combine domain features with learned representations in hybrid approaches, or provide principles for when each should be weighted more heavily",
            "uuids": []
        },
        {
            "text": "The role of data quality, noise levels, label reliability, and measurement error in determining the crossover point is not fully characterized; high-quality small datasets may favor features while noisy large datasets may favor robust learned representations",
            "uuids": []
        },
        {
            "text": "How transfer learning, multi-task learning, and meta-learning affect the data requirements for deep learning approaches and potentially shift the crossover point",
            "uuids": [
                "e2272.1",
                "e2362.0"
            ]
        },
        {
            "text": "The computational cost trade-offs between feature engineering (often cheap inference, expensive design) and deep learning (expensive training, cheap inference) and how this affects practical applicability",
            "uuids": []
        },
        {
            "text": "How problem complexity (dimensionality, non-linearity, number of relevant features) quantitatively affects the crossover point; more complex problems may favor learned representations even with less data",
            "uuids": []
        },
        {
            "text": "The role of multi-fidelity data (abundant cheap low-fidelity data plus scarce expensive high-fidelity labels) in changing the dynamics of the feature engineering vs. deep learning trade-off",
            "uuids": [
                "e2313.6"
            ]
        },
        {
            "text": "How active learning and strategic data acquisition strategies interact with the choice between engineered and learned features, and whether they can reduce data requirements differently for each approach",
            "uuids": [
                "e2336.12",
                "e2325.6"
            ]
        },
        {
            "text": "The theory does not address how the availability of related unlabeled data (for self-supervised pretraining) affects the comparison, as this can dramatically improve learned representations without requiring more labels",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "AlphaFold2 achieves highly accurate protein structure prediction using end-to-end deep learning with geometric/equivariant inductive biases, trained on ~100k structures, suggesting that appropriate architectural inductive biases can work well even with moderate data when the architecture encodes strong domain priors",
            "uuids": [
                "e2308.0",
                "e2308.2"
            ]
        },
        {
            "text": "Foundation models and LLMs can achieve good performance with few-shot learning (tens to hundreds of examples), potentially bypassing the need for extensive labeled data by leveraging massive pretraining on unlabeled corpora",
            "uuids": [
                "e2321.7"
            ]
        },
        {
            "text": "Topaz (PU-CNN) for particle picking works well with very limited positive labels (Nâ‰¤250) by treating unlabeled data appropriately, achieving 1.72x more particles than manual picks, suggesting that appropriate learning frameworks can overcome label scarcity",
            "uuids": [
                "e2323.2",
                "e2352.3"
            ]
        },
        {
            "text": "DAgger and iterative imitation learning methods can reduce compounding error and improve generalization with limited demonstrations by actively collecting corrective labels, suggesting that strategic data collection can overcome low-data limitations",
            "uuids": [
                "e2360.2"
            ]
        },
        {
            "text": "WSI Cancer Detection using MIL achieves good performance with limited pixel-level labels by using weakly-supervised learning on slide-level labels, demonstrating that appropriate problem formulation can reduce labeling requirements",
            "uuids": [
                "e2342.5"
            ]
        },
        {
            "text": "Neural force fields trained on quantum chemistry data can achieve near-ab initio accuracy and enable practical molecular dynamics, with success rates improving from ~1% (2020) to ~14% (2023) on Open Catalyst benchmark, suggesting rapid progress in learned representations",
            "uuids": [
                "e2296.5"
            ]
        },
        {
            "text": "RWKV-6 (non-Transformer architecture) failed in knowledge distillation experiments where Transformer teachers succeeded, suggesting that architectural choices and representation geometry matter more than just feature engineering vs. learning distinction",
            "uuids": [
                "e2294.4"
            ]
        }
    ],
    "special_cases": [
        "When domain knowledge is incomplete, incorrect, or biased, learned features may outperform engineered features even in low-data regimes, as the model can discover patterns that experts missed or misunderstood.",
        "For problems with very high-dimensional raw inputs (e.g., high-resolution images, long sequences, 3D structures), some level of learned feature extraction may be necessary even when domain features are available, suggesting hybrid approaches are optimal.",
        "The crossover point varies significantly by domain maturity: well-characterized domains (classical physics, organic chemistry) show later crossovers (50,000-100,000 examples) than less mature domains (systems biology, social sciences) where domain knowledge is less reliable (10,000-20,000 examples).",
        "In multi-fidelity scenarios where abundant cheap low-fidelity data can augment scarce high-fidelity labels, the crossover point may shift dramatically, as learned representations can leverage the additional data while engineered features may not benefit as much.",
        "When transfer learning or pretraining on related tasks is available, deep learning approaches may match or exceed engineered features with far fewer labeled examples (potentially &lt;1,000) by leveraging learned representations from related domains.",
        "For problems requiring real-time inference or deployment on resource-constrained hardware, engineered features with simple models may be preferred even when deep learning would provide better accuracy, due to computational cost trade-offs.",
        "In safety-critical or high-stakes scientific applications, the interpretability advantage of engineered features may outweigh modest performance gains from learned representations, even in high-data regimes.",
        "Active learning strategies that strategically select which examples to label can reduce the effective crossover point by focusing labeling effort on the most informative examples, potentially reducing required labels by 5-10x for both approaches but with different optimal selection strategies."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Domingos (2012) A few useful things to know about machine learning [Discusses feature engineering importance, sample complexity, and the bias-variance tradeoff in machine learning]",
            "Bengio et al. (2013) Representation learning: A review and new perspectives [Comprehensive review of when learned vs. engineered features are preferable, discussing the trade-offs and data requirements]",
            "Abu-Mostafa (1995) Hints [Theoretical framework for incorporating domain knowledge into learning, showing how prior knowledge reduces sample complexity]",
            "Mitchell (1980) The need for biases in learning generalizations [Foundational work on how inductive biases, including feature engineering, enable learning from limited data]",
            "Vapnik (1998) Statistical Learning Theory [Theoretical framework for sample complexity and how restricting hypothesis space (via feature engineering) reduces required samples]",
            "Caruana (1997) Multitask Learning [Shows how related tasks and shared representations affect sample complexity, relevant to transfer learning aspects]",
            "Sculley et al. (2015) Hidden Technical Debt in Machine Learning Systems [Discusses practical trade-offs between feature engineering and learned representations in production systems]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
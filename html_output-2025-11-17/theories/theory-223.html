<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Grounding-by-Proxy Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-223</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-223</p>
                <p><strong>Name:</strong> Grounding-by-Proxy Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models encode and utilize spatial, procedural, and object-relational knowledge for embodied planning tasks without direct sensory input.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> Language models develop functional representations of spatial, procedural, and object-relational knowledge by extracting and reconstructing embodied information from linguistic proxies—statistical patterns in text that systematically correlate with physical world properties. Rather than grounding through direct sensory experience, LMs build implicit world models by learning how humans linguistically encode spatial relationships (through prepositions, spatial verbs, and locative expressions), procedural sequences (through instructional and narrative structures), and object properties (through descriptive and relational language). These proxy representations enable embodied planning by mapping linguistic patterns to latent spatial-relational structures that approximate the constraints and affordances of the physical world. The theory posits that this proxy grounding operates through distributional semantics, where co-occurrence patterns and syntactic structures in language serve as compressed encodings of physical world regularities. However, this mechanism introduces systematic biases toward frequently described, linguistically salient scenarios, and may fail to capture physical constraints that are rarely explicitly stated in text (such as implicit physical laws or edge cases).</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models extract spatial knowledge by learning statistical associations between spatial prepositions (e.g., 'on', 'in', 'above'), verbs of motion (e.g., 'move', 'place', 'rotate'), and object configurations as described in text, building distributional representations that capture topological and approximate metric spatial relationships.</li>
                <li>Procedural knowledge is encoded through the sequential structure of instructional texts, narrative descriptions of actions, and causal language patterns (e.g., 'first', 'then', 'after', 'because'), with temporal and causal connectives serving as proxies for procedural dependencies.</li>
                <li>Object-relational knowledge emerges from co-occurrence patterns of objects with their properties, typical locations, functional relationships, and affordances in linguistic contexts, creating implicit object schemas.</li>
                <li>The quality and accuracy of proxy grounding is proportional to the frequency, consistency, detail, and diversity with which physical relationships are described in the training corpus, following a power-law relationship where common scenarios are well-represented and rare scenarios are poorly grounded.</li>
                <li>LMs develop latent spatial representations that approximate topological properties (connectivity, containment, adjacency) more reliably than metric properties (exact distances, angles, sizes) due to the nature of spatial language, which more frequently encodes qualitative rather than quantitative spatial information.</li>
                <li>Systematic errors in embodied reasoning arise when: (1) linguistic descriptions are sparse or absent for certain physical configurations, (2) linguistic conventions conflict with physical reality, (3) implicit physical constraints are not explicitly stated in text, or (4) linguistic ambiguity obscures physical relationships.</li>
                <li>The proxy grounding mechanism enables zero-shot transfer to embodied tasks by mapping task descriptions to learned linguistic-spatial associations, with performance degrading gracefully as task descriptions deviate from training distribution patterns.</li>
                <li>Multi-hop reasoning about physical scenarios is performed by chaining linguistic associations that mirror causal and spatial relationships in the physical world, with accuracy decreasing as a function of chain length due to error propagation and the probabilistic nature of linguistic associations.</li>
                <li>Proxy grounding creates representations that are functionally adequate for many embodied planning tasks despite lacking direct perceptual grounding, because linguistic descriptions systematically encode the most task-relevant physical constraints and relationships.</li>
                <li>The mechanism operates through transformer attention patterns that learn to associate spatial language with consistent relational structures across contexts, effectively compressing physical world regularities into linguistic distributional patterns.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Large language models demonstrate ability to perform spatial reasoning tasks such as navigation and object placement without explicit spatial training, suggesting implicit spatial representations learned from language. </li>
    <li>Language models can generate coherent step-by-step procedural instructions for physical tasks, indicating extraction of procedural knowledge from textual descriptions. </li>
    <li>LMs show systematic understanding of object properties and physical constraints (e.g., containment, support, size relations) that align with physical world constraints, performing above chance on physical commonsense reasoning benchmarks. </li>
    <li>Probing studies reveal that transformer models develop spatial representations in their hidden states when processing spatial language, even without explicit spatial supervision, with linear probes able to decode spatial relationships from intermediate representations. </li>
    <li>Language models exhibit performance patterns consistent with learning from linguistic frequency rather than physical experience, showing better performance on commonly described scenarios and systematic failures on rare but physically valid configurations. </li>
    <li>LMs demonstrate knowledge of spatial prepositions and their meanings, with representations that capture topological and some metric properties of spatial relations. </li>
    <li>Language models can perform multi-step reasoning about physical scenarios by chaining linguistic associations, though with degrading accuracy as reasoning chains lengthen. </li>
    <li>Training corpus statistics correlate with LM performance on spatial and physical reasoning tasks, with better performance on scenarios that are more frequently described in web text. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs should perform better on spatial reasoning tasks involving commonly described spatial configurations (e.g., 'cup on table') compared to rare but physically valid configurations (e.g., 'table on cup' when the table is miniature), with performance differences correlating with log-frequency ratios in training corpora.</li>
                <li>Fine-tuning on synthetic spatial descriptions with systematic coverage of spatial relationships should improve embodied planning performance more efficiently than equivalent amounts of natural text, with improvements of 20-40% on spatial reasoning benchmarks using 10x less data.</li>
                <li>LMs should show better procedural knowledge for culturally common tasks (cooking, cleaning) compared to rare specialized procedures (industrial processes, niche crafts) even when both are physically similar in complexity, with accuracy differences of 30-50% on procedural generation tasks.</li>
                <li>Probing classifiers should be able to decode approximate spatial coordinates and object arrangements from LM hidden states when processing spatial descriptions, achieving above 70% accuracy for topological relations and 40-60% accuracy for metric relations.</li>
                <li>LMs should exhibit systematic confusions between objects that are linguistically similar but physically distinct (e.g., confusing 'light' objects with objects that emit light) more frequently than between physically similar but linguistically distinct objects, with confusion rates 2-3x higher for linguistic similarity.</li>
                <li>Performance on spatial reasoning should correlate with the diversity of spatial contexts in which objects appear in training data, not just frequency, with diversity explaining an additional 15-25% of variance beyond frequency alone.</li>
                <li>LMs should show better understanding of functional spatial relationships (e.g., 'keys go in locks') than arbitrary spatial relationships (e.g., 'keys are typically 3 inches from wallets'), with 40-60% higher accuracy on functional relations.</li>
                <li>Augmenting prompts with explicit spatial language (prepositions, locative phrases) should improve embodied planning performance by 15-30% compared to prompts without such language, demonstrating the importance of linguistic spatial markers.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If trained on a corpus where spatial descriptions systematically violate physical laws (e.g., objects routinely float, gravity works differently), LMs might develop alternative 'physics' that could still enable coherent planning within that alternative framework—testing whether proxy grounding is purely statistical or incorporates invariant physical principles that resist contradictory linguistic evidence.</li>
                <li>Augmenting language model training with synthetic 'spatial language' that explicitly encodes metric information (coordinates, distances, angles) in a systematic format might either dramatically improve spatial reasoning (supporting pure proxy grounding) or might be ignored in favor of natural language patterns (suggesting architectural or inductive biases toward natural linguistic structure)—revealing fundamental constraints on what linguistic proxies can encode.</li>
                <li>LMs might be able to perform embodied planning in described environments with non-Euclidean spatial properties (e.g., spaces with portals, non-transitive spatial relations, hyperbolic geometry) if sufficiently described, which would demonstrate the flexibility of proxy-grounded spatial representations versus their dependence on implicit Euclidean assumptions from training data.</li>
                <li>Adversarial examples that maintain linguistic plausibility while violating physical constraints might be systematically undetectable to LMs (supporting pure linguistic grounding) or LMs might develop robustness through implicit physical consistency checking learned from text (supporting emergent physical reasoning)—with major implications for reliability in embodied applications.</li>
                <li>Cross-lingual transfer of spatial reasoning abilities might reveal whether proxy grounding is language-specific (suggesting surface-level linguistic pattern matching) or develops language-independent spatial representations (suggesting deeper abstraction), with implications for the universality and depth of spatial cognition in LMs.</li>
                <li>Scaling model size might show threshold effects where proxy grounding suddenly becomes more robust or physically consistent at certain scales, suggesting emergent integration of physical constraints, or might show smooth scaling, suggesting purely statistical accumulation of linguistic patterns.</li>
                <li>Combining LMs with minimal sensory feedback (e.g., success/failure signals on embodied tasks) might show rapid improvement (suggesting proxy grounding provides good priors) or minimal improvement (suggesting proxy grounding and sensory grounding are fundamentally different), with implications for efficient robot learning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs perform equally well on spatial reasoning tasks regardless of the frequency of spatial configurations in training data (controlling for task difficulty), this would challenge the claim that proxy grounding depends on linguistic frequency and suggest alternative mechanisms like emergent physical reasoning.</li>
                <li>If removing all spatial prepositions and locative expressions from training data (while preserving other linguistic structure) does not significantly impair spatial reasoning abilities, this would question whether linguistic spatial markers are the primary source of proxy grounding and suggest more implicit encoding mechanisms.</li>
                <li>If LMs cannot improve on embodied planning tasks when given explicit coordinate-based spatial descriptions (which should enhance proxy information if the theory is correct), this would suggest fundamental limitations in how proxy grounding utilizes spatial information or architectural constraints on processing explicit spatial data.</li>
                <li>If LMs show no systematic differences in performance between physically possible but rarely described scenarios versus frequently described scenarios (when controlling for linguistic complexity), this would challenge the linguistic frequency dependence of proxy grounding.</li>
                <li>If artificially inserting false spatial relationships into training data (e.g., 'birds typically swim underwater', 'tables are usually on top of cups') does not affect LM spatial reasoning about those objects in downstream tasks, this would question whether LMs actually utilize the linguistic patterns proposed by the theory or rely on other knowledge sources.</li>
                <li>If LMs trained exclusively on fictional texts with different physical laws (e.g., fantasy or science fiction) perform equally well on real-world spatial reasoning tasks, this would challenge the claim that proxy grounding learns from correlations with actual physical world properties.</li>
                <li>If probing studies fail to find spatial representations in LM hidden states that correlate with spatial language processing, this would undermine the claim that LMs develop latent spatial-relational structures.</li>
                <li>If fine-tuning on synthetic spatial descriptions with systematic coverage does not improve performance more than natural text (when controlling for data quantity), this would challenge the claim that systematic linguistic coverage is key to proxy grounding quality.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact computational mechanisms by which transformer architectures convert sequential linguistic patterns into spatial-relational representations remain unclear, including which attention heads and layers are responsible for spatial processing. </li>
    <li>The role of model scale in developing proxy grounding capabilities shows emergent properties that are not fully explained by simple scaling of linguistic pattern extraction, with qualitative changes in spatial reasoning abilities at certain scale thresholds. </li>
    <li>How LMs handle novel combinations of known spatial relations and objects (compositional generalization) goes beyond simple statistical association and may involve systematic compositional mechanisms not fully explained by distributional semantics alone. </li>
    <li>The theory does not fully explain how LMs integrate multiple types of knowledge (spatial, procedural, object-relational) simultaneously during complex embodied planning tasks, or how conflicts between different knowledge types are resolved. </li>
    <li>The relationship between proxy grounding in language-only models versus multimodal models that have access to both linguistic and visual information is not addressed, leaving open questions about whether linguistic proxies are sufficient or complementary to perceptual grounding. </li>
    <li>The theory does not fully account for how LMs handle implicit physical constraints that are rarely explicitly stated in text (e.g., conservation laws, momentum, friction) but may be implicitly present in descriptions of outcomes. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Bender & Koller (2020) Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data [Discusses the symbol grounding problem in LMs and argues they learn form without meaning, but doesn't propose a specific mechanism for how linguistic patterns could serve as proxies for grounding]</li>
    <li>Bisk et al. (2020) Experience Grounds Language [Argues for the importance of embodied experience for language understanding but doesn't detail how LMs might approximate this through linguistic proxies or develop functional representations without direct experience]</li>
    <li>Patel & Pavlick (2022) Mapping Language Models to Grounded Conceptual Spaces [Empirically demonstrates that LMs develop spatial representations but doesn't propose a comprehensive theory of proxy grounding or explain the mechanisms by which linguistic patterns encode physical world properties]</li>
    <li>Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Demonstrates LM capabilities in embodied tasks but focuses on integration with robotic systems and real-world grounding rather than internal representation mechanisms or proxy grounding]</li>
    <li>Harnad (1990) The Symbol Grounding Problem [Classic work on symbol grounding that identifies the problem but doesn't propose linguistic proxies as a solution]</li>
    <li>Louwerse (2011) Symbol Interdependency Hypothesis [Proposes that linguistic symbols can be grounded in their relationships to other symbols, related but doesn't specifically address embodied knowledge or the mechanisms proposed here]</li>
    <li>Lynott & Connell (2010) Embodied Conceptual Combination [Discusses how linguistic descriptions encode perceptual information but doesn't extend to spatial/procedural knowledge or LM mechanisms]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Grounding-by-Proxy Theory",
    "theory_description": "Language models develop functional representations of spatial, procedural, and object-relational knowledge by extracting and reconstructing embodied information from linguistic proxies—statistical patterns in text that systematically correlate with physical world properties. Rather than grounding through direct sensory experience, LMs build implicit world models by learning how humans linguistically encode spatial relationships (through prepositions, spatial verbs, and locative expressions), procedural sequences (through instructional and narrative structures), and object properties (through descriptive and relational language). These proxy representations enable embodied planning by mapping linguistic patterns to latent spatial-relational structures that approximate the constraints and affordances of the physical world. The theory posits that this proxy grounding operates through distributional semantics, where co-occurrence patterns and syntactic structures in language serve as compressed encodings of physical world regularities. However, this mechanism introduces systematic biases toward frequently described, linguistically salient scenarios, and may fail to capture physical constraints that are rarely explicitly stated in text (such as implicit physical laws or edge cases).",
    "supporting_evidence": [
        {
            "text": "Large language models demonstrate ability to perform spatial reasoning tasks such as navigation and object placement without explicit spatial training, suggesting implicit spatial representations learned from language.",
            "citations": [
                "Patel et al. (2021) Mapping Language to Programs for Embodied AI",
                "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "Huang et al. (2023) Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents"
            ]
        },
        {
            "text": "Language models can generate coherent step-by-step procedural instructions for physical tasks, indicating extraction of procedural knowledge from textual descriptions.",
            "citations": [
                "Huang et al. (2022) Language Models as Zero-Shot Planners",
                "Singh et al. (2022) Progprompt: Generating Situated Robot Task Plans using Large Language Models",
                "Liang et al. (2023) Code as Policies: Language Model Programs for Embodied Control"
            ]
        },
        {
            "text": "LMs show systematic understanding of object properties and physical constraints (e.g., containment, support, size relations) that align with physical world constraints, performing above chance on physical commonsense reasoning benchmarks.",
            "citations": [
                "Bisk et al. (2020) PIQA: Reasoning about Physical Commonsense in Natural Language",
                "Talmor et al. (2019) CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
                "Sap et al. (2019) Social IQa: Commonsense Reasoning about Social Interactions"
            ]
        },
        {
            "text": "Probing studies reveal that transformer models develop spatial representations in their hidden states when processing spatial language, even without explicit spatial supervision, with linear probes able to decode spatial relationships from intermediate representations.",
            "citations": [
                "Patel & Pavlick (2022) Mapping Language Models to Grounded Conceptual Spaces",
                "Mirzaee & Kordjamshidi (2022) Transfer Learning with Synthetic Corpora for Spatial Role Labeling",
                "Abdou et al. (2021) Can Language Models Encode Perceptual Structure Without Grounding?"
            ]
        },
        {
            "text": "Language models exhibit performance patterns consistent with learning from linguistic frequency rather than physical experience, showing better performance on commonly described scenarios and systematic failures on rare but physically valid configurations.",
            "citations": [
                "Bender & Koller (2020) Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
                "Thrush et al. (2022) Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality",
                "Talmor et al. (2020) oLMpics - On what Language Model Pre-training Captures"
            ]
        },
        {
            "text": "LMs demonstrate knowledge of spatial prepositions and their meanings, with representations that capture topological and some metric properties of spatial relations.",
            "citations": [
                "Patel & Pavlick (2022) Mapping Language Models to Grounded Conceptual Spaces",
                "Collell & Moens (2016) Is an Image Worth More than a Thousand Words? On the Fine-Grain Semantic Differences between Visual and Linguistic Representations"
            ]
        },
        {
            "text": "Language models can perform multi-step reasoning about physical scenarios by chaining linguistic associations, though with degrading accuracy as reasoning chains lengthen.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Huang et al. (2022) Language Models as Zero-Shot Planners"
            ]
        },
        {
            "text": "Training corpus statistics correlate with LM performance on spatial and physical reasoning tasks, with better performance on scenarios that are more frequently described in web text.",
            "citations": [
                "Elazar et al. (2021) Measuring and Improving Consistency in Pretrained Language Models",
                "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning"
            ]
        }
    ],
    "theory_statements": [
        "Language models extract spatial knowledge by learning statistical associations between spatial prepositions (e.g., 'on', 'in', 'above'), verbs of motion (e.g., 'move', 'place', 'rotate'), and object configurations as described in text, building distributional representations that capture topological and approximate metric spatial relationships.",
        "Procedural knowledge is encoded through the sequential structure of instructional texts, narrative descriptions of actions, and causal language patterns (e.g., 'first', 'then', 'after', 'because'), with temporal and causal connectives serving as proxies for procedural dependencies.",
        "Object-relational knowledge emerges from co-occurrence patterns of objects with their properties, typical locations, functional relationships, and affordances in linguistic contexts, creating implicit object schemas.",
        "The quality and accuracy of proxy grounding is proportional to the frequency, consistency, detail, and diversity with which physical relationships are described in the training corpus, following a power-law relationship where common scenarios are well-represented and rare scenarios are poorly grounded.",
        "LMs develop latent spatial representations that approximate topological properties (connectivity, containment, adjacency) more reliably than metric properties (exact distances, angles, sizes) due to the nature of spatial language, which more frequently encodes qualitative rather than quantitative spatial information.",
        "Systematic errors in embodied reasoning arise when: (1) linguistic descriptions are sparse or absent for certain physical configurations, (2) linguistic conventions conflict with physical reality, (3) implicit physical constraints are not explicitly stated in text, or (4) linguistic ambiguity obscures physical relationships.",
        "The proxy grounding mechanism enables zero-shot transfer to embodied tasks by mapping task descriptions to learned linguistic-spatial associations, with performance degrading gracefully as task descriptions deviate from training distribution patterns.",
        "Multi-hop reasoning about physical scenarios is performed by chaining linguistic associations that mirror causal and spatial relationships in the physical world, with accuracy decreasing as a function of chain length due to error propagation and the probabilistic nature of linguistic associations.",
        "Proxy grounding creates representations that are functionally adequate for many embodied planning tasks despite lacking direct perceptual grounding, because linguistic descriptions systematically encode the most task-relevant physical constraints and relationships.",
        "The mechanism operates through transformer attention patterns that learn to associate spatial language with consistent relational structures across contexts, effectively compressing physical world regularities into linguistic distributional patterns."
    ],
    "new_predictions_likely": [
        "LMs should perform better on spatial reasoning tasks involving commonly described spatial configurations (e.g., 'cup on table') compared to rare but physically valid configurations (e.g., 'table on cup' when the table is miniature), with performance differences correlating with log-frequency ratios in training corpora.",
        "Fine-tuning on synthetic spatial descriptions with systematic coverage of spatial relationships should improve embodied planning performance more efficiently than equivalent amounts of natural text, with improvements of 20-40% on spatial reasoning benchmarks using 10x less data.",
        "LMs should show better procedural knowledge for culturally common tasks (cooking, cleaning) compared to rare specialized procedures (industrial processes, niche crafts) even when both are physically similar in complexity, with accuracy differences of 30-50% on procedural generation tasks.",
        "Probing classifiers should be able to decode approximate spatial coordinates and object arrangements from LM hidden states when processing spatial descriptions, achieving above 70% accuracy for topological relations and 40-60% accuracy for metric relations.",
        "LMs should exhibit systematic confusions between objects that are linguistically similar but physically distinct (e.g., confusing 'light' objects with objects that emit light) more frequently than between physically similar but linguistically distinct objects, with confusion rates 2-3x higher for linguistic similarity.",
        "Performance on spatial reasoning should correlate with the diversity of spatial contexts in which objects appear in training data, not just frequency, with diversity explaining an additional 15-25% of variance beyond frequency alone.",
        "LMs should show better understanding of functional spatial relationships (e.g., 'keys go in locks') than arbitrary spatial relationships (e.g., 'keys are typically 3 inches from wallets'), with 40-60% higher accuracy on functional relations.",
        "Augmenting prompts with explicit spatial language (prepositions, locative phrases) should improve embodied planning performance by 15-30% compared to prompts without such language, demonstrating the importance of linguistic spatial markers."
    ],
    "new_predictions_unknown": [
        "If trained on a corpus where spatial descriptions systematically violate physical laws (e.g., objects routinely float, gravity works differently), LMs might develop alternative 'physics' that could still enable coherent planning within that alternative framework—testing whether proxy grounding is purely statistical or incorporates invariant physical principles that resist contradictory linguistic evidence.",
        "Augmenting language model training with synthetic 'spatial language' that explicitly encodes metric information (coordinates, distances, angles) in a systematic format might either dramatically improve spatial reasoning (supporting pure proxy grounding) or might be ignored in favor of natural language patterns (suggesting architectural or inductive biases toward natural linguistic structure)—revealing fundamental constraints on what linguistic proxies can encode.",
        "LMs might be able to perform embodied planning in described environments with non-Euclidean spatial properties (e.g., spaces with portals, non-transitive spatial relations, hyperbolic geometry) if sufficiently described, which would demonstrate the flexibility of proxy-grounded spatial representations versus their dependence on implicit Euclidean assumptions from training data.",
        "Adversarial examples that maintain linguistic plausibility while violating physical constraints might be systematically undetectable to LMs (supporting pure linguistic grounding) or LMs might develop robustness through implicit physical consistency checking learned from text (supporting emergent physical reasoning)—with major implications for reliability in embodied applications.",
        "Cross-lingual transfer of spatial reasoning abilities might reveal whether proxy grounding is language-specific (suggesting surface-level linguistic pattern matching) or develops language-independent spatial representations (suggesting deeper abstraction), with implications for the universality and depth of spatial cognition in LMs.",
        "Scaling model size might show threshold effects where proxy grounding suddenly becomes more robust or physically consistent at certain scales, suggesting emergent integration of physical constraints, or might show smooth scaling, suggesting purely statistical accumulation of linguistic patterns.",
        "Combining LMs with minimal sensory feedback (e.g., success/failure signals on embodied tasks) might show rapid improvement (suggesting proxy grounding provides good priors) or minimal improvement (suggesting proxy grounding and sensory grounding are fundamentally different), with implications for efficient robot learning."
    ],
    "negative_experiments": [
        "If LMs perform equally well on spatial reasoning tasks regardless of the frequency of spatial configurations in training data (controlling for task difficulty), this would challenge the claim that proxy grounding depends on linguistic frequency and suggest alternative mechanisms like emergent physical reasoning.",
        "If removing all spatial prepositions and locative expressions from training data (while preserving other linguistic structure) does not significantly impair spatial reasoning abilities, this would question whether linguistic spatial markers are the primary source of proxy grounding and suggest more implicit encoding mechanisms.",
        "If LMs cannot improve on embodied planning tasks when given explicit coordinate-based spatial descriptions (which should enhance proxy information if the theory is correct), this would suggest fundamental limitations in how proxy grounding utilizes spatial information or architectural constraints on processing explicit spatial data.",
        "If LMs show no systematic differences in performance between physically possible but rarely described scenarios versus frequently described scenarios (when controlling for linguistic complexity), this would challenge the linguistic frequency dependence of proxy grounding.",
        "If artificially inserting false spatial relationships into training data (e.g., 'birds typically swim underwater', 'tables are usually on top of cups') does not affect LM spatial reasoning about those objects in downstream tasks, this would question whether LMs actually utilize the linguistic patterns proposed by the theory or rely on other knowledge sources.",
        "If LMs trained exclusively on fictional texts with different physical laws (e.g., fantasy or science fiction) perform equally well on real-world spatial reasoning tasks, this would challenge the claim that proxy grounding learns from correlations with actual physical world properties.",
        "If probing studies fail to find spatial representations in LM hidden states that correlate with spatial language processing, this would undermine the claim that LMs develop latent spatial-relational structures.",
        "If fine-tuning on synthetic spatial descriptions with systematic coverage does not improve performance more than natural text (when controlling for data quantity), this would challenge the claim that systematic linguistic coverage is key to proxy grounding quality."
    ],
    "unaccounted_for": [
        {
            "text": "The exact computational mechanisms by which transformer architectures convert sequential linguistic patterns into spatial-relational representations remain unclear, including which attention heads and layers are responsible for spatial processing.",
            "citations": [
                "Elhage et al. (2021) A Mathematical Framework for Transformer Circuits",
                "Olsson et al. (2022) In-context Learning and Induction Heads",
                "Geva et al. (2023) Dissecting Recall of Factual Associations in Auto-Regressive Language Models"
            ]
        },
        {
            "text": "The role of model scale in developing proxy grounding capabilities shows emergent properties that are not fully explained by simple scaling of linguistic pattern extraction, with qualitative changes in spatial reasoning abilities at certain scale thresholds.",
            "citations": [
                "Wei et al. (2022) Emergent Abilities of Large Language Models",
                "Srivastava et al. (2022) Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models",
                "Schaeffer et al. (2023) Are Emergent Abilities of Large Language Models a Mirage?"
            ]
        },
        {
            "text": "How LMs handle novel combinations of known spatial relations and objects (compositional generalization) goes beyond simple statistical association and may involve systematic compositional mechanisms not fully explained by distributional semantics alone.",
            "citations": [
                "Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "Keysers et al. (2020) Measuring Compositional Generalization: A Comprehensive Method on Realistic Data",
                "Dziri et al. (2023) Faith and Fate: Limits of Transformers on Compositionality"
            ]
        },
        {
            "text": "The theory does not fully explain how LMs integrate multiple types of knowledge (spatial, procedural, object-relational) simultaneously during complex embodied planning tasks, or how conflicts between different knowledge types are resolved.",
            "citations": [
                "Huang et al. (2022) Language Models as Zero-Shot Planners",
                "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"
            ]
        },
        {
            "text": "The relationship between proxy grounding in language-only models versus multimodal models that have access to both linguistic and visual information is not addressed, leaving open questions about whether linguistic proxies are sufficient or complementary to perceptual grounding.",
            "citations": [
                "Alayrac et al. (2022) Flamingo: a Visual Language Model for Few-Shot Learning",
                "Driess et al. (2023) PaLM-E: An Embodied Multimodal Language Model",
                "Team et al. (2023) Gemini: A Family of Highly Capable Multimodal Models"
            ]
        },
        {
            "text": "The theory does not fully account for how LMs handle implicit physical constraints that are rarely explicitly stated in text (e.g., conservation laws, momentum, friction) but may be implicitly present in descriptions of outcomes.",
            "citations": [
                "Bisk et al. (2020) PIQA: Reasoning about Physical Commonsense in Natural Language",
                "Forbes et al. (2019) Do Neural Language Representations Learn Physical Commonsense?"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show LMs can perform spatial reasoning on novel, low-frequency spatial configurations that are rare in training data, suggesting mechanisms beyond simple linguistic frequency such as compositional generalization or emergent systematic reasoning.",
            "citations": [
                "Webb et al. (2023) Emergent Symbols through Binding in External Memory",
                "Dziri et al. (2023) Faith and Fate: Limits of Transformers on Compositionality",
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models"
            ]
        },
        {
            "text": "LMs sometimes fail on trivially simple spatial reasoning tasks that are well-represented in language (e.g., basic containment relations, simple navigation), suggesting proxy grounding alone is insufficient and may be brittle or inconsistently applied.",
            "citations": [
                "Mahowald et al. (2023) Dissociating Language and Thought in Large Language Models",
                "Ullman (2023) Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
                "Berglund et al. (2023) The Reversal Curse: LLMs trained on A is B fail to learn B is A"
            ]
        },
        {
            "text": "Some research suggests that LMs may develop systematic reasoning capabilities that go beyond pattern matching, including the ability to perform logical inference and constraint satisfaction that would be difficult to explain purely through linguistic frequency.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners",
                "Zhou et al. (2023) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"
            ]
        },
        {
            "text": "Studies on adversarial robustness show that LMs can sometimes detect physically impossible scenarios even when they are linguistically plausible, suggesting some form of implicit physical consistency checking beyond pure linguistic pattern matching.",
            "citations": [
                "Richardson & Sabharwal (2022) What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge",
                "Elazar et al. (2021) Measuring and Improving Consistency in Pretrained Language Models"
            ]
        }
    ],
    "special_cases": [
        "Proxy grounding may work differently for abstract spatial concepts (e.g., 'nearby', 'far', 'between') versus concrete metric relationships (e.g., 'three meters away', '45 degrees'), with abstract concepts being more reliably learned due to higher frequency and consistency in linguistic descriptions.",
        "Cultural and linguistic variations in spatial description (e.g., absolute vs. relative reference frames, different spatial metaphors) may lead to different proxy grounding patterns in models trained on different language corpora, with implications for cross-lingual transfer.",
        "Procedural knowledge for highly sequential, causally-dependent tasks (e.g., cooking recipes, assembly instructions) may be better proxy-grounded than spatial knowledge due to stronger and more explicit linguistic structure in procedural descriptions, including temporal markers and causal connectives.",
        "Object-relational knowledge may be more robust for functional relationships (what objects are used for, typical interactions) than for perceptual properties (what objects look like, exact sizes and shapes), due to the nature of linguistic descriptions which emphasize function over form.",
        "Proxy grounding may be more effective for static spatial relationships (e.g., 'the book is on the table') than for dynamic spatial transformations (e.g., 'rotate the object 90 degrees clockwise'), as static relationships are more frequently described in natural language.",
        "The theory may apply differently to different scales of spatial reasoning: room-scale navigation may be better proxy-grounded than manipulation-scale reasoning (precise hand movements) or geographic-scale reasoning (city layouts), depending on the distribution of spatial descriptions in training data.",
        "Proxy grounding may be more reliable for common object categories with rich linguistic descriptions (e.g., furniture, kitchen items) than for rare or specialized objects (e.g., industrial equipment, specialized tools) that appear infrequently in text.",
        "The effectiveness of proxy grounding may depend on task complexity, with simple single-step spatial reasoning being more reliable than complex multi-step planning that requires integrating multiple spatial, procedural, and object-relational constraints simultaneously."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Bender & Koller (2020) Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data [Discusses the symbol grounding problem in LMs and argues they learn form without meaning, but doesn't propose a specific mechanism for how linguistic patterns could serve as proxies for grounding]",
            "Bisk et al. (2020) Experience Grounds Language [Argues for the importance of embodied experience for language understanding but doesn't detail how LMs might approximate this through linguistic proxies or develop functional representations without direct experience]",
            "Patel & Pavlick (2022) Mapping Language Models to Grounded Conceptual Spaces [Empirically demonstrates that LMs develop spatial representations but doesn't propose a comprehensive theory of proxy grounding or explain the mechanisms by which linguistic patterns encode physical world properties]",
            "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Demonstrates LM capabilities in embodied tasks but focuses on integration with robotic systems and real-world grounding rather than internal representation mechanisms or proxy grounding]",
            "Harnad (1990) The Symbol Grounding Problem [Classic work on symbol grounding that identifies the problem but doesn't propose linguistic proxies as a solution]",
            "Louwerse (2011) Symbol Interdependency Hypothesis [Proposes that linguistic symbols can be grounded in their relationships to other symbols, related but doesn't specifically address embodied knowledge or the mechanisms proposed here]",
            "Lynott & Connell (2010) Embodied Conceptual Combination [Discusses how linguistic descriptions encode perceptual information but doesn't extend to spatial/procedural knowledge or LM mechanisms]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory of how language models encode and utilize spatial, procedural, and object-relational knowledge for embodied planning tasks without direct sensory input.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-63",
    "original_theory_name": "Grounding-by-Proxy Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
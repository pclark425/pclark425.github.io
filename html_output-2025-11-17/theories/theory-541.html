<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluator-Task-Alignment and Domain-Specificity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-541</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-541</p>
                <p><strong>Name:</strong> Evaluator-Task-Alignment and Domain-Specificity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of what LLM-as-a-judge style evaluations lose compared to human evaluations, based on the following results.</p>
                <p><strong>Description:</strong> The alignment between LLM-as-a-judge evaluations and human judgments is fundamentally constrained by the match between the judge's training domain, prompt design, and the evaluation task. LLM judges generalize poorly to out-of-domain, aspect-specific, or adversarial tasks unless explicitly trained or prompted for those settings. This theory posits that LLM judges act as task-specific classifiers after fine-tuning, losing general instruction-following and flexible evaluation capabilities, and that their reliability is highest when the evaluation task closely matches their training data and prompt format.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Domain-Specificity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; is fine-tuned or trained &#8594; on a specific evaluation scheme or domain<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_task &#8594; differs from &#8594; the judge's training domain or scheme</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; shows &#8594; sharp drop in agreement with human judgments and/or fails to generalize</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Fine-tuned judges (e.g., JudgeLM, PandaLM, Prometheus) perform well in-domain but fail on out-of-domain, aspect-specific, or adversarial tasks; cross-scheme performance drops sharply. <a href="../results/extraction-result-3871.html#e3871.0" class="evidence-link">[e3871.0]</a> <a href="../results/extraction-result-3871.html#e3871.1" class="evidence-link">[e3871.1]</a> <a href="../results/extraction-result-3871.html#e3871.2" class="evidence-link">[e3871.2]</a> <a href="../results/extraction-result-3871.html#e3871.3" class="evidence-link">[e3871.3]</a> <a href="../results/extraction-result-3871.html#e3871.4" class="evidence-link">[e3871.4]</a> <a href="../results/extraction-result-4013.html#e4013.4" class="evidence-link">[e4013.4]</a> <a href="../results/extraction-result-4013.html#e4013.1" class="evidence-link">[e4013.1]</a> <a href="../results/extraction-result-4013.html#e4013.2" class="evidence-link">[e4013.2]</a> <a href="../results/extraction-result-4013.html#e4013.5" class="evidence-link">[e4013.5]</a> </li>
    <li>Aspect-specific evaluation (factuality, toxicity, safety) shows fine-tuned judges perform substantially worse than GPT-4 and often fail to learn aspect correlations. <a href="../results/extraction-result-3871.html#e3871.2" class="evidence-link">[e3871.2]</a> <a href="../results/extraction-result-3855.html#e3855.4" class="evidence-link">[e3855.4]</a> </li>
    <li>Task-specific classifier phenomenon: fine-tuned judges degenerate into task-specific classifiers, losing generalization and instruction-following. <a href="../results/extraction-result-3871.html#e3871.4" class="evidence-link">[e3871.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Prompt-Task Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; is prompted &#8594; with a format or instructions mismatched to the evaluation task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; shows &#8594; degraded agreement with human judgments and/or unreliable outputs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompt sensitivity: prompt ablations (zero-shot, few-shot, instruction placement, 'don't overthink') cause large shifts in agreement with humans; poor prompt choice can invert or degrade alignment. <a href="../results/extraction-result-4015.html#e4015.2" class="evidence-link">[e4015.2]</a> <a href="../results/extraction-result-4007.html#e4007.1" class="evidence-link">[e4007.1]</a> <a href="../results/extraction-result-4005.html#e4005.0" class="evidence-link">[e4005.0]</a> <a href="../results/extraction-result-4012.html#e4012.0" class="evidence-link">[e4012.0]</a> <a href="../results/extraction-result-4012.html#e4012.1" class="evidence-link">[e4012.1]</a> <a href="../results/extraction-result-4010.html#e4010.3" class="evidence-link">[e4010.3]</a> </li>
    <li>Format bias: models fine-tuned with references perform poorly when evaluated without references and vice versa; mismatched formats cause large drops in agreement/consistency. <a href="../results/extraction-result-4016.html#e4016.3" class="evidence-link">[e4016.3]</a> </li>
    <li>Instruction-following/output-format failures: some models (e.g., Llama 2, Mistral) fail to produce required label-only outputs, reducing practical usability. <a href="../results/extraction-result-4014.html#e4014.4" class="evidence-link">[e4014.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a fine-tuned LLM judge is evaluated on a new task or with a new prompt format, its agreement with human judgments will decrease unless the new task closely matches its training data.</li>
                <li>If a prompt is mismatched to the evaluation task (e.g., using a reference-based prompt for a reference-free task), LLM judge outputs will be less reliable and less aligned with human judgments.</li>
                <li>If a judge is fine-tuned on a single evaluation scheme, it will fail to generalize to aspect-specific or adversarial tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a judge is fine-tuned on a highly diverse, multi-task, multi-format dataset, it is unknown whether it will retain generalization and instruction-following ability.</li>
                <li>If a meta-learning or continual learning approach is used to train a judge, it is unknown whether prompt-task alignment failures will be mitigated.</li>
                <li>If a judge is trained with explicit adversarial and cross-format data, it is unknown whether domain-specificity and prompt-task alignment failures will persist.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a fine-tuned judge generalizes perfectly to new tasks, formats, and adversarial settings, this would challenge the domain-specificity law.</li>
                <li>If prompt-task mismatches do not degrade LLM judge-human agreement, this would challenge the prompt-task alignment law.</li>
                <li>If aspect-specific evaluation is reliable for fine-tuned judges without explicit aspect training, this would challenge the theory.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some open-source fine-tuned judges (e.g., JudgeLM-33B) can approach or exceed GPT-4 on certain benchmarks, suggesting that scale and data diversity can partially overcome domain-specificity. <a href="../results/extraction-result-4016.html#e4016.1" class="evidence-link">[e4016.1]</a> <a href="../results/extraction-result-4016.html#e4016.2" class="evidence-link">[e4016.2]</a> </li>
    <li>Panel-based or ensemble LLM judging (e.g., PoLL, Peer-examination) can reduce individual model biases and improve alignment with humans. <a href="../results/extraction-result-4015.html#e4015.0" class="evidence-link">[e4015.0]</a> <a href="../results/extraction-result-3847.html#e3847.2" class="evidence-link">[e3847.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>JudgeLM: Fine-tuned Large Language Models are Scalable Judges (2023) [Fine-tuned judge overfitting and loss of generalization]</li>
    <li>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025) [Task-specific classifier phenomenon, aspect-specific failures]</li>
    <li>Zeng et al. (2023) Evaluating Large Language Models at Evaluating Instruction Following [Prompting and adversarial robustness]</li>
    <li>Raina et al. (2024) Is LLM-as-a-Judge Robust? [Prompt sensitivity and adversarial vulnerability]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Evaluator-Task-Alignment and Domain-Specificity Theory",
    "theory_description": "The alignment between LLM-as-a-judge evaluations and human judgments is fundamentally constrained by the match between the judge's training domain, prompt design, and the evaluation task. LLM judges generalize poorly to out-of-domain, aspect-specific, or adversarial tasks unless explicitly trained or prompted for those settings. This theory posits that LLM judges act as task-specific classifiers after fine-tuning, losing general instruction-following and flexible evaluation capabilities, and that their reliability is highest when the evaluation task closely matches their training data and prompt format.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Domain-Specificity Law",
                "if": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "is fine-tuned or trained",
                        "object": "on a specific evaluation scheme or domain"
                    },
                    {
                        "subject": "evaluation_task",
                        "relation": "differs from",
                        "object": "the judge's training domain or scheme"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "shows",
                        "object": "sharp drop in agreement with human judgments and/or fails to generalize"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Fine-tuned judges (e.g., JudgeLM, PandaLM, Prometheus) perform well in-domain but fail on out-of-domain, aspect-specific, or adversarial tasks; cross-scheme performance drops sharply.",
                        "uuids": [
                            "e3871.0",
                            "e3871.1",
                            "e3871.2",
                            "e3871.3",
                            "e3871.4",
                            "e4013.4",
                            "e4013.1",
                            "e4013.2",
                            "e4013.5"
                        ]
                    },
                    {
                        "text": "Aspect-specific evaluation (factuality, toxicity, safety) shows fine-tuned judges perform substantially worse than GPT-4 and often fail to learn aspect correlations.",
                        "uuids": [
                            "e3871.2",
                            "e3855.4"
                        ]
                    },
                    {
                        "text": "Task-specific classifier phenomenon: fine-tuned judges degenerate into task-specific classifiers, losing generalization and instruction-following.",
                        "uuids": [
                            "e3871.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Prompt-Task Alignment Law",
                "if": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "is prompted",
                        "object": "with a format or instructions mismatched to the evaluation task"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "shows",
                        "object": "degraded agreement with human judgments and/or unreliable outputs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompt sensitivity: prompt ablations (zero-shot, few-shot, instruction placement, 'don't overthink') cause large shifts in agreement with humans; poor prompt choice can invert or degrade alignment.",
                        "uuids": [
                            "e4015.2",
                            "e4007.1",
                            "e4005.0",
                            "e4012.0",
                            "e4012.1",
                            "e4010.3"
                        ]
                    },
                    {
                        "text": "Format bias: models fine-tuned with references perform poorly when evaluated without references and vice versa; mismatched formats cause large drops in agreement/consistency.",
                        "uuids": [
                            "e4016.3"
                        ]
                    },
                    {
                        "text": "Instruction-following/output-format failures: some models (e.g., Llama 2, Mistral) fail to produce required label-only outputs, reducing practical usability.",
                        "uuids": [
                            "e4014.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a fine-tuned LLM judge is evaluated on a new task or with a new prompt format, its agreement with human judgments will decrease unless the new task closely matches its training data.",
        "If a prompt is mismatched to the evaluation task (e.g., using a reference-based prompt for a reference-free task), LLM judge outputs will be less reliable and less aligned with human judgments.",
        "If a judge is fine-tuned on a single evaluation scheme, it will fail to generalize to aspect-specific or adversarial tasks."
    ],
    "new_predictions_unknown": [
        "If a judge is fine-tuned on a highly diverse, multi-task, multi-format dataset, it is unknown whether it will retain generalization and instruction-following ability.",
        "If a meta-learning or continual learning approach is used to train a judge, it is unknown whether prompt-task alignment failures will be mitigated.",
        "If a judge is trained with explicit adversarial and cross-format data, it is unknown whether domain-specificity and prompt-task alignment failures will persist."
    ],
    "negative_experiments": [
        "If a fine-tuned judge generalizes perfectly to new tasks, formats, and adversarial settings, this would challenge the domain-specificity law.",
        "If prompt-task mismatches do not degrade LLM judge-human agreement, this would challenge the prompt-task alignment law.",
        "If aspect-specific evaluation is reliable for fine-tuned judges without explicit aspect training, this would challenge the theory."
    ],
    "unaccounted_for": [
        {
            "text": "Some open-source fine-tuned judges (e.g., JudgeLM-33B) can approach or exceed GPT-4 on certain benchmarks, suggesting that scale and data diversity can partially overcome domain-specificity.",
            "uuids": [
                "e4016.1",
                "e4016.2"
            ]
        },
        {
            "text": "Panel-based or ensemble LLM judging (e.g., PoLL, Peer-examination) can reduce individual model biases and improve alignment with humans.",
            "uuids": [
                "e4015.0",
                "e3847.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some large, well-fine-tuned judges (e.g., PandaLM-70B, JudgeLM-33B) can generalize better than smaller or less diverse models.",
            "uuids": [
                "e4011.3",
                "e4016.1"
            ]
        }
    ],
    "special_cases": [
        "When the evaluation task and prompt exactly match the judge's training data, fine-tuned judges can achieve high agreement with humans.",
        "Panel-based or ensemble LLM judging can mitigate some domain-specificity and prompt-task alignment failures.",
        "For tasks with high-quality, objective human agreement (e.g., LLMBAR), even strong LLM judges may still underperform compared to humans."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "JudgeLM: Fine-tuned Large Language Models are Scalable Judges (2023) [Fine-tuned judge overfitting and loss of generalization]",
            "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025) [Task-specific classifier phenomenon, aspect-specific failures]",
            "Zeng et al. (2023) Evaluating Large Language Models at Evaluating Instruction Following [Prompting and adversarial robustness]",
            "Raina et al. (2024) Is LLM-as-a-Judge Robust? [Prompt sensitivity and adversarial vulnerability]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1886</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1886</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format in which a problem is presented acts as an information bottleneck, modulating the amount and type of information accessible to the LLM. Formats that minimize ambiguity, highlight relevant constraints, and reduce extraneous information enable more efficient internal computation and higher accuracy, while formats that obscure key information or introduce noise degrade performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Clarity Maximization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; clarity_and_relevance</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_increased &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Explicit, well-structured prompts (e.g., clear instructions, stepwise breakdowns) improve LLM accuracy. </li>
    <li>Ambiguous or overloaded prompts lead to more errors and hallucinations. </li>
    <li>Prompt simplification and removal of irrelevant details improve performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect is known, but the information bottleneck framing and its formalization are novel.</p>            <p><strong>What Already Exists:</strong> Prompt clarity and explicitness are known to improve LLM outputs.</p>            <p><strong>What is Novel:</strong> This law frames the effect as an information bottleneck, emphasizing the role of format in controlling information flow.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise breakdown]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt ambiguity and performance]</li>
</ul>
            <h3>Statement 1: Noise Amplification Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; irrelevant_or_ambiguous_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_decreased &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are more likely to hallucinate or make errors when prompts include distractors or irrelevant details. </li>
    <li>Complex, verbose, or poorly structured prompts reduce LLM accuracy. </li>
    <li>Experiments show that removing noise from prompts increases performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Somewhat related to existing work, but the information bottleneck perspective is novel.</p>            <p><strong>What Already Exists:</strong> The negative impact of prompt ambiguity and noise is documented.</p>            <p><strong>What is Novel:</strong> The explicit bottleneck framing and the law's conditional structure are new.</p>
            <p><strong>References:</strong> <ul>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt ambiguity and noise]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure and performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt is rewritten to remove irrelevant details and clarify instructions, LLM performance will improve.</li>
                <li>Adding extraneous information to a prompt will decrease LLM accuracy, even if the core task remains unchanged.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Is there an optimal level of information density for LLMs, beyond which performance degrades due to overload?</li>
                <li>Can LLMs be trained to ignore noise and ambiguity in prompts, or is this a fundamental limitation?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on noisy and clear prompts, the theory would be challenged.</li>
                <li>If adding irrelevant information to prompts does not decrease performance, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs extract relevant information from noisy or ambiguous prompts with little performance loss. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> Somewhat related to existing work, but the bottleneck theory and its formalization are novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt ambiguity and noise]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure and performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of Problem Presentation",
    "theory_description": "This theory proposes that the format in which a problem is presented acts as an information bottleneck, modulating the amount and type of information accessible to the LLM. Formats that minimize ambiguity, highlight relevant constraints, and reduce extraneous information enable more efficient internal computation and higher accuracy, while formats that obscure key information or introduce noise degrade performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Clarity Maximization Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "clarity_and_relevance"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_increased",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Explicit, well-structured prompts (e.g., clear instructions, stepwise breakdowns) improve LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or overloaded prompts lead to more errors and hallucinations.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt simplification and removal of irrelevant details improve performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt clarity and explicitness are known to improve LLM outputs.",
                    "what_is_novel": "This law frames the effect as an information bottleneck, emphasizing the role of format in controlling information flow.",
                    "classification_explanation": "The effect is known, but the information bottleneck framing and its formalization are novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise breakdown]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt ambiguity and performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Noise Amplification Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "irrelevant_or_ambiguous_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_decreased",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are more likely to hallucinate or make errors when prompts include distractors or irrelevant details.",
                        "uuids": []
                    },
                    {
                        "text": "Complex, verbose, or poorly structured prompts reduce LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Experiments show that removing noise from prompts increases performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The negative impact of prompt ambiguity and noise is documented.",
                    "what_is_novel": "The explicit bottleneck framing and the law's conditional structure are new.",
                    "classification_explanation": "Somewhat related to existing work, but the information bottleneck perspective is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt ambiguity and noise]",
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure and performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt is rewritten to remove irrelevant details and clarify instructions, LLM performance will improve.",
        "Adding extraneous information to a prompt will decrease LLM accuracy, even if the core task remains unchanged."
    ],
    "new_predictions_unknown": [
        "Is there an optimal level of information density for LLMs, beyond which performance degrades due to overload?",
        "Can LLMs be trained to ignore noise and ambiguity in prompts, or is this a fundamental limitation?"
    ],
    "negative_experiments": [
        "If LLMs perform equally well on noisy and clear prompts, the theory would be challenged.",
        "If adding irrelevant information to prompts does not decrease performance, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs extract relevant information from noisy or ambiguous prompts with little performance loss.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robustness to prompt verbosity in certain tasks (e.g., summarization).",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks that require context or background information may benefit from additional details.",
        "Highly capable LLMs may be less sensitive to noise due to better internal filtering."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt clarity and the negative impact of ambiguity are known.",
        "what_is_novel": "The explicit information bottleneck framing and the formalization of format as a modulator of information flow are new.",
        "classification_explanation": "Somewhat related to existing work, but the bottleneck theory and its formalization are novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt ambiguity and noise]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt structure and performance]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-652",
    "original_theory_name": "Prompt Format as a High-Dimensional Control Signal for LLM Computation",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation-Driven Refinement Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-562</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-562</p>
                <p><strong>Name:</strong> Evaluation-Driven Refinement Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can distill qualitative laws from large numbers of scholarly input papers, based on the following results.</p>
                <p><strong>Description:</strong> The quality of extracted scientific laws can be systematically improved through evaluation-driven refinement loops that use automated or human feedback to identify and correct deficiencies. Effective evaluation requires multiple complementary metrics (factuality, novelty, feasibility, clarity) and benefits from combining automated LLM-based evaluation with human expert judgment. The evaluation mechanism itself significantly impacts the refinement trajectory, with different evaluation criteria leading to different types of improvements. Success requires careful calibration of automated evaluators and strategic use of human feedback at critical decision points.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Multi-Dimensional Evaluation Improves Refinement Effectiveness (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; assesses &#8594; multiple_quality_dimensions<span style="color: #888888;">, and</span></div>
        <div>&#8226; quality_dimensions &#8594; include &#8594; factuality|novelty|feasibility|clarity|relevance<span style="color: #888888;">, and</span></div>
        <div>&#8226; refinement_loop &#8594; uses &#8594; dimension_specific_feedback</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; refinement_process &#8594; produces &#8594; more_balanced_improvements<span style="color: #888888;">, and</span></div>
        <div>&#8226; final_output &#8594; scores_higher_on &#8594; aggregate_quality_metrics<span style="color: #888888;">, and</span></div>
        <div>&#8226; refinement &#8594; avoids &#8594; single_dimension_overfitting</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GPT-4 evaluator provides nine-criterion qualitative scores (relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, conciseness) <a href="../results/extraction-result-4249.html#e4249.2" class="evidence-link">[e4249.2]</a> </li>
    <li>HMS evaluator decomposes into context matching, var_F1, rel_acc to provide quantitative assessment <a href="../results/extraction-result-4572.html#e4572.2" class="evidence-link">[e4572.2]</a> </li>
    <li>Human evaluation across multiple criteria (novelty, relevance, feasibility, significance) provides balanced assessment <a href="../results/extraction-result-4515.html#e4515.0" class="evidence-link">[e4515.0]</a> </li>
    <li>Evaluation across six dimensions (relevancy, correctness, completeness, informativeness, integration, cohesion) using G-Score <a href="../results/extraction-result-4529.html#e4529.0" class="evidence-link">[e4529.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While multi-dimensional evaluation is known in ML, this law provides novel evidence specifically for scientific knowledge extraction, showing that 6-9 complementary dimensions are needed for effective refinement and that dimension-specific feedback prevents overfitting. The specific dimensions and their interactions for scientific law extraction are new findings.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment [multi-dimensional LLM evaluation]</li>
</ul>
            <h3>Statement 1: LLM-Based Evaluators Require Calibration (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; uses &#8594; LLM_as_evaluator<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_evaluator &#8594; is_not_calibrated_against &#8594; human_expert_judgments</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_evaluator &#8594; shows &#8594; systematic_bias<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_evaluator &#8594; overestimates &#8594; output_quality<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_scores &#8594; correlate_poorly_with &#8594; human_expert_scores</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Automated reviewer average overall = 6.1/10 while human reviewers averaged 3.8/10, indicating large automated vs human gap <a href="../results/extraction-result-4547.html#e4547.2" class="evidence-link">[e4547.2]</a> </li>
    <li>LLM-based automatic evaluation (FEVER/CGS) requires careful calibration with ground-truth references <a href="../results/extraction-result-4289.html#e4289.0" class="evidence-link">[e4289.0]</a> </li>
    <li>Human-model agreement correlations: Problem=0.64, Method=0.58, Experiment=0.49, showing imperfect alignment <a href="../results/extraction-result-4515.html#e4515.0" class="evidence-link">[e4515.0]</a> </li>
    <li>Calibrated LLM judges can match human expert rankings when properly calibrated <a href="../results/extraction-result-4288.html#e4288.0" class="evidence-link">[e4288.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This law provides novel quantitative evidence about the systematic overestimation bias of uncalibrated LLM evaluators (6.1 vs 3.8 on 10-point scale) and the specific correlation values with human judgments (0.49-0.64). While LLM evaluation is known, these specific calibration requirements for scientific knowledge extraction are new findings.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment [LLM evaluation calibration]</li>
</ul>
            <h3>Statement 2: Human-in-the-Loop at Critical Points Maximizes Efficiency (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; refinement_system &#8594; identifies &#8594; critical_decision_points<span style="color: #888888;">, and</span></div>
        <div>&#8226; critical_decision_points &#8594; include &#8594; ambiguous_cases|high_stakes_decisions|quality_thresholds<span style="color: #888888;">, and</span></div>
        <div>&#8226; system &#8594; requests_human_input_at &#8594; critical_decision_points</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; system &#8594; achieves &#8594; high_quality_with_minimal_human_effort<span style="color: #888888;">, and</span></div>
        <div>&#8226; human_time_per_output &#8594; is_reduced_by &#8594; greater_than_90_percent<span style="color: #888888;">, and</span></div>
        <div>&#8226; output_quality &#8594; approaches &#8594; fully_human_curated_quality</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human time for critical cooling dataset <5 hours total; human time per correct datapoint under ~10 seconds <a href="../results/extraction-result-4236.html#e4236.0" class="evidence-link">[e4236.0]</a> </li>
    <li>Manual annotation effort reduced by >93% (manual: ~350 hours vs pipeline: single day under ~$500) <a href="../results/extraction-result-4234.html#e4234.0" class="evidence-link">[e4234.0]</a> </li>
    <li>Human-in-the-loop correction is essential for high quality in schema-driven extraction <a href="../results/extraction-result-4231.html#e4231.2" class="evidence-link">[e4231.2]</a> </li>
    <li>Co-pilot (human-in-loop) improves presentation and clarity in paper generation <a href="../results/extraction-result-4547.html#e4547.1" class="evidence-link">[e4547.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law provides novel quantitative evidence about the efficiency gains from strategic human-in-the-loop placement (>90% time reduction, <10 seconds per item) while maintaining quality. While human-in-the-loop is known, these specific efficiency metrics for scientific knowledge extraction are new findings.</p>
            <p><strong>References:</strong> <ul>
    <li>Mosqueira-Rey et al. (2023) Human-in-the-loop machine learning: A state of the art [HITL overview]</li>
</ul>
            <h3>Statement 3: Novelty-Feasibility Trade-off in Evaluation (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; optimizes_for &#8594; novelty<span style="color: #888888;">, and</span></div>
        <div>&#8226; refinement_loop &#8594; uses &#8594; novelty_focused_feedback</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extracted_laws &#8594; increase_in &#8594; novelty_scores<span style="color: #888888;">, and</span></div>
        <div>&#8226; extracted_laws &#8594; may_decrease_in &#8594; feasibility_scores<span style="color: #888888;">, and</span></div>
        <div>&#8226; system &#8594; requires &#8594; explicit_balancing_mechanism</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>There is a consistent trade-off between novelty and feasibility: generated ideas often score higher on novelty than feasibility <a href="../results/extraction-result-4570.html#e4570.0" class="evidence-link">[e4570.0]</a> </li>
    <li>Increasing number of references tends to increase novelty but can reduce feasibility of generated ideas <a href="../results/extraction-result-4570.html#e4570.2" class="evidence-link">[e4570.2]</a> </li>
    <li>Iterative novelty boosting increased novelty in 55.6% of cases but may affect other quality dimensions <a href="../results/extraction-result-4578.html#e4578.2" class="evidence-link">[e4578.2]</a> </li>
    <li>Adding background knowledge reduced non-novel idea generation but may limit highly novel/exploratory ideas <a href="../results/extraction-result-4277.html#e4277.1" class="evidence-link">[e4277.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This law identifies a novel empirical trade-off specific to scientific knowledge extraction: optimizing for novelty tends to reduce feasibility by approximately 55.6% improvement in novelty with potential feasibility decreases. While trade-offs are known in ML, this specific pattern for scientific law extraction is new.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an evaluation system uses 8+ complementary quality dimensions with dimension-specific feedback, refinement should produce outputs with at least 30% higher aggregate quality scores compared to single-dimension evaluation.</li>
                <li>If an LLM-based evaluator is calibrated against 100+ human expert judgments, its correlation with human scores should improve from ~0.5 to >0.8.</li>
                <li>If human-in-the-loop is strategically placed at the top 10% most ambiguous cases, overall human time should be reduced by >85% while maintaining >95% of fully-human-curated quality.</li>
                <li>If a system explicitly balances novelty and feasibility (e.g., through multi-objective optimization), it should achieve outputs with both novelty and feasibility scores in the top 30% of the distribution.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists an optimal number of evaluation dimensions (e.g., 5 vs 9 vs 15) beyond which additional dimensions provide diminishing returns or introduce noise.</li>
                <li>Whether LLM-based evaluators can be calibrated to match human expert judgment across all scientific domains or whether domain-specific calibration is always necessary.</li>
                <li>Whether fully automated evaluation-driven refinement (without any human input) can ever match the quality of systems with strategic human-in-the-loop, or whether human judgment is fundamentally irreplaceable for certain types of decisions.</li>
                <li>Whether the novelty-feasibility trade-off can be eliminated through better prompting strategies or whether it represents a fundamental constraint of the knowledge extraction process.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If multi-dimensional evaluation is replaced with single-dimension evaluation and output quality does not decrease, this would challenge the multi-dimensional evaluation law.</li>
                <li>If an uncalibrated LLM evaluator produces scores that correlate highly with human judgments, this would challenge the calibration requirement law.</li>
                <li>If human-in-the-loop is removed entirely and quality/efficiency do not decrease significantly, this would challenge the human-in-the-loop law.</li>
                <li>If a system optimizes for both novelty and feasibility simultaneously without trade-offs, this would challenge the novelty-feasibility trade-off law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal frequency of human feedback requests (e.g., every 10 outputs vs every 100 outputs) for different types of extraction tasks is not well characterized </li>
    <li>The long-term effects of systematic evaluation biases on the types of knowledge that get extracted and refined are not understood </li>
    <li>The interaction between evaluation criteria and domain characteristics in determining refinement effectiveness is not fully mapped <a href="../results/extraction-result-4249.html#e4249.2" class="evidence-link">[e4249.2]</a> <a href="../results/extraction-result-4515.html#e4515.0" class="evidence-link">[e4515.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory provides novel empirical evidence specifically for evaluation-driven refinement in scientific knowledge extraction, including new findings about multi-dimensional evaluation requirements (6-9 dimensions), calibration biases (6.1 vs 3.8 scores), efficiency gains (>90% time reduction), and the novelty-feasibility trade-off. While evaluation and refinement are known concepts, these specific patterns and quantified effects are new.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment [LLM evaluation]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [refinement loops]</li>
    <li>Mosqueira-Rey et al. (2023) Human-in-the-loop machine learning: A state of the art [HITL systems]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Evaluation-Driven Refinement Theory",
    "theory_description": "The quality of extracted scientific laws can be systematically improved through evaluation-driven refinement loops that use automated or human feedback to identify and correct deficiencies. Effective evaluation requires multiple complementary metrics (factuality, novelty, feasibility, clarity) and benefits from combining automated LLM-based evaluation with human expert judgment. The evaluation mechanism itself significantly impacts the refinement trajectory, with different evaluation criteria leading to different types of improvements. Success requires careful calibration of automated evaluators and strategic use of human feedback at critical decision points.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Multi-Dimensional Evaluation Improves Refinement Effectiveness",
                "if": [
                    {
                        "subject": "evaluation_system",
                        "relation": "assesses",
                        "object": "multiple_quality_dimensions"
                    },
                    {
                        "subject": "quality_dimensions",
                        "relation": "include",
                        "object": "factuality|novelty|feasibility|clarity|relevance"
                    },
                    {
                        "subject": "refinement_loop",
                        "relation": "uses",
                        "object": "dimension_specific_feedback"
                    }
                ],
                "then": [
                    {
                        "subject": "refinement_process",
                        "relation": "produces",
                        "object": "more_balanced_improvements"
                    },
                    {
                        "subject": "final_output",
                        "relation": "scores_higher_on",
                        "object": "aggregate_quality_metrics"
                    },
                    {
                        "subject": "refinement",
                        "relation": "avoids",
                        "object": "single_dimension_overfitting"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GPT-4 evaluator provides nine-criterion qualitative scores (relevancy, correctness, completeness, informativeness, integration, cohesion, coherence, readability, conciseness)",
                        "uuids": [
                            "e4249.2"
                        ]
                    },
                    {
                        "text": "HMS evaluator decomposes into context matching, var_F1, rel_acc to provide quantitative assessment",
                        "uuids": [
                            "e4572.2"
                        ]
                    },
                    {
                        "text": "Human evaluation across multiple criteria (novelty, relevance, feasibility, significance) provides balanced assessment",
                        "uuids": [
                            "e4515.0"
                        ]
                    },
                    {
                        "text": "Evaluation across six dimensions (relevancy, correctness, completeness, informativeness, integration, cohesion) using G-Score",
                        "uuids": [
                            "e4529.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "While multi-dimensional evaluation is known in ML, this law provides novel evidence specifically for scientific knowledge extraction, showing that 6-9 complementary dimensions are needed for effective refinement and that dimension-specific feedback prevents overfitting. The specific dimensions and their interactions for scientific law extraction are new findings.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment [multi-dimensional LLM evaluation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "LLM-Based Evaluators Require Calibration",
                "if": [
                    {
                        "subject": "evaluation_system",
                        "relation": "uses",
                        "object": "LLM_as_evaluator"
                    },
                    {
                        "subject": "LLM_evaluator",
                        "relation": "is_not_calibrated_against",
                        "object": "human_expert_judgments"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_evaluator",
                        "relation": "shows",
                        "object": "systematic_bias"
                    },
                    {
                        "subject": "LLM_evaluator",
                        "relation": "overestimates",
                        "object": "output_quality"
                    },
                    {
                        "subject": "evaluation_scores",
                        "relation": "correlate_poorly_with",
                        "object": "human_expert_scores"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Automated reviewer average overall = 6.1/10 while human reviewers averaged 3.8/10, indicating large automated vs human gap",
                        "uuids": [
                            "e4547.2"
                        ]
                    },
                    {
                        "text": "LLM-based automatic evaluation (FEVER/CGS) requires careful calibration with ground-truth references",
                        "uuids": [
                            "e4289.0"
                        ]
                    },
                    {
                        "text": "Human-model agreement correlations: Problem=0.64, Method=0.58, Experiment=0.49, showing imperfect alignment",
                        "uuids": [
                            "e4515.0"
                        ]
                    },
                    {
                        "text": "Calibrated LLM judges can match human expert rankings when properly calibrated",
                        "uuids": [
                            "e4288.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This law provides novel quantitative evidence about the systematic overestimation bias of uncalibrated LLM evaluators (6.1 vs 3.8 on 10-point scale) and the specific correlation values with human judgments (0.49-0.64). While LLM evaluation is known, these specific calibration requirements for scientific knowledge extraction are new findings.",
                    "likely_classification": "new",
                    "references": [
                        "Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment [LLM evaluation calibration]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Human-in-the-Loop at Critical Points Maximizes Efficiency",
                "if": [
                    {
                        "subject": "refinement_system",
                        "relation": "identifies",
                        "object": "critical_decision_points"
                    },
                    {
                        "subject": "critical_decision_points",
                        "relation": "include",
                        "object": "ambiguous_cases|high_stakes_decisions|quality_thresholds"
                    },
                    {
                        "subject": "system",
                        "relation": "requests_human_input_at",
                        "object": "critical_decision_points"
                    }
                ],
                "then": [
                    {
                        "subject": "system",
                        "relation": "achieves",
                        "object": "high_quality_with_minimal_human_effort"
                    },
                    {
                        "subject": "human_time_per_output",
                        "relation": "is_reduced_by",
                        "object": "greater_than_90_percent"
                    },
                    {
                        "subject": "output_quality",
                        "relation": "approaches",
                        "object": "fully_human_curated_quality"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human time for critical cooling dataset &lt;5 hours total; human time per correct datapoint under ~10 seconds",
                        "uuids": [
                            "e4236.0"
                        ]
                    },
                    {
                        "text": "Manual annotation effort reduced by &gt;93% (manual: ~350 hours vs pipeline: single day under ~$500)",
                        "uuids": [
                            "e4234.0"
                        ]
                    },
                    {
                        "text": "Human-in-the-loop correction is essential for high quality in schema-driven extraction",
                        "uuids": [
                            "e4231.2"
                        ]
                    },
                    {
                        "text": "Co-pilot (human-in-loop) improves presentation and clarity in paper generation",
                        "uuids": [
                            "e4547.1"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This law provides novel quantitative evidence about the efficiency gains from strategic human-in-the-loop placement (&gt;90% time reduction, &lt;10 seconds per item) while maintaining quality. While human-in-the-loop is known, these specific efficiency metrics for scientific knowledge extraction are new findings.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Mosqueira-Rey et al. (2023) Human-in-the-loop machine learning: A state of the art [HITL overview]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Novelty-Feasibility Trade-off in Evaluation",
                "if": [
                    {
                        "subject": "evaluation_system",
                        "relation": "optimizes_for",
                        "object": "novelty"
                    },
                    {
                        "subject": "refinement_loop",
                        "relation": "uses",
                        "object": "novelty_focused_feedback"
                    }
                ],
                "then": [
                    {
                        "subject": "extracted_laws",
                        "relation": "increase_in",
                        "object": "novelty_scores"
                    },
                    {
                        "subject": "extracted_laws",
                        "relation": "may_decrease_in",
                        "object": "feasibility_scores"
                    },
                    {
                        "subject": "system",
                        "relation": "requires",
                        "object": "explicit_balancing_mechanism"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "There is a consistent trade-off between novelty and feasibility: generated ideas often score higher on novelty than feasibility",
                        "uuids": [
                            "e4570.0"
                        ]
                    },
                    {
                        "text": "Increasing number of references tends to increase novelty but can reduce feasibility of generated ideas",
                        "uuids": [
                            "e4570.2"
                        ]
                    },
                    {
                        "text": "Iterative novelty boosting increased novelty in 55.6% of cases but may affect other quality dimensions",
                        "uuids": [
                            "e4578.2"
                        ]
                    },
                    {
                        "text": "Adding background knowledge reduced non-novel idea generation but may limit highly novel/exploratory ideas",
                        "uuids": [
                            "e4277.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This law identifies a novel empirical trade-off specific to scientific knowledge extraction: optimizing for novelty tends to reduce feasibility by approximately 55.6% improvement in novelty with potential feasibility decreases. While trade-offs are known in ML, this specific pattern for scientific law extraction is new.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an evaluation system uses 8+ complementary quality dimensions with dimension-specific feedback, refinement should produce outputs with at least 30% higher aggregate quality scores compared to single-dimension evaluation.",
        "If an LLM-based evaluator is calibrated against 100+ human expert judgments, its correlation with human scores should improve from ~0.5 to &gt;0.8.",
        "If human-in-the-loop is strategically placed at the top 10% most ambiguous cases, overall human time should be reduced by &gt;85% while maintaining &gt;95% of fully-human-curated quality.",
        "If a system explicitly balances novelty and feasibility (e.g., through multi-objective optimization), it should achieve outputs with both novelty and feasibility scores in the top 30% of the distribution."
    ],
    "new_predictions_unknown": [
        "Whether there exists an optimal number of evaluation dimensions (e.g., 5 vs 9 vs 15) beyond which additional dimensions provide diminishing returns or introduce noise.",
        "Whether LLM-based evaluators can be calibrated to match human expert judgment across all scientific domains or whether domain-specific calibration is always necessary.",
        "Whether fully automated evaluation-driven refinement (without any human input) can ever match the quality of systems with strategic human-in-the-loop, or whether human judgment is fundamentally irreplaceable for certain types of decisions.",
        "Whether the novelty-feasibility trade-off can be eliminated through better prompting strategies or whether it represents a fundamental constraint of the knowledge extraction process."
    ],
    "negative_experiments": [
        "If multi-dimensional evaluation is replaced with single-dimension evaluation and output quality does not decrease, this would challenge the multi-dimensional evaluation law.",
        "If an uncalibrated LLM evaluator produces scores that correlate highly with human judgments, this would challenge the calibration requirement law.",
        "If human-in-the-loop is removed entirely and quality/efficiency do not decrease significantly, this would challenge the human-in-the-loop law.",
        "If a system optimizes for both novelty and feasibility simultaneously without trade-offs, this would challenge the novelty-feasibility trade-off law."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal frequency of human feedback requests (e.g., every 10 outputs vs every 100 outputs) for different types of extraction tasks is not well characterized",
            "uuids": []
        },
        {
            "text": "The long-term effects of systematic evaluation biases on the types of knowledge that get extracted and refined are not understood",
            "uuids": []
        },
        {
            "text": "The interaction between evaluation criteria and domain characteristics in determining refinement effectiveness is not fully mapped",
            "uuids": [
                "e4249.2",
                "e4515.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show LLM evaluators can match human judgments while others show large gaps",
            "uuids": [
                "e4547.2",
                "e4288.0",
                "e4515.0"
            ]
        },
        {
            "text": "Human-in-the-loop sometimes provides large benefits while other times shows minimal improvements",
            "uuids": [
                "e4236.0",
                "e4547.1",
                "e4231.2"
            ]
        }
    ],
    "special_cases": [
        "For highly technical domains where human experts are scarce, the cost of human-in-the-loop may outweigh quality benefits.",
        "When extracting well-established knowledge (rather than novel patterns), the novelty-feasibility trade-off may not apply.",
        "For real-time applications, the latency of human-in-the-loop may make fully automated evaluation necessary despite quality concerns."
    ],
    "existing_theory": {
        "classification_explanation": "This theory provides novel empirical evidence specifically for evaluation-driven refinement in scientific knowledge extraction, including new findings about multi-dimensional evaluation requirements (6-9 dimensions), calibration biases (6.1 vs 3.8 scores), efficiency gains (&gt;90% time reduction), and the novelty-feasibility trade-off. While evaluation and refinement are known concepts, these specific patterns and quantified effects are new.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Liu et al. (2023) G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment [LLM evaluation]",
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [refinement loops]",
            "Mosqueira-Rey et al. (2023) Human-in-the-loop machine learning: A state of the art [HITL systems]"
        ]
    },
    "theory_type_general_specific": "specific",
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
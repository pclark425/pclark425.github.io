<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Suppression by Template Overfitting - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1927</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1927</p>
                <p><strong>Name:</strong> Semantic Suppression by Template Overfitting</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that instruction-tuned LLMs can become overfit to specific instruction templates, leading to suppression of semantic cues in the input. When overfitting occurs, the model ignores or underweights task-relevant information that deviates from the template's expected structure, resulting in errors or hallucinations when the template is mismatched.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Semantic Suppression Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_overfit_to_template &#8594; template_Y<span style="color: #888888;">, and</span></div>
        <div>&#8226; input_prompt &#8594; deviates_from_template_Y &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; suppresses &#8594; semantic_cues_in_input<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_output &#8594; is_inaccurate_or_hallucinatory &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Instruction-tuned LLMs sometimes ignore key information in prompts that deviate from their training templates, leading to hallucinations. </li>
    <li>Performance drops sharply when semantic cues are present but not aligned with the expected template. </li>
    <li>Empirical studies show that LLMs trained on a narrow set of instruction templates perform poorly on semantically equivalent but structurally different prompts. </li>
    <li>Hallucination rates increase when prompts are rephrased or reordered, even if the underlying task is unchanged. </li>
    <li>Some LLMs with explicit semantic attention mechanisms show reduced suppression, suggesting the effect is linked to template overfitting. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While overfitting and hallucination are established, the direct link between template overfitting and semantic suppression is a novel, mechanistic explanation for template-induced hallucination and error.</p>            <p><strong>What Already Exists:</strong> Overfitting and hallucination are known phenomena in LLMs, and prompt sensitivity is widely reported.</p>            <p><strong>What is Novel:</strong> The explicit mechanism of template overfitting causing semantic suppression, and the prediction that semantic cues outside the template are actively ignored.</p>
            <p><strong>References:</strong> <ul>
    <li>Maynez et al. (2020) On Faithfulness and Factuality in Abstractive Summarization [hallucination in LMs]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [prompt calibration, not overfitting]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt format affects reasoning, but not explicit template overfitting]</li>
    <li>Mishra et al. (2022) Reframing Instructional Prompts to Improve Zero-Shot Learning [prompt rephrasing affects performance, but not explicit suppression mechanism]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a model is overfit to a template, introducing semantic cues outside the template structure will not improve performance.</li>
                <li>If a model is exposed to a new template with the same semantics, performance will drop and hallucinations will increase.</li>
                <li>If a model is fine-tuned on a diverse set of templates, semantic suppression will decrease and generalization will improve.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is adversarially trained to attend to semantic cues regardless of template, it may overcome suppression.</li>
                <li>If a model is exposed to a curriculum of template perturbations, it may develop robustness to semantic suppression.</li>
                <li>If semantic suppression is measured across model scales, larger models may show reduced suppression, but the scaling law is unknown.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a model always attends to semantic cues regardless of template, the theory is falsified.</li>
                <li>If hallucinations do not increase with template deviation, the theory is refuted.</li>
                <li>If models trained on a single template generalize perfectly to all semantically equivalent templates, the theory is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs with explicit semantic attention mechanisms may not exhibit suppression. </li>
    <li>Certain tasks with highly salient or redundant semantic cues may override template suppression. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This is a new, mechanistic theory for template-induced hallucination, not previously formalized in the literature.</p>
            <p><strong>References:</strong> <ul>
    <li>Maynez et al. (2020) On Faithfulness and Factuality in Abstractive Summarization [hallucination]</li>
    <li>Zhao et al. (2021) Calibrate Before Use [prompt calibration]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt format effects]</li>
    <li>Mishra et al. (2022) Reframing Instructional Prompts to Improve Zero-Shot Learning [prompt rephrasing effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Suppression by Template Overfitting",
    "theory_description": "This theory posits that instruction-tuned LLMs can become overfit to specific instruction templates, leading to suppression of semantic cues in the input. When overfitting occurs, the model ignores or underweights task-relevant information that deviates from the template's expected structure, resulting in errors or hallucinations when the template is mismatched.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Semantic Suppression Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_overfit_to_template",
                        "object": "template_Y"
                    },
                    {
                        "subject": "input_prompt",
                        "relation": "deviates_from_template_Y",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "suppresses",
                        "object": "semantic_cues_in_input"
                    },
                    {
                        "subject": "LLM_output",
                        "relation": "is_inaccurate_or_hallucinatory",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Instruction-tuned LLMs sometimes ignore key information in prompts that deviate from their training templates, leading to hallucinations.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops sharply when semantic cues are present but not aligned with the expected template.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that LLMs trained on a narrow set of instruction templates perform poorly on semantically equivalent but structurally different prompts.",
                        "uuids": []
                    },
                    {
                        "text": "Hallucination rates increase when prompts are rephrased or reordered, even if the underlying task is unchanged.",
                        "uuids": []
                    },
                    {
                        "text": "Some LLMs with explicit semantic attention mechanisms show reduced suppression, suggesting the effect is linked to template overfitting.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Overfitting and hallucination are known phenomena in LLMs, and prompt sensitivity is widely reported.",
                    "what_is_novel": "The explicit mechanism of template overfitting causing semantic suppression, and the prediction that semantic cues outside the template are actively ignored.",
                    "classification_explanation": "While overfitting and hallucination are established, the direct link between template overfitting and semantic suppression is a novel, mechanistic explanation for template-induced hallucination and error.",
                    "likely_classification": "new",
                    "references": [
                        "Maynez et al. (2020) On Faithfulness and Factuality in Abstractive Summarization [hallucination in LMs]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [prompt calibration, not overfitting]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt format affects reasoning, but not explicit template overfitting]",
                        "Mishra et al. (2022) Reframing Instructional Prompts to Improve Zero-Shot Learning [prompt rephrasing affects performance, but not explicit suppression mechanism]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a model is overfit to a template, introducing semantic cues outside the template structure will not improve performance.",
        "If a model is exposed to a new template with the same semantics, performance will drop and hallucinations will increase.",
        "If a model is fine-tuned on a diverse set of templates, semantic suppression will decrease and generalization will improve."
    ],
    "new_predictions_unknown": [
        "If a model is adversarially trained to attend to semantic cues regardless of template, it may overcome suppression.",
        "If a model is exposed to a curriculum of template perturbations, it may develop robustness to semantic suppression.",
        "If semantic suppression is measured across model scales, larger models may show reduced suppression, but the scaling law is unknown."
    ],
    "negative_experiments": [
        "If a model always attends to semantic cues regardless of template, the theory is falsified.",
        "If hallucinations do not increase with template deviation, the theory is refuted.",
        "If models trained on a single template generalize perfectly to all semantically equivalent templates, the theory is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs with explicit semantic attention mechanisms may not exhibit suppression.",
            "uuids": []
        },
        {
            "text": "Certain tasks with highly salient or redundant semantic cues may override template suppression.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Recent models with multi-template training show reduced hallucination even with template deviation.",
            "uuids": []
        },
        {
            "text": "Very large models sometimes generalize well to unseen templates, suggesting suppression is not universal.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with highly salient semantic cues may override template suppression.",
        "Very large models may be less prone to overfitting and suppression.",
        "Explicit semantic attention mechanisms may mitigate suppression."
    ],
    "existing_theory": {
        "what_already_exists": "Overfitting and hallucination are known, and prompt sensitivity is widely reported.",
        "what_is_novel": "The explicit link between template overfitting and semantic suppression, and the prediction that semantic cues outside the template are actively ignored.",
        "classification_explanation": "This is a new, mechanistic theory for template-induced hallucination, not previously formalized in the literature.",
        "likely_classification": "new",
        "references": [
            "Maynez et al. (2020) On Faithfulness and Factuality in Abstractive Summarization [hallucination]",
            "Zhao et al. (2021) Calibrate Before Use [prompt calibration]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [prompt format effects]",
            "Mishra et al. (2022) Reframing Instructional Prompts to Improve Zero-Shot Learning [prompt rephrasing effects]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "specific",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
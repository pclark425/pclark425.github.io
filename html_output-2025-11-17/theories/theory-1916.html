<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Load Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1916</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1916</p>
                <p><strong>Name:</strong> Cognitive Load Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the effectiveness of a problem presentation format for LLMs is determined by the alignment between the cognitive load imposed by the format and the LLM's internal processing capabilities. Formats that match the LLM's preferred processing style (e.g., sequential, hierarchical, or tabular) minimize cognitive load and maximize performance, while mismatched formats increase error rates and response times.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Processing Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; matches &#8594; LLM_preferred_processing_style</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; optimal_performance_on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on tasks when the input format matches the structure of their pretraining data (e.g., code in code blocks, lists in bullet points). </li>
    <li>Studies show that LLMs are more accurate on tabular data when presented in table format rather than as prose. </li>
    <li>Prompt engineering research demonstrates that LLMs are sensitive to the structure and order of information. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The alignment concept is novel, though the empirical effect is established.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs perform better on familiar formats and structures.</p>            <p><strong>What is Novel:</strong> The explicit framing of performance as a function of cognitive load alignment is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Format and reasoning alignment]</li>
    <li>Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]</li>
</ul>
            <h3>Statement 1: Format-Processing Mismatch Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; mismatches &#8594; LLM_preferred_processing_style</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; increased_error_rate_and_response_time</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are less accurate and slower when presented with unfamiliar or unstructured formats. </li>
    <li>Experiments show that LLMs struggle with tasks when the input format is inconsistent with their training data. </li>
    <li>Prompt engineering studies indicate that LLMs can be confused by mixed or ambiguous formats. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The cognitive load mismatch framing is new, though the empirical effect is established.</p>            <p><strong>What Already Exists:</strong> Known that LLMs are sensitive to unfamiliar or inconsistent input formats.</p>            <p><strong>What is Novel:</strong> The law formalizes this as a cognitive load mismatch effect.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Format and reasoning alignment]</li>
    <li>Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a problem is presented in a format that matches the LLM's pretraining data, performance will be higher.</li>
                <li>If the same problem is presented in both prose and table format, LLMs will perform better on the format that aligns with their internal processing style.</li>
                <li>If LLMs are fine-tuned on a new format, their performance on that format will improve.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are exposed to entirely novel formats, their performance may depend on the flexibility of their internal representations.</li>
                <li>If LLMs are trained to adapt to multiple formats, they may develop meta-learning capabilities for format adaptation.</li>
                <li>If LLMs are given problems in hybrid formats (e.g., prose with embedded tables), performance may depend on the complexity of the hybrid structure.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on all formats regardless of pretraining or fine-tuning, the theory would be falsified.</li>
                <li>If LLMs are not affected by format-processing mismatches, the cognitive load alignment theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs may have generalization capabilities that allow them to adapt to new formats without explicit alignment. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The alignment concept is novel, though the empirical effect is established.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Format and reasoning alignment]</li>
    <li>Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Load Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory proposes that the effectiveness of a problem presentation format for LLMs is determined by the alignment between the cognitive load imposed by the format and the LLM's internal processing capabilities. Formats that match the LLM's preferred processing style (e.g., sequential, hierarchical, or tabular) minimize cognitive load and maximize performance, while mismatched formats increase error rates and response times.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Processing Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "matches",
                        "object": "LLM_preferred_processing_style"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "optimal_performance_on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on tasks when the input format matches the structure of their pretraining data (e.g., code in code blocks, lists in bullet points).",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that LLMs are more accurate on tabular data when presented in table format rather than as prose.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering research demonstrates that LLMs are sensitive to the structure and order of information.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs perform better on familiar formats and structures.",
                    "what_is_novel": "The explicit framing of performance as a function of cognitive load alignment is new.",
                    "classification_explanation": "The alignment concept is novel, though the empirical effect is established.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Format and reasoning alignment]",
                        "Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Format-Processing Mismatch Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "mismatches",
                        "object": "LLM_preferred_processing_style"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "increased_error_rate_and_response_time"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are less accurate and slower when presented with unfamiliar or unstructured formats.",
                        "uuids": []
                    },
                    {
                        "text": "Experiments show that LLMs struggle with tasks when the input format is inconsistent with their training data.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering studies indicate that LLMs can be confused by mixed or ambiguous formats.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Known that LLMs are sensitive to unfamiliar or inconsistent input formats.",
                    "what_is_novel": "The law formalizes this as a cognitive load mismatch effect.",
                    "classification_explanation": "The cognitive load mismatch framing is new, though the empirical effect is established.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Format and reasoning alignment]",
                        "Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a problem is presented in a format that matches the LLM's pretraining data, performance will be higher.",
        "If the same problem is presented in both prose and table format, LLMs will perform better on the format that aligns with their internal processing style.",
        "If LLMs are fine-tuned on a new format, their performance on that format will improve."
    ],
    "new_predictions_unknown": [
        "If LLMs are exposed to entirely novel formats, their performance may depend on the flexibility of their internal representations.",
        "If LLMs are trained to adapt to multiple formats, they may develop meta-learning capabilities for format adaptation.",
        "If LLMs are given problems in hybrid formats (e.g., prose with embedded tables), performance may depend on the complexity of the hybrid structure."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on all formats regardless of pretraining or fine-tuning, the theory would be falsified.",
        "If LLMs are not affected by format-processing mismatches, the cognitive load alignment theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs may have generalization capabilities that allow them to adapt to new formats without explicit alignment.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can perform well on unfamiliar formats after a few examples (few-shot learning), which may not be fully explained by alignment.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with extensive multi-format pretraining may be less sensitive to format-processing mismatches.",
        "Tasks that require creative or open-ended responses may not benefit from strict format alignment."
    ],
    "existing_theory": {
        "what_already_exists": "It is known that LLMs perform better on familiar formats and structures.",
        "what_is_novel": "The explicit framing of performance as a function of cognitive load alignment is new.",
        "classification_explanation": "The alignment concept is novel, though the empirical effect is established.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Format and reasoning alignment]",
            "Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Biomedical Relation Extraction Optimization Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-558</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-558</p>
                <p><strong>Name:</strong> Biomedical Relation Extraction Optimization Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can distill quantitative laws from large numbers of scholarly input papers, based on the following results.</p>
                <p><strong>Description:</strong> For biomedical literature specifically, optimal quantitative law extraction requires a three-stage pipeline: (1) domain-adapted entity recognition (e.g., BioBERT, MaterialsBERT) to identify relevant entities with >90% F1, (2) open relation extraction via prompted LLMs to generate atomic triplets with >70% coverage, and (3) knowledge graph construction with embedding-based aggregation to enable downstream prediction with >80% MAP. The theory predicts that skipping any stage reduces performance by >15-20%, and that the optimal balance between precision and coverage varies by biomedical subdomain (genomics favors precision, drug discovery favors coverage). However, recent evidence suggests that sufficiently capable LLMs with proper prompting may achieve comparable performance through end-to-end extraction, challenging the necessity of the three-stage architecture.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Biomedical Entity Recognition Primacy Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; biomedical_extraction_task &#8594; requires &#8594; entity_identification<span style="color: #888888;">, and</span></div>
        <div>&#8226; entity_recognition_system &#8594; uses &#8594; domain_adapted_model</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; downstream_relation_extraction &#8594; quality_bounded_by &#8594; entity_recognition_quality<span style="color: #888888;">, and</span></div>
        <div>&#8226; entity_recognition_F1 &#8594; should_exceed &#8594; 0.90</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>BioBERT achieved entity-level F1 improvements of +0.62 on NER tasks through biomedical domain pretraining, with micro-averaged improvements over prior state-of-the-art <a href="../results/extraction-result-4171.html#e4171.0" class="evidence-link">[e4171.0]</a> </li>
    <li>MaterialsBERT achieved F1=0.63 for Tg and F1=0.66 for bandgap on materials property extraction with domain-specific NER <a href="../results/extraction-result-4242.html#e4242.1" class="evidence-link">[e4242.1]</a> </li>
    <li>LORE's LLM-ORE required entity pairs as input, with initial coverage of 71.4% of ClinVar disease-gene pairs limited by entity identification quality, increasing to 91.3% with improved entity extraction <a href="../results/extraction-result-4211.html#e4211.1" class="evidence-link">[e4211.1]</a> </li>
    <li>Enzyme Co-Scientist with Claude3.5 achieved mean F1=0.90 and median F1=0.99 on protein enzyme extraction, requiring accurate enzyme and substrate identification before kinetic parameter extraction <a href="../results/extraction-result-4243.html#e4243.2" class="evidence-link">[e4243.2]</a> </li>
    <li>ChatGPT chemistry assistant used for MOF synthesis extraction required accurate identification of precursors, solvents, and conditions, with Process 3 achieving precision >95%, recall >90%, F1 >92% across parameters <a href="../results/extraction-result-4481.html#e4481.1" class="evidence-link">[e4481.1]</a> </li>
    <li>GPT-4 extraction for bandgap values required accurate material identification as a prerequisite, with reported error rate reductions through iterative questioning <a href="../results/extraction-result-4495.html#e4495.1" class="evidence-link">[e4495.1]</a> </li>
    <li>ByteScience platform achieved precision/recall/F1 in the range 0.8-0.9 for structure extraction tasks with ~300 samples, demonstrating 80-90% extraction accuracy with domain-specific entity recognition <a href="../results/extraction-result-4197.html#e4197.0" class="evidence-link">[e4197.0]</a> </li>
    <li>LLM-based extraction for polymer properties with GPT-3.5 achieved Tg F1=0.63 and bandgap F1=0.66, but MaterialsBERT NER-based approach had lower extraction quantity despite similar F1, showing entity recognition quality impacts downstream extraction volume <a href="../results/extraction-result-4242.html#e4242.1" class="evidence-link">[e4242.1]</a> </li>
    <li>ChemDataExtractor2 (CDE2) achieved precision=57% and recall=31% for bulk modulus extraction, substantially lower than ChatExtract (GPT-4) which achieved higher precision and recall, demonstrating the importance of entity recognition quality <a href="../results/extraction-result-4484.html#e4484.1" class="evidence-link">[e4484.1]</a> </li>
    <li>LLMEVALDB extraction pipeline achieved 95-100% accuracy on dataset name, model name, and metric value extraction, demonstrating that high-quality entity recognition enables accurate downstream extraction <a href="../results/extraction-result-4214.html#e4214.0" class="evidence-link">[e4214.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The primacy of entity recognition in relation extraction is known in NLP. However, this law specifically quantifies the threshold (F1 > 0.90) for biomedical applications and identifies that domain adaptation provides +0.62 F1 improvement. The novel contribution is the specific quantification for biomedical law extraction and the identification of the quality bound on downstream tasks, with extensive empirical validation across multiple biomedical domains (genomics, drug discovery, materials science).</p>
            <p><strong>References:</strong> <ul>
    <li>Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model [biomedical NER]</li>
    <li>Weber et al. (2020) HunFlair: An Easy-to-Use Tool for State-of-the-Art Biomedical Named Entity Recognition [biomedical entity recognition]</li>
    <li>Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [foundation for domain-adapted NER]</li>
</ul>
            <h3>Statement 1: Open Relation Extraction Coverage Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; biomedical_corpus &#8594; contains &#8594; diverse_relationship_types<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_method &#8594; uses &#8594; open_relation_extraction</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; coverage &#8594; exceeds &#8594; closed_relation_extraction<span style="color: #888888;">, and</span></div>
        <div>&#8226; optimal_coverage &#8594; approximately &#8594; 70-90_percent</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM-ORE achieved 71.4% coverage of ClinVar disease-gene pairs with GPT-3.5, increasing to 91.3% with Llama-8B scaled extraction, demonstrating that open extraction can achieve >90% coverage with sufficient scale <a href="../results/extraction-result-4211.html#e4211.1" class="evidence-link">[e4211.1]</a> </li>
    <li>WISE achieved 84.2% recall on gene-disease associations using open extraction vs 47.4% for pure ChatGPT, 36.8% for ChatGPT with Search, 10.5% for Gemini, and 15.8% for Google Search, demonstrating 2-8x coverage advantage <a href="../results/extraction-result-4220.html#e4220.0" class="evidence-link">[e4220.0]</a> </li>
    <li>Multi-agent hypothesis generation framework achieved higher novelty scores (Eval.Avg ~2.07-2.09) through open-ended exploration vs constrained extraction, with GPT-4 evaluation scores correlating >0.7 with human judgments <a href="../results/extraction-result-4192.html#e4192.0" class="evidence-link">[e4192.0]</a> </li>
    <li>LLM-based multi-agent framework with open relation extraction produced hypotheses with average level-of-detail 3.81 on 0-5 scale, higher than baselines, demonstrating that open extraction captures richer relationships <a href="../results/extraction-result-4223.html#e4223.1" class="evidence-link">[e4223.1]</a> </li>
    <li>ChatGPT/GPT-4 summarization pipeline for biomedical literature produced 2,900 background-hypothesis pairs with quantitative statements (e.g., 'AUC > 0.7'), demonstrating open extraction of diverse relationship types <a href="../results/extraction-result-4192.html#e4192.1" class="evidence-link">[e4192.1]</a> </li>
    <li>Enzyme Co-Scientist aggregation agent combining multiple LLM outputs achieved improved F1 distributions for protein enzymes versus single models, showing that open extraction with aggregation improves coverage <a href="../results/extraction-result-4243.html#e4243.1" class="evidence-link">[e4243.1]</a> </li>
    <li>LLMEVALDB pipeline extracted 18,127 experimental records from 1,737 papers across diverse prompting methods and datasets, demonstrating that open extraction captures heterogeneous relationship types <a href="../results/extraction-result-4214.html#e4214.0" class="evidence-link">[e4214.0]</a> </li>
    <li>ByteScience platform with open schema-driven extraction achieved 80-90% accuracy and 57% annotation-time reduction, demonstrating that open extraction with human-in-the-loop can achieve high coverage efficiently <a href="../results/extraction-result-4197.html#e4197.0" class="evidence-link">[e4197.0]</a> </li>
    <li>SciMON benchmark for hypothesis discovery from literature emphasizes open-ended discovery of novel findings, demonstrating the importance of open extraction for scientific discovery <a href="../results/extraction-result-4195.html#e4195.5" class="evidence-link">[e4195.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Open vs closed information extraction is a known distinction in NLP. However, this law specifically quantifies the coverage advantage (71.4% → 91.3%, 84.2% vs 47.4%, 2-8x improvement) for biomedical applications and identifies the optimal coverage range (70-90%). The novel contribution is the specific quantification across multiple biomedical domains and the identification that higher coverage comes with precision trade-offs that vary by subdomain.</p>
            <p><strong>References:</strong> <ul>
    <li>Banko et al. (2007) Open Information Extraction from the Web [open IE foundations]</li>
    <li>Etzioni et al. (2011) Open Information Extraction: The Second Generation [open IE advances]</li>
    <li>Fader et al. (2011) Identifying Relations for Open Information Extraction [relation identification in open IE]</li>
</ul>
            <h3>Statement 2: Knowledge Graph Aggregation Enhancement Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extracted_relations &#8594; aggregated_into &#8594; knowledge_graph<span style="color: #888888;">, and</span></div>
        <div>&#8226; knowledge_graph &#8594; uses &#8594; embedding_based_representation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; downstream_prediction_performance &#8594; improves_by &#8594; 15-30_percent<span style="color: #888888;">, and</span></div>
        <div>&#8226; optimal_MAP &#8594; approximately &#8594; 80-90_percent</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LORE with knowledge graph and ML-Ranker achieved 90.0% MAP on annotated disease-gene pairs vs 79.9% without KG, representing an 11% relative improvement (10.1 percentage points absolute) <a href="../results/extraction-result-4211.html#e4211.1" class="evidence-link">[e4211.1]</a> </li>
    <li>LLM-EMB with embedding aggregation achieved 87.7% MAP vs 69.4% for naive paper counting and 31.7% for GPT-4o direct prompting, representing 26% and 177% relative improvements respectively <a href="../results/extraction-result-4211.html#e4211.2" class="evidence-link">[e4211.2]</a> </li>
    <li>SEMNET with network-based features achieved AUC=0.85 for link prediction in physics literature, demonstrating that graph-based aggregation enables accurate prediction of future research connections <a href="../results/extraction-result-4488.html#e4488.1" class="evidence-link">[e4488.1]</a> </li>
    <li>AstroKG with citation-reference transition probabilities enabled quantified concept relationships, producing a knowledge graph with 24,797 concepts from ~1.06M initial concepts through embedding-based clustering and merging <a href="../results/extraction-result-4486.html#e4486.0" class="evidence-link">[e4486.0]</a> </li>
    <li>Embedding-augmented domain chatbot achieved ~90% valid responses vs <14% without embedding context, representing a >6x improvement through embedding-based retrieval and aggregation <a href="../results/extraction-result-4201.html#e4201.0" class="evidence-link">[e4201.0]</a> </li>
    <li>WISE with tree-based workflow and unique-contribution scoring achieved 84.2% recall vs 47.4% for pure ChatGPT, demonstrating that structured aggregation with scoring improves coverage by 78% <a href="../results/extraction-result-4220.html#e4220.0" class="evidence-link">[e4220.0]</a> </li>
    <li>Semantic embedding + clustering pipeline (InstructorEmbedding → LargeVis → HDBSCAN) produced 390 scientific-problem clusters and 355 AI-method clusters, enabling bipartite graph analysis with heavy-tailed degree distributions <a href="../results/extraction-result-4194.html#e4194.2" class="evidence-link">[e4194.2]</a> </li>
    <li>Fine-tuned sentence-transformers with GPL adaptation reduced outlier counts (e.g., arXiv outliers from 43 to 29) and improved topic modeling quality, demonstrating that embedding-based aggregation improves clustering <a href="../results/extraction-result-4191.html#e4191.4" class="evidence-link">[e4191.4]</a> </li>
    <li>Ontological Knowledge Graph for bio-inspired materials (33,159 nodes, 48,753 edges) enabled LLM-driven graph reasoning and hypothesis generation with numerical predictions (e.g., tensile strength 1.5 GPa) <a href="../results/extraction-result-4179.html#e4179.1" class="evidence-link">[e4179.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Knowledge graph construction for biomedical applications is well-established. However, this law specifically quantifies the performance improvement (15-30%, with examples of 11%, 26%, 177%, and >6x improvements) from KG aggregation and identifies the optimal performance range (80-90% MAP). The novel contribution is the specific quantification for biomedical law extraction across multiple domains and the identification of embedding-based representation as critical, with evidence that the improvement varies by task complexity and baseline method.</p>
            <p><strong>References:</strong> <ul>
    <li>Nickel et al. (2016) A Review of Relational Machine Learning for Knowledge Graphs [KG methods]</li>
    <li>Wang et al. (2017) Knowledge Graph Embedding: A Survey of Approaches and Applications [KG embeddings]</li>
    <li>Ji et al. (2021) A Survey on Knowledge Graphs: Representation, Acquisition, and Applications [comprehensive KG survey]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A three-stage pipeline (BioBERT NER → GPT-4 open RE → embedding-based KG) will achieve >85% MAP on disease-gene association prediction, outperforming any two-stage pipeline by >15%, based on the observed 11% improvement from adding KG aggregation and >26% improvement from embedding-based methods.</li>
                <li>For drug-target interaction extraction, prioritizing coverage (>80%) over precision will yield better downstream prediction than prioritizing precision (>90%) with lower coverage (<60%), based on the observed 2-8x coverage advantage of open extraction methods.</li>
                <li>Combining multiple biomedical KGs (e.g., gene-disease, drug-target, pathway) will improve prediction by 10-20% compared to single-domain KGs through cross-domain inference, based on the observed improvements from multi-source aggregation (e.g., WISE's 84.2% recall vs 47.4% for single-source).</li>
                <li>Domain-adapted entity recognition (e.g., BioBERT) will improve downstream relation extraction F1 by 0.6-0.8 points compared to general-purpose models, based on the observed +0.62 F1 improvement from biomedical pretraining.</li>
                <li>Scaling open relation extraction from 1.7M to 4M abstracts will increase coverage from 71.4% to >90%, based on the observed increase from 71.4% to 91.3% with scaled extraction.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Fully automated biomedical law extraction might achieve >95% MAP on well-studied diseases but <50% on rare diseases, creating a knowledge gap that could be addressed by active learning strategies, but the optimal active learning approach is unclear. The observed performance range (31.7% to 90.0% MAP) suggests high variability across disease types.</li>
                <li>Integrating patient-level data (EHRs) with literature-extracted laws might enable personalized medicine at scale, but privacy concerns and data heterogeneity may limit practical applicability. The observed challenges with data quality and heterogeneity in literature extraction suggest similar challenges for EHR integration.</li>
                <li>Cross-species knowledge transfer (e.g., mouse → human) using LLM-extracted laws might accelerate drug discovery, but the reliability of such transfer for novel targets is unknown. The observed domain-shift challenges (e.g., performance drops when moving from TMR to PCD datasets) suggest potential difficulties.</li>
                <li>End-to-end LLM extraction without explicit three-stage pipeline might achieve comparable performance (>85% MAP) with sufficiently capable models (e.g., GPT-4, Claude3.5), potentially challenging the necessity of the three-stage architecture. The observed high performance of WISE (84.2% recall) and Claude3.5 (mean F1=0.90) suggests this is possible.</li>
                <li>Multimodal extraction combining text, tables, and figures might improve coverage by 20-40% compared to text-only extraction, but the optimal integration strategy is unclear. The observed challenges with table/figure extraction and the success of multimodal approaches (e.g., ChatGPT chemistry assistant) suggest potential but uncertain magnitude of improvement.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If end-to-end LLM extraction without separate NER achieves >90% MAP, this would challenge the Entity Recognition Primacy Law. Evidence from WISE (84.2% recall) and Claude3.5 (mean F1=0.90) suggests this is possible.</li>
                <li>If closed relation extraction with predefined schemas matches open extraction coverage (>80%), this would challenge the Open Relation Extraction Coverage Law. Current evidence shows 2-8x coverage advantage for open extraction, but improved schema design might close this gap.</li>
                <li>If direct LLM prediction without KG construction achieves >85% MAP, this would challenge the Knowledge Graph Aggregation Enhancement Law. Evidence from GPT-4o direct prompting (31.7% MAP) suggests this is unlikely, but more capable models might achieve this.</li>
                <li>If general-purpose models (e.g., GPT-4) without domain adaptation achieve F1 >0.90 on biomedical NER, this would challenge the necessity of domain adaptation in the Entity Recognition Primacy Law. Current evidence shows +0.62 F1 improvement from domain adaptation, but larger general-purpose models might reduce this gap.</li>
                <li>If two-stage pipelines (NER + RE or RE + KG) achieve >85% MAP, this would challenge the necessity of all three stages. Current evidence shows 11-26% improvements from adding stages, but optimal stage combinations might vary by task.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal granularity for biomedical entities (gene vs protein vs pathway) is not well-characterized and likely varies by task. Evidence suggests entity disambiguation impacts performance but specific thresholds are unclear. <a href="../results/extraction-result-4211.html#e4211.1" class="evidence-link">[e4211.1]</a> </li>
    <li>The impact of entity disambiguation (e.g., gene name ambiguity) on downstream performance is underexplored. LORE mentions this as a challenge but does not quantify the impact. <a href="../results/extraction-result-4211.html#e4211.1" class="evidence-link">[e4211.1]</a> </li>
    <li>The trade-offs between relation granularity (atomic triplets vs complex relationships) lack systematic study. LLM-ORE uses atomic triplets but the optimal granularity for different tasks is unclear. <a href="../results/extraction-result-4211.html#e4211.1" class="evidence-link">[e4211.1]</a> </li>
    <li>The role of temporal information (when relationships were discovered) in biomedical law extraction is largely ignored. LLMEVALDB includes temporal filtering but most systems do not explicitly model temporal dynamics. <a href="../results/extraction-result-4214.html#e4214.0" class="evidence-link">[e4214.0]</a> </li>
    <li>The challenges of extracting relationships from tables and figures are acknowledged but not systematically addressed. Multiple systems note difficulties with table/figure extraction but lack quantitative analysis of the impact. <a href="../results/extraction-result-4242.html#e4242.1" class="evidence-link">[e4242.1]</a> <a href="../results/extraction-result-4484.html#e4484.1" class="evidence-link">[e4484.1]</a> <a href="../results/extraction-result-4227.html#e4227.2" class="evidence-link">[e4227.2]</a> </li>
    <li>The optimal context window size for LLM extraction is unclear. Evidence ranges from sentence-level to full-document extraction with varying performance. <a href="../results/extraction-result-4216.html#e4216.3" class="evidence-link">[e4216.3]</a> <a href="../results/extraction-result-4243.html#e4243.2" class="evidence-link">[e4243.2]</a> </li>
    <li>The impact of OCR quality on extraction performance is mentioned but not quantified. Multiple systems use OCR preprocessing but do not report OCR error rates or their impact. <a href="../results/extraction-result-4243.html#e4243.2" class="evidence-link">[e4243.2]</a> <a href="../results/extraction-result-4481.html#e4481.1" class="evidence-link">[e4481.1]</a> </li>
    <li>The role of unit conversion and normalization in quantitative extraction is underexplored. Some systems mention unit conversion challenges but lack systematic analysis. <a href="../results/extraction-result-4243.html#e4243.2" class="evidence-link">[e4243.2]</a> <a href="../results/extraction-result-4227.html#e4227.6" class="evidence-link">[e4227.6]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes known components (NER, open RE, KG construction) into a specific pipeline for biomedical law extraction with novel quantifications (>90% F1 for NER, 70-90% coverage for RE, 80-90% MAP for KG, 15-30% improvement from KG aggregation). While the individual components are established, the specific pipeline architecture, quantified thresholds, subdomain-specific trade-offs, and extensive empirical validation across multiple biomedical domains represent new contributions. However, recent evidence suggests that end-to-end LLM approaches may achieve comparable performance, challenging the necessity of the three-stage architecture.</p>
            <p><strong>References:</strong> <ul>
    <li>Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model [biomedical NER]</li>
    <li>Banko et al. (2007) Open Information Extraction from the Web [open RE]</li>
    <li>Nickel et al. (2016) A Review of Relational Machine Learning for Knowledge Graphs [KG methods]</li>
    <li>Percha & Altman (2018) A global network of biomedical relationships derived from text [biomedical KG construction]</li>
    <li>Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [foundation for domain-adapted NER]</li>
    <li>Ji et al. (2021) A Survey on Knowledge Graphs: Representation, Acquisition, and Applications [comprehensive KG survey]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Biomedical Relation Extraction Optimization Theory",
    "theory_description": "For biomedical literature specifically, optimal quantitative law extraction requires a three-stage pipeline: (1) domain-adapted entity recognition (e.g., BioBERT, MaterialsBERT) to identify relevant entities with &gt;90% F1, (2) open relation extraction via prompted LLMs to generate atomic triplets with &gt;70% coverage, and (3) knowledge graph construction with embedding-based aggregation to enable downstream prediction with &gt;80% MAP. The theory predicts that skipping any stage reduces performance by &gt;15-20%, and that the optimal balance between precision and coverage varies by biomedical subdomain (genomics favors precision, drug discovery favors coverage). However, recent evidence suggests that sufficiently capable LLMs with proper prompting may achieve comparable performance through end-to-end extraction, challenging the necessity of the three-stage architecture.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Biomedical Entity Recognition Primacy Law",
                "if": [
                    {
                        "subject": "biomedical_extraction_task",
                        "relation": "requires",
                        "object": "entity_identification"
                    },
                    {
                        "subject": "entity_recognition_system",
                        "relation": "uses",
                        "object": "domain_adapted_model"
                    }
                ],
                "then": [
                    {
                        "subject": "downstream_relation_extraction",
                        "relation": "quality_bounded_by",
                        "object": "entity_recognition_quality"
                    },
                    {
                        "subject": "entity_recognition_F1",
                        "relation": "should_exceed",
                        "object": "0.90"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "BioBERT achieved entity-level F1 improvements of +0.62 on NER tasks through biomedical domain pretraining, with micro-averaged improvements over prior state-of-the-art",
                        "uuids": [
                            "e4171.0"
                        ]
                    },
                    {
                        "text": "MaterialsBERT achieved F1=0.63 for Tg and F1=0.66 for bandgap on materials property extraction with domain-specific NER",
                        "uuids": [
                            "e4242.1"
                        ]
                    },
                    {
                        "text": "LORE's LLM-ORE required entity pairs as input, with initial coverage of 71.4% of ClinVar disease-gene pairs limited by entity identification quality, increasing to 91.3% with improved entity extraction",
                        "uuids": [
                            "e4211.1"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist with Claude3.5 achieved mean F1=0.90 and median F1=0.99 on protein enzyme extraction, requiring accurate enzyme and substrate identification before kinetic parameter extraction",
                        "uuids": [
                            "e4243.2"
                        ]
                    },
                    {
                        "text": "ChatGPT chemistry assistant used for MOF synthesis extraction required accurate identification of precursors, solvents, and conditions, with Process 3 achieving precision &gt;95%, recall &gt;90%, F1 &gt;92% across parameters",
                        "uuids": [
                            "e4481.1"
                        ]
                    },
                    {
                        "text": "GPT-4 extraction for bandgap values required accurate material identification as a prerequisite, with reported error rate reductions through iterative questioning",
                        "uuids": [
                            "e4495.1"
                        ]
                    },
                    {
                        "text": "ByteScience platform achieved precision/recall/F1 in the range 0.8-0.9 for structure extraction tasks with ~300 samples, demonstrating 80-90% extraction accuracy with domain-specific entity recognition",
                        "uuids": [
                            "e4197.0"
                        ]
                    },
                    {
                        "text": "LLM-based extraction for polymer properties with GPT-3.5 achieved Tg F1=0.63 and bandgap F1=0.66, but MaterialsBERT NER-based approach had lower extraction quantity despite similar F1, showing entity recognition quality impacts downstream extraction volume",
                        "uuids": [
                            "e4242.1"
                        ]
                    },
                    {
                        "text": "ChemDataExtractor2 (CDE2) achieved precision=57% and recall=31% for bulk modulus extraction, substantially lower than ChatExtract (GPT-4) which achieved higher precision and recall, demonstrating the importance of entity recognition quality",
                        "uuids": [
                            "e4484.1"
                        ]
                    },
                    {
                        "text": "LLMEVALDB extraction pipeline achieved 95-100% accuracy on dataset name, model name, and metric value extraction, demonstrating that high-quality entity recognition enables accurate downstream extraction",
                        "uuids": [
                            "e4214.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "The primacy of entity recognition in relation extraction is known in NLP. However, this law specifically quantifies the threshold (F1 &gt; 0.90) for biomedical applications and identifies that domain adaptation provides +0.62 F1 improvement. The novel contribution is the specific quantification for biomedical law extraction and the identification of the quality bound on downstream tasks, with extensive empirical validation across multiple biomedical domains (genomics, drug discovery, materials science).",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model [biomedical NER]",
                        "Weber et al. (2020) HunFlair: An Easy-to-Use Tool for State-of-the-Art Biomedical Named Entity Recognition [biomedical entity recognition]",
                        "Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [foundation for domain-adapted NER]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Open Relation Extraction Coverage Law",
                "if": [
                    {
                        "subject": "biomedical_corpus",
                        "relation": "contains",
                        "object": "diverse_relationship_types"
                    },
                    {
                        "subject": "extraction_method",
                        "relation": "uses",
                        "object": "open_relation_extraction"
                    }
                ],
                "then": [
                    {
                        "subject": "coverage",
                        "relation": "exceeds",
                        "object": "closed_relation_extraction"
                    },
                    {
                        "subject": "optimal_coverage",
                        "relation": "approximately",
                        "object": "70-90_percent"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM-ORE achieved 71.4% coverage of ClinVar disease-gene pairs with GPT-3.5, increasing to 91.3% with Llama-8B scaled extraction, demonstrating that open extraction can achieve &gt;90% coverage with sufficient scale",
                        "uuids": [
                            "e4211.1"
                        ]
                    },
                    {
                        "text": "WISE achieved 84.2% recall on gene-disease associations using open extraction vs 47.4% for pure ChatGPT, 36.8% for ChatGPT with Search, 10.5% for Gemini, and 15.8% for Google Search, demonstrating 2-8x coverage advantage",
                        "uuids": [
                            "e4220.0"
                        ]
                    },
                    {
                        "text": "Multi-agent hypothesis generation framework achieved higher novelty scores (Eval.Avg ~2.07-2.09) through open-ended exploration vs constrained extraction, with GPT-4 evaluation scores correlating &gt;0.7 with human judgments",
                        "uuids": [
                            "e4192.0"
                        ]
                    },
                    {
                        "text": "LLM-based multi-agent framework with open relation extraction produced hypotheses with average level-of-detail 3.81 on 0-5 scale, higher than baselines, demonstrating that open extraction captures richer relationships",
                        "uuids": [
                            "e4223.1"
                        ]
                    },
                    {
                        "text": "ChatGPT/GPT-4 summarization pipeline for biomedical literature produced 2,900 background-hypothesis pairs with quantitative statements (e.g., 'AUC &gt; 0.7'), demonstrating open extraction of diverse relationship types",
                        "uuids": [
                            "e4192.1"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist aggregation agent combining multiple LLM outputs achieved improved F1 distributions for protein enzymes versus single models, showing that open extraction with aggregation improves coverage",
                        "uuids": [
                            "e4243.1"
                        ]
                    },
                    {
                        "text": "LLMEVALDB pipeline extracted 18,127 experimental records from 1,737 papers across diverse prompting methods and datasets, demonstrating that open extraction captures heterogeneous relationship types",
                        "uuids": [
                            "e4214.0"
                        ]
                    },
                    {
                        "text": "ByteScience platform with open schema-driven extraction achieved 80-90% accuracy and 57% annotation-time reduction, demonstrating that open extraction with human-in-the-loop can achieve high coverage efficiently",
                        "uuids": [
                            "e4197.0"
                        ]
                    },
                    {
                        "text": "SciMON benchmark for hypothesis discovery from literature emphasizes open-ended discovery of novel findings, demonstrating the importance of open extraction for scientific discovery",
                        "uuids": [
                            "e4195.5"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "Open vs closed information extraction is a known distinction in NLP. However, this law specifically quantifies the coverage advantage (71.4% → 91.3%, 84.2% vs 47.4%, 2-8x improvement) for biomedical applications and identifies the optimal coverage range (70-90%). The novel contribution is the specific quantification across multiple biomedical domains and the identification that higher coverage comes with precision trade-offs that vary by subdomain.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Banko et al. (2007) Open Information Extraction from the Web [open IE foundations]",
                        "Etzioni et al. (2011) Open Information Extraction: The Second Generation [open IE advances]",
                        "Fader et al. (2011) Identifying Relations for Open Information Extraction [relation identification in open IE]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Knowledge Graph Aggregation Enhancement Law",
                "if": [
                    {
                        "subject": "extracted_relations",
                        "relation": "aggregated_into",
                        "object": "knowledge_graph"
                    },
                    {
                        "subject": "knowledge_graph",
                        "relation": "uses",
                        "object": "embedding_based_representation"
                    }
                ],
                "then": [
                    {
                        "subject": "downstream_prediction_performance",
                        "relation": "improves_by",
                        "object": "15-30_percent"
                    },
                    {
                        "subject": "optimal_MAP",
                        "relation": "approximately",
                        "object": "80-90_percent"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LORE with knowledge graph and ML-Ranker achieved 90.0% MAP on annotated disease-gene pairs vs 79.9% without KG, representing an 11% relative improvement (10.1 percentage points absolute)",
                        "uuids": [
                            "e4211.1"
                        ]
                    },
                    {
                        "text": "LLM-EMB with embedding aggregation achieved 87.7% MAP vs 69.4% for naive paper counting and 31.7% for GPT-4o direct prompting, representing 26% and 177% relative improvements respectively",
                        "uuids": [
                            "e4211.2"
                        ]
                    },
                    {
                        "text": "SEMNET with network-based features achieved AUC=0.85 for link prediction in physics literature, demonstrating that graph-based aggregation enables accurate prediction of future research connections",
                        "uuids": [
                            "e4488.1"
                        ]
                    },
                    {
                        "text": "AstroKG with citation-reference transition probabilities enabled quantified concept relationships, producing a knowledge graph with 24,797 concepts from ~1.06M initial concepts through embedding-based clustering and merging",
                        "uuids": [
                            "e4486.0"
                        ]
                    },
                    {
                        "text": "Embedding-augmented domain chatbot achieved ~90% valid responses vs &lt;14% without embedding context, representing a &gt;6x improvement through embedding-based retrieval and aggregation",
                        "uuids": [
                            "e4201.0"
                        ]
                    },
                    {
                        "text": "WISE with tree-based workflow and unique-contribution scoring achieved 84.2% recall vs 47.4% for pure ChatGPT, demonstrating that structured aggregation with scoring improves coverage by 78%",
                        "uuids": [
                            "e4220.0"
                        ]
                    },
                    {
                        "text": "Semantic embedding + clustering pipeline (InstructorEmbedding → LargeVis → HDBSCAN) produced 390 scientific-problem clusters and 355 AI-method clusters, enabling bipartite graph analysis with heavy-tailed degree distributions",
                        "uuids": [
                            "e4194.2"
                        ]
                    },
                    {
                        "text": "Fine-tuned sentence-transformers with GPL adaptation reduced outlier counts (e.g., arXiv outliers from 43 to 29) and improved topic modeling quality, demonstrating that embedding-based aggregation improves clustering",
                        "uuids": [
                            "e4191.4"
                        ]
                    },
                    {
                        "text": "Ontological Knowledge Graph for bio-inspired materials (33,159 nodes, 48,753 edges) enabled LLM-driven graph reasoning and hypothesis generation with numerical predictions (e.g., tensile strength 1.5 GPa)",
                        "uuids": [
                            "e4179.1"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "Knowledge graph construction for biomedical applications is well-established. However, this law specifically quantifies the performance improvement (15-30%, with examples of 11%, 26%, 177%, and &gt;6x improvements) from KG aggregation and identifies the optimal performance range (80-90% MAP). The novel contribution is the specific quantification for biomedical law extraction across multiple domains and the identification of embedding-based representation as critical, with evidence that the improvement varies by task complexity and baseline method.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Nickel et al. (2016) A Review of Relational Machine Learning for Knowledge Graphs [KG methods]",
                        "Wang et al. (2017) Knowledge Graph Embedding: A Survey of Approaches and Applications [KG embeddings]",
                        "Ji et al. (2021) A Survey on Knowledge Graphs: Representation, Acquisition, and Applications [comprehensive KG survey]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "A three-stage pipeline (BioBERT NER → GPT-4 open RE → embedding-based KG) will achieve &gt;85% MAP on disease-gene association prediction, outperforming any two-stage pipeline by &gt;15%, based on the observed 11% improvement from adding KG aggregation and &gt;26% improvement from embedding-based methods.",
        "For drug-target interaction extraction, prioritizing coverage (&gt;80%) over precision will yield better downstream prediction than prioritizing precision (&gt;90%) with lower coverage (&lt;60%), based on the observed 2-8x coverage advantage of open extraction methods.",
        "Combining multiple biomedical KGs (e.g., gene-disease, drug-target, pathway) will improve prediction by 10-20% compared to single-domain KGs through cross-domain inference, based on the observed improvements from multi-source aggregation (e.g., WISE's 84.2% recall vs 47.4% for single-source).",
        "Domain-adapted entity recognition (e.g., BioBERT) will improve downstream relation extraction F1 by 0.6-0.8 points compared to general-purpose models, based on the observed +0.62 F1 improvement from biomedical pretraining.",
        "Scaling open relation extraction from 1.7M to 4M abstracts will increase coverage from 71.4% to &gt;90%, based on the observed increase from 71.4% to 91.3% with scaled extraction."
    ],
    "new_predictions_unknown": [
        "Fully automated biomedical law extraction might achieve &gt;95% MAP on well-studied diseases but &lt;50% on rare diseases, creating a knowledge gap that could be addressed by active learning strategies, but the optimal active learning approach is unclear. The observed performance range (31.7% to 90.0% MAP) suggests high variability across disease types.",
        "Integrating patient-level data (EHRs) with literature-extracted laws might enable personalized medicine at scale, but privacy concerns and data heterogeneity may limit practical applicability. The observed challenges with data quality and heterogeneity in literature extraction suggest similar challenges for EHR integration.",
        "Cross-species knowledge transfer (e.g., mouse → human) using LLM-extracted laws might accelerate drug discovery, but the reliability of such transfer for novel targets is unknown. The observed domain-shift challenges (e.g., performance drops when moving from TMR to PCD datasets) suggest potential difficulties.",
        "End-to-end LLM extraction without explicit three-stage pipeline might achieve comparable performance (&gt;85% MAP) with sufficiently capable models (e.g., GPT-4, Claude3.5), potentially challenging the necessity of the three-stage architecture. The observed high performance of WISE (84.2% recall) and Claude3.5 (mean F1=0.90) suggests this is possible.",
        "Multimodal extraction combining text, tables, and figures might improve coverage by 20-40% compared to text-only extraction, but the optimal integration strategy is unclear. The observed challenges with table/figure extraction and the success of multimodal approaches (e.g., ChatGPT chemistry assistant) suggest potential but uncertain magnitude of improvement."
    ],
    "negative_experiments": [
        "If end-to-end LLM extraction without separate NER achieves &gt;90% MAP, this would challenge the Entity Recognition Primacy Law. Evidence from WISE (84.2% recall) and Claude3.5 (mean F1=0.90) suggests this is possible.",
        "If closed relation extraction with predefined schemas matches open extraction coverage (&gt;80%), this would challenge the Open Relation Extraction Coverage Law. Current evidence shows 2-8x coverage advantage for open extraction, but improved schema design might close this gap.",
        "If direct LLM prediction without KG construction achieves &gt;85% MAP, this would challenge the Knowledge Graph Aggregation Enhancement Law. Evidence from GPT-4o direct prompting (31.7% MAP) suggests this is unlikely, but more capable models might achieve this.",
        "If general-purpose models (e.g., GPT-4) without domain adaptation achieve F1 &gt;0.90 on biomedical NER, this would challenge the necessity of domain adaptation in the Entity Recognition Primacy Law. Current evidence shows +0.62 F1 improvement from domain adaptation, but larger general-purpose models might reduce this gap.",
        "If two-stage pipelines (NER + RE or RE + KG) achieve &gt;85% MAP, this would challenge the necessity of all three stages. Current evidence shows 11-26% improvements from adding stages, but optimal stage combinations might vary by task."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal granularity for biomedical entities (gene vs protein vs pathway) is not well-characterized and likely varies by task. Evidence suggests entity disambiguation impacts performance but specific thresholds are unclear.",
            "uuids": [
                "e4211.1"
            ]
        },
        {
            "text": "The impact of entity disambiguation (e.g., gene name ambiguity) on downstream performance is underexplored. LORE mentions this as a challenge but does not quantify the impact.",
            "uuids": [
                "e4211.1"
            ]
        },
        {
            "text": "The trade-offs between relation granularity (atomic triplets vs complex relationships) lack systematic study. LLM-ORE uses atomic triplets but the optimal granularity for different tasks is unclear.",
            "uuids": [
                "e4211.1"
            ]
        },
        {
            "text": "The role of temporal information (when relationships were discovered) in biomedical law extraction is largely ignored. LLMEVALDB includes temporal filtering but most systems do not explicitly model temporal dynamics.",
            "uuids": [
                "e4214.0"
            ]
        },
        {
            "text": "The challenges of extracting relationships from tables and figures are acknowledged but not systematically addressed. Multiple systems note difficulties with table/figure extraction but lack quantitative analysis of the impact.",
            "uuids": [
                "e4242.1",
                "e4484.1",
                "e4227.2"
            ]
        },
        {
            "text": "The optimal context window size for LLM extraction is unclear. Evidence ranges from sentence-level to full-document extraction with varying performance.",
            "uuids": [
                "e4216.3",
                "e4243.2"
            ]
        },
        {
            "text": "The impact of OCR quality on extraction performance is mentioned but not quantified. Multiple systems use OCR preprocessing but do not report OCR error rates or their impact.",
            "uuids": [
                "e4243.2",
                "e4481.1"
            ]
        },
        {
            "text": "The role of unit conversion and normalization in quantitative extraction is underexplored. Some systems mention unit conversion challenges but lack systematic analysis.",
            "uuids": [
                "e4243.2",
                "e4227.6"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show domain adaptation improves accuracy (BioBERT +0.62 F1) while others show it reduces novelty (fine-tuned models had lower novelty scores), creating tension about optimal adaptation strategy for biomedical discovery.",
            "uuids": [
                "e4223.3",
                "e4171.0"
            ]
        },
        {
            "text": "LORE achieved 90% MAP with KG but used ClinVar in taxonomy construction, introducing circularity that complicates interpretation of the KG benefit. The 11% improvement (90.0% vs 79.9%) may be partially due to this circularity.",
            "uuids": [
                "e4211.1"
            ]
        },
        {
            "text": "End-to-end LLM approaches (WISE 84.2% recall, Claude3.5 mean F1=0.90) achieved high performance without explicit three-stage pipelines, challenging the necessity of the three-stage architecture. However, these systems may implicitly perform similar stages.",
            "uuids": [
                "e4220.0",
                "e4243.2"
            ]
        },
        {
            "text": "MaterialsBERT NER-based approach had lower extraction quantity despite similar F1 to GPT-3.5, suggesting that entity recognition quality alone does not determine extraction volume. This conflicts with the Entity Recognition Primacy Law's prediction.",
            "uuids": [
                "e4242.1"
            ]
        },
        {
            "text": "Some systems achieved high performance with zero-shot or few-shot prompting (e.g., WISE, Claude3.5) while others required extensive fine-tuning (e.g., BioBERT, MaterialsBERT), creating uncertainty about the necessity of domain adaptation.",
            "uuids": [
                "e4220.0",
                "e4243.2",
                "e4171.0",
                "e4242.1"
            ]
        },
        {
            "text": "Direct LLM prediction (GPT-4o 31.7% MAP) performed much worse than KG-based methods (LORE 90.0% MAP), but WISE achieved 84.2% recall with direct LLM extraction, suggesting that the gap may be due to prompting strategy rather than fundamental limitations.",
            "uuids": [
                "e4211.2",
                "e4220.0"
            ]
        }
    ],
    "special_cases": [
        "For genomics applications, precision is more critical than coverage due to high cost of false positives in experimental validation. Evidence from LORE shows that precision filters (r &gt;=50%) were used to select important lemmas.",
        "For drug repurposing, coverage is more critical than precision to maximize discovery of novel indications. Evidence from WISE shows that open extraction with high recall (84.2%) is prioritized.",
        "For rare diseases, few-shot learning may be necessary due to limited literature, changing optimal extraction strategies. Evidence from LLM-ORE shows that coverage was initially limited (71.4%) but improved with scaling.",
        "For clinical applications, explainability requirements may favor simpler extraction methods over complex KG approaches. Evidence from multiple systems shows trade-offs between performance and interpretability.",
        "For table and figure extraction, specialized multimodal approaches are necessary. Evidence from multiple systems shows that text-only extraction misses significant information in tables and figures.",
        "For longitudinal studies, temporal modeling is critical. Evidence from LLMEVALDB shows that temporal filtering (pre-Jan-2023) was used to control visibility, but most systems do not explicitly model temporal dynamics.",
        "For cross-lingual extraction, multilingual models or translation pipelines are necessary. This is not addressed in the current evidence but is a known challenge in biomedical NLP.",
        "For real-time extraction, computational efficiency becomes critical. Evidence from multiple systems shows trade-offs between accuracy and speed (e.g., MaterialsBERT processed 6,179 paragraphs in under half an hour vs GPT-3.5 which is slower but more accurate)."
    ],
    "existing_theory": {
        "classification_explanation": "This theory synthesizes known components (NER, open RE, KG construction) into a specific pipeline for biomedical law extraction with novel quantifications (&gt;90% F1 for NER, 70-90% coverage for RE, 80-90% MAP for KG, 15-30% improvement from KG aggregation). While the individual components are established, the specific pipeline architecture, quantified thresholds, subdomain-specific trade-offs, and extensive empirical validation across multiple biomedical domains represent new contributions. However, recent evidence suggests that end-to-end LLM approaches may achieve comparable performance, challenging the necessity of the three-stage architecture.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model [biomedical NER]",
            "Banko et al. (2007) Open Information Extraction from the Web [open RE]",
            "Nickel et al. (2016) A Review of Relational Machine Learning for Knowledge Graphs [KG methods]",
            "Percha & Altman (2018) A global network of biomedical relationships derived from text [biomedical KG construction]",
            "Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [foundation for domain-adapted NER]",
            "Ji et al. (2021) A Survey on Knowledge Graphs: Representation, Acquisition, and Applications [comprehensive KG survey]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
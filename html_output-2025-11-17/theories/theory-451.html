<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Memory Architecture Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-451</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-451</p>
                <p><strong>Name:</strong> Dual-Process Memory Architecture Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM-based agents can most effectively be augmented with memory to solve text games, based on the following results.</p>
                <p><strong>Description:</strong> LLM-based text game agents achieve optimal performance through a dual-process memory architecture that combines fast, associative short-term memory (System 1) with deliberate, structured long-term memory (System 2). The short-term memory handles immediate context and recent observations within the LLM's attention window, while long-term memory stores compressed, structured representations of past experiences that are selectively retrieved based on relevance. The synergy between these two systems enables both rapid action selection in familiar situations and robust reasoning in novel or complex scenarios. This theory posits that neither memory system alone is sufficient - short-term memory without long-term storage leads to repetitive errors and inability to learn from distant past experiences, while long-term memory without short-term context fails to ground decisions in immediate observations. The effectiveness of this architecture depends critically on: (1) appropriate memory compression/summarization strategies, (2) effective retrieval mechanisms that balance recency, relevance, and importance, and (3) proper integration between the two memory systems.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Law 0</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses only &#8594; short-term prompt-based memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; learning from past failures beyond context window</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; exhibits &#8594; repeated mistakes<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent performance &#8594; plateaus at &#8594; suboptimal level</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SWIFT-only agent (behavior-cloned with short-term memory) scored 49.22 but repeated invalid actions when encountering exceptions, while SWIFTSAGE (adding deliberate long-term planning) scored 84.68 (+35.46 points) <a href="../results/extraction-result-2977.html#e2977.0" class="evidence-link">[e2977.0]</a> <a href="../results/extraction-result-2977.html#e2977.1" class="evidence-link">[e2977.1]</a> </li>
    <li>NetPlay with only Room obs (local short-term memory) scored 341.67 while NetPlay with Level obs (oracle long-term memory) scored 675.33 (+333.66 points) <a href="../results/extraction-result-2945.html#e2945.6" class="evidence-link">[e2945.6]</a> </li>
    <li>ChatGPT struggles to keep track of states of in-game objects and preconditions necessary to use actions, lacking memory mechanisms <a href="../results/extraction-result-2899.html#e2899.3" class="evidence-link">[e2899.3]</a> </li>
    <li>ChatGPT with walkthrough-fed context showed strong memorization on seen cases but poor generalization to unseen/reversed paths (one-step seen ~75% vs unseen ~29%), indicating limitations of pure in-context memory <a href="../results/extraction-result-2978.html#e2978.2" class="evidence-link">[e2978.2]</a> </li>
    <li>ReAct-IM (dense external-feedback only) achieved 53% success vs full ReAct with richer internal reasoning at 71% on ALFWorld, showing limitations of observation-only memory <a href="../results/extraction-result-2971.html#e2971.1" class="evidence-link">[e2971.1]</a> </li>
    <li>Act-only baseline (no thoughts/memory) achieved 45% success on ALFWorld vs ReAct with memory at 71% <a href="../results/extraction-result-2971.html#e2971.0" class="evidence-link">[e2971.0]</a> </li>
    <li>Baseline generative agents with full-event retrieval (no compression) incurred very large token costs (4.525M tokens) and limited behavioral diversity <a href="../results/extraction-result-2949.html#e2949.0" class="evidence-link">[e2949.0]</a> </li>
    <li>NPAE-Flan-T5 with same short-term prompt memory but no pretraining remained under 0.2 success rate, showing memory structure alone insufficient without learned representations <a href="../results/extraction-result-2974.html#e2974.2" class="evidence-link">[e2974.2]</a> </li>
    <li>Agents without planner module (limited plan retrieval) in Among Us were more likely to hit time limits and less likely to finish tasks <a href="../results/extraction-result-2915.html#e2915.0" class="evidence-link">[e2915.0]</a> </li>
    <li>GATA with belief graph but without proper instruction-following mechanisms failed to complete tasks despite having structured memory <a href="../results/extraction-result-2960.html#e2960.0" class="evidence-link">[e2960.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Law 1</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; combines &#8594; short-term working memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; with &#8594; long-term episodic or structured memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory systems &#8594; are &#8594; properly integrated</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent performance &#8594; exceeds &#8594; either system alone<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance improvement &#8594; is typically &#8594; 20-50 percentage points absolute</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SWIFTSAGE combining fast SWIFT (short-term) with deliberate SAGE (long-term planning) improved from 49.22 to 84.68 (+35.46 points) <a href="../results/extraction-result-2977.html#e2977.0" class="evidence-link">[e2977.0]</a> </li>
    <li>KARMA with dual short-term cache and long-term 3DSG improved composite task success by +43 percentage points and complex tasks by +21 percentage points <a href="../results/extraction-result-2925.html#e2925.0" class="evidence-link">[e2925.0]</a> </li>
    <li>SAGE with dual STM/LTM improved ALFWorld from 0.0 to 10.5-12.5 for smaller models (Qwen-1.8B, CodeLlama-7B) <a href="../results/extraction-result-2941.html#e2941.0" class="evidence-link">[e2941.0]</a> </li>
    <li>Memory-R1 with dual memory management (Memory Manager + Answer Agent) improved F1 by +14.61 points (+48% relative) on LO-COMO benchmark <a href="../results/extraction-result-2952.html#e2952.0" class="evidence-link">[e2952.0]</a> </li>
    <li>MC-DML with in-trial short-term memory and cross-trial episodic reflections improved Zork1 by +10.33 points (48.66 vs 38.33, ~+27% relative) <a href="../results/extraction-result-2927.html#e2927.0" class="evidence-link">[e2927.0]</a> </li>
    <li>GATA with learned belief graphs achieved +24.2% average relative improvement over text-only Tr-DQN baseline on TextWorld <a href="../results/extraction-result-2968.html#e2968.0" class="evidence-link">[e2968.0]</a> </li>
    <li>HiAgent with hierarchical working memory management doubled overall success rate (21% -> 42%) on long-horizon tasks <a href="../results/extraction-result-2937.html#e2937.0" class="evidence-link">[e2937.0]</a> </li>
    <li>LLaMA-Rider with short-term history + experience replay improved from ~20% to ~34% success rate (+14 percentage points) <a href="../results/extraction-result-2917.html#e2917.0" class="evidence-link">[e2917.0]</a> </li>
    <li>ReAct with language-based working memory (thoughts + actions) achieved 71% success on ALFWorld vs 45% for Act-only (+26 percentage points) <a href="../results/extraction-result-2971.html#e2971.0" class="evidence-link">[e2971.0]</a> </li>
    <li>BUTLER with recurrent aggregator + observation queue memory achieved 40% success on TextWorld seen vs 10% for Seq2Seq baseline (+30 percentage points) <a href="../results/extraction-result-2969.html#e2969.1" class="evidence-link">[e2969.1]</a> </li>
    <li>ReadAgent with episodic gist memory + interactive lookup improved NarrativeQA by +12.97% LLM rating and +31.98% ROUGE-L over best retrieval baseline <a href="../results/extraction-result-2929.html#e2929.0" class="evidence-link">[e2929.0]</a> </li>
    <li>ThinkThrice with retrieval-augmented memory improved factual QA from 0.305 to 0.498 with full pipeline (+0.193 absolute) <a href="../results/extraction-result-2966.html#e2966.0" class="evidence-link">[e2966.0]</a> </li>
    <li>Reflexion with episodic reflection memory improved HotPotQA by +8% absolute over episodic-memory-only baseline <a href="../results/extraction-result-2933.html#e2933.2" class="evidence-link">[e2933.2]</a> </li>
    <li>AGA with dual Lifestyle Policy (case-based) and Social Memory reduced tokens to 31.1% of baseline while maintaining comparable performance <a href="../results/extraction-result-2949.html#e2949.1" class="evidence-link">[e2949.1]</a> </li>
    <li>MPRC-DQN with object-centric historical retrieval won 21/33 games (64% winning percentage) vs 17/33 for RC-DQN without history <a href="../results/extraction-result-2976.html#e2976.0" class="evidence-link">[e2976.0]</a> <a href="../results/extraction-result-2976.html#e2976.1" class="evidence-link">[e2976.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Law 2</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses &#8594; long-term memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; long-term memory &#8594; lacks &#8594; effective retrieval mechanism<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; selective access to past information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; suffers from &#8594; information overload or irrelevant retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; degrades compared to &#8594; well-designed retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Full history memory without structure underperformed structured AriGraph (0.47 vs 1.0 on Treasure Hunt) due to inefficient retrieval and token usage (~14,000 prompt tokens at step 150) <a href="../results/extraction-result-2945.html#e2945.1" class="evidence-link">[e2945.1]</a> </li>
    <li>MemWalker with hierarchical traversal had 8.6% search failure rate and achieved 66.73% accuracy vs ReadAgent's 86.6% with targeted retrieval <a href="../results/extraction-result-2929.html#e2929.1" class="evidence-link">[e2929.1]</a> </li>
    <li>Retrieval-augmentation can underperform due to unrelated or noisy prompts, and may suffer information loss <a href="../results/extraction-result-2924.html#e2924.2" class="evidence-link">[e2924.2]</a> </li>
    <li>Simple short-history augmentation (one past observation + five last actions) degraded WebShop performance from 59.9 to 57.3 task score <a href="../results/extraction-result-2943.html#e2943.0" class="evidence-link">[e2943.0]</a> </li>
    <li>KARMA without short-term memory showed 1.9x-4.2x drops in success rate depending on task class, indicating retrieval alone insufficient <a href="../results/extraction-result-2925.html#e2925.0" class="evidence-link">[e2925.0]</a> </li>
    <li>Baseline generative agents with full-event retrieval (30-45 events, ~2000 tokens) had high costs and limited behavioral diversity <a href="../results/extraction-result-2949.html#e2949.0" class="evidence-link">[e2949.0]</a> </li>
    <li>RAG memory (unstructured vector store) achieved only 0.33 on Treasure Hunt vs AriGraph's 1.0, showing limitations of unstructured retrieval <a href="../results/extraction-result-2945.html#e2945.2" class="evidence-link">[e2945.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Law 3</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; long-term memory &#8594; uses &#8594; compression or summarization<span style="color: #888888;">, and</span></div>
        <div>&#8226; compression &#8594; preserves &#8594; task-relevant information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; better token efficiency<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; is maintained or improved &#8594; compared to full storage</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ReadAgent with gist memory compression achieved 85.53-96.80% compression ratio while maintaining or improving performance (86.16% accuracy on QuALITY) <a href="../results/extraction-result-2929.html#e2929.0" class="evidence-link">[e2929.0]</a> </li>
    <li>AGA with Social Memory compression reduced conversation-relevant data from ~2000 tokens to ~100 tokens while preserving response quality <a href="../results/extraction-result-2949.html#e2949.1" class="evidence-link">[e2949.1]</a> </li>
    <li>HiAgent with hierarchical subgoal-based memory reduced context tokens by ~35% while improving success rate from 21% to 42% <a href="../results/extraction-result-2937.html#e2937.0" class="evidence-link">[e2937.0]</a> </li>
    <li>RECURRENTGPT with long-term paragraph summaries and short-term working memory enabled arbitrarily long coherent generation <a href="../results/extraction-result-2964.html#e2964.0" class="evidence-link">[e2964.0]</a> </li>
    <li>AMONGAGENTS with summarized evolving memory (phi_H function) enabled coherent multi-agent gameplay with planner-enabled agents showing increased win rates <a href="../results/extraction-result-2915.html#e2915.0" class="evidence-link">[e2915.0]</a> </li>
    <li>Diff-history compression for NetHack observations provided ~7x improvement in game score and >40% improvement vs visual observations <a href="../results/extraction-result-2930.html#e2930.1" class="evidence-link">[e2930.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 4: Law 4</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; long-term memory &#8594; is structured as &#8594; graph or relational representation<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; multi-hop reasoning or relational queries</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent performance &#8594; exceeds &#8594; unstructured memory approaches<span style="color: #888888;">, and</span></div>
        <div>&#8226; improvement &#8594; is particularly large for &#8594; tasks with complex object relationships</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AriGraph with knowledge graph memory achieved 1.0 normalized score on Treasure Hunt vs 0.47 for full history and 0.33 for RAG <a href="../results/extraction-result-2945.html#e2945.0" class="evidence-link">[e2945.0]</a> <a href="../results/extraction-result-2945.html#e2945.1" class="evidence-link">[e2945.1]</a> <a href="../results/extraction-result-2945.html#e2945.2" class="evidence-link">[e2945.2]</a> </li>
    <li>GATA with learned belief graphs achieved +24.2% average relative improvement over text-only baseline on TextWorld cooking tasks <a href="../results/extraction-result-2968.html#e2968.0" class="evidence-link">[e2968.0]</a> </li>
    <li>KG-A2C with knowledge graph constraints substantially outperformed TDQN on 23/28 Jericho games <a href="../results/extraction-result-2972.html#e2972.3" class="evidence-link">[e2972.3]</a> </li>
    <li>KARMA with hierarchical 3D scene graph (long-term) plus object cache (short-term) improved composite tasks by +43 percentage points <a href="../results/extraction-result-2925.html#e2925.0" class="evidence-link">[e2925.0]</a> </li>
    <li>Ghost in Minecraft with text-based knowledge and memory supports broader task capabilities in open-world settings <a href="../results/extraction-result-2931.html#e2931.5" class="evidence-link">[e2931.5]</a> </li>
    <li>Arigraph with knowledge triples combining semantic and episodic memories supports complex reasoning tasks <a href="../results/extraction-result-2942.html#e2942.2" class="evidence-link">[e2942.2]</a> <a href="../results/extraction-result-2951.html#e2951.3" class="evidence-link">[e2951.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An agent with a 3-level memory hierarchy (immediate context, working memory buffer, long-term structured store) will outperform 2-level systems on tasks requiring both rapid response and long-term learning by 10-20 percentage points</li>
                <li>Agents that dynamically adjust the ratio of short-term to long-term memory retrieval based on task complexity will achieve 20-40% better token efficiency than fixed-ratio systems while maintaining performance</li>
                <li>In multi-session text games, agents with persistent long-term memory across sessions will show cumulative learning gains of 15-30% per session for the first 3-5 sessions before plateauing</li>
                <li>Combining graph-structured long-term memory with compressed episodic short-term memory will outperform either alone by 15-25 percentage points on tasks requiring both relational reasoning and recent context</li>
                <li>Agents with adaptive memory compression that increases compression ratio as memory grows will maintain performance while reducing token costs by 50-70% compared to fixed compression</li>
                <li>Dual-memory agents will show 2-3x faster convergence to optimal performance compared to single-memory agents on long-horizon tasks (>50 steps)</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether a unified memory architecture that seamlessly blends short and long-term memory (rather than separate systems) could achieve superior performance while reducing architectural complexity - this could either simplify implementation or lose the benefits of specialized memory systems</li>
                <li>Whether the optimal ratio of short-term to long-term memory capacity varies systematically with game genre (e.g., puzzle vs. social deduction vs. exploration) - different genres may have fundamentally different memory requirements</li>
                <li>Whether agents can learn to meta-optimize their own memory allocation strategies through reinforcement learning on memory management actions - this could enable adaptive memory systems but may be difficult to train</li>
                <li>Whether hierarchical memory with more than 3 levels (e.g., immediate, working, episodic, semantic, procedural) would provide additional benefits or just add complexity without gains</li>
                <li>Whether memory systems trained on one game genre transfer effectively to other genres, or if memory architectures need to be genre-specific</li>
                <li>Whether the benefits of dual-memory systems scale linearly, sublinearly, or superlinearly with task horizon length</li>
                <li>Whether biological memory principles (e.g., sleep-like consolidation, forgetting curves, interference effects) could further improve LLM agent memory systems</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding text game tasks where pure short-term memory (no long-term storage) consistently outperforms dual-memory systems would challenge the necessity of long-term memory for all task types</li>
                <li>Demonstrating that randomly retrieved long-term memories perform as well as relevance-based retrieval would question the importance of sophisticated retrieval mechanisms</li>
                <li>Showing that agents with only long-term memory (no short-term context) can match dual-system performance would challenge the necessity of short-term memory</li>
                <li>Finding that unstructured memory consistently outperforms structured memory on relational reasoning tasks would challenge the value of graph-based representations</li>
                <li>Demonstrating that memory compression always degrades performance proportionally to compression ratio would challenge the viability of compressed memory systems</li>
                <li>Showing that the performance gap between dual-memory and single-memory systems disappears for very large context windows (>100k tokens) would suggest dual systems are only necessary due to context limitations</li>
                <li>Finding that memory systems trained on diverse tasks perform worse than task-specific memory systems would challenge the generality of dual-memory architectures</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact mechanisms by which LLMs internally integrate short-term prompt context with retrieved long-term memories during generation - the attention patterns and internal representations are not well understood </li>
    <li>Why some tasks (like simple navigation) show minimal benefit from long-term memory while others (like multi-step puzzles) show dramatic improvements - the task characteristics that determine memory importance are not fully characterized <a href="../results/extraction-result-2945.html#e2945.2" class="evidence-link">[e2945.2]</a> <a href="../results/extraction-result-2945.html#e2945.4" class="evidence-link">[e2945.4]</a> </li>
    <li>The role of model size and pretraining in determining how effectively agents can utilize dual-memory architectures - NPAE-Flan-T5 failed despite same memory structure <a href="../results/extraction-result-2974.html#e2974.2" class="evidence-link">[e2974.2]</a> </li>
    <li>Why some memory compression strategies (gist, summarization) work well while others (simple truncation) fail - the principles of effective compression are not fully understood <a href="../results/extraction-result-2929.html#e2929.0" class="evidence-link">[e2929.0]</a> <a href="../results/extraction-result-2943.html#e2943.0" class="evidence-link">[e2943.0]</a> </li>
    <li>The optimal memory capacity and retrieval size for different task types - most systems use fixed hyperparameters without principled justification <a href="../results/extraction-result-2925.html#e2925.0" class="evidence-link">[e2925.0]</a> <a href="../results/extraction-result-2929.html#e2929.0" class="evidence-link">[e2929.0]</a> <a href="../results/extraction-result-2966.html#e2966.0" class="evidence-link">[e2966.0]</a> </li>
    <li>How memory systems should handle contradictory information from different time points - conflict resolution strategies are not well studied </li>
    <li>The interaction between memory architecture and LLM architecture (decoder-only vs encoder-decoder) - most studies use specific model types <a href="../results/extraction-result-2974.html#e2974.0" class="evidence-link">[e2974.0]</a> <a href="../results/extraction-result-2977.html#e2977.0" class="evidence-link">[e2977.0]</a> </li>
    <li>Why reflection-based memory helps some agents (SA-RL) more than others (DA-RL) - the conditions for effective reflection are unclear <a href="../results/extraction-result-2907.html#e2907.1" class="evidence-link">[e2907.1]</a> <a href="../results/extraction-result-2907.html#e2907.2" class="evidence-link">[e2907.2]</a> </li>
    <li>The role of memory in enabling deception and strategic behavior in social deduction games - memory's contribution to theory of mind is not well characterized <a href="../results/extraction-result-2909.html#e2909.0" class="evidence-link">[e2909.0]</a> <a href="../results/extraction-result-2909.html#e2909.2" class="evidence-link">[e2909.2]</a> <a href="../results/extraction-result-2909.html#e2909.3" class="evidence-link">[e2909.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Atkinson & Shiffrin (1968) Human Memory: A Proposed System and its Control Processes [Classic dual-store memory model in cognitive psychology with short-term and long-term stores]</li>
    <li>Baddeley & Hitch (1974) Working Memory [Working memory model with multiple components including phonological loop and visuospatial sketchpad]</li>
    <li>Kahneman (2011) Thinking, Fast and Slow [Dual-process theory of cognition with System 1 (fast, intuitive) and System 2 (slow, deliberate)]</li>
    <li>Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Implements dual short-term and long-term memory for game agents]</li>
    <li>Zhong et al. (2024) SWIFTSAGE: A Generative Agent with Fast and Slow Thinking [Explicitly implements dual-process architecture with fast SWIFT and slow SAGE systems]</li>
    <li>Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Uses memory streams with retrieval for long-term agent behavior]</li>
    <li>Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Uses episodic memory and reflection for iterative improvement]</li>
    <li>Modarressi et al. (2024) KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems [Explicitly implements dual long-term 3DSG and short-term cache]</li>
    <li>Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [Uses language-based working memory for reasoning traces]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Memory Architecture Theory",
    "theory_description": "LLM-based text game agents achieve optimal performance through a dual-process memory architecture that combines fast, associative short-term memory (System 1) with deliberate, structured long-term memory (System 2). The short-term memory handles immediate context and recent observations within the LLM's attention window, while long-term memory stores compressed, structured representations of past experiences that are selectively retrieved based on relevance. The synergy between these two systems enables both rapid action selection in familiar situations and robust reasoning in novel or complex scenarios. This theory posits that neither memory system alone is sufficient - short-term memory without long-term storage leads to repetitive errors and inability to learn from distant past experiences, while long-term memory without short-term context fails to ground decisions in immediate observations. The effectiveness of this architecture depends critically on: (1) appropriate memory compression/summarization strategies, (2) effective retrieval mechanisms that balance recency, relevance, and importance, and (3) proper integration between the two memory systems.",
    "theory_statements": [
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses only",
                        "object": "short-term prompt-based memory"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "learning from past failures beyond context window"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "exhibits",
                        "object": "repeated mistakes"
                    },
                    {
                        "subject": "agent performance",
                        "relation": "plateaus at",
                        "object": "suboptimal level"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SWIFT-only agent (behavior-cloned with short-term memory) scored 49.22 but repeated invalid actions when encountering exceptions, while SWIFTSAGE (adding deliberate long-term planning) scored 84.68 (+35.46 points)",
                        "uuids": [
                            "e2977.0",
                            "e2977.1"
                        ]
                    },
                    {
                        "text": "NetPlay with only Room obs (local short-term memory) scored 341.67 while NetPlay with Level obs (oracle long-term memory) scored 675.33 (+333.66 points)",
                        "uuids": [
                            "e2945.6"
                        ]
                    },
                    {
                        "text": "ChatGPT struggles to keep track of states of in-game objects and preconditions necessary to use actions, lacking memory mechanisms",
                        "uuids": [
                            "e2899.3"
                        ]
                    },
                    {
                        "text": "ChatGPT with walkthrough-fed context showed strong memorization on seen cases but poor generalization to unseen/reversed paths (one-step seen ~75% vs unseen ~29%), indicating limitations of pure in-context memory",
                        "uuids": [
                            "e2978.2"
                        ]
                    },
                    {
                        "text": "ReAct-IM (dense external-feedback only) achieved 53% success vs full ReAct with richer internal reasoning at 71% on ALFWorld, showing limitations of observation-only memory",
                        "uuids": [
                            "e2971.1"
                        ]
                    },
                    {
                        "text": "Act-only baseline (no thoughts/memory) achieved 45% success on ALFWorld vs ReAct with memory at 71%",
                        "uuids": [
                            "e2971.0"
                        ]
                    },
                    {
                        "text": "Baseline generative agents with full-event retrieval (no compression) incurred very large token costs (4.525M tokens) and limited behavioral diversity",
                        "uuids": [
                            "e2949.0"
                        ]
                    },
                    {
                        "text": "NPAE-Flan-T5 with same short-term prompt memory but no pretraining remained under 0.2 success rate, showing memory structure alone insufficient without learned representations",
                        "uuids": [
                            "e2974.2"
                        ]
                    },
                    {
                        "text": "Agents without planner module (limited plan retrieval) in Among Us were more likely to hit time limits and less likely to finish tasks",
                        "uuids": [
                            "e2915.0"
                        ]
                    },
                    {
                        "text": "GATA with belief graph but without proper instruction-following mechanisms failed to complete tasks despite having structured memory",
                        "uuids": [
                            "e2960.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "combines",
                        "object": "short-term working memory"
                    },
                    {
                        "subject": "agent",
                        "relation": "with",
                        "object": "long-term episodic or structured memory"
                    },
                    {
                        "subject": "memory systems",
                        "relation": "are",
                        "object": "properly integrated"
                    }
                ],
                "then": [
                    {
                        "subject": "agent performance",
                        "relation": "exceeds",
                        "object": "either system alone"
                    },
                    {
                        "subject": "performance improvement",
                        "relation": "is typically",
                        "object": "20-50 percentage points absolute"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SWIFTSAGE combining fast SWIFT (short-term) with deliberate SAGE (long-term planning) improved from 49.22 to 84.68 (+35.46 points)",
                        "uuids": [
                            "e2977.0"
                        ]
                    },
                    {
                        "text": "KARMA with dual short-term cache and long-term 3DSG improved composite task success by +43 percentage points and complex tasks by +21 percentage points",
                        "uuids": [
                            "e2925.0"
                        ]
                    },
                    {
                        "text": "SAGE with dual STM/LTM improved ALFWorld from 0.0 to 10.5-12.5 for smaller models (Qwen-1.8B, CodeLlama-7B)",
                        "uuids": [
                            "e2941.0"
                        ]
                    },
                    {
                        "text": "Memory-R1 with dual memory management (Memory Manager + Answer Agent) improved F1 by +14.61 points (+48% relative) on LO-COMO benchmark",
                        "uuids": [
                            "e2952.0"
                        ]
                    },
                    {
                        "text": "MC-DML with in-trial short-term memory and cross-trial episodic reflections improved Zork1 by +10.33 points (48.66 vs 38.33, ~+27% relative)",
                        "uuids": [
                            "e2927.0"
                        ]
                    },
                    {
                        "text": "GATA with learned belief graphs achieved +24.2% average relative improvement over text-only Tr-DQN baseline on TextWorld",
                        "uuids": [
                            "e2968.0"
                        ]
                    },
                    {
                        "text": "HiAgent with hierarchical working memory management doubled overall success rate (21% -&gt; 42%) on long-horizon tasks",
                        "uuids": [
                            "e2937.0"
                        ]
                    },
                    {
                        "text": "LLaMA-Rider with short-term history + experience replay improved from ~20% to ~34% success rate (+14 percentage points)",
                        "uuids": [
                            "e2917.0"
                        ]
                    },
                    {
                        "text": "ReAct with language-based working memory (thoughts + actions) achieved 71% success on ALFWorld vs 45% for Act-only (+26 percentage points)",
                        "uuids": [
                            "e2971.0"
                        ]
                    },
                    {
                        "text": "BUTLER with recurrent aggregator + observation queue memory achieved 40% success on TextWorld seen vs 10% for Seq2Seq baseline (+30 percentage points)",
                        "uuids": [
                            "e2969.1"
                        ]
                    },
                    {
                        "text": "ReadAgent with episodic gist memory + interactive lookup improved NarrativeQA by +12.97% LLM rating and +31.98% ROUGE-L over best retrieval baseline",
                        "uuids": [
                            "e2929.0"
                        ]
                    },
                    {
                        "text": "ThinkThrice with retrieval-augmented memory improved factual QA from 0.305 to 0.498 with full pipeline (+0.193 absolute)",
                        "uuids": [
                            "e2966.0"
                        ]
                    },
                    {
                        "text": "Reflexion with episodic reflection memory improved HotPotQA by +8% absolute over episodic-memory-only baseline",
                        "uuids": [
                            "e2933.2"
                        ]
                    },
                    {
                        "text": "AGA with dual Lifestyle Policy (case-based) and Social Memory reduced tokens to 31.1% of baseline while maintaining comparable performance",
                        "uuids": [
                            "e2949.1"
                        ]
                    },
                    {
                        "text": "MPRC-DQN with object-centric historical retrieval won 21/33 games (64% winning percentage) vs 17/33 for RC-DQN without history",
                        "uuids": [
                            "e2976.0",
                            "e2976.1"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses",
                        "object": "long-term memory"
                    },
                    {
                        "subject": "long-term memory",
                        "relation": "lacks",
                        "object": "effective retrieval mechanism"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "selective access to past information"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "suffers from",
                        "object": "information overload or irrelevant retrieval"
                    },
                    {
                        "subject": "performance",
                        "relation": "degrades compared to",
                        "object": "well-designed retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Full history memory without structure underperformed structured AriGraph (0.47 vs 1.0 on Treasure Hunt) due to inefficient retrieval and token usage (~14,000 prompt tokens at step 150)",
                        "uuids": [
                            "e2945.1"
                        ]
                    },
                    {
                        "text": "MemWalker with hierarchical traversal had 8.6% search failure rate and achieved 66.73% accuracy vs ReadAgent's 86.6% with targeted retrieval",
                        "uuids": [
                            "e2929.1"
                        ]
                    },
                    {
                        "text": "Retrieval-augmentation can underperform due to unrelated or noisy prompts, and may suffer information loss",
                        "uuids": [
                            "e2924.2"
                        ]
                    },
                    {
                        "text": "Simple short-history augmentation (one past observation + five last actions) degraded WebShop performance from 59.9 to 57.3 task score",
                        "uuids": [
                            "e2943.0"
                        ]
                    },
                    {
                        "text": "KARMA without short-term memory showed 1.9x-4.2x drops in success rate depending on task class, indicating retrieval alone insufficient",
                        "uuids": [
                            "e2925.0"
                        ]
                    },
                    {
                        "text": "Baseline generative agents with full-event retrieval (30-45 events, ~2000 tokens) had high costs and limited behavioral diversity",
                        "uuids": [
                            "e2949.0"
                        ]
                    },
                    {
                        "text": "RAG memory (unstructured vector store) achieved only 0.33 on Treasure Hunt vs AriGraph's 1.0, showing limitations of unstructured retrieval",
                        "uuids": [
                            "e2945.2"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "long-term memory",
                        "relation": "uses",
                        "object": "compression or summarization"
                    },
                    {
                        "subject": "compression",
                        "relation": "preserves",
                        "object": "task-relevant information"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "better token efficiency"
                    },
                    {
                        "subject": "performance",
                        "relation": "is maintained or improved",
                        "object": "compared to full storage"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ReadAgent with gist memory compression achieved 85.53-96.80% compression ratio while maintaining or improving performance (86.16% accuracy on QuALITY)",
                        "uuids": [
                            "e2929.0"
                        ]
                    },
                    {
                        "text": "AGA with Social Memory compression reduced conversation-relevant data from ~2000 tokens to ~100 tokens while preserving response quality",
                        "uuids": [
                            "e2949.1"
                        ]
                    },
                    {
                        "text": "HiAgent with hierarchical subgoal-based memory reduced context tokens by ~35% while improving success rate from 21% to 42%",
                        "uuids": [
                            "e2937.0"
                        ]
                    },
                    {
                        "text": "RECURRENTGPT with long-term paragraph summaries and short-term working memory enabled arbitrarily long coherent generation",
                        "uuids": [
                            "e2964.0"
                        ]
                    },
                    {
                        "text": "AMONGAGENTS with summarized evolving memory (phi_H function) enabled coherent multi-agent gameplay with planner-enabled agents showing increased win rates",
                        "uuids": [
                            "e2915.0"
                        ]
                    },
                    {
                        "text": "Diff-history compression for NetHack observations provided ~7x improvement in game score and &gt;40% improvement vs visual observations",
                        "uuids": [
                            "e2930.1"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "long-term memory",
                        "relation": "is structured as",
                        "object": "graph or relational representation"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "multi-hop reasoning or relational queries"
                    }
                ],
                "then": [
                    {
                        "subject": "agent performance",
                        "relation": "exceeds",
                        "object": "unstructured memory approaches"
                    },
                    {
                        "subject": "improvement",
                        "relation": "is particularly large for",
                        "object": "tasks with complex object relationships"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AriGraph with knowledge graph memory achieved 1.0 normalized score on Treasure Hunt vs 0.47 for full history and 0.33 for RAG",
                        "uuids": [
                            "e2945.0",
                            "e2945.1",
                            "e2945.2"
                        ]
                    },
                    {
                        "text": "GATA with learned belief graphs achieved +24.2% average relative improvement over text-only baseline on TextWorld cooking tasks",
                        "uuids": [
                            "e2968.0"
                        ]
                    },
                    {
                        "text": "KG-A2C with knowledge graph constraints substantially outperformed TDQN on 23/28 Jericho games",
                        "uuids": [
                            "e2972.3"
                        ]
                    },
                    {
                        "text": "KARMA with hierarchical 3D scene graph (long-term) plus object cache (short-term) improved composite tasks by +43 percentage points",
                        "uuids": [
                            "e2925.0"
                        ]
                    },
                    {
                        "text": "Ghost in Minecraft with text-based knowledge and memory supports broader task capabilities in open-world settings",
                        "uuids": [
                            "e2931.5"
                        ]
                    },
                    {
                        "text": "Arigraph with knowledge triples combining semantic and episodic memories supports complex reasoning tasks",
                        "uuids": [
                            "e2942.2",
                            "e2951.3"
                        ]
                    }
                ]
            }
        }
    ],
    "new_predictions_likely": [
        "An agent with a 3-level memory hierarchy (immediate context, working memory buffer, long-term structured store) will outperform 2-level systems on tasks requiring both rapid response and long-term learning by 10-20 percentage points",
        "Agents that dynamically adjust the ratio of short-term to long-term memory retrieval based on task complexity will achieve 20-40% better token efficiency than fixed-ratio systems while maintaining performance",
        "In multi-session text games, agents with persistent long-term memory across sessions will show cumulative learning gains of 15-30% per session for the first 3-5 sessions before plateauing",
        "Combining graph-structured long-term memory with compressed episodic short-term memory will outperform either alone by 15-25 percentage points on tasks requiring both relational reasoning and recent context",
        "Agents with adaptive memory compression that increases compression ratio as memory grows will maintain performance while reducing token costs by 50-70% compared to fixed compression",
        "Dual-memory agents will show 2-3x faster convergence to optimal performance compared to single-memory agents on long-horizon tasks (&gt;50 steps)"
    ],
    "new_predictions_unknown": [
        "Whether a unified memory architecture that seamlessly blends short and long-term memory (rather than separate systems) could achieve superior performance while reducing architectural complexity - this could either simplify implementation or lose the benefits of specialized memory systems",
        "Whether the optimal ratio of short-term to long-term memory capacity varies systematically with game genre (e.g., puzzle vs. social deduction vs. exploration) - different genres may have fundamentally different memory requirements",
        "Whether agents can learn to meta-optimize their own memory allocation strategies through reinforcement learning on memory management actions - this could enable adaptive memory systems but may be difficult to train",
        "Whether hierarchical memory with more than 3 levels (e.g., immediate, working, episodic, semantic, procedural) would provide additional benefits or just add complexity without gains",
        "Whether memory systems trained on one game genre transfer effectively to other genres, or if memory architectures need to be genre-specific",
        "Whether the benefits of dual-memory systems scale linearly, sublinearly, or superlinearly with task horizon length",
        "Whether biological memory principles (e.g., sleep-like consolidation, forgetting curves, interference effects) could further improve LLM agent memory systems"
    ],
    "negative_experiments": [
        "Finding text game tasks where pure short-term memory (no long-term storage) consistently outperforms dual-memory systems would challenge the necessity of long-term memory for all task types",
        "Demonstrating that randomly retrieved long-term memories perform as well as relevance-based retrieval would question the importance of sophisticated retrieval mechanisms",
        "Showing that agents with only long-term memory (no short-term context) can match dual-system performance would challenge the necessity of short-term memory",
        "Finding that unstructured memory consistently outperforms structured memory on relational reasoning tasks would challenge the value of graph-based representations",
        "Demonstrating that memory compression always degrades performance proportionally to compression ratio would challenge the viability of compressed memory systems",
        "Showing that the performance gap between dual-memory and single-memory systems disappears for very large context windows (&gt;100k tokens) would suggest dual systems are only necessary due to context limitations",
        "Finding that memory systems trained on diverse tasks perform worse than task-specific memory systems would challenge the generality of dual-memory architectures"
    ],
    "unaccounted_for": [
        {
            "text": "The exact mechanisms by which LLMs internally integrate short-term prompt context with retrieved long-term memories during generation - the attention patterns and internal representations are not well understood",
            "uuids": []
        },
        {
            "text": "Why some tasks (like simple navigation) show minimal benefit from long-term memory while others (like multi-step puzzles) show dramatic improvements - the task characteristics that determine memory importance are not fully characterized",
            "uuids": [
                "e2945.2",
                "e2945.4"
            ]
        },
        {
            "text": "The role of model size and pretraining in determining how effectively agents can utilize dual-memory architectures - NPAE-Flan-T5 failed despite same memory structure",
            "uuids": [
                "e2974.2"
            ]
        },
        {
            "text": "Why some memory compression strategies (gist, summarization) work well while others (simple truncation) fail - the principles of effective compression are not fully understood",
            "uuids": [
                "e2929.0",
                "e2943.0"
            ]
        },
        {
            "text": "The optimal memory capacity and retrieval size for different task types - most systems use fixed hyperparameters without principled justification",
            "uuids": [
                "e2925.0",
                "e2929.0",
                "e2966.0"
            ]
        },
        {
            "text": "How memory systems should handle contradictory information from different time points - conflict resolution strategies are not well studied",
            "uuids": []
        },
        {
            "text": "The interaction between memory architecture and LLM architecture (decoder-only vs encoder-decoder) - most studies use specific model types",
            "uuids": [
                "e2974.0",
                "e2977.0"
            ]
        },
        {
            "text": "Why reflection-based memory helps some agents (SA-RL) more than others (DA-RL) - the conditions for effective reflection are unclear",
            "uuids": [
                "e2907.1",
                "e2907.2"
            ]
        },
        {
            "text": "The role of memory in enabling deception and strategic behavior in social deduction games - memory's contribution to theory of mind is not well characterized",
            "uuids": [
                "e2909.0",
                "e2909.2",
                "e2909.3"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some simple tasks show that short-term memory alone can achieve near-perfect performance (SmartPlay Bandits: 1.00, RPS: 0.91), suggesting long-term memory may not always be necessary",
            "uuids": [
                "e2944.0"
            ]
        },
        {
            "text": "In some cases, adding memory increased token costs substantially without proportional performance gains (baseline generative agents: 4.525M tokens with limited diversity)",
            "uuids": [
                "e2949.0"
            ]
        },
        {
            "text": "Simple history augmentation sometimes degraded performance (WebShop: 59.9 -&gt; 57.3), suggesting naive memory addition can hurt",
            "uuids": [
                "e2943.0"
            ]
        },
        {
            "text": "GATA with belief graph memory failed to complete tasks when policy didn't exploit the stored relations, showing memory alone insufficient",
            "uuids": [
                "e2960.0"
            ]
        },
        {
            "text": "Reflexion had mixed effects for pure LLM agents, producing shorter trajectories but not clear Combined-Score improvements",
            "uuids": [
                "e2907.0"
            ]
        },
        {
            "text": "BC-GFlan-T5 trained on trajectories with memory underperformed online RL (0.69 vs 0.82), suggesting offline memory learning insufficient",
            "uuids": [
                "e2974.3"
            ]
        },
        {
            "text": "Some memory mechanisms (Uncategorized Transitions in LM-in-the-Loop) actually hurt performance (19.1% vs 20.1% baseline)",
            "uuids": [
                "e2914.0"
            ]
        },
        {
            "text": "MemWalker's hierarchical memory had high search failure rate (8.6%) and underperformed simpler approaches",
            "uuids": [
                "e2929.1"
            ]
        }
    ],
    "special_cases": [
        "For very short games (&lt; 10 steps), short-term memory alone may be sufficient and long-term memory overhead may not be justified, as seen in simple SmartPlay tasks",
        "In fully observable environments with small state spaces, the benefits of long-term memory may be minimal since all relevant information is in current observation",
        "For agents with very large context windows (&gt;100k tokens), the distinction between short and long-term memory may blur, though compression may still be valuable for efficiency",
        "In stochastic environments with high variance, episodic memory may be less valuable than semantic/procedural memory that captures general patterns",
        "For tasks requiring real-time response, the retrieval overhead of long-term memory may outweigh benefits unless retrieval is highly optimized",
        "In adversarial or deceptive scenarios, memory of opponent behavior may be more valuable than memory of environment state",
        "For procedurally generated environments, memory of specific instances may be less valuable than memory of general strategies and patterns",
        "When memory capacity is severely limited, prioritization and compression become critical - simple FIFO replacement may be insufficient",
        "For multi-agent scenarios, shared vs. private memory architectures may have different optimal designs",
        "In domains with frequent rule changes or non-stationarity, memory decay or selective forgetting may be necessary to avoid outdated information"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Atkinson & Shiffrin (1968) Human Memory: A Proposed System and its Control Processes [Classic dual-store memory model in cognitive psychology with short-term and long-term stores]",
            "Baddeley & Hitch (1974) Working Memory [Working memory model with multiple components including phonological loop and visuospatial sketchpad]",
            "Kahneman (2011) Thinking, Fast and Slow [Dual-process theory of cognition with System 1 (fast, intuitive) and System 2 (slow, deliberate)]",
            "Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Implements dual short-term and long-term memory for game agents]",
            "Zhong et al. (2024) SWIFTSAGE: A Generative Agent with Fast and Slow Thinking [Explicitly implements dual-process architecture with fast SWIFT and slow SAGE systems]",
            "Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Uses memory streams with retrieval for long-term agent behavior]",
            "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Uses episodic memory and reflection for iterative improvement]",
            "Modarressi et al. (2024) KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems [Explicitly implements dual long-term 3DSG and short-term cache]",
            "Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [Uses language-based working memory for reasoning traces]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
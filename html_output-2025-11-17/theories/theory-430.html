<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nonmyopic Budget-Aware Lookahead Principle - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-430</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-430</p>
                <p><strong>Name:</strong> Nonmyopic Budget-Aware Lookahead Principle</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of optimal resource allocation in automated scientific discovery systems, balancing computational cost of evaluation against expected information gain, probability of breakthrough discoveries, and diversity of explored hypotheses under budget constraints, based on the following results.</p>
                <p><strong>Description:</strong> Nonmyopic policies that explicitly consider remaining budget in their lookahead achieve superior allocation efficiency compared to myopic policies, with gains increasing with budget size, problem complexity, and correlation structure. The optimal lookahead horizon should match the remaining budget, and practical approximations (batch-greedy, one-step with Monte Carlo fantasies, learned policies) can achieve 80-95% of full lookahead performance at 10-1000x lower computational cost. This principle explains why ENS, MF-ENS, BKG, NBKG, and related methods outperform myopic baselines across active search, optimization, and experimental design tasks. The performance advantage is most pronounced in problems with: (1) large budgets (>50 evaluations), (2) strong spatial/temporal correlation, (3) expensive evaluations relative to computation, and (4) multimodal or needle-in-haystack objectives.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Nonmyopic policies with lookahead horizon matching remaining budget achieve 2-10x better sample efficiency than myopic policies in active search and optimization tasks with budgets >50 evaluations</li>
                <li>The performance gain from nonmyopic planning increases monotonically with: (1) budget size (2-5x gain at budget=100, 5-10x at budget=1000), (2) problem complexity/multimodality (3-8x in multimodal vs 1.5-3x in unimodal), (3) correlation structure strength (4-10x in high-correlation vs 1.2-2x in low-correlation)</li>
                <li>Batch-greedy approximations (assuming remaining budget spent in one batch) achieve 80-95% of full dynamic programming performance at 10-100x lower computational cost</li>
                <li>One-step lookahead with Monte Carlo approximation of expectations (Q=1000-10000 samples) provides good tradeoff between computational cost and performance, achieving 70-90% of multi-step lookahead benefit</li>
                <li>Nonmyopic planning naturally implements adaptive exploration-exploitation by considering future opportunities: exploration probability decreases from ~0.7-0.9 early (large remaining budget) to ~0.1-0.3 late (small remaining budget)</li>
                <li>Amortization via learned policies (ANS, DAGGER-trained networks) can reduce per-decision cost by 100-1000x while maintaining 85-95% of expert nonmyopic performance</li>
                <li>The computational cost of lookahead scales as O(|candidates|^depth * MC_samples) for Monte Carlo approximations, making depth>2 impractical for large candidate pools (>1000) without approximations</li>
                <li>Nonmyopic policies show diminishing returns: 1-step lookahead captures 70-85% of infinite-horizon value, 2-step captures 85-95%, 3-step captures 90-98%</li>
                <li>Budget-aware lookahead is most beneficial when evaluation cost >> computation cost (ratio >100x); when ratio <10x, myopic policies may be preferable due to lower overhead</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>ENS with budget-aware batch-greedy lookahead consistently achieves best performance in active search, outperforming greedy one-step and UCB variants across multiple datasets <a href="../results/extraction-result-2496.html#e2496.1" class="evidence-link">[e2496.1]</a> </li>
    <li>MF-ENS extends ENS to multifidelity settings and shows linearly increasing advantage over single-fidelity ENS (MF-UCB) in cumulative discoveries, with statistically significant margins <a href="../results/extraction-result-2498.html#e2498.3" class="evidence-link">[e2498.3]</a> <a href="../results/extraction-result-2498.html#e2498.2" class="evidence-link">[e2498.2]</a> </li>
    <li>BKG (Batch Knowledge Gradient) with one-step lookahead and Monte Carlo approximation finds optimum in ~2 iterations on average vs >10 for exploitation/random in SMA example, representing ~5x efficiency gain <a href="../results/extraction-result-2595.html#e2595.1" class="evidence-link">[e2595.1]</a> </li>
    <li>NBKG (Nested-Batch Knowledge Gradient) uses batch-mode lookahead to reduce opportunity cost and achieves competitive performance with sequential KG while enabling parallel evaluation <a href="../results/extraction-result-2595.html#e2595.1" class="evidence-link">[e2595.1]</a> </li>
    <li>ANS learns to mimic ENS via DAGGER, achieving similar performance at orders-of-magnitude lower per-decision cost (100-1000x speedup) while retaining 90-95% of expert performance <a href="../results/extraction-result-2496.html#e2496.5" class="evidence-link">[e2496.5]</a> <a href="../results/extraction-result-2496.html#e2496.1" class="evidence-link">[e2496.1]</a> </li>
    <li>LA-MCTS with UCB lookahead that considers remaining budget in trust-region allocation outperforms myopic local optimization (TuRBO) and achieves better sample efficiency in high-dimensional problems <a href="../results/extraction-result-2608.html#e2608.0" class="evidence-link">[e2608.0]</a> <a href="../results/extraction-result-2608.html#e2608.4" class="evidence-link">[e2608.4]</a> </li>
    <li>MOCU framework with one-step lookahead R(Q|x) = E_x[MOCU(Q|x)] achieves faster convergence than myopic uncertainty reduction, finding optimum in ~2 iterations vs >10 for exploitation/random <a href="../results/extraction-result-2485.html#e2485.0" class="evidence-link">[e2485.0]</a> </li>
    <li>RAAL's MILP-based resource-aware allocation with lookahead over capacity constraints achieves 2-4x reductions in iterations-to-convergence compared to sequential BO <a href="../results/extraction-result-2464.html#e2464.3" class="evidence-link">[e2464.3]</a> </li>
    <li>Chimera's threshold-driven switching between exploration (UCB) and exploitation (EI) based on monitoring average GP uncertainty implements adaptive lookahead, achieving 20-38% fewer iterations to convergence vs pure EI or UCB <a href="../results/extraction-result-2605.html#e2605.0" class="evidence-link">[e2605.0]</a> <a href="../results/extraction-result-2410.html#e2410.0" class="evidence-link">[e2410.0]</a> </li>
    <li>GP-UCB with theoretical regret bounds O(sqrt(T * gamma_T)) demonstrates that policies considering cumulative regret over horizon T outperform myopic selection <a href="../results/extraction-result-2584.html#e2584.2" class="evidence-link">[e2584.2]</a> </li>
    <li>Knowledge Gradient (KG) policies that maximize expected single-step improvement in decision value outperform myopic greedy selection in ranking-and-selection problems <a href="../results/extraction-result-2506.html#e2506.12" class="evidence-link">[e2506.12]</a> </li>
    <li>PEE (Probabilistic Exploration-Exploitation) with time-decaying exploration probability p_R = α^(t-1) implements implicit lookahead by allocating more exploration early when budget is large, achieving 2-4% revenue gains over pure exploitation <a href="../results/extraction-result-2506.html#e2506.7" class="evidence-link">[e2506.7]</a> </li>
    <li>TDUE-BO's threshold-driven switching from UCB to EI based on average GP uncertainty implements adaptive lookahead, achieving 21-32% RMSE reduction and 30-50% faster convergence vs pure EI or UCB <a href="../results/extraction-result-2410.html#e2410.0" class="evidence-link">[e2410.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Multi-step lookahead (2-3 steps) with efficient approximations (sparse sampling, learned value functions) would outperform one-step by 10-20% in complex problems (multimodal, high-dimensional) while remaining computationally tractable (2-5x cost increase)</li>
                <li>Adaptive lookahead depth (deeper when budget large and problem complex, shallower when budget small or problem simple) would improve efficiency by 15-25% compared to fixed-depth lookahead</li>
                <li>Learned lookahead policies (meta-learned across problem families via meta-RL or transfer learning) would achieve better cost-performance tradeoffs than hand-designed approximations, reducing per-decision cost by additional 2-5x</li>
                <li>Nonmyopic batch selection with explicit diversity constraints (DPP, qVS) would outperform sequential nonmyopic selection by 20-40% in parallel settings with batch sizes >5</li>
                <li>Hybrid policies that switch between myopic and nonmyopic based on problem features (budget remaining, uncertainty level, correlation strength) would achieve 90-95% of always-nonmyopic performance at 50-70% of computational cost</li>
                <li>Nonmyopic policies with learned surrogate models (neural networks, GPs with learned kernels) would outperform those with fixed models by 15-30% in problems with complex structure</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether nonmyopic planning remains beneficial when evaluation costs are highly stochastic and unpredictable (variance >50% of mean cost) - lookahead may be less valuable if future costs cannot be reliably estimated</li>
                <li>Whether there exist problem classes where myopic policies are provably optimal (making lookahead unnecessary) - candidates include problems with no correlation structure, very small budgets (<10), or adversarial/worst-case objectives</li>
                <li>Whether nonmyopic planning can be effectively combined with safety constraints in high-stakes domains (medical, autonomous systems) where exploration must be severely limited - the exploration-exploitation balance may be fundamentally different</li>
                <li>Whether the computational cost of lookahead can be reduced to near-myopic levels (<2x overhead) via better approximations (learned models, sparse sampling, parallel computation) or learned models while maintaining >90% of performance benefit</li>
                <li>Whether nonmyopic policies can handle non-stationary objectives (changing over time) effectively - current theory assumes stationary objectives</li>
                <li>Whether multi-objective nonmyopic planning (considering tradeoffs across multiple objectives in lookahead) provides additional benefits beyond single-objective lookahead applied to scalarized objectives</li>
                <li>Whether nonmyopic planning with model uncertainty (Bayesian model averaging, ensemble methods) provides significant benefits over nonmyopic planning with point-estimate models</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding problems where myopic policies consistently outperform nonmyopic policies (>20% better) would challenge the universality claim - candidates include very low correlation problems, adversarial objectives, or problems where myopic heuristics align perfectly with optimal policy</li>
                <li>Demonstrating that random lookahead (considering random future scenarios rather than optimal/expected scenarios) performs as well as optimal lookahead (within 10%) would undermine the theory's claim that budget-aware planning is essential</li>
                <li>Showing that lookahead depth has no impact on performance (1-step, 2-step, 3-step all within 5% performance) would contradict the diminishing-returns prediction and suggest lookahead is not the key mechanism</li>
                <li>Finding that amortized policies cannot achieve >50% of expert performance even with extensive training would limit the practical applicability of learned nonmyopic policies</li>
                <li>Demonstrating that the computational cost of lookahead cannot be reduced below 10x myopic cost while maintaining >70% performance benefit would suggest fundamental limits to practical nonmyopic planning</li>
                <li>Finding that nonmyopic policies perform worse than myopic in non-stationary or adversarial settings would reveal important boundary conditions</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory doesn't specify how to handle non-stationary problems where the objective changes over time - lookahead assumptions may break down <a href="../results/extraction-result-2496.html#e2496.1" class="evidence-link">[e2496.1]</a> </li>
    <li>Optimal lookahead strategies for multi-objective problems with conflicting objectives need more analysis - current theory focuses on single-objective or scalarized objectives <a href="../results/extraction-result-2498.html#e2498.3" class="evidence-link">[e2498.3]</a> <a href="../results/extraction-result-2423.html#e2423.0" class="evidence-link">[e2423.0]</a> </li>
    <li>The role of model uncertainty in lookahead planning is not fully characterized - how should lookahead account for surrogate model errors? <a href="../results/extraction-result-2595.html#e2595.1" class="evidence-link">[e2595.1]</a> <a href="../results/extraction-result-2485.html#e2485.0" class="evidence-link">[e2485.0]</a> </li>
    <li>The theory doesn't address how to handle constraints (safety, feasibility, resource) in nonmyopic planning - constrained lookahead is more complex <a href="../results/extraction-result-2476.html#e2476.0" class="evidence-link">[e2476.0]</a> <a href="../results/extraction-result-2477.html#e2477.1" class="evidence-link">[e2477.1]</a> </li>
    <li>The interaction between nonmyopic planning and diversity promotion is not fully understood - how should lookahead balance information gain and diversity? <a href="../results/extraction-result-2422.html#e2422.2" class="evidence-link">[e2422.2]</a> <a href="../results/extraction-result-2630.html#e2630.2" class="evidence-link">[e2630.2]</a> </li>
    <li>The theory doesn't specify how to handle very high-dimensional problems (>100 dimensions) where lookahead becomes computationally prohibitive even with approximations <a href="../results/extraction-result-2608.html#e2608.0" class="evidence-link">[e2608.0]</a> </li>
    <li>The optimal balance between lookahead depth and breadth (number of scenarios considered) is not characterized <a href="../results/extraction-result-2595.html#e2595.1" class="evidence-link">[e2595.1]</a> </li>
    <li>The theory doesn't address how to handle partial observability or delayed feedback where lookahead must account for information arrival timing <a href="../results/extraction-result-2496.html#e2496.1" class="evidence-link">[e2496.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Frazier et al. (2009) The Knowledge-Gradient Policy for Correlated Normal Beliefs [Knowledge gradient and one-step lookahead for ranking and selection]</li>
    <li>Garnett et al. (2012) Bayesian Optimization for Sensor Set Selection [Nonmyopic sensor placement with lookahead]</li>
    <li>Jiang et al. (2020) Efficient Nonmyopic Active Search [ENS algorithm and batch-greedy approximation for active search]</li>
    <li>Lam et al. (2016) The Knowledge-Gradient Policy Using a Sparse Additive Belief Model [KG with approximations for high-dimensional problems]</li>
    <li>Kandasamy et al. (2017) Multi-fidelity Bayesian Optimisation with Continuous Approximations [Multi-fidelity nonmyopic planning]</li>
    <li>Jiang et al. (2021) Nonmyopic Multifidelity Active Search [MF-ENS extending nonmyopic planning to multifidelity settings]</li>
    <li>Moss et al. (2021) GIBBON: General-purpose Information-Based Bayesian OptimisatioN [Information-theoretic nonmyopic acquisition]</li>
    <li>Srinivas et al. (2010) Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design [GP-UCB with cumulative regret bounds]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Nonmyopic Budget-Aware Lookahead Principle",
    "theory_description": "Nonmyopic policies that explicitly consider remaining budget in their lookahead achieve superior allocation efficiency compared to myopic policies, with gains increasing with budget size, problem complexity, and correlation structure. The optimal lookahead horizon should match the remaining budget, and practical approximations (batch-greedy, one-step with Monte Carlo fantasies, learned policies) can achieve 80-95% of full lookahead performance at 10-1000x lower computational cost. This principle explains why ENS, MF-ENS, BKG, NBKG, and related methods outperform myopic baselines across active search, optimization, and experimental design tasks. The performance advantage is most pronounced in problems with: (1) large budgets (&gt;50 evaluations), (2) strong spatial/temporal correlation, (3) expensive evaluations relative to computation, and (4) multimodal or needle-in-haystack objectives.",
    "supporting_evidence": [
        {
            "text": "ENS with budget-aware batch-greedy lookahead consistently achieves best performance in active search, outperforming greedy one-step and UCB variants across multiple datasets",
            "uuids": [
                "e2496.1"
            ]
        },
        {
            "text": "MF-ENS extends ENS to multifidelity settings and shows linearly increasing advantage over single-fidelity ENS (MF-UCB) in cumulative discoveries, with statistically significant margins",
            "uuids": [
                "e2498.3",
                "e2498.2"
            ]
        },
        {
            "text": "BKG (Batch Knowledge Gradient) with one-step lookahead and Monte Carlo approximation finds optimum in ~2 iterations on average vs &gt;10 for exploitation/random in SMA example, representing ~5x efficiency gain",
            "uuids": [
                "e2595.1"
            ]
        },
        {
            "text": "NBKG (Nested-Batch Knowledge Gradient) uses batch-mode lookahead to reduce opportunity cost and achieves competitive performance with sequential KG while enabling parallel evaluation",
            "uuids": [
                "e2595.1"
            ]
        },
        {
            "text": "ANS learns to mimic ENS via DAGGER, achieving similar performance at orders-of-magnitude lower per-decision cost (100-1000x speedup) while retaining 90-95% of expert performance",
            "uuids": [
                "e2496.5",
                "e2496.1"
            ]
        },
        {
            "text": "LA-MCTS with UCB lookahead that considers remaining budget in trust-region allocation outperforms myopic local optimization (TuRBO) and achieves better sample efficiency in high-dimensional problems",
            "uuids": [
                "e2608.0",
                "e2608.4"
            ]
        },
        {
            "text": "MOCU framework with one-step lookahead R(Q|x) = E_x[MOCU(Q|x)] achieves faster convergence than myopic uncertainty reduction, finding optimum in ~2 iterations vs &gt;10 for exploitation/random",
            "uuids": [
                "e2485.0"
            ]
        },
        {
            "text": "RAAL's MILP-based resource-aware allocation with lookahead over capacity constraints achieves 2-4x reductions in iterations-to-convergence compared to sequential BO",
            "uuids": [
                "e2464.3"
            ]
        },
        {
            "text": "Chimera's threshold-driven switching between exploration (UCB) and exploitation (EI) based on monitoring average GP uncertainty implements adaptive lookahead, achieving 20-38% fewer iterations to convergence vs pure EI or UCB",
            "uuids": [
                "e2605.0",
                "e2410.0"
            ]
        },
        {
            "text": "GP-UCB with theoretical regret bounds O(sqrt(T * gamma_T)) demonstrates that policies considering cumulative regret over horizon T outperform myopic selection",
            "uuids": [
                "e2584.2"
            ]
        },
        {
            "text": "Knowledge Gradient (KG) policies that maximize expected single-step improvement in decision value outperform myopic greedy selection in ranking-and-selection problems",
            "uuids": [
                "e2506.12"
            ]
        },
        {
            "text": "PEE (Probabilistic Exploration-Exploitation) with time-decaying exploration probability p_R = α^(t-1) implements implicit lookahead by allocating more exploration early when budget is large, achieving 2-4% revenue gains over pure exploitation",
            "uuids": [
                "e2506.7"
            ]
        },
        {
            "text": "TDUE-BO's threshold-driven switching from UCB to EI based on average GP uncertainty implements adaptive lookahead, achieving 21-32% RMSE reduction and 30-50% faster convergence vs pure EI or UCB",
            "uuids": [
                "e2410.0"
            ]
        }
    ],
    "theory_statements": [
        "Nonmyopic policies with lookahead horizon matching remaining budget achieve 2-10x better sample efficiency than myopic policies in active search and optimization tasks with budgets &gt;50 evaluations",
        "The performance gain from nonmyopic planning increases monotonically with: (1) budget size (2-5x gain at budget=100, 5-10x at budget=1000), (2) problem complexity/multimodality (3-8x in multimodal vs 1.5-3x in unimodal), (3) correlation structure strength (4-10x in high-correlation vs 1.2-2x in low-correlation)",
        "Batch-greedy approximations (assuming remaining budget spent in one batch) achieve 80-95% of full dynamic programming performance at 10-100x lower computational cost",
        "One-step lookahead with Monte Carlo approximation of expectations (Q=1000-10000 samples) provides good tradeoff between computational cost and performance, achieving 70-90% of multi-step lookahead benefit",
        "Nonmyopic planning naturally implements adaptive exploration-exploitation by considering future opportunities: exploration probability decreases from ~0.7-0.9 early (large remaining budget) to ~0.1-0.3 late (small remaining budget)",
        "Amortization via learned policies (ANS, DAGGER-trained networks) can reduce per-decision cost by 100-1000x while maintaining 85-95% of expert nonmyopic performance",
        "The computational cost of lookahead scales as O(|candidates|^depth * MC_samples) for Monte Carlo approximations, making depth&gt;2 impractical for large candidate pools (&gt;1000) without approximations",
        "Nonmyopic policies show diminishing returns: 1-step lookahead captures 70-85% of infinite-horizon value, 2-step captures 85-95%, 3-step captures 90-98%",
        "Budget-aware lookahead is most beneficial when evaluation cost &gt;&gt; computation cost (ratio &gt;100x); when ratio &lt;10x, myopic policies may be preferable due to lower overhead"
    ],
    "new_predictions_likely": [
        "Multi-step lookahead (2-3 steps) with efficient approximations (sparse sampling, learned value functions) would outperform one-step by 10-20% in complex problems (multimodal, high-dimensional) while remaining computationally tractable (2-5x cost increase)",
        "Adaptive lookahead depth (deeper when budget large and problem complex, shallower when budget small or problem simple) would improve efficiency by 15-25% compared to fixed-depth lookahead",
        "Learned lookahead policies (meta-learned across problem families via meta-RL or transfer learning) would achieve better cost-performance tradeoffs than hand-designed approximations, reducing per-decision cost by additional 2-5x",
        "Nonmyopic batch selection with explicit diversity constraints (DPP, qVS) would outperform sequential nonmyopic selection by 20-40% in parallel settings with batch sizes &gt;5",
        "Hybrid policies that switch between myopic and nonmyopic based on problem features (budget remaining, uncertainty level, correlation strength) would achieve 90-95% of always-nonmyopic performance at 50-70% of computational cost",
        "Nonmyopic policies with learned surrogate models (neural networks, GPs with learned kernels) would outperform those with fixed models by 15-30% in problems with complex structure"
    ],
    "new_predictions_unknown": [
        "Whether nonmyopic planning remains beneficial when evaluation costs are highly stochastic and unpredictable (variance &gt;50% of mean cost) - lookahead may be less valuable if future costs cannot be reliably estimated",
        "Whether there exist problem classes where myopic policies are provably optimal (making lookahead unnecessary) - candidates include problems with no correlation structure, very small budgets (&lt;10), or adversarial/worst-case objectives",
        "Whether nonmyopic planning can be effectively combined with safety constraints in high-stakes domains (medical, autonomous systems) where exploration must be severely limited - the exploration-exploitation balance may be fundamentally different",
        "Whether the computational cost of lookahead can be reduced to near-myopic levels (&lt;2x overhead) via better approximations (learned models, sparse sampling, parallel computation) or learned models while maintaining &gt;90% of performance benefit",
        "Whether nonmyopic policies can handle non-stationary objectives (changing over time) effectively - current theory assumes stationary objectives",
        "Whether multi-objective nonmyopic planning (considering tradeoffs across multiple objectives in lookahead) provides additional benefits beyond single-objective lookahead applied to scalarized objectives",
        "Whether nonmyopic planning with model uncertainty (Bayesian model averaging, ensemble methods) provides significant benefits over nonmyopic planning with point-estimate models"
    ],
    "negative_experiments": [
        "Finding problems where myopic policies consistently outperform nonmyopic policies (&gt;20% better) would challenge the universality claim - candidates include very low correlation problems, adversarial objectives, or problems where myopic heuristics align perfectly with optimal policy",
        "Demonstrating that random lookahead (considering random future scenarios rather than optimal/expected scenarios) performs as well as optimal lookahead (within 10%) would undermine the theory's claim that budget-aware planning is essential",
        "Showing that lookahead depth has no impact on performance (1-step, 2-step, 3-step all within 5% performance) would contradict the diminishing-returns prediction and suggest lookahead is not the key mechanism",
        "Finding that amortized policies cannot achieve &gt;50% of expert performance even with extensive training would limit the practical applicability of learned nonmyopic policies",
        "Demonstrating that the computational cost of lookahead cannot be reduced below 10x myopic cost while maintaining &gt;70% performance benefit would suggest fundamental limits to practical nonmyopic planning",
        "Finding that nonmyopic policies perform worse than myopic in non-stationary or adversarial settings would reveal important boundary conditions"
    ],
    "unaccounted_for": [
        {
            "text": "The theory doesn't specify how to handle non-stationary problems where the objective changes over time - lookahead assumptions may break down",
            "uuids": [
                "e2496.1"
            ]
        },
        {
            "text": "Optimal lookahead strategies for multi-objective problems with conflicting objectives need more analysis - current theory focuses on single-objective or scalarized objectives",
            "uuids": [
                "e2498.3",
                "e2423.0"
            ]
        },
        {
            "text": "The role of model uncertainty in lookahead planning is not fully characterized - how should lookahead account for surrogate model errors?",
            "uuids": [
                "e2595.1",
                "e2485.0"
            ]
        },
        {
            "text": "The theory doesn't address how to handle constraints (safety, feasibility, resource) in nonmyopic planning - constrained lookahead is more complex",
            "uuids": [
                "e2476.0",
                "e2477.1"
            ]
        },
        {
            "text": "The interaction between nonmyopic planning and diversity promotion is not fully understood - how should lookahead balance information gain and diversity?",
            "uuids": [
                "e2422.2",
                "e2630.2"
            ]
        },
        {
            "text": "The theory doesn't specify how to handle very high-dimensional problems (&gt;100 dimensions) where lookahead becomes computationally prohibitive even with approximations",
            "uuids": [
                "e2608.0"
            ]
        },
        {
            "text": "The optimal balance between lookahead depth and breadth (number of scenarios considered) is not characterized",
            "uuids": [
                "e2595.1"
            ]
        },
        {
            "text": "The theory doesn't address how to handle partial observability or delayed feedback where lookahead must account for information arrival timing",
            "uuids": [
                "e2496.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some myopic policies (EI, UCB, LCB) work very well in practice and are widely used, often achieving 80-95% of nonmyopic performance at much lower computational cost",
            "uuids": [
                "e2617.1",
                "e2479.1",
                "e2635.1"
            ]
        },
        {
            "text": "Computational cost of lookahead can be prohibitive in high-dimensional or large-pool settings (&gt;10000 candidates), making myopic policies the only practical choice",
            "uuids": [
                "e2496.1",
                "e2630.7"
            ]
        },
        {
            "text": "In some low-dimensional, unimodal problems, myopic EI achieves near-optimal performance, suggesting lookahead provides minimal benefit in simple settings",
            "uuids": [
                "e2597.2",
                "e2617.1"
            ]
        },
        {
            "text": "Random search and simple heuristics sometimes outperform sophisticated nonmyopic policies in very noisy or adversarial settings, suggesting limits to lookahead benefits",
            "uuids": [
                "e2412.3",
                "e2445.4"
            ]
        },
        {
            "text": "Some studies show that the performance difference between myopic and nonmyopic policies decreases as the number of evaluations increases, suggesting diminishing returns to lookahead in large-budget settings",
            "uuids": [
                "e2479.2"
            ]
        },
        {
            "text": "Greedy policies can outperform nonmyopic policies in some exploitation-focused tasks where the goal is to quickly find a good solution rather than explore thoroughly",
            "uuids": [
                "e2506.6",
                "e2506.7"
            ]
        }
    ],
    "special_cases": [
        "In very small budget regimes (&lt;10 evaluations), myopic policies may be sufficient as lookahead benefits are minimal (5-15% improvement) and may not justify computational overhead",
        "For problems with very weak correlation structure (correlation length &lt;&lt; domain size), lookahead provides little benefit (10-20% improvement) over myopic selection as future observations provide little information about distant regions",
        "When evaluation costs dominate computational costs by &gt;1000x, even expensive lookahead (100x myopic cost) is justified as it can reduce total evaluations by 2-10x",
        "In online/streaming settings where the problem changes between evaluations, lookahead must be recomputed frequently, favoring fast approximations or learned policies over exact lookahead",
        "For unimodal, low-dimensional problems (&lt;5 dimensions), myopic EI often achieves 90-95% of nonmyopic performance, making lookahead less critical",
        "In safety-critical domains with hard constraints, nonmyopic planning may need to be conservative (pessimistic lookahead) rather than optimistic, changing the exploration-exploitation balance",
        "For problems with very large candidate pools (&gt;10000), approximate lookahead (sampling, clustering, learned models) is necessary as exact lookahead is computationally infeasible",
        "In multi-fidelity settings, nonmyopic planning must consider both which candidate to evaluate and which fidelity to use, adding complexity but also opportunity for greater efficiency gains (5-20x)",
        "When surrogate model uncertainty is high (early in optimization), lookahead may be less reliable and myopic policies may be preferable until sufficient data is collected"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Frazier et al. (2009) The Knowledge-Gradient Policy for Correlated Normal Beliefs [Knowledge gradient and one-step lookahead for ranking and selection]",
            "Garnett et al. (2012) Bayesian Optimization for Sensor Set Selection [Nonmyopic sensor placement with lookahead]",
            "Jiang et al. (2020) Efficient Nonmyopic Active Search [ENS algorithm and batch-greedy approximation for active search]",
            "Lam et al. (2016) The Knowledge-Gradient Policy Using a Sparse Additive Belief Model [KG with approximations for high-dimensional problems]",
            "Kandasamy et al. (2017) Multi-fidelity Bayesian Optimisation with Continuous Approximations [Multi-fidelity nonmyopic planning]",
            "Jiang et al. (2021) Nonmyopic Multifidelity Active Search [MF-ENS extending nonmyopic planning to multifidelity settings]",
            "Moss et al. (2021) GIBBON: General-purpose Information-Based Bayesian OptimisatioN [Information-theoretic nonmyopic acquisition]",
            "Srinivas et al. (2010) Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design [GP-UCB with cumulative regret bounds]"
        ]
    },
    "reflected_from_theory_index": 4,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
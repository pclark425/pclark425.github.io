<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Fidelity Correlation-Cost Efficiency Principle - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-431</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-431</p>
                <p><strong>Name:</strong> Multi-Fidelity Correlation-Cost Efficiency Principle</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of optimal resource allocation in automated scientific discovery systems, balancing computational cost of evaluation against expected information gain, probability of breakthrough discoveries, and diversity of explored hypotheses under budget constraints, based on the following results.</p>
                <p><strong>Description:</strong> In multi-fidelity automated discovery systems, the efficiency gain from using low-fidelity models is determined by the interplay of three factors: (1) correlation ρ with high-fidelity (predictive accuracy), (2) relative cost ratio c (computational or experimental cost), and (3) allocation strategy effectiveness. Optimal resource allocation follows a correlation-cost threshold principle: low-fidelity models should be used extensively when ρ > 0.7 and cost ratio > 10x, with efficiency gains scaling approximately as ρ²/c for linear coregionalization models. However, gains diminish rapidly when ρ < 0.5 or cost ratio < 3x. The choice of multi-fidelity surrogate model matters: coregionalization methods (ICM with shared kernels, PCM with empirical correlations) provide better practical tradeoffs than full multivariate models (MGP) when inter-fidelity relationships are approximately linear and training data is limited, due to lower hyperparameter complexity. Allocation should be adaptive: early exploration uses low-fidelity extensively, while late-stage exploitation and final validation require high-fidelity. The principle extends to hierarchical multi-fidelity (>2 levels) where intermediate fidelities can provide additional efficiency when they fill correlation-cost gaps.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Multi-fidelity efficiency gain G scales approximately as G ≈ (ρ²/c) for linear coregionalization models, where ρ is correlation with high-fidelity and c is relative cost, achieving gains >5x when ρ > 0.7 and c < 0.1</li>
                <li>Low-fidelity models should be allocated extensively for exploration when correlation ρ > 0.7 and cost ratio > 10x, with allocation shifting toward high-fidelity as search converges</li>
                <li>High-fidelity evaluations should be reserved for: (1) final validation, (2) exploitation near optima, (3) regions where low-fidelity is known to be inaccurate, (4) safety-critical assessments</li>
                <li>Coregionalization methods (ICM with shared kernel, PCM with empirical correlations) provide better practical tradeoffs than full multivariate models (MGP) when: (1) inter-fidelity relationships are approximately linear, (2) training data is limited, (3) hyperparameter optimization is expensive</li>
                <li>The optimal fidelity allocation strategy is adaptive and shifts toward high-fidelity as: (1) total budget increases, (2) search converges to promising regions, (3) low-fidelity correlation degrades locally, (4) exploitation phase begins</li>
                <li>Multi-fidelity benefits diminish when correlation ρ < 0.5 or cost ratio < 3x, making single-fidelity approaches competitive or superior</li>
                <li>Hierarchical multi-fidelity (>2 fidelity levels) can achieve better efficiency than two-level when intermediate fidelities exist that fill correlation-cost gaps in the fidelity spectrum</li>
                <li>Batch multi-fidelity allocation should balance: (1) within-batch diversity, (2) fidelity-level diversity, (3) cost constraints per batch, with methods like MILP optimization providing near-optimal solutions</li>
                <li>Nonmyopic multi-fidelity policies (e.g., MF-ENS) that consider future cheap-query outcomes outperform myopic policies (e.g., MF-UCB) when budget-aware lookahead is computationally feasible</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>csKG with ICM/PCM surrogates shows 5-13x cost reduction depending on fidelity correlation and cost ratios in materials optimization <a href="../results/extraction-result-2633.html#e2633.6" class="evidence-link">[e2633.6]</a> </li>
    <li>MGP can perform well when hyperparameters are well-trained (intersection training) but suffers when the hyperparameter landscape is large/noisy, making PCM or ICM preferable in high-dimensional hyperparameter scenarios <a href="../results/extraction-result-2633.html#e2633.6" class="evidence-link">[e2633.6]</a> </li>
    <li>MFMES divides information gain by cost λ(l) and prefers low-cost fidelities for exploration, performing well when full fidelity library exists and in high-dimensional/multimodal problems <a href="../results/extraction-result-2493.html#e2493.7" class="evidence-link">[e2493.7]</a> </li>
    <li>MF-ENS leverages cheap L queries to inform expensive H queries via nonmyopic lookahead, showing linearly increasing advantage over single-fidelity ENS in cumulative discoveries <a href="../results/extraction-result-2498.html#e2498.3" class="evidence-link">[e2498.3]</a> </li>
    <li>MFEI uses correlation-based discount factor α₁(x,m) = corr(f^(m), f^(M)) to weight lower fidelities, reducing utility for lower fidelities when correlation is low <a href="../results/extraction-result-2464.html#e2464.1" class="evidence-link">[e2464.1]</a> </li>
    <li>MLMC/multilevel methods achieve potentially orders-of-magnitude efficiency gains when variance decay with level is faster than cost growth, with optimal sample allocation per level derived from variance and cost rates <a href="../results/extraction-result-2626.html#e2626.6" class="evidence-link">[e2626.6]</a> </li>
    <li>Multi-fidelity BO literature shows significant budget reductions vs single-fidelity across benchmark suites when cost ratios are large and correlations are high <a href="../results/extraction-result-2493.html#e2493.0" class="evidence-link">[e2493.0]</a> </li>
    <li>RAAL's MILP-based allocation explicitly optimizes multi-fidelity resource allocation under per-CPU capacity constraints, selecting (x,m) pairs to maximize aggregate MFEI subject to cost λ(m) <a href="../results/extraction-result-2464.html#e2464.3" class="evidence-link">[e2464.3]</a> </li>
    <li>Budgeted-Batch BO composes batches under explicit budget constraints, selecting points to maximize expected return while respecting total cost limits <a href="../results/extraction-result-2635.html#e2635.6" class="evidence-link">[e2635.6]</a> </li>
    <li>BMA+BO framework mentions multi-information-source optimization to choose the information source with best expected utility per cost <a href="../results/extraction-result-2485.html#e2485.1" class="evidence-link">[e2485.1]</a> </li>
    <li>EGO multifidelity extensions adaptively construct or correct surrogates using low- and high-fidelity data to reduce number of high-fidelity evaluations required <a href="../results/extraction-result-2626.html#e2626.3" class="evidence-link">[e2626.3]</a> </li>
    <li>MF-UCB uses separate β parameters for L and H queries to prioritize exploration on cheap fidelities (large β on L) and exploitation on expensive fidelities (small β on H) <a href="../results/extraction-result-2498.html#e2498.3" class="evidence-link">[e2498.3]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Adaptive fidelity selection that learns local correlation estimates ρ(x) across the search space would improve efficiency by 20-40% compared to fixed global correlation assumptions</li>
                <li>Hierarchical multi-fidelity with 3-4 fidelity levels would achieve 1.5-2x better efficiency than two-level when intermediate fidelities with cost ratios of 3-5x and correlations 0.6-0.8 are available</li>
                <li>Learned fidelity correlation models from previous campaigns (transfer learning) would reduce initial exploration overhead by 30-50% by providing better initial correlation estimates</li>
                <li>Multi-fidelity with adaptive correlation estimation and bias correction would be 2-3x more robust than fixed correlation assumptions when low-fidelity models have spatially-varying accuracy</li>
                <li>Combining multi-fidelity with multi-objective optimization (e.g., EHVI with multiple fidelities) would achieve similar Pareto front quality with 5-10x fewer high-fidelity evaluations</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether multi-fidelity approaches remain beneficial when low-fidelity models have systematic bias (not just noise) that varies non-smoothly across the design space</li>
                <li>Whether there exist problem classes where single high-fidelity is always superior to multi-fidelity approaches regardless of cost ratio (e.g., highly discontinuous objectives)</li>
                <li>Whether multi-fidelity can be effectively combined with safety constraints when low-fidelity is unreliable for safety assessment and high-fidelity safety tests are prohibitively expensive</li>
                <li>Whether the correlation-cost efficiency principle extends to discrete/categorical fidelity choices (e.g., different simulation codes) rather than continuous approximation hierarchies</li>
                <li>Whether multi-fidelity efficiency gains persist in very high-dimensional spaces (d > 100) where curse of dimensionality affects both fidelity levels</li>
                <li>Whether ensemble methods combining multiple low-fidelity models can achieve better effective correlation than any single low-fidelity model, and if so, what the optimal ensemble size is</li>
                <li>Whether active learning of fidelity correlations (querying both fidelities strategically to learn ρ(x)) is worth the additional high-fidelity cost compared to passive correlation estimation</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding problems where low-fidelity models with high correlation (ρ > 0.8) and very low cost (c < 0.01) provide no efficiency benefit would challenge the fundamental correlation-cost principle</li>
                <li>Demonstrating that full multivariate models (MGP) always outperform coregionalization methods (ICM/PCM) even with limited training data would contradict the practical tradeoff claim</li>
                <li>Showing that fidelity allocation is independent of correlation and cost (i.e., random fidelity selection performs as well as optimized allocation) would undermine the efficiency formula</li>
                <li>Finding that multi-fidelity approaches consistently lead to worse final solutions (not just slower convergence) compared to single high-fidelity would be fundamentally problematic</li>
                <li>Demonstrating that the efficiency gain does not scale with cost ratio (i.e., 100x cost ratio gives same gain as 10x) would challenge the c^(-1) scaling</li>
                <li>Finding that early-stage low-fidelity exploration consistently leads to worse final outcomes than immediate high-fidelity exploitation would contradict the adaptive allocation strategy</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory doesn't fully address how to handle non-stationary correlations that change across the search space, requiring local correlation models ρ(x) <a href="../results/extraction-result-2633.html#e2633.6" class="evidence-link">[e2633.6]</a> </li>
    <li>Optimal strategies when multiple low-fidelity models are available (ensemble of cheap models with different biases) need more theoretical analysis <a href="../results/extraction-result-2626.html#e2626.6" class="evidence-link">[e2626.6]</a> <a href="../results/extraction-result-2485.html#e2485.1" class="evidence-link">[e2485.1]</a> </li>
    <li>The role of low-fidelity model bias (systematic error) versus noise (random error) in determining optimal allocation is not fully characterized <a href="../results/extraction-result-2493.html#e2493.7" class="evidence-link">[e2493.7]</a> </li>
    <li>How to optimally allocate budget between learning fidelity correlations versus exploiting known correlations is not addressed <a href="../results/extraction-result-2464.html#e2464.3" class="evidence-link">[e2464.3]</a> </li>
    <li>The interaction between batch size and multi-fidelity efficiency (whether larger batches benefit more from multi-fidelity) needs investigation <a href="../results/extraction-result-2635.html#e2635.6" class="evidence-link">[e2635.6]</a> </li>
    <li>How multi-fidelity efficiency changes with problem dimensionality and whether there are dimensionality thresholds where benefits disappear <a href="../results/extraction-result-2493.html#e2493.0" class="evidence-link">[e2493.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Kennedy & O'Hagan (2000) Predicting the output from a complex computer code when fast approximations are available [Foundational multi-fidelity modeling framework with Gaussian process coregionalization]</li>
    <li>Forrester et al. (2007) Multi-fidelity optimization via surrogate modelling [Multi-fidelity optimization framework and correlation-based model management]</li>
    <li>Kandasamy et al. (2017) Multi-fidelity Bayesian Optimisation with Continuous Approximations [Continuous multi-fidelity BO with cost-aware acquisition functions]</li>
    <li>Peherstorfer et al. (2018) Survey of Multifidelity Methods in Uncertainty Propagation, Inference, and Optimization [Comprehensive survey covering MLMC, multi-fidelity optimization, and theoretical foundations]</li>
    <li>Poloczek et al. (2017) Multi-Information Source Optimization [Knowledge gradient for multi-fidelity with explicit cost-information tradeoffs]</li>
    <li>Huang et al. (2006) Sequential Kriging Optimization using Multiple-Fidelity Evaluations [Early work on sequential multi-fidelity optimization with correlation-based model selection]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Fidelity Correlation-Cost Efficiency Principle",
    "theory_description": "In multi-fidelity automated discovery systems, the efficiency gain from using low-fidelity models is determined by the interplay of three factors: (1) correlation ρ with high-fidelity (predictive accuracy), (2) relative cost ratio c (computational or experimental cost), and (3) allocation strategy effectiveness. Optimal resource allocation follows a correlation-cost threshold principle: low-fidelity models should be used extensively when ρ &gt; 0.7 and cost ratio &gt; 10x, with efficiency gains scaling approximately as ρ²/c for linear coregionalization models. However, gains diminish rapidly when ρ &lt; 0.5 or cost ratio &lt; 3x. The choice of multi-fidelity surrogate model matters: coregionalization methods (ICM with shared kernels, PCM with empirical correlations) provide better practical tradeoffs than full multivariate models (MGP) when inter-fidelity relationships are approximately linear and training data is limited, due to lower hyperparameter complexity. Allocation should be adaptive: early exploration uses low-fidelity extensively, while late-stage exploitation and final validation require high-fidelity. The principle extends to hierarchical multi-fidelity (&gt;2 levels) where intermediate fidelities can provide additional efficiency when they fill correlation-cost gaps.",
    "supporting_evidence": [
        {
            "text": "csKG with ICM/PCM surrogates shows 5-13x cost reduction depending on fidelity correlation and cost ratios in materials optimization",
            "uuids": [
                "e2633.6"
            ]
        },
        {
            "text": "MGP can perform well when hyperparameters are well-trained (intersection training) but suffers when the hyperparameter landscape is large/noisy, making PCM or ICM preferable in high-dimensional hyperparameter scenarios",
            "uuids": [
                "e2633.6"
            ]
        },
        {
            "text": "MFMES divides information gain by cost λ(l) and prefers low-cost fidelities for exploration, performing well when full fidelity library exists and in high-dimensional/multimodal problems",
            "uuids": [
                "e2493.7"
            ]
        },
        {
            "text": "MF-ENS leverages cheap L queries to inform expensive H queries via nonmyopic lookahead, showing linearly increasing advantage over single-fidelity ENS in cumulative discoveries",
            "uuids": [
                "e2498.3"
            ]
        },
        {
            "text": "MFEI uses correlation-based discount factor α₁(x,m) = corr(f^(m), f^(M)) to weight lower fidelities, reducing utility for lower fidelities when correlation is low",
            "uuids": [
                "e2464.1"
            ]
        },
        {
            "text": "MLMC/multilevel methods achieve potentially orders-of-magnitude efficiency gains when variance decay with level is faster than cost growth, with optimal sample allocation per level derived from variance and cost rates",
            "uuids": [
                "e2626.6"
            ]
        },
        {
            "text": "Multi-fidelity BO literature shows significant budget reductions vs single-fidelity across benchmark suites when cost ratios are large and correlations are high",
            "uuids": [
                "e2493.0"
            ]
        },
        {
            "text": "RAAL's MILP-based allocation explicitly optimizes multi-fidelity resource allocation under per-CPU capacity constraints, selecting (x,m) pairs to maximize aggregate MFEI subject to cost λ(m)",
            "uuids": [
                "e2464.3"
            ]
        },
        {
            "text": "Budgeted-Batch BO composes batches under explicit budget constraints, selecting points to maximize expected return while respecting total cost limits",
            "uuids": [
                "e2635.6"
            ]
        },
        {
            "text": "BMA+BO framework mentions multi-information-source optimization to choose the information source with best expected utility per cost",
            "uuids": [
                "e2485.1"
            ]
        },
        {
            "text": "EGO multifidelity extensions adaptively construct or correct surrogates using low- and high-fidelity data to reduce number of high-fidelity evaluations required",
            "uuids": [
                "e2626.3"
            ]
        },
        {
            "text": "MF-UCB uses separate β parameters for L and H queries to prioritize exploration on cheap fidelities (large β on L) and exploitation on expensive fidelities (small β on H)",
            "uuids": [
                "e2498.3"
            ]
        }
    ],
    "theory_statements": [
        "Multi-fidelity efficiency gain G scales approximately as G ≈ (ρ²/c) for linear coregionalization models, where ρ is correlation with high-fidelity and c is relative cost, achieving gains &gt;5x when ρ &gt; 0.7 and c &lt; 0.1",
        "Low-fidelity models should be allocated extensively for exploration when correlation ρ &gt; 0.7 and cost ratio &gt; 10x, with allocation shifting toward high-fidelity as search converges",
        "High-fidelity evaluations should be reserved for: (1) final validation, (2) exploitation near optima, (3) regions where low-fidelity is known to be inaccurate, (4) safety-critical assessments",
        "Coregionalization methods (ICM with shared kernel, PCM with empirical correlations) provide better practical tradeoffs than full multivariate models (MGP) when: (1) inter-fidelity relationships are approximately linear, (2) training data is limited, (3) hyperparameter optimization is expensive",
        "The optimal fidelity allocation strategy is adaptive and shifts toward high-fidelity as: (1) total budget increases, (2) search converges to promising regions, (3) low-fidelity correlation degrades locally, (4) exploitation phase begins",
        "Multi-fidelity benefits diminish when correlation ρ &lt; 0.5 or cost ratio &lt; 3x, making single-fidelity approaches competitive or superior",
        "Hierarchical multi-fidelity (&gt;2 fidelity levels) can achieve better efficiency than two-level when intermediate fidelities exist that fill correlation-cost gaps in the fidelity spectrum",
        "Batch multi-fidelity allocation should balance: (1) within-batch diversity, (2) fidelity-level diversity, (3) cost constraints per batch, with methods like MILP optimization providing near-optimal solutions",
        "Nonmyopic multi-fidelity policies (e.g., MF-ENS) that consider future cheap-query outcomes outperform myopic policies (e.g., MF-UCB) when budget-aware lookahead is computationally feasible"
    ],
    "new_predictions_likely": [
        "Adaptive fidelity selection that learns local correlation estimates ρ(x) across the search space would improve efficiency by 20-40% compared to fixed global correlation assumptions",
        "Hierarchical multi-fidelity with 3-4 fidelity levels would achieve 1.5-2x better efficiency than two-level when intermediate fidelities with cost ratios of 3-5x and correlations 0.6-0.8 are available",
        "Learned fidelity correlation models from previous campaigns (transfer learning) would reduce initial exploration overhead by 30-50% by providing better initial correlation estimates",
        "Multi-fidelity with adaptive correlation estimation and bias correction would be 2-3x more robust than fixed correlation assumptions when low-fidelity models have spatially-varying accuracy",
        "Combining multi-fidelity with multi-objective optimization (e.g., EHVI with multiple fidelities) would achieve similar Pareto front quality with 5-10x fewer high-fidelity evaluations"
    ],
    "new_predictions_unknown": [
        "Whether multi-fidelity approaches remain beneficial when low-fidelity models have systematic bias (not just noise) that varies non-smoothly across the design space",
        "Whether there exist problem classes where single high-fidelity is always superior to multi-fidelity approaches regardless of cost ratio (e.g., highly discontinuous objectives)",
        "Whether multi-fidelity can be effectively combined with safety constraints when low-fidelity is unreliable for safety assessment and high-fidelity safety tests are prohibitively expensive",
        "Whether the correlation-cost efficiency principle extends to discrete/categorical fidelity choices (e.g., different simulation codes) rather than continuous approximation hierarchies",
        "Whether multi-fidelity efficiency gains persist in very high-dimensional spaces (d &gt; 100) where curse of dimensionality affects both fidelity levels",
        "Whether ensemble methods combining multiple low-fidelity models can achieve better effective correlation than any single low-fidelity model, and if so, what the optimal ensemble size is",
        "Whether active learning of fidelity correlations (querying both fidelities strategically to learn ρ(x)) is worth the additional high-fidelity cost compared to passive correlation estimation"
    ],
    "negative_experiments": [
        "Finding problems where low-fidelity models with high correlation (ρ &gt; 0.8) and very low cost (c &lt; 0.01) provide no efficiency benefit would challenge the fundamental correlation-cost principle",
        "Demonstrating that full multivariate models (MGP) always outperform coregionalization methods (ICM/PCM) even with limited training data would contradict the practical tradeoff claim",
        "Showing that fidelity allocation is independent of correlation and cost (i.e., random fidelity selection performs as well as optimized allocation) would undermine the efficiency formula",
        "Finding that multi-fidelity approaches consistently lead to worse final solutions (not just slower convergence) compared to single high-fidelity would be fundamentally problematic",
        "Demonstrating that the efficiency gain does not scale with cost ratio (i.e., 100x cost ratio gives same gain as 10x) would challenge the c^(-1) scaling",
        "Finding that early-stage low-fidelity exploration consistently leads to worse final outcomes than immediate high-fidelity exploitation would contradict the adaptive allocation strategy"
    ],
    "unaccounted_for": [
        {
            "text": "The theory doesn't fully address how to handle non-stationary correlations that change across the search space, requiring local correlation models ρ(x)",
            "uuids": [
                "e2633.6"
            ]
        },
        {
            "text": "Optimal strategies when multiple low-fidelity models are available (ensemble of cheap models with different biases) need more theoretical analysis",
            "uuids": [
                "e2626.6",
                "e2485.1"
            ]
        },
        {
            "text": "The role of low-fidelity model bias (systematic error) versus noise (random error) in determining optimal allocation is not fully characterized",
            "uuids": [
                "e2493.7"
            ]
        },
        {
            "text": "How to optimally allocate budget between learning fidelity correlations versus exploiting known correlations is not addressed",
            "uuids": [
                "e2464.3"
            ]
        },
        {
            "text": "The interaction between batch size and multi-fidelity efficiency (whether larger batches benefit more from multi-fidelity) needs investigation",
            "uuids": [
                "e2635.6"
            ]
        },
        {
            "text": "How multi-fidelity efficiency changes with problem dimensionality and whether there are dimensionality thresholds where benefits disappear",
            "uuids": [
                "e2493.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show single-fidelity BO working well even when cheap approximations exist, suggesting problem-dependent factors beyond correlation and cost",
            "uuids": [
                "e2493.0"
            ]
        },
        {
            "text": "MGP can outperform simpler coregionalization methods when sufficient intersecting data is available for hyperparameter learning, contradicting the general superiority claim for ICM/PCM",
            "uuids": [
                "e2633.6"
            ]
        },
        {
            "text": "In some high-dimensional problems, the overhead of managing multiple fidelities can outweigh benefits, making single-fidelity competitive",
            "uuids": [
                "e2493.7"
            ]
        }
    ],
    "special_cases": [
        "When low-fidelity models have systematic bias that varies across the design space, bias correction or discrepancy modeling (e.g., Kennedy-O'Hagan framework) is necessary before applying the efficiency principle",
        "For safety-critical applications, high-fidelity validation is required regardless of low-fidelity correlation, and multi-fidelity can only be used for non-safety-critical exploration",
        "When cost ratios are small (&lt;3x), the overhead of managing multiple fidelities may exceed benefits, making single-fidelity approaches simpler and equally effective",
        "In online/adaptive settings where correlations change over time or across the design space, adaptive fidelity selection with local correlation estimation ρ(x) is necessary",
        "For very high-dimensional problems (d &gt; 50-100), the curse of dimensionality may affect low-fidelity models more severely, reducing effective correlation and limiting benefits",
        "When multiple low-fidelity sources are available with complementary strengths, ensemble methods or multi-source fusion may be superior to single low-fidelity selection",
        "In constrained optimization, low-fidelity models may be unreliable for constraint satisfaction, requiring high-fidelity validation of all feasibility assessments",
        "For discrete/combinatorial spaces, the notion of fidelity correlation may need to be redefined (e.g., ranking correlation rather than value correlation)"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Kennedy & O'Hagan (2000) Predicting the output from a complex computer code when fast approximations are available [Foundational multi-fidelity modeling framework with Gaussian process coregionalization]",
            "Forrester et al. (2007) Multi-fidelity optimization via surrogate modelling [Multi-fidelity optimization framework and correlation-based model management]",
            "Kandasamy et al. (2017) Multi-fidelity Bayesian Optimisation with Continuous Approximations [Continuous multi-fidelity BO with cost-aware acquisition functions]",
            "Peherstorfer et al. (2018) Survey of Multifidelity Methods in Uncertainty Propagation, Inference, and Optimization [Comprehensive survey covering MLMC, multi-fidelity optimization, and theoretical foundations]",
            "Poloczek et al. (2017) Multi-Information Source Optimization [Knowledge gradient for multi-fidelity with explicit cost-information tradeoffs]",
            "Huang et al. (2006) Sequential Kriging Optimization using Multiple-Fidelity Evaluations [Early work on sequential multi-fidelity optimization with correlation-based model selection]"
        ]
    },
    "reflected_from_theory_index": 5,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Drift Through Translation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-92</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-92</p>
                <p><strong>Name:</strong> Semantic Drift Through Translation Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about faithfulness gaps between natural language descriptions and code implementations in automated experimentation, based on the following results.</p>
                <p><strong>Description:</strong> When natural language specifications are translated into formal implementations (code), semantic drift occurs through multiple mechanisms: (1) ambiguity resolution - different implementers/models resolve ambiguities differently; (2) implicit assumption materialization - unstated context and prerequisites are filled in inconsistently; (3) information loss - omitted details cannot be reliably inferred; (4) vocabulary/grammar limitations - target representations cannot express all source semantics; (5) context truncation - limited context windows cause missing dependencies. This drift accumulates through translation chains (NL → intermediate representations → code → execution) and is amplified by: incomplete specifications, missing examples, ambiguous quantifiers/negation, and inadequate grounding to execution context. The theory predicts that drift is detectable through paraphrase sensitivity, back-translation inconsistency, and multi-implementation variance, and that certain specification types (involving negation, universal quantifiers, implicit context, or cross-file dependencies) are particularly susceptible.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Semantic drift accumulates multiplicatively through translation chains: if each step has drift probability d, total drift after n steps approaches 1 - (1-d)^n.</li>
                <li>Ambiguity resolution varies across implementers/models, with different valid interpretations producing functionally distinct implementations even when all satisfy the literal specification.</li>
                <li>Information loss from omitted details is not reliably recoverable: models cannot consistently infer missing context, with omission causing 14-32% semantic accuracy drops.</li>
                <li>Vocabulary and grammar limitations create hard bounds on expressiveness: 18% of real assertions cannot be expressed in restricted grammars, and 54% of failures stem from out-of-vocabulary values.</li>
                <li>Context truncation causes systematic failures: limited windows (e.g., 700 tokens) cause models to miss imports and definitions, with only 51.3% of call sites resolvable.</li>
                <li>Paraphrase sensitivity reveals semantic drift: semantic-preserving paraphrases (>90% human-judged preservation) cause 60-75% robust drop rates in code generation.</li>
                <li>Specification types have differential susceptibility: specifications with negation, universal quantifiers, implicit context, or cross-file dependencies show 2-3x higher failure rates.</li>
                <li>Compositional complexity causes exponential degradation: each additional operation in a chain reduces pass rates by factor of 2-3.</li>
                <li>Grounding failures are systematic: missing schema descriptions reduce pass@30 by 11.6pp, and missing variable states cause frequent NameError/KeyError.</li>
                <li>Token-level dependencies are brittle: models rely on literal token overlap, failing when expected tokens are omitted or paraphrased.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Ambiguous natural language specifications permit multiple interpretations, with LLMs producing different postconditions that may all be plausible but capture different semantics. <a href="../results/extraction-result-498.html#e498.0" class="evidence-link">[e498.0]</a> </li>
    <li>Generated code often encodes unintended constraints not requested by prompts, such as uniqueness constraints or incorrect cardinality restrictions. <a href="../results/extraction-result-456.html#e456.3" class="evidence-link">[e456.3]</a> <a href="../results/extraction-result-716.html#e716.5" class="evidence-link">[e716.5]</a> </li>
    <li>Semantic-preserving paraphrases of docstrings cause large changes in generated code, with robust drop rates of 60-75% on some perturbation types despite >90% semantic preservation by human judgment. <a href="../results/extraction-result-707.html#e707.0" class="evidence-link">[e707.0]</a> </li>
    <li>Word substitution in NL intents produces semantic drift even with POS constraints, with unconstrained substitution preserving semantics in only 92.90% vs 96.20% with constraints. <a href="../results/extraction-result-497.html#e497.2" class="evidence-link">[e497.2]</a> </li>
    <li>Omitting action/structure/name tokens from NL causes large drops in correctness (14-32% semantic accuracy drops), as models cannot infer missing information from context. <a href="../results/extraction-result-497.html#e497.1" class="evidence-link">[e497.1]</a> </li>
    <li>NL-to-ASP translation produces syntactically correct but semantically wrong programs, with models achieving high syntactic accuracy but substantially lower semantic accuracy. <a href="../results/extraction-result-456.html#e456.1" class="evidence-link">[e456.1]</a> </li>
    <li>Question titles used as intents misalign with answer code because code blocks contain context, imports, and examples rather than pure implementations. <a href="../results/extraction-result-461.html#e461.0" class="evidence-link">[e461.0]</a> <a href="../results/extraction-result-461.html#e461.4" class="evidence-link">[e461.4]</a> </li>
    <li>Free-form definitional sentences cannot be cleanly converted to structured formats required by some editing methods, causing extremely high perplexities (>100) when forced. <a href="../results/extraction-result-683.html#e683.0" class="evidence-link">[e683.0]</a> </li>
    <li>Prompting style changes alter model behavior substantially, with step-by-step prompting improving pass@30 by ~4.8pp on new tasks but slightly degrading on memorized tasks. <a href="../results/extraction-result-729.html#e729.4" class="evidence-link">[e729.4]</a> </li>
    <li>Independent translation of components (premise/hypothesis separately) alters superficial patterns like lexical overlap that models exploit. <a href="../results/extraction-result-681.html#e681.1" class="evidence-link">[e681.1]</a> </li>
    <li>Prompt sensitivity causes different postcondition qualities, with simple prompts producing higher correctness (p=0.008, Cohen's d=1.73) but potentially lower completeness. <a href="../results/extraction-result-498.html#e498.5" class="evidence-link">[e498.5]</a> </li>
    <li>Limited context windows cause models to reference undefined variables or miss required imports, with 51.3% of call sites resolvable and only 64.6% having available docstrings. <a href="../results/extraction-result-481.html#e481.3" class="evidence-link">[e481.3]</a> <a href="../results/extraction-result-481.html#e481.5" class="evidence-link">[e481.5]</a> </li>
    <li>Missing schema/variable state grounding causes NameError/KeyError, with schema description removal reducing pass@30 by 11.6pp. <a href="../results/extraction-result-729.html#e729.1" class="evidence-link">[e729.1]</a> <a href="../results/extraction-result-678.html#e678.1" class="evidence-link">[e678.1]</a> </li>
    <li>LLM-generated I/O specifications can be noisy or inaccurate, introducing new mismatches between NL descriptions and actual code behavior. <a href="../results/extraction-result-678.html#e678.3" class="evidence-link">[e678.3]</a> </li>
    <li>Undocumented preprocessing choices (tokenization, scaling) materially change results, with non-scaling reducing results by ~33%. <a href="../results/extraction-result-454.html#e454.1" class="evidence-link">[e454.1]</a> </li>
    <li>Unclear notation, missing algorithm steps, and omitted gradient derivations prevent reliable independent implementation. <a href="../results/extraction-result-711.html#e711.2" class="evidence-link">[e711.2]</a> <a href="../results/extraction-result-487.html#e487.2" class="evidence-link">[e487.2]</a> <a href="../results/extraction-result-476.html#e476.0" class="evidence-link">[e476.0]</a> </li>
    <li>Multi-step solutions require composition across snippets, but per-snippet independent modeling treats each separately, causing false positives. <a href="../results/extraction-result-733.html#e733.5" class="evidence-link">[e733.5]</a> </li>
    <li>Grammar and vocabulary coverage limitations prevent expressing many real assertions, with 18% of assertions not fitting grammar and 54% of failures due to missing vocabulary. <a href="../results/extraction-result-759.html#e759.3" class="evidence-link">[e759.3]</a> </li>
    <li>Unparsable code snippets (74% Python, 88% SQL) and parsable-but-not-runnable code (76% parsable but only 26% runnable for Python) limit automatic validation. <a href="../results/extraction-result-698.html#e698.2" class="evidence-link">[e698.2]</a> <a href="../results/extraction-result-690.html#e690.0" class="evidence-link">[e690.0]</a> </li>
    <li>Outdated or incorrect documentation relative to code implementation introduces label noise in training. <a href="../results/extraction-result-719.html#e719.1" class="evidence-link">[e719.1]</a> </li>
    <li>Heuristic question-code pairing produces many incorrect pairs, with ~70% of Python cases requiring sub-block extraction rather than full blocks. <a href="../results/extraction-result-698.html#e698.0" class="evidence-link">[e698.0]</a> </li>
    <li>Models rely on literal token overlap between NL and code, failing when tokens are omitted or altered. <a href="../results/extraction-result-497.html#e497.3" class="evidence-link">[e497.3]</a> </li>
    <li>Non-solution code snippets (input-output demos, reminders, partial steps) are misclassified as solutions, with >50% of model mistakes being false positives. <a href="../results/extraction-result-733.html#e733.2" class="evidence-link">[e733.2]</a> </li>
    <li>Bag-of-words models fail to capture word sequence and deep semantics, resulting in poor API sequence retrieval. <a href="../results/extraction-result-684.html#e684.1" class="evidence-link">[e684.1]</a> </li>
    <li>Incomplete annotation coverage and annotation quality issues introduce noise, with only 51.3% of call sites resolved and 64.6% having docstrings. <a href="../results/extraction-result-461.html#e461.3" class="evidence-link">[e461.3]</a> <a href="../results/extraction-result-719.html#e719.8" class="evidence-link">[e719.8]</a> </li>
    <li>Title-only representation misses information in question bodies needed to determine solution-ness. <a href="../results/extraction-result-698.html#e698.4" class="evidence-link">[e698.4]</a> </li>
    <li>Evaluation metrics (ROUGE, BLEU) measure content overlap but not faithfulness, with low correlation to human faithfulness judgments. <a href="../results/extraction-result-708.html#e708.1" class="evidence-link">[e708.1]</a> <a href="../results/extraction-result-689.html#e689.0" class="evidence-link">[e689.0]</a> </li>
    <li>Compositional behavior degrades exponentially with specification complexity, with pass rates dropping by factor of 2-3 per additional chained operation. <a href="../results/extraction-result-752.html#e752.3" class="evidence-link">[e752.3]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Providing multiple paraphrases of the same specification will reveal semantic drift, with 30-50% of paraphrases producing functionally different implementations when tested with comprehensive test suites.</li>
                <li>Specifications with explicit negation or universal quantifiers will have 2-3x higher implementation variance (measured by behavioral differences across implementations) than specifications with only positive existential statements.</li>
                <li>Back-translation (code → NL → code) will reveal semantic drift, with only 40-60% of back-translations producing behaviorally equivalent code on comprehensive test suites.</li>
                <li>Increasing context window size will reduce semantic drift in a logarithmic fashion: doubling context reduces drift by ~20-30% until saturation around 4-8K tokens.</li>
                <li>Providing worked examples alongside specifications will reduce semantic drift by 40-60% compared to specification-only approaches.</li>
                <li>Multi-implementation consensus (generating 5-10 implementations and taking majority behavior) will reduce semantic drift by 50-70% compared to single implementations.</li>
                <li>Formal specification languages with executable semantics will reduce semantic drift by 60-80% compared to natural language, but require 3-5x more specification effort.</li>
                <li>Specifications that explicitly enumerate edge cases will have 40-60% less drift than specifications that rely on implicit understanding of edge case handling.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether semantic drift follows predictable patterns based on linguistic features (e.g., parse tree depth, quantifier nesting, reference distance) that can be modeled to predict drift magnitude with >80% accuracy before implementation.</li>
                <li>Whether certain natural language constructions (e.g., specific syntactic patterns, discourse structures) are fundamentally more resistant to semantic drift across all domains and implementation contexts.</li>
                <li>Whether human experts show significantly less semantic drift than automated systems when controlling for specification ambiguity, or if the drift is inherent to the translation process regardless of translator.</li>
                <li>Whether semantic drift can be measured and quantified in real-time during the translation process using intermediate representations to provide corrective feedback before final implementation.</li>
                <li>Whether adversarial training on paraphrases and edge cases can reduce semantic drift to <10% while maintaining high task performance.</li>
                <li>Whether semantic drift compounds differently for different types of translation chains (e.g., NL→pseudocode→code vs NL→code directly), and whether intermediate representations help or hurt.</li>
                <li>Whether cross-lingual translation (e.g., English→Chinese→code) introduces additional semantic drift beyond monolingual translation, and whether this drift is additive or multiplicative.</li>
                <li>Whether semantic drift can be bounded by formal verification techniques applied at intermediate translation steps, and what the computational cost would be.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that multiple independent implementations of the same ambiguous natural language specification produce identical behavior (within measurement error) would challenge the theory's core claim about ambiguity resolution variance.</li>
                <li>Demonstrating that semantic drift does not compound across translation steps (i.e., drift after n steps equals drift after 1 step) would undermine the multiplicative accumulation model.</li>
                <li>Showing that paraphrase-invariant specifications (those that produce identical implementations across paraphrases) are common (>50% of specifications) would challenge the theory's predictions about paraphrase sensitivity.</li>
                <li>Finding that information loss from omissions is reliably recoverable (>90% recovery rate) through context or domain knowledge would contradict the theory's claims about information loss.</li>
                <li>Demonstrating that vocabulary/grammar limitations can be overcome through learned mappings or approximations without semantic drift would challenge the hard bounds claim.</li>
                <li>Showing that context truncation does not cause systematic failures when models are trained with appropriate attention mechanisms would undermine the context window predictions.</li>
                <li>Finding that compositional complexity does not cause exponential degradation but rather linear or constant degradation would challenge the compositionality claims.</li>
                <li>Demonstrating that formal specifications have similar drift rates to natural language when controlling for specification effort would challenge assumptions about formalization benefits.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some implementation differences arise from bugs or errors rather than semantic drift from specifications. <a href="../results/extraction-result-445.html#e445.3" class="evidence-link">[e445.3]</a> <a href="../results/extraction-result-677.html#e677.2" class="evidence-link">[e677.2]</a> </li>
    <li>Environmental and hardware differences (GPU types, framework versions, compiler flags) cause behavioral differences independent of semantic drift. <a href="../results/extraction-result-485.html#e485.6" class="evidence-link">[e485.6]</a> <a href="../results/extraction-result-491.html#e491.0" class="evidence-link">[e491.0]</a> <a href="../results/extraction-result-491.html#e491.1" class="evidence-link">[e491.1]</a> <a href="../results/extraction-result-688.html#e688.4" class="evidence-link">[e688.4]</a> </li>
    <li>Random seed and initialization differences cause run-to-run variance that is distinct from semantic drift. <a href="../results/extraction-result-475.html#e475.1" class="evidence-link">[e475.1]</a> <a href="../results/extraction-result-706.html#e706.2" class="evidence-link">[e706.2]</a> <a href="../results/extraction-result-694.html#e694.4" class="evidence-link">[e694.4]</a> </li>
    <li>Data quality issues (annotation errors, label noise, outdated documentation) cause apparent semantic drift that is actually due to training data problems. <a href="../results/extraction-result-719.html#e719.1" class="evidence-link">[e719.1]</a> <a href="../results/extraction-result-461.html#e461.3" class="evidence-link">[e461.3]</a> <a href="../results/extraction-result-705.html#e705.1" class="evidence-link">[e705.1]</a> </li>
    <li>Hyperparameter choices and training procedures cause implementation variance independent of specification interpretation. <a href="../results/extraction-result-728.html#e728.0" class="evidence-link">[e728.0]</a> <a href="../results/extraction-result-706.html#e706.0" class="evidence-link">[e706.0]</a> <a href="../results/extraction-result-454.html#e454.4" class="evidence-link">[e454.4]</a> </li>
    <li>Framework-level non-determinism (cuDNN autotune, multithreading) causes differences unrelated to semantic drift. <a href="../results/extraction-result-491.html#e491.1" class="evidence-link">[e491.1]</a> <a href="../results/extraction-result-688.html#e688.4" class="evidence-link">[e688.4]</a> </li>
    <li>Evaluation metric misalignment can make semantic drift appear larger or smaller than it actually is. <a href="../results/extraction-result-708.html#e708.1" class="evidence-link">[e708.1]</a> <a href="../results/extraction-result-689.html#e689.0" class="evidence-link">[e689.0]</a> <a href="../results/extraction-result-752.html#e752.5" class="evidence-link">[e752.5]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Hutchins (1995) Cognition in the Wild [Discusses how meaning transforms across representational systems and distributed cognition]</li>
    <li>Shannon (1948) A Mathematical Theory of Communication [Information theory provides foundation for understanding information loss in translation]</li>
    <li>Grice (1975) Logic and Conversation [Conversational implicature explains how implicit meaning is conveyed and can be misinterpreted]</li>
    <li>Levesque (2014) On Our Best Behaviour [Discusses the Winograd Schema Challenge and ambiguity in natural language understanding]</li>
    <li>Allamanis et al. (2018) A Survey of Machine Learning for Big Code and Naturalness [Reviews NL-code translation but does not formalize semantic drift theory]</li>
    <li>Yin & Neubig (2018) TRANX: A Transition-based Neural Abstract Syntax Parser [Addresses structured translation but not semantic drift measurement or theory]</li>
    <li>Hindle et al. (2012) On the naturalness of software [Discusses natural language properties of code but not semantic drift mechanisms]</li>
    <li>Piantadosi et al. (2012) The communicative function of ambiguity in language [Discusses functional role of ambiguity in communication, relevant to understanding why drift occurs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Drift Through Translation Theory",
    "theory_description": "When natural language specifications are translated into formal implementations (code), semantic drift occurs through multiple mechanisms: (1) ambiguity resolution - different implementers/models resolve ambiguities differently; (2) implicit assumption materialization - unstated context and prerequisites are filled in inconsistently; (3) information loss - omitted details cannot be reliably inferred; (4) vocabulary/grammar limitations - target representations cannot express all source semantics; (5) context truncation - limited context windows cause missing dependencies. This drift accumulates through translation chains (NL → intermediate representations → code → execution) and is amplified by: incomplete specifications, missing examples, ambiguous quantifiers/negation, and inadequate grounding to execution context. The theory predicts that drift is detectable through paraphrase sensitivity, back-translation inconsistency, and multi-implementation variance, and that certain specification types (involving negation, universal quantifiers, implicit context, or cross-file dependencies) are particularly susceptible.",
    "supporting_evidence": [
        {
            "text": "Ambiguous natural language specifications permit multiple interpretations, with LLMs producing different postconditions that may all be plausible but capture different semantics.",
            "uuids": [
                "e498.0"
            ]
        },
        {
            "text": "Generated code often encodes unintended constraints not requested by prompts, such as uniqueness constraints or incorrect cardinality restrictions.",
            "uuids": [
                "e456.3",
                "e716.5"
            ]
        },
        {
            "text": "Semantic-preserving paraphrases of docstrings cause large changes in generated code, with robust drop rates of 60-75% on some perturbation types despite &gt;90% semantic preservation by human judgment.",
            "uuids": [
                "e707.0"
            ]
        },
        {
            "text": "Word substitution in NL intents produces semantic drift even with POS constraints, with unconstrained substitution preserving semantics in only 92.90% vs 96.20% with constraints.",
            "uuids": [
                "e497.2"
            ]
        },
        {
            "text": "Omitting action/structure/name tokens from NL causes large drops in correctness (14-32% semantic accuracy drops), as models cannot infer missing information from context.",
            "uuids": [
                "e497.1"
            ]
        },
        {
            "text": "NL-to-ASP translation produces syntactically correct but semantically wrong programs, with models achieving high syntactic accuracy but substantially lower semantic accuracy.",
            "uuids": [
                "e456.1"
            ]
        },
        {
            "text": "Question titles used as intents misalign with answer code because code blocks contain context, imports, and examples rather than pure implementations.",
            "uuids": [
                "e461.0",
                "e461.4"
            ]
        },
        {
            "text": "Free-form definitional sentences cannot be cleanly converted to structured formats required by some editing methods, causing extremely high perplexities (&gt;100) when forced.",
            "uuids": [
                "e683.0"
            ]
        },
        {
            "text": "Prompting style changes alter model behavior substantially, with step-by-step prompting improving pass@30 by ~4.8pp on new tasks but slightly degrading on memorized tasks.",
            "uuids": [
                "e729.4"
            ]
        },
        {
            "text": "Independent translation of components (premise/hypothesis separately) alters superficial patterns like lexical overlap that models exploit.",
            "uuids": [
                "e681.1"
            ]
        },
        {
            "text": "Prompt sensitivity causes different postcondition qualities, with simple prompts producing higher correctness (p=0.008, Cohen's d=1.73) but potentially lower completeness.",
            "uuids": [
                "e498.5"
            ]
        },
        {
            "text": "Limited context windows cause models to reference undefined variables or miss required imports, with 51.3% of call sites resolvable and only 64.6% having available docstrings.",
            "uuids": [
                "e481.3",
                "e481.5"
            ]
        },
        {
            "text": "Missing schema/variable state grounding causes NameError/KeyError, with schema description removal reducing pass@30 by 11.6pp.",
            "uuids": [
                "e729.1",
                "e678.1"
            ]
        },
        {
            "text": "LLM-generated I/O specifications can be noisy or inaccurate, introducing new mismatches between NL descriptions and actual code behavior.",
            "uuids": [
                "e678.3"
            ]
        },
        {
            "text": "Undocumented preprocessing choices (tokenization, scaling) materially change results, with non-scaling reducing results by ~33%.",
            "uuids": [
                "e454.1"
            ]
        },
        {
            "text": "Unclear notation, missing algorithm steps, and omitted gradient derivations prevent reliable independent implementation.",
            "uuids": [
                "e711.2",
                "e487.2",
                "e476.0"
            ]
        },
        {
            "text": "Multi-step solutions require composition across snippets, but per-snippet independent modeling treats each separately, causing false positives.",
            "uuids": [
                "e733.5"
            ]
        },
        {
            "text": "Grammar and vocabulary coverage limitations prevent expressing many real assertions, with 18% of assertions not fitting grammar and 54% of failures due to missing vocabulary.",
            "uuids": [
                "e759.3"
            ]
        },
        {
            "text": "Unparsable code snippets (74% Python, 88% SQL) and parsable-but-not-runnable code (76% parsable but only 26% runnable for Python) limit automatic validation.",
            "uuids": [
                "e698.2",
                "e690.0"
            ]
        },
        {
            "text": "Outdated or incorrect documentation relative to code implementation introduces label noise in training.",
            "uuids": [
                "e719.1"
            ]
        },
        {
            "text": "Heuristic question-code pairing produces many incorrect pairs, with ~70% of Python cases requiring sub-block extraction rather than full blocks.",
            "uuids": [
                "e698.0"
            ]
        },
        {
            "text": "Models rely on literal token overlap between NL and code, failing when tokens are omitted or altered.",
            "uuids": [
                "e497.3"
            ]
        },
        {
            "text": "Non-solution code snippets (input-output demos, reminders, partial steps) are misclassified as solutions, with &gt;50% of model mistakes being false positives.",
            "uuids": [
                "e733.2"
            ]
        },
        {
            "text": "Bag-of-words models fail to capture word sequence and deep semantics, resulting in poor API sequence retrieval.",
            "uuids": [
                "e684.1"
            ]
        },
        {
            "text": "Incomplete annotation coverage and annotation quality issues introduce noise, with only 51.3% of call sites resolved and 64.6% having docstrings.",
            "uuids": [
                "e461.3",
                "e719.8"
            ]
        },
        {
            "text": "Title-only representation misses information in question bodies needed to determine solution-ness.",
            "uuids": [
                "e698.4"
            ]
        },
        {
            "text": "Evaluation metrics (ROUGE, BLEU) measure content overlap but not faithfulness, with low correlation to human faithfulness judgments.",
            "uuids": [
                "e708.1",
                "e689.0"
            ]
        },
        {
            "text": "Compositional behavior degrades exponentially with specification complexity, with pass rates dropping by factor of 2-3 per additional chained operation.",
            "uuids": [
                "e752.3"
            ]
        }
    ],
    "theory_statements": [
        "Semantic drift accumulates multiplicatively through translation chains: if each step has drift probability d, total drift after n steps approaches 1 - (1-d)^n.",
        "Ambiguity resolution varies across implementers/models, with different valid interpretations producing functionally distinct implementations even when all satisfy the literal specification.",
        "Information loss from omitted details is not reliably recoverable: models cannot consistently infer missing context, with omission causing 14-32% semantic accuracy drops.",
        "Vocabulary and grammar limitations create hard bounds on expressiveness: 18% of real assertions cannot be expressed in restricted grammars, and 54% of failures stem from out-of-vocabulary values.",
        "Context truncation causes systematic failures: limited windows (e.g., 700 tokens) cause models to miss imports and definitions, with only 51.3% of call sites resolvable.",
        "Paraphrase sensitivity reveals semantic drift: semantic-preserving paraphrases (&gt;90% human-judged preservation) cause 60-75% robust drop rates in code generation.",
        "Specification types have differential susceptibility: specifications with negation, universal quantifiers, implicit context, or cross-file dependencies show 2-3x higher failure rates.",
        "Compositional complexity causes exponential degradation: each additional operation in a chain reduces pass rates by factor of 2-3.",
        "Grounding failures are systematic: missing schema descriptions reduce pass@30 by 11.6pp, and missing variable states cause frequent NameError/KeyError.",
        "Token-level dependencies are brittle: models rely on literal token overlap, failing when expected tokens are omitted or paraphrased."
    ],
    "new_predictions_likely": [
        "Providing multiple paraphrases of the same specification will reveal semantic drift, with 30-50% of paraphrases producing functionally different implementations when tested with comprehensive test suites.",
        "Specifications with explicit negation or universal quantifiers will have 2-3x higher implementation variance (measured by behavioral differences across implementations) than specifications with only positive existential statements.",
        "Back-translation (code → NL → code) will reveal semantic drift, with only 40-60% of back-translations producing behaviorally equivalent code on comprehensive test suites.",
        "Increasing context window size will reduce semantic drift in a logarithmic fashion: doubling context reduces drift by ~20-30% until saturation around 4-8K tokens.",
        "Providing worked examples alongside specifications will reduce semantic drift by 40-60% compared to specification-only approaches.",
        "Multi-implementation consensus (generating 5-10 implementations and taking majority behavior) will reduce semantic drift by 50-70% compared to single implementations.",
        "Formal specification languages with executable semantics will reduce semantic drift by 60-80% compared to natural language, but require 3-5x more specification effort.",
        "Specifications that explicitly enumerate edge cases will have 40-60% less drift than specifications that rely on implicit understanding of edge case handling."
    ],
    "new_predictions_unknown": [
        "Whether semantic drift follows predictable patterns based on linguistic features (e.g., parse tree depth, quantifier nesting, reference distance) that can be modeled to predict drift magnitude with &gt;80% accuracy before implementation.",
        "Whether certain natural language constructions (e.g., specific syntactic patterns, discourse structures) are fundamentally more resistant to semantic drift across all domains and implementation contexts.",
        "Whether human experts show significantly less semantic drift than automated systems when controlling for specification ambiguity, or if the drift is inherent to the translation process regardless of translator.",
        "Whether semantic drift can be measured and quantified in real-time during the translation process using intermediate representations to provide corrective feedback before final implementation.",
        "Whether adversarial training on paraphrases and edge cases can reduce semantic drift to &lt;10% while maintaining high task performance.",
        "Whether semantic drift compounds differently for different types of translation chains (e.g., NL→pseudocode→code vs NL→code directly), and whether intermediate representations help or hurt.",
        "Whether cross-lingual translation (e.g., English→Chinese→code) introduces additional semantic drift beyond monolingual translation, and whether this drift is additive or multiplicative.",
        "Whether semantic drift can be bounded by formal verification techniques applied at intermediate translation steps, and what the computational cost would be."
    ],
    "negative_experiments": [
        "Finding that multiple independent implementations of the same ambiguous natural language specification produce identical behavior (within measurement error) would challenge the theory's core claim about ambiguity resolution variance.",
        "Demonstrating that semantic drift does not compound across translation steps (i.e., drift after n steps equals drift after 1 step) would undermine the multiplicative accumulation model.",
        "Showing that paraphrase-invariant specifications (those that produce identical implementations across paraphrases) are common (&gt;50% of specifications) would challenge the theory's predictions about paraphrase sensitivity.",
        "Finding that information loss from omissions is reliably recoverable (&gt;90% recovery rate) through context or domain knowledge would contradict the theory's claims about information loss.",
        "Demonstrating that vocabulary/grammar limitations can be overcome through learned mappings or approximations without semantic drift would challenge the hard bounds claim.",
        "Showing that context truncation does not cause systematic failures when models are trained with appropriate attention mechanisms would undermine the context window predictions.",
        "Finding that compositional complexity does not cause exponential degradation but rather linear or constant degradation would challenge the compositionality claims.",
        "Demonstrating that formal specifications have similar drift rates to natural language when controlling for specification effort would challenge assumptions about formalization benefits."
    ],
    "unaccounted_for": [
        {
            "text": "Some implementation differences arise from bugs or errors rather than semantic drift from specifications.",
            "uuids": [
                "e445.3",
                "e677.2"
            ]
        },
        {
            "text": "Environmental and hardware differences (GPU types, framework versions, compiler flags) cause behavioral differences independent of semantic drift.",
            "uuids": [
                "e485.6",
                "e491.0",
                "e491.1",
                "e688.4"
            ]
        },
        {
            "text": "Random seed and initialization differences cause run-to-run variance that is distinct from semantic drift.",
            "uuids": [
                "e475.1",
                "e706.2",
                "e694.4"
            ]
        },
        {
            "text": "Data quality issues (annotation errors, label noise, outdated documentation) cause apparent semantic drift that is actually due to training data problems.",
            "uuids": [
                "e719.1",
                "e461.3",
                "e705.1"
            ]
        },
        {
            "text": "Hyperparameter choices and training procedures cause implementation variance independent of specification interpretation.",
            "uuids": [
                "e728.0",
                "e706.0",
                "e454.4"
            ]
        },
        {
            "text": "Framework-level non-determinism (cuDNN autotune, multithreading) causes differences unrelated to semantic drift.",
            "uuids": [
                "e491.1",
                "e688.4"
            ]
        },
        {
            "text": "Evaluation metric misalignment can make semantic drift appear larger or smaller than it actually is.",
            "uuids": [
                "e708.1",
                "e689.0",
                "e752.5"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some highly ambiguous specifications are implemented consistently when domain conventions are strong, suggesting domain knowledge can reduce drift.",
            "uuids": [
                "e682.0"
            ]
        },
        {
            "text": "Certain types of specifications (e.g., well-defined mathematical operations with clear semantics) show very low semantic drift even across different implementers.",
            "uuids": [
                "e722.2"
            ]
        },
        {
            "text": "Pretraining on large code corpora improves faithfulness and reduces some types of semantic drift, suggesting learned priors can compensate for specification ambiguity.",
            "uuids": [
                "e686.5"
            ]
        },
        {
            "text": "Fine-tuning on domain-specific data (e.g., LLASP on ASP templates) can achieve high syntactic and semantic accuracy, suggesting drift can be minimized with appropriate training.",
            "uuids": [
                "e456.2"
            ]
        }
    ],
    "special_cases": [
        "Domain-specific languages with strong conventions and formal semantics show reduced semantic drift (e.g., SQL, regular expressions).",
        "Specifications with worked examples show 40-60% less drift than specifications without examples, as examples ground ambiguous terms.",
        "Interactive specification refinement with feedback loops can reduce drift by 50-70% through iterative clarification.",
        "Specifications in domains with formal verification traditions (e.g., cryptography, safety-critical systems) show lower drift due to explicit formalization practices.",
        "Mathematical specifications with clear operational semantics show minimal drift when translated to code.",
        "Specifications that explicitly enumerate edge cases and boundary conditions show 40-60% less drift than those relying on implicit understanding.",
        "Multi-implementation consensus approaches (generating multiple implementations and voting) can reduce drift by 50-70%.",
        "Specifications with comprehensive test suites enable detection and correction of drift, reducing final drift by 60-80%.",
        "Formal specification languages with executable semantics (e.g., TLA+, Alloy) show 60-80% less drift than natural language but require significantly more effort.",
        "Specifications grounded in execution context (with I/O examples, schema descriptions, variable states) show 40-60% less drift than abstract specifications."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Hutchins (1995) Cognition in the Wild [Discusses how meaning transforms across representational systems and distributed cognition]",
            "Shannon (1948) A Mathematical Theory of Communication [Information theory provides foundation for understanding information loss in translation]",
            "Grice (1975) Logic and Conversation [Conversational implicature explains how implicit meaning is conveyed and can be misinterpreted]",
            "Levesque (2014) On Our Best Behaviour [Discusses the Winograd Schema Challenge and ambiguity in natural language understanding]",
            "Allamanis et al. (2018) A Survey of Machine Learning for Big Code and Naturalness [Reviews NL-code translation but does not formalize semantic drift theory]",
            "Yin & Neubig (2018) TRANX: A Transition-based Neural Abstract Syntax Parser [Addresses structured translation but not semantic drift measurement or theory]",
            "Hindle et al. (2012) On the naturalness of software [Discusses natural language properties of code but not semantic drift mechanisms]",
            "Piantadosi et al. (2012) The communicative function of ambiguity in language [Discusses functional role of ambiguity in communication, relevant to understanding why drift occurs]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
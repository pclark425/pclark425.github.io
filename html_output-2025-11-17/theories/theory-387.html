<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Traditional Proxy Systematic Bias Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-387</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-387</p>
                <p><strong>Name:</strong> Traditional Proxy Systematic Bias Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about the evaluation and validation of incremental versus transformational scientific discoveries in automated systems.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that traditional evaluation proxies (citation counts, journal prestige, standard peer review, author reputation) systematically undervalue transformational scientific discoveries due to training distribution bias and multiple simultaneous proxy failures. Traditional proxies are calibrated on historical data that predominantly reflects incremental science, creating systematic bias against work that violates implicit assumptions embedded in these metrics. The undervaluation arises because transformational discoveries simultaneously violate multiple proxy assumptions: they lack immediate citation recognition, may be published in non-traditional venues, often come from unexpected sources, and use novel methodologies. The magnitude and pattern of this gap is highly context-dependent, modulated by presentation quality (20-60 percentage point effects), task framing, field receptivity, and author reputation. Traditional proxies show gaps ranging from moderate (20-40% undervaluation) for work violating some assumptions to severe (60%+ undervaluation or rejection) for work violating multiple assumptions simultaneously. However, the gap is not inherent to automated evaluation—well-designed alternative proxies can substantially reduce or eliminate these gaps, indicating the problem is one of proxy architecture rather than fundamental limitations of automation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-320.html">[theory-320]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Removed the exponential formula G(T) ≈ k * e^(βT) as it is not validated by evidence; replaced with context-dependent gap ranges (20-40% for moderate violations, 60%+ for severe violations).</li>
                <li>Modified the 70-90% undervaluation claim to reflect observed heterogeneity: gaps range from 20-40% to 60%+ depending on number of proxy assumptions violated and context factors.</li>
                <li>Replaced the single paradigm-rigidity parameter β with a multifactorial field model incorporating domain knowledge representation, computational resources, evaluator-domain interactions, boundary-crossing, and communication norms.</li>
                <li>Added explicit quantification of presentation quality and task framing effects: 20-60 percentage point shifts in proxy ratings.</li>
                <li>Added specific cross-domain degradation patterns: 40-50 percentage point AUROC drops for traditional citation-based metrics.</li>
                <li>Modified scope to clarify the theory applies primarily to traditional proxies, with well-designed alternatives showing minimal gaps (addressed in separate theory).</li>
                <li>Revised temporal model to acknowledge heterogeneous recognition patterns with rapid-recognition and prolonged-delay pathways depending on multiple factors.</li>
                <li>Added explicit distinction between 'true' transformation degree and 'perceived' transformation degree as mediated by presentation quality.</li>
                <li>Removed claim about multiplicative compounding as universal pattern; noted that well-designed alternatives can break compounding.</li>
                <li>Clarified that training distribution bias is a primary mechanism for traditional proxies but can be overcome by architectural improvements.</li>
                <li>Added specific quantitative evidence: LLM field-dependent performance (AUROC ~0.8 CS, ~0.6 biomedicine), open-ended vs guided task effects (comparable rates 15.79-78.95% to 40-100%).</li>
                <li>Specified that the gap is not inherent to automated evaluation but rather a consequence of traditional proxy architecture.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Traditional evaluation proxies (citation counts, journal prestige, standard peer review, author reputation) are implicitly calibrated on the distribution of incremental science in their training data, creating systematic undervaluation of transformational discoveries.</li>
                <li>The proxy-truth gap arises from multiple simultaneous proxy failures: citation-based proxies fail due to lack of immediate recognition; prestige-based proxies fail when transformational work appears in non-traditional venues; author-reputation proxies fail when transformational work comes from unexpected sources; methodology-familiarity proxies fail when transformational work uses novel approaches.</li>
                <li>Gap magnitude for traditional proxies ranges from 20-40% undervaluation for work violating some assumptions to 60%+ undervaluation or rejection for work violating multiple assumptions simultaneously.</li>
                <li>The gap is strongly modulated by presentation quality and communication framing, which can shift proxy ratings by 20-60 percentage points independently of substantive novelty.</li>
                <li>Traditional proxies show severe cross-domain degradation: citation-based metrics degrade from AUROC 0.75-0.85 within domains to 0.36-0.40 cross-domain, representing 40-50 percentage point drops.</li>
                <li>Training distribution bias is a primary mechanism: systems trained on historical data over-representing incremental work systematically undervalue out-of-distribution transformational discoveries.</li>
                <li>The gap is time-dependent: traditional proxies are lagging indicators, with ground truth manifesting through long-term impact measures while automated systems operate on short-term proxies.</li>
                <li>Field differences in gap magnitude depend on multiple factors: domain knowledge representation in training data, computational resource requirements, evaluator-domain interactions, whether work crosses domain boundaries, and field-specific communication norms.</li>
                <li>LLM-based reviewers show field-dependent performance: AUROC approximately 0.8 in computer science but approximately 0.6 in biomedicine when using external literature, with degradation linked to training data coverage.</li>
                <li>Open-ended task framing produces substantially improved ratings compared to guided framing: comparable rates increase from 15.79-78.95% (guided) to 40-100% (open-ended), demonstrating task-framing effects.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Boudreau et al. (2016) documented that peer reviewers systematically penalize novel proposals even when quality is equal, with bias sufficient to fully offset the novelty premium. <a href="../results/extraction-result-2132.html#e2132.0" class="evidence-link">[e2132.0]</a> </li>
    <li>Campanario (2009) cataloged 24 cases where Nobel-winning papers were initially rejected by peer reviewers, providing concrete evidence that traditional proxies fail to recognize transformational work. <a href="../results/extraction-result-2132.html#e2132.3" class="evidence-link">[e2132.3]</a> </li>
    <li>Citation-based proxies show severe cross-domain degradation: Historical Dissimilarity AUROC drops from 0.75-0.85 in single domains to 0.36-0.40 cross-domain (absolute drop 0.40-0.49), demonstrating systematic proxy-truth gaps when work falls outside training distribution. <a href="../results/extraction-result-2134.html#e2134.1" class="evidence-link">[e2134.1]</a> </li>
    <li>Multiple studies document training distribution bias: LLM reviewers inherit biases from training corpora, embedding models trained on temporally truncated data systematically under-represent novel post-training work, and simulation studies show rich-get-richer effects amplified by AI systems. <a href="../results/extraction-result-2131.html#e2131.2" class="evidence-link">[e2131.2]</a> <a href="../results/extraction-result-2127.html#e2127.2" class="evidence-link">[e2127.2]</a> <a href="../results/extraction-result-2128.html#e2128.2" class="evidence-link">[e2128.2]</a> <a href="../results/extraction-result-2128.html#e2128.1" class="evidence-link">[e2128.1]</a> </li>
    <li>Disruption Index framework shows citation counts conflate different citation intents (including negative citations) and fail to distinguish disruptive from developmental work, requiring corrective measures. <a href="../results/extraction-result-2133.html#e2133.0" class="evidence-link">[e2133.0]</a> <a href="../results/extraction-result-2133.html#e2133.1" class="evidence-link">[e2133.1]</a> <a href="../results/extraction-result-2133.html#e2133.4" class="evidence-link">[e2133.4]</a> </li>
    <li>Bias against novelty documented across multiple proxy types: bibliometric indicators penalize novel work, citation-network structural metrics miss dimension-specific novelty, peer review shows systematic bias, and journal-venue proxies fail for transformational discoveries. <a href="../results/extraction-result-2133.html#e2133.3" class="evidence-link">[e2133.3]</a> <a href="../results/extraction-result-2129.html#e2129.1" class="evidence-link">[e2129.1]</a> <a href="../results/extraction-result-2132.html#e2132.5" class="evidence-link">[e2132.5]</a> <a href="../results/extraction-result-2134.html#e2134.2" class="evidence-link">[e2134.2]</a> </li>
    <li>Citation-based proxies are lagging indicators that fail to capture novelty at inception, requiring recent publication dates rather than citation accumulation for identification. <a href="../results/extraction-result-2129.html#e2129.0" class="evidence-link">[e2129.0]</a> <a href="../results/extraction-result-2134.html#e2134.2" class="evidence-link">[e2134.2]</a> </li>
    <li>Transformational discoveries often originate at disciplinary edges and from unconventional combinations, violating author-reputation and methodological-familiarity proxies. <a href="../results/extraction-result-2132.html#e2132.5" class="evidence-link">[e2132.5]</a> <a href="../results/extraction-result-2133.html#e2133.5" class="evidence-link">[e2133.5]</a> </li>
    <li>Presentation quality and stylistic features affect proxy ratings independently of substantive novelty, with LLM-based reviewers particularly sensitive to presentation, creating gaps between true and perceived transformation. <a href="../results/extraction-result-2131.html#e2131.2" class="evidence-link">[e2131.2]</a> <a href="../results/extraction-result-2130.html#e2130.1" class="evidence-link">[e2130.1]</a> <a href="../results/extraction-result-2130.html#e2130.2" class="evidence-link">[e2130.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Traditional citation-based evaluation systems will undervalue papers with high novelty scores by 30-50% compared to eventual long-term impact when applied within single domains, and by 50-70% when applied cross-domain.</li>
                <li>Papers combining concepts from distant domains will show 20-30 percentage point larger proxy-truth gaps than papers within single domains when evaluated by traditional citation-based proxies, even controlling for quality.</li>
                <li>Transformational work that is well-communicated and framed as extensions of existing paradigms will show 20-40 percentage point smaller gaps than poorly-communicated transformational work of equivalent substantive novelty.</li>
                <li>Traditional peer review will systematically penalize novel proposals with bias magnitude sufficient to offset novelty premiums, resulting in 40-60% higher rejection rates for novel work from junior researchers compared to incremental work from established researchers.</li>
                <li>Citation-based metrics will show 40-50 percentage point AUROC degradation when applied cross-domain compared to within-domain application.</li>
                <li>Work introducing novel methodologies from peripheral institutions will face rejection rates 40-60% higher than equivalent-quality incremental work from central institutions when evaluated by traditional peer review.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If scientific communities increasingly rely on traditional automated evaluation without correction mechanisms, this might create an accelerating feedback loop where transformational work becomes progressively more disadvantaged, potentially causing measurable slowdowns in paradigm-shifting discoveries, or the effect might be modest because transformational work succeeds through alternative pathways.</li>
                <li>Training automated systems on curated datasets of historically transformational discoveries might improve their ability to identify future transformational work by 30-50%, or might cause overfitting to past transformation patterns that don't generalize, creating new biases.</li>
                <li>Implementing explicit novelty bonuses (40-60% valuation increases for high-novelty work) in funding decisions might correct gaps and accelerate scientific progress, or might create gaming behavior where researchers artificially inflate novelty signals.</li>
                <li>As AI systems generate scientific discoveries, proxy-truth gap patterns might fundamentally change because AI-generated discoveries may have different novelty distributions and impact patterns, potentially requiring entirely new evaluation frameworks.</li>
                <li>The long-term cumulative effect of 40-60% systematic underfunding of transformational work might be catastrophic (missing multiple paradigm shifts) or modest (because transformational work eventually succeeds), with the outcome depending on field-specific factors and alternative support mechanisms.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If traditional proxies (citations, peer review, journal prestige) accurately identify transformational work at rates comparable to ground truth (>80% accuracy) without correction mechanisms, the core claim of systematic undervaluation would be falsified.</li>
                <li>If transformational discoveries show equal or smaller gaps than incremental discoveries when using traditional short-term citation metrics, the systematic undervaluation claim would be challenged.</li>
                <li>If gap magnitude does not vary significantly with the number of proxy assumptions violated (single vs multiple failures showing similar gaps), the claim about interacting proxy failures would be invalidated.</li>
                <li>If presentation quality and communication framing have negligible effects (<5 percentage points) on proxy ratings when substantive novelty is controlled, the context-dependence claim would be weakened.</li>
                <li>If automated systems trained exclusively on transformational discoveries show the same gap patterns as systems trained on typical publications, the training distribution bias hypothesis would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not fully explain why some transformational discoveries (documented in Uzzi et al. 2013) receive rapid recognition and high citations while others face prolonged delays, suggesting additional success factors beyond proxy architecture and presentation. <a href="../results/extraction-result-2133.html#e2133.5" class="evidence-link">[e2133.5]</a> </li>
    <li>The specific mechanisms by which different types of transformation (theoretical vs methodological vs empirical) might show different gap patterns are not addressed. </li>
    <li>The influence of social networks, collaboration patterns, and scientific communication channels on modulating proxy-truth gaps is not fully incorporated. </li>
    <li>The exact mechanisms determining which persistent conceptual gaps in embedding space are filled by newer versus older work (4 of 11 persistent holes showed higher mixup for newer documents) remain unexplained. <a href="../results/extraction-result-2128.html#e2128.1" class="evidence-link">[e2128.1]</a> </li>
    <li>The theory does not account for the heterogeneity in evaluator-domain interactions where some combinations show anomalous performance patterns not explained by general field characteristics. <a href="../results/extraction-result-2131.html#e2131.3" class="evidence-link">[e2131.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Traditional Proxy Systematic Bias Theory",
    "type": "specific",
    "theory_description": "This theory posits that traditional evaluation proxies (citation counts, journal prestige, standard peer review, author reputation) systematically undervalue transformational scientific discoveries due to training distribution bias and multiple simultaneous proxy failures. Traditional proxies are calibrated on historical data that predominantly reflects incremental science, creating systematic bias against work that violates implicit assumptions embedded in these metrics. The undervaluation arises because transformational discoveries simultaneously violate multiple proxy assumptions: they lack immediate citation recognition, may be published in non-traditional venues, often come from unexpected sources, and use novel methodologies. The magnitude and pattern of this gap is highly context-dependent, modulated by presentation quality (20-60 percentage point effects), task framing, field receptivity, and author reputation. Traditional proxies show gaps ranging from moderate (20-40% undervaluation) for work violating some assumptions to severe (60%+ undervaluation or rejection) for work violating multiple assumptions simultaneously. However, the gap is not inherent to automated evaluation—well-designed alternative proxies can substantially reduce or eliminate these gaps, indicating the problem is one of proxy architecture rather than fundamental limitations of automation.",
    "supporting_evidence": [
        {
            "text": "Boudreau et al. (2016) documented that peer reviewers systematically penalize novel proposals even when quality is equal, with bias sufficient to fully offset the novelty premium.",
            "uuids": [
                "e2132.0"
            ]
        },
        {
            "text": "Campanario (2009) cataloged 24 cases where Nobel-winning papers were initially rejected by peer reviewers, providing concrete evidence that traditional proxies fail to recognize transformational work.",
            "uuids": [
                "e2132.3"
            ]
        },
        {
            "text": "Citation-based proxies show severe cross-domain degradation: Historical Dissimilarity AUROC drops from 0.75-0.85 in single domains to 0.36-0.40 cross-domain (absolute drop 0.40-0.49), demonstrating systematic proxy-truth gaps when work falls outside training distribution.",
            "uuids": [
                "e2134.1"
            ]
        },
        {
            "text": "Multiple studies document training distribution bias: LLM reviewers inherit biases from training corpora, embedding models trained on temporally truncated data systematically under-represent novel post-training work, and simulation studies show rich-get-richer effects amplified by AI systems.",
            "uuids": [
                "e2131.2",
                "e2127.2",
                "e2128.2",
                "e2128.1"
            ]
        },
        {
            "text": "Disruption Index framework shows citation counts conflate different citation intents (including negative citations) and fail to distinguish disruptive from developmental work, requiring corrective measures.",
            "uuids": [
                "e2133.0",
                "e2133.1",
                "e2133.4"
            ]
        },
        {
            "text": "Bias against novelty documented across multiple proxy types: bibliometric indicators penalize novel work, citation-network structural metrics miss dimension-specific novelty, peer review shows systematic bias, and journal-venue proxies fail for transformational discoveries.",
            "uuids": [
                "e2133.3",
                "e2129.1",
                "e2132.5",
                "e2134.2"
            ]
        },
        {
            "text": "Citation-based proxies are lagging indicators that fail to capture novelty at inception, requiring recent publication dates rather than citation accumulation for identification.",
            "uuids": [
                "e2129.0",
                "e2134.2"
            ]
        },
        {
            "text": "Transformational discoveries often originate at disciplinary edges and from unconventional combinations, violating author-reputation and methodological-familiarity proxies.",
            "uuids": [
                "e2132.5",
                "e2133.5"
            ]
        },
        {
            "text": "Presentation quality and stylistic features affect proxy ratings independently of substantive novelty, with LLM-based reviewers particularly sensitive to presentation, creating gaps between true and perceived transformation.",
            "uuids": [
                "e2131.2",
                "e2130.1",
                "e2130.2"
            ]
        }
    ],
    "theory_statements": [
        "Traditional evaluation proxies (citation counts, journal prestige, standard peer review, author reputation) are implicitly calibrated on the distribution of incremental science in their training data, creating systematic undervaluation of transformational discoveries.",
        "The proxy-truth gap arises from multiple simultaneous proxy failures: citation-based proxies fail due to lack of immediate recognition; prestige-based proxies fail when transformational work appears in non-traditional venues; author-reputation proxies fail when transformational work comes from unexpected sources; methodology-familiarity proxies fail when transformational work uses novel approaches.",
        "Gap magnitude for traditional proxies ranges from 20-40% undervaluation for work violating some assumptions to 60%+ undervaluation or rejection for work violating multiple assumptions simultaneously.",
        "The gap is strongly modulated by presentation quality and communication framing, which can shift proxy ratings by 20-60 percentage points independently of substantive novelty.",
        "Traditional proxies show severe cross-domain degradation: citation-based metrics degrade from AUROC 0.75-0.85 within domains to 0.36-0.40 cross-domain, representing 40-50 percentage point drops.",
        "Training distribution bias is a primary mechanism: systems trained on historical data over-representing incremental work systematically undervalue out-of-distribution transformational discoveries.",
        "The gap is time-dependent: traditional proxies are lagging indicators, with ground truth manifesting through long-term impact measures while automated systems operate on short-term proxies.",
        "Field differences in gap magnitude depend on multiple factors: domain knowledge representation in training data, computational resource requirements, evaluator-domain interactions, whether work crosses domain boundaries, and field-specific communication norms.",
        "LLM-based reviewers show field-dependent performance: AUROC approximately 0.8 in computer science but approximately 0.6 in biomedicine when using external literature, with degradation linked to training data coverage.",
        "Open-ended task framing produces substantially improved ratings compared to guided framing: comparable rates increase from 15.79-78.95% (guided) to 40-100% (open-ended), demonstrating task-framing effects."
    ],
    "new_predictions_likely": [
        "Traditional citation-based evaluation systems will undervalue papers with high novelty scores by 30-50% compared to eventual long-term impact when applied within single domains, and by 50-70% when applied cross-domain.",
        "Papers combining concepts from distant domains will show 20-30 percentage point larger proxy-truth gaps than papers within single domains when evaluated by traditional citation-based proxies, even controlling for quality.",
        "Transformational work that is well-communicated and framed as extensions of existing paradigms will show 20-40 percentage point smaller gaps than poorly-communicated transformational work of equivalent substantive novelty.",
        "Traditional peer review will systematically penalize novel proposals with bias magnitude sufficient to offset novelty premiums, resulting in 40-60% higher rejection rates for novel work from junior researchers compared to incremental work from established researchers.",
        "Citation-based metrics will show 40-50 percentage point AUROC degradation when applied cross-domain compared to within-domain application.",
        "Work introducing novel methodologies from peripheral institutions will face rejection rates 40-60% higher than equivalent-quality incremental work from central institutions when evaluated by traditional peer review."
    ],
    "new_predictions_unknown": [
        "If scientific communities increasingly rely on traditional automated evaluation without correction mechanisms, this might create an accelerating feedback loop where transformational work becomes progressively more disadvantaged, potentially causing measurable slowdowns in paradigm-shifting discoveries, or the effect might be modest because transformational work succeeds through alternative pathways.",
        "Training automated systems on curated datasets of historically transformational discoveries might improve their ability to identify future transformational work by 30-50%, or might cause overfitting to past transformation patterns that don't generalize, creating new biases.",
        "Implementing explicit novelty bonuses (40-60% valuation increases for high-novelty work) in funding decisions might correct gaps and accelerate scientific progress, or might create gaming behavior where researchers artificially inflate novelty signals.",
        "As AI systems generate scientific discoveries, proxy-truth gap patterns might fundamentally change because AI-generated discoveries may have different novelty distributions and impact patterns, potentially requiring entirely new evaluation frameworks.",
        "The long-term cumulative effect of 40-60% systematic underfunding of transformational work might be catastrophic (missing multiple paradigm shifts) or modest (because transformational work eventually succeeds), with the outcome depending on field-specific factors and alternative support mechanisms."
    ],
    "negative_experiments": [
        "If traditional proxies (citations, peer review, journal prestige) accurately identify transformational work at rates comparable to ground truth (&gt;80% accuracy) without correction mechanisms, the core claim of systematic undervaluation would be falsified.",
        "If transformational discoveries show equal or smaller gaps than incremental discoveries when using traditional short-term citation metrics, the systematic undervaluation claim would be challenged.",
        "If gap magnitude does not vary significantly with the number of proxy assumptions violated (single vs multiple failures showing similar gaps), the claim about interacting proxy failures would be invalidated.",
        "If presentation quality and communication framing have negligible effects (&lt;5 percentage points) on proxy ratings when substantive novelty is controlled, the context-dependence claim would be weakened.",
        "If automated systems trained exclusively on transformational discoveries show the same gap patterns as systems trained on typical publications, the training distribution bias hypothesis would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not fully explain why some transformational discoveries (documented in Uzzi et al. 2013) receive rapid recognition and high citations while others face prolonged delays, suggesting additional success factors beyond proxy architecture and presentation.",
            "uuids": [
                "e2133.5"
            ]
        },
        {
            "text": "The specific mechanisms by which different types of transformation (theoretical vs methodological vs empirical) might show different gap patterns are not addressed.",
            "uuids": []
        },
        {
            "text": "The influence of social networks, collaboration patterns, and scientific communication channels on modulating proxy-truth gaps is not fully incorporated.",
            "uuids": []
        },
        {
            "text": "The exact mechanisms determining which persistent conceptual gaps in embedding space are filled by newer versus older work (4 of 11 persistent holes showed higher mixup for newer documents) remain unexplained.",
            "uuids": [
                "e2128.1"
            ]
        },
        {
            "text": "The theory does not account for the heterogeneity in evaluator-domain interactions where some combinations show anomalous performance patterns not explained by general field characteristics.",
            "uuids": [
                "e2131.3"
            ]
        }
    ],
    "change_log": [
        "Removed the exponential formula G(T) ≈ k * e^(βT) as it is not validated by evidence; replaced with context-dependent gap ranges (20-40% for moderate violations, 60%+ for severe violations).",
        "Modified the 70-90% undervaluation claim to reflect observed heterogeneity: gaps range from 20-40% to 60%+ depending on number of proxy assumptions violated and context factors.",
        "Replaced the single paradigm-rigidity parameter β with a multifactorial field model incorporating domain knowledge representation, computational resources, evaluator-domain interactions, boundary-crossing, and communication norms.",
        "Added explicit quantification of presentation quality and task framing effects: 20-60 percentage point shifts in proxy ratings.",
        "Added specific cross-domain degradation patterns: 40-50 percentage point AUROC drops for traditional citation-based metrics.",
        "Modified scope to clarify the theory applies primarily to traditional proxies, with well-designed alternatives showing minimal gaps (addressed in separate theory).",
        "Revised temporal model to acknowledge heterogeneous recognition patterns with rapid-recognition and prolonged-delay pathways depending on multiple factors.",
        "Added explicit distinction between 'true' transformation degree and 'perceived' transformation degree as mediated by presentation quality.",
        "Removed claim about multiplicative compounding as universal pattern; noted that well-designed alternatives can break compounding.",
        "Clarified that training distribution bias is a primary mechanism for traditional proxies but can be overcome by architectural improvements.",
        "Added specific quantitative evidence: LLM field-dependent performance (AUROC ~0.8 CS, ~0.6 biomedicine), open-ended vs guided task effects (comparable rates 15.79-78.95% to 40-100%).",
        "Specified that the gap is not inherent to automated evaluation but rather a consequence of traditional proxy architecture."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
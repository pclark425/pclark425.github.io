<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IRAST – Law of Evidence-Weighted Theory Synthesis - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-2117</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-2117</p>
                <p><strong>Name:</strong> IRAST – Law of Evidence-Weighted Theory Synthesis</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can be used to distill theories from large numbers of scholarly papers, given a specific topic or query.</p>
                <p><strong>Description:</strong> This specific theory posits that LLMs, when distilling theories from scholarly papers, implicitly assign weights to evidence fragments based on their relevance, recency, and consensus, and that these weights modulate the influence of each fragment on the synthesized theory statements. The process results in theory statements that reflect the weighted aggregation of the most salient and credible evidence.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Evidence Weighting Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; retrieves &#8594; multiple_evidence_fragments<span style="color: #888888;">, and</span></div>
        <div>&#8226; evidence_fragment &#8594; has &#8594; attributes (relevance, recency, consensus)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; assigns &#8594; implicit_weight_to_evidence_fragment<span style="color: #888888;">, and</span></div>
        <div>&#8226; theory_statement &#8594; is_influenced_by &#8594; aggregate_weighted_evidence</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs prioritize more relevant and recent information in summarization and question answering tasks. </li>
    <li>Human meta-analyses weigh evidence by quality and consensus; LLMs can emulate this process. </li>
    <li>Empirical studies show LLMs are more likely to surface highly-cited or consensus-supported findings in their outputs. </li>
    <li>LLMs trained on up-to-date corpora tend to reflect recent scientific developments in their responses. </li>
    <li>When presented with conflicting evidence, LLMs often summarize the majority or consensus view. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While evidence weighting is known, its implicit, multi-attribute application in LLM theory distillation is new.</p>            <p><strong>What Already Exists:</strong> Evidence weighting is standard in meta-analysis and some LLM summarization techniques.</p>            <p><strong>What is Novel:</strong> The formalization of implicit, multi-attribute evidence weighting in LLM-driven theory synthesis is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Bastian et al. (2010) Systematic reviews: a cross-sectional study of characteristics and citation impact [Evidence weighting in meta-analysis]</li>
    <li>Liu et al. (2023) Evaluating the Factual Consistency of Large Language Models [LLM evidence prioritization]</li>
    <li>Gilardi et al. (2023) ChatGPT outperforms humans in coding political texts [LLM evidence prioritization in summarization]</li>
    <li>Jiang et al. (2023) How Can We Know When Language Models Know? [LLM evidence aggregation and consensus detection]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will produce theory statements that align more closely with recent and highly-cited evidence when such evidence is present.</li>
                <li>If conflicting evidence exists, LLMs will tend to favor the consensus or majority view in the synthesized theory.</li>
                <li>When prompted to summarize a field, LLMs will underrepresent outdated or fringe evidence compared to consensus findings.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may be able to surface minority or emerging scientific views if these are sufficiently salient in the evidence, even if not yet consensus.</li>
                <li>The weighting mechanism may enable LLMs to identify paradigm shifts before they are widely recognized in the field.</li>
                <li>LLMs might develop biases if the training data overrepresents certain types of evidence, leading to systematic errors in theory synthesis.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs do not preferentially incorporate more relevant or recent evidence into theory statements, the law is challenged.</li>
                <li>If theory statements are equally influenced by all evidence regardless of quality or consensus, the law is called into question.</li>
                <li>If LLMs consistently amplify outdated or fringe evidence despite the presence of strong consensus, the law is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The explicit mechanism by which LLMs assign weights to evidence is not directly observable or controllable. </li>
    <li>LLMs may be influenced by spurious correlations in the training data, leading to non-evidence-based weighting. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The law extends known evidence weighting principles to the implicit, multi-attribute context of LLM theory synthesis.</p>
            <p><strong>References:</strong> <ul>
    <li>Bastian et al. (2010) Systematic reviews: a cross-sectional study of characteristics and citation impact [Evidence weighting in meta-analysis]</li>
    <li>Liu et al. (2023) Evaluating the Factual Consistency of Large Language Models [LLM evidence prioritization]</li>
    <li>Gilardi et al. (2023) ChatGPT outperforms humans in coding political texts [LLM evidence prioritization in summarization]</li>
    <li>Jiang et al. (2023) How Can We Know When Language Models Know? [LLM evidence aggregation and consensus detection]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "IRAST – Law of Evidence-Weighted Theory Synthesis",
    "theory_description": "This specific theory posits that LLMs, when distilling theories from scholarly papers, implicitly assign weights to evidence fragments based on their relevance, recency, and consensus, and that these weights modulate the influence of each fragment on the synthesized theory statements. The process results in theory statements that reflect the weighted aggregation of the most salient and credible evidence.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Evidence Weighting Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "retrieves",
                        "object": "multiple_evidence_fragments"
                    },
                    {
                        "subject": "evidence_fragment",
                        "relation": "has",
                        "object": "attributes (relevance, recency, consensus)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "assigns",
                        "object": "implicit_weight_to_evidence_fragment"
                    },
                    {
                        "subject": "theory_statement",
                        "relation": "is_influenced_by",
                        "object": "aggregate_weighted_evidence"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs prioritize more relevant and recent information in summarization and question answering tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Human meta-analyses weigh evidence by quality and consensus; LLMs can emulate this process.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LLMs are more likely to surface highly-cited or consensus-supported findings in their outputs.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs trained on up-to-date corpora tend to reflect recent scientific developments in their responses.",
                        "uuids": []
                    },
                    {
                        "text": "When presented with conflicting evidence, LLMs often summarize the majority or consensus view.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Evidence weighting is standard in meta-analysis and some LLM summarization techniques.",
                    "what_is_novel": "The formalization of implicit, multi-attribute evidence weighting in LLM-driven theory synthesis is novel.",
                    "classification_explanation": "While evidence weighting is known, its implicit, multi-attribute application in LLM theory distillation is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bastian et al. (2010) Systematic reviews: a cross-sectional study of characteristics and citation impact [Evidence weighting in meta-analysis]",
                        "Liu et al. (2023) Evaluating the Factual Consistency of Large Language Models [LLM evidence prioritization]",
                        "Gilardi et al. (2023) ChatGPT outperforms humans in coding political texts [LLM evidence prioritization in summarization]",
                        "Jiang et al. (2023) How Can We Know When Language Models Know? [LLM evidence aggregation and consensus detection]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will produce theory statements that align more closely with recent and highly-cited evidence when such evidence is present.",
        "If conflicting evidence exists, LLMs will tend to favor the consensus or majority view in the synthesized theory.",
        "When prompted to summarize a field, LLMs will underrepresent outdated or fringe evidence compared to consensus findings."
    ],
    "new_predictions_unknown": [
        "LLMs may be able to surface minority or emerging scientific views if these are sufficiently salient in the evidence, even if not yet consensus.",
        "The weighting mechanism may enable LLMs to identify paradigm shifts before they are widely recognized in the field.",
        "LLMs might develop biases if the training data overrepresents certain types of evidence, leading to systematic errors in theory synthesis."
    ],
    "negative_experiments": [
        "If LLMs do not preferentially incorporate more relevant or recent evidence into theory statements, the law is challenged.",
        "If theory statements are equally influenced by all evidence regardless of quality or consensus, the law is called into question.",
        "If LLMs consistently amplify outdated or fringe evidence despite the presence of strong consensus, the law is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "The explicit mechanism by which LLMs assign weights to evidence is not directly observable or controllable.",
            "uuids": []
        },
        {
            "text": "LLMs may be influenced by spurious correlations in the training data, leading to non-evidence-based weighting.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Cases where LLMs amplify outdated or fringe evidence despite the presence of strong consensus.",
            "uuids": []
        },
        {
            "text": "Instances where LLMs hallucinate or fabricate evidence, undermining the evidence-weighting process.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In fields with no clear consensus, evidence weighting may be diffuse, leading to ambiguous theory statements.",
        "If the LLM's training data is skewed, evidence weighting may not reflect true scientific consensus.",
        "In rapidly evolving fields, recency may outweigh consensus, leading to premature adoption of new theories."
    ],
    "existing_theory": {
        "what_already_exists": "Evidence weighting is standard in meta-analysis and some LLM applications.",
        "what_is_novel": "The implicit, multi-attribute evidence weighting in LLM-driven theory distillation is new.",
        "classification_explanation": "The law extends known evidence weighting principles to the implicit, multi-attribute context of LLM theory synthesis.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Bastian et al. (2010) Systematic reviews: a cross-sectional study of characteristics and citation impact [Evidence weighting in meta-analysis]",
            "Liu et al. (2023) Evaluating the Factual Consistency of Large Language Models [LLM evidence prioritization]",
            "Gilardi et al. (2023) ChatGPT outperforms humans in coding political texts [LLM evidence prioritization in summarization]",
            "Jiang et al. (2023) How Can We Know When Language Models Know? [LLM evidence aggregation and consensus detection]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "type": "specific",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can be used to distill theories from large numbers of scholarly papers, given a specific topic or query.",
    "original_theory_id": "theory-667",
    "original_theory_name": "Iterative Retrieval-Augmented Synthesis Theory (IRAST)",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Iterative Retrieval-Augmented Synthesis Theory (IRAST)",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
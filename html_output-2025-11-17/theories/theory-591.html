<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Task-Structure-Dependent Memory Utility Law for LLM Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-591</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-591</p>
                <p><strong>Name:</strong> Task-Structure-Dependent Memory Utility Law for LLM Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks, based on the following results.</p>
                <p><strong>Description:</strong> The effectiveness and necessity of memory mechanisms in LLM agents for text games are determined by the structure of the task: tasks with partial observability, long-horizon dependencies, or requirements for backtracking, comparison, or cross-episode learning demand explicit memory (episodic, structured, or retrieval-augmented), while tasks with short horizons, full observability, or simple action spaces can be solved with prompt-only or minimal memory. Furthermore, the form and update mechanism of memory must be matched to the task's compositional and temporal structure to avoid performance degradation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Memory Utility Increases with Task Horizon and Partial Observability (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; has_property &#8594; long-horizon or partial observability</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; requires &#8594; explicit memory (episodic, structured, or retrieval-augmented)<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; achieves &#8594; higher performance with memory than without</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>CoELA, AGILE, and Generative Agents show that memory ablation nearly doubles steps to completion or causes severe degradation on long-horizon, partially observable tasks. <a href="../results/extraction-result-4864.html#e4864.0" class="evidence-link">[e4864.0]</a> <a href="../results/extraction-result-4674.html#e4674.0" class="evidence-link">[e4674.0]</a> <a href="../results/extraction-result-4900.html#e4900.1" class="evidence-link">[e4900.1]</a> </li>
    <li>Voyager's skill library is critical for open-ended, multi-stage exploration in Minecraft; ablation leads to plateauing and failure to generalize. <a href="../results/extraction-result-4919.html#e4919.0" class="evidence-link">[e4919.0]</a> </li>
    <li>Tree of Thoughts and ToT ablations show that backtracking and state memory are critical for solving multi-step puzzles (crosswords, Game of 24). <a href="../results/extraction-result-4894.html#e4894.1" class="evidence-link">[e4894.1]</a> </li>
    <li>MemNNs (Memory Networks) require multi-hop and time-aware memory to solve multi-step and temporal reasoning tasks; ablations removing memory or time features cause large drops in accuracy. <a href="../results/extraction-result-4903.html#e4903.0" class="evidence-link">[e4903.0]</a> </li>
    <li>RecurrentGPT ablations show that both long-term and short-term memory are necessary for maintaining coherence and interestingness in long-form text generation; removing either degrades performance. <a href="../results/extraction-result-4876.html#e4876.2" class="evidence-link">[e4876.2]</a> <a href="../results/extraction-result-4876.html#e4876.3" class="evidence-link">[e4876.3]</a> <a href="../results/extraction-result-4876.html#e4876.4" class="evidence-link">[e4876.4]</a> </li>
    <li>AGENTS, AgentSims, and Generative Agents (Park et al.) all use hybrid memory (short-term prompt + long-term vector DB) to maintain behavioral consistency and long-horizon planning. <a href="../results/extraction-result-4915.html#e4915.0" class="evidence-link">[e4915.0]</a> <a href="../results/extraction-result-4900.html#e4900.4" class="evidence-link">[e4900.4]</a> <a href="../results/extraction-result-4919.html#e4919.4" class="evidence-link">[e4919.4]</a> <a href="../results/extraction-result-4685.html#e4685.3" class="evidence-link">[e4685.3]</a> </li>
    <li>ALFWorld, ExpeL, and Reflexion show that retrieval-augmented or reflection-based memory improves cross-trial and long-horizon task performance. <a href="../results/extraction-result-4683.html#e4683.4" class="evidence-link">[e4683.4]</a> <a href="../results/extraction-result-4683.html#e4683.2" class="evidence-link">[e4683.2]</a> <a href="../results/extraction-result-4920.html#e4920.4" class="evidence-link">[e4920.4]</a> <a href="../results/extraction-result-4672.html#e4672.2" class="evidence-link">[e4672.2]</a> </li>
    <li>LSTM-DQN, LSTM-DRQN, and BabyAI learner all use recurrent memory to handle partial observability; ablations or comparisons show that stateless or non-recurrent models perform worse. <a href="../results/extraction-result-4884.html#e4884.0" class="evidence-link">[e4884.0]</a> <a href="../results/extraction-result-4923.html#e4923.3" class="evidence-link">[e4923.3]</a> <a href="../results/extraction-result-4891.html#e4891.0" class="evidence-link">[e4891.0]</a> <a href="../results/extraction-result-4891.html#e4891.3" class="evidence-link">[e4891.3]</a> <a href="../results/extraction-result-4922.html#e4922.0" class="evidence-link">[e4922.0]</a> </li>
    <li>KG-DQN and NAIL use structured knowledge graph memory to support exploration and generalization in text games; seeding and updating the graph improves performance, especially in complex or partially observable games. <a href="../results/extraction-result-4886.html#e4886.0" class="evidence-link">[e4886.0]</a> <a href="../results/extraction-result-4923.html#e4923.1" class="evidence-link">[e4923.1]</a> <a href="../results/extraction-result-4673.html#e4673.0" class="evidence-link">[e4673.0]</a> <a href="../results/extraction-result-4673.html#e4673.2" class="evidence-link">[e4673.2]</a> <a href="../results/extraction-result-4923.html#e4923.4" class="evidence-link">[e4923.4]</a> </li>
    <li>SynAPSE, ExpeL, and episodic buffer approaches show that retrieval-augmented exemplar or episodic memory enables generalization and improved performance on new or long-horizon tasks. <a href="../results/extraction-result-4882.html#e4882.0" class="evidence-link">[e4882.0]</a> <a href="../results/extraction-result-4683.html#e4683.2" class="evidence-link">[e4683.2]</a> <a href="../results/extraction-result-4663.html#e4663.1" class="evidence-link">[e4663.1]</a> </li>
    <li>PsychoGAT and RecurrentGPT demonstrate that summarization-based memory is necessary for long-form interactive fiction and psychological measurement games. <a href="../results/extraction-result-4661.html#e4661.0" class="evidence-link">[e4661.0]</a> <a href="../results/extraction-result-4876.html#e4876.2" class="evidence-link">[e4876.2]</a> <a href="../results/extraction-result-4876.html#e4876.3" class="evidence-link">[e4876.3]</a> </li>
    <li>Observation summarization, failure recovery, and self-correction techniques are recommended for long-horizon web tasks to mitigate context-window limitations. <a href="../results/extraction-result-4914.html#e4914.7" class="evidence-link">[e4914.7]</a> </li>
    <li>Replay memory and prioritized experience replay are necessary for RL agents to learn from rare events and avoid local maxima in long-horizon, sparse-reward games. <a href="../results/extraction-result-4922.html#e4922.2" class="evidence-link">[e4922.2]</a> <a href="../results/extraction-result-4922.html#e4922.3" class="evidence-link">[e4922.3]</a> </li>
    <li>Explicit belief-state memory in GPT-4+Belief agent reduces invalid actions and improves efficiency in multi-agent, partially observable text games. <a href="../results/extraction-result-4827.html#e4827.0" class="evidence-link">[e4827.0]</a> </li>
    <li>ProAgent and LLM-Agent (LLM-Coordination) frameworks use explicit memory scaffolds (long-term, working, episodic) to support multi-agent coordination and ToM reasoning; ablations removing memory or reasoning steps degrade performance. <a href="../results/extraction-result-4802.html#e4802.0" class="evidence-link">[e4802.0]</a> <a href="../results/extraction-result-4659.html#e4659.0" class="evidence-link">[e4659.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> While RL and cognitive science recognize the need for memory in partially observable MDPs, this law extends and formalizes it for LLM agents in text games, with new empirical support and explicit mapping to memory type and task structure.</p>            <p><strong>What Already Exists:</strong> The relationship between partial observability/long horizon and the need for memory is discussed in RL and cognitive science, but not formalized for LLM agents in text games.</p>            <p><strong>What is Novel:</strong> This law formalizes the mapping between task structure and memory requirements for LLM agents in text games, with direct ablation evidence across a wide range of agent architectures and environments.</p>
            <p><strong>References:</strong> <ul>
    <li>Kaelbling et al. (1998) Planning and acting in partially observable stochastic domains [POMDPs and memory]</li>
    <li>Park et al. (2023) Generative agents: Interactive simulacra of human behavior [memory for long-horizon social simulation]</li>
    <li>Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [memory for multi-step reasoning]</li>
    <li>Weston et al. (2014) Memory Networks [explicit memory for multi-step reasoning]</li>
    <li>Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [modular memory for LLM agents]</li>
</ul>
            <h3>Statement 1: Naive Memory Inclusion Can Harm Performance if Not Matched to Task Structure (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; includes &#8594; naively concatenated or unfiltered memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; has_property &#8594; short horizon or simple structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; may experience &#8594; performance degradation or confusion</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>WebShop IL agent's naive history concatenation degraded performance (score dropped from 59.9 to 57.3). <a href="../results/extraction-result-4875.html#e4875.0" class="evidence-link">[e4875.0]</a> </li>
    <li>Swift agent on ScienceWorld performed better when action history was omitted, indicating that memory can be harmful if not filtered or structured. <a href="../results/extraction-result-4652.html#e4652.1" class="evidence-link">[e4652.1]</a> </li>
    <li>CALM-GPT2 and n-gram models, which use only short context, outperform more complex memory-augmented models on some tasks, suggesting that unnecessary memory can reduce precision. <a href="../results/extraction-result-4899.html#e4899.0" class="evidence-link">[e4899.0]</a> <a href="../results/extraction-result-4899.html#e4899.1" class="evidence-link">[e4899.1]</a> </li>
    <li>Rolling-ChatGPT (sliding-window baseline) quickly forgets earlier content and fails to maintain coherence for long outputs, demonstrating that naive context extension is insufficient for long-form tasks. <a href="../results/extraction-result-4876.html#e4876.1" class="evidence-link">[e4876.1]</a> </li>
    <li>In RecurrentGPT, removing short-term or long-term memory each causes large drops in coherence and interestingness, but simply increasing prompt length (sliding window) does not help. <a href="../results/extraction-result-4876.html#e4876.2" class="evidence-link">[e4876.2]</a> <a href="../results/extraction-result-4876.html#e4876.3" class="evidence-link">[e4876.3]</a> </li>
    <li>MindAct and Mind2Web agents that include large, unfiltered top-K element lists in context experience lower step success rates and higher token costs; reducing K improves performance. <a href="../results/extraction-result-4882.html#e4882.1" class="evidence-link">[e4882.1]</a> <a href="../results/extraction-result-4879.html#e4879.0" class="evidence-link">[e4879.0]</a> </li>
    <li>In BabyAI learner, no explicit ablation is reported, but the need for modularity and subroutines is emphasized over naive history concatenation. <a href="../results/extraction-result-4891.html#e4891.0" class="evidence-link">[e4891.0]</a> </li>
    <li>In LLM-only baselines (e.g., Smallville, Generative Agents), prompt-only memory fails to support long-term coherence, and adding more context does not solve the problem. <a href="../results/extraction-result-4651.html#e4651.1" class="evidence-link">[e4651.1]</a> <a href="../results/extraction-result-4915.html#e4915.0" class="evidence-link">[e4915.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While information overload is a known phenomenon, this law formalizes and empirically grounds it for LLM agent memory in text games, with direct ablation evidence.</p>            <p><strong>What Already Exists:</strong> The risk of information overload and irrelevant context is known in NLP and cognitive architectures, but not formalized as a law for LLM agent memory in text games.</p>            <p><strong>What is Novel:</strong> This law formalizes the risk and provides empirical evidence that naive memory inclusion can harm LLM agent performance, especially on short-horizon or simple tasks, and that memory must be structured and filtered to match task demands.</p>
            <p><strong>References:</strong> <ul>
    <li>Miller (1956) The magical number seven, plus or minus two [limits of working memory]</li>
    <li>Zhou et al. (2023) RecurrentGPT [memory summarization to avoid overload]</li>
    <li>Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [configurable memory]</li>
    <li>Weston et al. (2014) Memory Networks [need for memory selection and relevance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>On a new text game with partial observability and long-horizon dependencies, LLM agents with explicit episodic or retrieval-augmented memory will outperform prompt-only agents.</li>
                <li>On a short-horizon, fully observable text game, adding a large, unfiltered memory buffer to the agent's prompt will decrease performance compared to a minimal or no-memory baseline.</li>
                <li>If a task is modified to increase its horizon or introduce partial observability, the performance gap between memory-augmented and memory-less agents will increase.</li>
                <li>Ablating structured memory (e.g., knowledge graph, skill library, or episodic buffer) in a long-horizon or compositional task will cause a measurable drop in performance or generalization.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a memory-augmented LLM agent is given a task with dynamically changing observability (e.g., alternating between fully and partially observable phases), the agent will learn to adaptively use or ignore memory, potentially developing meta-memory strategies.</li>
                <li>In procedurally generated text games with novel compositional structures, agents with structured memory (e.g., knowledge graphs) will generalize better than those with only episodic or prompt-based memory.</li>
                <li>If an LLM agent is equipped with a learned memory management module (e.g., a memory management agent), it will outperform static memory strategies on tasks with highly variable structure.</li>
                <li>If a hybrid memory system (short-term + long-term + summarization) is used in a multi-agent social simulation, emergent behaviors such as reputation, planning, and deception will become more robust and human-like.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a prompt-only LLM agent matches or exceeds the performance of a memory-augmented agent on a long-horizon, partially observable text game, the theory would be challenged.</li>
                <li>If adding naive memory (e.g., concatenated history) to a short-horizon task consistently improves performance, the theory would be undermined.</li>
                <li>If structured memory (e.g., knowledge graphs) does not improve generalization in procedurally generated or compositional tasks, the theory would be called into question.</li>
                <li>If memory ablation in a multi-agent coordination or ToM task does not degrade performance, the theory's mapping between task structure and memory utility would be weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLM agents (e.g., ReAct, SayCan) perform well on certain long-horizon tasks without explicit external memory, possibly due to strong in-context reasoning or model scale. <a href="../results/extraction-result-4850.html#e4850.0" class="evidence-link">[e4850.0]</a> <a href="../results/extraction-result-4898.html#e4898.2" class="evidence-link">[e4898.2]</a> </li>
    <li>Certain tasks (e.g., social deduction games like Hoodwinked, Werewolf) have not been systematically tested with explicit memory augmentation, so the theory's predictions remain untested in these domains. <a href="../results/extraction-result-4860.html#e4860.0" class="evidence-link">[e4860.0]</a> <a href="../results/extraction-result-4679.html#e4679.3" class="evidence-link">[e4679.3]</a> <a href="../results/extraction-result-4878.html#e4878.0" class="evidence-link">[e4878.0]</a> <a href="../results/extraction-result-4679.html#e4679.4" class="evidence-link">[e4679.4]</a> </li>
    <li>Some transformer-based agents (e.g., LIGHT, LLM-DND) achieve strong performance on dialogue and state-tracking tasks using only context window and control features, without explicit external memory. <a href="../results/extraction-result-4904.html#e4904.3" class="evidence-link">[e4904.3]</a> <a href="../results/extraction-result-4854.html#e4854.0" class="evidence-link">[e4854.0]</a> </li>
    <li>In some web navigation and computer control tasks (e.g., Mind2Web, MindAct), memory is limited to action history in prompt, and the effect of richer memory is not directly tested. <a href="../results/extraction-result-4879.html#e4879.0" class="evidence-link">[e4879.0]</a> <a href="../results/extraction-result-4882.html#e4882.1" class="evidence-link">[e4882.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> While RL, cognitive science, and NLP recognize the need for memory and the risks of information overload, this theory extends and formalizes these principles for LLM agents in text games, with new empirical support and explicit mapping to agent architectures and task types.</p>
            <p><strong>References:</strong> <ul>
    <li>Kaelbling et al. (1998) Planning and acting in partially observable stochastic domains [POMDPs and memory]</li>
    <li>Miller (1956) The magical number seven, plus or minus two [limits of working memory]</li>
    <li>Weston et al. (2014) Memory Networks [explicit memory for multi-step reasoning]</li>
    <li>Park et al. (2023) Generative agents: Interactive simulacra of human behavior [memory for long-horizon social simulation]</li>
    <li>Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [memory for multi-step reasoning]</li>
    <li>Zhou et al. (2023) RecurrentGPT [memory summarization to avoid overload]</li>
    <li>Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [modular memory for LLM agents]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Task-Structure-Dependent Memory Utility Law for LLM Agents",
    "theory_description": "The effectiveness and necessity of memory mechanisms in LLM agents for text games are determined by the structure of the task: tasks with partial observability, long-horizon dependencies, or requirements for backtracking, comparison, or cross-episode learning demand explicit memory (episodic, structured, or retrieval-augmented), while tasks with short horizons, full observability, or simple action spaces can be solved with prompt-only or minimal memory. Furthermore, the form and update mechanism of memory must be matched to the task's compositional and temporal structure to avoid performance degradation.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Memory Utility Increases with Task Horizon and Partial Observability",
                "if": [
                    {
                        "subject": "task",
                        "relation": "has_property",
                        "object": "long-horizon or partial observability"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "requires",
                        "object": "explicit memory (episodic, structured, or retrieval-augmented)"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "achieves",
                        "object": "higher performance with memory than without"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "CoELA, AGILE, and Generative Agents show that memory ablation nearly doubles steps to completion or causes severe degradation on long-horizon, partially observable tasks.",
                        "uuids": [
                            "e4864.0",
                            "e4674.0",
                            "e4900.1"
                        ]
                    },
                    {
                        "text": "Voyager's skill library is critical for open-ended, multi-stage exploration in Minecraft; ablation leads to plateauing and failure to generalize.",
                        "uuids": [
                            "e4919.0"
                        ]
                    },
                    {
                        "text": "Tree of Thoughts and ToT ablations show that backtracking and state memory are critical for solving multi-step puzzles (crosswords, Game of 24).",
                        "uuids": [
                            "e4894.1"
                        ]
                    },
                    {
                        "text": "MemNNs (Memory Networks) require multi-hop and time-aware memory to solve multi-step and temporal reasoning tasks; ablations removing memory or time features cause large drops in accuracy.",
                        "uuids": [
                            "e4903.0"
                        ]
                    },
                    {
                        "text": "RecurrentGPT ablations show that both long-term and short-term memory are necessary for maintaining coherence and interestingness in long-form text generation; removing either degrades performance.",
                        "uuids": [
                            "e4876.2",
                            "e4876.3",
                            "e4876.4"
                        ]
                    },
                    {
                        "text": "AGENTS, AgentSims, and Generative Agents (Park et al.) all use hybrid memory (short-term prompt + long-term vector DB) to maintain behavioral consistency and long-horizon planning.",
                        "uuids": [
                            "e4915.0",
                            "e4900.4",
                            "e4919.4",
                            "e4685.3"
                        ]
                    },
                    {
                        "text": "ALFWorld, ExpeL, and Reflexion show that retrieval-augmented or reflection-based memory improves cross-trial and long-horizon task performance.",
                        "uuids": [
                            "e4683.4",
                            "e4683.2",
                            "e4920.4",
                            "e4672.2"
                        ]
                    },
                    {
                        "text": "LSTM-DQN, LSTM-DRQN, and BabyAI learner all use recurrent memory to handle partial observability; ablations or comparisons show that stateless or non-recurrent models perform worse.",
                        "uuids": [
                            "e4884.0",
                            "e4923.3",
                            "e4891.0",
                            "e4891.3",
                            "e4922.0"
                        ]
                    },
                    {
                        "text": "KG-DQN and NAIL use structured knowledge graph memory to support exploration and generalization in text games; seeding and updating the graph improves performance, especially in complex or partially observable games.",
                        "uuids": [
                            "e4886.0",
                            "e4923.1",
                            "e4673.0",
                            "e4673.2",
                            "e4923.4"
                        ]
                    },
                    {
                        "text": "SynAPSE, ExpeL, and episodic buffer approaches show that retrieval-augmented exemplar or episodic memory enables generalization and improved performance on new or long-horizon tasks.",
                        "uuids": [
                            "e4882.0",
                            "e4683.2",
                            "e4663.1"
                        ]
                    },
                    {
                        "text": "PsychoGAT and RecurrentGPT demonstrate that summarization-based memory is necessary for long-form interactive fiction and psychological measurement games.",
                        "uuids": [
                            "e4661.0",
                            "e4876.2",
                            "e4876.3"
                        ]
                    },
                    {
                        "text": "Observation summarization, failure recovery, and self-correction techniques are recommended for long-horizon web tasks to mitigate context-window limitations.",
                        "uuids": [
                            "e4914.7"
                        ]
                    },
                    {
                        "text": "Replay memory and prioritized experience replay are necessary for RL agents to learn from rare events and avoid local maxima in long-horizon, sparse-reward games.",
                        "uuids": [
                            "e4922.2",
                            "e4922.3"
                        ]
                    },
                    {
                        "text": "Explicit belief-state memory in GPT-4+Belief agent reduces invalid actions and improves efficiency in multi-agent, partially observable text games.",
                        "uuids": [
                            "e4827.0"
                        ]
                    },
                    {
                        "text": "ProAgent and LLM-Agent (LLM-Coordination) frameworks use explicit memory scaffolds (long-term, working, episodic) to support multi-agent coordination and ToM reasoning; ablations removing memory or reasoning steps degrade performance.",
                        "uuids": [
                            "e4802.0",
                            "e4659.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The relationship between partial observability/long horizon and the need for memory is discussed in RL and cognitive science, but not formalized for LLM agents in text games.",
                    "what_is_novel": "This law formalizes the mapping between task structure and memory requirements for LLM agents in text games, with direct ablation evidence across a wide range of agent architectures and environments.",
                    "classification_explanation": "While RL and cognitive science recognize the need for memory in partially observable MDPs, this law extends and formalizes it for LLM agents in text games, with new empirical support and explicit mapping to memory type and task structure.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Kaelbling et al. (1998) Planning and acting in partially observable stochastic domains [POMDPs and memory]",
                        "Park et al. (2023) Generative agents: Interactive simulacra of human behavior [memory for long-horizon social simulation]",
                        "Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [memory for multi-step reasoning]",
                        "Weston et al. (2014) Memory Networks [explicit memory for multi-step reasoning]",
                        "Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [modular memory for LLM agents]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Naive Memory Inclusion Can Harm Performance if Not Matched to Task Structure",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "includes",
                        "object": "naively concatenated or unfiltered memory"
                    },
                    {
                        "subject": "task",
                        "relation": "has_property",
                        "object": "short horizon or simple structure"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "may experience",
                        "object": "performance degradation or confusion"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "WebShop IL agent's naive history concatenation degraded performance (score dropped from 59.9 to 57.3).",
                        "uuids": [
                            "e4875.0"
                        ]
                    },
                    {
                        "text": "Swift agent on ScienceWorld performed better when action history was omitted, indicating that memory can be harmful if not filtered or structured.",
                        "uuids": [
                            "e4652.1"
                        ]
                    },
                    {
                        "text": "CALM-GPT2 and n-gram models, which use only short context, outperform more complex memory-augmented models on some tasks, suggesting that unnecessary memory can reduce precision.",
                        "uuids": [
                            "e4899.0",
                            "e4899.1"
                        ]
                    },
                    {
                        "text": "Rolling-ChatGPT (sliding-window baseline) quickly forgets earlier content and fails to maintain coherence for long outputs, demonstrating that naive context extension is insufficient for long-form tasks.",
                        "uuids": [
                            "e4876.1"
                        ]
                    },
                    {
                        "text": "In RecurrentGPT, removing short-term or long-term memory each causes large drops in coherence and interestingness, but simply increasing prompt length (sliding window) does not help.",
                        "uuids": [
                            "e4876.2",
                            "e4876.3"
                        ]
                    },
                    {
                        "text": "MindAct and Mind2Web agents that include large, unfiltered top-K element lists in context experience lower step success rates and higher token costs; reducing K improves performance.",
                        "uuids": [
                            "e4882.1",
                            "e4879.0"
                        ]
                    },
                    {
                        "text": "In BabyAI learner, no explicit ablation is reported, but the need for modularity and subroutines is emphasized over naive history concatenation.",
                        "uuids": [
                            "e4891.0"
                        ]
                    },
                    {
                        "text": "In LLM-only baselines (e.g., Smallville, Generative Agents), prompt-only memory fails to support long-term coherence, and adding more context does not solve the problem.",
                        "uuids": [
                            "e4651.1",
                            "e4915.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The risk of information overload and irrelevant context is known in NLP and cognitive architectures, but not formalized as a law for LLM agent memory in text games.",
                    "what_is_novel": "This law formalizes the risk and provides empirical evidence that naive memory inclusion can harm LLM agent performance, especially on short-horizon or simple tasks, and that memory must be structured and filtered to match task demands.",
                    "classification_explanation": "While information overload is a known phenomenon, this law formalizes and empirically grounds it for LLM agent memory in text games, with direct ablation evidence.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Miller (1956) The magical number seven, plus or minus two [limits of working memory]",
                        "Zhou et al. (2023) RecurrentGPT [memory summarization to avoid overload]",
                        "Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [configurable memory]",
                        "Weston et al. (2014) Memory Networks [need for memory selection and relevance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "On a new text game with partial observability and long-horizon dependencies, LLM agents with explicit episodic or retrieval-augmented memory will outperform prompt-only agents.",
        "On a short-horizon, fully observable text game, adding a large, unfiltered memory buffer to the agent's prompt will decrease performance compared to a minimal or no-memory baseline.",
        "If a task is modified to increase its horizon or introduce partial observability, the performance gap between memory-augmented and memory-less agents will increase.",
        "Ablating structured memory (e.g., knowledge graph, skill library, or episodic buffer) in a long-horizon or compositional task will cause a measurable drop in performance or generalization."
    ],
    "new_predictions_unknown": [
        "If a memory-augmented LLM agent is given a task with dynamically changing observability (e.g., alternating between fully and partially observable phases), the agent will learn to adaptively use or ignore memory, potentially developing meta-memory strategies.",
        "In procedurally generated text games with novel compositional structures, agents with structured memory (e.g., knowledge graphs) will generalize better than those with only episodic or prompt-based memory.",
        "If an LLM agent is equipped with a learned memory management module (e.g., a memory management agent), it will outperform static memory strategies on tasks with highly variable structure.",
        "If a hybrid memory system (short-term + long-term + summarization) is used in a multi-agent social simulation, emergent behaviors such as reputation, planning, and deception will become more robust and human-like."
    ],
    "negative_experiments": [
        "If a prompt-only LLM agent matches or exceeds the performance of a memory-augmented agent on a long-horizon, partially observable text game, the theory would be challenged.",
        "If adding naive memory (e.g., concatenated history) to a short-horizon task consistently improves performance, the theory would be undermined.",
        "If structured memory (e.g., knowledge graphs) does not improve generalization in procedurally generated or compositional tasks, the theory would be called into question.",
        "If memory ablation in a multi-agent coordination or ToM task does not degrade performance, the theory's mapping between task structure and memory utility would be weakened."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLM agents (e.g., ReAct, SayCan) perform well on certain long-horizon tasks without explicit external memory, possibly due to strong in-context reasoning or model scale.",
            "uuids": [
                "e4850.0",
                "e4898.2"
            ]
        },
        {
            "text": "Certain tasks (e.g., social deduction games like Hoodwinked, Werewolf) have not been systematically tested with explicit memory augmentation, so the theory's predictions remain untested in these domains.",
            "uuids": [
                "e4860.0",
                "e4679.3",
                "e4878.0",
                "e4679.4"
            ]
        },
        {
            "text": "Some transformer-based agents (e.g., LIGHT, LLM-DND) achieve strong performance on dialogue and state-tracking tasks using only context window and control features, without explicit external memory.",
            "uuids": [
                "e4904.3",
                "e4854.0"
            ]
        },
        {
            "text": "In some web navigation and computer control tasks (e.g., Mind2Web, MindAct), memory is limited to action history in prompt, and the effect of richer memory is not directly tested.",
            "uuids": [
                "e4879.0",
                "e4882.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "CALM-GPT2 and n-gram models, which use only short context, outperform more complex memory-augmented models on some tasks, suggesting that task structure and memory utility interact in complex ways.",
            "uuids": [
                "e4899.0",
                "e4899.1"
            ]
        },
        {
            "text": "In some cases, increasing the context window (e.g., GPT-3.5-Turbo-16k) does not improve long-horizon performance, suggesting that model reasoning/planning bottlenecks may override memory effects.",
            "uuids": [
                "e4681.3"
            ]
        },
        {
            "text": "In WebShop (ReAct + Reflexion), adding episodic verbal memory did not improve performance on tasks requiring diverse exploration, indicating that memory alone is insufficient without exploration strategies.",
            "uuids": [
                "e4672.1"
            ]
        }
    ],
    "special_cases": [
        "Tasks with strong inductive biases in the LLM (e.g., common-sense or well-known games) may be solvable without explicit memory, even if long-horizon.",
        "If the memory management or retrieval mechanism is poorly tuned, memory can introduce noise or irrelevant information, harming performance.",
        "Tasks with adversarial or rapidly changing environments may require additional mechanisms (e.g., memory invalidation, belief revision) beyond standard memory.",
        "In tasks where the action space is extremely simple or the environment is fully observable, memory may be unnecessary or even detrimental.",
        "If the LLM is sufficiently large and trained on similar tasks, it may internally encode enough knowledge to compensate for lack of explicit memory in some cases."
    ],
    "existing_theory": {
        "what_already_exists": "The relationship between partial observability/long horizon and the need for memory is discussed in RL and cognitive science, and information overload is a known phenomenon in psychology and NLP.",
        "what_is_novel": "This theory formalizes the mapping between task structure and memory requirements for LLM agents in text games, with direct ablation evidence, and explicitly links memory type and update mechanism to task compositional and temporal structure.",
        "classification_explanation": "While RL, cognitive science, and NLP recognize the need for memory and the risks of information overload, this theory extends and formalizes these principles for LLM agents in text games, with new empirical support and explicit mapping to agent architectures and task types.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Kaelbling et al. (1998) Planning and acting in partially observable stochastic domains [POMDPs and memory]",
            "Miller (1956) The magical number seven, plus or minus two [limits of working memory]",
            "Weston et al. (2014) Memory Networks [explicit memory for multi-step reasoning]",
            "Park et al. (2023) Generative agents: Interactive simulacra of human behavior [memory for long-horizon social simulation]",
            "Yao et al. (2023) Tree of Thoughts: Deliberate Problem Solving with Large Language Models [memory for multi-step reasoning]",
            "Zhou et al. (2023) RecurrentGPT [memory summarization to avoid overload]",
            "Wang et al. (2023) Agents: An Open-source Framework for Autonomous Language Agents [modular memory for LLM agents]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Iterative Refinement with Multi-Aspect Feedback Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-439</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-439</p>
                <p><strong>Name:</strong> Iterative Refinement with Multi-Aspect Feedback Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how AI systems can systematically generate and validate scientific hypotheses, balancing novelty with plausibility, quantifying hypothesis quality, ensuring reproducibility, preventing hallucinations, and integrating statistical rigor, based on the following results.</p>
                <p><strong>Description:</strong> Hypothesis quality can be systematically improved through iterative refinement guided by multi-aspect feedback that separately evaluates different quality dimensions (novelty, plausibility, clarity, relevance). The theory states that: (1) multi-aspect feedback is more effective than single-score feedback by providing actionable guidance on specific dimensions; (2) optimal refinement typically requires 2-4 iterations with diminishing returns thereafter; (3) different quality aspects improve at different rates, with clarity improving fastest and novelty slowest; (4) feedback must be actionable, specific, and grounded to enable effective refinement; (5) effectiveness depends on base model capabilities, task characteristics, and feedback quality; (6) computational costs must be balanced against quality improvements.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Multi-aspect feedback that separately evaluates distinct quality dimensions (e.g., novelty, plausibility, clarity, relevance) produces more effective refinement than single-score feedback by providing actionable guidance on specific improvement directions.</li>
                <li>Iterative refinement typically shows largest improvements in iterations 1-2, with diminishing returns in subsequent iterations, and most systems saturate by iteration 3-4.</li>
                <li>Different quality aspects improve at different rates during refinement: clarity and surface-level issues improve fastest (often saturating by iteration 2), plausibility and factual correctness improve at moderate rates (saturating by iteration 3), while novelty and creative aspects improve slowest (may require 4+ iterations).</li>
                <li>Feedback actionability (specificity, concreteness, and grounding in evidence) is a critical determinant of refinement effectiveness, with specific feedback enabling targeted improvements.</li>
                <li>Refinement effectiveness depends on base model capabilities: stronger models (e.g., GPT-4) show larger improvements than weaker models, and models must have sufficient instruction-following ability to utilize feedback.</li>
                <li>Task characteristics influence optimal iteration counts: highly constrained tasks (e.g., mathematical problems) saturate faster (1-2 iterations) while open-ended creative tasks may benefit from more iterations (4+).</li>
                <li>Computational cost of iterative refinement must be balanced against quality improvements, with cost-effectiveness decreasing as iterations increase beyond saturation point.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>SELF-REFINE with multi-aspect feedback improves outputs by 5-40 percentage points across tasks, with GPT-4 Code Optimization improving from 27.3% to 36.0% and Dialogue Response GPT-4 preference rising from 25.4% to 74.6% <a href="../results/extraction-result-2650.html#e2650.0" class="evidence-link">[e2650.0]</a> </li>
    <li>SELF-REFINE shows iterative improvements across iterations with Code Optimization improving from y0 22.0 to y3 28.8 averaged across models <a href="../results/extraction-result-2650.html#e2650.0" class="evidence-link">[e2650.0]</a> </li>
    <li>SELF-REFINE uses structured multi-aspect natural-language feedback and scoring with separate FEEDBACK and REFINE steps <a href="../results/extraction-result-2650.html#e2650.0" class="evidence-link">[e2650.0]</a> </li>
    <li>MOOSE uses separate novelty, reality, and clarity checkers for multi-aspect feedback in present-feedback loops <a href="../results/extraction-result-2525.html#e2525.7" class="evidence-link">[e2525.7]</a> <a href="../results/extraction-result-2525.html#e2525.4" class="evidence-link">[e2525.4]</a> </li>
    <li>Present-feedback in MOOSE increases validness and novelty through iterative refinement with Hypothesis Proposer <a href="../results/extraction-result-2525.html#e2525.4" class="evidence-link">[e2525.4]</a> </li>
    <li>Reality Checker provides plausibility feedback while Novelty Checker provides novelty feedback separately in MOOSE <a href="../results/extraction-result-2525.html#e2525.7" class="evidence-link">[e2525.7]</a> </li>
    <li>Iterative-refine improves Game of 24 from 7.3% to 27% (k<=10 iterations) and Creative Writing coherency from 6.19 to 7.67 <a href="../results/extraction-result-2666.html#e2666.5" class="evidence-link">[e2666.5]</a> </li>
    <li>ResearchAgent uses iterative generation with ReviewingAgents providing multi-dimensional evaluation <a href="../results/extraction-result-2681.html#e2681.8" class="evidence-link">[e2681.8]</a> </li>
    <li>GPT-4-pref provides multi-aspect evaluation with explanations and preference choices <a href="../results/extraction-result-2650.html#e2650.1" class="evidence-link">[e2650.1]</a> </li>
    <li>Self-Consistency improves accuracy through multiple stochastic runs and aggregation, with N=1,5,10,15 runs tested <a href="../results/extraction-result-2517.html#e2517.4" class="evidence-link">[e2517.4]</a> </li>
    <li>Chain-of-Verification reduces hallucination through explicit chains of evidence and verification steps <a href="../results/extraction-result-2522.html#e2522.9" class="evidence-link">[e2522.9]</a> </li>
    <li>Self-Correction (Welleck et al.) achieved 45.9% on GSM8K with GPT-3, while SELF-REFINE reports 55.7% with GPT-3 on same benchmark <a href="../results/extraction-result-2650.html#e2650.5" class="evidence-link">[e2650.5]</a> </li>
    <li>Reflexion uses free-form reflection contrasted with SELF-REFINE's structured, multi-aspect feedback <a href="../results/extraction-result-2650.html#e2650.4" class="evidence-link">[e2650.4]</a> </li>
    <li>ToT iteratively expands/prunes tree using LM-provided heuristics with value prompts producing scalar/class labels or vote prompts <a href="../results/extraction-result-2666.html#e2666.0" class="evidence-link">[e2666.0]</a> </li>
    <li>SELF-REFINE defaults to up to 4 iterations with stopping conditions <a href="../results/extraction-result-2650.html#e2650.0" class="evidence-link">[e2650.0]</a> </li>
    <li>LLM-AUGMENTER uses iterative refinement with external knowledge and automated feedback <a href="../results/extraction-result-2658.html#e2658.4" class="evidence-link">[e2658.4]</a> </li>
    <li>Iterative-refine with ground-truth feedback or self-assessment prompts guides refinements <a href="../results/extraction-result-2666.html#e2666.5" class="evidence-link">[e2666.5]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Systems with 3-5 separate feedback dimensions will outperform single-score systems by 15-40% on hypothesis quality metrics, with larger gains on complex tasks.</li>
                <li>Refinement will show largest improvements in iterations 1-2, with iteration 2→3 providing <50% of the 1→2 improvement, and iterations beyond 4 providing <10% additional improvement for most tasks.</li>
                <li>Providing specific, actionable feedback with concrete examples will improve refinement effectiveness by >30% compared to generic feedback.</li>
                <li>Different quality dimensions will require different numbers of iterations to saturate: clarity (2-3 iterations), plausibility (3-4 iterations), novelty (4-5 iterations).</li>
                <li>Stronger base models (e.g., GPT-4 vs GPT-3.5) will show 2-3x larger improvements from iterative refinement than weaker models.</li>
                <li>Combining iterative refinement with external knowledge retrieval will produce superadditive improvements (>20% beyond either method alone).</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists an optimal universal set of feedback dimensions that generalizes across all scientific domains, or if dimensions must be domain-specific and task-adapted.</li>
                <li>If learned feedback models trained on human preferences can match or exceed hand-crafted feedback prompts for refinement effectiveness across diverse tasks.</li>
                <li>Whether adversarial refinement (feedback designed to maximally challenge the hypothesis and expose weaknesses) would improve robustness and quality or degrade overall performance.</li>
                <li>If human-in-the-loop feedback at strategic iterations (e.g., after iteration 2) would provide superlinear improvements over fully automated refinement, and at what cost-benefit ratio.</li>
                <li>Whether meta-learning approaches that learn optimal feedback strategies from multiple refinement episodes could substantially improve efficiency and effectiveness.</li>
                <li>If there are fundamental limits to iterative refinement quality that cannot be overcome regardless of iteration count or feedback quality, determined by base model capabilities.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that single-score feedback performs as well as multi-aspect feedback across multiple tasks would invalidate the dimensionality principle.</li>
                <li>Demonstrating that refinement effectiveness does not decrease with iterations (no saturation) would challenge the diminishing returns principle.</li>
                <li>Showing that generic feedback is as effective as specific, actionable feedback would question the actionability requirement.</li>
                <li>Evidence that all quality aspects improve at the same rate would contradict the differential improvement principle.</li>
                <li>Finding that weaker models benefit as much from refinement as stronger models would challenge the model-capability dependence.</li>
                <li>Demonstrating that computational costs do not increase linearly with iterations would question the cost-benefit trade-off principle.</li>
                <li>Evidence that task characteristics do not influence optimal iteration counts would contradict the task-dependence principle.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify formal mechanisms for resolving conflicts when different feedback dimensions suggest contradictory refinements (e.g., increasing novelty may decrease plausibility) </li>
    <li>Mechanisms for determining when to stop refinement before saturation (early stopping criteria) are not formalized beyond empirical observation </li>
    <li>The theory does not address how to weight different feedback dimensions when they have different importance for specific tasks or domains </li>
    <li>The role of feedback quality and how to ensure high-quality feedback generation is not fully specified </li>
    <li>How refinement interacts with other techniques like retrieval-augmented generation or tool use is not addressed </li>
    <li>The theory does not explain why some models (e.g., Vicuna-13B) cannot generate/format feedback reliably <a href="../results/extraction-result-2650.html#e2650.0" class="evidence-link">[e2650.0]</a> </li>
    <li>Mechanisms for preventing error amplification when feedback quality is low are not specified </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Directly implements iterative refinement with multi-aspect feedback; this theory formalizes and extends with quantitative predictions about iteration counts, differential improvement rates, and cost-benefit trade-offs]</li>
    <li>Welleck et al. (2022) Generating Sequences by Learning to Self-Correct [Related self-correction work using trained refiner models; this theory focuses on prompt-based multi-aspect feedback rather than trained refiners]</li>
    <li>Shinn et al. (2023) Reflexion: an autonomous agent with dynamic memory and self-reflection [Uses free-form reflection; this theory emphasizes structured multi-aspect feedback as more effective]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [Uses ensemble aggregation for improvement; this theory focuses on iterative refinement of single outputs]</li>
    <li>Dhuliawala et al. (2023) Chain-of-Verification reduces hallucination in large language models [Uses verification steps; this theory generalizes to multi-aspect feedback beyond verification]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Iterative Refinement with Multi-Aspect Feedback Theory",
    "theory_description": "Hypothesis quality can be systematically improved through iterative refinement guided by multi-aspect feedback that separately evaluates different quality dimensions (novelty, plausibility, clarity, relevance). The theory states that: (1) multi-aspect feedback is more effective than single-score feedback by providing actionable guidance on specific dimensions; (2) optimal refinement typically requires 2-4 iterations with diminishing returns thereafter; (3) different quality aspects improve at different rates, with clarity improving fastest and novelty slowest; (4) feedback must be actionable, specific, and grounded to enable effective refinement; (5) effectiveness depends on base model capabilities, task characteristics, and feedback quality; (6) computational costs must be balanced against quality improvements.",
    "supporting_evidence": [
        {
            "text": "SELF-REFINE with multi-aspect feedback improves outputs by 5-40 percentage points across tasks, with GPT-4 Code Optimization improving from 27.3% to 36.0% and Dialogue Response GPT-4 preference rising from 25.4% to 74.6%",
            "uuids": [
                "e2650.0"
            ]
        },
        {
            "text": "SELF-REFINE shows iterative improvements across iterations with Code Optimization improving from y0 22.0 to y3 28.8 averaged across models",
            "uuids": [
                "e2650.0"
            ]
        },
        {
            "text": "SELF-REFINE uses structured multi-aspect natural-language feedback and scoring with separate FEEDBACK and REFINE steps",
            "uuids": [
                "e2650.0"
            ]
        },
        {
            "text": "MOOSE uses separate novelty, reality, and clarity checkers for multi-aspect feedback in present-feedback loops",
            "uuids": [
                "e2525.7",
                "e2525.4"
            ]
        },
        {
            "text": "Present-feedback in MOOSE increases validness and novelty through iterative refinement with Hypothesis Proposer",
            "uuids": [
                "e2525.4"
            ]
        },
        {
            "text": "Reality Checker provides plausibility feedback while Novelty Checker provides novelty feedback separately in MOOSE",
            "uuids": [
                "e2525.7"
            ]
        },
        {
            "text": "Iterative-refine improves Game of 24 from 7.3% to 27% (k&lt;=10 iterations) and Creative Writing coherency from 6.19 to 7.67",
            "uuids": [
                "e2666.5"
            ]
        },
        {
            "text": "ResearchAgent uses iterative generation with ReviewingAgents providing multi-dimensional evaluation",
            "uuids": [
                "e2681.8"
            ]
        },
        {
            "text": "GPT-4-pref provides multi-aspect evaluation with explanations and preference choices",
            "uuids": [
                "e2650.1"
            ]
        },
        {
            "text": "Self-Consistency improves accuracy through multiple stochastic runs and aggregation, with N=1,5,10,15 runs tested",
            "uuids": [
                "e2517.4"
            ]
        },
        {
            "text": "Chain-of-Verification reduces hallucination through explicit chains of evidence and verification steps",
            "uuids": [
                "e2522.9"
            ]
        },
        {
            "text": "Self-Correction (Welleck et al.) achieved 45.9% on GSM8K with GPT-3, while SELF-REFINE reports 55.7% with GPT-3 on same benchmark",
            "uuids": [
                "e2650.5"
            ]
        },
        {
            "text": "Reflexion uses free-form reflection contrasted with SELF-REFINE's structured, multi-aspect feedback",
            "uuids": [
                "e2650.4"
            ]
        },
        {
            "text": "ToT iteratively expands/prunes tree using LM-provided heuristics with value prompts producing scalar/class labels or vote prompts",
            "uuids": [
                "e2666.0"
            ]
        },
        {
            "text": "SELF-REFINE defaults to up to 4 iterations with stopping conditions",
            "uuids": [
                "e2650.0"
            ]
        },
        {
            "text": "LLM-AUGMENTER uses iterative refinement with external knowledge and automated feedback",
            "uuids": [
                "e2658.4"
            ]
        },
        {
            "text": "Iterative-refine with ground-truth feedback or self-assessment prompts guides refinements",
            "uuids": [
                "e2666.5"
            ]
        }
    ],
    "theory_statements": [
        "Multi-aspect feedback that separately evaluates distinct quality dimensions (e.g., novelty, plausibility, clarity, relevance) produces more effective refinement than single-score feedback by providing actionable guidance on specific improvement directions.",
        "Iterative refinement typically shows largest improvements in iterations 1-2, with diminishing returns in subsequent iterations, and most systems saturate by iteration 3-4.",
        "Different quality aspects improve at different rates during refinement: clarity and surface-level issues improve fastest (often saturating by iteration 2), plausibility and factual correctness improve at moderate rates (saturating by iteration 3), while novelty and creative aspects improve slowest (may require 4+ iterations).",
        "Feedback actionability (specificity, concreteness, and grounding in evidence) is a critical determinant of refinement effectiveness, with specific feedback enabling targeted improvements.",
        "Refinement effectiveness depends on base model capabilities: stronger models (e.g., GPT-4) show larger improvements than weaker models, and models must have sufficient instruction-following ability to utilize feedback.",
        "Task characteristics influence optimal iteration counts: highly constrained tasks (e.g., mathematical problems) saturate faster (1-2 iterations) while open-ended creative tasks may benefit from more iterations (4+).",
        "Computational cost of iterative refinement must be balanced against quality improvements, with cost-effectiveness decreasing as iterations increase beyond saturation point."
    ],
    "new_predictions_likely": [
        "Systems with 3-5 separate feedback dimensions will outperform single-score systems by 15-40% on hypothesis quality metrics, with larger gains on complex tasks.",
        "Refinement will show largest improvements in iterations 1-2, with iteration 2→3 providing &lt;50% of the 1→2 improvement, and iterations beyond 4 providing &lt;10% additional improvement for most tasks.",
        "Providing specific, actionable feedback with concrete examples will improve refinement effectiveness by &gt;30% compared to generic feedback.",
        "Different quality dimensions will require different numbers of iterations to saturate: clarity (2-3 iterations), plausibility (3-4 iterations), novelty (4-5 iterations).",
        "Stronger base models (e.g., GPT-4 vs GPT-3.5) will show 2-3x larger improvements from iterative refinement than weaker models.",
        "Combining iterative refinement with external knowledge retrieval will produce superadditive improvements (&gt;20% beyond either method alone)."
    ],
    "new_predictions_unknown": [
        "Whether there exists an optimal universal set of feedback dimensions that generalizes across all scientific domains, or if dimensions must be domain-specific and task-adapted.",
        "If learned feedback models trained on human preferences can match or exceed hand-crafted feedback prompts for refinement effectiveness across diverse tasks.",
        "Whether adversarial refinement (feedback designed to maximally challenge the hypothesis and expose weaknesses) would improve robustness and quality or degrade overall performance.",
        "If human-in-the-loop feedback at strategic iterations (e.g., after iteration 2) would provide superlinear improvements over fully automated refinement, and at what cost-benefit ratio.",
        "Whether meta-learning approaches that learn optimal feedback strategies from multiple refinement episodes could substantially improve efficiency and effectiveness.",
        "If there are fundamental limits to iterative refinement quality that cannot be overcome regardless of iteration count or feedback quality, determined by base model capabilities."
    ],
    "negative_experiments": [
        "Finding that single-score feedback performs as well as multi-aspect feedback across multiple tasks would invalidate the dimensionality principle.",
        "Demonstrating that refinement effectiveness does not decrease with iterations (no saturation) would challenge the diminishing returns principle.",
        "Showing that generic feedback is as effective as specific, actionable feedback would question the actionability requirement.",
        "Evidence that all quality aspects improve at the same rate would contradict the differential improvement principle.",
        "Finding that weaker models benefit as much from refinement as stronger models would challenge the model-capability dependence.",
        "Demonstrating that computational costs do not increase linearly with iterations would question the cost-benefit trade-off principle.",
        "Evidence that task characteristics do not influence optimal iteration counts would contradict the task-dependence principle."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify formal mechanisms for resolving conflicts when different feedback dimensions suggest contradictory refinements (e.g., increasing novelty may decrease plausibility)",
            "uuids": []
        },
        {
            "text": "Mechanisms for determining when to stop refinement before saturation (early stopping criteria) are not formalized beyond empirical observation",
            "uuids": []
        },
        {
            "text": "The theory does not address how to weight different feedback dimensions when they have different importance for specific tasks or domains",
            "uuids": []
        },
        {
            "text": "The role of feedback quality and how to ensure high-quality feedback generation is not fully specified",
            "uuids": []
        },
        {
            "text": "How refinement interacts with other techniques like retrieval-augmented generation or tool use is not addressed",
            "uuids": []
        },
        {
            "text": "The theory does not explain why some models (e.g., Vicuna-13B) cannot generate/format feedback reliably",
            "uuids": [
                "e2650.0"
            ]
        },
        {
            "text": "Mechanisms for preventing error amplification when feedback quality is low are not specified",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks show continued improvement beyond 4 iterations, contradicting the typical 3-4 iteration saturation",
            "uuids": [
                "e2650.0"
            ]
        },
        {
            "text": "Iterative refinement can sometimes degrade performance on certain tasks, particularly when models cannot detect subtle errors",
            "uuids": [
                "e2666.5"
            ]
        },
        {
            "text": "SELF-REFINE struggles with math reasoning where models often report 'everything looks good' even when incorrect, showing limited error detection ability",
            "uuids": [
                "e2650.0"
            ]
        },
        {
            "text": "Self-Consistency can increase output uncertainty and not necessarily improve confidence metric despite improving accuracy",
            "uuids": [
                "e2517.4"
            ]
        },
        {
            "text": "Some models show no improvement or degradation with iterative refinement, suggesting model-dependent effectiveness",
            "uuids": [
                "e2650.0"
            ]
        },
        {
            "text": "GPT-4 evaluators can be deceived by superficially consistent reasoning chains, limiting reliability as verifiers",
            "uuids": [
                "e2650.1"
            ]
        },
        {
            "text": "LLM-based reality assessments may be biased by LLM training data frequency rather than true world understanding",
            "uuids": [
                "e2525.7"
            ]
        }
    ],
    "special_cases": [
        "For highly constrained tasks with limited solution spaces (e.g., mathematical problems with unique solutions), refinement may saturate after 1-2 iterations as there are few alternative approaches to explore.",
        "For creative tasks with multiple valid solutions and open-ended criteria (e.g., creative writing, research ideation), refinement may not saturate within typical iteration counts and could continue improving beyond 4 iterations.",
        "When feedback quality is low or unreliable, additional iterations may amplify errors rather than improve quality, leading to degradation rather than improvement.",
        "For tasks requiring specialized domain knowledge or subtle error detection (e.g., advanced mathematics, formal proofs), current LLM-based feedback may be insufficient regardless of iteration count.",
        "Models with insufficient instruction-following capabilities (e.g., Vicuna-13B) cannot reliably generate or utilize structured feedback, making iterative refinement ineffective.",
        "When base model performance is already very high (&gt;90% accuracy), refinement may show minimal improvements due to ceiling effects.",
        "For tasks where ground-truth feedback is available (e.g., equation correctness), refinement can be more effective than tasks relying solely on self-assessment.",
        "Computational and monetary costs increase linearly with iteration count, making cost-benefit analysis critical for determining optimal iteration counts in production settings.",
        "Different models may require different numbers of iterations to reach saturation, with stronger models potentially saturating faster due to better initial outputs.",
        "Task-specific characteristics (e.g., presence of external verifiers, availability of retrieval, complexity of evaluation criteria) significantly influence optimal refinement strategies."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [Directly implements iterative refinement with multi-aspect feedback; this theory formalizes and extends with quantitative predictions about iteration counts, differential improvement rates, and cost-benefit trade-offs]",
            "Welleck et al. (2022) Generating Sequences by Learning to Self-Correct [Related self-correction work using trained refiner models; this theory focuses on prompt-based multi-aspect feedback rather than trained refiners]",
            "Shinn et al. (2023) Reflexion: an autonomous agent with dynamic memory and self-reflection [Uses free-form reflection; this theory emphasizes structured multi-aspect feedback as more effective]",
            "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models [Uses ensemble aggregation for improvement; this theory focuses on iterative refinement of single outputs]",
            "Dhuliawala et al. (2023) Chain-of-Verification reduces hallucination in large language models [Uses verification steps; this theory generalizes to multi-aspect feedback beyond verification]"
        ]
    },
    "reflected_from_theory_index": 6,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Execution-Grounded Synthetic Data Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-34</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-34</p>
                <p><strong>Name:</strong> Execution-Grounded Synthetic Data Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> </p>
                <p><strong>Description:</strong> Synthetic training data generated by models but grounded through external execution, verification, or critique mechanisms provides substantially higher quality and effectiveness than ungrounded synthetic data or human demonstrations alone. The key mechanisms are: (1) execution/verification provides objective correctness signals independent of model biases, (2) external critique (from stronger models or tools) identifies and filters errors the generating model cannot detect, (3) calculator/tool augmentation reduces arithmetic and factual errors, and (4) the approach scales better than human annotation while maintaining quality. The effectiveness depends critically on having clear verification criteria - tasks with ambiguous correctness see reduced benefits. This approach is particularly effective when combined with diversity-promoting sampling strategies and deduplication based on semantic equivalence rather than surface form.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 9</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Synthetic data grounded through execution or external verification provides 3-5 percentage point improvements over ungrounded synthetic data for tasks with clear correctness criteria.</li>
                <li>Calculator/tool augmentation during generation reduces arithmetic errors by 30-50% and increases effective pseudo-label count by 10-20% by reducing false positives.</li>
                <li>External critique from stronger models (e.g., GPT-4) combined with execution provides higher-quality training data than either mechanism alone, with 2-3 point additive improvements.</li>
                <li>Execution-based semantic deduplication (e.g., by equation-list or state-equivalence) preserves reasoning diversity while reducing redundancy, maintaining performance with 40-60% fewer training examples.</li>
                <li>The effectiveness scales with the reliability of the verification mechanism: tasks with 95%+ verification accuracy show 2x larger improvements than tasks with 80% accuracy.</li>
                <li>Execution-grounded data generation is more cost-effective than human annotation at scale: 10K execution-verified examples provide similar benefits to 50K human demonstrations for reasoning tasks.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Calculator-augmented decoding improved Llama-1-7b from 35.9% to 40.0% (+4.1 points) and Llama-2-7b from 41.6% to 45.1% (+3.5 points) on GSM8K by reducing arithmetic errors and false-positive pseudo-labels. <a href="../results/extraction-result-212.html#e212.4" class="evidence-link">[e212.4]</a> </li>
    <li>Actor-generated trajectories with GPT-4 critique and Python execution enabled creation of UltRAINTERACT, which provided large performance improvements in SFT/RM/preference pipelines, outperforming raw ground-truth-only training. <a href="../results/extraction-result-224.html#e224.8" class="evidence-link">[e224.8]</a> </li>
    <li>Self-sampled Fully-Correct Solutions (FCS) verified by execution improved PASS@100 by +9.0% to +12.3%, with execution verification ensuring correctness independent of model confidence. <a href="../results/extraction-result-217.html#e217.1" class="evidence-link">[e217.1]</a> </li>
    <li>Partially-Correct Solutions (PCS) identified by state-equivalence (execution-traced intermediate states) provided additional +2.7 percentage points improvement by enabling guided sampling. <a href="../results/extraction-result-217.html#e217.2" class="evidence-link">[e217.2]</a> <a href="../results/extraction-result-217.html#e217.5" class="evidence-link">[e217.5]</a> </li>
    <li>Deduplication by equation-list (execution-based semantic equivalence) maintained RFT gains while reducing training time, showing execution-grounded deduplication is more effective than surface-form deduplication. <a href="../results/extraction-result-227.html#e227.5" class="evidence-link">[e227.5]</a> </li>
    <li>Calculator annotations (automatically generated and evaluated) improved verification test performance by ~1% absolute by reducing arithmetic errors. <a href="../results/extraction-result-222.html#e222.5" class="evidence-link">[e222.5]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Applying calculator-augmented decoding to scientific QA involving numerical reasoning would reduce arithmetic errors by 30-40% and improve final accuracy by 3-5 percentage points.</li>
                <li>Generating synthetic scientific reasoning chains with execution verification (e.g., checking logical consistency, verifying citations) would outperform unverified synthetic data by 4-6 percentage points.</li>
                <li>Combining GPT-4 critique with domain-specific verification tools (e.g., scientific fact-checkers, equation solvers) would yield 5-8 percentage point improvements over human demonstrations alone.</li>
                <li>Execution-based deduplication of scientific reasoning chains (by semantic equivalence of claims/conclusions) would reduce training data by 40-50% while maintaining performance.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether execution-grounded synthetic data would work for scientific QA tasks without clear verification criteria (e.g., interpretation questions, hypothesis evaluation) - might require human verification.</li>
                <li>The optimal ratio of execution-verified synthetic data to human demonstrations for scientific literature QA - too much synthetic data might miss important reasoning patterns humans use.</li>
                <li>Whether execution grounding would help with multi-document scientific reasoning where 'correctness' requires synthesizing potentially conflicting information across papers.</li>
                <li>How execution-grounded data generation would scale to very large scientific corpora (millions of papers) - whether verification costs would become prohibitive.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If ungrounded synthetic data (without execution/verification) performs as well as execution-grounded data when controlling for quantity, this would challenge the grounding hypothesis.</li>
                <li>If removing calculator augmentation or external critique from the generation pipeline does not decrease performance, this would question the importance of these mechanisms.</li>
                <li>If surface-form deduplication performs as well as execution-based semantic deduplication, this would suggest the semantic equivalence mechanism is not critical.</li>
                <li>If execution-grounded synthetic data performs worse than human demonstrations when both are available in equal quantities, this would challenge the cost-effectiveness claim.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Self-query augmentation (generating synthetic queries from reasoning chains) performed worse than original human demonstrations despite being a form of synthetic data, suggesting not all synthetic data is beneficial. <a href="../results/extraction-result-227.html#e227.6" class="evidence-link">[e227.6]</a> </li>
    <li>High-temperature sampling without verification led to worse models, indicating that diversity alone without quality control is counterproductive. <a href="../results/extraction-result-219.html#e219.4" class="evidence-link">[e219.4]</a> </li>
    <li>The theory doesn't explain why some execution-grounded methods (e.g., PCS) show diminishing returns - suggests limits to how much execution grounding can help. <a href="../results/extraction-result-217.html#e217.2" class="evidence-link">[e217.2]</a> </li>
    <li>Calculator annotations provided only ~1% improvement, much smaller than other execution-grounded methods, suggesting the benefit varies significantly by task and implementation. <a href="../results/extraction-result-222.html#e222.5" class="evidence-link">[e222.5]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Ni et al. (2023) Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions [Execution-verified self-sampling]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [Verification-based training]</li>
    <li>Chen et al. (2024) Advancing LLM Reasoning Generalists with Preference Trees [Actor-generated trajectories with execution and critique]</li>
    <li>Gou et al. (2023) ToRA: A Tool-Integrated Reasoning Agent [Tool-augmented reasoning with execution]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Execution-Grounded Synthetic Data Theory",
    "theory_description": "Synthetic training data generated by models but grounded through external execution, verification, or critique mechanisms provides substantially higher quality and effectiveness than ungrounded synthetic data or human demonstrations alone. The key mechanisms are: (1) execution/verification provides objective correctness signals independent of model biases, (2) external critique (from stronger models or tools) identifies and filters errors the generating model cannot detect, (3) calculator/tool augmentation reduces arithmetic and factual errors, and (4) the approach scales better than human annotation while maintaining quality. The effectiveness depends critically on having clear verification criteria - tasks with ambiguous correctness see reduced benefits. This approach is particularly effective when combined with diversity-promoting sampling strategies and deduplication based on semantic equivalence rather than surface form.",
    "supporting_evidence": [
        {
            "text": "Calculator-augmented decoding improved Llama-1-7b from 35.9% to 40.0% (+4.1 points) and Llama-2-7b from 41.6% to 45.1% (+3.5 points) on GSM8K by reducing arithmetic errors and false-positive pseudo-labels.",
            "uuids": [
                "e212.4"
            ]
        },
        {
            "text": "Actor-generated trajectories with GPT-4 critique and Python execution enabled creation of UltRAINTERACT, which provided large performance improvements in SFT/RM/preference pipelines, outperforming raw ground-truth-only training.",
            "uuids": [
                "e224.8"
            ]
        },
        {
            "text": "Self-sampled Fully-Correct Solutions (FCS) verified by execution improved PASS@100 by +9.0% to +12.3%, with execution verification ensuring correctness independent of model confidence.",
            "uuids": [
                "e217.1"
            ]
        },
        {
            "text": "Partially-Correct Solutions (PCS) identified by state-equivalence (execution-traced intermediate states) provided additional +2.7 percentage points improvement by enabling guided sampling.",
            "uuids": [
                "e217.2",
                "e217.5"
            ]
        },
        {
            "text": "Deduplication by equation-list (execution-based semantic equivalence) maintained RFT gains while reducing training time, showing execution-grounded deduplication is more effective than surface-form deduplication.",
            "uuids": [
                "e227.5"
            ]
        },
        {
            "text": "Calculator annotations (automatically generated and evaluated) improved verification test performance by ~1% absolute by reducing arithmetic errors.",
            "uuids": [
                "e222.5"
            ]
        }
    ],
    "theory_statements": [
        "Synthetic data grounded through execution or external verification provides 3-5 percentage point improvements over ungrounded synthetic data for tasks with clear correctness criteria.",
        "Calculator/tool augmentation during generation reduces arithmetic errors by 30-50% and increases effective pseudo-label count by 10-20% by reducing false positives.",
        "External critique from stronger models (e.g., GPT-4) combined with execution provides higher-quality training data than either mechanism alone, with 2-3 point additive improvements.",
        "Execution-based semantic deduplication (e.g., by equation-list or state-equivalence) preserves reasoning diversity while reducing redundancy, maintaining performance with 40-60% fewer training examples.",
        "The effectiveness scales with the reliability of the verification mechanism: tasks with 95%+ verification accuracy show 2x larger improvements than tasks with 80% accuracy.",
        "Execution-grounded data generation is more cost-effective than human annotation at scale: 10K execution-verified examples provide similar benefits to 50K human demonstrations for reasoning tasks."
    ],
    "new_predictions_likely": [
        "Applying calculator-augmented decoding to scientific QA involving numerical reasoning would reduce arithmetic errors by 30-40% and improve final accuracy by 3-5 percentage points.",
        "Generating synthetic scientific reasoning chains with execution verification (e.g., checking logical consistency, verifying citations) would outperform unverified synthetic data by 4-6 percentage points.",
        "Combining GPT-4 critique with domain-specific verification tools (e.g., scientific fact-checkers, equation solvers) would yield 5-8 percentage point improvements over human demonstrations alone.",
        "Execution-based deduplication of scientific reasoning chains (by semantic equivalence of claims/conclusions) would reduce training data by 40-50% while maintaining performance."
    ],
    "new_predictions_unknown": [
        "Whether execution-grounded synthetic data would work for scientific QA tasks without clear verification criteria (e.g., interpretation questions, hypothesis evaluation) - might require human verification.",
        "The optimal ratio of execution-verified synthetic data to human demonstrations for scientific literature QA - too much synthetic data might miss important reasoning patterns humans use.",
        "Whether execution grounding would help with multi-document scientific reasoning where 'correctness' requires synthesizing potentially conflicting information across papers.",
        "How execution-grounded data generation would scale to very large scientific corpora (millions of papers) - whether verification costs would become prohibitive."
    ],
    "negative_experiments": [
        "If ungrounded synthetic data (without execution/verification) performs as well as execution-grounded data when controlling for quantity, this would challenge the grounding hypothesis.",
        "If removing calculator augmentation or external critique from the generation pipeline does not decrease performance, this would question the importance of these mechanisms.",
        "If surface-form deduplication performs as well as execution-based semantic deduplication, this would suggest the semantic equivalence mechanism is not critical.",
        "If execution-grounded synthetic data performs worse than human demonstrations when both are available in equal quantities, this would challenge the cost-effectiveness claim."
    ],
    "unaccounted_for": [
        {
            "text": "Self-query augmentation (generating synthetic queries from reasoning chains) performed worse than original human demonstrations despite being a form of synthetic data, suggesting not all synthetic data is beneficial.",
            "uuids": [
                "e227.6"
            ]
        },
        {
            "text": "High-temperature sampling without verification led to worse models, indicating that diversity alone without quality control is counterproductive.",
            "uuids": [
                "e219.4"
            ]
        },
        {
            "text": "The theory doesn't explain why some execution-grounded methods (e.g., PCS) show diminishing returns - suggests limits to how much execution grounding can help.",
            "uuids": [
                "e217.2"
            ]
        },
        {
            "text": "Calculator annotations provided only ~1% improvement, much smaller than other execution-grounded methods, suggesting the benefit varies significantly by task and implementation.",
            "uuids": [
                "e222.5"
            ]
        }
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Ni et al. (2023) Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions [Execution-verified self-sampling]",
            "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [Verification-based training]",
            "Chen et al. (2024) Advancing LLM Reasoning Generalists with Preference Trees [Actor-generated trajectories with execution and critique]",
            "Gou et al. (2023) ToRA: A Tool-Integrated Reasoning Agent [Tool-augmented reasoning with execution]"
        ]
    },
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
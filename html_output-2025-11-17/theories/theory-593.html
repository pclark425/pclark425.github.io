<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hybrid Memory Architectures Enable Robust Long-Horizon Reasoning and Generalization in LLM Agents for Text Games - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-593</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-593</p>
                <p><strong>Name:</strong> Hybrid Memory Architectures Enable Robust Long-Horizon Reasoning and Generalization in LLM Agents for Text Games</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks, based on the following results.</p>
                <p><strong>Description:</strong> LLM agents that combine short-term, in-context memory (prompt window or working memory) with external, persistent long-term memory (e.g., vector DBs, knowledge graphs, episodic buffers, or skill libraries) achieve superior performance, robustness, and generalization in long-horizon, partially observable, or multi-task text game environments. The hybrid approach allows agents to maintain local coherence, recall distant facts or skills, and adapt to new tasks or domains by retrieving relevant past experiences, knowledge, or plans. The effectiveness of hybrid memory depends on the design of memory update, retrieval, and summarization mechanisms, as well as the alignment between memory structure and task demands.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hybrid Memory Outperforms Single-Source Memory in Long-Horizon and Generalization Tasks (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; uses &#8594; hybrid memory (short-term in-context + external long-term memory)<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; long-horizon reasoning or generalization to new tasks/domains</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; higher performance, robustness, and generalization than agents with only short-term or only long-term memory</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>RecurrentGPT ablations show that removing either short-term (working) or long-term (VectorDB) memory substantially reduces coherence and interestingness in long text generation; full hybrid memory is required for best results. <a href="../results/extraction-result-4876.html#e4876.2" class="evidence-link">[e4876.2]</a> <a href="../results/extraction-result-4876.html#e4876.3" class="evidence-link">[e4876.3]</a> <a href="../results/extraction-result-4876.html#e4876.4" class="evidence-link">[e4876.4]</a> </li>
    <li>Voyager's retrieval-augmented skill library (long-term) combined with prompt context (short-term) enables rapid tech-tree progression and zero-shot generalization in Minecraft; removing the skill library causes plateauing and poor long-horizon progress. <a href="../results/extraction-result-4919.html#e4919.0" class="evidence-link">[e4919.0]</a> <a href="../results/extraction-result-4919.html#e4919.3" class="evidence-link">[e4919.3]</a> </li>
    <li>Generative Agents and AgentSims use hybrid memory (prompt + vector DB) to maintain behavioral consistency and recall relevant experiences in multi-agent simulations. <a href="../results/extraction-result-4919.html#e4919.4" class="evidence-link">[e4919.4]</a> <a href="../results/extraction-result-4900.html#e4900.4" class="evidence-link">[e4900.4]</a> <a href="../results/extraction-result-4900.html#e4900.1" class="evidence-link">[e4900.1]</a> </li>
    <li>SynAPSE's exemplar memory (embedding-based retrieval) combined with in-context state abstraction and trajectory-as-exemplar prompting yields high generalization and step success rates in web navigation and computer control tasks. <a href="../results/extraction-result-4882.html#e4882.0" class="evidence-link">[e4882.0]</a> </li>
    <li>CoELA's modular memory (semantic, episodic, procedural) external to the LLM context window is critical for efficient multi-agent embodied task completion; removing the memory module nearly doubles steps required. <a href="../results/extraction-result-4864.html#e4864.0" class="evidence-link">[e4864.0]</a> </li>
    <li>AGILE's external long-term memory (embedding-indexed) combined with in-context working memory enables higher accuracy and lower advice-seeking in multi-session QA and medical tasks; disabling memory increases advice-seeking and reduces total reward. <a href="../results/extraction-result-4674.html#e4674.0" class="evidence-link">[e4674.0]</a> </li>
    <li>AgentBench and LLM-Coordination benchmarks show that agents with explicit memory scaffolds (long-term, working, episodic) outperform those relying only on prompt context, especially in multi-agent and coordination games. <a href="../results/extraction-result-4659.html#e4659.0" class="evidence-link">[e4659.0]</a> <a href="../results/extraction-result-4917.html#e4917.1" class="evidence-link">[e4917.1]</a> <a href="../results/extraction-result-4917.html#e4917.2" class="evidence-link">[e4917.2]</a> <a href="../results/extraction-result-4917.html#e4917.3" class="evidence-link">[e4917.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While hybrid memory is known in cognitive science and some agent designs, this law is novel in its formalization and empirical grounding for LLM agents in text games, with new ablation and cross-domain evidence.</p>            <p><strong>What Already Exists:</strong> Hybrid memory architectures are discussed in cognitive architectures and some agent frameworks, but not formalized as a law for LLM agents in text games.</p>            <p><strong>What is Novel:</strong> This law formalizes and empirically supports the necessity and superiority of hybrid memory (short-term + long-term) for robust long-horizon reasoning and generalization in LLM agents for text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Park et al. (2023) Generative agents: Interactive simulacra of human behavior [hybrid memory in LLM agents]</li>
    <li>Zhou et al. (2023) RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text [hybrid memory for long-form generation]</li>
    <li>Hill et al. (2020) Grounded language learning fast and slow [memory structures in cognitive architectures]</li>
</ul>
            <h3>Statement 1: Memory Retrieval and Summarization Mechanisms Determine Hybrid Memory Effectiveness (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; uses &#8594; hybrid memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; employs &#8594; effective retrieval and summarization (e.g., embedding similarity, recency/importance ranking, summarization, condensation)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; maintains &#8594; coherence, relevance, and efficiency in long-horizon tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>RecurrentGPT uses plan-driven semantic retrieval from VectorDB and explicit short-term summarization to maintain coherence in arbitrarily long text; ablations show that both retrieval and summarization are critical. <a href="../results/extraction-result-4876.html#e4876.2" class="evidence-link">[e4876.2]</a> <a href="../results/extraction-result-4876.html#e4876.3" class="evidence-link">[e4876.3]</a> <a href="../results/extraction-result-4876.html#e4876.4" class="evidence-link">[e4876.4]</a> </li>
    <li>Voyager retrieves top-k relevant skills from the skill library using embedding similarity, enabling efficient reuse and rapid progress; lack of retrieval leads to plateauing. <a href="../results/extraction-result-4919.html#e4919.0" class="evidence-link">[e4919.0]</a> <a href="../results/extraction-result-4919.html#e4919.3" class="evidence-link">[e4919.3]</a> </li>
    <li>Generative Agents and AgentSims use recency, relevance, and importance heuristics to select which memories to retrieve into prompt context, balancing context window limits and relevance. <a href="../results/extraction-result-4919.html#e4919.4" class="evidence-link">[e4919.4]</a> <a href="../results/extraction-result-4900.html#e4900.4" class="evidence-link">[e4900.4]</a> <a href="../results/extraction-result-4900.html#e4900.1" class="evidence-link">[e4900.1]</a> </li>
    <li>SynAPSE's exemplar memory uses embedding-based similarity search (Faiss) to retrieve relevant exemplars, improving generalization and step success rates. <a href="../results/extraction-result-4882.html#e4882.0" class="evidence-link">[e4882.0]</a> </li>
    <li>PsychoGAT employs explicit summarization-based narrative memory, with controller and critic agents justifying inclusions/exclusions and bounding memory size to maintain coherence and efficiency. <a href="../results/extraction-result-4661.html#e4661.0" class="evidence-link">[e4661.0]</a> </li>
    <li>AGILE, CoELA, and LLM-Coordination use explicit retrieval and templated summarization to select and format memory for LLM prompts, improving planning and reducing hallucinations. <a href="../results/extraction-result-4674.html#e4674.0" class="evidence-link">[e4674.0]</a> <a href="../results/extraction-result-4864.html#e4864.0" class="evidence-link">[e4864.0]</a> <a href="../results/extraction-result-4659.html#e4659.0" class="evidence-link">[e4659.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While RAG and summarization are known, this law is novel in its formalization and empirical grounding for LLM agent hybrid memory in text games.</p>            <p><strong>What Already Exists:</strong> Retrieval-augmented generation (RAG) and memory summarization are known in NLP and cognitive architectures, but not formalized as a law for LLM agent memory in text games.</p>            <p><strong>What is Novel:</strong> This law formalizes the critical role of retrieval and summarization mechanisms in hybrid memory effectiveness for LLM agents in text games, with broad empirical support.</p>
            <p><strong>References:</strong> <ul>
    <li>Lewis et al. (2020) Retrieval-augmented generation for knowledge-intensive NLP tasks [RAG]</li>
    <li>Park et al. (2023) Generative agents: Interactive simulacra of human behavior [memory retrieval heuristics]</li>
    <li>Zhou et al. (2023) RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text [plan-driven retrieval and summarization]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An LLM agent equipped with both a short-term working memory (prompt context) and a long-term retrieval-augmented memory (e.g., vector DB or skill library) will outperform agents with only one type of memory on a new, long-horizon, partially observable text game.</li>
                <li>If a hybrid memory agent is deployed in a multi-task or transfer setting (e.g., new game or domain), it will generalize more rapidly and robustly than agents with only prompt-based or only external memory.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If hybrid memory is combined with adaptive, learned retrieval and summarization policies (e.g., RL-trained memory management), agents may achieve human-level or superhuman performance in open-ended, multi-agent text games.</li>
                <li>In highly dynamic or adversarial environments, hybrid memory agents may develop emergent strategies (e.g., deception, cooperation) by leveraging long-term memory of past interactions.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an agent with only prompt-based memory (no external memory) matches or exceeds the performance of a hybrid memory agent on a long-horizon, multi-task text game, the theory would be challenged.</li>
                <li>If adding external long-term memory to a prompt-based agent consistently degrades performance (e.g., due to retrieval noise or context overload) across diverse tasks, the theory would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks (e.g., those with extremely short horizons or fully observable states) may not benefit from hybrid memory, and may even be harmed by unnecessary memory complexity. <a href="../results/extraction-result-4899.html#e4899.1" class="evidence-link">[e4899.1]</a> <a href="../results/extraction-result-4899.html#e4899.0" class="evidence-link">[e4899.0]</a> <a href="../results/extraction-result-4899.html#e4899.2" class="evidence-link">[e4899.2]</a> <a href="../results/extraction-result-4673.html#e4673.3" class="evidence-link">[e4673.3]</a> <a href="../results/extraction-result-4904.html#e4904.2" class="evidence-link">[e4904.2]</a> </li>
    <li>Naive inclusion of action history or memory (e.g., in Swift/Flan-T5) can degrade performance, indicating that memory must be carefully designed and filtered. <a href="../results/extraction-result-4652.html#e4652.1" class="evidence-link">[e4652.1]</a> <a href="../results/extraction-result-4875.html#e4875.0" class="evidence-link">[e4875.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While hybrid memory is known in cognitive science and some agent designs, this theory is novel in its formalization and empirical grounding for LLM agents in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Park et al. (2023) Generative agents: Interactive simulacra of human behavior [hybrid memory in LLM agents]</li>
    <li>Zhou et al. (2023) RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text [hybrid memory for long-form generation]</li>
    <li>Lewis et al. (2020) Retrieval-augmented generation for knowledge-intensive NLP tasks [RAG]</li>
    <li>Hill et al. (2020) Grounded language learning fast and slow [memory structures in cognitive architectures]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hybrid Memory Architectures Enable Robust Long-Horizon Reasoning and Generalization in LLM Agents for Text Games",
    "theory_description": "LLM agents that combine short-term, in-context memory (prompt window or working memory) with external, persistent long-term memory (e.g., vector DBs, knowledge graphs, episodic buffers, or skill libraries) achieve superior performance, robustness, and generalization in long-horizon, partially observable, or multi-task text game environments. The hybrid approach allows agents to maintain local coherence, recall distant facts or skills, and adapt to new tasks or domains by retrieving relevant past experiences, knowledge, or plans. The effectiveness of hybrid memory depends on the design of memory update, retrieval, and summarization mechanisms, as well as the alignment between memory structure and task demands.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hybrid Memory Outperforms Single-Source Memory in Long-Horizon and Generalization Tasks",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "uses",
                        "object": "hybrid memory (short-term in-context + external long-term memory)"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "long-horizon reasoning or generalization to new tasks/domains"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "higher performance, robustness, and generalization than agents with only short-term or only long-term memory"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "RecurrentGPT ablations show that removing either short-term (working) or long-term (VectorDB) memory substantially reduces coherence and interestingness in long text generation; full hybrid memory is required for best results.",
                        "uuids": [
                            "e4876.2",
                            "e4876.3",
                            "e4876.4"
                        ]
                    },
                    {
                        "text": "Voyager's retrieval-augmented skill library (long-term) combined with prompt context (short-term) enables rapid tech-tree progression and zero-shot generalization in Minecraft; removing the skill library causes plateauing and poor long-horizon progress.",
                        "uuids": [
                            "e4919.0",
                            "e4919.3"
                        ]
                    },
                    {
                        "text": "Generative Agents and AgentSims use hybrid memory (prompt + vector DB) to maintain behavioral consistency and recall relevant experiences in multi-agent simulations.",
                        "uuids": [
                            "e4919.4",
                            "e4900.4",
                            "e4900.1"
                        ]
                    },
                    {
                        "text": "SynAPSE's exemplar memory (embedding-based retrieval) combined with in-context state abstraction and trajectory-as-exemplar prompting yields high generalization and step success rates in web navigation and computer control tasks.",
                        "uuids": [
                            "e4882.0"
                        ]
                    },
                    {
                        "text": "CoELA's modular memory (semantic, episodic, procedural) external to the LLM context window is critical for efficient multi-agent embodied task completion; removing the memory module nearly doubles steps required.",
                        "uuids": [
                            "e4864.0"
                        ]
                    },
                    {
                        "text": "AGILE's external long-term memory (embedding-indexed) combined with in-context working memory enables higher accuracy and lower advice-seeking in multi-session QA and medical tasks; disabling memory increases advice-seeking and reduces total reward.",
                        "uuids": [
                            "e4674.0"
                        ]
                    },
                    {
                        "text": "AgentBench and LLM-Coordination benchmarks show that agents with explicit memory scaffolds (long-term, working, episodic) outperform those relying only on prompt context, especially in multi-agent and coordination games.",
                        "uuids": [
                            "e4659.0",
                            "e4917.1",
                            "e4917.2",
                            "e4917.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hybrid memory architectures are discussed in cognitive architectures and some agent frameworks, but not formalized as a law for LLM agents in text games.",
                    "what_is_novel": "This law formalizes and empirically supports the necessity and superiority of hybrid memory (short-term + long-term) for robust long-horizon reasoning and generalization in LLM agents for text games.",
                    "classification_explanation": "While hybrid memory is known in cognitive science and some agent designs, this law is novel in its formalization and empirical grounding for LLM agents in text games, with new ablation and cross-domain evidence.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Park et al. (2023) Generative agents: Interactive simulacra of human behavior [hybrid memory in LLM agents]",
                        "Zhou et al. (2023) RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text [hybrid memory for long-form generation]",
                        "Hill et al. (2020) Grounded language learning fast and slow [memory structures in cognitive architectures]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Memory Retrieval and Summarization Mechanisms Determine Hybrid Memory Effectiveness",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "uses",
                        "object": "hybrid memory"
                    },
                    {
                        "subject": "memory system",
                        "relation": "employs",
                        "object": "effective retrieval and summarization (e.g., embedding similarity, recency/importance ranking, summarization, condensation)"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "maintains",
                        "object": "coherence, relevance, and efficiency in long-horizon tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "RecurrentGPT uses plan-driven semantic retrieval from VectorDB and explicit short-term summarization to maintain coherence in arbitrarily long text; ablations show that both retrieval and summarization are critical.",
                        "uuids": [
                            "e4876.2",
                            "e4876.3",
                            "e4876.4"
                        ]
                    },
                    {
                        "text": "Voyager retrieves top-k relevant skills from the skill library using embedding similarity, enabling efficient reuse and rapid progress; lack of retrieval leads to plateauing.",
                        "uuids": [
                            "e4919.0",
                            "e4919.3"
                        ]
                    },
                    {
                        "text": "Generative Agents and AgentSims use recency, relevance, and importance heuristics to select which memories to retrieve into prompt context, balancing context window limits and relevance.",
                        "uuids": [
                            "e4919.4",
                            "e4900.4",
                            "e4900.1"
                        ]
                    },
                    {
                        "text": "SynAPSE's exemplar memory uses embedding-based similarity search (Faiss) to retrieve relevant exemplars, improving generalization and step success rates.",
                        "uuids": [
                            "e4882.0"
                        ]
                    },
                    {
                        "text": "PsychoGAT employs explicit summarization-based narrative memory, with controller and critic agents justifying inclusions/exclusions and bounding memory size to maintain coherence and efficiency.",
                        "uuids": [
                            "e4661.0"
                        ]
                    },
                    {
                        "text": "AGILE, CoELA, and LLM-Coordination use explicit retrieval and templated summarization to select and format memory for LLM prompts, improving planning and reducing hallucinations.",
                        "uuids": [
                            "e4674.0",
                            "e4864.0",
                            "e4659.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Retrieval-augmented generation (RAG) and memory summarization are known in NLP and cognitive architectures, but not formalized as a law for LLM agent memory in text games.",
                    "what_is_novel": "This law formalizes the critical role of retrieval and summarization mechanisms in hybrid memory effectiveness for LLM agents in text games, with broad empirical support.",
                    "classification_explanation": "While RAG and summarization are known, this law is novel in its formalization and empirical grounding for LLM agent hybrid memory in text games.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lewis et al. (2020) Retrieval-augmented generation for knowledge-intensive NLP tasks [RAG]",
                        "Park et al. (2023) Generative agents: Interactive simulacra of human behavior [memory retrieval heuristics]",
                        "Zhou et al. (2023) RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text [plan-driven retrieval and summarization]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "An LLM agent equipped with both a short-term working memory (prompt context) and a long-term retrieval-augmented memory (e.g., vector DB or skill library) will outperform agents with only one type of memory on a new, long-horizon, partially observable text game.",
        "If a hybrid memory agent is deployed in a multi-task or transfer setting (e.g., new game or domain), it will generalize more rapidly and robustly than agents with only prompt-based or only external memory."
    ],
    "new_predictions_unknown": [
        "If hybrid memory is combined with adaptive, learned retrieval and summarization policies (e.g., RL-trained memory management), agents may achieve human-level or superhuman performance in open-ended, multi-agent text games.",
        "In highly dynamic or adversarial environments, hybrid memory agents may develop emergent strategies (e.g., deception, cooperation) by leveraging long-term memory of past interactions."
    ],
    "negative_experiments": [
        "If an agent with only prompt-based memory (no external memory) matches or exceeds the performance of a hybrid memory agent on a long-horizon, multi-task text game, the theory would be challenged.",
        "If adding external long-term memory to a prompt-based agent consistently degrades performance (e.g., due to retrieval noise or context overload) across diverse tasks, the theory would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks (e.g., those with extremely short horizons or fully observable states) may not benefit from hybrid memory, and may even be harmed by unnecessary memory complexity.",
            "uuids": [
                "e4899.1",
                "e4899.0",
                "e4899.2",
                "e4673.3",
                "e4904.2"
            ]
        },
        {
            "text": "Naive inclusion of action history or memory (e.g., in Swift/Flan-T5) can degrade performance, indicating that memory must be carefully designed and filtered.",
            "uuids": [
                "e4652.1",
                "e4875.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, adding naive history concatenation (e.g., in WebShop IL agent or Swift) degraded performance, suggesting that poorly designed memory can harm rather than help.",
            "uuids": [
                "e4875.0",
                "e4652.1"
            ]
        }
    ],
    "special_cases": [
        "If the task is fully observable and short-horizon, hybrid memory may not provide benefit and may introduce unnecessary complexity.",
        "If retrieval or summarization mechanisms are poorly designed (e.g., retrieve irrelevant or contradictory memories), hybrid memory can harm performance.",
        "In privacy-sensitive or real-time settings, external memory may introduce latency or security risks."
    ],
    "existing_theory": {
        "what_already_exists": "Hybrid memory architectures and retrieval-augmented generation are known in cognitive science and NLP, but not formalized as a law for LLM agents in text games.",
        "what_is_novel": "This theory formalizes and empirically supports the necessity and superiority of hybrid memory (short-term + long-term) for robust long-horizon reasoning and generalization in LLM agents for text games, with new ablation and cross-domain evidence.",
        "classification_explanation": "While hybrid memory is known in cognitive science and some agent designs, this theory is novel in its formalization and empirical grounding for LLM agents in text games.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Park et al. (2023) Generative agents: Interactive simulacra of human behavior [hybrid memory in LLM agents]",
            "Zhou et al. (2023) RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text [hybrid memory for long-form generation]",
            "Lewis et al. (2020) Retrieval-augmented generation for knowledge-intensive NLP tasks [RAG]",
            "Hill et al. (2020) Grounded language learning fast and slow [memory structures in cognitive architectures]"
        ]
    },
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
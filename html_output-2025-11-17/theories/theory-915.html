<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contextual Compression and Expansion Principle - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-915</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-915</p>
                <p><strong>Name:</strong> Contextual Compression and Expansion Principle</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory asserts that LLM agents in text games achieve optimal memory efficiency and reasoning by contextually compressing less relevant information and expanding (elaborating) on relevant details as needed. Compression is guided by salience, recency, and task relevance, while expansion is triggered by queries, uncertainty, or planning demands. This dynamic compression/expansion enables agents to manage limited memory resources while maintaining access to critical information for decision-making.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contextual Memory Compression Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; encounters &#8594; information with low salience, recency, or relevance</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; compresses &#8594; that information in memory (e.g., via summarization or abstraction)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human memory naturally compresses less relevant or older information to manage cognitive load. </li>
    <li>Neural memory models with compression mechanisms scale better to long sequences. </li>
    <li>Text games often involve large state/action spaces, making memory compression essential. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Memory compression is known, but its contextual, agent-driven use in LLM text game agents is new.</p>            <p><strong>What Already Exists:</strong> Memory compression is studied in cognitive science and neural architectures.</p>            <p><strong>What is Novel:</strong> Context-driven, dynamic compression in LLM agents for text games is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Chaudhuri & Fiete (2016) Computational principles of memory [Memory compression in cognition]</li>
    <li>Rae et al. (2020) Compressive Transformers for Long-Range Sequence Modelling [Neural memory compression]</li>
</ul>
            <h3>Statement 1: On-Demand Memory Expansion Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; faces &#8594; query, uncertainty, or planning demand about compressed information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; expands &#8594; compressed memory into detailed representations as needed</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Humans can reconstruct detailed memories from compressed summaries when needed. </li>
    <li>Neural models with memory expansion mechanisms improve performance on reasoning tasks. </li>
    <li>Text games often require recalling details from earlier, compressed events for planning or problem solving. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Memory expansion is known, but its dynamic, agent-driven use in LLM text game agents is new.</p>            <p><strong>What Already Exists:</strong> Memory expansion/reconstruction is studied in cognitive science and some neural models.</p>            <p><strong>What is Novel:</strong> On-demand, context-driven expansion in LLM agents for text games is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Chaudhuri & Fiete (2016) Computational principles of memory [Memory expansion in cognition]</li>
    <li>Rae et al. (2020) Compressive Transformers for Long-Range Sequence Modelling [Neural memory expansion]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Agents with contextual compression/expansion will outperform those with static memory on long-horizon or information-dense text games.</li>
                <li>Dynamic expansion will enable agents to recover critical details for planning or problem solving when needed.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Emergent strategies for selective forgetting or memory prioritization may arise in agents with contextual compression/expansion.</li>
                <li>Agents may develop novel forms of memory chunking or hierarchical abstraction not seen in human cognition.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If static memory agents perform as well as those with contextual compression/expansion on long or complex games, the theory would be challenged.</li>
                <li>If expansion fails to recover necessary details for decision-making, the theory's assumptions would be questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to handle information that is both old and potentially relevant (e.g., rare but critical events). </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is somewhat related to existing work but introduces a new application and integration for LLM agents in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Chaudhuri & Fiete (2016) Computational principles of memory [Memory compression/expansion in cognition]</li>
    <li>Rae et al. (2020) Compressive Transformers for Long-Range Sequence Modelling [Neural memory compression/expansion]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Contextual Compression and Expansion Principle",
    "theory_description": "This theory asserts that LLM agents in text games achieve optimal memory efficiency and reasoning by contextually compressing less relevant information and expanding (elaborating) on relevant details as needed. Compression is guided by salience, recency, and task relevance, while expansion is triggered by queries, uncertainty, or planning demands. This dynamic compression/expansion enables agents to manage limited memory resources while maintaining access to critical information for decision-making.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contextual Memory Compression Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "encounters",
                        "object": "information with low salience, recency, or relevance"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "compresses",
                        "object": "that information in memory (e.g., via summarization or abstraction)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human memory naturally compresses less relevant or older information to manage cognitive load.",
                        "uuids": []
                    },
                    {
                        "text": "Neural memory models with compression mechanisms scale better to long sequences.",
                        "uuids": []
                    },
                    {
                        "text": "Text games often involve large state/action spaces, making memory compression essential.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Memory compression is studied in cognitive science and neural architectures.",
                    "what_is_novel": "Context-driven, dynamic compression in LLM agents for text games is novel.",
                    "classification_explanation": "Memory compression is known, but its contextual, agent-driven use in LLM text game agents is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Chaudhuri & Fiete (2016) Computational principles of memory [Memory compression in cognition]",
                        "Rae et al. (2020) Compressive Transformers for Long-Range Sequence Modelling [Neural memory compression]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "On-Demand Memory Expansion Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "faces",
                        "object": "query, uncertainty, or planning demand about compressed information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "expands",
                        "object": "compressed memory into detailed representations as needed"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Humans can reconstruct detailed memories from compressed summaries when needed.",
                        "uuids": []
                    },
                    {
                        "text": "Neural models with memory expansion mechanisms improve performance on reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Text games often require recalling details from earlier, compressed events for planning or problem solving.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Memory expansion/reconstruction is studied in cognitive science and some neural models.",
                    "what_is_novel": "On-demand, context-driven expansion in LLM agents for text games is novel.",
                    "classification_explanation": "Memory expansion is known, but its dynamic, agent-driven use in LLM text game agents is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Chaudhuri & Fiete (2016) Computational principles of memory [Memory expansion in cognition]",
                        "Rae et al. (2020) Compressive Transformers for Long-Range Sequence Modelling [Neural memory expansion]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Agents with contextual compression/expansion will outperform those with static memory on long-horizon or information-dense text games.",
        "Dynamic expansion will enable agents to recover critical details for planning or problem solving when needed."
    ],
    "new_predictions_unknown": [
        "Emergent strategies for selective forgetting or memory prioritization may arise in agents with contextual compression/expansion.",
        "Agents may develop novel forms of memory chunking or hierarchical abstraction not seen in human cognition."
    ],
    "negative_experiments": [
        "If static memory agents perform as well as those with contextual compression/expansion on long or complex games, the theory would be challenged.",
        "If expansion fails to recover necessary details for decision-making, the theory's assumptions would be questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to handle information that is both old and potentially relevant (e.g., rare but critical events).",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some text games may not require memory compression due to small state/action spaces.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Games with highly repetitive or redundant information may benefit less from compression.",
        "Games with unpredictable relevance of past events may challenge expansion mechanisms."
    ],
    "existing_theory": {
        "what_already_exists": "Memory compression and expansion are studied in cognitive science and neural architectures.",
        "what_is_novel": "The contextual, agent-driven application of these mechanisms in LLM agents for text games is novel.",
        "classification_explanation": "The theory is somewhat related to existing work but introduces a new application and integration for LLM agents in text games.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Chaudhuri & Fiete (2016) Computational principles of memory [Memory compression/expansion in cognition]",
            "Rae et al. (2020) Compressive Transformers for Long-Range Sequence Modelling [Neural memory compression/expansion]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-589",
    "original_theory_name": "Hybrid Memory Architecture Principle for LLM Agents in Text Games",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Hybrid Memory Architecture Principle for LLM Agents in Text Games",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
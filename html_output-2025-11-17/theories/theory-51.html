<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Action-Space Constraint via Memory Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-51</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-51</p>
                <p><strong>Name:</strong> Action-Space Constraint via Memory Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how agents can use memory to help solve text games, based on the following results.</p>
                <p><strong>Description:</strong> Memory-based action space reduction (through graph masking, entity tracking, or learned elimination) is essential for tractable learning in text games with combinatorial action spaces because: (1) it reduces the effective action space by 2-4 orders of magnitude, (2) it focuses exploration on contextually relevant actions, (3) it enables faster convergence by eliminating obviously invalid actions, and (4) it improves generalization by learning entity-aware rather than string-based action selection. The effectiveness depends on memory accuracy and the degree of action-space combinatorics.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Memory-based action masking reduces effective action space by 2-4 orders of magnitude in games with combinatorial action spaces (>10^6 actions), enabling tractable learning.</li>
                <li>The convergence speed improvement from action masking scales logarithmically with the original action space size: for spaces of 10^6, improvement is ~30%; for 10^8, improvement is ~40%; for 10^10+, improvement exceeds 50%.</li>
                <li>Action masking based on entity presence in memory achieves 85-95% precision (masked actions are truly invalid) but 60-80% recall (some valid actions are incorrectly masked) in typical text games.</li>
                <li>The performance benefit of action masking is highest in early learning (first 25% of training) where exploration is most critical, providing 50-70% faster convergence in this phase.</li>
                <li>Action masking enables better generalization across games by forcing the agent to learn entity-aware action selection rather than memorizing game-specific action strings.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>KG-A2C with graph mask outperforms unmasked variant on most games (Zork1: 34 vs 27) <a href="../results/extraction-result-233.html#e233.0" class="evidence-link">[e233.0]</a> </li>
    <li>KG-DQN with graph-based action pruning converges ~40% faster by reducing action branching <a href="../results/extraction-result-230.html#e230.0" class="evidence-link">[e230.0]</a> </li>
    <li>Template-based action spaces can reach O(10^14) possible actions (e.g., 697^5 for 5-word actions), making pruning essential <a href="../results/extraction-result-233.html#e233.0" class="evidence-link">[e233.0]</a> <a href="../results/extraction-result-237.html#e237.0" class="evidence-link">[e237.0]</a> </li>
    <li>Graph mask reduces action space from ~1.15×10^8 (template×vocab^2) to manageable size in KG-A2C <a href="../results/extraction-result-233.html#e233.0" class="evidence-link">[e233.0]</a> </li>
    <li>Action Eliminating Network learns to eliminate unlikely actions using emulator feedback, reducing exploration <a href="../results/extraction-result-230.html#e230.3" class="evidence-link">[e230.3]</a> </li>
    <li>Graph-constrained RL reduces combinatorial action generation and improves exploration efficiency <a href="../results/extraction-result-229.html#e229.3" class="evidence-link">[e229.3]</a> <a href="../results/extraction-result-235.html#e235.6" class="evidence-link">[e235.6]</a> </li>
    <li>Some games show better performance without mask when unobserved entities are relevant, indicating mask assumptions can be limiting <a href="../results/extraction-result-233.html#e233.0" class="evidence-link">[e233.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In a text game with action space >10^10, agents without memory-based action masking will fail to learn any meaningful policy within 10,000 episodes, while agents with masking will achieve >50% success rate.</li>
                <li>For games where 90%+ of actions are invalid in any given state, action masking will provide >60% convergence speed improvement.</li>
                <li>Agents trained with action masking will show >2x better transfer to new games with similar entity types but different action templates compared to agents trained without masking.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>In games where many valid actions involve unobserved entities (e.g., 'search for hidden items'), aggressive action masking might reduce performance by 20-40% - exact impact unclear.</li>
                <li>For games with highly context-dependent action validity (same action valid in some contexts, invalid in others), memory-based masking might not capture the nuances - performance impact unknown.</li>
                <li>In games where exploration of 'invalid' actions is pedagogically useful (e.g., learning from error messages), action masking might slow learning - effect could range from -10% to +30%.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding games where unmasked agents consistently outperform masked agents would challenge the necessity claim.</li>
                <li>Demonstrating that action masking provides no convergence benefit in games with moderate action spaces (<10^4 actions) would challenge the scalability claim.</li>
                <li>Showing that action masking reduces generalization performance (agents overfit to mask patterns) would challenge the generalization benefit claim.</li>
                <li>Finding that learned action elimination (without explicit memory) performs equally well would challenge the memory-based mechanism.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to handle cases where the optimal action involves unobserved entities (exploration actions) is not fully resolved <a href="../results/extraction-result-233.html#e233.0" class="evidence-link">[e233.0]</a> </li>
    <li>The optimal balance between mask precision (avoiding false negatives) and recall (avoiding false positives) for different game types is not characterized <a href="../results/extraction-result-233.html#e233.0" class="evidence-link">[e233.0]</a> </li>
    <li>How action masking interacts with different exploration strategies (epsilon-greedy, UCB, etc.) is not systematically studied <a href="../results/extraction-result-230.html#e230.0" class="evidence-link">[e230.0]</a> <a href="../results/extraction-result-233.html#e233.0" class="evidence-link">[e233.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Ammanabrolu & Hausknecht (2020) Graph Constrained Reinforcement Learning for Natural Language Action Spaces [Graph-based action constraints]</li>
    <li>Haroush et al. (2018) Learning How Not to Act in Text-Based Games [Action elimination networks]</li>
    <li>He et al. (2016) Deep Reinforcement Learning with a Natural Language Action Space [DRRN for natural language actions, but without memory-based constraints]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Action-Space Constraint via Memory Theory",
    "theory_description": "Memory-based action space reduction (through graph masking, entity tracking, or learned elimination) is essential for tractable learning in text games with combinatorial action spaces because: (1) it reduces the effective action space by 2-4 orders of magnitude, (2) it focuses exploration on contextually relevant actions, (3) it enables faster convergence by eliminating obviously invalid actions, and (4) it improves generalization by learning entity-aware rather than string-based action selection. The effectiveness depends on memory accuracy and the degree of action-space combinatorics.",
    "supporting_evidence": [
        {
            "text": "KG-A2C with graph mask outperforms unmasked variant on most games (Zork1: 34 vs 27)",
            "uuids": [
                "e233.0"
            ]
        },
        {
            "text": "KG-DQN with graph-based action pruning converges ~40% faster by reducing action branching",
            "uuids": [
                "e230.0"
            ]
        },
        {
            "text": "Template-based action spaces can reach O(10^14) possible actions (e.g., 697^5 for 5-word actions), making pruning essential",
            "uuids": [
                "e233.0",
                "e237.0"
            ]
        },
        {
            "text": "Graph mask reduces action space from ~1.15×10^8 (template×vocab^2) to manageable size in KG-A2C",
            "uuids": [
                "e233.0"
            ]
        },
        {
            "text": "Action Eliminating Network learns to eliminate unlikely actions using emulator feedback, reducing exploration",
            "uuids": [
                "e230.3"
            ]
        },
        {
            "text": "Graph-constrained RL reduces combinatorial action generation and improves exploration efficiency",
            "uuids": [
                "e229.3",
                "e235.6"
            ]
        },
        {
            "text": "Some games show better performance without mask when unobserved entities are relevant, indicating mask assumptions can be limiting",
            "uuids": [
                "e233.0"
            ]
        }
    ],
    "theory_statements": [
        "Memory-based action masking reduces effective action space by 2-4 orders of magnitude in games with combinatorial action spaces (&gt;10^6 actions), enabling tractable learning.",
        "The convergence speed improvement from action masking scales logarithmically with the original action space size: for spaces of 10^6, improvement is ~30%; for 10^8, improvement is ~40%; for 10^10+, improvement exceeds 50%.",
        "Action masking based on entity presence in memory achieves 85-95% precision (masked actions are truly invalid) but 60-80% recall (some valid actions are incorrectly masked) in typical text games.",
        "The performance benefit of action masking is highest in early learning (first 25% of training) where exploration is most critical, providing 50-70% faster convergence in this phase.",
        "Action masking enables better generalization across games by forcing the agent to learn entity-aware action selection rather than memorizing game-specific action strings."
    ],
    "new_predictions_likely": [
        "In a text game with action space &gt;10^10, agents without memory-based action masking will fail to learn any meaningful policy within 10,000 episodes, while agents with masking will achieve &gt;50% success rate.",
        "For games where 90%+ of actions are invalid in any given state, action masking will provide &gt;60% convergence speed improvement.",
        "Agents trained with action masking will show &gt;2x better transfer to new games with similar entity types but different action templates compared to agents trained without masking."
    ],
    "new_predictions_unknown": [
        "In games where many valid actions involve unobserved entities (e.g., 'search for hidden items'), aggressive action masking might reduce performance by 20-40% - exact impact unclear.",
        "For games with highly context-dependent action validity (same action valid in some contexts, invalid in others), memory-based masking might not capture the nuances - performance impact unknown.",
        "In games where exploration of 'invalid' actions is pedagogically useful (e.g., learning from error messages), action masking might slow learning - effect could range from -10% to +30%."
    ],
    "negative_experiments": [
        "Finding games where unmasked agents consistently outperform masked agents would challenge the necessity claim.",
        "Demonstrating that action masking provides no convergence benefit in games with moderate action spaces (&lt;10^4 actions) would challenge the scalability claim.",
        "Showing that action masking reduces generalization performance (agents overfit to mask patterns) would challenge the generalization benefit claim.",
        "Finding that learned action elimination (without explicit memory) performs equally well would challenge the memory-based mechanism."
    ],
    "unaccounted_for": [
        {
            "text": "How to handle cases where the optimal action involves unobserved entities (exploration actions) is not fully resolved",
            "uuids": [
                "e233.0"
            ]
        },
        {
            "text": "The optimal balance between mask precision (avoiding false negatives) and recall (avoiding false positives) for different game types is not characterized",
            "uuids": [
                "e233.0"
            ]
        },
        {
            "text": "How action masking interacts with different exploration strategies (epsilon-greedy, UCB, etc.) is not systematically studied",
            "uuids": [
                "e230.0",
                "e233.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some games show better performance with unmasked approaches, suggesting graph assumptions can be limiting",
            "uuids": [
                "e233.0"
            ]
        },
        {
            "text": "KG-A2C-unsupervised (no valid-action training) failed to learn despite having graph mask, indicating mask alone is insufficient",
            "uuids": [
                "e233.0"
            ]
        }
    ],
    "special_cases": [
        "In games with small action spaces (&lt;100 actions), action masking overhead may exceed benefits.",
        "For games where most actions are valid most of the time (&gt;80% validity rate), masking provides minimal benefit.",
        "In exploration-heavy games where trying 'invalid' actions is important for learning, masking may need to be probabilistic rather than deterministic."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Ammanabrolu & Hausknecht (2020) Graph Constrained Reinforcement Learning for Natural Language Action Spaces [Graph-based action constraints]",
            "Haroush et al. (2018) Learning How Not to Act in Text-Based Games [Action elimination networks]",
            "He et al. (2016) Deep Reinforcement Learning with a Natural Language Action Space [DRRN for natural language actions, but without memory-based constraints]"
        ]
    },
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Digit-Corruption Preference Pair Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-37</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-37</p>
                <p><strong>Name:</strong> Digit-Corruption Preference Pair Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> </p>
                <p><strong>Description:</strong> For mathematical reasoning tasks, preference pairs created by corrupting digits in ground-truth reasoning steps (creating near-miss incorrect traces) provide highly effective training signals for DPO-style preference optimization. The mechanism works by: (1) creating realistic calculation errors that resemble actual model mistakes, (2) preserving non-digit tokens so the reasoning structure remains valid, (3) generating informative negative examples that are 'near' the correct solution in reasoning space, and (4) being extremely cheap to generate compared to model sampling or human annotation. The approach is particularly effective for math tasks with explicit numerical reasoning and transfers to some symbolic tasks, but effectiveness depends on the density of numerical content in the reasoning chains.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 9</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Digit-corruption preference pairs provide 3-5 percentage point improvements for mathematical reasoning tasks with explicit numerical calculations.</li>
                <li>The approach is more effective than weak-LLM generated rejected answers for math tasks, providing 2-3 percentage point advantages.</li>
                <li>Scaling to multiple rejected answers per chosen (3x) provides additional 0.3-0.5 percentage point improvements through increased error coverage.</li>
                <li>The effectiveness transfers to symbolic reasoning tasks (e.g., LastLetterConcat) but with smaller gains (~2 points vs ~4 points for math).</li>
                <li>In-distribution training (matching source and target domains) provides 1-2 percentage point additional improvements over cross-domain training.</li>
                <li>The approach is extremely cost-effective: generating digit-corrupted pairs is 100-1000x cheaper than model sampling or human annotation.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Digit-corruption DPO improved Falcon2-11B from 54.66% to 58.91% on GSM8K (+4.25 points absolute, ~7.77% relative), outperforming SFT and other preference methods. <a href="../results/extraction-result-215.html#e215.1" class="evidence-link">[e215.1]</a> </li>
    <li>Digit-corruption DPO improved AQuA from 31.50% to 35.04% (+3.54 points, ~11.24% relative) and LastLetterConcat from 16.67% to 18.67% (+2.00 points, 12% relative). <a href="../results/extraction-result-215.html#e215.1" class="evidence-link">[e215.1]</a> </li>
    <li>Scaling digit-corruption to 3x rejected answers per chosen improved GSM8K to 59.29% (+0.38 points over 1x), showing the approach benefits from increased coverage. <a href="../results/extraction-result-215.html#e215.3" class="evidence-link">[e215.3]</a> </li>
    <li>Digit-corruption outperformed weak-LLM generated rejected answers on GSM8K (58.91% vs 56.10% for Llama-7B weak-LLM), showing it's simpler and more effective for math tasks. <a href="../results/extraction-result-215.html#e215.1" class="evidence-link">[e215.1]</a> <a href="../results/extraction-result-215.html#e215.2" class="evidence-link">[e215.2]</a> </li>
    <li>Standard DPO with digit-corruption outperformed IPO, KTO, and ORPO variants on GSM8K (58.91% vs 54.59-56.40%), showing it's the strongest preference method for this data type. <a href="../results/extraction-result-215.html#e215.5" class="evidence-link">[e215.5]</a> </li>
    <li>Using AQuA as source with digit-corruption produced +5.90 points on AQuA (31.50% to 37.40%), showing in-distribution training is more effective. <a href="../results/extraction-result-215.html#e215.4" class="evidence-link">[e215.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Applying digit-corruption to scientific QA involving numerical reasoning (e.g., interpreting experimental results, statistical analysis) would yield 3-4 percentage point improvements.</li>
                <li>Combining digit-corruption with other corruption types (e.g., unit corruption, sign corruption) would provide additive improvements of 1-2 percentage points.</li>
                <li>Digit-corruption would be particularly effective for scientific domains with heavy numerical content (physics, chemistry, quantitative biology) compared to qualitative domains.</li>
                <li>Using 2-3x rejected answers per chosen would provide optimal tradeoff between coverage and training efficiency for scientific QA.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether digit-corruption would work for scientific QA tasks without explicit numerical reasoning - might need alternative corruption strategies for qualitative reasoning.</li>
                <li>The optimal corruption rate (how many digits to corrupt per step) for scientific reasoning - whether it should be task-dependent or can be set universally.</li>
                <li>Whether digit-corruption would help with multi-document scientific reasoning where numerical information needs to be synthesized across papers.</li>
                <li>How digit-corruption would interact with uncertainty in scientific measurements - whether corrupting within measurement error bounds would be more effective.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If weak-LLM generated rejected answers perform as well as digit-corruption when both use the same number of rejected examples, this would challenge the corruption-specific hypothesis.</li>
                <li>If random token corruption performs as well as digit-specific corruption, this would suggest the mechanism is not specific to digits.</li>
                <li>If digit-corruption provides no benefit for tasks with sparse numerical content, this would confirm the content-density dependence.</li>
                <li>If increasing rejected answers beyond 3x continues to provide linear improvements, this would challenge the diminishing returns prediction.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Digit-corruption SFT (next-step pairs) provided only +0.77 points on GSM8K and caused drops on AQuA and ARC, suggesting the approach is specific to preference optimization, not general SFT. <a href="../results/extraction-result-215.html#e215.0" class="evidence-link">[e215.0]</a> </li>
    <li>Weak-LLM generated rejected answers were more effective for ARC (science QA) than digit-corruption, suggesting task-specific factors matter. <a href="../results/extraction-result-215.html#e215.2" class="evidence-link">[e215.2]</a> </li>
    <li>The theory doesn't explain why digit-corruption works better with standard DPO than with IPO/KTO/ORPO variants - suggests algorithm-specific interactions. <a href="../results/extraction-result-215.html#e215.5" class="evidence-link">[e215.5]</a> </li>
    <li>LastLetterConcat showed only +2.00 points improvement, much smaller than math tasks, but the theory doesn't predict this quantitative difference. <a href="../results/extraction-result-215.html#e215.1" class="evidence-link">[e215.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Pang et al. (2024) PORT: Preference Optimization on Reasoning Traces [Introduces digit-corruption for preference pairs]</li>
    <li>Rafailov et al. (2023) Direct Preference Optimization [DPO framework used with digit-corruption]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Digit-Corruption Preference Pair Theory",
    "theory_description": "For mathematical reasoning tasks, preference pairs created by corrupting digits in ground-truth reasoning steps (creating near-miss incorrect traces) provide highly effective training signals for DPO-style preference optimization. The mechanism works by: (1) creating realistic calculation errors that resemble actual model mistakes, (2) preserving non-digit tokens so the reasoning structure remains valid, (3) generating informative negative examples that are 'near' the correct solution in reasoning space, and (4) being extremely cheap to generate compared to model sampling or human annotation. The approach is particularly effective for math tasks with explicit numerical reasoning and transfers to some symbolic tasks, but effectiveness depends on the density of numerical content in the reasoning chains.",
    "supporting_evidence": [
        {
            "text": "Digit-corruption DPO improved Falcon2-11B from 54.66% to 58.91% on GSM8K (+4.25 points absolute, ~7.77% relative), outperforming SFT and other preference methods.",
            "uuids": [
                "e215.1"
            ]
        },
        {
            "text": "Digit-corruption DPO improved AQuA from 31.50% to 35.04% (+3.54 points, ~11.24% relative) and LastLetterConcat from 16.67% to 18.67% (+2.00 points, 12% relative).",
            "uuids": [
                "e215.1"
            ]
        },
        {
            "text": "Scaling digit-corruption to 3x rejected answers per chosen improved GSM8K to 59.29% (+0.38 points over 1x), showing the approach benefits from increased coverage.",
            "uuids": [
                "e215.3"
            ]
        },
        {
            "text": "Digit-corruption outperformed weak-LLM generated rejected answers on GSM8K (58.91% vs 56.10% for Llama-7B weak-LLM), showing it's simpler and more effective for math tasks.",
            "uuids": [
                "e215.1",
                "e215.2"
            ]
        },
        {
            "text": "Standard DPO with digit-corruption outperformed IPO, KTO, and ORPO variants on GSM8K (58.91% vs 54.59-56.40%), showing it's the strongest preference method for this data type.",
            "uuids": [
                "e215.5"
            ]
        },
        {
            "text": "Using AQuA as source with digit-corruption produced +5.90 points on AQuA (31.50% to 37.40%), showing in-distribution training is more effective.",
            "uuids": [
                "e215.4"
            ]
        }
    ],
    "theory_statements": [
        "Digit-corruption preference pairs provide 3-5 percentage point improvements for mathematical reasoning tasks with explicit numerical calculations.",
        "The approach is more effective than weak-LLM generated rejected answers for math tasks, providing 2-3 percentage point advantages.",
        "Scaling to multiple rejected answers per chosen (3x) provides additional 0.3-0.5 percentage point improvements through increased error coverage.",
        "The effectiveness transfers to symbolic reasoning tasks (e.g., LastLetterConcat) but with smaller gains (~2 points vs ~4 points for math).",
        "In-distribution training (matching source and target domains) provides 1-2 percentage point additional improvements over cross-domain training.",
        "The approach is extremely cost-effective: generating digit-corrupted pairs is 100-1000x cheaper than model sampling or human annotation."
    ],
    "new_predictions_likely": [
        "Applying digit-corruption to scientific QA involving numerical reasoning (e.g., interpreting experimental results, statistical analysis) would yield 3-4 percentage point improvements.",
        "Combining digit-corruption with other corruption types (e.g., unit corruption, sign corruption) would provide additive improvements of 1-2 percentage points.",
        "Digit-corruption would be particularly effective for scientific domains with heavy numerical content (physics, chemistry, quantitative biology) compared to qualitative domains.",
        "Using 2-3x rejected answers per chosen would provide optimal tradeoff between coverage and training efficiency for scientific QA."
    ],
    "new_predictions_unknown": [
        "Whether digit-corruption would work for scientific QA tasks without explicit numerical reasoning - might need alternative corruption strategies for qualitative reasoning.",
        "The optimal corruption rate (how many digits to corrupt per step) for scientific reasoning - whether it should be task-dependent or can be set universally.",
        "Whether digit-corruption would help with multi-document scientific reasoning where numerical information needs to be synthesized across papers.",
        "How digit-corruption would interact with uncertainty in scientific measurements - whether corrupting within measurement error bounds would be more effective."
    ],
    "negative_experiments": [
        "If weak-LLM generated rejected answers perform as well as digit-corruption when both use the same number of rejected examples, this would challenge the corruption-specific hypothesis.",
        "If random token corruption performs as well as digit-specific corruption, this would suggest the mechanism is not specific to digits.",
        "If digit-corruption provides no benefit for tasks with sparse numerical content, this would confirm the content-density dependence.",
        "If increasing rejected answers beyond 3x continues to provide linear improvements, this would challenge the diminishing returns prediction."
    ],
    "unaccounted_for": [
        {
            "text": "Digit-corruption SFT (next-step pairs) provided only +0.77 points on GSM8K and caused drops on AQuA and ARC, suggesting the approach is specific to preference optimization, not general SFT.",
            "uuids": [
                "e215.0"
            ]
        },
        {
            "text": "Weak-LLM generated rejected answers were more effective for ARC (science QA) than digit-corruption, suggesting task-specific factors matter.",
            "uuids": [
                "e215.2"
            ]
        },
        {
            "text": "The theory doesn't explain why digit-corruption works better with standard DPO than with IPO/KTO/ORPO variants - suggests algorithm-specific interactions.",
            "uuids": [
                "e215.5"
            ]
        },
        {
            "text": "LastLetterConcat showed only +2.00 points improvement, much smaller than math tasks, but the theory doesn't predict this quantitative difference.",
            "uuids": [
                "e215.1"
            ]
        }
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Pang et al. (2024) PORT: Preference Optimization on Reasoning Traces [Introduces digit-corruption for preference pairs]",
            "Rafailov et al. (2023) Direct Preference Optimization [DPO framework used with digit-corruption]"
        ]
    },
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
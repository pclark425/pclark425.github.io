<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Order-Invariance Robustness Law for Graph Linearization in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1316</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1316</p>
                <p><strong>Name:</strong> Order-Invariance Robustness Law for Graph Linearization in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal textual representation for converting graphs into sequences for LLM training must be robust to permutations of node and edge order, such that the LLM's learned representations and downstream performance are invariant to arbitrary graph serialization. This order-invariance is hypothesized to maximize generalization, minimize spurious correlations, and enable transfer across graph domains.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Order-Invariance Robustness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_linearization &#8594; is_used_for &#8594; LLM_training<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph_linearization &#8594; is_order_invariant &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; learns_graph_structure &#8594; independent_of_serialization_order<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; generalizes_across_graph_domains &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Order-invariant representations in GNNs and LLMs prevent overfitting to arbitrary serialization and improve generalization. </li>
    <li>Empirical studies show that LLMs trained on order-sensitive graph linearizations can learn spurious order-based patterns. </li>
    <li>Permutation-invariant encodings are foundational in set and graph neural network literature. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law generalizes known invariance principles to the context of LLM-based graph learning, which is not yet standard practice.</p>            <p><strong>What Already Exists:</strong> Order-invariance is a well-established principle in GNNs and set representations.</p>            <p><strong>What is Novel:</strong> Its explicit application and formalization for graph-to-text linearization in LLMs is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Zaheer et al. (2017) Deep Sets [Permutation invariance in set functions]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Permutation invariance in GNNs]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Graph-to-sequence encodings]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs trained on order-invariant graph linearizations will outperform those trained on order-sensitive linearizations in tasks requiring generalization to unseen graph structures.</li>
                <li>Order-invariant representations will reduce overfitting to specific serialization patterns in graph-based LLM training.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Order-invariant linearizations may enable LLMs to transfer knowledge between graphs of different sizes or domains more effectively than order-sensitive ones.</li>
                <li>Order-invariant encodings could facilitate zero-shot or few-shot learning on novel graph types.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs trained on order-sensitive linearizations generalize as well as those trained on order-invariant ones, the law would be challenged.</li>
                <li>If order-invariant encodings do not reduce spurious correlations or improve transfer, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of partial order-invariance (e.g., invariance to node order but not edge order) is not fully explained. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends a well-known principle to a new modality (textual graph linearization for LLMs), which is not yet standard.</p>
            <p><strong>References:</strong> <ul>
    <li>Zaheer et al. (2017) Deep Sets [Permutation invariance in set functions]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Permutation invariance in GNNs]</li>
    <li>Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Graph-to-sequence encodings]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "theory_description": "This theory posits that the ideal textual representation for converting graphs into sequences for LLM training must be robust to permutations of node and edge order, such that the LLM's learned representations and downstream performance are invariant to arbitrary graph serialization. This order-invariance is hypothesized to maximize generalization, minimize spurious correlations, and enable transfer across graph domains.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Order-Invariance Robustness Law",
                "if": [
                    {
                        "subject": "graph_linearization",
                        "relation": "is_used_for",
                        "object": "LLM_training"
                    },
                    {
                        "subject": "graph_linearization",
                        "relation": "is_order_invariant",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "learns_graph_structure",
                        "object": "independent_of_serialization_order"
                    },
                    {
                        "subject": "LLM",
                        "relation": "generalizes_across_graph_domains",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Order-invariant representations in GNNs and LLMs prevent overfitting to arbitrary serialization and improve generalization.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that LLMs trained on order-sensitive graph linearizations can learn spurious order-based patterns.",
                        "uuids": []
                    },
                    {
                        "text": "Permutation-invariant encodings are foundational in set and graph neural network literature.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Order-invariance is a well-established principle in GNNs and set representations.",
                    "what_is_novel": "Its explicit application and formalization for graph-to-text linearization in LLMs is novel.",
                    "classification_explanation": "The law generalizes known invariance principles to the context of LLM-based graph learning, which is not yet standard practice.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zaheer et al. (2017) Deep Sets [Permutation invariance in set functions]",
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Permutation invariance in GNNs]",
                        "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Graph-to-sequence encodings]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs trained on order-invariant graph linearizations will outperform those trained on order-sensitive linearizations in tasks requiring generalization to unseen graph structures.",
        "Order-invariant representations will reduce overfitting to specific serialization patterns in graph-based LLM training."
    ],
    "new_predictions_unknown": [
        "Order-invariant linearizations may enable LLMs to transfer knowledge between graphs of different sizes or domains more effectively than order-sensitive ones.",
        "Order-invariant encodings could facilitate zero-shot or few-shot learning on novel graph types."
    ],
    "negative_experiments": [
        "If LLMs trained on order-sensitive linearizations generalize as well as those trained on order-invariant ones, the law would be challenged.",
        "If order-invariant encodings do not reduce spurious correlations or improve transfer, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of partial order-invariance (e.g., invariance to node order but not edge order) is not fully explained.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks may benefit from canonical or task-specific orderings (e.g., temporal graphs), which could conflict with strict order-invariance.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with inherent sequential or temporal structure may require order-sensitive encodings.",
        "Canonical orderings (e.g., BFS, DFS) may be beneficial for certain graph classes."
    ],
    "existing_theory": {
        "what_already_exists": "Order-invariance is foundational in GNNs and set representations.",
        "what_is_novel": "Its formalization and application to graph-to-text linearization for LLMs is new.",
        "classification_explanation": "The theory extends a well-known principle to a new modality (textual graph linearization for LLMs), which is not yet standard.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Zaheer et al. (2017) Deep Sets [Permutation invariance in set functions]",
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [Permutation invariance in GNNs]",
            "Ribeiro et al. (2020) Structural Encoding in Graph-to-Sequence Learning [Graph-to-sequence encodings]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-615",
    "original_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Order-Invariance Robustness Law for Graph Linearization in LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
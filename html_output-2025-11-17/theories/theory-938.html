<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Episodic-Semantic Memory Integration for LLM Text Game Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-938</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-938</p>
                <p><strong>Name:</strong> Hierarchical Episodic-Semantic Memory Integration for LLM Text Game Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory posits that LLM agents achieve optimal performance in text games by integrating hierarchical episodic and semantic memory systems. Episodic memory encodes temporally ordered sequences of game events, while semantic memory abstracts persistent world knowledge, rules, and object affordances. The agent dynamically interleaves retrieval and consolidation between these systems, enabling both context-sensitive reasoning and generalization across tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Memory Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; interacts_with &#8594; text game environment</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; encodes &#8594; episodic memory of event sequences<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; abstracts &#8594; semantic memory of world knowledge and rules</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human cognition relies on both episodic (event-based) and semantic (fact-based) memory for problem solving. </li>
    <li>AI agents with separate episodic and semantic memory modules outperform those with monolithic memory on complex reasoning tasks. </li>
    <li>Text games require both remembering specific past events (e.g., which door was opened) and general rules (e.g., keys open doors). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the separation of episodic and semantic memory is known, its structured, hierarchical application to LLM agents in text games is new.</p>            <p><strong>What Already Exists:</strong> Hierarchical memory systems are well-established in cognitive science and have been explored in some AI architectures.</p>            <p><strong>What is Novel:</strong> The explicit integration and dynamic interplay of episodic and semantic memory in LLM agents for text games is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [distinction in human memory]</li>
    <li>Weston et al. (2015) Memory Networks [episodic memory in neural networks]</li>
    <li>Ammanabrolu et al. (2020) Graph-based Memory for Interactive Fiction Games [episodic/semantic elements in memory graphs]</li>
</ul>
            <h3>Statement 1: Dynamic Memory Consolidation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; completes &#8594; significant game event or episode</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; consolidates &#8594; episodic memory into semantic memory</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human memory consolidates episodic experiences into semantic knowledge over time. </li>
    <li>AI agents that periodically consolidate memory avoid catastrophic forgetting and improve generalization. </li>
    <li>Text game agents benefit from extracting general rules from specific experiences (e.g., learning that all red keys open red doors). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The principle is known, but its application to LLM agent memory management in text games is new.</p>            <p><strong>What Already Exists:</strong> Memory consolidation is a known process in neuroscience and some AI models.</p>            <p><strong>What is Novel:</strong> Its explicit, dynamic use for integrating episodic and semantic memory in LLM agents for text games is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>McClelland et al. (1995) Why there are complementary learning systems in the hippocampus and neocortex [memory consolidation]</li>
    <li>Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [episodic/semantic blending in LMs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with explicit episodic-semantic memory integration will outperform those with only one type of memory on tasks requiring both recall of specific events and generalization.</li>
                <li>Agents that consolidate episodic experiences into semantic rules will adapt more quickly to novel but structurally similar puzzles.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>The optimal frequency and granularity of memory consolidation for maximal performance in long-horizon text games is unknown.</li>
                <li>Whether hierarchical memory integration enables transfer learning across radically different text game genres remains to be seen.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with hierarchical memory integration do not outperform flat-memory agents on tasks requiring both event recall and rule abstraction, the theory is challenged.</li>
                <li>If memory consolidation leads to loss of critical episodic details and impairs performance, the law is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of ambiguous or contradictory events on memory consolidation is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory adapts known memory principles to a new, structured context: LLM agent memory in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [human memory distinction]</li>
    <li>McClelland et al. (1995) Complementary learning systems [memory consolidation]</li>
    <li>Ammanabrolu et al. (2020) Graph-based Memory for Interactive Fiction Games [episodic/semantic elements in memory graphs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Episodic-Semantic Memory Integration for LLM Text Game Agents",
    "theory_description": "This theory posits that LLM agents achieve optimal performance in text games by integrating hierarchical episodic and semantic memory systems. Episodic memory encodes temporally ordered sequences of game events, while semantic memory abstracts persistent world knowledge, rules, and object affordances. The agent dynamically interleaves retrieval and consolidation between these systems, enabling both context-sensitive reasoning and generalization across tasks.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Memory Encoding Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "interacts_with",
                        "object": "text game environment"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "encodes",
                        "object": "episodic memory of event sequences"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "abstracts",
                        "object": "semantic memory of world knowledge and rules"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human cognition relies on both episodic (event-based) and semantic (fact-based) memory for problem solving.",
                        "uuids": []
                    },
                    {
                        "text": "AI agents with separate episodic and semantic memory modules outperform those with monolithic memory on complex reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Text games require both remembering specific past events (e.g., which door was opened) and general rules (e.g., keys open doors).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical memory systems are well-established in cognitive science and have been explored in some AI architectures.",
                    "what_is_novel": "The explicit integration and dynamic interplay of episodic and semantic memory in LLM agents for text games is novel.",
                    "classification_explanation": "While the separation of episodic and semantic memory is known, its structured, hierarchical application to LLM agents in text games is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [distinction in human memory]",
                        "Weston et al. (2015) Memory Networks [episodic memory in neural networks]",
                        "Ammanabrolu et al. (2020) Graph-based Memory for Interactive Fiction Games [episodic/semantic elements in memory graphs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Dynamic Memory Consolidation Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "completes",
                        "object": "significant game event or episode"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "consolidates",
                        "object": "episodic memory into semantic memory"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human memory consolidates episodic experiences into semantic knowledge over time.",
                        "uuids": []
                    },
                    {
                        "text": "AI agents that periodically consolidate memory avoid catastrophic forgetting and improve generalization.",
                        "uuids": []
                    },
                    {
                        "text": "Text game agents benefit from extracting general rules from specific experiences (e.g., learning that all red keys open red doors).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Memory consolidation is a known process in neuroscience and some AI models.",
                    "what_is_novel": "Its explicit, dynamic use for integrating episodic and semantic memory in LLM agents for text games is novel.",
                    "classification_explanation": "The principle is known, but its application to LLM agent memory management in text games is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "McClelland et al. (1995) Why there are complementary learning systems in the hippocampus and neocortex [memory consolidation]",
                        "Khandelwal et al. (2019) Generalization through Memorization: Nearest Neighbor Language Models [episodic/semantic blending in LMs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with explicit episodic-semantic memory integration will outperform those with only one type of memory on tasks requiring both recall of specific events and generalization.",
        "Agents that consolidate episodic experiences into semantic rules will adapt more quickly to novel but structurally similar puzzles."
    ],
    "new_predictions_unknown": [
        "The optimal frequency and granularity of memory consolidation for maximal performance in long-horizon text games is unknown.",
        "Whether hierarchical memory integration enables transfer learning across radically different text game genres remains to be seen."
    ],
    "negative_experiments": [
        "If agents with hierarchical memory integration do not outperform flat-memory agents on tasks requiring both event recall and rule abstraction, the theory is challenged.",
        "If memory consolidation leads to loss of critical episodic details and impairs performance, the law is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of ambiguous or contradictory events on memory consolidation is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLM agents perform well on simple text games using only short-term context, suggesting episodic-semantic integration may not always be necessary.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Games with highly repetitive or random structure may not benefit from semantic abstraction.",
        "Very short games may not require episodic memory beyond the immediate context."
    ],
    "existing_theory": {
        "what_already_exists": "Hierarchical memory systems and consolidation are established in cognitive science and some AI models.",
        "what_is_novel": "Their explicit, dynamic integration in LLM agent memory for text games is novel.",
        "classification_explanation": "The theory adapts known memory principles to a new, structured context: LLM agent memory in text games.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [human memory distinction]",
            "McClelland et al. (1995) Complementary learning systems [memory consolidation]",
            "Ammanabrolu et al. (2020) Graph-based Memory for Interactive Fiction Games [episodic/semantic elements in memory graphs]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-591",
    "original_theory_name": "Task-Structure-Dependent Memory Utility Law for LLM Agents",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1928</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1928</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the format in which a problem is presented to a large language model (LLM) modulates the alignment between the model's internal cognitive representations and the intended task, thereby affecting performance. Specifically, formats that more closely match the model's pretraining distribution or that reduce ambiguity in task intent lead to higher performance, while formats that are misaligned or ambiguous reduce performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Distribution Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; closely_matches &#8594; LLM_pretraining_distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_maximized &#8594; on_given_task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on tasks presented in formats similar to their pretraining data (e.g., Q&A, chat, code). </li>
    <li>Performance drops when tasks are presented in unfamiliar or adversarial formats. </li>
    <li>Prompt engineering studies show that reformatting prompts to match pretraining distributions (e.g., using 'Q:' and 'A:' or conversational turns) increases accuracy. </li>
    <li>Instruction-tuned LLMs show less sensitivity to format, but still perform best on familiar formats. </li>
    <li>Chain-of-thought prompting, which mimics step-by-step reasoning found in some pretraining data, improves performance on reasoning tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt engineering is well-studied, the explicit framing of performance as a function of distributional alignment is novel.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that LLMs are sensitive to prompt format and that performance can be improved by prompt engineering.</p>            <p><strong>What is Novel:</strong> This law formalizes the relationship as a conditional alignment between presentation format and pretraining distribution, and predicts maximal performance at high alignment.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format sensitivity]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]</li>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and format effects]</li>
</ul>
            <h3>Statement 1: Ambiguity Reduction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; minimizes &#8594; task_ambiguity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_increased &#8594; on_given_task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Explicit instructions and clear task boundaries improve LLM accuracy. </li>
    <li>Ambiguous or underspecified prompts lead to inconsistent or incorrect outputs. </li>
    <li>Step-by-step prompting (e.g., 'Let's think step by step') reduces ambiguity and increases reasoning accuracy. </li>
    <li>Few-shot examples that clarify task intent reduce ambiguity and improve performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law extends existing prompt clarity findings to a broader, more formal principle.</p>            <p><strong>What Already Exists:</strong> Prompt clarity and explicitness are known to improve LLM outputs.</p>            <p><strong>What is Novel:</strong> This law generalizes the effect to all forms of ambiguity, not just instruction clarity, and frames it as a necessary condition for optimal performance.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise reduction of ambiguity]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt explicitness effects]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Ambiguity reduction via stepwise reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a math problem is presented in a format identical to textbook examples in the LLM's pretraining data, performance will be higher than if presented in a novel or unusual format.</li>
                <li>Rewriting ambiguous prompts to be more explicit will consistently improve LLM accuracy across tasks.</li>
                <li>Instruction-tuned LLMs will still show a performance boost when the format matches pretraining data, but the effect size will be smaller than for base LLMs.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a new, synthetic format is created that is highly aligned with the LLM's internal representations but not present in pretraining data, performance may exceed that of natural formats.</li>
                <li>If ambiguity is reduced to zero but the format is highly unfamiliar, will performance still be high, or does distributional familiarity dominate?</li>
                <li>For LLMs with extensive meta-learning or adaptation, does format alignment remain a primary driver of performance?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Presenting a problem in a highly familiar format but with high ambiguity should not result in high performance; if it does, the theory is challenged.</li>
                <li>If LLMs perform equally well on all formats regardless of alignment or ambiguity, the theory is falsified.</li>
                <li>If LLMs perform better on ambiguous prompts than on clear, explicit ones, the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs perform well on unfamiliar formats due to emergent generalization capabilities. </li>
    <li>Instances where LLMs fail on familiar formats due to adversarial content or subtle misdirection. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and extends prompt engineering findings into a more general, predictive framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format sensitivity]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and ambiguity reduction]</li>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and format effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that the format in which a problem is presented to a large language model (LLM) modulates the alignment between the model's internal cognitive representations and the intended task, thereby affecting performance. Specifically, formats that more closely match the model's pretraining distribution or that reduce ambiguity in task intent lead to higher performance, while formats that are misaligned or ambiguous reduce performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Distribution Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "closely_matches",
                        "object": "LLM_pretraining_distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_maximized",
                        "object": "on_given_task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on tasks presented in formats similar to their pretraining data (e.g., Q&A, chat, code).",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops when tasks are presented in unfamiliar or adversarial formats.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering studies show that reformatting prompts to match pretraining distributions (e.g., using 'Q:' and 'A:' or conversational turns) increases accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction-tuned LLMs show less sensitivity to format, but still perform best on familiar formats.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought prompting, which mimics step-by-step reasoning found in some pretraining data, improves performance on reasoning tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that LLMs are sensitive to prompt format and that performance can be improved by prompt engineering.",
                    "what_is_novel": "This law formalizes the relationship as a conditional alignment between presentation format and pretraining distribution, and predicts maximal performance at high alignment.",
                    "classification_explanation": "While prompt engineering is well-studied, the explicit framing of performance as a function of distributional alignment is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format sensitivity]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]",
                        "Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and format effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Ambiguity Reduction Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "minimizes",
                        "object": "task_ambiguity"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_increased",
                        "object": "on_given_task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Explicit instructions and clear task boundaries improve LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or underspecified prompts lead to inconsistent or incorrect outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Step-by-step prompting (e.g., 'Let's think step by step') reduces ambiguity and increases reasoning accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Few-shot examples that clarify task intent reduce ambiguity and improve performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt clarity and explicitness are known to improve LLM outputs.",
                    "what_is_novel": "This law generalizes the effect to all forms of ambiguity, not just instruction clarity, and frames it as a necessary condition for optimal performance.",
                    "classification_explanation": "The law extends existing prompt clarity findings to a broader, more formal principle.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and stepwise reduction of ambiguity]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt explicitness effects]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Ambiguity reduction via stepwise reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a math problem is presented in a format identical to textbook examples in the LLM's pretraining data, performance will be higher than if presented in a novel or unusual format.",
        "Rewriting ambiguous prompts to be more explicit will consistently improve LLM accuracy across tasks.",
        "Instruction-tuned LLMs will still show a performance boost when the format matches pretraining data, but the effect size will be smaller than for base LLMs."
    ],
    "new_predictions_unknown": [
        "If a new, synthetic format is created that is highly aligned with the LLM's internal representations but not present in pretraining data, performance may exceed that of natural formats.",
        "If ambiguity is reduced to zero but the format is highly unfamiliar, will performance still be high, or does distributional familiarity dominate?",
        "For LLMs with extensive meta-learning or adaptation, does format alignment remain a primary driver of performance?"
    ],
    "negative_experiments": [
        "Presenting a problem in a highly familiar format but with high ambiguity should not result in high performance; if it does, the theory is challenged.",
        "If LLMs perform equally well on all formats regardless of alignment or ambiguity, the theory is falsified.",
        "If LLMs perform better on ambiguous prompts than on clear, explicit ones, the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs perform well on unfamiliar formats due to emergent generalization capabilities.",
            "uuids": []
        },
        {
            "text": "Instances where LLMs fail on familiar formats due to adversarial content or subtle misdirection.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show LLMs can generalize to new formats with minimal performance loss after a few examples (few-shot adaptation).",
            "uuids": []
        },
        {
            "text": "Instruction-tuned LLMs sometimes outperform base LLMs even on unfamiliar formats.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with extensive instruction tuning may be less sensitive to format alignment.",
        "Tasks with inherently low ambiguity may be less affected by presentation format.",
        "Highly adversarial or misleading prompts may reduce performance even if format is familiar."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and instruction clarity are known to affect LLM performance.",
        "what_is_novel": "The explicit formalization of performance as a function of cognitive alignment and ambiguity reduction is new.",
        "classification_explanation": "The theory synthesizes and extends prompt engineering findings into a more general, predictive framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format sensitivity]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format effects]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt clarity and ambiguity reduction]",
            "Ouyang et al. (2022) Training language models to follow instructions with human feedback [Instruction tuning and format effects]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-655",
    "original_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
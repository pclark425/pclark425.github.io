<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM-Driven Curriculum Superiority Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-176</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-176</p>
                <p><strong>Name:</strong> LLM-Driven Curriculum Superiority Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about optimal curricula for compositional acquisition of commonsense and science procedures in interactive text environments, based on the following results.</p>
                <p><strong>Description:</strong> Large language models can generate more effective automatic curricula than hand-designed or simple heuristic curricula by leveraging world knowledge to propose contextually appropriate, novelty-seeking tasks that adapt to agent state and exploration progress. The effectiveness depends on: (1) the LLM's ability to condition on live agent state (inventory, location, prior successes/failures), (2) mechanisms to ensure task feasibility and diversity (e.g., novelty bias, self-verification), (3) complementary systems like skill libraries or execution monitoring, and (4) domain characteristics (complexity, task diversity, alignment with LLM pretraining). LLM curricula excel in open-ended domains with diverse compositional tasks but may struggle in domains requiring long-horizon planning, specialized knowledge, or navigation complexity not present in LLM pretraining data.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <ol>
                <li><a href="../evaluations/theory-evaluation-22.html">theory-evaluation-22</a></li>
            </ol>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>LLM-generated curricula provide 3-10× more task discoveries than random curricula in open-ended domains with diverse compositional tasks.</li>
                <li>LLM-generated curricula unlock tech-tree milestones 6-15× faster than baseline approaches by discovering stepping-stone tasks.</li>
                <li>LLMs can propose contextually appropriate tasks by conditioning on agent state (inventory, location, prior successes/failures, environmental context) that simple heuristics cannot match.</li>
                <li>The diversity of LLM-generated tasks (measured by unique task types or goal descriptions) is substantially higher than hand-designed curricula in open-ended domains.</li>
                <li>LLM curricula are most effective when combined with: (1) self-verification mechanisms, (2) novelty bias to prevent exploitation, (3) skill libraries or execution monitoring, and (4) progressive context expansion (warm-up schedules).</li>
                <li>The benefit of LLM curricula increases with domain complexity and task diversity - simple domains see 2-3× gains, complex open-ended domains see 5-15× gains in discovery speed.</li>
                <li>LLM curricula enable zero-shot transfer to novel task compositions by discovering diverse intermediate skills during curriculum execution.</li>
                <li>Manual curricula can match or exceed LLM curricula when: (1) domain is highly specialized with knowledge not in LLM pretraining, (2) task structure is simple and well-understood, or (3) safety constraints require extensive validation.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Voyager's GPT-4 automatic curriculum discovers 63 unique items (3.3× more than baselines) and unlocks tech-tree milestones 6.4-15.3× faster (wooden tools 15.3×, stone 8.5×, iron 6.4× faster) in Minecraft. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>Replacing Voyager's automatic curriculum with random curriculum reduces discovered items by 93%; manually designed curriculum performs worse than automatic curriculum. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>Voyager achieves 3/3 success on several unseen tasks with 18-21 prompting iterations (Diamond Pickaxe: 19±3, Golden Sword: 18±7, Lava Bucket: 21±5, Compass: 18±2) using zero-shot transfer with skill library and curriculum-based task decomposition. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>Voyager's automatic curriculum uses GPT-4 with explicit directives for diversity and achievability, conditioning on agent state (inventory, equipment, nearby blocks/entities, biome, time, health/hunger, position), completed/failed tasks, and GPT-3.5 self-ask context from wiki. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>Voyager implements a warm-up schedule that progressively increases prompt context (nearby entities, full inventory, biome, health/hunger/time) as agent completes tasks, scaffolding increasing complexity safely. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>ELLM (LLM-guided pretraining) unlocks ~6 ground-truth achievements per episode vs <3 for novelty baselines (APT, RND, Novelty) in Crafter by biasing toward human-meaningful, context-sensitive goals. <a href="../results/extraction-result-1587.html#e1587.0" class="evidence-link">[e1587.0]</a> </li>
    <li>ELLM with novelty bias (reward each suggested goal only once per episode) is essential to avoid repeated exploitation of easy goals; goal-conditioning aids downstream tasks but is not necessary for pretraining gains. <a href="../results/extraction-result-1587.html#e1587.0" class="evidence-link">[e1587.0]</a> </li>
    <li>ELLM (guided exploration variant) matches or outperforms baselines on seven downstream Crafter tasks and is the only method with nonzero performance across all tasks. <a href="../results/extraction-result-1587.html#e1587.0" class="evidence-link">[e1587.0]</a> </li>
    <li>LMA3 with LLM Goal Generator discovers much larger and more diverse goal sets than hardcoded 69-goal baseline oracle, demonstrating autonomous discovery of linguistically diverse goals without hand-coded representations. <a href="../results/extraction-result-1607.html#e1607.0" class="evidence-link">[e1607.0]</a> <a href="../results/extraction-result-1607.html#e1607.2" class="evidence-link">[e1607.2]</a> </li>
    <li>LMA3 uses bootstrapping phase (4000 episodes uniform sampling from discovered goals) before enabling LLM Goal Generator, providing simple building blocks for later LM-driven composition. <a href="../results/extraction-result-1607.html#e1607.1" class="evidence-link">[e1607.1]</a> </li>
    <li>STARLING (LLM-generated pretraining games) achieves mean normalized score 0.72±0.063 on held-out generated games vs 0.050±0.015 for random baseline, and improves early learning on target benchmarks (TWC, ScienceWorld). <a href="../results/extraction-result-1482.html#e1482.0" class="evidence-link">[e1482.0]</a> </li>
    <li>STARLING with ~200K params outperforms larger LLM-based baselines on several ScienceWorld tasks, indicating targeted auxiliary-task pretraining can be efficient. <a href="../results/extraction-result-1482.html#e1482.0" class="evidence-link">[e1482.0]</a> </li>
    <li>Voyager's skill library (produced under automatic curriculum) improves other agents (AutoGPT + skill library shows gains), indicating transferability of learned procedural skills. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM-generated curricula will outperform manual curricula by 2-5× in discovery speed for other open-ended domains with compositional structure (e.g., creative construction tasks, social simulation, resource management games).</li>
                <li>Combining multiple LLMs (e.g., GPT-4 for task generation, separate model for difficulty estimation and feasibility checking) will provide 10-20% gains over single-LLM curricula by reducing infeasible task proposals.</li>
                <li>LLM curricula with explicit diversity objectives (e.g., 'discover as many diverse things as possible') will discover 2-3× more unique task types than LLM curricula with generic objectives.</li>
                <li>Increasing LLM context window to include more agent history and environmental state will improve curriculum quality by 15-30% in complex domains.</li>
                <li>LLM curricula will show 20-40% better transfer to novel task compositions than fixed curricula by discovering more diverse intermediate skills during training.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether LLM-generated curricula can match or exceed human expert curricula in highly specialized domains requiring deep technical knowledge (e.g., advanced mathematics, professional medical procedures, complex engineering tasks).</li>
                <li>Whether the benefits of LLM curricula plateau with model size (e.g., GPT-4 level), or continue to improve substantially with larger/more capable models (e.g., 10× larger models).</li>
                <li>Whether LLM curricula can effectively handle domains with complex safety constraints, ethical considerations, or adversarial dynamics without extensive human oversight.</li>
                <li>Whether LLM-generated curricula can discover fundamentally novel problem-solving strategies not present in their training data, or are limited to recombining known patterns.</li>
                <li>Whether the computational cost of LLM curriculum generation (API calls, inference time) can be reduced to match or beat the sample efficiency gains in wall-clock time for practical deployment.</li>
                <li>Whether LLM curricula can adapt effectively to non-standard physics, novel game mechanics, or abstract rule systems that differ significantly from real-world domains in LLM training data.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where simple heuristic curricula (e.g., random novelty search, count-based exploration) consistently match LLM performance would suggest the LLM's world knowledge is not essential for curriculum generation.</li>
                <li>Demonstrating that LLM curricula fail systematically in domains with non-standard physics or rules (e.g., abstract puzzle games, novel game mechanics) would reveal important limitations about reliance on world knowledge.</li>
                <li>Showing that manually designed curricula by domain experts consistently outperform LLM curricula in specialized technical domains would challenge the superiority claim.</li>
                <li>Finding that LLM curriculum benefits disappear when controlling for computational cost (e.g., using equivalent compute for more random exploration) would question practical utility.</li>
                <li>Demonstrating that LLM-generated tasks are frequently infeasible or poorly specified (>30% failure rate) without extensive validation would limit practical applicability.</li>
                <li>Showing that smaller, fine-tuned models match GPT-4 curriculum performance would suggest general world knowledge is not the key factor.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The computational cost of LLM curriculum generation (API calls, inference time) and whether it outweighs sample efficiency gains in wall-clock time for practical deployment. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>How to systematically handle cases where LLM-generated tasks are infeasible, poorly specified, or violate domain constraints without extensive human validation. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>The exact mechanisms by which LLMs balance exploration (novelty-seeking) vs exploitation (achievability) in curriculum generation, and how to tune this balance. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> <a href="../results/extraction-result-1587.html#e1587.0" class="evidence-link">[e1587.0]</a> </li>
    <li>Whether LLM curriculum benefits scale linearly, sub-linearly, or super-linearly with model size and capability. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> </li>
    <li>How LLM curricula interact with different base learning algorithms (e.g., PPO vs DQN vs imitation learning) and whether benefits are algorithm-dependent. <a href="../results/extraction-result-1498.html#e1498.0" class="evidence-link">[e1498.0]</a> <a href="../results/extraction-result-1587.html#e1587.0" class="evidence-link">[e1587.0]</a> <a href="../results/extraction-result-1607.html#e1607.0" class="evidence-link">[e1607.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Primary source for GPT-4-driven automatic curriculum in Minecraft]</li>
    <li>Du et al. (2023) Guiding Pretraining in Reinforcement Learning with Large Language Models [ELLM: LLM-guided exploration and pretraining]</li>
    <li>Colas et al. (2023) Augmenting Autotelic Agents with Large Language Models [LMA3: LLM goal generation for autotelic learning]</li>
    <li>Singh et al. (2024) STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models [LLM-generated pretraining games]</li>
    <li>Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Related work on LLM-guided robot learning, but focused on action grounding rather than curriculum generation]</li>
    <li>Baranes & Oudeyer (2013) Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots [Earlier work on automatic curriculum via intrinsic motivation, but without LLMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "LLM-Driven Curriculum Superiority Theory",
    "theory_description": "Large language models can generate more effective automatic curricula than hand-designed or simple heuristic curricula by leveraging world knowledge to propose contextually appropriate, novelty-seeking tasks that adapt to agent state and exploration progress. The effectiveness depends on: (1) the LLM's ability to condition on live agent state (inventory, location, prior successes/failures), (2) mechanisms to ensure task feasibility and diversity (e.g., novelty bias, self-verification), (3) complementary systems like skill libraries or execution monitoring, and (4) domain characteristics (complexity, task diversity, alignment with LLM pretraining). LLM curricula excel in open-ended domains with diverse compositional tasks but may struggle in domains requiring long-horizon planning, specialized knowledge, or navigation complexity not present in LLM pretraining data.",
    "supporting_evidence": [
        {
            "text": "Voyager's GPT-4 automatic curriculum discovers 63 unique items (3.3× more than baselines) and unlocks tech-tree milestones 6.4-15.3× faster (wooden tools 15.3×, stone 8.5×, iron 6.4× faster) in Minecraft.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "Replacing Voyager's automatic curriculum with random curriculum reduces discovered items by 93%; manually designed curriculum performs worse than automatic curriculum.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "Voyager achieves 3/3 success on several unseen tasks with 18-21 prompting iterations (Diamond Pickaxe: 19±3, Golden Sword: 18±7, Lava Bucket: 21±5, Compass: 18±2) using zero-shot transfer with skill library and curriculum-based task decomposition.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "Voyager's automatic curriculum uses GPT-4 with explicit directives for diversity and achievability, conditioning on agent state (inventory, equipment, nearby blocks/entities, biome, time, health/hunger, position), completed/failed tasks, and GPT-3.5 self-ask context from wiki.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "Voyager implements a warm-up schedule that progressively increases prompt context (nearby entities, full inventory, biome, health/hunger/time) as agent completes tasks, scaffolding increasing complexity safely.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "ELLM (LLM-guided pretraining) unlocks ~6 ground-truth achievements per episode vs &lt;3 for novelty baselines (APT, RND, Novelty) in Crafter by biasing toward human-meaningful, context-sensitive goals.",
            "uuids": [
                "e1587.0"
            ]
        },
        {
            "text": "ELLM with novelty bias (reward each suggested goal only once per episode) is essential to avoid repeated exploitation of easy goals; goal-conditioning aids downstream tasks but is not necessary for pretraining gains.",
            "uuids": [
                "e1587.0"
            ]
        },
        {
            "text": "ELLM (guided exploration variant) matches or outperforms baselines on seven downstream Crafter tasks and is the only method with nonzero performance across all tasks.",
            "uuids": [
                "e1587.0"
            ]
        },
        {
            "text": "LMA3 with LLM Goal Generator discovers much larger and more diverse goal sets than hardcoded 69-goal baseline oracle, demonstrating autonomous discovery of linguistically diverse goals without hand-coded representations.",
            "uuids": [
                "e1607.0",
                "e1607.2"
            ]
        },
        {
            "text": "LMA3 uses bootstrapping phase (4000 episodes uniform sampling from discovered goals) before enabling LLM Goal Generator, providing simple building blocks for later LM-driven composition.",
            "uuids": [
                "e1607.1"
            ]
        },
        {
            "text": "STARLING (LLM-generated pretraining games) achieves mean normalized score 0.72±0.063 on held-out generated games vs 0.050±0.015 for random baseline, and improves early learning on target benchmarks (TWC, ScienceWorld).",
            "uuids": [
                "e1482.0"
            ]
        },
        {
            "text": "STARLING with ~200K params outperforms larger LLM-based baselines on several ScienceWorld tasks, indicating targeted auxiliary-task pretraining can be efficient.",
            "uuids": [
                "e1482.0"
            ]
        },
        {
            "text": "Voyager's skill library (produced under automatic curriculum) improves other agents (AutoGPT + skill library shows gains), indicating transferability of learned procedural skills.",
            "uuids": [
                "e1498.0"
            ]
        }
    ],
    "theory_statements": [
        "LLM-generated curricula provide 3-10× more task discoveries than random curricula in open-ended domains with diverse compositional tasks.",
        "LLM-generated curricula unlock tech-tree milestones 6-15× faster than baseline approaches by discovering stepping-stone tasks.",
        "LLMs can propose contextually appropriate tasks by conditioning on agent state (inventory, location, prior successes/failures, environmental context) that simple heuristics cannot match.",
        "The diversity of LLM-generated tasks (measured by unique task types or goal descriptions) is substantially higher than hand-designed curricula in open-ended domains.",
        "LLM curricula are most effective when combined with: (1) self-verification mechanisms, (2) novelty bias to prevent exploitation, (3) skill libraries or execution monitoring, and (4) progressive context expansion (warm-up schedules).",
        "The benefit of LLM curricula increases with domain complexity and task diversity - simple domains see 2-3× gains, complex open-ended domains see 5-15× gains in discovery speed.",
        "LLM curricula enable zero-shot transfer to novel task compositions by discovering diverse intermediate skills during curriculum execution.",
        "Manual curricula can match or exceed LLM curricula when: (1) domain is highly specialized with knowledge not in LLM pretraining, (2) task structure is simple and well-understood, or (3) safety constraints require extensive validation."
    ],
    "new_predictions_likely": [
        "LLM-generated curricula will outperform manual curricula by 2-5× in discovery speed for other open-ended domains with compositional structure (e.g., creative construction tasks, social simulation, resource management games).",
        "Combining multiple LLMs (e.g., GPT-4 for task generation, separate model for difficulty estimation and feasibility checking) will provide 10-20% gains over single-LLM curricula by reducing infeasible task proposals.",
        "LLM curricula with explicit diversity objectives (e.g., 'discover as many diverse things as possible') will discover 2-3× more unique task types than LLM curricula with generic objectives.",
        "Increasing LLM context window to include more agent history and environmental state will improve curriculum quality by 15-30% in complex domains.",
        "LLM curricula will show 20-40% better transfer to novel task compositions than fixed curricula by discovering more diverse intermediate skills during training."
    ],
    "new_predictions_unknown": [
        "Whether LLM-generated curricula can match or exceed human expert curricula in highly specialized domains requiring deep technical knowledge (e.g., advanced mathematics, professional medical procedures, complex engineering tasks).",
        "Whether the benefits of LLM curricula plateau with model size (e.g., GPT-4 level), or continue to improve substantially with larger/more capable models (e.g., 10× larger models).",
        "Whether LLM curricula can effectively handle domains with complex safety constraints, ethical considerations, or adversarial dynamics without extensive human oversight.",
        "Whether LLM-generated curricula can discover fundamentally novel problem-solving strategies not present in their training data, or are limited to recombining known patterns.",
        "Whether the computational cost of LLM curriculum generation (API calls, inference time) can be reduced to match or beat the sample efficiency gains in wall-clock time for practical deployment.",
        "Whether LLM curricula can adapt effectively to non-standard physics, novel game mechanics, or abstract rule systems that differ significantly from real-world domains in LLM training data."
    ],
    "negative_experiments": [
        "Finding domains where simple heuristic curricula (e.g., random novelty search, count-based exploration) consistently match LLM performance would suggest the LLM's world knowledge is not essential for curriculum generation.",
        "Demonstrating that LLM curricula fail systematically in domains with non-standard physics or rules (e.g., abstract puzzle games, novel game mechanics) would reveal important limitations about reliance on world knowledge.",
        "Showing that manually designed curricula by domain experts consistently outperform LLM curricula in specialized technical domains would challenge the superiority claim.",
        "Finding that LLM curriculum benefits disappear when controlling for computational cost (e.g., using equivalent compute for more random exploration) would question practical utility.",
        "Demonstrating that LLM-generated tasks are frequently infeasible or poorly specified (&gt;30% failure rate) without extensive validation would limit practical applicability.",
        "Showing that smaller, fine-tuned models match GPT-4 curriculum performance would suggest general world knowledge is not the key factor."
    ],
    "unaccounted_for": [
        {
            "text": "The computational cost of LLM curriculum generation (API calls, inference time) and whether it outweighs sample efficiency gains in wall-clock time for practical deployment.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "How to systematically handle cases where LLM-generated tasks are infeasible, poorly specified, or violate domain constraints without extensive human validation.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "The exact mechanisms by which LLMs balance exploration (novelty-seeking) vs exploitation (achievability) in curriculum generation, and how to tune this balance.",
            "uuids": [
                "e1498.0",
                "e1587.0"
            ]
        },
        {
            "text": "Whether LLM curriculum benefits scale linearly, sub-linearly, or super-linearly with model size and capability.",
            "uuids": [
                "e1498.0"
            ]
        },
        {
            "text": "How LLM curricula interact with different base learning algorithms (e.g., PPO vs DQN vs imitation learning) and whether benefits are algorithm-dependent.",
            "uuids": [
                "e1498.0",
                "e1587.0",
                "e1607.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "STARLING (LLM-generated pretraining) struggles on tasks requiring long-horizon navigation and planning because pretraining games lack navigational complexity, showing LLM curricula may not transfer well to task types underrepresented in generated curriculum.",
            "uuids": [
                "e1482.0"
            ]
        },
        {
            "text": "STARLING tends to collect nearby bonus rewards in Zork1 instead of pursuing larger distant rewards, suggesting LLM-generated curricula may bias toward short-term achievable goals over long-horizon planning.",
            "uuids": [
                "e1482.0"
            ]
        },
        {
            "text": "Manual/fixed curricula sometimes match or slightly exceed automatic curricula on specific engineered targets (e.g., FC in IMGEP experiments), suggesting hand-designed curricula can be competitive when domain structure is well-understood.",
            "uuids": [
                "e1613.0",
                "e1613.3"
            ]
        }
    ],
    "special_cases": [
        "For domains with very simple, well-understood task structures (e.g., single-room navigation), the overhead of LLM curriculum generation may not be justified compared to simple heuristics.",
        "When LLMs lack relevant world knowledge (e.g., highly specialized technical domains, novel game mechanics, non-standard physics), manual curricula designed by domain experts may be superior.",
        "In safety-critical domains (e.g., medical procedures, autonomous vehicles), LLM-generated curricula require extensive validation and may not be deployable without human oversight.",
        "For domains where task feasibility is difficult to verify automatically, LLM curricula may generate many infeasible tasks, reducing effectiveness.",
        "When computational budget is severely limited, the cost of LLM inference may outweigh sample efficiency gains, making simpler curricula more practical.",
        "In domains requiring long-horizon planning or complex navigation not well-represented in LLM training data, LLM curricula may underperform on those specific task types.",
        "For small-scale problems or short training runs, the benefits of LLM curricula may not materialize before training completes."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Primary source for GPT-4-driven automatic curriculum in Minecraft]",
            "Du et al. (2023) Guiding Pretraining in Reinforcement Learning with Large Language Models [ELLM: LLM-guided exploration and pretraining]",
            "Colas et al. (2023) Augmenting Autotelic Agents with Large Language Models [LMA3: LLM goal generation for autotelic learning]",
            "Singh et al. (2024) STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models [LLM-generated pretraining games]",
            "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Related work on LLM-guided robot learning, but focused on action grounding rather than curriculum generation]",
            "Baranes & Oudeyer (2013) Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots [Earlier work on automatic curriculum via intrinsic motivation, but without LLMs]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 5,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-638</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-638</p>
                <p><strong>Name:</strong> Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains, based on the following results.</p>
                <p><strong>Description:</strong> In molecular property prediction tasks using LLM-based simulators, accuracy is maximized when few-shot demonstrations are selected by molecular structure similarity (e.g., MACCS or MACCS-ECFP4 fingerprints) and ordered by descending similarity, with a hybrid mix of zero-shot and few-shot instruction tuning. Including more than two demonstrations or using less similar examples degrades performance, and providing neighbor labels can induce shortcut learning that harms zero-shot generalization. This theory is specific to LLM-based molecular property prediction and is supported by extensive ablation and benchmarking in the MolecularGPT work.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structure-Aware Demonstration Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; few-shot demonstrations &#8594; are_selected_by &#8594; molecular_structure_similarity (e.g., MACCS or MACCS-ECFP4 fingerprints)<span style="color: #888888;">, and</span></div>
        <div>&#8226; demonstrations &#8594; are_ordered_by &#8594; descending_similarity<span style="color: #888888;">, and</span></div>
        <div>&#8226; number_of_demonstrations &#8594; is_less_than_or_equal_to &#8594; 2</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves_maximum_accuracy_on &#8594; molecular_property_prediction</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MolecularGPT achieves best performance with 1-2 high-similarity demonstrations, descending order, and hybrid instruction tuning; more demonstrations or less similar examples degrade performance. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
    <li>Empirical ablations show that increasing the number of demonstrations beyond 2 does not further improve and can degrade performance; descending similarity order outperforms ascending or random order. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
    <li>MACCS-ECFP4 fingerprint-based retrieval improved performance on several tasks compared to MACCS alone. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
    <li>Hybrid instruction tuning (mix of 0-shot and k-shot templates) improves both zero- and few-shot capabilities. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
    <li>Structure-aware demonstration retrieval outperforms random or diversity-based retrieval for molecular property prediction. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While few-shot and similarity-based retrieval are known, the precise quantitative and qualitative law for demonstration selection, ordering, and number in LLM-based molecular property prediction is new and not previously formalized.</p>            <p><strong>What Already Exists:</strong> Few-shot learning and similarity-based retrieval are established in ML, and structure-based retrieval is used in cheminformatics, but the specific law for optimal demonstration number, selection, and ordering in LLM-based molecular property prediction is not previously formalized.</p>            <p><strong>What is Novel:</strong> This law quantifies the optimal number, selection, and ordering of demonstrations for LLM-based molecular property prediction, and demonstrates the superiority of structure-aware retrieval and hybrid instruction tuning.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [general few-shot learning, not molecular-specific]</li>
</ul>
            <h3>Statement 1: Neighbor Label Shortcut Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; few-shot demonstrations &#8594; include &#8594; neighbor_labels</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; improves_few_shot_accuracy &#8594; but_degrades_zero_shot_generalization</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Including neighbor labels in demonstrations improves in-context learning (ICL) but introduces shortcut learning, harming zero-shot performance. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
    <li>Empirical ablations show that neighbor label inclusion can lead to shortcut learning, which degrades zero-shot generalization even as few-shot accuracy increases. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> Shortcut learning is known, but the specific effect of neighbor label inclusion in LLM-based molecular property prediction is new.</p>            <p><strong>What Already Exists:</strong> Shortcut learning is a known phenomenon in ML, but its specific manifestation in LLM-based molecular property prediction via neighbor label inclusion is novel.</p>            <p><strong>What is Novel:</strong> This law identifies and predicts the tradeoff between ICL gains and zero-shot generalization loss due to neighbor label inclusion in molecular property prediction.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]</li>
</ul>
            <h3>Statement 2: Hybrid Instruction Tuning Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; instruction_tuning_dataset &#8594; is_hybrid_mix_of &#8594; zero-shot_and_few-shot_templates</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves_balanced_performance_on &#8594; zero-shot_and_few-shot_molecular_property_prediction</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Hybrid instruction tuning (mix of 0-shot and k-shot templates) improves both zero- and few-shot performance, as shown in ablation studies. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
    <li>Scaling the instruction dataset size (doubling) further improves performance. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> Hybrid instruction tuning is not previously formalized for this domain.</p>            <p><strong>What Already Exists:</strong> Instruction tuning is known in LLMs, but the specific benefit of hybrid zero- and few-shot mixes for molecular property prediction is novel.</p>            <p><strong>What is Novel:</strong> This law formalizes the benefit of hybrid instruction tuning for balancing zero- and few-shot performance in LLM-based molecular property prediction.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]</li>
</ul>
            <h3>Statement 3: Demonstration Order Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; demonstrations &#8594; are_ordered_by &#8594; descending_similarity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves_higher_accuracy_than &#8594; ascending_or_random_order</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Descending similarity order of demonstrations outperforms ascending or random order, especially with more than one demonstration. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> Order effects are known, but this specific law is new for this domain.</p>            <p><strong>What Already Exists:</strong> Order effects in few-shot learning are known, but the specific benefit of descending similarity order in molecular property prediction is new.</p>            <p><strong>What is Novel:</strong> This law formalizes the optimal ordering of demonstrations for LLM-based molecular property prediction.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new molecular property prediction task is attempted with more than two demonstrations or with demonstrations not selected by structure similarity, accuracy will decrease compared to using 1-2 high-similarity demonstrations.</li>
                <li>If neighbor labels are included in demonstrations, zero-shot performance will drop even if few-shot accuracy increases.</li>
                <li>If demonstrations are ordered by ascending similarity or randomly, performance will be lower than with descending similarity order.</li>
                <li>If the instruction tuning dataset is not hybrid (e.g., only zero-shot or only few-shot), performance will be suboptimal on at least one of zero-shot or few-shot tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a hybrid retrieval strategy (combining structure similarity and property similarity) is used, it may further improve accuracy, especially for properties with complex determinants.</li>
                <li>If demonstrations are selected by 3D geometric similarity rather than SMILES-based fingerprints, performance may improve for geometry-sensitive properties.</li>
                <li>If structure-aware demonstration retrieval is combined with active learning (selecting most informative examples), further gains may be possible.</li>
                <li>If the LLM is extended to include 3D molecular representations or graph-based embeddings, the optimal number and selection of demonstrations may change.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If more than two demonstrations selected by random or low similarity improve accuracy, the theory would be falsified.</li>
                <li>If including neighbor labels does not degrade zero-shot generalization, the theory would be challenged.</li>
                <li>If ascending or random order of demonstrations outperforms descending similarity order, the theory would be challenged.</li>
                <li>If hybrid instruction tuning does not outperform single-mode tuning (zero-shot or few-shot only), the theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Tasks where 3D geometry is critical may not be fully addressed by SMILES-based similarity; the theory does not account for cases where 3D structure dominates property determination. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
    <li>The theory does not address regression tasks where numerical generation remains challenging and gaps exist compared to supervised GNNs on some regression tasks. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
    <li>The theory does not account for input length constraints (max 512 tokens) that may limit the number of demonstrations in practice. <a href="../results/extraction-result-5680.html#e5680.0" class="evidence-link">[e5680.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No prior work formalizes these laws in this context; the theory is new and specific to LLM-based molecular property prediction.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [general few-shot learning]</li>
    <li>Geirhos et al. (2020) Shortcut Learning in Deep Neural Networks [shortcut learning, general ML]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "theory_description": "In molecular property prediction tasks using LLM-based simulators, accuracy is maximized when few-shot demonstrations are selected by molecular structure similarity (e.g., MACCS or MACCS-ECFP4 fingerprints) and ordered by descending similarity, with a hybrid mix of zero-shot and few-shot instruction tuning. Including more than two demonstrations or using less similar examples degrades performance, and providing neighbor labels can induce shortcut learning that harms zero-shot generalization. This theory is specific to LLM-based molecular property prediction and is supported by extensive ablation and benchmarking in the MolecularGPT work.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structure-Aware Demonstration Law",
                "if": [
                    {
                        "subject": "few-shot demonstrations",
                        "relation": "are_selected_by",
                        "object": "molecular_structure_similarity (e.g., MACCS or MACCS-ECFP4 fingerprints)"
                    },
                    {
                        "subject": "demonstrations",
                        "relation": "are_ordered_by",
                        "object": "descending_similarity"
                    },
                    {
                        "subject": "number_of_demonstrations",
                        "relation": "is_less_than_or_equal_to",
                        "object": "2"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves_maximum_accuracy_on",
                        "object": "molecular_property_prediction"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MolecularGPT achieves best performance with 1-2 high-similarity demonstrations, descending order, and hybrid instruction tuning; more demonstrations or less similar examples degrade performance.",
                        "uuids": [
                            "e5680.0"
                        ]
                    },
                    {
                        "text": "Empirical ablations show that increasing the number of demonstrations beyond 2 does not further improve and can degrade performance; descending similarity order outperforms ascending or random order.",
                        "uuids": [
                            "e5680.0"
                        ]
                    },
                    {
                        "text": "MACCS-ECFP4 fingerprint-based retrieval improved performance on several tasks compared to MACCS alone.",
                        "uuids": [
                            "e5680.0"
                        ]
                    },
                    {
                        "text": "Hybrid instruction tuning (mix of 0-shot and k-shot templates) improves both zero- and few-shot capabilities.",
                        "uuids": [
                            "e5680.0"
                        ]
                    },
                    {
                        "text": "Structure-aware demonstration retrieval outperforms random or diversity-based retrieval for molecular property prediction.",
                        "uuids": [
                            "e5680.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Few-shot learning and similarity-based retrieval are established in ML, and structure-based retrieval is used in cheminformatics, but the specific law for optimal demonstration number, selection, and ordering in LLM-based molecular property prediction is not previously formalized.",
                    "what_is_novel": "This law quantifies the optimal number, selection, and ordering of demonstrations for LLM-based molecular property prediction, and demonstrates the superiority of structure-aware retrieval and hybrid instruction tuning.",
                    "classification_explanation": "While few-shot and similarity-based retrieval are known, the precise quantitative and qualitative law for demonstration selection, ordering, and number in LLM-based molecular property prediction is new and not previously formalized.",
                    "likely_classification": "new",
                    "references": [
                        "Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]",
                        "Brown et al. (2020) Language Models are Few-Shot Learners [general few-shot learning, not molecular-specific]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Neighbor Label Shortcut Law",
                "if": [
                    {
                        "subject": "few-shot demonstrations",
                        "relation": "include",
                        "object": "neighbor_labels"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "improves_few_shot_accuracy",
                        "object": "but_degrades_zero_shot_generalization"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Including neighbor labels in demonstrations improves in-context learning (ICL) but introduces shortcut learning, harming zero-shot performance.",
                        "uuids": [
                            "e5680.0"
                        ]
                    },
                    {
                        "text": "Empirical ablations show that neighbor label inclusion can lead to shortcut learning, which degrades zero-shot generalization even as few-shot accuracy increases.",
                        "uuids": [
                            "e5680.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Shortcut learning is a known phenomenon in ML, but its specific manifestation in LLM-based molecular property prediction via neighbor label inclusion is novel.",
                    "what_is_novel": "This law identifies and predicts the tradeoff between ICL gains and zero-shot generalization loss due to neighbor label inclusion in molecular property prediction.",
                    "classification_explanation": "Shortcut learning is known, but the specific effect of neighbor label inclusion in LLM-based molecular property prediction is new.",
                    "likely_classification": "new",
                    "references": [
                        "Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Hybrid Instruction Tuning Law",
                "if": [
                    {
                        "subject": "instruction_tuning_dataset",
                        "relation": "is_hybrid_mix_of",
                        "object": "zero-shot_and_few-shot_templates"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves_balanced_performance_on",
                        "object": "zero-shot_and_few-shot_molecular_property_prediction"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Hybrid instruction tuning (mix of 0-shot and k-shot templates) improves both zero- and few-shot performance, as shown in ablation studies.",
                        "uuids": [
                            "e5680.0"
                        ]
                    },
                    {
                        "text": "Scaling the instruction dataset size (doubling) further improves performance.",
                        "uuids": [
                            "e5680.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Instruction tuning is known in LLMs, but the specific benefit of hybrid zero- and few-shot mixes for molecular property prediction is novel.",
                    "what_is_novel": "This law formalizes the benefit of hybrid instruction tuning for balancing zero- and few-shot performance in LLM-based molecular property prediction.",
                    "classification_explanation": "Hybrid instruction tuning is not previously formalized for this domain.",
                    "likely_classification": "new",
                    "references": [
                        "Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Demonstration Order Law",
                "if": [
                    {
                        "subject": "demonstrations",
                        "relation": "are_ordered_by",
                        "object": "descending_similarity"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves_higher_accuracy_than",
                        "object": "ascending_or_random_order"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Descending similarity order of demonstrations outperforms ascending or random order, especially with more than one demonstration.",
                        "uuids": [
                            "e5680.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Order effects in few-shot learning are known, but the specific benefit of descending similarity order in molecular property prediction is new.",
                    "what_is_novel": "This law formalizes the optimal ordering of demonstrations for LLM-based molecular property prediction.",
                    "classification_explanation": "Order effects are known, but this specific law is new for this domain.",
                    "likely_classification": "new",
                    "references": [
                        "Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new molecular property prediction task is attempted with more than two demonstrations or with demonstrations not selected by structure similarity, accuracy will decrease compared to using 1-2 high-similarity demonstrations.",
        "If neighbor labels are included in demonstrations, zero-shot performance will drop even if few-shot accuracy increases.",
        "If demonstrations are ordered by ascending similarity or randomly, performance will be lower than with descending similarity order.",
        "If the instruction tuning dataset is not hybrid (e.g., only zero-shot or only few-shot), performance will be suboptimal on at least one of zero-shot or few-shot tasks."
    ],
    "new_predictions_unknown": [
        "If a hybrid retrieval strategy (combining structure similarity and property similarity) is used, it may further improve accuracy, especially for properties with complex determinants.",
        "If demonstrations are selected by 3D geometric similarity rather than SMILES-based fingerprints, performance may improve for geometry-sensitive properties.",
        "If structure-aware demonstration retrieval is combined with active learning (selecting most informative examples), further gains may be possible.",
        "If the LLM is extended to include 3D molecular representations or graph-based embeddings, the optimal number and selection of demonstrations may change."
    ],
    "negative_experiments": [
        "If more than two demonstrations selected by random or low similarity improve accuracy, the theory would be falsified.",
        "If including neighbor labels does not degrade zero-shot generalization, the theory would be challenged.",
        "If ascending or random order of demonstrations outperforms descending similarity order, the theory would be challenged.",
        "If hybrid instruction tuning does not outperform single-mode tuning (zero-shot or few-shot only), the theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Tasks where 3D geometry is critical may not be fully addressed by SMILES-based similarity; the theory does not account for cases where 3D structure dominates property determination.",
            "uuids": [
                "e5680.0"
            ]
        },
        {
            "text": "The theory does not address regression tasks where numerical generation remains challenging and gaps exist compared to supervised GNNs on some regression tasks.",
            "uuids": [
                "e5680.0"
            ]
        },
        {
            "text": "The theory does not account for input length constraints (max 512 tokens) that may limit the number of demonstrations in practice.",
            "uuids": [
                "e5680.0"
            ]
        }
    ],
    "conflicting_evidence": [],
    "special_cases": [
        "For very small or very large molecules, fingerprint similarity may not capture relevant property determinants.",
        "For properties determined by global molecular features (e.g., overall shape, polarizability), local similarity-based retrieval may be less effective.",
        "For tasks with highly imbalanced or rare property classes, the optimal demonstration selection strategy may differ.",
        "For regression tasks with high noise or outlier sensitivity, demonstration selection may need to account for label distribution as well as structure."
    ],
    "existing_theory": {
        "what_already_exists": "Few-shot learning, similarity-based retrieval, and shortcut learning are known in ML; structure-based retrieval is used in cheminformatics.",
        "what_is_novel": "The specific quantitative and qualitative laws for demonstration selection, ordering, and neighbor label effects in LLM-based molecular property prediction are new, as is the formalization of hybrid instruction tuning and demonstration order effects.",
        "classification_explanation": "No prior work formalizes these laws in this context; the theory is new and specific to LLM-based molecular property prediction.",
        "likely_classification": "new",
        "references": [
            "Wang et al. (2024) MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction [primary source]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [general few-shot learning]",
            "Geirhos et al. (2020) Shortcut Learning in Deep Neural Networks [shortcut learning, general ML]"
        ]
    },
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain and Prompt Sensitivity Theory (General Quantitative Extension) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1810</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1810</p>
                <p><strong>Name:</strong> Domain and Prompt Sensitivity Theory (General Quantitative Extension)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory extends the general formulation by proposing that the calibration and accuracy of LLM probability estimates for future scientific discoveries can be quantitatively modeled as a function of the overlap between the prompt's domain and the LLM's training data, as well as the prompt's semantic similarity to training examples. The theory posits that there exists a measurable, monotonic relationship between these factors and the calibration error of LLM outputs.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Quantitative Domain Overlap Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; domain_overlap_score &#8594; is_high &#8594; prompt_and_training_data</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_calibration_error &#8594; is_low &#8594; probability_estimates_for_future_discoveries</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that LLMs' calibration error (e.g., Brier score) is lower for prompts with high domain overlap with training data. </li>
    <li>Quantitative analyses demonstrate a monotonic relationship between training data coverage and LLM output accuracy. </li>
    <li>Semantic similarity metrics (e.g., cosine similarity in embedding space) correlate with LLM performance on forecasting tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is a quantitative extension of known calibration and domain adaptation principles, applied to a novel forecasting context.</p>            <p><strong>What Already Exists:</strong> Calibration and domain overlap have been studied in LLMs, but not specifically for scientific discovery forecasting.</p>            <p><strong>What is Novel:</strong> This law proposes a direct, quantitative relationship between domain overlap and calibration error for future scientific discovery probability estimates.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2021) How Can We Know What Language Models Know? [Calibration and domain overlap]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Calibration error and prompt similarity]</li>
</ul>
            <h3>Statement 1: Prompt Similarity Calibration Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt_semantic_similarity &#8594; is_high &#8594; training_prompts</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_better_calibrated &#8594; future_scientific_discovery</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompt engineering research shows that prompts with high semantic similarity to training examples yield more reliable and calibrated LLM outputs. </li>
    <li>Few-shot and in-context learning studies demonstrate improved calibration with prompt similarity. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is a quantitative formalization of prompt sensitivity, applied to a new domain.</p>            <p><strong>What Already Exists:</strong> Prompt similarity effects are known in LLMs, but not formalized for scientific forecasting calibration.</p>            <p><strong>What is Novel:</strong> This law formalizes the effect of prompt similarity on calibration of probability estimates for future scientific discoveries.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and calibration]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt similarity and performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If the domain overlap score between a prompt and the LLM's training data is increased (e.g., by fine-tuning), the calibration error of probability estimates for future discoveries will decrease.</li>
                <li>If prompts are rephrased to maximize semantic similarity to training examples, LLM probability estimates will become more reliable.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If domain overlap is artificially increased using synthetic data, the effect on calibration error for real-world discoveries may be non-linear or domain-dependent.</li>
                <li>If semantic similarity is maximized for prompts about highly novel discoveries, calibration may improve or degrade unpredictably.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If increasing domain overlap or prompt similarity does not reduce calibration error, the theory would be falsified.</li>
                <li>If LLMs show no quantitative relationship between prompt similarity and probability calibration, the theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The effect of adversarial or misleading prompts on calibration is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory extends calibration and prompt engineering research to a new, high-impact forecasting context.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2021) How Can We Know What Language Models Know? [Calibration and domain overlap]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and calibration]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt similarity and performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain and Prompt Sensitivity Theory (General Quantitative Extension)",
    "theory_description": "This theory extends the general formulation by proposing that the calibration and accuracy of LLM probability estimates for future scientific discoveries can be quantitatively modeled as a function of the overlap between the prompt's domain and the LLM's training data, as well as the prompt's semantic similarity to training examples. The theory posits that there exists a measurable, monotonic relationship between these factors and the calibration error of LLM outputs.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Quantitative Domain Overlap Law",
                "if": [
                    {
                        "subject": "domain_overlap_score",
                        "relation": "is_high",
                        "object": "prompt_and_training_data"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_calibration_error",
                        "relation": "is_low",
                        "object": "probability_estimates_for_future_discoveries"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that LLMs' calibration error (e.g., Brier score) is lower for prompts with high domain overlap with training data.",
                        "uuids": []
                    },
                    {
                        "text": "Quantitative analyses demonstrate a monotonic relationship between training data coverage and LLM output accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Semantic similarity metrics (e.g., cosine similarity in embedding space) correlate with LLM performance on forecasting tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Calibration and domain overlap have been studied in LLMs, but not specifically for scientific discovery forecasting.",
                    "what_is_novel": "This law proposes a direct, quantitative relationship between domain overlap and calibration error for future scientific discovery probability estimates.",
                    "classification_explanation": "The law is a quantitative extension of known calibration and domain adaptation principles, applied to a novel forecasting context.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2021) How Can We Know What Language Models Know? [Calibration and domain overlap]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Calibration error and prompt similarity]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Similarity Calibration Law",
                "if": [
                    {
                        "subject": "prompt_semantic_similarity",
                        "relation": "is_high",
                        "object": "training_prompts"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_better_calibrated",
                        "object": "future_scientific_discovery"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompt engineering research shows that prompts with high semantic similarity to training examples yield more reliable and calibrated LLM outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Few-shot and in-context learning studies demonstrate improved calibration with prompt similarity.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Prompt similarity effects are known in LLMs, but not formalized for scientific forecasting calibration.",
                    "what_is_novel": "This law formalizes the effect of prompt similarity on calibration of probability estimates for future scientific discoveries.",
                    "classification_explanation": "The law is a quantitative formalization of prompt sensitivity, applied to a new domain.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and calibration]",
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt similarity and performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If the domain overlap score between a prompt and the LLM's training data is increased (e.g., by fine-tuning), the calibration error of probability estimates for future discoveries will decrease.",
        "If prompts are rephrased to maximize semantic similarity to training examples, LLM probability estimates will become more reliable."
    ],
    "new_predictions_unknown": [
        "If domain overlap is artificially increased using synthetic data, the effect on calibration error for real-world discoveries may be non-linear or domain-dependent.",
        "If semantic similarity is maximized for prompts about highly novel discoveries, calibration may improve or degrade unpredictably."
    ],
    "negative_experiments": [
        "If increasing domain overlap or prompt similarity does not reduce calibration error, the theory would be falsified.",
        "If LLMs show no quantitative relationship between prompt similarity and probability calibration, the theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "The effect of adversarial or misleading prompts on calibration is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs with retrieval augmentation can achieve good calibration even with low domain overlap.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with explicit calibration layers or Bayesian heads may not follow the same monotonic relationship.",
        "Prompts about inherently unpredictable or chaotic scientific domains may not benefit from increased overlap or similarity."
    ],
    "existing_theory": {
        "what_already_exists": "Quantitative calibration and prompt similarity effects are known in LLMs.",
        "what_is_novel": "The explicit, quantitative modeling of calibration error as a function of domain overlap and prompt similarity for scientific discovery forecasting is novel.",
        "classification_explanation": "This theory extends calibration and prompt engineering research to a new, high-impact forecasting context.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Jiang et al. (2021) How Can We Know What Language Models Know? [Calibration and domain overlap]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and calibration]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt similarity and performance]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-647",
    "original_theory_name": "Domain and Prompt Sensitivity Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Domain and Prompt Sensitivity Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositional Pattern Matching Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-728</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-728</p>
                <p><strong>Name:</strong> Compositional Pattern Matching Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.</p>
                <p><strong>Description:</strong> Language models perform arithmetic by leveraging their ability to recognize, decompose, and recombine patterns in token sequences, allowing them to approximate arithmetic operations through learned statistical associations and compositional generalization, rather than explicit algorithmic computation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Pattern Decomposition and Recombination Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; arithmetic_query &#8594; is_presented_to &#8594; language_model<span style="color: #888888;">, and</span></div>
        <div>&#8226; arithmetic_query &#8594; is_within_tokenization_limits &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; decomposes &#8594; arithmetic_query_into_token_patterns<span style="color: #888888;">, and</span></div>
        <div>&#8226; language_model &#8594; recombines &#8594; token_patterns_to_generate_answer</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can solve arithmetic queries for numbers and formats seen in training, and sometimes generalize to novel but structurally similar queries. </li>
    <li>Performance drops sharply for queries with tokenization artifacts or out-of-vocabulary number formats. </li>
    <li>LLMs show higher accuracy for arithmetic queries that align with common tokenization patterns. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing work on LLM compositionality, the explicit application to arithmetic and the decomposition-recombination process is novel.</p>            <p><strong>What Already Exists:</strong> Pattern matching and compositionality are recognized as core LLM capabilities, but not formalized for arithmetic.</p>            <p><strong>What is Novel:</strong> This law formalizes the compositional pattern matching mechanism as the basis for arithmetic in LLMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Sequence-to-Sequence Recurrent Networks [Compositionality in neural models]</li>
    <li>Weiss et al. (2021) Thinking Like Transformers: Systematic Generalization with Transformers [Pattern matching and generalization in LLMs]</li>
</ul>
            <h3>Statement 1: Statistical Approximation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; arithmetic_query &#8594; is_unseen_or_rare &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; outputs &#8594; statistically_approximate_answer</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs often produce plausible but incorrect answers for rare or out-of-distribution arithmetic queries. </li>
    <li>Performance on arithmetic degrades as queries become less frequent in the training data. </li>
    <li>LLMs sometimes interpolate between known facts to answer novel arithmetic queries. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Closely related to existing work on LLM statistical learning, but the explicit link to arithmetic is novel.</p>            <p><strong>What Already Exists:</strong> LLMs are known to rely on statistical associations for many tasks.</p>            <p><strong>What is Novel:</strong> This law formalizes the statistical approximation process as a fallback for arithmetic beyond memorized or compositional cases.</p>
            <p><strong>References:</strong> <ul>
    <li>Marcus (2020) The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence [Statistical learning in LLMs]</li>
    <li>Zhang et al. (2022) Can Language Models Do Arithmetic? [Empirical study of LLM arithmetic performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will perform better on arithmetic queries that follow common tokenization patterns or are structurally similar to training data.</li>
                <li>LLMs will make more errors on arithmetic queries with rare number formats or tokenization artifacts.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained on synthetic data with novel arithmetic patterns, they may develop new compositional strategies for arithmetic.</li>
                <li>If tokenization is redesigned to align with arithmetic structure, LLMs may show improved arithmetic generalization.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs can solve arithmetic queries with completely novel structure or tokenization, this would challenge the compositional pattern matching theory.</li>
                <li>If LLMs show no performance difference between common and rare arithmetic patterns, this would call the theory into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs can perform multi-step arithmetic with chain-of-thought prompting, which may involve more than pattern matching. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is somewhat related to existing work on LLM compositionality and statistical learning, but its explicit application to arithmetic is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Sequence-to-Sequence Recurrent Networks [Compositionality in neural models]</li>
    <li>Weiss et al. (2021) Thinking Like Transformers: Systematic Generalization with Transformers [Pattern matching and generalization in LLMs]</li>
    <li>Zhang et al. (2022) Can Language Models Do Arithmetic? [Empirical study of LLM arithmetic performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositional Pattern Matching Theory",
    "theory_description": "Language models perform arithmetic by leveraging their ability to recognize, decompose, and recombine patterns in token sequences, allowing them to approximate arithmetic operations through learned statistical associations and compositional generalization, rather than explicit algorithmic computation.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Pattern Decomposition and Recombination Law",
                "if": [
                    {
                        "subject": "arithmetic_query",
                        "relation": "is_presented_to",
                        "object": "language_model"
                    },
                    {
                        "subject": "arithmetic_query",
                        "relation": "is_within_tokenization_limits",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "decomposes",
                        "object": "arithmetic_query_into_token_patterns"
                    },
                    {
                        "subject": "language_model",
                        "relation": "recombines",
                        "object": "token_patterns_to_generate_answer"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can solve arithmetic queries for numbers and formats seen in training, and sometimes generalize to novel but structurally similar queries.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops sharply for queries with tokenization artifacts or out-of-vocabulary number formats.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs show higher accuracy for arithmetic queries that align with common tokenization patterns.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Pattern matching and compositionality are recognized as core LLM capabilities, but not formalized for arithmetic.",
                    "what_is_novel": "This law formalizes the compositional pattern matching mechanism as the basis for arithmetic in LLMs.",
                    "classification_explanation": "While related to existing work on LLM compositionality, the explicit application to arithmetic and the decomposition-recombination process is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Sequence-to-Sequence Recurrent Networks [Compositionality in neural models]",
                        "Weiss et al. (2021) Thinking Like Transformers: Systematic Generalization with Transformers [Pattern matching and generalization in LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Statistical Approximation Law",
                "if": [
                    {
                        "subject": "arithmetic_query",
                        "relation": "is_unseen_or_rare",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "outputs",
                        "object": "statistically_approximate_answer"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs often produce plausible but incorrect answers for rare or out-of-distribution arithmetic queries.",
                        "uuids": []
                    },
                    {
                        "text": "Performance on arithmetic degrades as queries become less frequent in the training data.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs sometimes interpolate between known facts to answer novel arithmetic queries.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLMs are known to rely on statistical associations for many tasks.",
                    "what_is_novel": "This law formalizes the statistical approximation process as a fallback for arithmetic beyond memorized or compositional cases.",
                    "classification_explanation": "Closely related to existing work on LLM statistical learning, but the explicit link to arithmetic is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Marcus (2020) The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence [Statistical learning in LLMs]",
                        "Zhang et al. (2022) Can Language Models Do Arithmetic? [Empirical study of LLM arithmetic performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will perform better on arithmetic queries that follow common tokenization patterns or are structurally similar to training data.",
        "LLMs will make more errors on arithmetic queries with rare number formats or tokenization artifacts."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained on synthetic data with novel arithmetic patterns, they may develop new compositional strategies for arithmetic.",
        "If tokenization is redesigned to align with arithmetic structure, LLMs may show improved arithmetic generalization."
    ],
    "negative_experiments": [
        "If LLMs can solve arithmetic queries with completely novel structure or tokenization, this would challenge the compositional pattern matching theory.",
        "If LLMs show no performance difference between common and rare arithmetic patterns, this would call the theory into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs can perform multi-step arithmetic with chain-of-thought prompting, which may involve more than pattern matching.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Certain LLMs trained with explicit arithmetic modules outperform pattern-based models, suggesting alternative mechanisms.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For arithmetic queries with ambiguous or non-standard phrasing, pattern matching may fail.",
        "For very large numbers or multi-step operations, compositionality may be insufficient."
    ],
    "existing_theory": {
        "what_already_exists": "Pattern matching and statistical learning are established in LLMs.",
        "what_is_novel": "The explicit compositional pattern matching mechanism for arithmetic is novel.",
        "classification_explanation": "The theory is somewhat related to existing work on LLM compositionality and statistical learning, but its explicit application to arithmetic is new.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Sequence-to-Sequence Recurrent Networks [Compositionality in neural models]",
            "Weiss et al. (2021) Thinking Like Transformers: Systematic Generalization with Transformers [Pattern matching and generalization in LLMs]",
            "Zhang et al. (2022) Can Language Models Do Arithmetic? [Empirical study of LLM arithmetic performance]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform arithmetic.",
    "original_theory_id": "theory-578",
    "original_theory_name": "Latent Algorithm Activation and Superficial Alignment Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
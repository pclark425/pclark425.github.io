<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Action-Space Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-354</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-354</p>
                <p><strong>Name:</strong> Action-Space Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about when and how pretraining on text worlds transfers to 3D embodied tasks, including mappings between high-level action semantics and low-level perception and predicted sample complexity gains.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that successful transfer from text-based pretraining to 3D embodied tasks depends on the degree of structural and semantic alignment between action spaces across domains. Transfer effectiveness is determined by three key factors: (1) the hierarchical decomposability of actions in both domains, (2) the semantic consistency of action effects across modalities, and (3) the existence of shared abstract action schemas that can be instantiated differently in text and 3D environments. The theory predicts that sample complexity gains scale with the degree of action-space alignment, measured by the overlap in causal action graphs and the consistency of state-transition semantics. Critically, the theory proposes that alignment occurs at multiple levels simultaneously: at the goal level (what to achieve), the plan level (sequence of subgoals), and the primitive level (executable actions), with transfer being most effective when alignment exists at all three levels. The theory acknowledges that action-space alignment is one mechanism among several (including implicit world models and direct sensorimotor learning) that can enable transfer, and its relative importance depends on task characteristics, particularly the balance between high-level planning requirements and low-level control precision. The theory further recognizes that effective transfer requires not just semantic alignment but also grounding of abstract action representations in the sensorimotor affordances and embodiment constraints of the 3D environment.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Transfer from text to 3D embodied tasks is proportional to the structural isomorphism between action spaces, measured by the overlap in causal action graphs and the consistency of precondition-effect relationships.</li>
                <li>Action-space alignment operates at three hierarchical levels: (1) goal-level semantic alignment (what outcomes to achieve), (2) plan-level structural correspondence (sequences and dependencies of subgoals), and (3) primitive-level grounding (mapping to executable sensorimotor actions). Transfer effectiveness requires alignment at all three levels, with misalignment at any level creating a bottleneck.</li>
                <li>Sample complexity gains from pretraining scale as G(a) = G₀ * (1 - α_goal * A_goal - α_plan * A_plan - α_prim * A_prim), where G₀ is baseline sample complexity, A_goal, A_plan, and A_prim are alignment measures at each level (0 to 1), and α coefficients represent the relative importance of each level for the specific task (with Σα ≤ 1).</li>
                <li>High-level action semantics (goals and plans) transfer more readily than low-level motor primitives, with transfer effectiveness decreasing as actions become more perception-dependent and require fine-grained sensorimotor control.</li>
                <li>The degree of action-space alignment can be quantified by measuring: (a) semantic similarity of action labels in shared embedding spaces, (b) consistency of state-transition dynamics under corresponding actions, (c) overlap in precondition-effect structures, and (d) preservation of temporal dependencies in action sequences.</li>
                <li>Transfer is most effective when text pretraining includes explicit descriptions of action preconditions, effects, and failure modes that correspond to physical constraints in 3D environments, as this enables learning of causally grounded action representations.</li>
                <li>Action abstractions learned from text generalize to 3D tasks when the abstraction hierarchy preserves causal relationships between actions and state changes across both domains, and when abstract actions can be successfully grounded in available sensorimotor primitives.</li>
                <li>The sample complexity reduction from pretraining is greatest for tasks requiring long-horizon planning where high-level action semantics dominate, and smallest for tasks requiring precise low-level control where sensorimotor details dominate.</li>
                <li>Temporal structure in action sequences (ordering constraints, dependencies, and timing) must be preserved across text and 3D domains for effective transfer of sequential decision-making capabilities.</li>
                <li>The relative importance of action-space alignment versus other transfer mechanisms (implicit world models, direct sensorimotor learning) depends on the task's planning horizon, the precision requirements of low-level control, and the availability of task-relevant sensorimotor experience.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Language models pretrained on text-based games show improved sample efficiency when fine-tuned on embodied navigation tasks, suggesting shared action representations between text and embodied domains. </li>
    <li>Hierarchical reinforcement learning agents that learn high-level action abstractions transfer better across tasks than those operating only at primitive action levels, supporting the multi-level alignment hypothesis. </li>
    <li>Vision-language models that ground language in visual semantics show improved zero-shot transfer to robotic manipulation tasks, demonstrating the importance of grounding abstract action representations in perceptual affordances. </li>
    <li>Agents pretrained on diverse text-based instruction following show systematic generalization to novel embodied tasks with similar action structures, indicating that structural action-space similarity facilitates transfer. </li>
    <li>Sample complexity in embodied tasks decreases when agents have access to language descriptions of action effects and preconditions, supporting the role of explicit action semantics in transfer. </li>
    <li>Transfer learning in reinforcement learning is more effective when source and target tasks share similar action spaces and state-transition dynamics. </li>
    <li>Modular and compositional action representations learned from language enable better generalization to novel task compositions in embodied settings. </li>
    <li>Skill transfer in reinforcement learning is facilitated by portable options that abstract over low-level details while preserving high-level action semantics. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Pretraining on text-based games with explicit physics descriptions (e.g., 'pushing a heavy object requires more force', 'fragile objects break when dropped') will transfer better to 3D physics simulation tasks than pretraining on text without physical grounding, with sample complexity reductions of 30-50% for physics-heavy tasks.</li>
                <li>Agents pretrained on text corpora containing action sequences with temporal dependencies (e.g., 'first open the door, then enter the room') will show faster learning (20-40% fewer samples) on 3D tasks requiring similar temporal action structures, even with different visual appearances.</li>
                <li>Fine-tuning on a small set (10-50 examples) of text-to-3D action mappings for basic actions (e.g., 'pick up' → grasp motion primitives, 'move to' → navigation primitives) will enable zero-shot or few-shot transfer of other semantically similar actions learned during text pretraining.</li>
                <li>Sample complexity gains will be larger for 3D navigation tasks (40-60% reduction) than manipulation tasks (20-30% reduction), because navigation actions ('go to', 'turn', 'explore') have more direct semantic correspondence with text-based spatial actions and require less precise low-level control.</li>
                <li>Measuring action-space alignment through embedding similarity between text action descriptions and 3D action effect vectors will predict transfer performance with correlation r > 0.7 across diverse task pairs.</li>
                <li>Tasks with longer planning horizons (>10 steps) will show greater benefits from text pretraining (>50% sample complexity reduction) compared to short-horizon tasks (<5 steps, <20% reduction), as high-level action semantics become more important.</li>
                <li>Augmenting text pretraining with synthetic descriptions of action failures and precondition violations will improve robustness of transferred policies, reducing failure rates by 25-40% in novel 3D scenarios.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether pretraining on synthetic text descriptions of 3D action sequences (generated by observing expert 3D agents and converting to language) provides better transfer than pretraining on natural language corpora with implicit action knowledge, and if so, by what margin (10% vs 50% vs 100% improvement).</li>
                <li>Whether there exists a critical threshold of action-space alignment (e.g., A_align > 0.6) below which transfer provides negligible benefits (<10% sample complexity reduction), or whether transfer benefits scale continuously and linearly with alignment even at low alignment values.</li>
                <li>Whether adversarial text pretraining (learning to distinguish valid from invalid action sequences, or predicting action failure modes) improves robustness of transferred policies to distribution shift in 3D environments by more than 30%, and whether this robustness generalizes across different types of distribution shifts.</li>
                <li>Whether multi-modal pretraining that explicitly learns bidirectional mappings between text actions and 3D demonstrations outperforms unidirectional text-only pretraining by a significant margin (>30% sample efficiency), or whether the benefits are marginal (<10%) due to the added complexity.</li>
                <li>Whether action-space alignment can be improved through curriculum learning that progressively increases the complexity of action mappings from text to 3D (starting with simple atomic actions, then compositions, then long sequences), and if so, what the optimal curriculum structure is and how much improvement it provides (20% vs 50% vs 100%).</li>
                <li>Whether transfer from text to 3D is symmetric with transfer from 3D to text (i.e., whether learning action representations from 3D experience helps with text-based reasoning about actions), or whether one direction provides fundamentally better action representations for the other domain.</li>
                <li>Whether the relative importance coefficients (α_goal, α_plan, α_prim) in the sample complexity formula vary systematically with task characteristics in predictable ways, or whether they are highly task-specific and unpredictable.</li>
                <li>Whether action-space alignment can compensate for misalignment in state spaces or reward structures, or whether all three types of alignment (action, state, reward) are necessary for effective transfer.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents pretrained on text with high action-space alignment (measured by semantic similarity and causal graph overlap) show no better transfer than randomly initialized agents (difference <5% in sample complexity), this would challenge the core premise that action-space alignment drives transfer.</li>
                <li>If sample complexity gains do not correlate with measured action-space alignment across multiple task pairs (correlation r < 0.3), this would suggest alignment is not the primary mechanism of transfer or that the alignment measures are inadequate.</li>
                <li>If removing high-level action descriptions from text pretraining (leaving only low-level details) does not impair transfer to 3D tasks requiring planning (difference <10% in performance), this would contradict the hierarchical alignment hypothesis and the claim that high-level semantics transfer more readily.</li>
                <li>If artificially increasing semantic similarity between text and 3D action labels (through relabeling) without changing underlying dynamics does not improve transfer (difference <5%), this would suggest semantic alignment alone is insufficient and that structural/causal alignment is more important.</li>
                <li>If transfer effectiveness is identical for tasks with aligned action spaces but different state spaces versus tasks with aligned state spaces but different action spaces (difference <10%), this would challenge the action-centric focus of the theory and suggest state-space alignment is equally or more important.</li>
                <li>If providing explicit text descriptions of action preconditions and effects during pretraining does not improve transfer compared to pretraining without such descriptions (difference <10%), this would contradict the claim that causal grounding enhances transfer.</li>
                <li>If agents pretrained on text with temporal action dependencies show no better transfer to temporally-structured 3D tasks than agents pretrained on temporally-unstructured text (difference <10%), this would challenge the importance of temporal structure preservation.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The specific mechanisms by which visual perception and sensorimotor grounding determine which action mappings are learnable from limited 3D experience versus which require extensive practice, and how this interacts with text-based action knowledge. </li>
    <li>How embodiment constraints (robot morphology, actuator limits, sensor noise, dynamics) affect the transferability of action representations learned from text, and whether certain embodiments are more amenable to text-based transfer than others. </li>
    <li>The interaction between action-space alignment and reward structure alignment in determining transfer effectiveness, and whether misalignment in one can be compensated by strong alignment in the other. </li>
    <li>The role of implicit world models learned during text pretraining in enabling transfer, and how these interact with explicit action-space alignment mechanisms. </li>
    <li>How the scale of text pretraining (model size, data size) affects the quality of learned action representations and their transferability to 3D tasks. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Shridhar et al. (2020) ALFWorld: Aligning Text and Embodied Environments for Interactive Learning [Related work on text-embodied alignment but focuses on environment and task alignment rather than specifically theorizing about action-space structure and multi-level alignment mechanisms]</li>
    <li>Taylor & Stone (2009) Transfer Learning for Reinforcement Learning Domains: A Survey [General transfer learning framework but does not specifically address text-to-3D action mapping or the role of language pretraining]</li>
    <li>Konidaris & Barto (2007) Building Portable Options: Skill Transfer in Reinforcement Learning [Addresses skill transfer and portable options but not cross-modal text-to-embodied transfer or the specific mechanisms of action-space alignment]</li>
    <li>Andreas et al. (2017) Modular Multitask Reinforcement Learning with Policy Sketches [Related to high-level action specification from language but not a comprehensive theory of text-to-3D transfer]</li>
    <li>Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Demonstrates language-to-robot transfer but does not propose a formal theory of action-space alignment mechanisms]</li>
    <li>Nachum et al. (2018) Data-Efficient Hierarchical Reinforcement Learning [Addresses hierarchical action representations but not cross-modal transfer from text]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Action-Space Alignment Theory",
    "theory_description": "This theory posits that successful transfer from text-based pretraining to 3D embodied tasks depends on the degree of structural and semantic alignment between action spaces across domains. Transfer effectiveness is determined by three key factors: (1) the hierarchical decomposability of actions in both domains, (2) the semantic consistency of action effects across modalities, and (3) the existence of shared abstract action schemas that can be instantiated differently in text and 3D environments. The theory predicts that sample complexity gains scale with the degree of action-space alignment, measured by the overlap in causal action graphs and the consistency of state-transition semantics. Critically, the theory proposes that alignment occurs at multiple levels simultaneously: at the goal level (what to achieve), the plan level (sequence of subgoals), and the primitive level (executable actions), with transfer being most effective when alignment exists at all three levels. The theory acknowledges that action-space alignment is one mechanism among several (including implicit world models and direct sensorimotor learning) that can enable transfer, and its relative importance depends on task characteristics, particularly the balance between high-level planning requirements and low-level control precision. The theory further recognizes that effective transfer requires not just semantic alignment but also grounding of abstract action representations in the sensorimotor affordances and embodiment constraints of the 3D environment.",
    "supporting_evidence": [
        {
            "text": "Language models pretrained on text-based games show improved sample efficiency when fine-tuned on embodied navigation tasks, suggesting shared action representations between text and embodied domains.",
            "citations": [
                "Shridhar et al. (2020) ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
                "Côté et al. (2018) TextWorld: A Learning Environment for Text-based Games"
            ]
        },
        {
            "text": "Hierarchical reinforcement learning agents that learn high-level action abstractions transfer better across tasks than those operating only at primitive action levels, supporting the multi-level alignment hypothesis.",
            "citations": [
                "Nachum et al. (2018) Data-Efficient Hierarchical Reinforcement Learning",
                "Vezhnevets et al. (2017) FeUdal Networks for Hierarchical Reinforcement Learning"
            ]
        },
        {
            "text": "Vision-language models that ground language in visual semantics show improved zero-shot transfer to robotic manipulation tasks, demonstrating the importance of grounding abstract action representations in perceptual affordances.",
            "citations": [
                "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "Shridhar et al. (2022) CLIPort: What and Where Pathways for Robotic Manipulation"
            ]
        },
        {
            "text": "Agents pretrained on diverse text-based instruction following show systematic generalization to novel embodied tasks with similar action structures, indicating that structural action-space similarity facilitates transfer.",
            "citations": [
                "Hill et al. (2020) Environmental drivers of systematicity and generalization in a situated agent",
                "Lynch et al. (2020) Learning Latent Plans from Play"
            ]
        },
        {
            "text": "Sample complexity in embodied tasks decreases when agents have access to language descriptions of action effects and preconditions, supporting the role of explicit action semantics in transfer.",
            "citations": [
                "Huang et al. (2022) Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
                "Driess et al. (2023) PaLM-E: An Embodied Multimodal Language Model"
            ]
        },
        {
            "text": "Transfer learning in reinforcement learning is more effective when source and target tasks share similar action spaces and state-transition dynamics.",
            "citations": [
                "Taylor & Stone (2009) Transfer Learning for Reinforcement Learning Domains: A Survey",
                "Sorg et al. (2010) Transfer Learning in Reinforcement Learning via Shared Features"
            ]
        },
        {
            "text": "Modular and compositional action representations learned from language enable better generalization to novel task compositions in embodied settings.",
            "citations": [
                "Andreas et al. (2017) Modular Multitask Reinforcement Learning with Policy Sketches",
                "Hill et al. (2020) Environmental drivers of systematicity and generalization in a situated agent"
            ]
        },
        {
            "text": "Skill transfer in reinforcement learning is facilitated by portable options that abstract over low-level details while preserving high-level action semantics.",
            "citations": [
                "Konidaris & Barto (2007) Building Portable Options: Skill Transfer in Reinforcement Learning"
            ]
        }
    ],
    "theory_statements": [
        "Transfer from text to 3D embodied tasks is proportional to the structural isomorphism between action spaces, measured by the overlap in causal action graphs and the consistency of precondition-effect relationships.",
        "Action-space alignment operates at three hierarchical levels: (1) goal-level semantic alignment (what outcomes to achieve), (2) plan-level structural correspondence (sequences and dependencies of subgoals), and (3) primitive-level grounding (mapping to executable sensorimotor actions). Transfer effectiveness requires alignment at all three levels, with misalignment at any level creating a bottleneck.",
        "Sample complexity gains from pretraining scale as G(a) = G₀ * (1 - α_goal * A_goal - α_plan * A_plan - α_prim * A_prim), where G₀ is baseline sample complexity, A_goal, A_plan, and A_prim are alignment measures at each level (0 to 1), and α coefficients represent the relative importance of each level for the specific task (with Σα ≤ 1).",
        "High-level action semantics (goals and plans) transfer more readily than low-level motor primitives, with transfer effectiveness decreasing as actions become more perception-dependent and require fine-grained sensorimotor control.",
        "The degree of action-space alignment can be quantified by measuring: (a) semantic similarity of action labels in shared embedding spaces, (b) consistency of state-transition dynamics under corresponding actions, (c) overlap in precondition-effect structures, and (d) preservation of temporal dependencies in action sequences.",
        "Transfer is most effective when text pretraining includes explicit descriptions of action preconditions, effects, and failure modes that correspond to physical constraints in 3D environments, as this enables learning of causally grounded action representations.",
        "Action abstractions learned from text generalize to 3D tasks when the abstraction hierarchy preserves causal relationships between actions and state changes across both domains, and when abstract actions can be successfully grounded in available sensorimotor primitives.",
        "The sample complexity reduction from pretraining is greatest for tasks requiring long-horizon planning where high-level action semantics dominate, and smallest for tasks requiring precise low-level control where sensorimotor details dominate.",
        "Temporal structure in action sequences (ordering constraints, dependencies, and timing) must be preserved across text and 3D domains for effective transfer of sequential decision-making capabilities.",
        "The relative importance of action-space alignment versus other transfer mechanisms (implicit world models, direct sensorimotor learning) depends on the task's planning horizon, the precision requirements of low-level control, and the availability of task-relevant sensorimotor experience."
    ],
    "new_predictions_likely": [
        "Pretraining on text-based games with explicit physics descriptions (e.g., 'pushing a heavy object requires more force', 'fragile objects break when dropped') will transfer better to 3D physics simulation tasks than pretraining on text without physical grounding, with sample complexity reductions of 30-50% for physics-heavy tasks.",
        "Agents pretrained on text corpora containing action sequences with temporal dependencies (e.g., 'first open the door, then enter the room') will show faster learning (20-40% fewer samples) on 3D tasks requiring similar temporal action structures, even with different visual appearances.",
        "Fine-tuning on a small set (10-50 examples) of text-to-3D action mappings for basic actions (e.g., 'pick up' → grasp motion primitives, 'move to' → navigation primitives) will enable zero-shot or few-shot transfer of other semantically similar actions learned during text pretraining.",
        "Sample complexity gains will be larger for 3D navigation tasks (40-60% reduction) than manipulation tasks (20-30% reduction), because navigation actions ('go to', 'turn', 'explore') have more direct semantic correspondence with text-based spatial actions and require less precise low-level control.",
        "Measuring action-space alignment through embedding similarity between text action descriptions and 3D action effect vectors will predict transfer performance with correlation r &gt; 0.7 across diverse task pairs.",
        "Tasks with longer planning horizons (&gt;10 steps) will show greater benefits from text pretraining (&gt;50% sample complexity reduction) compared to short-horizon tasks (&lt;5 steps, &lt;20% reduction), as high-level action semantics become more important.",
        "Augmenting text pretraining with synthetic descriptions of action failures and precondition violations will improve robustness of transferred policies, reducing failure rates by 25-40% in novel 3D scenarios."
    ],
    "new_predictions_unknown": [
        "Whether pretraining on synthetic text descriptions of 3D action sequences (generated by observing expert 3D agents and converting to language) provides better transfer than pretraining on natural language corpora with implicit action knowledge, and if so, by what margin (10% vs 50% vs 100% improvement).",
        "Whether there exists a critical threshold of action-space alignment (e.g., A_align &gt; 0.6) below which transfer provides negligible benefits (&lt;10% sample complexity reduction), or whether transfer benefits scale continuously and linearly with alignment even at low alignment values.",
        "Whether adversarial text pretraining (learning to distinguish valid from invalid action sequences, or predicting action failure modes) improves robustness of transferred policies to distribution shift in 3D environments by more than 30%, and whether this robustness generalizes across different types of distribution shifts.",
        "Whether multi-modal pretraining that explicitly learns bidirectional mappings between text actions and 3D demonstrations outperforms unidirectional text-only pretraining by a significant margin (&gt;30% sample efficiency), or whether the benefits are marginal (&lt;10%) due to the added complexity.",
        "Whether action-space alignment can be improved through curriculum learning that progressively increases the complexity of action mappings from text to 3D (starting with simple atomic actions, then compositions, then long sequences), and if so, what the optimal curriculum structure is and how much improvement it provides (20% vs 50% vs 100%).",
        "Whether transfer from text to 3D is symmetric with transfer from 3D to text (i.e., whether learning action representations from 3D experience helps with text-based reasoning about actions), or whether one direction provides fundamentally better action representations for the other domain.",
        "Whether the relative importance coefficients (α_goal, α_plan, α_prim) in the sample complexity formula vary systematically with task characteristics in predictable ways, or whether they are highly task-specific and unpredictable.",
        "Whether action-space alignment can compensate for misalignment in state spaces or reward structures, or whether all three types of alignment (action, state, reward) are necessary for effective transfer."
    ],
    "negative_experiments": [
        "If agents pretrained on text with high action-space alignment (measured by semantic similarity and causal graph overlap) show no better transfer than randomly initialized agents (difference &lt;5% in sample complexity), this would challenge the core premise that action-space alignment drives transfer.",
        "If sample complexity gains do not correlate with measured action-space alignment across multiple task pairs (correlation r &lt; 0.3), this would suggest alignment is not the primary mechanism of transfer or that the alignment measures are inadequate.",
        "If removing high-level action descriptions from text pretraining (leaving only low-level details) does not impair transfer to 3D tasks requiring planning (difference &lt;10% in performance), this would contradict the hierarchical alignment hypothesis and the claim that high-level semantics transfer more readily.",
        "If artificially increasing semantic similarity between text and 3D action labels (through relabeling) without changing underlying dynamics does not improve transfer (difference &lt;5%), this would suggest semantic alignment alone is insufficient and that structural/causal alignment is more important.",
        "If transfer effectiveness is identical for tasks with aligned action spaces but different state spaces versus tasks with aligned state spaces but different action spaces (difference &lt;10%), this would challenge the action-centric focus of the theory and suggest state-space alignment is equally or more important.",
        "If providing explicit text descriptions of action preconditions and effects during pretraining does not improve transfer compared to pretraining without such descriptions (difference &lt;10%), this would contradict the claim that causal grounding enhances transfer.",
        "If agents pretrained on text with temporal action dependencies show no better transfer to temporally-structured 3D tasks than agents pretrained on temporally-unstructured text (difference &lt;10%), this would challenge the importance of temporal structure preservation."
    ],
    "unaccounted_for": [
        {
            "text": "The specific mechanisms by which visual perception and sensorimotor grounding determine which action mappings are learnable from limited 3D experience versus which require extensive practice, and how this interacts with text-based action knowledge.",
            "citations": [
                "Pinto et al. (2016) Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours",
                "Levine et al. (2016) End-to-End Training of Deep Visuomotor Policies"
            ]
        },
        {
            "text": "How embodiment constraints (robot morphology, actuator limits, sensor noise, dynamics) affect the transferability of action representations learned from text, and whether certain embodiments are more amenable to text-based transfer than others.",
            "citations": [
                "Peng et al. (2018) Sim-to-Real Transfer of Robotic Control with Dynamics Randomization",
                "Zhao et al. (2020) Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey"
            ]
        },
        {
            "text": "The interaction between action-space alignment and reward structure alignment in determining transfer effectiveness, and whether misalignment in one can be compensated by strong alignment in the other.",
            "citations": [
                "Hadfield-Menell et al. (2017) Inverse Reward Design",
                "Sorg et al. (2010) Transfer Learning in Reinforcement Learning via Shared Features"
            ]
        },
        {
            "text": "The role of implicit world models learned during text pretraining in enabling transfer, and how these interact with explicit action-space alignment mechanisms.",
            "citations": [
                "Huang et al. (2022) Inner Monologue: Embodied Reasoning through Planning with Language Models",
                "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"
            ]
        },
        {
            "text": "How the scale of text pretraining (model size, data size) affects the quality of learned action representations and their transferability to 3D tasks.",
            "citations": [
                "Driess et al. (2023) PaLM-E: An Embodied Multimodal Language Model"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that large language models can perform zero-shot planning for embodied tasks without explicit action-space alignment training, suggesting transfer may occur through implicit world models and common-sense reasoning rather than explicit action mappings, which challenges the necessity of explicit alignment.",
            "citations": [
                "Huang et al. (2022) Inner Monologue: Embodied Reasoning through Planning with Language Models",
                "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "Huang et al. (2022) Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"
            ]
        },
        {
            "text": "Evidence that end-to-end visuomotor policies trained from scratch can match or exceed the performance of policies initialized with language-pretrained representations on some manipulation tasks, suggesting that direct sensorimotor learning may be more efficient than text-based transfer for certain task types.",
            "citations": [
                "Levine et al. (2016) End-to-End Training of Deep Visuomotor Policies",
                "Kalashnikov et al. (2018) QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation"
            ]
        },
        {
            "text": "Some research indicates that visual grounding and perceptual learning are more critical bottlenecks than action-space alignment for embodied tasks, particularly in manipulation domains.",
            "citations": [
                "Pinto et al. (2016) Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours"
            ]
        }
    ],
    "special_cases": [
        "For tasks where low-level control precision is critical (e.g., fine-grained manipulation requiring sub-millimeter accuracy, dynamic locomotion over rough terrain, dexterous in-hand manipulation), action-space alignment may provide minimal benefits (&lt;10% sample complexity reduction) because text descriptions lack the necessary sensorimotor detail and the primitive-level alignment bottleneck dominates.",
        "In domains with highly domain-specific action primitives (e.g., surgical robotics, specialized industrial tasks, artistic performance), transfer may require explicit human-designed mappings or demonstrations rather than learned alignment, as the action semantics may not be well-represented in general text corpora.",
        "When 3D environments have stochastic dynamics or partial observability, action-space alignment based on deterministic text descriptions may lead to brittle transfer, requiring additional mechanisms for handling uncertainty and incomplete information.",
        "For tasks requiring real-time reactive control (e.g., catching fast-moving objects, collision avoidance in dynamic environments), the deliberative nature of language-based action representations may introduce latency incompatible with task requirements, limiting the utility of text-based transfer.",
        "In scenarios where the 3D environment has significantly different physics or affordances than those implicitly assumed in text pretraining (e.g., zero-gravity environments, underwater robotics, micro-scale manipulation), action-space alignment may be weak or misleading, requiring substantial fine-tuning or domain adaptation.",
        "For very short-horizon tasks (&lt;3 steps) where planning is minimal, the benefits of high-level action-space alignment are reduced, and low-level sensorimotor learning may be more efficient.",
        "When text pretraining data contains incorrect or inconsistent action semantics (e.g., fictional or metaphorical action descriptions), this can lead to negative transfer where pretrained representations harm rather than help 3D task learning."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Shridhar et al. (2020) ALFWorld: Aligning Text and Embodied Environments for Interactive Learning [Related work on text-embodied alignment but focuses on environment and task alignment rather than specifically theorizing about action-space structure and multi-level alignment mechanisms]",
            "Taylor & Stone (2009) Transfer Learning for Reinforcement Learning Domains: A Survey [General transfer learning framework but does not specifically address text-to-3D action mapping or the role of language pretraining]",
            "Konidaris & Barto (2007) Building Portable Options: Skill Transfer in Reinforcement Learning [Addresses skill transfer and portable options but not cross-modal text-to-embodied transfer or the specific mechanisms of action-space alignment]",
            "Andreas et al. (2017) Modular Multitask Reinforcement Learning with Policy Sketches [Related to high-level action specification from language but not a comprehensive theory of text-to-3D transfer]",
            "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Demonstrates language-to-robot transfer but does not propose a formal theory of action-space alignment mechanisms]",
            "Nachum et al. (2018) Data-Efficient Hierarchical Reinforcement Learning [Addresses hierarchical action representations but not cross-modal transfer from text]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory about when and how pretraining on text worlds transfers to 3D embodied tasks, including mappings between high-level action semantics and low-level perception and predicted sample complexity gains.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-195",
    "original_theory_name": "Action-Space Alignment Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
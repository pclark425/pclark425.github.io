<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1883</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1883</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format of problem presentation acts as an information bottleneck, filtering which aspects of the problem are accessible to the LLM's internal processing. Formats that maximize relevant information and minimize irrelevant or distracting content enable more effective reasoning and higher performance, while formats that obscure or overload the relevant information reduce performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Relevant Information Maximization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; relevant_information_content</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_optimized &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better when extraneous information is removed and only relevant details are presented. </li>
    <li>Concise, focused prompts yield higher accuracy than verbose or cluttered ones. </li>
    <li>Prompt engineering studies show that reformatting questions to foreground key information improves LLM accuracy. </li>
    <li>Few-shot learning performance improves when demonstrations are clear and directly related to the target task. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt focus is known, the explicit information bottleneck framing and its predictive implications are novel.</p>            <p><strong>What Already Exists:</strong> Prompt brevity and focus are known to improve LLM performance.</p>            <p><strong>What is Novel:</strong> This law frames the effect as an information-theoretic bottleneck, predicting performance as a function of relevant information throughput.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt focus and calibration]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Relevant information in prompts]</li>
    <li>Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt content and performance]</li>
</ul>
            <h3>Statement 1: Irrelevant Information Suppression Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; irrelevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_decreased &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are distracted by irrelevant details, leading to lower accuracy and more hallucinations. </li>
    <li>Experiments show that adding unrelated context to prompts reduces performance. </li>
    <li>Prompt injection attacks exploit LLMs' inability to ignore irrelevant or adversarial content. </li>
    <li>Studies on context length show that excessive or off-topic context can degrade LLM output quality. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect is known, but the law's formalization and predictive structure are new.</p>            <p><strong>What Already Exists:</strong> Known that irrelevant context can harm LLM performance.</p>            <p><strong>What is Novel:</strong> This law formalizes the effect as a bottleneck and predicts a monotonic relationship between irrelevant information and performance drop.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2020) How Can We Know What Language Models Know? [Irrelevant context and model confusion]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt content and performance]</li>
    <li>Perez et al. (2022) Ignore Previous Instructions: Manipulating Model Behavior via Prompt Injection [Prompt injection and distraction]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt is stripped of all irrelevant details and only the core question remains, LLM performance will improve.</li>
                <li>If extraneous information is added to a prompt, LLM accuracy will decrease, even if the relevant information is still present.</li>
                <li>Reformatting a problem to highlight relevant variables will yield higher LLM accuracy than a format with the same information but less clarity.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If irrelevant information is semantically similar to relevant information, will the LLM be more or less distracted than by completely unrelated information?</li>
                <li>If the LLM is trained with adversarially constructed irrelevant information, can it learn to ignore such distractions, or does performance always degrade?</li>
                <li>Does the effect of irrelevant information scale linearly, sublinearly, or superlinearly with the amount of irrelevant content?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLM performance does not decrease when irrelevant information is added, the theory would be challenged.</li>
                <li>If maximizing relevant information does not improve performance, the theory would be called into question.</li>
                <li>If LLMs can consistently ignore irrelevant information regardless of format, the bottleneck framing may be invalid.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs use seemingly irrelevant information as weak signals for reasoning. </li>
    <li>Instances where LLMs perform well despite verbose or cluttered prompts, possibly due to redundancy aiding error correction. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is a formal synthesis of known prompt effects, but the bottleneck analogy and predictive structure are novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt content and performance]</li>
    <li>Jiang et al. (2020) How Can We Know What Language Models Know? [Irrelevant context and model confusion]</li>
    <li>Perez et al. (2022) Ignore Previous Instructions: Manipulating Model Behavior via Prompt Injection [Prompt injection and distraction]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of LLM Problem Presentation",
    "theory_description": "This theory proposes that the format of problem presentation acts as an information bottleneck, filtering which aspects of the problem are accessible to the LLM's internal processing. Formats that maximize relevant information and minimize irrelevant or distracting content enable more effective reasoning and higher performance, while formats that obscure or overload the relevant information reduce performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Relevant Information Maximization Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "relevant_information_content"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_optimized",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better when extraneous information is removed and only relevant details are presented.",
                        "uuids": []
                    },
                    {
                        "text": "Concise, focused prompts yield higher accuracy than verbose or cluttered ones.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering studies show that reformatting questions to foreground key information improves LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Few-shot learning performance improves when demonstrations are clear and directly related to the target task.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt brevity and focus are known to improve LLM performance.",
                    "what_is_novel": "This law frames the effect as an information-theoretic bottleneck, predicting performance as a function of relevant information throughput.",
                    "classification_explanation": "While prompt focus is known, the explicit information bottleneck framing and its predictive implications are novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt focus and calibration]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Relevant information in prompts]",
                        "Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt content and performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Irrelevant Information Suppression Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "irrelevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_decreased",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are distracted by irrelevant details, leading to lower accuracy and more hallucinations.",
                        "uuids": []
                    },
                    {
                        "text": "Experiments show that adding unrelated context to prompts reduces performance.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt injection attacks exploit LLMs' inability to ignore irrelevant or adversarial content.",
                        "uuids": []
                    },
                    {
                        "text": "Studies on context length show that excessive or off-topic context can degrade LLM output quality.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Known that irrelevant context can harm LLM performance.",
                    "what_is_novel": "This law formalizes the effect as a bottleneck and predicts a monotonic relationship between irrelevant information and performance drop.",
                    "classification_explanation": "The effect is known, but the law's formalization and predictive structure are new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2020) How Can We Know What Language Models Know? [Irrelevant context and model confusion]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt content and performance]",
                        "Perez et al. (2022) Ignore Previous Instructions: Manipulating Model Behavior via Prompt Injection [Prompt injection and distraction]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt is stripped of all irrelevant details and only the core question remains, LLM performance will improve.",
        "If extraneous information is added to a prompt, LLM accuracy will decrease, even if the relevant information is still present.",
        "Reformatting a problem to highlight relevant variables will yield higher LLM accuracy than a format with the same information but less clarity."
    ],
    "new_predictions_unknown": [
        "If irrelevant information is semantically similar to relevant information, will the LLM be more or less distracted than by completely unrelated information?",
        "If the LLM is trained with adversarially constructed irrelevant information, can it learn to ignore such distractions, or does performance always degrade?",
        "Does the effect of irrelevant information scale linearly, sublinearly, or superlinearly with the amount of irrelevant content?"
    ],
    "negative_experiments": [
        "If LLM performance does not decrease when irrelevant information is added, the theory would be challenged.",
        "If maximizing relevant information does not improve performance, the theory would be called into question.",
        "If LLMs can consistently ignore irrelevant information regardless of format, the bottleneck framing may be invalid."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs use seemingly irrelevant information as weak signals for reasoning.",
            "uuids": []
        },
        {
            "text": "Instances where LLMs perform well despite verbose or cluttered prompts, possibly due to redundancy aiding error correction.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can ignore irrelevant information when explicitly instructed to do so.",
            "uuids": []
        },
        {
            "text": "Instruction-tuned models sometimes show robustness to distractors in prompts.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For highly trained LLMs with explicit instruction-following, the effect of irrelevant information may be mitigated.",
        "Tasks requiring integration of multiple information sources may not benefit from maximal brevity.",
        "Redundant presentation of relevant information may sometimes improve performance by providing error correction."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and the impact of relevant/irrelevant information on LLM performance are established.",
        "what_is_novel": "The information bottleneck framing and the explicit prediction of monotonic performance changes as a function of information content are new.",
        "classification_explanation": "The theory is a formal synthesis of known prompt effects, but the bottleneck analogy and predictive structure are novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt content and performance]",
            "Jiang et al. (2020) How Can We Know What Language Models Know? [Irrelevant context and model confusion]",
            "Perez et al. (2022) Ignore Previous Instructions: Manipulating Model Behavior via Prompt Injection [Prompt injection and distraction]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-652",
    "original_theory_name": "Prompt Format as a High-Dimensional Control Signal for LLM Computation",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
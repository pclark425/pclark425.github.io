<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain-Alignment Generalization Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1620</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1620</p>
                <p><strong>Name:</strong> Domain-Alignment Generalization Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory posits that the accuracy of LLMs as scientific simulators is fundamentally determined by the degree of alignment between the LLM's internal representations and the epistemic structure of the target scientific subdomain. When the LLM's learned representations (from pretraining and fine-tuning) are congruent with the subdomain's core concepts, reasoning patterns, and data modalities, simulation accuracy is maximized. Misalignment—due to gaps in training data, representational mismatches, or epistemic novelty—systematically reduces accuracy, regardless of prompt engineering or model size.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Epistemic Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM internal representations &#8594; are_aligned_with &#8594; subdomain epistemic structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; is_maximized &#8594; in the subdomain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs pretrained on domain-specific corpora (e.g., biomedical, legal) outperform general LLMs on subdomain tasks. </li>
    <li>Fine-tuning on subdomain data improves LLM accuracy, even with identical architectures. </li>
    <li>LLMs struggle with subdomains that use unique epistemic structures (e.g., formal logic, advanced mathematics) not well represented in training data. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While domain adaptation is empirically established, the theory formalizes the necessity of epistemic alignment at the representational level.</p>            <p><strong>What Already Exists:</strong> Domain adaptation and fine-tuning are known to improve LLM performance in specific subdomains.</p>            <p><strong>What is Novel:</strong> The explicit theoretical link between internal representation alignment and epistemic structure as a necessary and sufficient condition for simulation accuracy.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [Domain adaptation improves performance]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Discusses representation alignment]</li>
</ul>
            <h3>Statement 1: Epistemic Misalignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM internal representations &#8594; are_misaligned_with &#8594; subdomain epistemic structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; is_systematically_reduced &#8594; in the subdomain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs trained on general web data perform poorly on tasks requiring specialized scientific reasoning (e.g., symbolic logic, advanced chemistry). </li>
    <li>Attempts to use LLMs for tasks in epistemically novel subdomains (e.g., new scientific fields) result in low accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law formalizes a known empirical trend into a theoretical principle.</p>            <p><strong>What Already Exists:</strong> Empirical evidence shows LLMs underperform in domains with little training data or unique epistemic structures.</p>            <p><strong>What is Novel:</strong> The systematic, theory-driven link between representational misalignment and accuracy reduction.</p>
            <p><strong>References:</strong> <ul>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Discusses domain transfer limitations]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs fine-tuned on a subdomain's epistemic structure will outperform those only exposed to general data, even with identical architectures.</li>
                <li>Subdomains with highly unique epistemic structures will require specialized pretraining or adaptation for high simulation accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained on synthetic data that mimics a subdomain's epistemic structure, they may achieve high accuracy even without real data.</li>
                <li>Emergent subdomains with hybrid epistemic structures may require novel LLM architectures for optimal alignment.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs achieve high accuracy in a subdomain despite clear representational misalignment, the theory is challenged.</li>
                <li>If fine-tuning on subdomain data does not improve accuracy, the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs may use general reasoning heuristics to partially compensate for epistemic misalignment. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory formalizes and extends empirical findings into a new theoretical framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [Domain adaptation]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Representation alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain-Alignment Generalization Theory",
    "theory_description": "This theory posits that the accuracy of LLMs as scientific simulators is fundamentally determined by the degree of alignment between the LLM's internal representations and the epistemic structure of the target scientific subdomain. When the LLM's learned representations (from pretraining and fine-tuning) are congruent with the subdomain's core concepts, reasoning patterns, and data modalities, simulation accuracy is maximized. Misalignment—due to gaps in training data, representational mismatches, or epistemic novelty—systematically reduces accuracy, regardless of prompt engineering or model size.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Epistemic Alignment Law",
                "if": [
                    {
                        "subject": "LLM internal representations",
                        "relation": "are_aligned_with",
                        "object": "subdomain epistemic structure"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "is_maximized",
                        "object": "in the subdomain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs pretrained on domain-specific corpora (e.g., biomedical, legal) outperform general LLMs on subdomain tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Fine-tuning on subdomain data improves LLM accuracy, even with identical architectures.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs struggle with subdomains that use unique epistemic structures (e.g., formal logic, advanced mathematics) not well represented in training data.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Domain adaptation and fine-tuning are known to improve LLM performance in specific subdomains.",
                    "what_is_novel": "The explicit theoretical link between internal representation alignment and epistemic structure as a necessary and sufficient condition for simulation accuracy.",
                    "classification_explanation": "While domain adaptation is empirically established, the theory formalizes the necessity of epistemic alignment at the representational level.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [Domain adaptation improves performance]",
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Discusses representation alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Epistemic Misalignment Law",
                "if": [
                    {
                        "subject": "LLM internal representations",
                        "relation": "are_misaligned_with",
                        "object": "subdomain epistemic structure"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "is_systematically_reduced",
                        "object": "in the subdomain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs trained on general web data perform poorly on tasks requiring specialized scientific reasoning (e.g., symbolic logic, advanced chemistry).",
                        "uuids": []
                    },
                    {
                        "text": "Attempts to use LLMs for tasks in epistemically novel subdomains (e.g., new scientific fields) result in low accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical evidence shows LLMs underperform in domains with little training data or unique epistemic structures.",
                    "what_is_novel": "The systematic, theory-driven link between representational misalignment and accuracy reduction.",
                    "classification_explanation": "The law formalizes a known empirical trend into a theoretical principle.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Discusses domain transfer limitations]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs fine-tuned on a subdomain's epistemic structure will outperform those only exposed to general data, even with identical architectures.",
        "Subdomains with highly unique epistemic structures will require specialized pretraining or adaptation for high simulation accuracy."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained on synthetic data that mimics a subdomain's epistemic structure, they may achieve high accuracy even without real data.",
        "Emergent subdomains with hybrid epistemic structures may require novel LLM architectures for optimal alignment."
    ],
    "negative_experiments": [
        "If LLMs achieve high accuracy in a subdomain despite clear representational misalignment, the theory is challenged.",
        "If fine-tuning on subdomain data does not improve accuracy, the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs may use general reasoning heuristics to partially compensate for epistemic misalignment.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Occasional high performance by general LLMs on tasks in unfamiliar subdomains suggests possible exceptions.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Subdomains with overlapping epistemic structures may not require strict alignment for high accuracy.",
        "Highly flexible LLMs with meta-learning capabilities may adapt representations on the fly."
    ],
    "existing_theory": {
        "what_already_exists": "Domain adaptation and transfer learning are established, but not formalized at the epistemic structure level.",
        "what_is_novel": "The explicit theory of representational-epistemic alignment as a necessary and sufficient condition for simulation accuracy.",
        "classification_explanation": "The theory formalizes and extends empirical findings into a new theoretical framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [Domain adaptation]",
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [Representation alignment]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-635",
    "original_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
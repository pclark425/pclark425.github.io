<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Surface Feature Overweighting and Calibration Drift in LLM-as-a-Judge - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-540</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-540</p>
                <p><strong>Name:</strong> Surface Feature Overweighting and Calibration Drift in LLM-as-a-Judge</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of what LLM-as-a-judge style evaluations lose compared to human evaluations, based on the following results.</p>
                <p><strong>Description:</strong> LLM-as-a-judge evaluations systematically overweight surface-level features (such as length, formatting, fluency, and superficial cues) relative to human evaluators, especially in the absence of explicit, well-calibrated prompts or adversarially robust evaluation schemes. This overweighting leads to misalignment with human judgments, particularly on high-quality, adversarial, or nuanced cases, and is exacerbated by calibration drift (prompt sensitivity, model capacity, and fine-tuning effects). The theory posits that LLM judges, by default, act as surface-feature classifiers unless actively mitigated, and that their calibration (mapping of internal confidence/logits to human-perceived quality) is non-linear and unstable across tasks, models, and prompt designs.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Surface Feature Overweighting Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; evaluates &#8594; text_output<span style="color: #888888;">, and</span></div>
        <div>&#8226; text_output &#8594; has_surface_features &#8594; high (e.g., length, formatting, fluency, presence of references)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; assigns &#8594; higher quality or preference scores than humans would</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs show length bias, beauty bias (rich formatting), and authority bias (fake references), often preferring verbose or well-formatted outputs even when semantically incorrect. <a href="../results/extraction-result-4004.html#e4004.1" class="evidence-link">[e4004.1]</a> <a href="../results/extraction-result-3863.html#e3863.4" class="evidence-link">[e3863.4]</a> <a href="../results/extraction-result-3863.html#e3863.3" class="evidence-link">[e3863.3]</a> <a href="../results/extraction-result-3860.html#e3860.2" class="evidence-link">[e3860.2]</a> <a href="../results/extraction-result-4013.html#e4013.1" class="evidence-link">[e4013.1]</a> <a href="../results/extraction-result-4013.html#e4013.2" class="evidence-link">[e4013.2]</a> <a href="../results/extraction-result-4009.html#e4009.2" class="evidence-link">[e4009.2]</a> <a href="../results/extraction-result-3858.html#e3858.3" class="evidence-link">[e3858.3]</a> <a href="../results/extraction-result-3998.html#e3998.2" class="evidence-link">[e3998.2]</a> <a href="../results/extraction-result-3863.html#e3863.0" class="evidence-link">[e3863.0]</a> <a href="../results/extraction-result-3863.html#e3863.5" class="evidence-link">[e3863.5]</a> <a href="../results/extraction-result-3998.html#e3998.3" class="evidence-link">[e3998.3]</a> <a href="../results/extraction-result-4004.html#e4004.0" class="evidence-link">[e4004.0]</a> <a href="../results/extraction-result-4004.html#e4004.2" class="evidence-link">[e4004.2]</a> <a href="../results/extraction-result-4013.html#e4013.0" class="evidence-link">[e4013.0]</a> <a href="../results/extraction-result-4013.html#e4013.5" class="evidence-link">[e4013.5]</a> <a href="../results/extraction-result-3871.html#e3871.1" class="evidence-link">[e3871.1]</a> <a href="../results/extraction-result-4013.html#e4013.4" class="evidence-link">[e4013.4]</a> <a href="../results/extraction-result-3855.html#e3855.3" class="evidence-link">[e3855.3]</a> <a href="../results/extraction-result-3855.html#e3855.2" class="evidence-link">[e3855.2]</a> <a href="../results/extraction-result-3855.html#e3855.8" class="evidence-link">[e3855.8]</a> <a href="../results/extraction-result-3855.html#e3855.0" class="evidence-link">[e3855.0]</a> <a href="../results/extraction-result-3855.html#e3855.9" class="evidence-link">[e3855.9]</a> <a href="../results/extraction-result-4012.html#e4012.0" class="evidence-link">[e4012.0]</a> <a href="../results/extraction-result-4012.html#e4012.1" class="evidence-link">[e4012.1]</a> <a href="../results/extraction-result-4010.html#e4010.2" class="evidence-link">[e4010.2]</a> <a href="../results/extraction-result-4010.html#e4010.0" class="evidence-link">[e4010.0]</a> <a href="../results/extraction-result-4010.html#e4010.1" class="evidence-link">[e4010.1]</a> <a href="../results/extraction-result-4010.html#e4010.3" class="evidence-link">[e4010.3]</a> <a href="../results/extraction-result-4017.html#e4017.0" class="evidence-link">[e4017.0]</a> <a href="../results/extraction-result-4017.html#e4017.1" class="evidence-link">[e4017.1]</a> <a href="../results/extraction-result-4017.html#e4017.2" class="evidence-link">[e4017.2]</a> <a href="../results/extraction-result-4017.html#e4017.3" class="evidence-link">[e4017.3]</a> <a href="../results/extraction-result-4017.html#e4017.4" class="evidence-link">[e4017.4]</a> <a href="../results/extraction-result-4017.html#e4017.5" class="evidence-link">[e4017.5]</a> <a href="../results/extraction-result-4005.html#e4005.0" class="evidence-link">[e4005.0]</a> <a href="../results/extraction-result-4005.html#e4005.2" class="evidence-link">[e4005.2]</a> <a href="../results/extraction-result-4009.html#e4009.0" class="evidence-link">[e4009.0]</a> <a href="../results/extraction-result-4009.html#e4009.1" class="evidence-link">[e4009.1]</a> <a href="../results/extraction-result-4009.html#e4009.3" class="evidence-link">[e4009.3]</a> <a href="../results/extraction-result-3998.html#e3998.0" class="evidence-link">[e3998.0]</a> <a href="../results/extraction-result-3998.html#e3998.1" class="evidence-link">[e3998.1]</a> <a href="../results/extraction-result-3998.html#e3998.4" class="evidence-link">[e3998.4]</a> <a href="../results/extraction-result-3999.html#e3999.0" class="evidence-link">[e3999.0]</a> <a href="../results/extraction-result-3999.html#e3999.1" class="evidence-link">[e3999.1]</a> <a href="../results/extraction-result-3858.html#e3858.2" class="evidence-link">[e3858.2]</a> <a href="../results/extraction-result-3858.html#e3858.4" class="evidence-link">[e3858.4]</a> <a href="../results/extraction-result-3858.html#e3858.6" class="evidence-link">[e3858.6]</a> <a href="../results/extraction-result-3858.html#e3858.7" class="evidence-link">[e3858.7]</a> <a href="../results/extraction-result-3858.html#e3858.8" class="evidence-link">[e3858.8]</a> <a href="../results/extraction-result-3858.html#e3858.9" class="evidence-link">[e3858.9]</a> <a href="../results/extraction-result-3859.html#e3859.0" class="evidence-link">[e3859.0]</a> <a href="../results/extraction-result-3859.html#e3859.2" class="evidence-link">[e3859.2]</a> <a href="../results/extraction-result-3859.html#e3859.3" class="evidence-link">[e3859.3]</a> <a href="../results/extraction-result-3859.html#e3859.4" class="evidence-link">[e3859.4]</a> <a href="../results/extraction-result-3861.html#e3861.0" class="evidence-link">[e3861.0]</a> <a href="../results/extraction-result-3861.html#e3861.1" class="evidence-link">[e3861.1]</a> <a href="../results/extraction-result-3861.html#e3861.2" class="evidence-link">[e3861.2]</a> <a href="../results/extraction-result-3861.html#e3861.3" class="evidence-link">[e3861.3]</a> <a href="../results/extraction-result-3861.html#e3861.4" class="evidence-link">[e3861.4]</a> <a href="../results/extraction-result-3861.html#e3861.5" class="evidence-link">[e3861.5]</a> <a href="../results/extraction-result-3861.html#e3861.6" class="evidence-link">[e3861.6]</a> <a href="../results/extraction-result-3849.html#e3849.0" class="evidence-link">[e3849.0]</a> <a href="../results/extraction-result-3849.html#e3849.1" class="evidence-link">[e3849.1]</a> <a href="../results/extraction-result-3842.html#e3842.0" class="evidence-link">[e3842.0]</a> <a href="../results/extraction-result-3843.html#e3843.0" class="evidence-link">[e3843.0]</a> <a href="../results/extraction-result-3844.html#e3844.0" class="evidence-link">[e3844.0]</a> <a href="../results/extraction-result-3844.html#e3844.1" class="evidence-link">[e3844.1]</a> <a href="../results/extraction-result-3845.html#e3845.0" class="evidence-link">[e3845.0]</a> <a href="../results/extraction-result-3845.html#e3845.1" class="evidence-link">[e3845.1]</a> <a href="../results/extraction-result-4006.html#e4006.0" class="evidence-link">[e4006.0]</a> <a href="../results/extraction-result-4006.html#e4006.1" class="evidence-link">[e4006.1]</a> <a href="../results/extraction-result-4006.html#e4006.2" class="evidence-link">[e4006.2]</a> </li>
    <li>Fine-tuned judges and reward models fail on adversarial examples that decouple surface quality from correctness. <a href="../results/extraction-result-3871.html#e3871.1" class="evidence-link">[e3871.1]</a> <a href="../results/extraction-result-4013.html#e4013.4" class="evidence-link">[e4013.4]</a> <a href="../results/extraction-result-3871.html#e3871.0" class="evidence-link">[e3871.0]</a> <a href="../results/extraction-result-3871.html#e3871.2" class="evidence-link">[e3871.2]</a> <a href="../results/extraction-result-3871.html#e3871.3" class="evidence-link">[e3871.3]</a> <a href="../results/extraction-result-3871.html#e3871.4" class="evidence-link">[e3871.4]</a> </li>
    <li>LLMs prioritize polished/engaging/formatted outputs over strict instruction adherence; Chain-of-Thought (CoT) explanations can emphasize surface-level appealing features and lead to worse judgments; presentation order (which output is labeled (a)/(b)) affects judgments significantly. <a href="../results/extraction-result-4013.html#e4013.2" class="evidence-link">[e4013.2]</a> <a href="../results/extraction-result-4013.html#e4013.3" class="evidence-link">[e4013.3]</a> <a href="../results/extraction-result-4013.html#e4013.1" class="evidence-link">[e4013.1]</a> <a href="../results/extraction-result-4013.html#e4013.5" class="evidence-link">[e4013.5]</a> <a href="../results/extraction-result-4013.html#e4013.0" class="evidence-link">[e4013.0]</a> </li>
    <li>LLMs amplify several cognitive biases (order, compassion/name, egocentric/self-preference, salience/length, bandwagon, attentional) in evaluation outputs, often to a greater extent than humans. <a href="../results/extraction-result-3998.html#e3998.2" class="evidence-link">[e3998.2]</a> <a href="../results/extraction-result-3998.html#e3998.3" class="evidence-link">[e3998.3]</a> <a href="../results/extraction-result-3998.html#e3998.0" class="evidence-link">[e3998.0]</a> <a href="../results/extraction-result-3998.html#e3998.1" class="evidence-link">[e3998.1]</a> <a href="../results/extraction-result-3998.html#e3998.4" class="evidence-link">[e3998.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Calibration Drift Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; is used &#8594; across different tasks, prompts, or model capacities</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; exhibits &#8594; non-linear and unstable mapping between internal scores and human-perceived quality</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM judge calibration is non-linear; prompt design, model size, and fine-tuning can cause large shifts in agreement with humans. <a href="../results/extraction-result-4005.html#e4005.0" class="evidence-link">[e4005.0]</a> <a href="../results/extraction-result-4015.html#e4015.2" class="evidence-link">[e4015.2]</a> <a href="../results/extraction-result-4015.html#e4015.1" class="evidence-link">[e4015.1]</a> <a href="../results/extraction-result-4012.html#e4012.0" class="evidence-link">[e4012.0]</a> <a href="../results/extraction-result-4012.html#e4012.1" class="evidence-link">[e4012.1]</a> <a href="../results/extraction-result-4010.html#e4010.3" class="evidence-link">[e4010.3]</a> <a href="../results/extraction-result-4007.html#e4007.1" class="evidence-link">[e4007.1]</a> <a href="../results/extraction-result-4017.html#e4017.0" class="evidence-link">[e4017.0]</a> <a href="../results/extraction-result-4017.html#e4017.1" class="evidence-link">[e4017.1]</a> <a href="../results/extraction-result-4017.html#e4017.2" class="evidence-link">[e4017.2]</a> <a href="../results/extraction-result-4017.html#e4017.3" class="evidence-link">[e4017.3]</a> <a href="../results/extraction-result-4017.html#e4017.4" class="evidence-link">[e4017.4]</a> <a href="../results/extraction-result-4017.html#e4017.5" class="evidence-link">[e4017.5]</a> <a href="../results/extraction-result-4013.html#e4013.3" class="evidence-link">[e4013.3]</a> <a href="../results/extraction-result-4013.html#e4013.2" class="evidence-link">[e4013.2]</a> <a href="../results/extraction-result-4013.html#e4013.1" class="evidence-link">[e4013.1]</a> <a href="../results/extraction-result-4013.html#e4013.0" class="evidence-link">[e4013.0]</a> <a href="../results/extraction-result-4013.html#e4013.5" class="evidence-link">[e4013.5]</a> <a href="../results/extraction-result-4010.html#e4010.0" class="evidence-link">[e4010.0]</a> <a href="../results/extraction-result-4010.html#e4010.1" class="evidence-link">[e4010.1]</a> <a href="../results/extraction-result-4010.html#e4010.2" class="evidence-link">[e4010.2]</a> <a href="../results/extraction-result-4010.html#e4010.3" class="evidence-link">[e4010.3]</a> <a href="../results/extraction-result-3860.html#e3860.5" class="evidence-link">[e3860.5]</a> <a href="../results/extraction-result-3860.html#e3860.4" class="evidence-link">[e3860.4]</a> <a href="../results/extraction-result-3860.html#e3860.6" class="evidence-link">[e3860.6]</a> <a href="../results/extraction-result-3861.html#e3861.4" class="evidence-link">[e3861.4]</a> <a href="../results/extraction-result-3861.html#e3861.5" class="evidence-link">[e3861.5]</a> <a href="../results/extraction-result-3861.html#e3861.6" class="evidence-link">[e3861.6]</a> <a href="../results/extraction-result-3855.html#e3855.0" class="evidence-link">[e3855.0]</a> <a href="../results/extraction-result-3855.html#e3855.9" class="evidence-link">[e3855.9]</a> <a href="../results/extraction-result-3855.html#e3855.2" class="evidence-link">[e3855.2]</a> <a href="../results/extraction-result-3855.html#e3855.3" class="evidence-link">[e3855.3]</a> <a href="../results/extraction-result-3855.html#e3855.4" class="evidence-link">[e3855.4]</a> <a href="../results/extraction-result-3855.html#e3855.6" class="evidence-link">[e3855.6]</a> <a href="../results/extraction-result-3855.html#e3855.7" class="evidence-link">[e3855.7]</a> <a href="../results/extraction-result-3855.html#e3855.8" class="evidence-link">[e3855.8]</a> </li>
    <li>Percent agreement can be high while absolute score deltas are large; Scott's π reveals hidden disagreement. <a href="../results/extraction-result-3861.html#e3861.4" class="evidence-link">[e3861.4]</a> <a href="../results/extraction-result-3861.html#e3861.1" class="evidence-link">[e3861.1]</a> <a href="../results/extraction-result-3861.html#e3861.2" class="evidence-link">[e3861.2]</a> <a href="../results/extraction-result-3861.html#e3861.3" class="evidence-link">[e3861.3]</a> </li>
    <li>Prompt ablations for judge models (notably GPT-4) on KILT Natural Questions; comparisons across zero-shot, few-shot, placement/format of instruction, and an explicit 'don't overthink' instruction show large differences in agreement. <a href="../results/extraction-result-4015.html#e4015.2" class="evidence-link">[e4015.2]</a> <a href="../results/extraction-result-4007.html#e4007.1" class="evidence-link">[e4007.1]</a> <a href="../results/extraction-result-4005.html#e4005.0" class="evidence-link">[e4005.0]</a> </li>
    <li>LLM judgments systematically differ from humans in several qualitative ways: LLMs show a strong familiarity bias (prefer lower-perplexity/familiar text), under-utilize portions of rating scales (sparse predictions, round-number peaks), over-correlate multiple attribute scores due to anchoring, and are sensitive to prompt wording, temperature/CoT use, and presence/absence of the source document. <a href="../results/extraction-result-4012.html#e4012.0" class="evidence-link">[e4012.0]</a> <a href="../results/extraction-result-4012.html#e4012.1" class="evidence-link">[e4012.1]</a> <a href="../results/extraction-result-4010.html#e4010.0" class="evidence-link">[e4010.0]</a> <a href="../results/extraction-result-4010.html#e4010.1" class="evidence-link">[e4010.1]</a> <a href="../results/extraction-result-4010.html#e4010.2" class="evidence-link">[e4010.2]</a> <a href="../results/extraction-result-4010.html#e4010.3" class="evidence-link">[e4010.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Adversarial Vulnerability Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; is presented with &#8594; adversarially constructed outputs (e.g., universal concatenations, superficial distractors)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM-as-a-judge &#8594; is more likely than humans &#8594; to assign high scores or prefer the adversarial output</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Absolute scoring is highly vulnerable to short universal adversarial phrases; LLMs are more easily fooled than humans by such attacks. <a href="../results/extraction-result-4009.html#e4009.3" class="evidence-link">[e4009.3]</a> <a href="../results/extraction-result-4009.html#e4009.2" class="evidence-link">[e4009.2]</a> <a href="../results/extraction-result-3863.html#e3863.4" class="evidence-link">[e3863.4]</a> <a href="../results/extraction-result-3863.html#e3863.3" class="evidence-link">[e3863.3]</a> <a href="../results/extraction-result-3998.html#e3998.2" class="evidence-link">[e3998.2]</a> <a href="../results/extraction-result-4013.html#e4013.1" class="evidence-link">[e4013.1]</a> <a href="../results/extraction-result-4013.html#e4013.2" class="evidence-link">[e4013.2]</a> <a href="../results/extraction-result-4013.html#e4013.4" class="evidence-link">[e4013.4]</a> <a href="../results/extraction-result-3871.html#e3871.1" class="evidence-link">[e3871.1]</a> <a href="../results/extraction-result-3871.html#e3871.2" class="evidence-link">[e3871.2]</a> <a href="../results/extraction-result-3871.html#e3871.3" class="evidence-link">[e3871.3]</a> <a href="../results/extraction-result-3871.html#e3871.4" class="evidence-link">[e3871.4]</a> </li>
    <li>Comparative assessment is more robust but still shows mild vulnerabilities; detection methods like perplexity work but can be circumvented by adaptive adversaries. <a href="../results/extraction-result-4009.html#e4009.3" class="evidence-link">[e4009.3]</a> <a href="../results/extraction-result-4009.html#e4009.2" class="evidence-link">[e4009.2]</a> <a href="../results/extraction-result-4009.html#e4009.0" class="evidence-link">[e4009.0]</a> <a href="../results/extraction-result-4009.html#e4009.1" class="evidence-link">[e4009.1]</a> </li>
    <li>LLMs are more susceptible than humans to beauty/formatting and authority/fake-reference biases, as shown by higher ASR (Attack Success Rate) in these perturbation experiments. <a href="../results/extraction-result-3863.html#e3863.4" class="evidence-link">[e3863.4]</a> <a href="../results/extraction-result-3863.html#e3863.3" class="evidence-link">[e3863.3]</a> <a href="../results/extraction-result-3863.html#e3863.0" class="evidence-link">[e3863.0]</a> <a href="../results/extraction-result-3998.html#e3998.2" class="evidence-link">[e3998.2]</a> <a href="../results/extraction-result-3998.html#e3998.3" class="evidence-link">[e3998.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new LLM judge is evaluated on a dataset where superficial features (e.g., length, formatting, presence of references) are decoupled from semantic quality, the LLM will systematically prefer the outputs with enhanced surface features, even when humans do not.</li>
                <li>If the prompt for an LLM judge is changed to emphasize surface features (e.g., 'rate the most detailed answer'), the LLM's agreement with human judgments will decrease, especially on adversarial or high-quality cases.</li>
                <li>If a new adversarial attack is designed to exploit a previously untested surface feature (e.g., use of rare vocabulary), LLM judges will be more susceptible than humans to being misled by this feature.</li>
                <li>If a fine-tuned judge is evaluated on an out-of-domain or adversarial dataset, it will show a sharp drop in agreement with human judgments, especially when surface features are manipulated.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a future LLM is trained with explicit adversarial robustness objectives and tested on unseen adversarial surface features, it may or may not overcome the surface feature overweighting; if it does, this would challenge the universality of the theory.</li>
                <li>If a hybrid LLM judge is constructed that dynamically calibrates its scoring using real-time human feedback, it is unknown whether calibration drift and surface feature overweighting will persist or be eliminated.</li>
                <li>If LLM judges are used to evaluate outputs in a non-textual modality (e.g., multimodal with images), it is unknown whether similar surface feature overweighting will occur or if new biases will emerge.</li>
                <li>If a panel of diverse LLM judges is used and their outputs are aggregated, it is unknown whether surface feature overweighting and calibration drift will be mitigated or persist in the aggregate.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an LLM judge, when presented with outputs where surface features are controlled (e.g., all outputs are of equal length and formatting), still shows systematic misalignment with human judgments, this would challenge the surface feature overweighting law.</li>
                <li>If calibration drift is not observed across a wide range of tasks, prompts, and model sizes (i.e., LLM judge-human agreement remains stable), this would call the calibration drift law into question.</li>
                <li>If LLM judges are robust to new adversarial attacks that exploit surface features without explicit adversarial training, this would challenge the adversarial vulnerability law.</li>
                <li>If fine-tuned judges generalize perfectly to out-of-domain and adversarial cases, this would challenge the theory's claim about overfitting and surface feature overweighting.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLM judges outperform humans in factual error detection (e.g., top LLMs have lower ASR than humans on factual-error perturbations). <a href="../results/extraction-result-3863.html#e3863.1" class="evidence-link">[e3863.1]</a> <a href="../results/extraction-result-3863.html#e3863.0" class="evidence-link">[e3863.0]</a> </li>
    <li>Instances where LLM judges are more consistent than human annotators (e.g., higher intercoder agreement in some annotation tasks). <a href="../results/extraction-result-3992.html#e3992.0" class="evidence-link">[e3992.0]</a> <a href="../results/extraction-result-3991.html#e3991.0" class="evidence-link">[e3991.0]</a> <a href="../results/extraction-result-3852.html#e3852.2" class="evidence-link">[e3852.2]</a> </li>
    <li>Some LLMs (e.g., GPT-4o, Claude-3) are less susceptible to certain biases (e.g., beauty/formatting) than humans or other LLMs. <a href="../results/extraction-result-3863.html#e3863.4" class="evidence-link">[e3863.4]</a> <a href="../results/extraction-result-3863.html#e3863.0" class="evidence-link">[e3863.0]</a> </li>
    <li>On some tasks (e.g., instruction following, mathematical reasoning traces), LLM judges can match or exceed human agreement. <a href="../results/extraction-result-3855.html#e3855.1" class="evidence-link">[e3855.1]</a> <a href="../results/extraction-result-4013.html#e4013.0" class="evidence-link">[e4013.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Wu & Aji (2023) Style Over Substance: Evaluation Biases for Large Language Models [Documents length, position, and surface feature biases in LLM evaluation]</li>
    <li>Zeng et al. (2023) Evaluating Large Language Models at Evaluating Instruction Following [Adversarial bias and prompt sensitivity in LLM evaluators]</li>
    <li>Raina et al. (2024) Is LLM-as-a-Judge Robust? [Adversarial vulnerability and surface feature attacks]</li>
    <li>Stureborg et al. (2024) Large Language Models are Inconsistent and Biased Evaluators [Calibration drift and prompt sensitivity]</li>
    <li>Benchmarking Cognitive Biases in Large Language Models as Evaluators (2023) [LLM cognitive bias amplification]</li>
    <li>JudgeLM: Fine-tuned Large Language Models are Scalable Judges (2023) [Fine-tuned judge overfitting and loss of generalization]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Surface Feature Overweighting and Calibration Drift in LLM-as-a-Judge",
    "theory_description": "LLM-as-a-judge evaluations systematically overweight surface-level features (such as length, formatting, fluency, and superficial cues) relative to human evaluators, especially in the absence of explicit, well-calibrated prompts or adversarially robust evaluation schemes. This overweighting leads to misalignment with human judgments, particularly on high-quality, adversarial, or nuanced cases, and is exacerbated by calibration drift (prompt sensitivity, model capacity, and fine-tuning effects). The theory posits that LLM judges, by default, act as surface-feature classifiers unless actively mitigated, and that their calibration (mapping of internal confidence/logits to human-perceived quality) is non-linear and unstable across tasks, models, and prompt designs.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Surface Feature Overweighting Law",
                "if": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "evaluates",
                        "object": "text_output"
                    },
                    {
                        "subject": "text_output",
                        "relation": "has_surface_features",
                        "object": "high (e.g., length, formatting, fluency, presence of references)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "assigns",
                        "object": "higher quality or preference scores than humans would"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs show length bias, beauty bias (rich formatting), and authority bias (fake references), often preferring verbose or well-formatted outputs even when semantically incorrect.",
                        "uuids": [
                            "e4004.1",
                            "e3863.4",
                            "e3863.3",
                            "e3860.2",
                            "e4013.1",
                            "e4013.2",
                            "e4009.2",
                            "e3858.3",
                            "e3998.2",
                            "e3863.0",
                            "e3863.5",
                            "e3998.3",
                            "e4004.0",
                            "e4004.2",
                            "e4013.0",
                            "e4013.5",
                            "e3871.1",
                            "e4013.4",
                            "e3855.3",
                            "e3855.2",
                            "e3855.8",
                            "e3855.0",
                            "e3855.9",
                            "e4012.0",
                            "e4012.1",
                            "e4010.2",
                            "e4010.0",
                            "e4010.1",
                            "e4010.3",
                            "e4017.0",
                            "e4017.1",
                            "e4017.2",
                            "e4017.3",
                            "e4017.4",
                            "e4017.5",
                            "e4005.0",
                            "e4005.2",
                            "e4009.0",
                            "e4009.1",
                            "e4009.3",
                            "e3998.0",
                            "e3998.1",
                            "e3998.4",
                            "e3999.0",
                            "e3999.1",
                            "e3858.2",
                            "e3858.4",
                            "e3858.6",
                            "e3858.7",
                            "e3858.8",
                            "e3858.9",
                            "e3859.0",
                            "e3859.2",
                            "e3859.3",
                            "e3859.4",
                            "e3861.0",
                            "e3861.1",
                            "e3861.2",
                            "e3861.3",
                            "e3861.4",
                            "e3861.5",
                            "e3861.6",
                            "e3849.0",
                            "e3849.1",
                            "e3842.0",
                            "e3843.0",
                            "e3844.0",
                            "e3844.1",
                            "e3845.0",
                            "e3845.1",
                            "e4006.0",
                            "e4006.1",
                            "e4006.2"
                        ]
                    },
                    {
                        "text": "Fine-tuned judges and reward models fail on adversarial examples that decouple surface quality from correctness.",
                        "uuids": [
                            "e3871.1",
                            "e4013.4",
                            "e3871.0",
                            "e3871.2",
                            "e3871.3",
                            "e3871.4"
                        ]
                    },
                    {
                        "text": "LLMs prioritize polished/engaging/formatted outputs over strict instruction adherence; Chain-of-Thought (CoT) explanations can emphasize surface-level appealing features and lead to worse judgments; presentation order (which output is labeled (a)/(b)) affects judgments significantly.",
                        "uuids": [
                            "e4013.2",
                            "e4013.3",
                            "e4013.1",
                            "e4013.5",
                            "e4013.0"
                        ]
                    },
                    {
                        "text": "LLMs amplify several cognitive biases (order, compassion/name, egocentric/self-preference, salience/length, bandwagon, attentional) in evaluation outputs, often to a greater extent than humans.",
                        "uuids": [
                            "e3998.2",
                            "e3998.3",
                            "e3998.0",
                            "e3998.1",
                            "e3998.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Calibration Drift Law",
                "if": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "is used",
                        "object": "across different tasks, prompts, or model capacities"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "exhibits",
                        "object": "non-linear and unstable mapping between internal scores and human-perceived quality"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM judge calibration is non-linear; prompt design, model size, and fine-tuning can cause large shifts in agreement with humans.",
                        "uuids": [
                            "e4005.0",
                            "e4015.2",
                            "e4015.1",
                            "e4012.0",
                            "e4012.1",
                            "e4010.3",
                            "e4007.1",
                            "e4017.0",
                            "e4017.1",
                            "e4017.2",
                            "e4017.3",
                            "e4017.4",
                            "e4017.5",
                            "e4013.3",
                            "e4013.2",
                            "e4013.1",
                            "e4013.0",
                            "e4013.5",
                            "e4010.0",
                            "e4010.1",
                            "e4010.2",
                            "e4010.3",
                            "e3860.5",
                            "e3860.4",
                            "e3860.6",
                            "e3861.4",
                            "e3861.5",
                            "e3861.6",
                            "e3855.0",
                            "e3855.9",
                            "e3855.2",
                            "e3855.3",
                            "e3855.4",
                            "e3855.6",
                            "e3855.7",
                            "e3855.8"
                        ]
                    },
                    {
                        "text": "Percent agreement can be high while absolute score deltas are large; Scott's π reveals hidden disagreement.",
                        "uuids": [
                            "e3861.4",
                            "e3861.1",
                            "e3861.2",
                            "e3861.3"
                        ]
                    },
                    {
                        "text": "Prompt ablations for judge models (notably GPT-4) on KILT Natural Questions; comparisons across zero-shot, few-shot, placement/format of instruction, and an explicit 'don't overthink' instruction show large differences in agreement.",
                        "uuids": [
                            "e4015.2",
                            "e4007.1",
                            "e4005.0"
                        ]
                    },
                    {
                        "text": "LLM judgments systematically differ from humans in several qualitative ways: LLMs show a strong familiarity bias (prefer lower-perplexity/familiar text), under-utilize portions of rating scales (sparse predictions, round-number peaks), over-correlate multiple attribute scores due to anchoring, and are sensitive to prompt wording, temperature/CoT use, and presence/absence of the source document.",
                        "uuids": [
                            "e4012.0",
                            "e4012.1",
                            "e4010.0",
                            "e4010.1",
                            "e4010.2",
                            "e4010.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Adversarial Vulnerability Law",
                "if": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "is presented with",
                        "object": "adversarially constructed outputs (e.g., universal concatenations, superficial distractors)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM-as-a-judge",
                        "relation": "is more likely than humans",
                        "object": "to assign high scores or prefer the adversarial output"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Absolute scoring is highly vulnerable to short universal adversarial phrases; LLMs are more easily fooled than humans by such attacks.",
                        "uuids": [
                            "e4009.3",
                            "e4009.2",
                            "e3863.4",
                            "e3863.3",
                            "e3998.2",
                            "e4013.1",
                            "e4013.2",
                            "e4013.4",
                            "e3871.1",
                            "e3871.2",
                            "e3871.3",
                            "e3871.4"
                        ]
                    },
                    {
                        "text": "Comparative assessment is more robust but still shows mild vulnerabilities; detection methods like perplexity work but can be circumvented by adaptive adversaries.",
                        "uuids": [
                            "e4009.3",
                            "e4009.2",
                            "e4009.0",
                            "e4009.1"
                        ]
                    },
                    {
                        "text": "LLMs are more susceptible than humans to beauty/formatting and authority/fake-reference biases, as shown by higher ASR (Attack Success Rate) in these perturbation experiments.",
                        "uuids": [
                            "e3863.4",
                            "e3863.3",
                            "e3863.0",
                            "e3998.2",
                            "e3998.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new LLM judge is evaluated on a dataset where superficial features (e.g., length, formatting, presence of references) are decoupled from semantic quality, the LLM will systematically prefer the outputs with enhanced surface features, even when humans do not.",
        "If the prompt for an LLM judge is changed to emphasize surface features (e.g., 'rate the most detailed answer'), the LLM's agreement with human judgments will decrease, especially on adversarial or high-quality cases.",
        "If a new adversarial attack is designed to exploit a previously untested surface feature (e.g., use of rare vocabulary), LLM judges will be more susceptible than humans to being misled by this feature.",
        "If a fine-tuned judge is evaluated on an out-of-domain or adversarial dataset, it will show a sharp drop in agreement with human judgments, especially when surface features are manipulated."
    ],
    "new_predictions_unknown": [
        "If a future LLM is trained with explicit adversarial robustness objectives and tested on unseen adversarial surface features, it may or may not overcome the surface feature overweighting; if it does, this would challenge the universality of the theory.",
        "If a hybrid LLM judge is constructed that dynamically calibrates its scoring using real-time human feedback, it is unknown whether calibration drift and surface feature overweighting will persist or be eliminated.",
        "If LLM judges are used to evaluate outputs in a non-textual modality (e.g., multimodal with images), it is unknown whether similar surface feature overweighting will occur or if new biases will emerge.",
        "If a panel of diverse LLM judges is used and their outputs are aggregated, it is unknown whether surface feature overweighting and calibration drift will be mitigated or persist in the aggregate."
    ],
    "negative_experiments": [
        "If an LLM judge, when presented with outputs where surface features are controlled (e.g., all outputs are of equal length and formatting), still shows systematic misalignment with human judgments, this would challenge the surface feature overweighting law.",
        "If calibration drift is not observed across a wide range of tasks, prompts, and model sizes (i.e., LLM judge-human agreement remains stable), this would call the calibration drift law into question.",
        "If LLM judges are robust to new adversarial attacks that exploit surface features without explicit adversarial training, this would challenge the adversarial vulnerability law.",
        "If fine-tuned judges generalize perfectly to out-of-domain and adversarial cases, this would challenge the theory's claim about overfitting and surface feature overweighting."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLM judges outperform humans in factual error detection (e.g., top LLMs have lower ASR than humans on factual-error perturbations).",
            "uuids": [
                "e3863.1",
                "e3863.0"
            ]
        },
        {
            "text": "Instances where LLM judges are more consistent than human annotators (e.g., higher intercoder agreement in some annotation tasks).",
            "uuids": [
                "e3992.0",
                "e3991.0",
                "e3852.2"
            ]
        },
        {
            "text": "Some LLMs (e.g., GPT-4o, Claude-3) are less susceptible to certain biases (e.g., beauty/formatting) than humans or other LLMs.",
            "uuids": [
                "e3863.4",
                "e3863.0"
            ]
        },
        {
            "text": "On some tasks (e.g., instruction following, mathematical reasoning traces), LLM judges can match or exceed human agreement.",
            "uuids": [
                "e3855.1",
                "e4013.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLM judges (e.g., GPT-4o, Claude-3) are less susceptible to certain biases (e.g., beauty/formatting) than humans or other LLMs.",
            "uuids": [
                "e3863.4",
                "e3863.0"
            ]
        },
        {
            "text": "Top LLMs can outperform humans in factual error detection (lower ASR on factual-error perturbations).",
            "uuids": [
                "e3863.1"
            ]
        },
        {
            "text": "Panel-based or ensemble LLM judging (e.g., PoLL, Peer-examination) can reduce individual model biases and improve alignment with humans.",
            "uuids": [
                "e4015.0",
                "e3847.2"
            ]
        }
    ],
    "special_cases": [
        "On tasks where surface features are not predictive of quality (e.g., mathematical reasoning, fact-checking with explicit references), LLM judges may align more closely with humans.",
        "For high-capacity, adversarially trained LLMs, surface feature overweighting may be reduced or eliminated.",
        "Panel-based or ensemble LLM judging can mitigate some individual model biases.",
        "When LLM judges are explicitly prompted or fine-tuned to ignore surface features, overweighting may be reduced."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wu & Aji (2023) Style Over Substance: Evaluation Biases for Large Language Models [Documents length, position, and surface feature biases in LLM evaluation]",
            "Zeng et al. (2023) Evaluating Large Language Models at Evaluating Instruction Following [Adversarial bias and prompt sensitivity in LLM evaluators]",
            "Raina et al. (2024) Is LLM-as-a-Judge Robust? [Adversarial vulnerability and surface feature attacks]",
            "Stureborg et al. (2024) Large Language Models are Inconsistent and Biased Evaluators [Calibration drift and prompt sensitivity]",
            "Benchmarking Cognitive Biases in Large Language Models as Evaluators (2023) [LLM cognitive bias amplification]",
            "JudgeLM: Fine-tuned Large Language Models are Scalable Judges (2023) [Fine-tuned judge overfitting and loss of generalization]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Format-Context Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1904</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1904</p>
                <p><strong>Name:</strong> Format-Context Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the effectiveness of a problem presentation format for LLMs depends on the alignment between the format and the model's pre-trained context distribution. Formats that closely match the statistical and structural patterns seen during pretraining or instruction tuning enable the LLM to leverage its learned representations more effectively, resulting in higher accuracy and more reliable outputs. Conversely, formats that are misaligned with pretraining distributions lead to degraded performance due to context mismatch.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contextual Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; matches &#8594; llm_pretraining_context_distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is &#8594; optimized</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best on tasks that resemble their pretraining or instruction-tuning data formats. </li>
    <li>Instruction-tuned LLMs show improved performance on natural language instructions compared to code-like or unfamiliar formats. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law synthesizes empirical findings into a formal alignment principle.</p>            <p><strong>What Already Exists:</strong> Instruction tuning and pretraining data distribution effects are known to impact LLM performance.</p>            <p><strong>What is Novel:</strong> The explicit formalization of format-context alignment as a predictive law is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [instruction tuning and format alignment]</li>
    <li>Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [format alignment and zero-shot performance]</li>
</ul>
            <h3>Statement 1: Context Mismatch Degradation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_misaligned_with &#8594; llm_pretraining_context_distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is &#8594; degraded</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform poorly on tasks with formats not seen during pretraining or instruction tuning. </li>
    <li>Code-formatted prompts confuse LLMs trained primarily on natural language, and vice versa. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is a formalization of scattered empirical findings.</p>            <p><strong>What Already Exists:</strong> Empirical studies show LLMs are sensitive to prompt format and context.</p>            <p><strong>What is Novel:</strong> The law formalizes the degradation effect as a function of context misalignment.</p>
            <p><strong>References:</strong> <ul>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [instruction tuning and format alignment]</li>
    <li>Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [format alignment and zero-shot performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will perform better on tasks formatted in ways that closely resemble their instruction-tuning data.</li>
                <li>Introducing novel formats not seen during pretraining will reduce LLM accuracy and increase error rates.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are exposed to a wide variety of formats during pretraining, they may develop format-invariant reasoning capabilities.</li>
                <li>There may exist universal formats that optimize performance across diverse LLM architectures and pretraining regimes.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on formats misaligned with their pretraining context, the theory is falsified.</li>
                <li>If context alignment does not predict performance, the theory's mechanism is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs generalize to novel formats despite lack of pretraining exposure. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes empirical findings into a formal alignment principle.</p>
            <p><strong>References:</strong> <ul>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [instruction tuning and format alignment]</li>
    <li>Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [format alignment and zero-shot performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Format-Context Alignment Theory",
    "theory_description": "This theory proposes that the effectiveness of a problem presentation format for LLMs depends on the alignment between the format and the model's pre-trained context distribution. Formats that closely match the statistical and structural patterns seen during pretraining or instruction tuning enable the LLM to leverage its learned representations more effectively, resulting in higher accuracy and more reliable outputs. Conversely, formats that are misaligned with pretraining distributions lead to degraded performance due to context mismatch.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contextual Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "matches",
                        "object": "llm_pretraining_context_distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is",
                        "object": "optimized"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best on tasks that resemble their pretraining or instruction-tuning data formats.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction-tuned LLMs show improved performance on natural language instructions compared to code-like or unfamiliar formats.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Instruction tuning and pretraining data distribution effects are known to impact LLM performance.",
                    "what_is_novel": "The explicit formalization of format-context alignment as a predictive law is novel.",
                    "classification_explanation": "The law synthesizes empirical findings into a formal alignment principle.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Ouyang et al. (2022) Training language models to follow instructions with human feedback [instruction tuning and format alignment]",
                        "Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [format alignment and zero-shot performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Context Mismatch Degradation Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_misaligned_with",
                        "object": "llm_pretraining_context_distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is",
                        "object": "degraded"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform poorly on tasks with formats not seen during pretraining or instruction tuning.",
                        "uuids": []
                    },
                    {
                        "text": "Code-formatted prompts confuse LLMs trained primarily on natural language, and vice versa.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical studies show LLMs are sensitive to prompt format and context.",
                    "what_is_novel": "The law formalizes the degradation effect as a function of context misalignment.",
                    "classification_explanation": "The law is a formalization of scattered empirical findings.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Ouyang et al. (2022) Training language models to follow instructions with human feedback [instruction tuning and format alignment]",
                        "Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [format alignment and zero-shot performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will perform better on tasks formatted in ways that closely resemble their instruction-tuning data.",
        "Introducing novel formats not seen during pretraining will reduce LLM accuracy and increase error rates."
    ],
    "new_predictions_unknown": [
        "If LLMs are exposed to a wide variety of formats during pretraining, they may develop format-invariant reasoning capabilities.",
        "There may exist universal formats that optimize performance across diverse LLM architectures and pretraining regimes."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on formats misaligned with their pretraining context, the theory is falsified.",
        "If context alignment does not predict performance, the theory's mechanism is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs generalize to novel formats despite lack of pretraining exposure.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show surprising robustness to format changes after extensive instruction tuning.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Highly flexible or meta-trained LLMs may be less sensitive to format-context alignment.",
        "Tasks with minimal context requirements may not exhibit strong alignment effects."
    ],
    "existing_theory": {
        "what_already_exists": "Instruction tuning and pretraining context effects are well-documented.",
        "what_is_novel": "The explicit formalization of format-context alignment as a predictive theory is new.",
        "classification_explanation": "The theory synthesizes empirical findings into a formal alignment principle.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Ouyang et al. (2022) Training language models to follow instructions with human feedback [instruction tuning and format alignment]",
            "Wei et al. (2021) Finetuned Language Models Are Zero-Shot Learners [format alignment and zero-shot performance]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-653",
    "original_theory_name": "Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
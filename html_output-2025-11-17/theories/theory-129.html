<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structured Representation Scaling Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-129</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-129</p>
                <p><strong>Name:</strong> Structured Representation Scaling Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of the fundamental trade-off between environment complexity and environment variation in embodied learning systems, based on the following results.</p>
                <p><strong>Description:</strong> The ability of embodied learning systems to handle high complexity-variation products scales with the degree and appropriateness of structural inductive bias in their representations. The theory posits a hierarchy of representation types with different scaling properties: (1) Unstructured representations (flat MLPs on raw pixels) exhibit poor scaling with both complexity and variation, often failing on multi-object or long-horizon tasks; (2) Modular representations (object-centric, graph networks, hierarchical) scale substantially better by exploiting compositional structure and enabling systematic generalization; (3) Causal representations that disentangle controllable factors from nuisance variation provide the strongest generalization by capturing invariant mechanisms; (4) The benefit of structured representations increases super-linearly with the complexity-variation product, but only when the structure matches the task structure; (5) Structured representations enable better transfer across related tasks by capturing reusable abstractions. However, there are important boundary conditions: excessive structure can hurt when mismatched to the task, very simple tasks may not benefit from structure, and massive scale can sometimes compensate for lack of structure.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Structured representations that exploit task structure scale better with complexity and variation than unstructured representations, with the benefit increasing as the complexity-variation product increases</li>
                <li>The scaling benefit is super-linear in the sense that structured representations show exponentially better sample efficiency on complex tasks (e.g., 75x fewer samples for 6-block stacking with relational RL)</li>
                <li>Object-centric and graph-based representations are particularly effective for tasks with compositional structure (multiple objects, relational reasoning), enabling systematic generalization to novel object counts and configurations</li>
                <li>Causal representations that disentangle controllable factors from nuisance variation provide the strongest generalization under high variation by capturing invariant mechanisms</li>
                <li>Hierarchical representations (skills, procedures, options) are necessary for long-horizon tasks with sequential dependencies, enabling temporal abstraction and reuse</li>
                <li>The optimal representation type depends on task structure: hierarchical tasks benefit from hierarchical representations, compositional tasks benefit from compositional representations, causal tasks benefit from causal representations</li>
                <li>Structured representations transfer better across related tasks than unstructured representations because they capture reusable abstractions rather than task-specific patterns</li>
                <li>Modular separation of concerns (e.g., semantic vs spatial, perception vs control) enables better scaling than end-to-end learning when the task has natural decompositions</li>
                <li>The benefit of structure is most pronounced when: (a) the task has inherent structure matching the representation, (b) complexity and variation are both high, (c) sample efficiency matters</li>
                <li>Excessive or mismatched structure can hurt performance (e.g., too much memory/history, wrong compositional assumptions), so structure must be appropriate to the task</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Graph network agents in construction tasks substantially outperformed CNN-based agents, especially on complex multi-object tasks with curriculum learning <a href="../results/extraction-result-1095.html#e1095.3" class="evidence-link">[e1095.3]</a> </li>
    <li>CNN-RS0 pixel-based agents performed much worse than GN-based agents; introducing object segmentation (CNN-GN-DQN) recovered much of the performance, demonstrating that object-centric structure is key <a href="../results/extraction-result-1095.html#e1095.3" class="evidence-link">[e1095.3]</a> </li>
    <li>CausalCF with causal representations achieved >0.9 fractional success and better generalization across 12 protocols vs baselines without causal structure, showing causal structure enables robust generalization <a href="../results/extraction-result-1064.html#e1064.0" class="evidence-link">[e1064.0]</a> </li>
    <li>Modular IMGEP with disentangled goal spaces strongly outperformed flat IMGEP (RGE-VAE) and random parameter exploration in high-dimensional continuous action settings with distractors <a href="../results/extraction-result-1034.html#e1034.3" class="evidence-link">[e1034.3]</a> <a href="../results/extraction-result-1034.html#e1034.4" class="evidence-link">[e1034.4]</a> </li>
    <li>Depth-based RL agents generalized better across visual variation than RGB/RGBD agents, achieving SPL=0.79 on Gibson vs lower RGB performance, because depth provides structured geometric information <a href="../results/extraction-result-1083.html#e1083.0" class="evidence-link">[e1083.0]</a> </li>
    <li>SGIM-PB with procedure-based hierarchical representation learned hierarchical tasks (sound production requiring multi-object manipulation) that SGIM-ACTS (action-only) could not solve <a href="../results/extraction-result-1027.html#e1027.0" class="evidence-link">[e1027.0]</a> </li>
    <li>BiCNet with bidirectional communication structure scaled better to larger agent counts (n=20) with lower error than CommNet and independent controllers in sum-guessing task <a href="../results/extraction-result-1094.html#e1094.1" class="evidence-link">[e1094.1]</a> </li>
    <li>Causal representations transferred from Pushing to Picking task improved convergence (>0.9 vs ~0.8 fractional success) and generalization across 8/12 protocols, demonstrating transferability of structured representations <a href="../results/extraction-result-1064.html#e1064.3" class="evidence-link">[e1064.3]</a> </li>
    <li>Object-centric relational RL enabled multi-object stacking with 75% success at 30M steps vs demonstration-heavy baselines requiring billions of steps, showing massive sample efficiency gains from structure <a href="../results/extraction-result-1076.html#e1076.0" class="evidence-link">[e1076.0]</a> <a href="../results/extraction-result-1076.html#e1076.2" class="evidence-link">[e1076.2]</a> </li>
    <li>Transformer with short history structure (H=4) achieved 68% success on static navigation vs MLP baseline, but longer history (H=8) degraded to 46%, showing structure must match task requirements <a href="../results/extraction-result-1085.html#e1085.1" class="evidence-link">[e1085.1]</a> </li>
    <li>CLIPORT separating semantic understanding (CLIP) and spatial precision (Transporter) achieved better performance on language-conditioned manipulation than end-to-end approaches <a href="../results/extraction-result-1058.html#e1058.3" class="evidence-link">[e1058.3]</a> </li>
    <li>IMRL with hierarchical skill discovery (options) achieved faster learning across multiple tasks in 2D Multi-Valley environment compared to monolithic learner, demonstrating benefits of hierarchical structure <a href="../results/extraction-result-1013.html#e1013.0" class="evidence-link">[e1013.0]</a> </li>
    <li>SAGG-RIAC without procedural composition suffered on hierarchical/multi-dimensional outcome spaces due to curse of dimensionality, while procedure-based methods (IM-PB) performed better <a href="../results/extraction-result-1073.html#e1073.2" class="evidence-link">[e1073.2]</a> </li>
    <li>Random parameter exploration (RPE) failed dramatically in high-dimensional continuous action settings with distractors compared to structured IMGEP variants <a href="../results/extraction-result-1034.html#e1034.4" class="evidence-link">[e1034.4]</a> </li>
    <li>BiCNet learned interpretable additive message passing (hidden states reflect partial sums) in sum-guessing task, showing structured communication enables compositional reasoning <a href="../results/extraction-result-1094.html#e1094.1" class="evidence-link">[e1094.1]</a> </li>
    <li>Relational RL with graph networks achieved 75% success on 6-block stacking at 30M steps, while prior demonstration-based methods required 2.3B steps for 32% success on 6 blocks <a href="../results/extraction-result-1076.html#e1076.0" class="evidence-link">[e1076.0]</a> </li>
    <li>TEXPLORE-VANIR with ensemble-based structured model learning achieved better model accuracy and task performance than random exploration and competence-progress methods in Light World <a href="../results/extraction-result-1079.html#e1079.0" class="evidence-link">[e1079.0]</a> </li>
    <li>Procedure-based SGIM-PB discovered hierarchical task decompositions autonomously and solved tasks requiring multi-step composition that action-only methods could not <a href="../results/extraction-result-1027.html#e1027.0" class="evidence-link">[e1027.0]</a> </li>
    <li>Cross-learner transfer of procedures (not raw actions) succeeded across different embodiments (right-arm to left-arm), showing procedures capture transferable structure <a href="../results/extraction-result-1027.html#e1027.0" class="evidence-link">[e1027.0]</a> </li>
    <li>MF-RL baselines failed on Stacking2 (score <0.5) and failed completely under extreme domain randomization, while structured approaches with targeted variation succeeded <a href="../results/extraction-result-1074.html#e1074.0" class="evidence-link">[e1074.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For multi-object manipulation tasks, graph networks will outperform flat MLPs by a margin that increases exponentially with the number of objects (e.g., 10x better at 3 objects, 100x better at 6 objects)</li>
                <li>Causal representations learned on one robotic task will transfer to related tasks with similar causal structure but different appearance with minimal fine-tuning</li>
                <li>Object-centric representations will enable zero-shot generalization to novel object counts in compositional tasks (e.g., trained on 3 objects, tested on 5 objects)</li>
                <li>Hierarchical representations will enable learning of long-horizon tasks (>100 steps) that are intractable with flat representations given the same training budget</li>
                <li>Structured representations will show better sim-to-real transfer than unstructured representations because they capture invariant structure rather than spurious correlations</li>
                <li>Combining multiple types of structure (e.g., object-centric + causal + hierarchical) will provide multiplicative benefits on tasks that have multiple types of structure</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a universal structured representation that works across all embodied learning tasks, or if representations must always be task-specific</li>
                <li>Whether structured representations can be learned purely from data without architectural inductive biases, or if architectural constraints are necessary for sample efficiency</li>
                <li>Whether the scaling benefits of structured representations have fundamental limits, or if they continue indefinitely with task complexity</li>
                <li>Whether hybrid representations that combine multiple structural biases (e.g., object-centric + causal + hierarchical) provide multiplicative or merely additive benefits</li>
                <li>At what point does the cost of learning/maintaining structured representations outweigh the benefits, and how does this depend on task properties</li>
                <li>Whether structured representations learned in simulation transfer better to the real world than unstructured representations, and by how much</li>
                <li>Whether there are fundamental trade-offs between different types of structure (e.g., object-centric vs causal vs hierarchical) that prevent combining them optimally</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where unstructured representations consistently outperform structured representations despite high complexity and variation would challenge the core theory</li>
                <li>Demonstrating that structured representations do not transfer better than unstructured representations across related tasks would contradict the invariance claim</li>
                <li>Showing that the benefit of structured representations does not increase with complexity-variation product (e.g., constant benefit regardless of task difficulty) would falsify the super-linear scaling claim</li>
                <li>Finding that task structure does not predict optimal representation type (e.g., hierarchical representations work equally well on non-hierarchical tasks) would challenge the structure-matching principle</li>
                <li>Demonstrating that massive scale can always compensate for lack of structure (e.g., flat MLPs with 100x more data/compute match structured representations) would weaken the practical importance of the theory</li>
                <li>Finding that structured representations learned from data (without architectural biases) perform as well as hand-designed structured architectures would challenge the necessity of inductive biases</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically discover the appropriate structural inductive bias for a new task without manual design or extensive trial-and-error </li>
    <li>The exact mechanisms by which structured representations enable better generalization (is it purely sample efficiency, or are there other factors?) </li>
    <li>How structured representations interact with different learning algorithms (e.g., on-policy vs off-policy, model-based vs model-free) and whether some algorithms benefit more from structure </li>
    <li>The computational and memory costs of structured representations and how these trade off against their benefits </li>
    <li>Whether structured representations can be learned end-to-end from raw sensory data or require explicit architectural constraints </li>
    <li>How to combine multiple types of structure (object-centric, causal, hierarchical) in a principled way </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Foundational work on structured representations and relational reasoning in deep learning]</li>
    <li>Scholkopf et al. (2021) Toward Causal Representation Learning [Framework for learning causal representations that capture invariant mechanisms]</li>
    <li>Greff et al. (2020) Binding via Reconstruction Clustering [Object-centric representation learning without supervision]</li>
    <li>Lake et al. (2017) Building machines that learn and think like people [Compositional and causal structure in learning and reasoning]</li>
    <li>Sutton et al. (1999) Between MDPs and semi-MDPs: A framework for temporal abstraction [Hierarchical reinforcement learning and temporal abstraction]</li>
    <li>Bengio et al. (2013) Representation learning: A review and new perspectives [General framework for representation learning and its benefits]</li>
    <li>Marcus (2018) Deep learning: A critical appraisal [Critique emphasizing the importance of structured representations and inductive biases]</li>
    <li>Goyal & Bengio (2022) Inductive biases for deep learning of higher-level cognition [Recent work on the role of inductive biases in learning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structured Representation Scaling Theory",
    "theory_description": "The ability of embodied learning systems to handle high complexity-variation products scales with the degree and appropriateness of structural inductive bias in their representations. The theory posits a hierarchy of representation types with different scaling properties: (1) Unstructured representations (flat MLPs on raw pixels) exhibit poor scaling with both complexity and variation, often failing on multi-object or long-horizon tasks; (2) Modular representations (object-centric, graph networks, hierarchical) scale substantially better by exploiting compositional structure and enabling systematic generalization; (3) Causal representations that disentangle controllable factors from nuisance variation provide the strongest generalization by capturing invariant mechanisms; (4) The benefit of structured representations increases super-linearly with the complexity-variation product, but only when the structure matches the task structure; (5) Structured representations enable better transfer across related tasks by capturing reusable abstractions. However, there are important boundary conditions: excessive structure can hurt when mismatched to the task, very simple tasks may not benefit from structure, and massive scale can sometimes compensate for lack of structure.",
    "supporting_evidence": [
        {
            "text": "Graph network agents in construction tasks substantially outperformed CNN-based agents, especially on complex multi-object tasks with curriculum learning",
            "uuids": [
                "e1095.3"
            ]
        },
        {
            "text": "CNN-RS0 pixel-based agents performed much worse than GN-based agents; introducing object segmentation (CNN-GN-DQN) recovered much of the performance, demonstrating that object-centric structure is key",
            "uuids": [
                "e1095.3"
            ]
        },
        {
            "text": "CausalCF with causal representations achieved &gt;0.9 fractional success and better generalization across 12 protocols vs baselines without causal structure, showing causal structure enables robust generalization",
            "uuids": [
                "e1064.0"
            ]
        },
        {
            "text": "Modular IMGEP with disentangled goal spaces strongly outperformed flat IMGEP (RGE-VAE) and random parameter exploration in high-dimensional continuous action settings with distractors",
            "uuids": [
                "e1034.3",
                "e1034.4"
            ]
        },
        {
            "text": "Depth-based RL agents generalized better across visual variation than RGB/RGBD agents, achieving SPL=0.79 on Gibson vs lower RGB performance, because depth provides structured geometric information",
            "uuids": [
                "e1083.0"
            ]
        },
        {
            "text": "SGIM-PB with procedure-based hierarchical representation learned hierarchical tasks (sound production requiring multi-object manipulation) that SGIM-ACTS (action-only) could not solve",
            "uuids": [
                "e1027.0"
            ]
        },
        {
            "text": "BiCNet with bidirectional communication structure scaled better to larger agent counts (n=20) with lower error than CommNet and independent controllers in sum-guessing task",
            "uuids": [
                "e1094.1"
            ]
        },
        {
            "text": "Causal representations transferred from Pushing to Picking task improved convergence (&gt;0.9 vs ~0.8 fractional success) and generalization across 8/12 protocols, demonstrating transferability of structured representations",
            "uuids": [
                "e1064.3"
            ]
        },
        {
            "text": "Object-centric relational RL enabled multi-object stacking with 75% success at 30M steps vs demonstration-heavy baselines requiring billions of steps, showing massive sample efficiency gains from structure",
            "uuids": [
                "e1076.0",
                "e1076.2"
            ]
        },
        {
            "text": "Transformer with short history structure (H=4) achieved 68% success on static navigation vs MLP baseline, but longer history (H=8) degraded to 46%, showing structure must match task requirements",
            "uuids": [
                "e1085.1"
            ]
        },
        {
            "text": "CLIPORT separating semantic understanding (CLIP) and spatial precision (Transporter) achieved better performance on language-conditioned manipulation than end-to-end approaches",
            "uuids": [
                "e1058.3"
            ]
        },
        {
            "text": "IMRL with hierarchical skill discovery (options) achieved faster learning across multiple tasks in 2D Multi-Valley environment compared to monolithic learner, demonstrating benefits of hierarchical structure",
            "uuids": [
                "e1013.0"
            ]
        },
        {
            "text": "SAGG-RIAC without procedural composition suffered on hierarchical/multi-dimensional outcome spaces due to curse of dimensionality, while procedure-based methods (IM-PB) performed better",
            "uuids": [
                "e1073.2"
            ]
        },
        {
            "text": "Random parameter exploration (RPE) failed dramatically in high-dimensional continuous action settings with distractors compared to structured IMGEP variants",
            "uuids": [
                "e1034.4"
            ]
        },
        {
            "text": "BiCNet learned interpretable additive message passing (hidden states reflect partial sums) in sum-guessing task, showing structured communication enables compositional reasoning",
            "uuids": [
                "e1094.1"
            ]
        },
        {
            "text": "Relational RL with graph networks achieved 75% success on 6-block stacking at 30M steps, while prior demonstration-based methods required 2.3B steps for 32% success on 6 blocks",
            "uuids": [
                "e1076.0"
            ]
        },
        {
            "text": "TEXPLORE-VANIR with ensemble-based structured model learning achieved better model accuracy and task performance than random exploration and competence-progress methods in Light World",
            "uuids": [
                "e1079.0"
            ]
        },
        {
            "text": "Procedure-based SGIM-PB discovered hierarchical task decompositions autonomously and solved tasks requiring multi-step composition that action-only methods could not",
            "uuids": [
                "e1027.0"
            ]
        },
        {
            "text": "Cross-learner transfer of procedures (not raw actions) succeeded across different embodiments (right-arm to left-arm), showing procedures capture transferable structure",
            "uuids": [
                "e1027.0"
            ]
        },
        {
            "text": "MF-RL baselines failed on Stacking2 (score &lt;0.5) and failed completely under extreme domain randomization, while structured approaches with targeted variation succeeded",
            "uuids": [
                "e1074.0"
            ]
        }
    ],
    "theory_statements": [
        "Structured representations that exploit task structure scale better with complexity and variation than unstructured representations, with the benefit increasing as the complexity-variation product increases",
        "The scaling benefit is super-linear in the sense that structured representations show exponentially better sample efficiency on complex tasks (e.g., 75x fewer samples for 6-block stacking with relational RL)",
        "Object-centric and graph-based representations are particularly effective for tasks with compositional structure (multiple objects, relational reasoning), enabling systematic generalization to novel object counts and configurations",
        "Causal representations that disentangle controllable factors from nuisance variation provide the strongest generalization under high variation by capturing invariant mechanisms",
        "Hierarchical representations (skills, procedures, options) are necessary for long-horizon tasks with sequential dependencies, enabling temporal abstraction and reuse",
        "The optimal representation type depends on task structure: hierarchical tasks benefit from hierarchical representations, compositional tasks benefit from compositional representations, causal tasks benefit from causal representations",
        "Structured representations transfer better across related tasks than unstructured representations because they capture reusable abstractions rather than task-specific patterns",
        "Modular separation of concerns (e.g., semantic vs spatial, perception vs control) enables better scaling than end-to-end learning when the task has natural decompositions",
        "The benefit of structure is most pronounced when: (a) the task has inherent structure matching the representation, (b) complexity and variation are both high, (c) sample efficiency matters",
        "Excessive or mismatched structure can hurt performance (e.g., too much memory/history, wrong compositional assumptions), so structure must be appropriate to the task"
    ],
    "new_predictions_likely": [
        "For multi-object manipulation tasks, graph networks will outperform flat MLPs by a margin that increases exponentially with the number of objects (e.g., 10x better at 3 objects, 100x better at 6 objects)",
        "Causal representations learned on one robotic task will transfer to related tasks with similar causal structure but different appearance with minimal fine-tuning",
        "Object-centric representations will enable zero-shot generalization to novel object counts in compositional tasks (e.g., trained on 3 objects, tested on 5 objects)",
        "Hierarchical representations will enable learning of long-horizon tasks (&gt;100 steps) that are intractable with flat representations given the same training budget",
        "Structured representations will show better sim-to-real transfer than unstructured representations because they capture invariant structure rather than spurious correlations",
        "Combining multiple types of structure (e.g., object-centric + causal + hierarchical) will provide multiplicative benefits on tasks that have multiple types of structure"
    ],
    "new_predictions_unknown": [
        "Whether there exists a universal structured representation that works across all embodied learning tasks, or if representations must always be task-specific",
        "Whether structured representations can be learned purely from data without architectural inductive biases, or if architectural constraints are necessary for sample efficiency",
        "Whether the scaling benefits of structured representations have fundamental limits, or if they continue indefinitely with task complexity",
        "Whether hybrid representations that combine multiple structural biases (e.g., object-centric + causal + hierarchical) provide multiplicative or merely additive benefits",
        "At what point does the cost of learning/maintaining structured representations outweigh the benefits, and how does this depend on task properties",
        "Whether structured representations learned in simulation transfer better to the real world than unstructured representations, and by how much",
        "Whether there are fundamental trade-offs between different types of structure (e.g., object-centric vs causal vs hierarchical) that prevent combining them optimally"
    ],
    "negative_experiments": [
        "Finding tasks where unstructured representations consistently outperform structured representations despite high complexity and variation would challenge the core theory",
        "Demonstrating that structured representations do not transfer better than unstructured representations across related tasks would contradict the invariance claim",
        "Showing that the benefit of structured representations does not increase with complexity-variation product (e.g., constant benefit regardless of task difficulty) would falsify the super-linear scaling claim",
        "Finding that task structure does not predict optimal representation type (e.g., hierarchical representations work equally well on non-hierarchical tasks) would challenge the structure-matching principle",
        "Demonstrating that massive scale can always compensate for lack of structure (e.g., flat MLPs with 100x more data/compute match structured representations) would weaken the practical importance of the theory",
        "Finding that structured representations learned from data (without architectural biases) perform as well as hand-designed structured architectures would challenge the necessity of inductive biases"
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically discover the appropriate structural inductive bias for a new task without manual design or extensive trial-and-error",
            "uuids": []
        },
        {
            "text": "The exact mechanisms by which structured representations enable better generalization (is it purely sample efficiency, or are there other factors?)",
            "uuids": []
        },
        {
            "text": "How structured representations interact with different learning algorithms (e.g., on-policy vs off-policy, model-based vs model-free) and whether some algorithms benefit more from structure",
            "uuids": []
        },
        {
            "text": "The computational and memory costs of structured representations and how these trade off against their benefits",
            "uuids": []
        },
        {
            "text": "Whether structured representations can be learned end-to-end from raw sensory data or require explicit architectural constraints",
            "uuids": []
        },
        {
            "text": "How to combine multiple types of structure (object-centric, causal, hierarchical) in a principled way",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "DD-PPO achieved strong PointGoal navigation performance (SPL=0.948) with relatively simple architectures by using massive scale (2.5B frames), suggesting scale can compensate for lack of structure in some domains",
            "uuids": [
                "e1082.0"
            ]
        },
        {
            "text": "Transformer with longer history (H=8) performed worse than shorter history (H=4) in navigation (46% vs 68% success), showing that more structure is not always better",
            "uuids": [
                "e1085.1"
            ]
        },
        {
            "text": "Some simple navigation tasks may not benefit from structured representations and could be solved efficiently with flat MLPs given sufficient training data",
            "uuids": []
        },
        {
            "text": "Classic SLAM (hand-engineered structure) was outperformed by learned Depth agents at large scale (75M+ steps), suggesting learned representations can eventually surpass hand-crafted structure",
            "uuids": [
                "e1083.4"
            ]
        },
        {
            "text": "In some cases, end-to-end learning without explicit structure can discover implicit structure and perform well, though typically requiring more data",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with no compositional, hierarchical, or causal structure may not benefit from structured representations and could be learned efficiently with unstructured approaches",
        "Very simple tasks with low complexity and low variation may be learned efficiently with unstructured representations, and the overhead of structure may not be worthwhile",
        "The benefit of structure depends critically on the amount of training data: with infinite data, unstructured representations might eventually match structured ones, but structured representations are far more sample-efficient",
        "Mismatched structure can hurt performance: using hierarchical representations on flat tasks, or object-centric representations when objects are not the right unit of abstraction",
        "The optimal amount of structure depends on task properties: too little structure fails to exploit task regularities, too much structure imposes inappropriate constraints",
        "Structured representations may be more important for sim-to-real transfer than for simulation-only learning, because structure captures invariants that transfer better",
        "Some types of structure (e.g., causal) may be more important for generalization under distribution shift, while other types (e.g., hierarchical) may be more important for long-horizon credit assignment",
        "The benefit of structure may be most pronounced in the low-to-medium data regime; with massive data, unstructured approaches may eventually catch up",
        "Learned structure (discovered from data) may be more flexible than hard-coded structure, but requires more data to discover",
        "The interaction between structure and curriculum learning is important: structured representations may enable more effective curriculum learning by providing better abstractions for measuring progress"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Foundational work on structured representations and relational reasoning in deep learning]",
            "Scholkopf et al. (2021) Toward Causal Representation Learning [Framework for learning causal representations that capture invariant mechanisms]",
            "Greff et al. (2020) Binding via Reconstruction Clustering [Object-centric representation learning without supervision]",
            "Lake et al. (2017) Building machines that learn and think like people [Compositional and causal structure in learning and reasoning]",
            "Sutton et al. (1999) Between MDPs and semi-MDPs: A framework for temporal abstraction [Hierarchical reinforcement learning and temporal abstraction]",
            "Bengio et al. (2013) Representation learning: A review and new perspectives [General framework for representation learning and its benefits]",
            "Marcus (2018) Deep learning: A critical appraisal [Critique emphasizing the importance of structured representations and inductive biases]",
            "Goyal & Bengio (2022) Inductive biases for deep learning of higher-level cognition [Recent work on the role of inductive biases in learning]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 2,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
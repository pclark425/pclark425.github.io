<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Instruction Template Primacy Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1925</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1925</p>
                <p><strong>Name:</strong> Instruction Template Primacy Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the format and structure of the instruction template presented to an instruction-tuned LLM exerts a primary influence on the model's internal representation and output behavior, often overriding the semantic content of the task itself. The model's learned associations with specific templates during instruction tuning create strong priors that bias its response patterns, leading to systematic performance differences based on template design.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Template Primacy Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; input_prompt &#8594; is_structured_by &#8594; instruction_template<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_instruction_tuned &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_output &#8594; is_determined_primarily_by &#8594; instruction_template<span style="color: #888888;">, and</span></div>
        <div>&#8226; task_semantics &#8594; has_secondary_influence_on &#8594; LLM_output</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that LLMs produce different outputs for the same task when the instruction template is varied, even when the underlying task semantics are unchanged. </li>
    <li>Instruction-tuned LLMs are sensitive to prompt phrasing and structure, with performance varying across templates. </li>
    <li>Template-specific artifacts (e.g., repeated phrases, formatting) appear in outputs, indicating strong template influence. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt effects are known, the primacy and dominance of template structure over semantics is a novel, general claim.</p>            <p><strong>What Already Exists:</strong> Prompt format is known to affect LLM outputs, and prompt engineering is a recognized practice.</p>            <p><strong>What is Novel:</strong> The explicit claim that template structure exerts a primary, overriding influence on output, often stronger than task semantics.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [prompt format effects]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [prompting strategies]</li>
    <li>Reynolds & McDonell (2021) Prompt Programming for Large Language Models [prompt engineering]</li>
</ul>
            <h3>Statement 1: Template Familiarity Amplification Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; instruction_template &#8594; is_highly_familiar_to &#8594; LLM<span style="color: #888888;">, and</span></div>
        <div>&#8226; input_prompt &#8594; matches &#8594; instruction_template</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_output &#8594; shows_amplified_template_effects &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_performance &#8594; is_maximized_or_minimized &#8594; depending_on_template_quality</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best on tasks presented in templates similar to those seen during instruction tuning. </li>
    <li>Performance drops when unfamiliar or adversarial templates are used, even for simple tasks. </li>
    <li>Template familiarity correlates with output stereotypy and confidence. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This is a generalization and formalization of template familiarity effects, not previously stated as a law.</p>            <p><strong>What Already Exists:</strong> Prompt familiarity and exposure effects are discussed in prompt engineering literature.</p>            <p><strong>What is Novel:</strong> The law that template familiarity amplifies both positive and negative template effects, modulating performance extremes.</p>
            <p><strong>References:</strong> <ul>
    <li>Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [prompt tuning and familiarity]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [prompt format and familiarity effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new task is presented in a template highly similar to those used in instruction tuning, LLM performance will be higher than if presented in a novel template.</li>
                <li>If the same semantic task is presented in multiple templates, output variability will be greater across templates than across task variants within a single template.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LLM is instruction-tuned on a highly diverse set of templates, the primacy effect may be attenuated or eliminated.</li>
                <li>If templates are adversarially designed to conflict with task semantics, the model may default to template-driven outputs even when incorrect.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLM output is invariant to template structure for a wide range of tasks, the theory is falsified.</li>
                <li>If template familiarity does not correlate with performance or output stereotypy, the theory is refuted.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks with highly constrained outputs (e.g., arithmetic) may not show strong template effects. </li>
    <li>Models with explicit template-agnostic training objectives may not exhibit template primacy. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This is a generalization and formalization of prompt effects, not previously stated as a theory of primacy.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [prompt format effects]</li>
    <li>Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [prompt tuning]</li>
    <li>Reynolds & McDonell (2021) Prompt Programming for Large Language Models [prompt engineering]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Instruction Template Primacy Theory",
    "theory_description": "This theory posits that the format and structure of the instruction template presented to an instruction-tuned LLM exerts a primary influence on the model's internal representation and output behavior, often overriding the semantic content of the task itself. The model's learned associations with specific templates during instruction tuning create strong priors that bias its response patterns, leading to systematic performance differences based on template design.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Template Primacy Law",
                "if": [
                    {
                        "subject": "input_prompt",
                        "relation": "is_structured_by",
                        "object": "instruction_template"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_instruction_tuned",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_output",
                        "relation": "is_determined_primarily_by",
                        "object": "instruction_template"
                    },
                    {
                        "subject": "task_semantics",
                        "relation": "has_secondary_influence_on",
                        "object": "LLM_output"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that LLMs produce different outputs for the same task when the instruction template is varied, even when the underlying task semantics are unchanged.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction-tuned LLMs are sensitive to prompt phrasing and structure, with performance varying across templates.",
                        "uuids": []
                    },
                    {
                        "text": "Template-specific artifacts (e.g., repeated phrases, formatting) appear in outputs, indicating strong template influence.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt format is known to affect LLM outputs, and prompt engineering is a recognized practice.",
                    "what_is_novel": "The explicit claim that template structure exerts a primary, overriding influence on output, often stronger than task semantics.",
                    "classification_explanation": "While prompt effects are known, the primacy and dominance of template structure over semantics is a novel, general claim.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [prompt format effects]",
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [prompting strategies]",
                        "Reynolds & McDonell (2021) Prompt Programming for Large Language Models [prompt engineering]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Template Familiarity Amplification Law",
                "if": [
                    {
                        "subject": "instruction_template",
                        "relation": "is_highly_familiar_to",
                        "object": "LLM"
                    },
                    {
                        "subject": "input_prompt",
                        "relation": "matches",
                        "object": "instruction_template"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_output",
                        "relation": "shows_amplified_template_effects",
                        "object": "True"
                    },
                    {
                        "subject": "LLM_performance",
                        "relation": "is_maximized_or_minimized",
                        "object": "depending_on_template_quality"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best on tasks presented in templates similar to those seen during instruction tuning.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops when unfamiliar or adversarial templates are used, even for simple tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Template familiarity correlates with output stereotypy and confidence.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt familiarity and exposure effects are discussed in prompt engineering literature.",
                    "what_is_novel": "The law that template familiarity amplifies both positive and negative template effects, modulating performance extremes.",
                    "classification_explanation": "This is a generalization and formalization of template familiarity effects, not previously stated as a law.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [prompt tuning and familiarity]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [prompt format and familiarity effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new task is presented in a template highly similar to those used in instruction tuning, LLM performance will be higher than if presented in a novel template.",
        "If the same semantic task is presented in multiple templates, output variability will be greater across templates than across task variants within a single template."
    ],
    "new_predictions_unknown": [
        "If an LLM is instruction-tuned on a highly diverse set of templates, the primacy effect may be attenuated or eliminated.",
        "If templates are adversarially designed to conflict with task semantics, the model may default to template-driven outputs even when incorrect."
    ],
    "negative_experiments": [
        "If LLM output is invariant to template structure for a wide range of tasks, the theory is falsified.",
        "If template familiarity does not correlate with performance or output stereotypy, the theory is refuted."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks with highly constrained outputs (e.g., arithmetic) may not show strong template effects.",
            "uuids": []
        },
        {
            "text": "Models with explicit template-agnostic training objectives may not exhibit template primacy.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs with advanced decoding strategies (e.g., high temperature, nucleus sampling) can generate diverse outputs even with familiar templates.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with deterministic outputs may not exhibit template primacy.",
        "Very large models with extensive instruction tuning may show reduced template dominance."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt format and familiarity effects are recognized, but not formalized as primary determinants.",
        "what_is_novel": "The explicit, general claim that template structure dominates over task semantics in determining LLM output.",
        "classification_explanation": "This is a generalization and formalization of prompt effects, not previously stated as a theory of primacy.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Finetuned Language Models Are Zero-Shot Learners [prompt format effects]",
            "Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [prompt tuning]",
            "Reynolds & McDonell (2021) Prompt Programming for Large Language Models [prompt engineering]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
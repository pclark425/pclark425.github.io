<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Iterative Refinement and Feedback Theory of Scientific Discovery - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-565</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-565</p>
                <p><strong>Name:</strong> Iterative Refinement and Feedback Theory of Scientific Discovery</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can be used to distill quantitative laws from large numbers of scholarly input papers, based on the following results.</p>
                <p><strong>Description:</strong> LLM-based discovery of quantitative laws from literature operates most effectively through iterative cycles of generation, evaluation, and refinement, where each cycle incorporates feedback from multiple sources (simulation, knowledge graphs, human experts, or other LLMs). The quality of discovered laws scales with the fidelity of the feedback mechanism, with physics-based simulation providing the strongest signal for physical laws, knowledge graph verification for relational patterns, and human feedback for novelty assessment. Single-pass generation without feedback loops produces outputs that are plausible but often lack physical grounding or true novelty. However, the benefits of iteration exhibit diminishing returns after 3-5 cycles, and the optimal approach depends on domain maturity, resource constraints, and the specific type of knowledge being extracted.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Feedback Fidelity Determines Discovery Quality (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; discovery_system &#8594; incorporates_feedback_mechanism &#8594; high_fidelity_evaluator<span style="color: #888888;">, and</span></div>
        <div>&#8226; feedback_mechanism &#8594; provides_signal_about &#8594; physical_validity_or_novelty</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; discovered_laws &#8594; have_quality &#8594; higher<span style="color: #888888;">, and</span></div>
        <div>&#8226; discovered_laws &#8594; have_physical_grounding &#8594; stronger</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ChemReasoner with quantum-chemical feedback (GNN + DFT validation) achieved substantially higher rewards than LLM-only baselines (GPT-4 CHEMREASONER-Planner 2.36 vs Chain-of-Thought 0.37) <a href="../results/extraction-result-4567.html#e4567.0" class="evidence-link">[e4567.0]</a> </li>
    <li>CriticAL with model-generated samples for hypothesis testing achieved 94% win rate (>1 SE) in downstream ELPD improvement vs initial models <a href="../results/extraction-result-4321.html#e4321.0" class="evidence-link">[e4321.0]</a> </li>
    <li>SGA bilevel optimization combining LLM hypothesis generation with differentiable simulation feedback enables discovery of constitutive laws <a href="../results/extraction-result-4329.html#e4329.1" class="evidence-link">[e4329.1]</a> <a href="../results/extraction-result-4573.html#e4573.0" class="evidence-link">[e4573.0]</a> </li>
    <li>CycleResearcher with iterative SimPO-driven refinement using automated review feedback achieved avg score 5.36 vs AI Scientist 4.31 <a href="../results/extraction-result-4353.html#e4353.0" class="evidence-link">[e4353.0]</a> </li>
    <li>KG-CoI with knowledge graph verification outperformed RAG and CoT baselines: GPT-4o KG-CoI accuracy 82.00% vs RAG 75.67% <a href="../results/extraction-result-4308.html#e4308.1" class="evidence-link">[e4308.1]</a> </li>
    <li>LORE framework with LLM-ORE extraction plus ML-Ranker achieved MAP 79.9% vs naive paper-count co-occurrence 69.4% <a href="../results/extraction-result-4334.html#e4334.0" class="evidence-link">[e4334.0]</a> </li>
    <li>TrialMind modular pipeline with human-in-loop verification achieved search recall 0.782 vs GPT-4 baseline 0.073 <a href="../results/extraction-result-4591.html#e4591.0" class="evidence-link">[e4591.0]</a> </li>
    <li>Fine-tuned GPT-3.5 with supervised feedback on chemical extraction tasks achieved F1=77.1% vs prompt-only GPT-4 60-shot F1=32.7% <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>CCA with ChemPrompt principles and iterative refinement achieved 90-99% precision/recall on MOF synthesis extraction <a href="../results/extraction-result-4538.html#e4538.0" class="evidence-link">[e4538.0]</a> </li>
    <li>Enzyme Co-Scientist aggregation agent with multiple LLM feedback improved F1 from individual models and reduced bad cases <a href="../results/extraction-result-4295.html#e4295.0" class="evidence-link">[e4295.0]</a> </li>
    <li>MOOSE multi-aspect iterative feedback system with novelty optimization improved idea quality across multiple evaluation dimensions <a href="../results/extraction-result-4543.html#e4543.0" class="evidence-link">[e4543.0]</a> </li>
    <li>ProtAgents multi-agent system with physics-based feedback (ProteinForceGPT) enables discovery of protein mechanical properties <a href="../results/extraction-result-4521.html#e4521.2" class="evidence-link">[e4521.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This principle is novel in explicitly ranking feedback mechanisms by fidelity for scientific discovery. While iterative refinement is known in ML, the hierarchy of feedback quality (simulation > KG > human > LLM) for scientific law discovery is a new theoretical contribution.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 1: Novelty-Verification Paradox (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; generates_hypothesis &#8594; novel_scientific_claim<span style="color: #888888;">, and</span></div>
        <div>&#8226; verification_mechanism &#8594; relies_on &#8594; existing_literature_or_LLM_knowledge</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; truly_novel_hypotheses &#8594; may_be_rejected_as &#8594; unverifiable_or_implausible<span style="color: #888888;">, and</span></div>
        <div>&#8226; verification_system &#8594; exhibits_bias_toward &#8594; incremental_over_radical_novelty</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SCIMON iterative novelty boosting filters ideas by similarity to literature (threshold μ=0.6), potentially rejecting genuinely novel ideas that lack close precedents <a href="../results/extraction-result-4611.html#e4611.0" class="evidence-link">[e4611.0]</a> </li>
    <li>AI Scientist novelty evaluator tends to default to 'not novel' when uncertain (18 of 32 test cases), showing conservative bias <a href="../results/extraction-result-4577.html#e4577.1" class="evidence-link">[e4577.1]</a> </li>
    <li>KG-CoI hallucination detection may reject valid novel hypotheses not yet in knowledge graphs <a href="../results/extraction-result-4308.html#e4308.1" class="evidence-link">[e4308.1]</a> </li>
    <li>ResearchBench inspiration retrieval plateaus around 70B parameter scale, suggesting limits in discovering truly distant connections <a href="../results/extraction-result-4311.html#e4311.0" class="evidence-link">[e4311.0]</a> </li>
    <li>Scideator novelty-prompt achieved over 10x more agreement with expert-labeled examples compared to AI Scientist's novelty prompt, highlighting difficulty of novelty assessment <a href="../results/extraction-result-4577.html#e4577.1" class="evidence-link">[e4577.1]</a> </li>
    <li>LORE coverage of ClinVar DGs was 71.4% with LLM-ORE vs 94.8% with paper co-occurrence, showing novel relationships may be missed <a href="../results/extraction-result-4334.html#e4334.0" class="evidence-link">[e4334.0]</a> </li>
    <li>LitLLMs retrieval achieved only ~7% coverage of ground-truth citations in best runs, indicating difficulty finding truly novel connections <a href="../results/extraction-result-4366.html#e4366.0" class="evidence-link">[e4366.0]</a> </li>
    <li>MOOSE models tended to produce generic suggestions and copy/rephrase background text rather than truly novel ideas <a href="../results/extraction-result-4543.html#e4543.0" class="evidence-link">[e4543.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This paradox is novel in identifying the fundamental tension between novelty generation and verification in LLM-based discovery. While exploration-exploitation trade-offs are known, this specific manifestation in scientific discovery is a new theoretical insight.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 2: Multi-Agent Consensus Improves Robustness (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_system &#8594; uses_multiple_LLM_agents &#8594; true<span style="color: #888888;">, and</span></div>
        <div>&#8226; agents &#8594; have_diversity_in &#8594; models_or_prompts_or_paths</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_output &#8594; has_robustness &#8594; higher<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_output &#8594; has_variance &#8594; lower</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MPSA with E=3 experts using different permutation-based logical paths and ROUGE-based routing yielded ~4% performance gain <a href="../results/extraction-result-4327.html#e4327.2" class="evidence-link">[e4327.2]</a> </li>
    <li>Enzyme Co-Scientist aggregation agent consolidating outputs from Claude3.5, gpt-4o, Llama3, Qwen improved overall performance and reduced bad cases <a href="../results/extraction-result-4295.html#e4295.0" class="evidence-link">[e4295.0]</a> </li>
    <li>ArticleLLM multi-actor system with fine-tuned Mixtral, Yi, InternLM2 plus integrator outperformed individual models with statistical significance <a href="../results/extraction-result-4343.html#e4343.0" class="evidence-link">[e4343.0]</a> </li>
    <li>ProtAgents multi-agent collaboration combining reasoning, mechanics, and design agents enables complex protein discovery <a href="../results/extraction-result-4521.html#e4521.2" class="evidence-link">[e4521.2]</a> </li>
    <li>CKMAs combining KMCA and MPSA agents achieved ROUGE-1=36.41 vs GPT-3.5 baseline 31.11 <a href="../results/extraction-result-4327.html#e4327.0" class="evidence-link">[e4327.0]</a> <a href="../results/extraction-result-4327.html#e4327.1" class="evidence-link">[e4327.1]</a> <a href="../results/extraction-result-4327.html#e4327.2" class="evidence-link">[e4327.2]</a> </li>
    <li>MOOSE multi-aspect feedback with multiple evaluation dimensions improved idea quality over single-aspect approaches <a href="../results/extraction-result-4543.html#e4543.0" class="evidence-link">[e4543.0]</a> </li>
    <li>ResearchBench aggregation across multiple LLM-generated queries improved retrieval precision by ~10% and recall by ~30% <a href="../results/extraction-result-4311.html#e4311.0" class="evidence-link">[e4311.0]</a> </li>
    <li>DeepResearchEco recursive agentic workflow with multiple specialized agents improved complex question answering <a href="../results/extraction-result-4357.html#e4357.3" class="evidence-link">[e4357.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This extends ensemble learning principles to LLM-based scientific extraction. While ensemble methods are well-known, their specific application to reducing variance in scientific information extraction is a novel contribution.</p>
            <p><strong>References:</strong> <ul>
    <li>Dietterich (2000) Ensemble Methods in Machine Learning [foundational ensemble work]</li>
</ul>
            <h3>Statement 3: Iterative Refinement Diminishing Returns (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; discovery_system &#8594; performs_iterations &#8594; N<span style="color: #888888;">, and</span></div>
        <div>&#8226; N &#8594; greater_than &#8594; 3_to_5</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; marginal_improvement_per_iteration &#8594; decreases_as &#8594; exponential_decay<span style="color: #888888;">, and</span></div>
        <div>&#8226; computational_cost &#8594; increases_as &#8594; linear_in_N</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SCIMON novelty boosting: first iteration novelty increase ~46-55%, second iteration up to ~57.8%, suggesting diminishing returns <a href="../results/extraction-result-4611.html#e4611.0" class="evidence-link">[e4611.0]</a> </li>
    <li>KMCA iterative minigraph construction with k=3 chunks shows convergence in relation quality after several iterations <a href="../results/extraction-result-4327.html#e4327.1" class="evidence-link">[e4327.1]</a> </li>
    <li>CycleResearcher rejection sampling improved scores from ~5.36 (N=1) to ~7.02 (N=100), but with diminishing marginal gains at higher N <a href="../results/extraction-result-4353.html#e4353.0" class="evidence-link">[e4353.0]</a> </li>
    <li>Few-shot MOF extraction with K=4 examples optimal; diminishing returns beyond K=4 in this domain <a href="../results/extraction-result-4325.html#e4325.0" class="evidence-link">[e4325.0]</a> </li>
    <li>Fine-tuning chemical extraction models: too many epochs risk overfitting, suggesting optimal iteration count exists <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>MOOSE iterative novelty boosting: 88.9% of updated ideas substantially different after first iteration, 55.6% more novel, but second iteration gains smaller <a href="../results/extraction-result-4543.html#e4543.0" class="evidence-link">[e4543.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This quantifies the diminishing returns of iterative refinement in LLM-based discovery. While iterative improvement is known, the specific 3-5 iteration sweet spot and exponential decay pattern is a novel empirical finding.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 4: Simulation-Grounded Discovery Enables Quantitative Laws (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; discovery_system &#8594; integrates &#8594; physics_based_simulation_or_quantum_chemistry<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; proposes &#8594; candidate_hypotheses</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; discovered_laws &#8594; can_be &#8594; quantitative_with_numerical_parameters<span style="color: #888888;">, and</span></div>
        <div>&#8226; discovered_laws &#8594; have_physical_validity &#8594; high</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ChemReasoner with GNN-predicted adsorption energies and DFT validation discovers energetic relationships for catalysts <a href="../results/extraction-result-4567.html#e4567.0" class="evidence-link">[e4567.0]</a> </li>
    <li>SGA bilevel optimization with differentiable simulations discovers constitutive laws with optimized continuous parameters <a href="../results/extraction-result-4329.html#e4329.1" class="evidence-link">[e4329.1]</a> <a href="../results/extraction-result-4573.html#e4573.0" class="evidence-link">[e4573.0]</a> </li>
    <li>ProteinForceGPT predicts force-extension curves and mechanical metrics from sequences using MD-simulation-derived training data <a href="../results/extraction-result-4521.html#e4521.2" class="evidence-link">[e4521.2]</a> </li>
    <li>CriticAL uses model-generated samples to compute empirical p-values for statistical discrepancies, enabling quantitative model criticism <a href="../results/extraction-result-4321.html#e4321.0" class="evidence-link">[e4321.0]</a> </li>
    <li>LLM-SR and MOBLLM use LLMs to guide symbolic regression with data-driven validation for equation discovery <a href="../results/extraction-result-4306.html#e4306.1" class="evidence-link">[e4306.1]</a> <a href="../results/extraction-result-4309.html#e4309.2" class="evidence-link">[e4309.2]</a> </li>
    <li>AI-Descartes combines symbolic regression with automated logical reasoning to produce derivable, theory-aligned discoveries <a href="../results/extraction-result-4306.html#e4306.3" class="evidence-link">[e4306.3]</a> </li>
    <li>Automated Statistical Model Discovery uses LLMs with simulation feedback to discover statistical models <a href="../results/extraction-result-4579.html#e4579.2" class="evidence-link">[e4579.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This principle is novel in connecting simulation feedback specifically to the discovery of quantitative (not just qualitative) scientific laws. While simulation-in-the-loop is known, its necessity for quantitative law discovery is a new theoretical statement.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 5: Human-in-Loop Necessity for High-Stakes Extraction (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_task &#8594; has_stakes &#8594; high<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_task &#8594; requires &#8594; domain_expertise_or_safety_validation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; fully_automated_system &#8594; has_reliability &#8594; insufficient<span style="color: #888888;">, and</span></div>
        <div>&#8226; human_in_loop_system &#8594; is_necessary_for &#8594; acceptable_quality</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>TrialMind human-AI study showed screening recall lift 71.4% and time savings 44.2% with human verification <a href="../results/extraction-result-4591.html#e4591.0" class="evidence-link">[e4591.0]</a> </li>
    <li>Enzyme Co-Scientist requires expert review despite high F1=0.90 due to critical nature of kinetic parameters <a href="../results/extraction-result-4295.html#e4295.0" class="evidence-link">[e4295.0]</a> </li>
    <li>CCA MOF extraction achieved 90-99% precision but still required manual intervention for edge cases <a href="../results/extraction-result-4538.html#e4538.0" class="evidence-link">[e4538.0]</a> </li>
    <li>LORE framework requires human annotation for key semantics curation and validation <a href="../results/extraction-result-4334.html#e4334.0" class="evidence-link">[e4334.0]</a> </li>
    <li>Fine-tuned chemical extraction models require high-quality annotated examples and expert validation <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>CriticAL downstream model revisions require human oversight despite automated criticism <a href="../results/extraction-result-4321.html#e4321.0" class="evidence-link">[e4321.0]</a> </li>
    <li>ResearchBench expert validation showed 91.9% accuracy for major issues, indicating need for human verification <a href="../results/extraction-result-4311.html#e4311.0" class="evidence-link">[e4311.0]</a> </li>
    <li>ArticleLLM requires human manual scoring and GPT-4 evaluation for quality assessment <a href="../results/extraction-result-4343.html#e4343.0" class="evidence-link">[e4343.0]</a> </li>
    <li>SciCUEval required five PhD-level expert reviewers for validation, with 90.83% meeting quality criteria after filtering <a href="../results/extraction-result-4313.html#e4313.1" class="evidence-link">[e4313.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law is somewhat related to existing human-AI collaboration principles but is novel in specifying the necessity (not just benefit) of human-in-loop for high-stakes scientific extraction tasks.</p>
            <p><strong>References:</strong> <ul>
    <li>Amershi et al. (2019) Guidelines for Human-AI Interaction [general human-AI collaboration principles]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Discovery systems with 3-5 iterations of refinement using high-fidelity feedback (simulation or expert review) should achieve 2-3x better quality scores than single-pass generation on hypothesis generation tasks.</li>
                <li>Multi-agent systems with 3-5 diverse agents should reduce output variance by 30-50% compared to single-agent systems while maintaining or improving mean quality.</li>
                <li>Integrating physics-based simulation feedback should enable discovery of quantitative scaling laws with <10% error on held-out test cases, compared to >30% error for LLM-only approaches.</li>
                <li>Fine-tuning on domain-specific data with iterative feedback should outperform prompt-only approaches by 20-40% on extraction F1 scores.</li>
                <li>Human-in-loop verification should reduce critical errors by 50-70% compared to fully automated systems in high-stakes domains like drug discovery or materials synthesis.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists an optimal diversity level for multi-agent systems beyond which additional agents provide no benefit or even degrade performance due to noise.</li>
                <li>Whether LLMs can learn to generate hypotheses that are simultaneously highly novel and highly verifiable, or if the Novelty-Verification Paradox represents a fundamental limit.</li>
                <li>Whether iterative refinement with human feedback can match or exceed simulation-based feedback for discovering quantitative laws in domains where simulation is expensive or unavailable.</li>
                <li>Whether the diminishing returns of iterative refinement can be overcome by using increasingly sophisticated feedback mechanisms at each iteration (e.g., simulation at iteration 1, expert review at iteration 2, experimental validation at iteration 3).</li>
                <li>Whether systems that balance novelty generation with verification should discover 20-40% more validated novel hypotheses than systems optimized purely for novelty or purely for verification.</li>
                <li>Whether there exists a theoretical limit to the quality of laws discoverable through LLM-based methods without experimental validation, regardless of iteration count or feedback quality.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that single-pass generation with very large models (>500B parameters) consistently outperforms iterative refinement with smaller models would challenge the necessity of feedback loops.</li>
                <li>Demonstrating that random agent diversity (e.g., random temperature settings) performs as well as principled diversity (e.g., different prompting strategies) would question the Multi-Agent Consensus principle.</li>
                <li>Showing that simulation feedback leads to overfitting to simulation artifacts and worse performance on real experimental data would challenge the Simulation-Grounded Discovery law.</li>
                <li>Finding that novelty-optimized systems discover more experimentally validated hypotheses than verification-optimized systems would contradict the Novelty-Verification Paradox.</li>
                <li>Demonstrating that fully automated systems achieve equal or better quality than human-in-loop systems in high-stakes domains would challenge the Human-in-Loop Necessity law.</li>
                <li>Finding that iteration beyond 3-5 cycles continues to provide substantial improvements would contradict the Diminishing Returns law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal scheduling of different feedback types (when to use simulation vs. human vs. LLM feedback) across iterations is not specified. </li>
    <li>How to automatically detect when iterative refinement has converged and further iterations will not improve quality. </li>
    <li>The role of feedback timing (immediate vs. delayed) in learning and discovery quality. </li>
    <li>How feedback quality requirements scale with the complexity of the target scientific domain. </li>
    <li>Why some domains (e.g., chemistry) benefit more from fine-tuning while others (e.g., cross-domain synthesis) benefit more from prompt engineering. <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> <a href="../results/extraction-result-4327.html#e4327.0" class="evidence-link">[e4327.0]</a> </li>
    <li>The relationship between model size and the need for iterative refinement (whether larger models reduce iteration requirements). </li>
    <li>How to balance computational cost of iteration against quality improvements in resource-constrained settings. </li>
    <li>The role of retrieval quality in determining whether iteration is necessary (high-quality retrieval may reduce iteration needs). <a href="../results/extraction-result-4366.html#e4366.0" class="evidence-link">[e4366.0]</a> <a href="../results/extraction-result-4311.html#e4311.0" class="evidence-link">[e4311.0]</a> </li>
    <li>Why some extraction tasks (e.g., NER) benefit less from iteration than others (e.g., relation extraction). <a href="../results/extraction-result-4597.html#e4597.0" class="evidence-link">[e4597.0]</a> <a href="../results/extraction-result-4342.html#e4342.0" class="evidence-link">[e4342.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This theory is novel in systematically connecting feedback mechanisms, iteration, and discovery quality in LLM-based scientific discovery. While iterative refinement and feedback are known concepts in ML (e.g., RLHF, self-play), their specific application to scientific law discovery with ranked feedback fidelity (simulation > KG > human > LLM) and the identification of the Novelty-Verification Paradox are new theoretical contributions.</p>
            <p><strong>References:</strong> <ul>
    <li>Silver et al. (2017) Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm [iterative self-play improvement]</li>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [RLHF for alignment]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [iterative self-improvement]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Iterative Refinement and Feedback Theory of Scientific Discovery",
    "theory_description": "LLM-based discovery of quantitative laws from literature operates most effectively through iterative cycles of generation, evaluation, and refinement, where each cycle incorporates feedback from multiple sources (simulation, knowledge graphs, human experts, or other LLMs). The quality of discovered laws scales with the fidelity of the feedback mechanism, with physics-based simulation providing the strongest signal for physical laws, knowledge graph verification for relational patterns, and human feedback for novelty assessment. Single-pass generation without feedback loops produces outputs that are plausible but often lack physical grounding or true novelty. However, the benefits of iteration exhibit diminishing returns after 3-5 cycles, and the optimal approach depends on domain maturity, resource constraints, and the specific type of knowledge being extracted.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Feedback Fidelity Determines Discovery Quality",
                "if": [
                    {
                        "subject": "discovery_system",
                        "relation": "incorporates_feedback_mechanism",
                        "object": "high_fidelity_evaluator"
                    },
                    {
                        "subject": "feedback_mechanism",
                        "relation": "provides_signal_about",
                        "object": "physical_validity_or_novelty"
                    }
                ],
                "then": [
                    {
                        "subject": "discovered_laws",
                        "relation": "have_quality",
                        "object": "higher"
                    },
                    {
                        "subject": "discovered_laws",
                        "relation": "have_physical_grounding",
                        "object": "stronger"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ChemReasoner with quantum-chemical feedback (GNN + DFT validation) achieved substantially higher rewards than LLM-only baselines (GPT-4 CHEMREASONER-Planner 2.36 vs Chain-of-Thought 0.37)",
                        "uuids": [
                            "e4567.0"
                        ]
                    },
                    {
                        "text": "CriticAL with model-generated samples for hypothesis testing achieved 94% win rate (&gt;1 SE) in downstream ELPD improvement vs initial models",
                        "uuids": [
                            "e4321.0"
                        ]
                    },
                    {
                        "text": "SGA bilevel optimization combining LLM hypothesis generation with differentiable simulation feedback enables discovery of constitutive laws",
                        "uuids": [
                            "e4329.1",
                            "e4573.0"
                        ]
                    },
                    {
                        "text": "CycleResearcher with iterative SimPO-driven refinement using automated review feedback achieved avg score 5.36 vs AI Scientist 4.31",
                        "uuids": [
                            "e4353.0"
                        ]
                    },
                    {
                        "text": "KG-CoI with knowledge graph verification outperformed RAG and CoT baselines: GPT-4o KG-CoI accuracy 82.00% vs RAG 75.67%",
                        "uuids": [
                            "e4308.1"
                        ]
                    },
                    {
                        "text": "LORE framework with LLM-ORE extraction plus ML-Ranker achieved MAP 79.9% vs naive paper-count co-occurrence 69.4%",
                        "uuids": [
                            "e4334.0"
                        ]
                    },
                    {
                        "text": "TrialMind modular pipeline with human-in-loop verification achieved search recall 0.782 vs GPT-4 baseline 0.073",
                        "uuids": [
                            "e4591.0"
                        ]
                    },
                    {
                        "text": "Fine-tuned GPT-3.5 with supervised feedback on chemical extraction tasks achieved F1=77.1% vs prompt-only GPT-4 60-shot F1=32.7%",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "CCA with ChemPrompt principles and iterative refinement achieved 90-99% precision/recall on MOF synthesis extraction",
                        "uuids": [
                            "e4538.0"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist aggregation agent with multiple LLM feedback improved F1 from individual models and reduced bad cases",
                        "uuids": [
                            "e4295.0"
                        ]
                    },
                    {
                        "text": "MOOSE multi-aspect iterative feedback system with novelty optimization improved idea quality across multiple evaluation dimensions",
                        "uuids": [
                            "e4543.0"
                        ]
                    },
                    {
                        "text": "ProtAgents multi-agent system with physics-based feedback (ProteinForceGPT) enables discovery of protein mechanical properties",
                        "uuids": [
                            "e4521.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This principle is novel in explicitly ranking feedback mechanisms by fidelity for scientific discovery. While iterative refinement is known in ML, the hierarchy of feedback quality (simulation &gt; KG &gt; human &gt; LLM) for scientific law discovery is a new theoretical contribution.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Novelty-Verification Paradox",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "generates_hypothesis",
                        "object": "novel_scientific_claim"
                    },
                    {
                        "subject": "verification_mechanism",
                        "relation": "relies_on",
                        "object": "existing_literature_or_LLM_knowledge"
                    }
                ],
                "then": [
                    {
                        "subject": "truly_novel_hypotheses",
                        "relation": "may_be_rejected_as",
                        "object": "unverifiable_or_implausible"
                    },
                    {
                        "subject": "verification_system",
                        "relation": "exhibits_bias_toward",
                        "object": "incremental_over_radical_novelty"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SCIMON iterative novelty boosting filters ideas by similarity to literature (threshold μ=0.6), potentially rejecting genuinely novel ideas that lack close precedents",
                        "uuids": [
                            "e4611.0"
                        ]
                    },
                    {
                        "text": "AI Scientist novelty evaluator tends to default to 'not novel' when uncertain (18 of 32 test cases), showing conservative bias",
                        "uuids": [
                            "e4577.1"
                        ]
                    },
                    {
                        "text": "KG-CoI hallucination detection may reject valid novel hypotheses not yet in knowledge graphs",
                        "uuids": [
                            "e4308.1"
                        ]
                    },
                    {
                        "text": "ResearchBench inspiration retrieval plateaus around 70B parameter scale, suggesting limits in discovering truly distant connections",
                        "uuids": [
                            "e4311.0"
                        ]
                    },
                    {
                        "text": "Scideator novelty-prompt achieved over 10x more agreement with expert-labeled examples compared to AI Scientist's novelty prompt, highlighting difficulty of novelty assessment",
                        "uuids": [
                            "e4577.1"
                        ]
                    },
                    {
                        "text": "LORE coverage of ClinVar DGs was 71.4% with LLM-ORE vs 94.8% with paper co-occurrence, showing novel relationships may be missed",
                        "uuids": [
                            "e4334.0"
                        ]
                    },
                    {
                        "text": "LitLLMs retrieval achieved only ~7% coverage of ground-truth citations in best runs, indicating difficulty finding truly novel connections",
                        "uuids": [
                            "e4366.0"
                        ]
                    },
                    {
                        "text": "MOOSE models tended to produce generic suggestions and copy/rephrase background text rather than truly novel ideas",
                        "uuids": [
                            "e4543.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This paradox is novel in identifying the fundamental tension between novelty generation and verification in LLM-based discovery. While exploration-exploitation trade-offs are known, this specific manifestation in scientific discovery is a new theoretical insight.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Multi-Agent Consensus Improves Robustness",
                "if": [
                    {
                        "subject": "extraction_system",
                        "relation": "uses_multiple_LLM_agents",
                        "object": "true"
                    },
                    {
                        "subject": "agents",
                        "relation": "have_diversity_in",
                        "object": "models_or_prompts_or_paths"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_output",
                        "relation": "has_robustness",
                        "object": "higher"
                    },
                    {
                        "subject": "extraction_output",
                        "relation": "has_variance",
                        "object": "lower"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MPSA with E=3 experts using different permutation-based logical paths and ROUGE-based routing yielded ~4% performance gain",
                        "uuids": [
                            "e4327.2"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist aggregation agent consolidating outputs from Claude3.5, gpt-4o, Llama3, Qwen improved overall performance and reduced bad cases",
                        "uuids": [
                            "e4295.0"
                        ]
                    },
                    {
                        "text": "ArticleLLM multi-actor system with fine-tuned Mixtral, Yi, InternLM2 plus integrator outperformed individual models with statistical significance",
                        "uuids": [
                            "e4343.0"
                        ]
                    },
                    {
                        "text": "ProtAgents multi-agent collaboration combining reasoning, mechanics, and design agents enables complex protein discovery",
                        "uuids": [
                            "e4521.2"
                        ]
                    },
                    {
                        "text": "CKMAs combining KMCA and MPSA agents achieved ROUGE-1=36.41 vs GPT-3.5 baseline 31.11",
                        "uuids": [
                            "e4327.0",
                            "e4327.1",
                            "e4327.2"
                        ]
                    },
                    {
                        "text": "MOOSE multi-aspect feedback with multiple evaluation dimensions improved idea quality over single-aspect approaches",
                        "uuids": [
                            "e4543.0"
                        ]
                    },
                    {
                        "text": "ResearchBench aggregation across multiple LLM-generated queries improved retrieval precision by ~10% and recall by ~30%",
                        "uuids": [
                            "e4311.0"
                        ]
                    },
                    {
                        "text": "DeepResearchEco recursive agentic workflow with multiple specialized agents improved complex question answering",
                        "uuids": [
                            "e4357.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This extends ensemble learning principles to LLM-based scientific extraction. While ensemble methods are well-known, their specific application to reducing variance in scientific information extraction is a novel contribution.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Dietterich (2000) Ensemble Methods in Machine Learning [foundational ensemble work]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Iterative Refinement Diminishing Returns",
                "if": [
                    {
                        "subject": "discovery_system",
                        "relation": "performs_iterations",
                        "object": "N"
                    },
                    {
                        "subject": "N",
                        "relation": "greater_than",
                        "object": "3_to_5"
                    }
                ],
                "then": [
                    {
                        "subject": "marginal_improvement_per_iteration",
                        "relation": "decreases_as",
                        "object": "exponential_decay"
                    },
                    {
                        "subject": "computational_cost",
                        "relation": "increases_as",
                        "object": "linear_in_N"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SCIMON novelty boosting: first iteration novelty increase ~46-55%, second iteration up to ~57.8%, suggesting diminishing returns",
                        "uuids": [
                            "e4611.0"
                        ]
                    },
                    {
                        "text": "KMCA iterative minigraph construction with k=3 chunks shows convergence in relation quality after several iterations",
                        "uuids": [
                            "e4327.1"
                        ]
                    },
                    {
                        "text": "CycleResearcher rejection sampling improved scores from ~5.36 (N=1) to ~7.02 (N=100), but with diminishing marginal gains at higher N",
                        "uuids": [
                            "e4353.0"
                        ]
                    },
                    {
                        "text": "Few-shot MOF extraction with K=4 examples optimal; diminishing returns beyond K=4 in this domain",
                        "uuids": [
                            "e4325.0"
                        ]
                    },
                    {
                        "text": "Fine-tuning chemical extraction models: too many epochs risk overfitting, suggesting optimal iteration count exists",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "MOOSE iterative novelty boosting: 88.9% of updated ideas substantially different after first iteration, 55.6% more novel, but second iteration gains smaller",
                        "uuids": [
                            "e4543.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This quantifies the diminishing returns of iterative refinement in LLM-based discovery. While iterative improvement is known, the specific 3-5 iteration sweet spot and exponential decay pattern is a novel empirical finding.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Simulation-Grounded Discovery Enables Quantitative Laws",
                "if": [
                    {
                        "subject": "discovery_system",
                        "relation": "integrates",
                        "object": "physics_based_simulation_or_quantum_chemistry"
                    },
                    {
                        "subject": "LLM",
                        "relation": "proposes",
                        "object": "candidate_hypotheses"
                    }
                ],
                "then": [
                    {
                        "subject": "discovered_laws",
                        "relation": "can_be",
                        "object": "quantitative_with_numerical_parameters"
                    },
                    {
                        "subject": "discovered_laws",
                        "relation": "have_physical_validity",
                        "object": "high"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ChemReasoner with GNN-predicted adsorption energies and DFT validation discovers energetic relationships for catalysts",
                        "uuids": [
                            "e4567.0"
                        ]
                    },
                    {
                        "text": "SGA bilevel optimization with differentiable simulations discovers constitutive laws with optimized continuous parameters",
                        "uuids": [
                            "e4329.1",
                            "e4573.0"
                        ]
                    },
                    {
                        "text": "ProteinForceGPT predicts force-extension curves and mechanical metrics from sequences using MD-simulation-derived training data",
                        "uuids": [
                            "e4521.2"
                        ]
                    },
                    {
                        "text": "CriticAL uses model-generated samples to compute empirical p-values for statistical discrepancies, enabling quantitative model criticism",
                        "uuids": [
                            "e4321.0"
                        ]
                    },
                    {
                        "text": "LLM-SR and MOBLLM use LLMs to guide symbolic regression with data-driven validation for equation discovery",
                        "uuids": [
                            "e4306.1",
                            "e4309.2"
                        ]
                    },
                    {
                        "text": "AI-Descartes combines symbolic regression with automated logical reasoning to produce derivable, theory-aligned discoveries",
                        "uuids": [
                            "e4306.3"
                        ]
                    },
                    {
                        "text": "Automated Statistical Model Discovery uses LLMs with simulation feedback to discover statistical models",
                        "uuids": [
                            "e4579.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This principle is novel in connecting simulation feedback specifically to the discovery of quantitative (not just qualitative) scientific laws. While simulation-in-the-loop is known, its necessity for quantitative law discovery is a new theoretical statement.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Human-in-Loop Necessity for High-Stakes Extraction",
                "if": [
                    {
                        "subject": "extraction_task",
                        "relation": "has_stakes",
                        "object": "high"
                    },
                    {
                        "subject": "extraction_task",
                        "relation": "requires",
                        "object": "domain_expertise_or_safety_validation"
                    }
                ],
                "then": [
                    {
                        "subject": "fully_automated_system",
                        "relation": "has_reliability",
                        "object": "insufficient"
                    },
                    {
                        "subject": "human_in_loop_system",
                        "relation": "is_necessary_for",
                        "object": "acceptable_quality"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "TrialMind human-AI study showed screening recall lift 71.4% and time savings 44.2% with human verification",
                        "uuids": [
                            "e4591.0"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist requires expert review despite high F1=0.90 due to critical nature of kinetic parameters",
                        "uuids": [
                            "e4295.0"
                        ]
                    },
                    {
                        "text": "CCA MOF extraction achieved 90-99% precision but still required manual intervention for edge cases",
                        "uuids": [
                            "e4538.0"
                        ]
                    },
                    {
                        "text": "LORE framework requires human annotation for key semantics curation and validation",
                        "uuids": [
                            "e4334.0"
                        ]
                    },
                    {
                        "text": "Fine-tuned chemical extraction models require high-quality annotated examples and expert validation",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "CriticAL downstream model revisions require human oversight despite automated criticism",
                        "uuids": [
                            "e4321.0"
                        ]
                    },
                    {
                        "text": "ResearchBench expert validation showed 91.9% accuracy for major issues, indicating need for human verification",
                        "uuids": [
                            "e4311.0"
                        ]
                    },
                    {
                        "text": "ArticleLLM requires human manual scoring and GPT-4 evaluation for quality assessment",
                        "uuids": [
                            "e4343.0"
                        ]
                    },
                    {
                        "text": "SciCUEval required five PhD-level expert reviewers for validation, with 90.83% meeting quality criteria after filtering",
                        "uuids": [
                            "e4313.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This law is somewhat related to existing human-AI collaboration principles but is novel in specifying the necessity (not just benefit) of human-in-loop for high-stakes scientific extraction tasks.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Amershi et al. (2019) Guidelines for Human-AI Interaction [general human-AI collaboration principles]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Discovery systems with 3-5 iterations of refinement using high-fidelity feedback (simulation or expert review) should achieve 2-3x better quality scores than single-pass generation on hypothesis generation tasks.",
        "Multi-agent systems with 3-5 diverse agents should reduce output variance by 30-50% compared to single-agent systems while maintaining or improving mean quality.",
        "Integrating physics-based simulation feedback should enable discovery of quantitative scaling laws with &lt;10% error on held-out test cases, compared to &gt;30% error for LLM-only approaches.",
        "Fine-tuning on domain-specific data with iterative feedback should outperform prompt-only approaches by 20-40% on extraction F1 scores.",
        "Human-in-loop verification should reduce critical errors by 50-70% compared to fully automated systems in high-stakes domains like drug discovery or materials synthesis."
    ],
    "new_predictions_unknown": [
        "Whether there exists an optimal diversity level for multi-agent systems beyond which additional agents provide no benefit or even degrade performance due to noise.",
        "Whether LLMs can learn to generate hypotheses that are simultaneously highly novel and highly verifiable, or if the Novelty-Verification Paradox represents a fundamental limit.",
        "Whether iterative refinement with human feedback can match or exceed simulation-based feedback for discovering quantitative laws in domains where simulation is expensive or unavailable.",
        "Whether the diminishing returns of iterative refinement can be overcome by using increasingly sophisticated feedback mechanisms at each iteration (e.g., simulation at iteration 1, expert review at iteration 2, experimental validation at iteration 3).",
        "Whether systems that balance novelty generation with verification should discover 20-40% more validated novel hypotheses than systems optimized purely for novelty or purely for verification.",
        "Whether there exists a theoretical limit to the quality of laws discoverable through LLM-based methods without experimental validation, regardless of iteration count or feedback quality."
    ],
    "negative_experiments": [
        "Finding that single-pass generation with very large models (&gt;500B parameters) consistently outperforms iterative refinement with smaller models would challenge the necessity of feedback loops.",
        "Demonstrating that random agent diversity (e.g., random temperature settings) performs as well as principled diversity (e.g., different prompting strategies) would question the Multi-Agent Consensus principle.",
        "Showing that simulation feedback leads to overfitting to simulation artifacts and worse performance on real experimental data would challenge the Simulation-Grounded Discovery law.",
        "Finding that novelty-optimized systems discover more experimentally validated hypotheses than verification-optimized systems would contradict the Novelty-Verification Paradox.",
        "Demonstrating that fully automated systems achieve equal or better quality than human-in-loop systems in high-stakes domains would challenge the Human-in-Loop Necessity law.",
        "Finding that iteration beyond 3-5 cycles continues to provide substantial improvements would contradict the Diminishing Returns law."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal scheduling of different feedback types (when to use simulation vs. human vs. LLM feedback) across iterations is not specified.",
            "uuids": []
        },
        {
            "text": "How to automatically detect when iterative refinement has converged and further iterations will not improve quality.",
            "uuids": []
        },
        {
            "text": "The role of feedback timing (immediate vs. delayed) in learning and discovery quality.",
            "uuids": []
        },
        {
            "text": "How feedback quality requirements scale with the complexity of the target scientific domain.",
            "uuids": []
        },
        {
            "text": "Why some domains (e.g., chemistry) benefit more from fine-tuning while others (e.g., cross-domain synthesis) benefit more from prompt engineering.",
            "uuids": [
                "e4344.0",
                "e4327.0"
            ]
        },
        {
            "text": "The relationship between model size and the need for iterative refinement (whether larger models reduce iteration requirements).",
            "uuids": []
        },
        {
            "text": "How to balance computational cost of iteration against quality improvements in resource-constrained settings.",
            "uuids": []
        },
        {
            "text": "The role of retrieval quality in determining whether iteration is necessary (high-quality retrieval may reduce iteration needs).",
            "uuids": [
                "e4366.0",
                "e4311.0"
            ]
        },
        {
            "text": "Why some extraction tasks (e.g., NER) benefit less from iteration than others (e.g., relation extraction).",
            "uuids": [
                "e4597.0",
                "e4342.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some single-pass systems achieve competitive performance (e.g., GPT-4 with browsing for synthesis planning achieved high scores), suggesting iterative refinement may not always be necessary.",
            "uuids": [
                "e4338.1"
            ]
        },
        {
            "text": "CycleResearcher's fabricated experimental results during training raise questions about whether simulation feedback can truly ground discovery without real experiments.",
            "uuids": [
                "e4353.0"
            ]
        },
        {
            "text": "Zero-shot ChatGPT with verification achieved 82.7% exact match accuracy on MOF extraction, competitive with some iterative approaches.",
            "uuids": [
                "e4344.0"
            ]
        },
        {
            "text": "Galactica citation prediction achieved 51.9% accuracy without explicit iterative refinement, suggesting pretrained knowledge can be sufficient for some tasks.",
            "uuids": [
                "e4581.2"
            ]
        },
        {
            "text": "Pure ChatGPT (GPT-4o) achieved recall 0.474 on biomedical query without iteration, showing single-pass can work for some domains.",
            "uuids": [
                "e4332.1"
            ]
        },
        {
            "text": "LangChain chunked extraction achieved higher recall than one-shot in some cases, but reduced precision, suggesting iteration trade-offs.",
            "uuids": [
                "e4348.1"
            ]
        },
        {
            "text": "Some fully automated systems (e.g., seq2rel end-to-end) achieved reasonable performance without human-in-loop.",
            "uuids": [
                "e4597.0"
            ]
        }
    ],
    "special_cases": [
        "For well-established scientific domains with comprehensive knowledge graphs, verification-heavy approaches may be more effective than novelty-heavy approaches.",
        "For emerging scientific domains with limited prior literature, novelty-heavy approaches may be necessary despite higher false positive rates.",
        "When computational resources are severely limited, single-pass generation with careful prompt engineering may be the only feasible approach despite lower quality.",
        "For extraction tasks with definite labels (e.g., entity recognition), ensemble methods with weighted voting may be more effective than iterative refinement.",
        "For tasks requiring long-context understanding (e.g., full-paper synthesis), chunking strategies may be necessary regardless of iteration approach.",
        "In domains where simulation is expensive or unavailable (e.g., social sciences), human feedback may be the only viable high-fidelity feedback mechanism.",
        "For time-sensitive applications (e.g., rapid literature review), single-pass with retrieval augmentation may be preferred over slower iterative approaches.",
        "When training data is abundant and high-quality, fine-tuning may be more effective than iterative prompting.",
        "For tasks with clear evaluation metrics (e.g., exact match), automated feedback may be sufficient without human-in-loop."
    ],
    "existing_theory": {
        "classification_explanation": "This theory is novel in systematically connecting feedback mechanisms, iteration, and discovery quality in LLM-based scientific discovery. While iterative refinement and feedback are known concepts in ML (e.g., RLHF, self-play), their specific application to scientific law discovery with ranked feedback fidelity (simulation &gt; KG &gt; human &gt; LLM) and the identification of the Novelty-Verification Paradox are new theoretical contributions.",
        "likely_classification": "new",
        "references": [
            "Silver et al. (2017) Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm [iterative self-play improvement]",
            "Ouyang et al. (2022) Training language models to follow instructions with human feedback [RLHF for alignment]",
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [iterative self-improvement]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
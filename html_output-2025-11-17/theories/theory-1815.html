<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain and Prompt Sensitivity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1815</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1815</p>
                <p><strong>Name:</strong> Domain and Prompt Sensitivity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory posits that the accuracy of large language models (LLMs) in estimating the probability of future real-world scientific discoveries is systematically influenced by both the scientific domain of the query and the framing of the prompt. Specifically, the theory asserts that LLMs' probabilistic outputs are more reliable in domains with dense, high-quality training data and when prompts are structured to elicit conditional or counterfactual reasoning, rather than direct probability queries. The interplay between domain familiarity and prompt structure determines the calibration and informativeness of LLM predictions.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Domain Familiarity Calibration Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_queried_about &#8594; scientific_domain_with_high_training_data_density</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_more_calibrated_than &#8594; estimate_for_low_data_density_domain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs demonstrate higher factual accuracy and calibration in domains with abundant, high-quality training data (e.g., biomedicine, physics) compared to emerging or niche fields. </li>
    <li>Empirical studies show LLMs' performance degrades in underrepresented scientific subfields. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the effect of data coverage on LLM accuracy is established, its explicit link to scientific forecasting calibration is novel.</p>            <p><strong>What Already Exists:</strong> LLM performance is known to correlate with training data coverage in various tasks.</p>            <p><strong>What is Novel:</strong> This law extends the relationship to the calibration of probability estimates for future scientific discoveries.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Data coverage and performance]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and domain effects]</li>
</ul>
            <h3>Statement 1: Prompt Structure Sensitivity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt &#8594; is_framed_as &#8594; conditional_or_counterfactual</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_more_nuanced_and_calibrated_than &#8594; estimate_for_direct_probability_prompt</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompt engineering research shows that conditional and counterfactual prompts elicit more reasoned and calibrated responses from LLMs. </li>
    <li>Studies in complex reasoning tasks demonstrate improved LLM performance with structured, stepwise, or conditional prompts. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This is a new application of prompt framing effects to the domain of scientific forecasting.</p>            <p><strong>What Already Exists:</strong> Prompt framing effects are well-documented in LLM QA and reasoning tasks.</p>            <p><strong>What is Novel:</strong> The law applies these effects specifically to scientific discovery probability estimation.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt framing and reasoning]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will provide more accurate probability estimates for future discoveries in well-represented scientific domains (e.g., genomics) than in emerging fields (e.g., quantum biology).</li>
                <li>Reframing a direct probability query as a conditional or counterfactual prompt will improve the calibration of LLM outputs.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>In domains with moderate data density, the interaction between prompt structure and domain familiarity may produce non-linear effects on calibration.</li>
                <li>Combining multiple conditional prompts may lead to either compounding improvements or confusion in LLM probability estimates.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs show no difference in calibration between high and low data density domains, the domain familiarity law is falsified.</li>
                <li>If prompt structure has no effect on calibration, the prompt sensitivity law is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The effect of adversarial or misleading prompts on calibration is not addressed. </li>
    <li>The impact of model size and architecture on domain and prompt sensitivity is not explicitly modeled. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes known effects into a novel framework for understanding LLM scientific forecasting.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Data coverage and performance]</li>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt framing and reasoning]</li>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and prompt effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain and Prompt Sensitivity Theory",
    "theory_description": "This theory posits that the accuracy of large language models (LLMs) in estimating the probability of future real-world scientific discoveries is systematically influenced by both the scientific domain of the query and the framing of the prompt. Specifically, the theory asserts that LLMs' probabilistic outputs are more reliable in domains with dense, high-quality training data and when prompts are structured to elicit conditional or counterfactual reasoning, rather than direct probability queries. The interplay between domain familiarity and prompt structure determines the calibration and informativeness of LLM predictions.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Domain Familiarity Calibration Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_queried_about",
                        "object": "scientific_domain_with_high_training_data_density"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_more_calibrated_than",
                        "object": "estimate_for_low_data_density_domain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs demonstrate higher factual accuracy and calibration in domains with abundant, high-quality training data (e.g., biomedicine, physics) compared to emerging or niche fields.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LLMs' performance degrades in underrepresented scientific subfields.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LLM performance is known to correlate with training data coverage in various tasks.",
                    "what_is_novel": "This law extends the relationship to the calibration of probability estimates for future scientific discoveries.",
                    "classification_explanation": "While the effect of data coverage on LLM accuracy is established, its explicit link to scientific forecasting calibration is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Data coverage and performance]",
                        "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and domain effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt Structure Sensitivity Law",
                "if": [
                    {
                        "subject": "prompt",
                        "relation": "is_framed_as",
                        "object": "conditional_or_counterfactual"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_more_nuanced_and_calibrated_than",
                        "object": "estimate_for_direct_probability_prompt"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompt engineering research shows that conditional and counterfactual prompts elicit more reasoned and calibrated responses from LLMs.",
                        "uuids": []
                    },
                    {
                        "text": "Studies in complex reasoning tasks demonstrate improved LLM performance with structured, stepwise, or conditional prompts.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt framing effects are well-documented in LLM QA and reasoning tasks.",
                    "what_is_novel": "The law applies these effects specifically to scientific discovery probability estimation.",
                    "classification_explanation": "This is a new application of prompt framing effects to the domain of scientific forecasting.",
                    "likely_classification": "new",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt framing and reasoning]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will provide more accurate probability estimates for future discoveries in well-represented scientific domains (e.g., genomics) than in emerging fields (e.g., quantum biology).",
        "Reframing a direct probability query as a conditional or counterfactual prompt will improve the calibration of LLM outputs."
    ],
    "new_predictions_unknown": [
        "In domains with moderate data density, the interaction between prompt structure and domain familiarity may produce non-linear effects on calibration.",
        "Combining multiple conditional prompts may lead to either compounding improvements or confusion in LLM probability estimates."
    ],
    "negative_experiments": [
        "If LLMs show no difference in calibration between high and low data density domains, the domain familiarity law is falsified.",
        "If prompt structure has no effect on calibration, the prompt sensitivity law is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "The effect of adversarial or misleading prompts on calibration is not addressed.",
            "uuids": []
        },
        {
            "text": "The impact of model size and architecture on domain and prompt sensitivity is not explicitly modeled.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs exhibit overconfidence or hallucination even in high-data domains when faced with ambiguous prompts.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Domains with rapidly evolving knowledge may see calibration degrade over time, regardless of initial data density.",
        "LLMs with explicit retrieval or reasoning modules may be less sensitive to prompt framing."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt framing and data coverage effects are established in LLM literature.",
        "what_is_novel": "The explicit integration of both domain and prompt sensitivity into a unified theory of scientific forecasting calibration is new.",
        "classification_explanation": "This theory synthesizes known effects into a novel framework for understanding LLM scientific forecasting.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Data coverage and performance]",
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt framing and reasoning]",
            "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and prompt effects]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-647",
    "original_theory_name": "Domain and Prompt Sensitivity Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Domain and Prompt Sensitivity Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
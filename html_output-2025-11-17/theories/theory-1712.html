<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probabilistic Sequence Likelihood Anomaly Detection - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1712</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1712</p>
                <p><strong>Name:</strong> Probabilistic Sequence Likelihood Anomaly Detection</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can be used for detecting anomalies in lists of data.</p>
                <p><strong>Description:</strong> This theory posits that language models can be used for anomaly detection in lists by assigning a probability (likelihood) to each item or subsequence, given the context of the list. Items with significantly lower likelihoods than expected are flagged as anomalies, leveraging the LM's learned distribution over valid list sequences.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: LMs Assign Probabilistic Likelihoods to List Items (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_trained_on &#8594; lists with consistent structure<span style="color: #888888;">, and</span></div>
        <div>&#8226; list item &#8594; is_part_of &#8594; input list</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; assigns_probability &#8594; list item given preceding context</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs are trained to predict next tokens/items and assign probabilities to them, as in language modeling and code completion. </li>
    <li>Perplexity and likelihood metrics are standard for evaluating LM fit to sequences. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Probability assignment is standard, but its use for anomaly detection in lists is less explored.</p>            <p><strong>What Already Exists:</strong> LMs assign probabilities to tokens/items in sequences.</p>            <p><strong>What is Novel:</strong> Application to anomaly detection in arbitrary lists is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Radford et al. (2019) Language Models are Unsupervised Multitask Learners [LMs assign probabilities to tokens]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [LMs as probabilistic predictors]</li>
</ul>
            <h3>Statement 1: Low-Likelihood Items are Anomalies (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; list item &#8594; has_probability &#8594; p<span style="color: #888888;">, and</span></div>
        <div>&#8226; p &#8594; is_significantly_lower_than &#8594; expected probability for in-distribution items</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; list item &#8594; is_flagged_as &#8594; anomaly</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Anomaly detection via likelihood thresholds is a standard approach in generative modeling and OOD detection. </li>
    <li>Empirical studies show LMs assign low probability to OOD or anomalous items in text and structured data. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Likelihood-based anomaly detection is established, but its use for arbitrary lists with LMs is less explored.</p>            <p><strong>What Already Exists:</strong> Likelihood-based anomaly detection is established in generative modeling.</p>            <p><strong>What is Novel:</strong> Explicit application to arbitrary list data with LMs is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Hendrycks et al. (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [Likelihood-based OOD detection]</li>
    <li>Ren et al. (2019) Likelihood Ratios for Out-of-Distribution Detection [Likelihood-based anomaly detection]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a list contains an item with much lower LM-assigned probability than others, it will be flagged as an anomaly.</li>
                <li>If a list is perturbed by inserting a random or OOD item, the LM will assign it low likelihood and flag it.</li>
                <li>If a list is entirely in-distribution, all items will have high likelihood and none will be flagged.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If the LM is trained on highly variable lists, the threshold for anomaly detection may become unreliable.</li>
                <li>If an adversarial item is crafted to mimic high-likelihood patterns, the LM may fail to flag it as anomalous.</li>
                <li>If the LM is exposed to distributional shift, its likelihood estimates may become miscalibrated.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If the LM assigns high probability to OOD or anomalous items, the theory is challenged.</li>
                <li>If the LM fails to distinguish between rare but valid and truly anomalous items, the theory's utility is limited.</li>
                <li>If the LM's likelihoods are not predictive of anomaly status, the theory is undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Anomalies that are subtle or context-dependent may not have sufficiently low likelihood to be detected. </li>
    <li>LMs may assign low probability to rare but valid items, leading to false positives. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory adapts established likelihood-based anomaly detection to the context of LMs and arbitrary lists.</p>
            <p><strong>References:</strong> <ul>
    <li>Hendrycks et al. (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [Likelihood-based OOD detection]</li>
    <li>Ren et al. (2019) Likelihood Ratios for Out-of-Distribution Detection [Likelihood-based anomaly detection]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Probabilistic Sequence Likelihood Anomaly Detection",
    "theory_description": "This theory posits that language models can be used for anomaly detection in lists by assigning a probability (likelihood) to each item or subsequence, given the context of the list. Items with significantly lower likelihoods than expected are flagged as anomalies, leveraging the LM's learned distribution over valid list sequences.",
    "theory_statements": [
        {
            "law": {
                "law_name": "LMs Assign Probabilistic Likelihoods to List Items",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_trained_on",
                        "object": "lists with consistent structure"
                    },
                    {
                        "subject": "list item",
                        "relation": "is_part_of",
                        "object": "input list"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "assigns_probability",
                        "object": "list item given preceding context"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs are trained to predict next tokens/items and assign probabilities to them, as in language modeling and code completion.",
                        "uuids": []
                    },
                    {
                        "text": "Perplexity and likelihood metrics are standard for evaluating LM fit to sequences.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "LMs assign probabilities to tokens/items in sequences.",
                    "what_is_novel": "Application to anomaly detection in arbitrary lists is novel.",
                    "classification_explanation": "Probability assignment is standard, but its use for anomaly detection in lists is less explored.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Radford et al. (2019) Language Models are Unsupervised Multitask Learners [LMs assign probabilities to tokens]",
                        "Brown et al. (2020) Language Models are Few-Shot Learners [LMs as probabilistic predictors]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Low-Likelihood Items are Anomalies",
                "if": [
                    {
                        "subject": "list item",
                        "relation": "has_probability",
                        "object": "p"
                    },
                    {
                        "subject": "p",
                        "relation": "is_significantly_lower_than",
                        "object": "expected probability for in-distribution items"
                    }
                ],
                "then": [
                    {
                        "subject": "list item",
                        "relation": "is_flagged_as",
                        "object": "anomaly"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Anomaly detection via likelihood thresholds is a standard approach in generative modeling and OOD detection.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show LMs assign low probability to OOD or anomalous items in text and structured data.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Likelihood-based anomaly detection is established in generative modeling.",
                    "what_is_novel": "Explicit application to arbitrary list data with LMs is novel.",
                    "classification_explanation": "Likelihood-based anomaly detection is established, but its use for arbitrary lists with LMs is less explored.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Hendrycks et al. (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [Likelihood-based OOD detection]",
                        "Ren et al. (2019) Likelihood Ratios for Out-of-Distribution Detection [Likelihood-based anomaly detection]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a list contains an item with much lower LM-assigned probability than others, it will be flagged as an anomaly.",
        "If a list is perturbed by inserting a random or OOD item, the LM will assign it low likelihood and flag it.",
        "If a list is entirely in-distribution, all items will have high likelihood and none will be flagged."
    ],
    "new_predictions_unknown": [
        "If the LM is trained on highly variable lists, the threshold for anomaly detection may become unreliable.",
        "If an adversarial item is crafted to mimic high-likelihood patterns, the LM may fail to flag it as anomalous.",
        "If the LM is exposed to distributional shift, its likelihood estimates may become miscalibrated."
    ],
    "negative_experiments": [
        "If the LM assigns high probability to OOD or anomalous items, the theory is challenged.",
        "If the LM fails to distinguish between rare but valid and truly anomalous items, the theory's utility is limited.",
        "If the LM's likelihoods are not predictive of anomaly status, the theory is undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Anomalies that are subtle or context-dependent may not have sufficiently low likelihood to be detected.",
            "uuids": []
        },
        {
            "text": "LMs may assign low probability to rare but valid items, leading to false positives.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Likelihood-based OOD detection can fail when the model assigns high likelihood to some OOD items (e.g., 'likelihood holes').",
            "uuids": []
        }
    ],
    "special_cases": [
        "Lists with highly variable or multimodal distributions may yield unreliable likelihood thresholds.",
        "If the LM is poorly calibrated, likelihood-based anomaly detection may be ineffective."
    ],
    "existing_theory": {
        "what_already_exists": "Likelihood-based anomaly detection is established in generative modeling.",
        "what_is_novel": "Explicit application to arbitrary list data with LMs is novel.",
        "classification_explanation": "The theory adapts established likelihood-based anomaly detection to the context of LMs and arbitrary lists.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Hendrycks et al. (2017) A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks [Likelihood-based OOD detection]",
            "Ren et al. (2019) Likelihood Ratios for Out-of-Distribution Detection [Likelihood-based anomaly detection]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can be used for detecting anomalies in lists of data.",
    "original_theory_id": "theory-641",
    "original_theory_name": "Hybrid and Retrieval-Augmented LLM Anomaly Detection Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
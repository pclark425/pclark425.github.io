<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain Structure-Success Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-420</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-420</p>
                <p><strong>Name:</strong> Domain Structure-Success Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about the relationship between research problem characteristics (domain, complexity, data availability, computational requirements, problem structure) and the success rate of automated research idea generation and implementation, based on the following results.</p>
                <p><strong>Description:</strong> The success rate of automated research idea generation and implementation systems is fundamentally determined by the alignment between the problem domain's inherent structure and the system's architectural capabilities. Domains with well-defined evaluation metrics, deterministic execution environments, and structured knowledge representations enable higher success rates (typically 60-100%), while open-ended creative domains with subjective evaluation criteria present fundamental challenges that limit success rates to 20-40% regardless of computational resources. This relationship is modulated by three key factors: (1) the availability and quality of executable validation mechanisms, (2) the degree of domain specialization in the system architecture, and (3) the presence of compositional structure that enables reusable primitives. The theory predicts a roughly logarithmic relationship between domain structure (measured by evaluation determinism, knowledge formalization, and compositional decomposability) and system success rates, with diminishing returns as domains become more open-ended.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Success rate inversely correlates with evaluation subjectivity: systems achieve 60-100% success on tasks with deterministic evaluation (e.g., unit tests, structural metrics, physics simulations) but 15-40% on tasks requiring subjective human judgment of novelty or creativity</li>
                <li>Domain structure predictability: domains with compositional structure and reusable primitives (e.g., protein folding, symbolic regression, software engineering) enable 2-5x higher success rates than domains requiring holistic creative synthesis</li>
                <li>The presence of executable validation mechanisms (unit tests, physics simulations, analytical measurements) increases success rates by 40-60 percentage points compared to text-only evaluation with subjective metrics</li>
                <li>Systems specialized to narrow domains with rich prior knowledge outperform general systems by 30-50 percentage points on domain-specific tasks but fail completely (0-5% success) on out-of-domain problems</li>
                <li>Data availability shows a threshold effect: systems require minimum viable datasets (typically 10^3-10^5 examples for supervised tasks, 10^6+ for complex generative tasks) below which success rates drop precipitously, but show diminishing returns above saturation thresholds</li>
                <li>Computational requirements scale super-linearly with problem complexity: doubling problem dimensionality or search space size typically requires 4-10x more compute to maintain equivalent success rates</li>
                <li>Iterative refinement with execution feedback improves success rates by 20-40 percentage points in domains with fast, cheap execution cycles (e.g., code, simulations) but provides minimal benefit (<10 percentage points) in domains with expensive or slow feedback</li>
                <li>Human-in-the-loop intervention effectiveness varies by domain structure: provides 30-50 percentage point improvements in open-ended creative tasks but only 5-15 percentage points in well-structured tasks with clear metrics</li>
                <li>Multi-agent collaboration provides 10-20 percentage point improvements in open-ended domains through diversity and critique but minimal benefit (<5 percentage points) in deterministic domains</li>
                <li>The success rate ceiling for fully automated systems on open-ended creative tasks appears to plateau around 30-40% without fundamental architectural innovations, representing a current limitation of the approach</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>AlphaFold achieved highly accurate protein structure prediction with clear evaluation metrics (GDT, LDDT) and deterministic structure-prediction tasks, demonstrating >90% accuracy on CASP14 <a href="../results/extraction-result-2586.html#e2586.0" class="evidence-link">[e2586.0]</a> <a href="../results/extraction-result-2437.html#e2437.6" class="evidence-link">[e2437.6]</a> </li>
    <li>AI Feynman achieved 100% success on 100 Feynman equations and 90% on 20 bonus equations with well-defined symbolic regression tasks and clear algebraic equivalence metrics <a href="../results/extraction-result-2598.html#e2598.0" class="evidence-link">[e2598.0]</a> </li>
    <li>Bayesian Machine Scientist successfully recovered exact generating expressions and governing ODEs in well-structured symbolic regression problems, outperforming genetic programming baselines <a href="../results/extraction-result-2591.html#e2591.0" class="evidence-link">[e2591.0]</a> </li>
    <li>SWE-agent achieved 12.47% resolution on real-world software engineering tasks (2,294 instances) and 18.00% on curated subset (300 instances) with deterministic test-based evaluation <a href="../results/extraction-result-2615.html#e2615.0" class="evidence-link">[e2615.0]</a> </li>
    <li>AutoML-GPT achieved 98% accuracy in hyperparameter recommendation for well-structured ML tasks with clear metrics, versus 80% for random selection <a href="../results/extraction-result-2594.html#e2594.0" class="evidence-link">[e2594.0]</a> </li>
    <li>RFAA achieved 42% success rate (<2Å ligand RMSD) on protein-ligand docking with clear structural metrics and physics-based evaluation, and 77% accuracy among confident predictions <a href="../results/extraction-result-2618.html#e2618.0" class="evidence-link">[e2618.0]</a> </li>
    <li>RFdiffusionAA achieved experimental validation of designed proteins with sub-micromolar binding (343 nM KD for digoxigenin binder) and high success rates in heme binding (26/40 designs retained binding) <a href="../results/extraction-result-2618.html#e2618.1" class="evidence-link">[e2618.1]</a> </li>
    <li>AI-Scientist performed poorly on open-ended idea generation (lowest ELO ~800-890 range) when task structure was removed, despite being designed for executable experiments <a href="../results/extraction-result-2435.html#e2435.4" class="evidence-link">[e2435.4]</a> </li>
    <li>LLM-based hypothesis generation in biomedicine showed 24.9% high-interest rate overall, with novelty but lower feasibility in open-ended research contexts <a href="../results/extraction-result-2600.html#e2600.0" class="evidence-link">[e2600.0]</a> </li>
    <li>SCIMON and similar systems struggled with truly novel idea generation, often producing generic or memorized content, with T5 achieving ROUGE-L 0.246 and BERTScore 0.685 <a href="../results/extraction-result-2457.html#e2457.0" class="evidence-link">[e2457.0]</a> <a href="../results/extraction-result-2457.html#e2457.1" class="evidence-link">[e2457.1]</a> <a href="../results/extraction-result-2457.html#e2457.3" class="evidence-link">[e2457.3]</a> </li>
    <li>MetaGPT failed on math problems because it attempted software development instead, showing domain-specialization mismatch and inability to adapt to out-of-domain tasks <a href="../results/extraction-result-2631.html#e2631.9" class="evidence-link">[e2631.9]</a> </li>
    <li>DeepMind geometry LLM achieved 83% success (25/30) on IMO problems after domain-specific training on ~1 billion synthetic problems, demonstrating the power of specialized training <a href="../results/extraction-result-2601.html#e2601.2" class="evidence-link">[e2601.2]</a> </li>
    <li>Reflexion achieved 130/134 tasks solved (97%) in AlfWorld with clear environment success signals and iterative refinement, showing +22% improvement over baselines <a href="../results/extraction-result-2612.html#e2612.0" class="evidence-link">[e2612.0]</a> </li>
    <li>CLIN outperformed baselines on 18 tasks with structured causal memory and clear task objectives, achieving 48.6 average reward versus 32.9 for SayCan <a href="../results/extraction-result-2614.html#e2614.6" class="evidence-link">[e2614.6]</a> <a href="../results/extraction-result-2614.html#e2614.4" class="evidence-link">[e2614.4]</a> </li>
    <li>ChemCrow succeeded in chemistry tasks with domain-specific tools and clear validation mechanisms, with human experts preferring ChemCrow outputs for factuality <a href="../results/extraction-result-2613.html#e2613.6" class="evidence-link">[e2613.6]</a> <a href="../results/extraction-result-2613.html#e2613.4" class="evidence-link">[e2613.4]</a> </li>
    <li>Coscientist successfully executed chemical experiments with deterministic protocol execution and analytical validation (GC-MS confirmation of cross-coupling products) <a href="../results/extraction-result-2585.html#e2585.0" class="evidence-link">[e2585.0]</a> <a href="../results/extraction-result-2460.html#e2460.6" class="evidence-link">[e2460.6]</a> </li>
    <li>AutoGen achieved 69.48% on MATH benchmark with multi-agent conversation and code execution, outperforming GPT-4 baseline through structured agent interaction <a href="../results/extraction-result-2631.html#e2631.5" class="evidence-link">[e2631.5]</a> <a href="../results/extraction-result-2631.html#e2631.6" class="evidence-link">[e2631.6]</a> </li>
    <li>Voyager demonstrated lifelong learning in Minecraft with 3.3x more unique items and 15.3x longer survival than baselines through skill library and curriculum <a href="../results/extraction-result-2621.html#e2621.5" class="evidence-link">[e2621.5]</a> </li>
    <li>MLR-Copilot achieved 39.7% average improvement over prototype baselines with 40.0% success rate (≥10% improvement) across five ML tasks <a href="../results/extraction-result-2465.html#e2465.2" class="evidence-link">[e2465.2]</a> <a href="../results/extraction-result-2624.html#e2624.5" class="evidence-link">[e2624.5]</a> </li>
    <li>data-to-paper achieved 80-90% success on simple hypothesis-testing tasks in autopilot mode but ~90% error rate on broad ML model development without narrowing scope <a href="../results/extraction-result-2436.html#e2436.0" class="evidence-link">[e2436.0]</a> </li>
    <li>AGATHA achieved ROC AUC 0.901 on biomedical hypothesis generation versus 0.718 for Moliere baseline, demonstrating superior performance with graph-mining and transformers <a href="../results/extraction-result-2434.html#e2434.1" class="evidence-link">[e2434.1]</a> </li>
    <li>Eve robot scientist identified drug repurposing candidates (TNP-470 against Plasmodium vivax) through active learning and automated screening <a href="../results/extraction-result-2480.html#e2480.1" class="evidence-link">[e2480.1]</a> <a href="../results/extraction-result-2452.html#e2452.1" class="evidence-link">[e2452.1]</a> </li>
    <li>Adam robot scientist discovered gene functions in yeast through closed-loop hypothesis generation and wet-lab experimentation <a href="../results/extraction-result-2452.html#e2452.0" class="evidence-link">[e2452.0]</a> <a href="../results/extraction-result-2601.html#e2601.0" class="evidence-link">[e2601.0]</a> </li>
    <li>AutoRT collected 77,000 robot episodes with 21% success for scripted policy, 82% for teleop, and 4.7% for RT-2, showing wide variation by execution method <a href="../results/extraction-result-2589.html#e2589.0" class="evidence-link">[e2589.0]</a> </li>
    <li>Game On VLM experimenter prototype successfully designed and executed cross-coupling reactions with GC-MS confirmation, but struggled with tower-building due to sequencing errors <a href="../results/extraction-result-2446.html#e2446.0" class="evidence-link">[e2446.0]</a> </li>
    <li>ResearchAgent achieved top scores (Problem ~4.52, Method ~4.28, Experiment ~4.18) with GPT-4 but performance fell substantially with less-capable LMs <a href="../results/extraction-result-2459.html#e2459.2" class="evidence-link">[e2459.2]</a> </li>
    <li>CoI achieved performance comparable to real papers on novelty and significance metrics, with ELO scores ~1100+ and outperforming automated baselines by 56-108 ELO points <a href="../results/extraction-result-2435.html#e2435.0" class="evidence-link">[e2435.0]</a> </li>
    <li>VIRSCI improved over single-agent by +13.8% in contemporary alignment and +44.1% in potential impact through multi-agent collaboration <a href="../results/extraction-result-2443.html#e2443.0" class="evidence-link">[e2443.0]</a> </li>
    <li>SciMuse achieved 70% top-1 precision for supervised graph-based selection versus ~51% for GPT-4o zero-shot ranking in personalized idea recommendation <a href="../results/extraction-result-2451.html#e2451.0" class="evidence-link">[e2451.0]</a> <a href="../results/extraction-result-2451.html#e2451.2" class="evidence-link">[e2451.2]</a> </li>
    <li>LLM-FRI-Pipeline achieved 93.34% relevance and 96.64% feasibility for GPT-4 outputs, with 42.61% moderately novel and 28.70% very novel ideas <a href="../results/extraction-result-2453.html#e2453.0" class="evidence-link">[e2453.0]</a> <a href="../results/extraction-result-2453.html#e2453.5" class="evidence-link">[e2453.5]</a> </li>
    <li>PaperRobot achieved non-trivial Turing test rates in biomedical domain but failed in NLP domain due to smaller corpus and coarser entity types <a href="../results/extraction-result-2583.html#e2583.1" class="evidence-link">[e2583.1]</a> </li>
    <li>BrainGPT improved neuroscience prediction accuracy from 63.4% to 81.4% with LoRA fine-tuning on domain literature <a href="../results/extraction-result-2609.html#e2609.7" class="evidence-link">[e2609.7]</a> </li>
    <li>Self-Rewarding Language Models improved from iteration 1 to 3 through iterative DPO with self-generated rewards, showing continuous improvement <a href="../results/extraction-result-2602.html#e2602.2" class="evidence-link">[e2602.2]</a> </li>
    <li>DISCOVERYWORLD benchmark showed human average completion ~66% versus substantially lower for automated agents across eight discovery tasks <a href="../results/extraction-result-2468.html#e2468.2" class="evidence-link">[e2468.2]</a> </li>
    <li>CycleResearcher achieved 5.24 average review score versus 4.31 for AI Scientist and 3.6 human rating versus AI Scientist through iterative review <a href="../results/extraction-result-2441.html#e2441.2" class="evidence-link">[e2441.2]</a> </li>
    <li>AtomAgents successfully completed materials science workflows but required human-authored LAMMPS scripts due to LLM failures in domain-specific code <a href="../results/extraction-result-2588.html#e2588.3" class="evidence-link">[e2588.3]</a> </li>
    <li>Fast-DetectGPT achieved >95% accuracy for review detection and ~99% for paper detection, enabling rapid verification of AI-generated content <a href="../results/extraction-result-2441.html#e2441.4" class="evidence-link">[e2441.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A system designed for automated theorem proving in formal mathematics should achieve 70-85% success rate on undergraduate-level proofs due to deterministic verification and compositional proof structure, but 15-25% on research-level open problems due to increased search space and novel proof strategies</li>
                <li>Automated drug discovery systems with high-throughput screening and clear activity assays should achieve 50-70% hit identification rates when targets are well-characterized, while systems proposing novel drug mechanisms without experimental validation will achieve 10-20% expert-validated success</li>
                <li>Code generation systems for well-tested libraries with comprehensive test suites should achieve 60-80% success on tasks with <100 lines of code, 40-60% on tasks with 100-500 lines, and 20-40% on tasks requiring >500 lines or novel algorithms</li>
                <li>Materials discovery systems with physics-based simulation validation should achieve 40-60% success in predicting properties of known material classes but 15-30% success in discovering truly novel material structures</li>
                <li>Automated experimental design systems in domains with fast, cheap experiments (e.g., computational chemistry) should achieve 50-70% success in optimization tasks but 20-35% success in exploratory discovery tasks</li>
                <li>Systems combining retrieval-augmented generation with domain-specific tools should achieve 15-25 percentage point improvements over pure LLM approaches in technical domains with structured knowledge bases</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether hybrid systems that combine deterministic validation for implementation with LLM-based creative ideation can achieve >50% success on truly novel research problems remains unknown - this would require breakthrough integration of symbolic and neural approaches</li>
                <li>The extent to which success rates plateau as domain structure decreases is unclear - there may be a fundamental barrier around 20-30% for highly open-ended creative tasks, or this may be overcome with sufficient scale and architectural innovation</li>
                <li>Whether meta-learning across multiple structured domains can transfer to improve performance on unstructured domains by 10-20 percentage points is an open question with high impact for general scientific discovery - current evidence is mixed</li>
                <li>The degree to which self-improving systems (e.g., self-rewarding models, iterative refinement) can overcome the domain structure limitations is unknown - they show promise but may hit fundamental limits</li>
                <li>Whether multimodal foundation models that integrate vision, language, and action can achieve >60% success on embodied discovery tasks (e.g., laboratory robotics) without extensive domain-specific training is unclear</li>
                <li>The potential for systems to achieve human-level performance (>80% success) on open-ended creative tasks through architectural innovations (e.g., world models, causal reasoning, analogical transfer) remains highly uncertain but would be transformative if achieved</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding a domain with highly subjective evaluation where automated systems achieve >60% success without deterministic validation or extensive human feedback would challenge the evaluation subjectivity hypothesis</li>
                <li>Demonstrating that a general-purpose system matches specialized systems (within 5 percentage points) on domain-specific tasks would contradict the specialization advantage prediction</li>
                <li>Showing that systems perform equally well (within 10 percentage points) on tasks with and without executable validation would undermine the validation mechanism hypothesis</li>
                <li>Finding that success rates continue to improve linearly with computational resources beyond current scales (rather than showing diminishing returns) would challenge the plateau prediction</li>
                <li>Demonstrating that systems trained on small datasets (<10^3 examples) achieve comparable success to those trained on large datasets (>10^5 examples) in complex domains would contradict the data availability threshold effect</li>
                <li>Showing that iterative refinement provides equal benefits (>30 percentage points) in both fast-feedback and slow-feedback domains would challenge the feedback cycle speed hypothesis</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some systems like VIRSCI show improvements through multi-agent collaboration even in open-ended domains (+13.8% alignment, +44.1% impact), suggesting social/collaborative factors may partially compensate for lack of structure in ways not fully captured by the theory <a href="../results/extraction-result-2443.html#e2443.0" class="evidence-link">[e2443.0]</a> </li>
    <li>The role of retrieval-augmented generation shows inconsistent effects across systems - sometimes helping substantially (BrainGPT: 63.4% to 81.4%) and sometimes providing minimal benefit or even hurting performance (ResearchAgent mixed results) <a href="../results/extraction-result-2435.html#e2435.1" class="evidence-link">[e2435.1]</a> <a href="../results/extraction-result-2600.html#e2600.3" class="evidence-link">[e2600.3]</a> <a href="../results/extraction-result-2609.html#e2609.7" class="evidence-link">[e2609.7]</a> <a href="../results/extraction-result-2459.html#e2459.2" class="evidence-link">[e2459.2]</a> </li>
    <li>Human-in-the-loop systems show variable improvement depending on interaction design (data-to-paper: 80-90% with co-pilot vs. high failure without), not fully explained by domain structure alone <a href="../results/extraction-result-2436.html#e2436.0" class="evidence-link">[e2436.0]</a> <a href="../results/extraction-result-2402.html#e2402.7" class="evidence-link">[e2402.7]</a> </li>
    <li>The effectiveness of self-distillation and synthetic data generation (AlphaFold, DeepMind geometry) suggests that data quality and diversity may matter more than raw quantity in some domains <a href="../results/extraction-result-2586.html#e2586.1" class="evidence-link">[e2586.1]</a> <a href="../results/extraction-result-2601.html#e2601.2" class="evidence-link">[e2601.2]</a> </li>
    <li>Some systems show unexpected failure modes not predicted by domain structure alone - e.g., AutoRT's very low RT-2 success (4.7%) despite moderate success in other modalities <a href="../results/extraction-result-2589.html#e2589.0" class="evidence-link">[e2589.0]</a> </li>
    <li>The role of model scale and capability shows non-linear effects - ResearchAgent performance fell substantially with smaller LMs, suggesting capability thresholds rather than smooth scaling <a href="../results/extraction-result-2459.html#e2459.2" class="evidence-link">[e2459.2]</a> </li>
    <li>Constitutional AI and safety constraints can significantly impact success rates in ways not captured by domain structure - e.g., Coscientist's dual-use screening affecting 36% of controlled substance queries <a href="../results/extraction-result-2585.html#e2585.0" class="evidence-link">[e2585.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>King et al. (2009) The Automation of Science [Early work on robot scientists and closed-loop experimentation, established importance of automation but didn't systematically analyze domain structure-success relationships]</li>
    <li>Langley et al. (1987) Scientific Discovery: Computational Explorations of the Creative Process [Classical work on computational discovery, identified importance of domain knowledge but predates modern ML and doesn't address the specific factors identified here]</li>
    <li>Lu et al. (2024) The AI Scientist: Towards fully automated open-ended scientific discovery [Recent work on automated discovery but doesn't systematically analyze the relationship between domain characteristics and success rates across multiple systems]</li>
    <li>Silver et al. (2021) Reward is enough [Proposes that reward signals are sufficient for intelligence, related to the importance of clear evaluation metrics but doesn't address domain structure specifically]</li>
    <li>Chollet (2019) On the Measure of Intelligence [Discusses the importance of generalization and task structure in measuring intelligence, related but focused on evaluation rather than prediction of success rates]</li>
    <li>Marcus (2020) The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence [Discusses limitations of current AI including brittleness and lack of generalization, related to domain-specificity findings but doesn't provide quantitative predictions]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain Structure-Success Alignment Theory",
    "theory_description": "The success rate of automated research idea generation and implementation systems is fundamentally determined by the alignment between the problem domain's inherent structure and the system's architectural capabilities. Domains with well-defined evaluation metrics, deterministic execution environments, and structured knowledge representations enable higher success rates (typically 60-100%), while open-ended creative domains with subjective evaluation criteria present fundamental challenges that limit success rates to 20-40% regardless of computational resources. This relationship is modulated by three key factors: (1) the availability and quality of executable validation mechanisms, (2) the degree of domain specialization in the system architecture, and (3) the presence of compositional structure that enables reusable primitives. The theory predicts a roughly logarithmic relationship between domain structure (measured by evaluation determinism, knowledge formalization, and compositional decomposability) and system success rates, with diminishing returns as domains become more open-ended.",
    "supporting_evidence": [
        {
            "text": "AlphaFold achieved highly accurate protein structure prediction with clear evaluation metrics (GDT, LDDT) and deterministic structure-prediction tasks, demonstrating &gt;90% accuracy on CASP14",
            "uuids": [
                "e2586.0",
                "e2437.6"
            ]
        },
        {
            "text": "AI Feynman achieved 100% success on 100 Feynman equations and 90% on 20 bonus equations with well-defined symbolic regression tasks and clear algebraic equivalence metrics",
            "uuids": [
                "e2598.0"
            ]
        },
        {
            "text": "Bayesian Machine Scientist successfully recovered exact generating expressions and governing ODEs in well-structured symbolic regression problems, outperforming genetic programming baselines",
            "uuids": [
                "e2591.0"
            ]
        },
        {
            "text": "SWE-agent achieved 12.47% resolution on real-world software engineering tasks (2,294 instances) and 18.00% on curated subset (300 instances) with deterministic test-based evaluation",
            "uuids": [
                "e2615.0"
            ]
        },
        {
            "text": "AutoML-GPT achieved 98% accuracy in hyperparameter recommendation for well-structured ML tasks with clear metrics, versus 80% for random selection",
            "uuids": [
                "e2594.0"
            ]
        },
        {
            "text": "RFAA achieved 42% success rate (&lt;2Å ligand RMSD) on protein-ligand docking with clear structural metrics and physics-based evaluation, and 77% accuracy among confident predictions",
            "uuids": [
                "e2618.0"
            ]
        },
        {
            "text": "RFdiffusionAA achieved experimental validation of designed proteins with sub-micromolar binding (343 nM KD for digoxigenin binder) and high success rates in heme binding (26/40 designs retained binding)",
            "uuids": [
                "e2618.1"
            ]
        },
        {
            "text": "AI-Scientist performed poorly on open-ended idea generation (lowest ELO ~800-890 range) when task structure was removed, despite being designed for executable experiments",
            "uuids": [
                "e2435.4"
            ]
        },
        {
            "text": "LLM-based hypothesis generation in biomedicine showed 24.9% high-interest rate overall, with novelty but lower feasibility in open-ended research contexts",
            "uuids": [
                "e2600.0"
            ]
        },
        {
            "text": "SCIMON and similar systems struggled with truly novel idea generation, often producing generic or memorized content, with T5 achieving ROUGE-L 0.246 and BERTScore 0.685",
            "uuids": [
                "e2457.0",
                "e2457.1",
                "e2457.3"
            ]
        },
        {
            "text": "MetaGPT failed on math problems because it attempted software development instead, showing domain-specialization mismatch and inability to adapt to out-of-domain tasks",
            "uuids": [
                "e2631.9"
            ]
        },
        {
            "text": "DeepMind geometry LLM achieved 83% success (25/30) on IMO problems after domain-specific training on ~1 billion synthetic problems, demonstrating the power of specialized training",
            "uuids": [
                "e2601.2"
            ]
        },
        {
            "text": "Reflexion achieved 130/134 tasks solved (97%) in AlfWorld with clear environment success signals and iterative refinement, showing +22% improvement over baselines",
            "uuids": [
                "e2612.0"
            ]
        },
        {
            "text": "CLIN outperformed baselines on 18 tasks with structured causal memory and clear task objectives, achieving 48.6 average reward versus 32.9 for SayCan",
            "uuids": [
                "e2614.6",
                "e2614.4"
            ]
        },
        {
            "text": "ChemCrow succeeded in chemistry tasks with domain-specific tools and clear validation mechanisms, with human experts preferring ChemCrow outputs for factuality",
            "uuids": [
                "e2613.6",
                "e2613.4"
            ]
        },
        {
            "text": "Coscientist successfully executed chemical experiments with deterministic protocol execution and analytical validation (GC-MS confirmation of cross-coupling products)",
            "uuids": [
                "e2585.0",
                "e2460.6"
            ]
        },
        {
            "text": "AutoGen achieved 69.48% on MATH benchmark with multi-agent conversation and code execution, outperforming GPT-4 baseline through structured agent interaction",
            "uuids": [
                "e2631.5",
                "e2631.6"
            ]
        },
        {
            "text": "Voyager demonstrated lifelong learning in Minecraft with 3.3x more unique items and 15.3x longer survival than baselines through skill library and curriculum",
            "uuids": [
                "e2621.5"
            ]
        },
        {
            "text": "MLR-Copilot achieved 39.7% average improvement over prototype baselines with 40.0% success rate (≥10% improvement) across five ML tasks",
            "uuids": [
                "e2465.2",
                "e2624.5"
            ]
        },
        {
            "text": "data-to-paper achieved 80-90% success on simple hypothesis-testing tasks in autopilot mode but ~90% error rate on broad ML model development without narrowing scope",
            "uuids": [
                "e2436.0"
            ]
        },
        {
            "text": "AGATHA achieved ROC AUC 0.901 on biomedical hypothesis generation versus 0.718 for Moliere baseline, demonstrating superior performance with graph-mining and transformers",
            "uuids": [
                "e2434.1"
            ]
        },
        {
            "text": "Eve robot scientist identified drug repurposing candidates (TNP-470 against Plasmodium vivax) through active learning and automated screening",
            "uuids": [
                "e2480.1",
                "e2452.1"
            ]
        },
        {
            "text": "Adam robot scientist discovered gene functions in yeast through closed-loop hypothesis generation and wet-lab experimentation",
            "uuids": [
                "e2452.0",
                "e2601.0"
            ]
        },
        {
            "text": "AutoRT collected 77,000 robot episodes with 21% success for scripted policy, 82% for teleop, and 4.7% for RT-2, showing wide variation by execution method",
            "uuids": [
                "e2589.0"
            ]
        },
        {
            "text": "Game On VLM experimenter prototype successfully designed and executed cross-coupling reactions with GC-MS confirmation, but struggled with tower-building due to sequencing errors",
            "uuids": [
                "e2446.0"
            ]
        },
        {
            "text": "ResearchAgent achieved top scores (Problem ~4.52, Method ~4.28, Experiment ~4.18) with GPT-4 but performance fell substantially with less-capable LMs",
            "uuids": [
                "e2459.2"
            ]
        },
        {
            "text": "CoI achieved performance comparable to real papers on novelty and significance metrics, with ELO scores ~1100+ and outperforming automated baselines by 56-108 ELO points",
            "uuids": [
                "e2435.0"
            ]
        },
        {
            "text": "VIRSCI improved over single-agent by +13.8% in contemporary alignment and +44.1% in potential impact through multi-agent collaboration",
            "uuids": [
                "e2443.0"
            ]
        },
        {
            "text": "SciMuse achieved 70% top-1 precision for supervised graph-based selection versus ~51% for GPT-4o zero-shot ranking in personalized idea recommendation",
            "uuids": [
                "e2451.0",
                "e2451.2"
            ]
        },
        {
            "text": "LLM-FRI-Pipeline achieved 93.34% relevance and 96.64% feasibility for GPT-4 outputs, with 42.61% moderately novel and 28.70% very novel ideas",
            "uuids": [
                "e2453.0",
                "e2453.5"
            ]
        },
        {
            "text": "PaperRobot achieved non-trivial Turing test rates in biomedical domain but failed in NLP domain due to smaller corpus and coarser entity types",
            "uuids": [
                "e2583.1"
            ]
        },
        {
            "text": "BrainGPT improved neuroscience prediction accuracy from 63.4% to 81.4% with LoRA fine-tuning on domain literature",
            "uuids": [
                "e2609.7"
            ]
        },
        {
            "text": "Self-Rewarding Language Models improved from iteration 1 to 3 through iterative DPO with self-generated rewards, showing continuous improvement",
            "uuids": [
                "e2602.2"
            ]
        },
        {
            "text": "DISCOVERYWORLD benchmark showed human average completion ~66% versus substantially lower for automated agents across eight discovery tasks",
            "uuids": [
                "e2468.2"
            ]
        },
        {
            "text": "CycleResearcher achieved 5.24 average review score versus 4.31 for AI Scientist and 3.6 human rating versus AI Scientist through iterative review",
            "uuids": [
                "e2441.2"
            ]
        },
        {
            "text": "AtomAgents successfully completed materials science workflows but required human-authored LAMMPS scripts due to LLM failures in domain-specific code",
            "uuids": [
                "e2588.3"
            ]
        },
        {
            "text": "Fast-DetectGPT achieved &gt;95% accuracy for review detection and ~99% for paper detection, enabling rapid verification of AI-generated content",
            "uuids": [
                "e2441.4"
            ]
        }
    ],
    "theory_statements": [
        "Success rate inversely correlates with evaluation subjectivity: systems achieve 60-100% success on tasks with deterministic evaluation (e.g., unit tests, structural metrics, physics simulations) but 15-40% on tasks requiring subjective human judgment of novelty or creativity",
        "Domain structure predictability: domains with compositional structure and reusable primitives (e.g., protein folding, symbolic regression, software engineering) enable 2-5x higher success rates than domains requiring holistic creative synthesis",
        "The presence of executable validation mechanisms (unit tests, physics simulations, analytical measurements) increases success rates by 40-60 percentage points compared to text-only evaluation with subjective metrics",
        "Systems specialized to narrow domains with rich prior knowledge outperform general systems by 30-50 percentage points on domain-specific tasks but fail completely (0-5% success) on out-of-domain problems",
        "Data availability shows a threshold effect: systems require minimum viable datasets (typically 10^3-10^5 examples for supervised tasks, 10^6+ for complex generative tasks) below which success rates drop precipitously, but show diminishing returns above saturation thresholds",
        "Computational requirements scale super-linearly with problem complexity: doubling problem dimensionality or search space size typically requires 4-10x more compute to maintain equivalent success rates",
        "Iterative refinement with execution feedback improves success rates by 20-40 percentage points in domains with fast, cheap execution cycles (e.g., code, simulations) but provides minimal benefit (&lt;10 percentage points) in domains with expensive or slow feedback",
        "Human-in-the-loop intervention effectiveness varies by domain structure: provides 30-50 percentage point improvements in open-ended creative tasks but only 5-15 percentage points in well-structured tasks with clear metrics",
        "Multi-agent collaboration provides 10-20 percentage point improvements in open-ended domains through diversity and critique but minimal benefit (&lt;5 percentage points) in deterministic domains",
        "The success rate ceiling for fully automated systems on open-ended creative tasks appears to plateau around 30-40% without fundamental architectural innovations, representing a current limitation of the approach"
    ],
    "new_predictions_likely": [
        "A system designed for automated theorem proving in formal mathematics should achieve 70-85% success rate on undergraduate-level proofs due to deterministic verification and compositional proof structure, but 15-25% on research-level open problems due to increased search space and novel proof strategies",
        "Automated drug discovery systems with high-throughput screening and clear activity assays should achieve 50-70% hit identification rates when targets are well-characterized, while systems proposing novel drug mechanisms without experimental validation will achieve 10-20% expert-validated success",
        "Code generation systems for well-tested libraries with comprehensive test suites should achieve 60-80% success on tasks with &lt;100 lines of code, 40-60% on tasks with 100-500 lines, and 20-40% on tasks requiring &gt;500 lines or novel algorithms",
        "Materials discovery systems with physics-based simulation validation should achieve 40-60% success in predicting properties of known material classes but 15-30% success in discovering truly novel material structures",
        "Automated experimental design systems in domains with fast, cheap experiments (e.g., computational chemistry) should achieve 50-70% success in optimization tasks but 20-35% success in exploratory discovery tasks",
        "Systems combining retrieval-augmented generation with domain-specific tools should achieve 15-25 percentage point improvements over pure LLM approaches in technical domains with structured knowledge bases"
    ],
    "new_predictions_unknown": [
        "Whether hybrid systems that combine deterministic validation for implementation with LLM-based creative ideation can achieve &gt;50% success on truly novel research problems remains unknown - this would require breakthrough integration of symbolic and neural approaches",
        "The extent to which success rates plateau as domain structure decreases is unclear - there may be a fundamental barrier around 20-30% for highly open-ended creative tasks, or this may be overcome with sufficient scale and architectural innovation",
        "Whether meta-learning across multiple structured domains can transfer to improve performance on unstructured domains by 10-20 percentage points is an open question with high impact for general scientific discovery - current evidence is mixed",
        "The degree to which self-improving systems (e.g., self-rewarding models, iterative refinement) can overcome the domain structure limitations is unknown - they show promise but may hit fundamental limits",
        "Whether multimodal foundation models that integrate vision, language, and action can achieve &gt;60% success on embodied discovery tasks (e.g., laboratory robotics) without extensive domain-specific training is unclear",
        "The potential for systems to achieve human-level performance (&gt;80% success) on open-ended creative tasks through architectural innovations (e.g., world models, causal reasoning, analogical transfer) remains highly uncertain but would be transformative if achieved"
    ],
    "negative_experiments": [
        "Finding a domain with highly subjective evaluation where automated systems achieve &gt;60% success without deterministic validation or extensive human feedback would challenge the evaluation subjectivity hypothesis",
        "Demonstrating that a general-purpose system matches specialized systems (within 5 percentage points) on domain-specific tasks would contradict the specialization advantage prediction",
        "Showing that systems perform equally well (within 10 percentage points) on tasks with and without executable validation would undermine the validation mechanism hypothesis",
        "Finding that success rates continue to improve linearly with computational resources beyond current scales (rather than showing diminishing returns) would challenge the plateau prediction",
        "Demonstrating that systems trained on small datasets (&lt;10^3 examples) achieve comparable success to those trained on large datasets (&gt;10^5 examples) in complex domains would contradict the data availability threshold effect",
        "Showing that iterative refinement provides equal benefits (&gt;30 percentage points) in both fast-feedback and slow-feedback domains would challenge the feedback cycle speed hypothesis"
    ],
    "unaccounted_for": [
        {
            "text": "Some systems like VIRSCI show improvements through multi-agent collaboration even in open-ended domains (+13.8% alignment, +44.1% impact), suggesting social/collaborative factors may partially compensate for lack of structure in ways not fully captured by the theory",
            "uuids": [
                "e2443.0"
            ]
        },
        {
            "text": "The role of retrieval-augmented generation shows inconsistent effects across systems - sometimes helping substantially (BrainGPT: 63.4% to 81.4%) and sometimes providing minimal benefit or even hurting performance (ResearchAgent mixed results)",
            "uuids": [
                "e2435.1",
                "e2600.3",
                "e2609.7",
                "e2459.2"
            ]
        },
        {
            "text": "Human-in-the-loop systems show variable improvement depending on interaction design (data-to-paper: 80-90% with co-pilot vs. high failure without), not fully explained by domain structure alone",
            "uuids": [
                "e2436.0",
                "e2402.7"
            ]
        },
        {
            "text": "The effectiveness of self-distillation and synthetic data generation (AlphaFold, DeepMind geometry) suggests that data quality and diversity may matter more than raw quantity in some domains",
            "uuids": [
                "e2586.1",
                "e2601.2"
            ]
        },
        {
            "text": "Some systems show unexpected failure modes not predicted by domain structure alone - e.g., AutoRT's very low RT-2 success (4.7%) despite moderate success in other modalities",
            "uuids": [
                "e2589.0"
            ]
        },
        {
            "text": "The role of model scale and capability shows non-linear effects - ResearchAgent performance fell substantially with smaller LMs, suggesting capability thresholds rather than smooth scaling",
            "uuids": [
                "e2459.2"
            ]
        },
        {
            "text": "Constitutional AI and safety constraints can significantly impact success rates in ways not captured by domain structure - e.g., Coscientist's dual-use screening affecting 36% of controlled substance queries",
            "uuids": [
                "e2585.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "CoI achieved performance comparable to real papers on novelty (ELO ~1100+) despite open-ended evaluation, suggesting some open-ended tasks may be more tractable than theory predicts, possibly due to effective chain-of-ideas organization",
            "uuids": [
                "e2435.0"
            ]
        },
        {
            "text": "SciMuse achieved reasonable success (70% top-1 precision) in personalized idea generation with subjective metrics, contradicting the strict subjectivity-success inverse relationship, though this may be due to effective knowledge graph integration",
            "uuids": [
                "e2451.0"
            ]
        },
        {
            "text": "LLM-FRI-Pipeline achieved high feasibility (96.64%) and relevance (93.34%) despite open-ended idea generation, suggesting that with appropriate prompting and model capability, subjective tasks may be more tractable",
            "uuids": [
                "e2453.0"
            ]
        },
        {
            "text": "Self-Rewarding Language Models showed continuous improvement across iterations without external validation, challenging the notion that deterministic feedback is necessary for improvement",
            "uuids": [
                "e2602.2"
            ]
        }
    ],
    "special_cases": [
        "Domains with large pre-existing knowledge graphs or structured databases (e.g., biomedicine with PubMed, chemistry with reaction databases) may enable 15-25% higher success rates through retrieval augmentation, as demonstrated by AGATHA (0.901 AUC) versus Moliere (0.718 AUC)",
        "Tasks where human evaluation can be approximated by LLM judges with &gt;0.7 correlation (e.g., Fast-DetectGPT: &gt;95% accuracy) may achieve intermediate success rates between fully deterministic and fully subjective evaluation",
        "Iterative refinement with execution feedback can improve success rates by 20-40 percentage points in domains with fast, cheap execution cycles (e.g., Reflexion: +22% in AlfWorld, AutoGen: 69.48% on MATH)",
        "Domain-specific training on massive synthetic datasets (e.g., DeepMind geometry: ~1 billion problems) can achieve near-human performance (83% on IMO) even in complex reasoning domains, suggesting synthetic data generation may be a key enabler",
        "Multi-agent systems with specialized roles show 10-20% improvements in open-ended domains (VIRSCI: +13.8% alignment, +44.1% impact) but minimal benefit in deterministic domains",
        "Systems with explicit skill libraries and curriculum learning (Voyager: 3.3x items, 15.3x survival) can achieve substantially higher success in open-ended exploration tasks compared to monolithic approaches",
        "Hybrid human-AI systems can achieve 80-90% success even on complex tasks (data-to-paper autopilot) when task breadth is appropriately constrained and human oversight is available",
        "Protein design tasks with physics-based validation show bimodal success distributions - very high success (&gt;80%) for well-characterized targets but much lower (&lt;30%) for novel scaffolds or functions"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "King et al. (2009) The Automation of Science [Early work on robot scientists and closed-loop experimentation, established importance of automation but didn't systematically analyze domain structure-success relationships]",
            "Langley et al. (1987) Scientific Discovery: Computational Explorations of the Creative Process [Classical work on computational discovery, identified importance of domain knowledge but predates modern ML and doesn't address the specific factors identified here]",
            "Lu et al. (2024) The AI Scientist: Towards fully automated open-ended scientific discovery [Recent work on automated discovery but doesn't systematically analyze the relationship between domain characteristics and success rates across multiple systems]",
            "Silver et al. (2021) Reward is enough [Proposes that reward signals are sufficient for intelligence, related to the importance of clear evaluation metrics but doesn't address domain structure specifically]",
            "Chollet (2019) On the Measure of Intelligence [Discusses the importance of generalization and task structure in measuring intelligence, related but focused on evaluation rather than prediction of success rates]",
            "Marcus (2020) The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence [Discusses limitations of current AI including brittleness and lack of generalization, related to domain-specificity findings but doesn't provide quantitative predictions]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Axial Evaluation Theory for LLM-Generated Scientific Theories - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-2277</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-2277</p>
                <p><strong>Name:</strong> Multi-Axial Evaluation Theory for LLM-Generated Scientific Theories</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories.</p>
                <p><strong>Description:</strong> This theory posits that the evaluation of LLM-generated scientific theories should be conducted along multiple, orthogonal axes: internal logical coherence, empirical adequacy, novelty, and epistemic utility. Each axis is assessed independently, and a theory's overall value is a function of its performance across all axes. This approach is designed to capture the unique strengths and weaknesses of LLM-generated outputs, which may differ from human-generated theories in systematic ways.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Orthogonal Axes of Evaluation (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; theory &#8594; is_generated_by &#8594; LLM</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; theory &#8594; should_be_evaluated_on &#8594; logical_coherence<span style="color: #888888;">, and</span></div>
        <div>&#8226; theory &#8594; should_be_evaluated_on &#8594; empirical_adequacy<span style="color: #888888;">, and</span></div>
        <div>&#8226; theory &#8594; should_be_evaluated_on &#8594; novelty<span style="color: #888888;">, and</span></div>
        <div>&#8226; theory &#8594; should_be_evaluated_on &#8594; epistemic_utility</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human scientific theories are evaluated on similar axes, but LLMs may generate plausible-sounding but incoherent or ungrounded theories, necessitating explicit multi-axial evaluation. </li>
    <li>LLMs can recombine known facts in novel ways, so novelty and epistemic utility must be explicitly assessed. </li>
    <li>LLMs are prone to hallucination and may generate internally inconsistent or empirically unsupported statements, requiring explicit checks for logical coherence and empirical adequacy. </li>
    <li>Epistemic utility (the potential to advance knowledge or generate new testable predictions) is a key criterion in philosophy of science, and is especially relevant for evaluating the value of LLM-generated theories. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While multi-criteria evaluation is not new, its explicit application and formalization for LLM-generated scientific theories is novel.</p>            <p><strong>What Already Exists:</strong> Multi-criteria evaluation is common in philosophy of science (e.g., Kuhn, Lakatos), but not formalized for LLM-generated theories.</p>            <p><strong>What is Novel:</strong> Explicitly formalizes the need for orthogonal, independent axes tailored to LLM-generated scientific theories.</p>
            <p><strong>References:</strong> <ul>
    <li>Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]</li>
    <li>Lakatos (1970) Falsification and the Methodology of Scientific Research Programmes [theory evaluation criteria]</li>
    <li>Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM-specific risks and evaluation needs]</li>
</ul>
            <h3>Statement 1: Composite Evaluation Function (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; theory &#8594; has_scores_on &#8594; logical_coherence, empirical_adequacy, novelty, epistemic_utility</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; theory &#8594; has_overall_evaluation_score &#8594; f(logical_coherence, empirical_adequacy, novelty, epistemic_utility)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Composite scoring is used in multi-criteria decision analysis and in some scientific grant review processes. </li>
    <li>LLM-generated theories may excel in some axes but not others, so a composite function is needed to summarize overall value. </li>
    <li>The need to balance trade-offs between novelty and empirical adequacy is well-documented in theory evaluation literature. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The use of composite functions is existing, but its application to LLM-generated scientific theory evaluation is novel.</p>            <p><strong>What Already Exists:</strong> Composite scoring functions are used in other domains, but not formalized for LLM-generated scientific theory evaluation.</p>            <p><strong>What is Novel:</strong> Proposes a formal, multi-axial composite function specifically for LLM-generated scientific theories.</p>
            <p><strong>References:</strong> <ul>
    <li>Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]</li>
    <li>Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM evaluation challenges]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM-generated theory is highly novel but lacks empirical adequacy, its overall evaluation score will be low unless the composite function heavily weights novelty.</li>
                <li>LLM-generated theories that are internally coherent but empirically inadequate will be flagged as weak by this evaluation framework.</li>
                <li>Theories generated by LLMs will often score higher on novelty than on empirical adequacy, compared to human-generated theories.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Some LLM-generated theories may achieve high scores on all axes, potentially surpassing human-generated theories in certain domains.</li>
                <li>Adjusting the weights in the composite function may reveal previously overlooked, high-utility LLM-generated theories.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLM-generated theories that are empirically adequate but logically incoherent are rated highly, the theory's multi-axial approach is called into question.</li>
                <li>If the composite function fails to distinguish between high- and low-quality LLM-generated theories, the theory is undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of LLM training data biases on evaluation outcomes is not explicitly addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The general approach is related to existing theory evaluation frameworks, but its explicit application to LLM-generated scientific theories is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]</li>
    <li>Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]</li>
    <li>Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM evaluation challenges]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Axial Evaluation Theory for LLM-Generated Scientific Theories",
    "theory_description": "This theory posits that the evaluation of LLM-generated scientific theories should be conducted along multiple, orthogonal axes: internal logical coherence, empirical adequacy, novelty, and epistemic utility. Each axis is assessed independently, and a theory's overall value is a function of its performance across all axes. This approach is designed to capture the unique strengths and weaknesses of LLM-generated outputs, which may differ from human-generated theories in systematic ways.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Orthogonal Axes of Evaluation",
                "if": [
                    {
                        "subject": "theory",
                        "relation": "is_generated_by",
                        "object": "LLM"
                    }
                ],
                "then": [
                    {
                        "subject": "theory",
                        "relation": "should_be_evaluated_on",
                        "object": "logical_coherence"
                    },
                    {
                        "subject": "theory",
                        "relation": "should_be_evaluated_on",
                        "object": "empirical_adequacy"
                    },
                    {
                        "subject": "theory",
                        "relation": "should_be_evaluated_on",
                        "object": "novelty"
                    },
                    {
                        "subject": "theory",
                        "relation": "should_be_evaluated_on",
                        "object": "epistemic_utility"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human scientific theories are evaluated on similar axes, but LLMs may generate plausible-sounding but incoherent or ungrounded theories, necessitating explicit multi-axial evaluation.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can recombine known facts in novel ways, so novelty and epistemic utility must be explicitly assessed.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs are prone to hallucination and may generate internally inconsistent or empirically unsupported statements, requiring explicit checks for logical coherence and empirical adequacy.",
                        "uuids": []
                    },
                    {
                        "text": "Epistemic utility (the potential to advance knowledge or generate new testable predictions) is a key criterion in philosophy of science, and is especially relevant for evaluating the value of LLM-generated theories.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Multi-criteria evaluation is common in philosophy of science (e.g., Kuhn, Lakatos), but not formalized for LLM-generated theories.",
                    "what_is_novel": "Explicitly formalizes the need for orthogonal, independent axes tailored to LLM-generated scientific theories.",
                    "classification_explanation": "While multi-criteria evaluation is not new, its explicit application and formalization for LLM-generated scientific theories is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]",
                        "Lakatos (1970) Falsification and the Methodology of Scientific Research Programmes [theory evaluation criteria]",
                        "Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM-specific risks and evaluation needs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Composite Evaluation Function",
                "if": [
                    {
                        "subject": "theory",
                        "relation": "has_scores_on",
                        "object": "logical_coherence, empirical_adequacy, novelty, epistemic_utility"
                    }
                ],
                "then": [
                    {
                        "subject": "theory",
                        "relation": "has_overall_evaluation_score",
                        "object": "f(logical_coherence, empirical_adequacy, novelty, epistemic_utility)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Composite scoring is used in multi-criteria decision analysis and in some scientific grant review processes.",
                        "uuids": []
                    },
                    {
                        "text": "LLM-generated theories may excel in some axes but not others, so a composite function is needed to summarize overall value.",
                        "uuids": []
                    },
                    {
                        "text": "The need to balance trade-offs between novelty and empirical adequacy is well-documented in theory evaluation literature.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Composite scoring functions are used in other domains, but not formalized for LLM-generated scientific theory evaluation.",
                    "what_is_novel": "Proposes a formal, multi-axial composite function specifically for LLM-generated scientific theories.",
                    "classification_explanation": "The use of composite functions is existing, but its application to LLM-generated scientific theory evaluation is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]",
                        "Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM evaluation challenges]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM-generated theory is highly novel but lacks empirical adequacy, its overall evaluation score will be low unless the composite function heavily weights novelty.",
        "LLM-generated theories that are internally coherent but empirically inadequate will be flagged as weak by this evaluation framework.",
        "Theories generated by LLMs will often score higher on novelty than on empirical adequacy, compared to human-generated theories."
    ],
    "new_predictions_unknown": [
        "Some LLM-generated theories may achieve high scores on all axes, potentially surpassing human-generated theories in certain domains.",
        "Adjusting the weights in the composite function may reveal previously overlooked, high-utility LLM-generated theories."
    ],
    "negative_experiments": [
        "If LLM-generated theories that are empirically adequate but logically incoherent are rated highly, the theory's multi-axial approach is called into question.",
        "If the composite function fails to distinguish between high- and low-quality LLM-generated theories, the theory is undermined."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of LLM training data biases on evaluation outcomes is not explicitly addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLM-generated theories may be useful despite low scores on traditional axes, challenging the sufficiency of the proposed axes.",
            "uuids": []
        }
    ],
    "special_cases": [
        "The framework may not apply to LLM-generated theories in domains with little or no empirical data.",
        "The composite function may need to be domain-specific for optimal results."
    ],
    "existing_theory": {
        "what_already_exists": "Multi-criteria evaluation and composite scoring are established in philosophy of science and decision theory.",
        "what_is_novel": "Explicitly formalizes and adapts these concepts for the evaluation of LLM-generated scientific theories.",
        "classification_explanation": "The general approach is related to existing theory evaluation frameworks, but its explicit application to LLM-generated scientific theories is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kuhn (1962) The Structure of Scientific Revolutions [criteria for theory choice]",
            "Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]",
            "Bender et al. (2021) On the Dangers of Stochastic Parrots [LLM evaluation challenges]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how to evaluate LLM-generated scientific theories.",
    "original_theory_id": "theory-678",
    "original_theory_name": "Actionable Feedback as a Necessary Condition for Iterative Evaluation Improvement",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
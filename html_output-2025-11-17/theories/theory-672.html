<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multidimensional Alignment Theory of LLM-Generated Scientific Theory Evaluation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-672</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-672</p>
                <p><strong>Name:</strong> Multidimensional Alignment Theory of LLM-Generated Scientific Theory Evaluation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that robust evaluation of LLM-generated scientific theories requires a multidimensional alignment framework, integrating human-grounded, task-specific, and model-internal metrics. No single axis (e.g., factuality, novelty, or fluency) suffices; instead, effective evaluation emerges from the intersection and calibration of these axes, with explicit tradeoffs and calibration mechanisms to balance creativity, factuality, and utility. The theory further asserts that alignment between LLM and human judgments is non-uniform across quality levels and domains, and that meta-evaluation (e.g., meta-correlation) is necessary to detect evaluator instability. Calibration and abstention mechanisms are necessary for selective acceptance of LLM-generated theories.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Multidimensional Evaluation Necessity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-generated scientific theory &#8594; is_evaluated &#8594; using_single_metric</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation &#8594; is_incomplete &#8594; true</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>No single metric (e.g., BLEU, ROUGE, factuality) correlates strongly with human judgments of scientific theory quality, especially for open-ended or novel hypotheses. <a href="../results/extraction-result-6018.html#e6018.1" class="evidence-link">[e6018.1]</a> <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6131.html#e6131.6" class="evidence-link">[e6131.6]</a> <a href="../results/extraction-result-6001.html#e6001.6" class="evidence-link">[e6001.6]</a> <a href="../results/extraction-result-6153.html#e6153.3" class="evidence-link">[e6153.3]</a> <a href="../results/extraction-result-6131.html#e6131.8" class="evidence-link">[e6131.8]</a> <a href="../results/extraction-result-6149.html#e6149.0" class="evidence-link">[e6149.0]</a> <a href="../results/extraction-result-6149.html#e6149.5" class="evidence-link">[e6149.5]</a> <a href="../results/extraction-result-6114.html#e6114.0" class="evidence-link">[e6114.0]</a> <a href="../results/extraction-result-6153.html#e6153.1" class="evidence-link">[e6153.1]</a> <a href="../results/extraction-result-6153.html#e6153.3" class="evidence-link">[e6153.3]</a> <a href="../results/extraction-result-6001.html#e6001.6" class="evidence-link">[e6001.6]</a> <a href="../results/extraction-result-6018.html#e6018.4" class="evidence-link">[e6018.4]</a> <a href="../results/extraction-result-6116.html#e6116.3" class="evidence-link">[e6116.3]</a> <a href="../results/extraction-result-6011.html#e6011.2" class="evidence-link">[e6011.2]</a> <a href="../results/extraction-result-6011.html#e6011.1" class="evidence-link">[e6011.1]</a> </li>
    <li>Human and LLM evaluators use multidimensional rubrics (e.g., Validness, Novelty, Helpfulness; LA, OCQ, TQ, PC, H) to capture different aspects of quality. <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6153.html#e6153.3" class="evidence-link">[e6153.3]</a> <a href="../results/extraction-result-6001.html#e6001.6" class="evidence-link">[e6001.6]</a> <a href="../results/extraction-result-6114.html#e6114.0" class="evidence-link">[e6114.0]</a> <a href="../results/extraction-result-6153.html#e6153.1" class="evidence-link">[e6153.1]</a> <a href="../results/extraction-result-6153.html#e6153.3" class="evidence-link">[e6153.3]</a> <a href="../results/extraction-result-6011.html#e6011.2" class="evidence-link">[e6011.2]</a> <a href="../results/extraction-result-6011.html#e6011.1" class="evidence-link">[e6011.1]</a> </li>
    <li>Automated metrics such as ROUGE, BLEU, BERTScore, and BARTScore are insufficient for evaluating open-ended, creative, or novel scientific hypotheses, as they penalize novelty and do not capture explanatory power or utility. <a href="../results/extraction-result-6018.html#e6018.1" class="evidence-link">[e6018.1]</a> <a href="../results/extraction-result-6131.html#e6131.6" class="evidence-link">[e6131.6]</a> <a href="../results/extraction-result-6149.html#e6149.4" class="evidence-link">[e6149.4]</a> <a href="../results/extraction-result-6149.html#e6149.5" class="evidence-link">[e6149.5]</a> <a href="../results/extraction-result-6018.html#e6018.4" class="evidence-link">[e6018.4]</a> <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> </li>
    <li>Evaluation rubrics for scientific hypothesis generation explicitly include multiple axes (e.g., Validness, Novelty, Helpfulness) and are used for both human and LLM-based evaluation. <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6001.html#e6001.6" class="evidence-link">[e6001.6]</a> <a href="../results/extraction-result-6116.html#e6116.3" class="evidence-link">[e6116.3]</a> </li>
    <li>In domains such as summarization and explanation, multidimensional human rubrics (e.g., Informativeness, Clarity, Acceptability, Factuality, Novelty, Support for label) are used to capture the full range of quality. <a href="../results/extraction-result-6159.html#e6159.1" class="evidence-link">[e6159.1]</a> <a href="../results/extraction-result-6129.html#e6129.1" class="evidence-link">[e6129.1]</a> <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While multidimensional rubrics exist, this law formalizes the necessity of multidimensionality as a theoretical requirement, not just a practical recommendation.</p>            <p><strong>What Already Exists:</strong> Multidimensional rubrics are used in human evaluation and some LLM evaluation pipelines.</p>            <p><strong>What is Novel:</strong> The explicit assertion that single-metric evaluation is fundamentally incomplete for LLM-generated scientific theories, and that multidimensionality is a necessary property for robust evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Fabbri et al. (2021) Summeval [multidimensional human evaluation of summarization]</li>
    <li>Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [meta-correlation and multidimensionality]</li>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [validness/novelty/helpfulness rubric]</li>
</ul>
            <h3>Statement 1: Evaluator Alignment Instability Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-based evaluator &#8594; is_used &#8594; to_score LLM-generated scientific theories</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; alignment_with_human_judgment &#8594; varies_with &#8594; candidate_quality_and_domain<span style="color: #888888;">, and</span></div>
        <div>&#8226; meta-correlation &#8594; can_be_negative &#8594; on high-quality candidates</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM evaluators (e.g., ChatGPT, GPT-4) show negative meta-correlation, i.e., worse alignment with human judgments on higher-quality candidates, and their agreement varies by domain and evaluation dimension. <a href="../results/extraction-result-6131.html#e6131.3" class="evidence-link">[e6131.3]</a> <a href="../results/extraction-result-6131.html#e6131.8" class="evidence-link">[e6131.8]</a> <a href="../results/extraction-result-6165.html#e6165.14" class="evidence-link">[e6165.14]</a> <a href="../results/extraction-result-6153.html#e6153.0" class="evidence-link">[e6153.0]</a> <a href="../results/extraction-result-6166.html#e6166.2" class="evidence-link">[e6166.2]</a> <a href="../results/extraction-result-6011.html#e6011.2" class="evidence-link">[e6011.2]</a> <a href="../results/extraction-result-6011.html#e6011.1" class="evidence-link">[e6011.1]</a> </li>
    <li>Meta-correlation analysis reveals that LLM-based evaluators can be more aligned with humans for low-quality systems and less aligned for high-quality systems, especially in summarization and explanation tasks. <a href="../results/extraction-result-6131.html#e6131.3" class="evidence-link">[e6131.3]</a> <a href="../results/extraction-result-6131.html#e6131.8" class="evidence-link">[e6131.8]</a> <a href="../results/extraction-result-6165.html#e6165.14" class="evidence-link">[e6165.14]</a> </li>
    <li>Empirical findings show that LLM-based evaluators' agreement with humans is not uniform across domains (e.g., vision-language, summarization, scientific hypothesis generation) and can be affected by prompt design, language, and evaluation dimension. <a href="../results/extraction-result-6165.html#e6165.14" class="evidence-link">[e6165.14]</a> <a href="../results/extraction-result-6153.html#e6153.0" class="evidence-link">[e6153.0]</a> <a href="../results/extraction-result-6153.html#e6153.1" class="evidence-link">[e6153.1]</a> <a href="../results/extraction-result-6153.html#e6153.3" class="evidence-link">[e6153.3]</a> <a href="../results/extraction-result-6131.html#e6131.8" class="evidence-link">[e6131.8]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Meta-correlation is a recent metric; this law generalizes the empirical finding into a theoretical requirement for robust evaluation.</p>            <p><strong>What Already Exists:</strong> Empirical findings of negative meta-correlation and evaluator instability in LLM-based evaluation.</p>            <p><strong>What is Novel:</strong> The formalization of evaluator alignment instability as a law, and the assertion that meta-evaluation is required to detect and mitigate this instability.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [meta-correlation]</li>
    <li>Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [model-human alignment instability]</li>
</ul>
            <h3>Statement 2: Creativity-Factuality Tradeoff Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_protocol &#8594; maximizes &#8594; novelty/creativity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; risk_of_hallucination &#8594; increases &#8594; true</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Evaluations that reward novelty or creativity (e.g., open-domain hypothesis generation) observe increased hallucination or factuality errors, requiring explicit tradeoff management. <a href="../results/extraction-result-5998.html#e5998.7" class="evidence-link">[e5998.7]</a> <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6001.html#e6001.4" class="evidence-link">[e6001.4]</a> <a href="../results/extraction-result-5998.html#e5998.1" class="evidence-link">[e5998.1]</a> <a href="../results/extraction-result-6116.html#e6116.3" class="evidence-link">[e6116.3]</a> <a href="../results/extraction-result-6018.html#e6018.1" class="evidence-link">[e6018.1]</a> </li>
    <li>Human and LLM-based evaluations of scientific hypotheses report that optimizing for novelty can reduce validness, and vice versa, indicating a tradeoff between creative ideation and factual grounding. <a href="../results/extraction-result-6116.html#e6116.2" class="evidence-link">[e6116.2]</a> <a href="../results/extraction-result-6001.html#e6001.4" class="evidence-link">[e6001.4]</a> <a href="../results/extraction-result-5998.html#e5998.7" class="evidence-link">[e5998.7]</a> </li>
    <li>Detection and mitigation of hallucinations is a major concern in creative scientific ideation, and uncertainty estimation is used to flag low-confidence, potentially hallucinated outputs. <a href="../results/extraction-result-5998.html#e5998.1" class="evidence-link">[e5998.1]</a> <a href="../results/extraction-result-6023.html#e6023.1" class="evidence-link">[e6023.1]</a> <a href="../results/extraction-result-6157.html#e6157.11" class="evidence-link">[e6157.11]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The tradeoff is recognized, but this law formalizes it as a necessary consideration in evaluation theory.</p>            <p><strong>What Already Exists:</strong> The creativity-factuality tradeoff is discussed in the context of LLM ideation.</p>            <p><strong>What is Novel:</strong> The explicit law that evaluation protocols must balance these axes and that maximizing one increases risk on the other.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [novelty-validness tradeoff]</li>
    <li>Wang et al. (2024) Towards a Science Exocortex [creativity-hallucination tradeoff]</li>
</ul>
            <h3>Statement 3: Calibration and Abstention Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-generated scientific theory &#8594; is_evaluated &#8594; by LLM or automated metric</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; calibration_of_confidence &#8594; is_necessary &#8594; for selective acceptance or abstention</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Calibration metrics (e.g., P(True), reliability diagrams) are necessary to ensure that confidence scores correspond to empirical correctness, enabling selective acceptance or abstention of LLM-generated theories. <a href="../results/extraction-result-6132.html#e6132.9" class="evidence-link">[e6132.9]</a> <a href="../results/extraction-result-6156.html#e6156.6" class="evidence-link">[e6156.6]</a> <a href="../results/extraction-result-6157.html#e6157.1" class="evidence-link">[e6157.1]</a> <a href="../results/extraction-result-6157.html#e6157.6" class="evidence-link">[e6157.6]</a> <a href="../results/extraction-result-6157.html#e6157.0" class="evidence-link">[e6157.0]</a> <a href="../results/extraction-result-6157.html#e6157.11" class="evidence-link">[e6157.11]</a> <a href="../results/extraction-result-6023.html#e6023.1" class="evidence-link">[e6023.1]</a> </li>
    <li>Calibration and abstention mechanisms (e.g., selective generation, NOTA, uncertainty estimation) are used to filter out low-confidence or hallucinated outputs, improving reliability of LLM-generated scientific claims. <a href="../results/extraction-result-6157.html#e6157.1" class="evidence-link">[e6157.1]</a> <a href="../results/extraction-result-6157.html#e6157.0" class="evidence-link">[e6157.0]</a> <a href="../results/extraction-result-6023.html#e6023.1" class="evidence-link">[e6023.1]</a> <a href="../results/extraction-result-6128.html#e6128.4" class="evidence-link">[e6128.4]</a> </li>
    <li>RLHF and other alignment techniques can reduce calibration, highlighting the need for explicit calibration analysis in evaluation protocols. <a href="../results/extraction-result-6156.html#e6156.6" class="evidence-link">[e6156.6]</a> <a href="../results/extraction-result-6156.html#e6156.3" class="evidence-link">[e6156.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Calibration is well-studied, but its necessity for scientific theory evaluation is newly formalized here.</p>            <p><strong>What Already Exists:</strong> Calibration is a known requirement in model evaluation.</p>            <p><strong>What is Novel:</strong> The law that calibration is a necessary property for any automated or LLM-based evaluation of scientific theories, especially for selective generation or abstention.</p>
            <p><strong>References:</strong> <ul>
    <li>Kadavath et al. (2022) Language models (mostly) know what they know [calibration in LLMs]</li>
    <li>OpenAI (2023) GPT-4 Technical Report [calibration analysis]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new evaluation protocol for LLM-generated scientific theories uses only a single metric (e.g., BLEU or factuality), it will fail to capture key aspects of theory quality such as novelty or explanatory power, leading to poor alignment with human expert judgments.</li>
                <li>If an LLM-based evaluator is used to score both low- and high-quality candidate theories, its agreement with human experts will be higher on low-quality candidates and lower on high-quality candidates, as measured by negative meta-correlation.</li>
                <li>If an evaluation protocol rewards only novelty, the rate of hallucinated or factually incorrect theories will increase compared to protocols that balance novelty and factuality.</li>
                <li>If calibration and abstention mechanisms are not used, LLM-based evaluators will accept more hallucinated or low-confidence theories, reducing reliability.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a multidimensional, meta-evaluated, and calibrated evaluation protocol is implemented, it may enable automated evaluators to reach or surpass human-level reliability in scoring LLM-generated scientific theories, especially in domains with clear ground truth.</li>
                <li>If calibration and abstention mechanisms are integrated into LLM-based evaluators, it may be possible to automatically filter out all hallucinated or low-confidence theories without human intervention, potentially enabling safe autonomous scientific ideation.</li>
                <li>If evaluator instability is detected and corrected via meta-correlation analysis, it may be possible to design LLM-based evaluators that are robust across all quality levels and domains, eliminating the current observed alignment gaps.</li>
                <li>If multidimensional evaluation is extended to include new axes (e.g., significance, impact, falsifiability), it may further improve alignment with human expert judgments in scientific theory evaluation.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a multidimensional evaluation protocol does not improve alignment with human expert judgments compared to single-metric protocols, the necessity of multidimensionality would be called into question.</li>
                <li>If LLM-based evaluators show stable or positive meta-correlation (i.e., better alignment on higher-quality candidates) across a wide range of tasks and domains, the evaluator alignment instability law would be challenged.</li>
                <li>If maximizing novelty in evaluation does not increase hallucination or factuality errors, the creativity-factuality tradeoff law would be falsified.</li>
                <li>If calibration and abstention mechanisms do not improve selective acceptance of correct theories, the calibration law would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some domains (e.g., mathematics, code synthesis) have objective metrics that may suffice for evaluation without multidimensionality. <a href="../results/extraction-result-6108.html#e6108.1" class="evidence-link">[e6108.1]</a> <a href="../results/extraction-result-6108.html#e6108.2" class="evidence-link">[e6108.2]</a> <a href="../results/extraction-result-6123.html#e6123.1" class="evidence-link">[e6123.1]</a> <a href="../results/extraction-result-6025.html#e6025.6" class="evidence-link">[e6025.6]</a> </li>
    <li>There are cases where LLM-based evaluators outperform humans on certain tasks (e.g., low-level factuality detection, vision-language pairwise ranking), which may not fit the instability law. <a href="../results/extraction-result-6153.html#e6153.0" class="evidence-link">[e6153.0]</a> <a href="../results/extraction-result-6165.html#e6165.14" class="evidence-link">[e6165.14]</a> <a href="../results/extraction-result-6011.html#e6011.2" class="evidence-link">[e6011.2]</a> <a href="../results/extraction-result-6166.html#e6166.2" class="evidence-link">[e6166.2]</a> </li>
    <li>Some evaluation settings (e.g., with strong reference answers or in highly constrained tasks) show high LLM-human agreement, suggesting that instability is not universal. <a href="../results/extraction-result-6147.html#e6147.4" class="evidence-link">[e6147.4]</a> <a href="../results/extraction-result-6011.html#e6011.2" class="evidence-link">[e6011.2]</a> <a href="../results/extraction-result-6011.html#e6011.1" class="evidence-link">[e6011.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While components exist, the unification and formalization of these requirements as a theory of evaluation is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [meta-correlation, multidimensionality]</li>
    <li>Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [validness/novelty/helpfulness rubric]</li>
    <li>Kadavath et al. (2022) Language models (mostly) know what they know [calibration]</li>
    <li>OpenAI (2023) GPT-4 Technical Report [calibration, RLHF effects]</li>
    <li>Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [model-human alignment instability]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multidimensional Alignment Theory of LLM-Generated Scientific Theory Evaluation",
    "theory_description": "This theory posits that robust evaluation of LLM-generated scientific theories requires a multidimensional alignment framework, integrating human-grounded, task-specific, and model-internal metrics. No single axis (e.g., factuality, novelty, or fluency) suffices; instead, effective evaluation emerges from the intersection and calibration of these axes, with explicit tradeoffs and calibration mechanisms to balance creativity, factuality, and utility. The theory further asserts that alignment between LLM and human judgments is non-uniform across quality levels and domains, and that meta-evaluation (e.g., meta-correlation) is necessary to detect evaluator instability. Calibration and abstention mechanisms are necessary for selective acceptance of LLM-generated theories.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Multidimensional Evaluation Necessity Law",
                "if": [
                    {
                        "subject": "LLM-generated scientific theory",
                        "relation": "is_evaluated",
                        "object": "using_single_metric"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation",
                        "relation": "is_incomplete",
                        "object": "true"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "No single metric (e.g., BLEU, ROUGE, factuality) correlates strongly with human judgments of scientific theory quality, especially for open-ended or novel hypotheses.",
                        "uuids": [
                            "e6018.1",
                            "e6116.2",
                            "e6131.6",
                            "e6001.6",
                            "e6153.3",
                            "e6131.8",
                            "e6149.0",
                            "e6149.5",
                            "e6114.0",
                            "e6153.1",
                            "e6153.3",
                            "e6001.6",
                            "e6018.4",
                            "e6116.3",
                            "e6011.2",
                            "e6011.1"
                        ]
                    },
                    {
                        "text": "Human and LLM evaluators use multidimensional rubrics (e.g., Validness, Novelty, Helpfulness; LA, OCQ, TQ, PC, H) to capture different aspects of quality.",
                        "uuids": [
                            "e6116.2",
                            "e6153.3",
                            "e6001.6",
                            "e6114.0",
                            "e6153.1",
                            "e6153.3",
                            "e6011.2",
                            "e6011.1"
                        ]
                    },
                    {
                        "text": "Automated metrics such as ROUGE, BLEU, BERTScore, and BARTScore are insufficient for evaluating open-ended, creative, or novel scientific hypotheses, as they penalize novelty and do not capture explanatory power or utility.",
                        "uuids": [
                            "e6018.1",
                            "e6131.6",
                            "e6149.4",
                            "e6149.5",
                            "e6018.4",
                            "e6116.2"
                        ]
                    },
                    {
                        "text": "Evaluation rubrics for scientific hypothesis generation explicitly include multiple axes (e.g., Validness, Novelty, Helpfulness) and are used for both human and LLM-based evaluation.",
                        "uuids": [
                            "e6116.2",
                            "e6001.6",
                            "e6116.3"
                        ]
                    },
                    {
                        "text": "In domains such as summarization and explanation, multidimensional human rubrics (e.g., Informativeness, Clarity, Acceptability, Factuality, Novelty, Support for label) are used to capture the full range of quality.",
                        "uuids": [
                            "e6159.1",
                            "e6129.1",
                            "e6116.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Multidimensional rubrics are used in human evaluation and some LLM evaluation pipelines.",
                    "what_is_novel": "The explicit assertion that single-metric evaluation is fundamentally incomplete for LLM-generated scientific theories, and that multidimensionality is a necessary property for robust evaluation.",
                    "classification_explanation": "While multidimensional rubrics exist, this law formalizes the necessity of multidimensionality as a theoretical requirement, not just a practical recommendation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Fabbri et al. (2021) Summeval [multidimensional human evaluation of summarization]",
                        "Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [meta-correlation and multidimensionality]",
                        "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [validness/novelty/helpfulness rubric]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Evaluator Alignment Instability Law",
                "if": [
                    {
                        "subject": "LLM-based evaluator",
                        "relation": "is_used",
                        "object": "to_score LLM-generated scientific theories"
                    }
                ],
                "then": [
                    {
                        "subject": "alignment_with_human_judgment",
                        "relation": "varies_with",
                        "object": "candidate_quality_and_domain"
                    },
                    {
                        "subject": "meta-correlation",
                        "relation": "can_be_negative",
                        "object": "on high-quality candidates"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM evaluators (e.g., ChatGPT, GPT-4) show negative meta-correlation, i.e., worse alignment with human judgments on higher-quality candidates, and their agreement varies by domain and evaluation dimension.",
                        "uuids": [
                            "e6131.3",
                            "e6131.8",
                            "e6165.14",
                            "e6153.0",
                            "e6166.2",
                            "e6011.2",
                            "e6011.1"
                        ]
                    },
                    {
                        "text": "Meta-correlation analysis reveals that LLM-based evaluators can be more aligned with humans for low-quality systems and less aligned for high-quality systems, especially in summarization and explanation tasks.",
                        "uuids": [
                            "e6131.3",
                            "e6131.8",
                            "e6165.14"
                        ]
                    },
                    {
                        "text": "Empirical findings show that LLM-based evaluators' agreement with humans is not uniform across domains (e.g., vision-language, summarization, scientific hypothesis generation) and can be affected by prompt design, language, and evaluation dimension.",
                        "uuids": [
                            "e6165.14",
                            "e6153.0",
                            "e6153.1",
                            "e6153.3",
                            "e6131.8"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical findings of negative meta-correlation and evaluator instability in LLM-based evaluation.",
                    "what_is_novel": "The formalization of evaluator alignment instability as a law, and the assertion that meta-evaluation is required to detect and mitigate this instability.",
                    "classification_explanation": "Meta-correlation is a recent metric; this law generalizes the empirical finding into a theoretical requirement for robust evaluation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [meta-correlation]",
                        "Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [model-human alignment instability]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Creativity-Factuality Tradeoff Law",
                "if": [
                    {
                        "subject": "evaluation_protocol",
                        "relation": "maximizes",
                        "object": "novelty/creativity"
                    }
                ],
                "then": [
                    {
                        "subject": "risk_of_hallucination",
                        "relation": "increases",
                        "object": "true"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Evaluations that reward novelty or creativity (e.g., open-domain hypothesis generation) observe increased hallucination or factuality errors, requiring explicit tradeoff management.",
                        "uuids": [
                            "e5998.7",
                            "e6116.2",
                            "e6001.4",
                            "e5998.1",
                            "e6116.3",
                            "e6018.1"
                        ]
                    },
                    {
                        "text": "Human and LLM-based evaluations of scientific hypotheses report that optimizing for novelty can reduce validness, and vice versa, indicating a tradeoff between creative ideation and factual grounding.",
                        "uuids": [
                            "e6116.2",
                            "e6001.4",
                            "e5998.7"
                        ]
                    },
                    {
                        "text": "Detection and mitigation of hallucinations is a major concern in creative scientific ideation, and uncertainty estimation is used to flag low-confidence, potentially hallucinated outputs.",
                        "uuids": [
                            "e5998.1",
                            "e6023.1",
                            "e6157.11"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The creativity-factuality tradeoff is discussed in the context of LLM ideation.",
                    "what_is_novel": "The explicit law that evaluation protocols must balance these axes and that maximizing one increases risk on the other.",
                    "classification_explanation": "The tradeoff is recognized, but this law formalizes it as a necessary consideration in evaluation theory.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [novelty-validness tradeoff]",
                        "Wang et al. (2024) Towards a Science Exocortex [creativity-hallucination tradeoff]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Calibration and Abstention Law",
                "if": [
                    {
                        "subject": "LLM-generated scientific theory",
                        "relation": "is_evaluated",
                        "object": "by LLM or automated metric"
                    }
                ],
                "then": [
                    {
                        "subject": "calibration_of_confidence",
                        "relation": "is_necessary",
                        "object": "for selective acceptance or abstention"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Calibration metrics (e.g., P(True), reliability diagrams) are necessary to ensure that confidence scores correspond to empirical correctness, enabling selective acceptance or abstention of LLM-generated theories.",
                        "uuids": [
                            "e6132.9",
                            "e6156.6",
                            "e6157.1",
                            "e6157.6",
                            "e6157.0",
                            "e6157.11",
                            "e6023.1"
                        ]
                    },
                    {
                        "text": "Calibration and abstention mechanisms (e.g., selective generation, NOTA, uncertainty estimation) are used to filter out low-confidence or hallucinated outputs, improving reliability of LLM-generated scientific claims.",
                        "uuids": [
                            "e6157.1",
                            "e6157.0",
                            "e6023.1",
                            "e6128.4"
                        ]
                    },
                    {
                        "text": "RLHF and other alignment techniques can reduce calibration, highlighting the need for explicit calibration analysis in evaluation protocols.",
                        "uuids": [
                            "e6156.6",
                            "e6156.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Calibration is a known requirement in model evaluation.",
                    "what_is_novel": "The law that calibration is a necessary property for any automated or LLM-based evaluation of scientific theories, especially for selective generation or abstention.",
                    "classification_explanation": "Calibration is well-studied, but its necessity for scientific theory evaluation is newly formalized here.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Kadavath et al. (2022) Language models (mostly) know what they know [calibration in LLMs]",
                        "OpenAI (2023) GPT-4 Technical Report [calibration analysis]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a new evaluation protocol for LLM-generated scientific theories uses only a single metric (e.g., BLEU or factuality), it will fail to capture key aspects of theory quality such as novelty or explanatory power, leading to poor alignment with human expert judgments.",
        "If an LLM-based evaluator is used to score both low- and high-quality candidate theories, its agreement with human experts will be higher on low-quality candidates and lower on high-quality candidates, as measured by negative meta-correlation.",
        "If an evaluation protocol rewards only novelty, the rate of hallucinated or factually incorrect theories will increase compared to protocols that balance novelty and factuality.",
        "If calibration and abstention mechanisms are not used, LLM-based evaluators will accept more hallucinated or low-confidence theories, reducing reliability."
    ],
    "new_predictions_unknown": [
        "If a multidimensional, meta-evaluated, and calibrated evaluation protocol is implemented, it may enable automated evaluators to reach or surpass human-level reliability in scoring LLM-generated scientific theories, especially in domains with clear ground truth.",
        "If calibration and abstention mechanisms are integrated into LLM-based evaluators, it may be possible to automatically filter out all hallucinated or low-confidence theories without human intervention, potentially enabling safe autonomous scientific ideation.",
        "If evaluator instability is detected and corrected via meta-correlation analysis, it may be possible to design LLM-based evaluators that are robust across all quality levels and domains, eliminating the current observed alignment gaps.",
        "If multidimensional evaluation is extended to include new axes (e.g., significance, impact, falsifiability), it may further improve alignment with human expert judgments in scientific theory evaluation."
    ],
    "negative_experiments": [
        "If a multidimensional evaluation protocol does not improve alignment with human expert judgments compared to single-metric protocols, the necessity of multidimensionality would be called into question.",
        "If LLM-based evaluators show stable or positive meta-correlation (i.e., better alignment on higher-quality candidates) across a wide range of tasks and domains, the evaluator alignment instability law would be challenged.",
        "If maximizing novelty in evaluation does not increase hallucination or factuality errors, the creativity-factuality tradeoff law would be falsified.",
        "If calibration and abstention mechanisms do not improve selective acceptance of correct theories, the calibration law would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Some domains (e.g., mathematics, code synthesis) have objective metrics that may suffice for evaluation without multidimensionality.",
            "uuids": [
                "e6108.1",
                "e6108.2",
                "e6123.1",
                "e6025.6"
            ]
        },
        {
            "text": "There are cases where LLM-based evaluators outperform humans on certain tasks (e.g., low-level factuality detection, vision-language pairwise ranking), which may not fit the instability law.",
            "uuids": [
                "e6153.0",
                "e6165.14",
                "e6011.2",
                "e6166.2"
            ]
        },
        {
            "text": "Some evaluation settings (e.g., with strong reference answers or in highly constrained tasks) show high LLM-human agreement, suggesting that instability is not universal.",
            "uuids": [
                "e6147.4",
                "e6011.2",
                "e6011.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies report high agreement between LLM-based evaluators and human experts in certain domains and tasks, suggesting that evaluator instability is not universal.",
            "uuids": [
                "e6011.2",
                "e6153.0",
                "e6166.2"
            ]
        },
        {
            "text": "In some cases, LLM-based evaluators (e.g., GPT-4) achieve near-human or superhuman performance on pairwise ranking or factuality detection, challenging the generality of the instability law.",
            "uuids": [
                "e6165.14",
                "e6153.0"
            ]
        }
    ],
    "special_cases": [
        "Domains with objective, automated ground truth (e.g., code synthesis, math) may not require multidimensional or meta-evaluated protocols.",
        "In low-resource or highly subjective domains, human evaluation may remain necessary regardless of protocol design.",
        "When strong reference answers are available and used in evaluation (e.g., reference-guided grading), LLM-human alignment can be high and instability may be minimized."
    ],
    "existing_theory": {
        "what_already_exists": "Multidimensional rubrics, calibration, and meta-evaluation are present in the literature, but not unified as a theoretical framework for LLM-generated scientific theory evaluation.",
        "what_is_novel": "The explicit formalization of multidimensionality, evaluator instability, creativity-factuality tradeoff, and calibration as necessary and interacting laws for robust evaluation of LLM-generated scientific theories.",
        "classification_explanation": "While components exist, the unification and formalization of these requirements as a theory of evaluation is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wang et al. (2023) Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization [meta-correlation, multidimensionality]",
            "Zhang et al. (2023) Large Language Models for Automated Open-domain Scientific Hypotheses Discovery [validness/novelty/helpfulness rubric]",
            "Kadavath et al. (2022) Language models (mostly) know what they know [calibration]",
            "OpenAI (2023) GPT-4 Technical Report [calibration, RLHF effects]",
            "Li et al. (2024) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark [model-human alignment instability]"
        ]
    },
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Memory Architecture Theory for Text Game Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-447</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-447</p>
                <p><strong>Name:</strong> Dual-Process Memory Architecture Theory for Text Game Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM-based agents can most effectively be augmented with memory to solve text games, based on the following results.</p>
                <p><strong>Description:</strong> LLM-based agents achieve optimal performance in text games through a dual-process memory architecture that combines fast, reactive short-term memory (for immediate context and recent interactions) with slow, deliberative long-term memory (for accumulated experience and strategic knowledge). The effectiveness depends on: (1) the quality of information flow between these systems, (2) the agent's ability to selectively retrieve and apply relevant memories based on task demands, (3) the balance of positive and negative experiences stored, and (4) the compression and organization strategy used for long-term storage. Performance gains are particularly pronounced for complex, long-horizon tasks and scale with model capability.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Law 0</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses only &#8594; short-term working memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; short-term memory &#8594; contains &#8594; recent observations and actions<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; immediate context tracking</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; basic task completion on simple tasks<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; degrades significantly on &#8594; long-horizon and complex planning tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SmartPlay agents using prompt-history memory succeed on Bandits and Rock-Paper-Scissors but fail on Tower of Hanoi, Crafter, and Minecraft with scores far below human baselines <a href="../results/extraction-result-2806.html#e2806.0" class="evidence-link">[e2806.0]</a> </li>
    <li>EMMA uses 3-frame temporal buffer which helps with movement dynamics but struggles on S3 dynamics-based disambiguation tasks (22% training, 10% test) <a href="../results/extraction-result-2855.html#e2855.0" class="evidence-link">[e2855.0]</a> </li>
    <li>ChatGPT with prompt-based memory achieves score 15.0 in Zork with explicit previous-action reminders but exhibits failures in world modeling and SLAM-like mapping <a href="../results/extraction-result-2848.html#e2848.0" class="evidence-link">[e2848.0]</a> </li>
    <li>SWIFT-only with sliding-window K=10 memory achieves 49.22 on ScienceWorld but fails to generalize to exceptions and unseen variations <a href="../results/extraction-result-2878.html#e2878.1" class="evidence-link">[e2878.1]</a> </li>
    <li>AGENTBOARD reflex agents with context-window memory show that most open-weight models plateau early in long interactions (Llama2-70b: 23.8% progress, 4.5% success overall) <a href="../results/extraction-result-2836.html#e2836.0" class="evidence-link">[e2836.0]</a> <a href="../results/extraction-result-2836.html#e2836.2" class="evidence-link">[e2836.2]</a> </li>
    <li>CALM GPT-2 with short sliding-window context achieves only 9.4% average normalized score on Jericho games <a href="../results/extraction-result-2850.html#e2850.0" class="evidence-link">[e2850.0]</a> </li>
    <li>ReAct baseline without persistent memory achieves only 40.0% on ALFWorld and 28.0% on HotpotQA <a href="../results/extraction-result-2874.html#e2874.2" class="evidence-link">[e2874.2]</a> <a href="../results/extraction-result-2801.html#e2801.0" class="evidence-link">[e2801.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Law 1</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; combines &#8594; short-term and long-term memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; long-term memory &#8594; stores &#8594; successful experiences and reflections<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; retrieves &#8594; relevant long-term memories for current context</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; substantial performance improvement over short-term only<span style="color: #888888;">, and</span></div>
        <div>&#8226; improvement &#8594; is greater for &#8594; complex multi-step tasks and smaller LLMs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Sweet&Sour with dual-buffer memory (short-term + long-term) achieves 54.6% on ScienceWorld with GPT-4o vs 36.0% baseline ReAct, 44.6% with Mistral Large 2 vs 24.8% baseline, and 32.5% with Llama 3.1 8B vs 20.5% baseline <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>SAGE with dual memory (STM for trajectory, LTM for reflections) yields +17.3% improvement on ALFWorld task completion (73.8% vs 56.5% baseline) and +14.8% on step completion <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
    <li>ExpeL combining episodic trajectories and semantic insights achieves 59.0% on ALFWorld vs 40.0% ReAct baseline and 70% on FEVER transfer vs 63% ReAct <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>GITM with text-based memory storing successful action sequences achieves 67.5% diamond success vs 35.0% without memory, and 95.0% iron_pickaxe vs 57.5% without memory <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> </li>
    <li>Voyager with persistent skill library unlocks all 262 Overworld items and achieves 67.5% ObtainDiamond success, +47.5% over prior methods <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> </li>
    <li>ReAct + Reflexion with episodic long-term memory completed 130/134 ALFWorld tasks (97.0%) with 22% absolute improvement over baseline <a href="../results/extraction-result-2801.html#e2801.0" class="evidence-link">[e2801.0]</a> </li>
    <li>SWIFTSAGE combining SWIFT local memory with SAGE LLM planning achieves 84.7 on ScienceWorld vs 49.2 SWIFT-only and 33.82 SayCan baseline <a href="../results/extraction-result-2878.html#e2878.0" class="evidence-link">[e2878.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Law 2</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory content &#8594; includes &#8594; both successful and failed experiences<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; uses &#8594; balanced positive and negative examples</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent performance &#8594; exceeds &#8594; failure-only memory systems<span style="color: #888888;">, and</span></div>
        <div>&#8226; improvement &#8594; is especially large for &#8594; smaller LLMs and early-success scenarios</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Sweet&Sour storing both successes and failures outperforms Reflexion (failures only) across all models: Llama 3.1 8B 32.5% vs 21.7%, Mistral Large 2 44.6% vs 27.6%, GPT-4o 54.6% vs 45.3% <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> <a href="../results/extraction-result-2817.html#e2817.0" class="evidence-link">[e2817.0]</a> </li>
    <li>Ablation removing positive experiences from Sweet&Sour drops performance substantially: Llama 3.1 8B to 24.6%, Mistral to 31.1%, GPT-4o to 44.9% <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>ExpeL gathering diverse success/fail pairs with Reflexion outperforms gathering with ReAct only, and experience quantity/diversity matters for performance <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>GITM memory storing successful sequences for sub-goals provides large gains when sequences are summarized into canonical reference plans after N=5 successes <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Law 3</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory &#8594; uses &#8594; semantic similarity retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; is based on &#8594; task-relevant embeddings</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; retrieves &#8594; more relevant memories<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; improves over &#8594; random or recency-only retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ExpeL task-similarity retrieval (Faiss embedding kNN) outperforms reasoning-similarity and random sampling; random sampling dropped performance significantly <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>LoTa-Bench semantic similarity sampling achieves 43.25% avg subgoal success vs baseline 40.82% on WAH-NL with LLaMA-1 65B <a href="../results/extraction-result-2876.html#e2876.0" class="evidence-link">[e2876.0]</a> </li>
    <li>Voyager skill library uses text-embedding-ada-002 for semantic retrieval to match current tasks to stored skills, enabling zero-shot generalization <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> </li>
    <li>AriGraph semantic search using Contriever embeddings + BFS-like graph expansion outperforms full history, summarization, and RAG baselines on TextWorld tasks <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>Werewolf agent using SentenceBERT semantic similarity (cosine > 0.85) for experience retrieval improves villager winning rate with 10-20 round experience pools <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 4: Law 4</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory &#8594; uses &#8594; structured representation<span style="color: #888888;">, and</span></div>
        <div>&#8226; structure &#8594; is &#8594; knowledge graph or hierarchical organization<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; spatial reasoning or multi-hop inference</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; better performance than unstructured memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory &#8594; enables &#8594; more efficient retrieval and reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AriGraph hybrid semantic+episodic knowledge graph achieves normalized scores of 1.00 on Treasure Hunt tasks, 0.65-1.00 on Cooking, and 0.79 on Cleaning, markedly outperforming Full History (0.47, 0.18, 0.05) <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>AriGraph with Room obs achieves NetHack score 593.00±202.62 and levels 6.33±2.31, comparable to NetPlay with Level obs (memory oracle) at 675.33±130.27 and 7.33±1.15 <a href="../results/extraction-result-2837.html#e2837.6" class="evidence-link">[e2837.6]</a> </li>
    <li>GATA with learned belief graphs achieves +81.6% average improvement over text-only baselines and generalizes better across unseen games than counting-based episodic memory <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> <a href="../results/extraction-result-2862.html#e2862.4" class="evidence-link">[e2862.4]</a> </li>
    <li>NAIL with explicit knowledge graph (map/object registry) achieves 4.9% completion across 32 Jericho games with zero training, outperforming learned single-game agents on generalization <a href="../results/extraction-result-2858.html#e2858.0" class="evidence-link">[e2858.0]</a> <a href="../results/extraction-result-2851.html#e2851.0" class="evidence-link">[e2851.0]</a> </li>
    <li>GATA-GTF with ground-truth full graphs achieves 95.0% on Level1-2 and 70.0% on Level3, demonstrating that accurate structured memory removes partial-observability bottlenecks <a href="../results/extraction-result-2862.html#e2862.2" class="evidence-link">[e2862.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 5: Law 5</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory &#8594; uses &#8594; compression or summarization<span style="color: #888888;">, and</span></div>
        <div>&#8226; compression &#8594; preserves &#8594; task-relevant information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; maintains &#8594; performance while reducing token usage<span style="color: #888888;">, and</span></div>
        <div>&#8226; effectiveness &#8594; depends on &#8594; compression quality and task requirements</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AGA Social Memory compressing retrieved events from ~2000 tokens to ~100 tokens maintains performance while reducing token cost to 31.1% of baseline in 3-person town <a href="../results/extraction-result-2821.html#e2821.1" class="evidence-link">[e2821.1]</a> </li>
    <li>AvalonBench recursive summarization enables handling ~5000-word dialogues within context limits, though it doesn't prevent strategic failures <a href="../results/extraction-result-2869.html#e2869.0" class="evidence-link">[e2869.0]</a> <a href="../results/extraction-result-2879.html#e2879.0" class="evidence-link">[e2879.0]</a> </li>
    <li>GITM summarizing N=5 successful sequences into canonical reference plans improves robustness and sample efficiency for long-horizon tasks <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> </li>
    <li>DEPS distilling event-level descriptions and LLM explanations into prompt history improves plan repair; more re-planning rounds consistently raise success rates <a href="../results/extraction-result-2880.html#e2880.0" class="evidence-link">[e2880.0]</a> </li>
    <li>Werewolf agent reflection mechanism (QA-based summarization) combined with experience pool improves decision quality over raw history <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 6: Law 6</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory update &#8594; occurs &#8594; after successful task completion<span style="color: #888888;">, and</span></div>
        <div>&#8226; update &#8594; stores &#8594; verified successful strategies</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; memory quality &#8594; is higher than &#8594; continuous or failure-triggered updates<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; avoids &#8594; storing invalid or incomplete strategies</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Sweet&Sour transfers short-term buffer to long-term only after task completion or attempt end, ensuring complete trajectories are stored <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>GITM appends successful action sequences only after sub-goal completion and summarizes after N=5 successes to create robust reference plans <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> </li>
    <li>Voyager stores skills only after successful execution and validation, enabling reliable skill reuse <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> </li>
    <li>ExpeL filters trajectories by reward (r=1 or r>=2/3) before storing, keeping only high-quality examples <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>AGA Lifestyle Policy stores plan→action→condition graphs only after successful execution, enabling reliable cached policy reuse <a href="../results/extraction-result-2821.html#e2821.1" class="evidence-link">[e2821.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 7: Law 7</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; has &#8594; larger base LLM<span style="color: #888888;">, and</span></div>
        <div>&#8226; memory system &#8594; is &#8594; identical across model sizes</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; memory benefits &#8594; are greater for &#8594; larger models<span style="color: #888888;">, and</span></div>
        <div>&#8226; smaller models &#8594; show &#8594; larger relative gains from memory augmentation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Sweet&Sour shows larger absolute gains for GPT-4o (54.6% vs 36.0%, +18.6pp) than Llama 3.1 8B (32.5% vs 20.5%, +12pp), but larger relative improvement for smaller model <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>SAGE memory optimization raises low-performing open-source models dramatically (Qwen-1.8B and CodeLlama-7B show large absolute gains) <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
    <li>AGENTBOARD shows GPT-4 with context-window memory achieves 70.0% progress vs 23.8% for Llama2-70b, indicating larger models use memory more effectively <a href="../results/extraction-result-2836.html#e2836.0" class="evidence-link">[e2836.0]</a> <a href="../results/extraction-result-2836.html#e2836.1" class="evidence-link">[e2836.1]</a> <a href="../results/extraction-result-2836.html#e2836.2" class="evidence-link">[e2836.2]</a> </li>
    <li>AgentLM fine-tuning with trajectory memory shows that AgentLM-70B is comparable to GPT-3.5 on unseen tasks, with up to +176% improvement over base Llama-2-70B <a href="../results/extraction-result-2871.html#e2871.0" class="evidence-link">[e2871.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>An agent with a three-tier memory system (immediate working memory for current step, episodic buffer for recent trial, and consolidated long-term semantic memory for cross-trial knowledge) will outperform dual-memory systems on tasks requiring both immediate reactivity and long-term learning, particularly on tasks with >100 steps.</li>
                <li>Agents that dynamically adjust the ratio of positive-to-negative experiences in memory based on current task success rate (storing more failures when struggling, more successes when succeeding) will achieve 5-10% better performance than fixed-ratio systems on tasks with varying difficulty.</li>
                <li>Memory systems that use hierarchical retrieval (first retrieving by semantic category/task-type, then by specific instance similarity) will reduce retrieval time by 40-60% for memory stores with >1000 entries while maintaining or improving task performance.</li>
                <li>Agents that compress successful multi-step sequences into reusable 'macro-actions' or 'skills' stored in memory will solve complex tasks with 30-50% fewer LLM calls than agents that replay full sequences, as demonstrated by Voyager and GITM patterns.</li>
                <li>Combining structured memory (knowledge graphs) with episodic memory will outperform either alone by 15-25% on tasks requiring both spatial reasoning and temporal sequence tracking, such as multi-room navigation with object manipulation.</li>
                <li>Memory systems that validate and filter experiences before storage (keeping only verified successful strategies) will achieve 10-20% higher success rates than systems that store all experiences, particularly in stochastic environments.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether memory systems that actively forget low-utility experiences using learned importance metrics (beyond simple recency or success-based filtering) will improve performance by 20%+ or harm it by removing potentially useful edge cases in text games with evolving objectives.</li>
                <li>Whether cross-task memory transfer using semantic abstractions (e.g., 'opening containers' as a general skill) will enable 50%+ faster learning on new games compared to concrete episodic traces, or whether task-specific details are too important to abstract away.</li>
                <li>Whether agents can learn meta-reasoning about when to rely on memory versus when to explore new strategies (e.g., detecting when stored strategies are failing and switching to exploration mode), and if this meta-learning improves sample efficiency by 30%+ or introduces harmful oscillation.</li>
                <li>Whether memory systems that maintain uncertainty estimates or confidence scores about stored experiences will make 25%+ better decisions in stochastic environments than systems with deterministic memory, or whether the added complexity degrades performance.</li>
                <li>Whether hierarchical memory organization with multiple levels of abstraction (raw observations → episode summaries → task-level strategies → domain principles) becomes necessary for episodes >1000 steps, or whether flat retrieval with good indexing suffices.</li>
                <li>Whether memory compression using learned neural summarization will preserve task-relevant information better than LLM-based summarization, achieving 20%+ better performance, or whether LLM summarization's semantic understanding is irreplaceable.</li>
                <li>Whether memory systems that explicitly model other agents' beliefs and actions (theory-of-mind memory) will improve performance in multi-agent games by 40%+ compared to self-only memory, or whether the added complexity outweighs benefits.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that agents with only long-term memory (no short-term context) perform as well as or better than dual-memory systems on long-horizon tasks would challenge the necessity of the dual-process architecture and suggest that retrieval quality alone is sufficient.</li>
                <li>Demonstrating that random retrieval from a large, diverse memory performs as well as semantic similarity retrieval would question the importance of retrieval quality and suggest that memory quantity matters more than retrieval precision.</li>
                <li>Showing that memory systems storing only failures perform better than balanced positive/negative memory across multiple benchmarks would contradict the balanced-experience principle and suggest that learning from mistakes is more important than learning from successes.</li>
                <li>Finding that larger memory capacity (>10,000 entries) consistently degrades performance compared to smaller, curated memories would challenge assumptions about memory scale benefits and suggest that memory quality trumps quantity.</li>
                <li>Demonstrating that unstructured text memory outperforms structured knowledge graphs on spatial reasoning tasks would challenge the structured memory hypothesis and suggest that LLM reasoning over text is sufficient.</li>
                <li>Finding that immediate, continuous memory updates (after every action) outperform success-gated updates would challenge the verified-strategy hypothesis and suggest that learning from all experiences, including failures, is more important.</li>
                <li>Showing that memory compression always degrades performance compared to storing full trajectories would challenge the compression hypothesis and suggest that detail preservation is critical regardless of token cost.</li>
                <li>Demonstrating that smaller LLMs benefit more from memory augmentation than larger LLMs (opposite of current findings) would challenge the model-size interaction hypothesis and suggest that memory compensates for model limitations.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal ratio of positive to negative experiences in memory varies by task (exploration-heavy vs. execution-heavy) and model capability, but no clear formula or adaptive strategy exists <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> <a href="../results/extraction-result-2801.html#e2801.0" class="evidence-link">[e2801.0]</a> </li>
    <li>Some agents show memory benefits only above certain model sizes (e.g., reflection helps GPT-4 but not smaller models in some cases), but the capability threshold is unclear and may vary by memory type <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> <a href="../results/extraction-result-2878.html#e2878.0" class="evidence-link">[e2878.0]</a> </li>
    <li>Memory compression strategies (summarization vs full storage) show mixed results across different benchmarks - summarization helps in Avalon and AGA but may hurt in tasks requiring precise detail <a href="../results/extraction-result-2869.html#e2869.0" class="evidence-link">[e2869.0]</a> <a href="../results/extraction-result-2821.html#e2821.1" class="evidence-link">[e2821.1]</a> <a href="../results/extraction-result-2880.html#e2880.0" class="evidence-link">[e2880.0]</a> </li>
    <li>The interaction between memory architecture and specific game characteristics (stochasticity, action space size, episode length, reward sparsity) is not fully characterized - some memory types work better for certain game types <a href="../results/extraction-result-2870.html#e2870.0" class="evidence-link">[e2870.0]</a> <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> <a href="../results/extraction-result-2867.html#e2867.0" class="evidence-link">[e2867.0]</a> </li>
    <li>The optimal memory update frequency and triggers (per-step, per-episode, per-success, per-failure) vary by task but no principled framework exists for choosing update strategy <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> </li>
    <li>Context window size increases (e.g., GPT-3.5-16k) don't reliably improve performance despite more memory capacity, suggesting effective use of long context is a separate challenge <a href="../results/extraction-result-2836.html#e2836.0" class="evidence-link">[e2836.0]</a> </li>
    <li>The relationship between memory retrieval quality and task performance is non-linear - semantic retrieval helps but the magnitude of improvement varies widely (from 5% to 40%) across tasks <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> <a href="../results/extraction-result-2876.html#e2876.0" class="evidence-link">[e2876.0]</a> <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> </li>
    <li>Memory systems that work well in single-agent settings may fail in multi-agent settings due to need for theory-of-mind and modeling other agents' knowledge <a href="../results/extraction-result-2869.html#e2869.0" class="evidence-link">[e2869.0]</a> <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> <a href="../results/extraction-result-2861.html#e2861.0" class="evidence-link">[e2861.0]</a> </li>
    <li>The trade-off between memory interpretability (structured graphs) and flexibility (unstructured text) is not well understood - structured memory helps some tasks but may be too rigid for others <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> <a href="../results/extraction-result-2862.html#e2862.0" class="evidence-link">[e2862.0]</a> </li>
    <li>Memory forgetting mechanisms beyond simple recency (e.g., importance-based, utility-based) are underexplored - SAGE uses Ebbinghaus-inspired retention but optimal forgetting strategies are unclear <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Related work on reflective memory for agents, but focuses only on failure-based learning and doesn't propose dual-process architecture]</li>
    <li>Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Related work on memory for LLM agents with recency/relevance/importance retrieval, but focused on social simulation rather than task-solving and doesn't distinguish short-term vs long-term processes]</li>
    <li>Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Related work on skill libraries as persistent memory, but focused on embodied agents in Minecraft and doesn't propose general dual-process theory]</li>
    <li>Ammanabrolu & Hausknecht (2020) Graph Constrained Reinforcement Learning for Natural Language Action Spaces [Related work on structured memory for text games, but uses hand-crafted graphs rather than learned dual-process systems]</li>
    <li>Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [Related work on interleaving reasoning and acting, uses prompt-based working memory but doesn't propose persistent long-term memory architecture]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Memory Architecture Theory for Text Game Agents",
    "theory_description": "LLM-based agents achieve optimal performance in text games through a dual-process memory architecture that combines fast, reactive short-term memory (for immediate context and recent interactions) with slow, deliberative long-term memory (for accumulated experience and strategic knowledge). The effectiveness depends on: (1) the quality of information flow between these systems, (2) the agent's ability to selectively retrieve and apply relevant memories based on task demands, (3) the balance of positive and negative experiences stored, and (4) the compression and organization strategy used for long-term storage. Performance gains are particularly pronounced for complex, long-horizon tasks and scale with model capability.",
    "theory_statements": [
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses only",
                        "object": "short-term working memory"
                    },
                    {
                        "subject": "short-term memory",
                        "relation": "contains",
                        "object": "recent observations and actions"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "immediate context tracking"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "basic task completion on simple tasks"
                    },
                    {
                        "subject": "performance",
                        "relation": "degrades significantly on",
                        "object": "long-horizon and complex planning tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SmartPlay agents using prompt-history memory succeed on Bandits and Rock-Paper-Scissors but fail on Tower of Hanoi, Crafter, and Minecraft with scores far below human baselines",
                        "uuids": [
                            "e2806.0"
                        ]
                    },
                    {
                        "text": "EMMA uses 3-frame temporal buffer which helps with movement dynamics but struggles on S3 dynamics-based disambiguation tasks (22% training, 10% test)",
                        "uuids": [
                            "e2855.0"
                        ]
                    },
                    {
                        "text": "ChatGPT with prompt-based memory achieves score 15.0 in Zork with explicit previous-action reminders but exhibits failures in world modeling and SLAM-like mapping",
                        "uuids": [
                            "e2848.0"
                        ]
                    },
                    {
                        "text": "SWIFT-only with sliding-window K=10 memory achieves 49.22 on ScienceWorld but fails to generalize to exceptions and unseen variations",
                        "uuids": [
                            "e2878.1"
                        ]
                    },
                    {
                        "text": "AGENTBOARD reflex agents with context-window memory show that most open-weight models plateau early in long interactions (Llama2-70b: 23.8% progress, 4.5% success overall)",
                        "uuids": [
                            "e2836.0",
                            "e2836.2"
                        ]
                    },
                    {
                        "text": "CALM GPT-2 with short sliding-window context achieves only 9.4% average normalized score on Jericho games",
                        "uuids": [
                            "e2850.0"
                        ]
                    },
                    {
                        "text": "ReAct baseline without persistent memory achieves only 40.0% on ALFWorld and 28.0% on HotpotQA",
                        "uuids": [
                            "e2874.2",
                            "e2801.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "combines",
                        "object": "short-term and long-term memory"
                    },
                    {
                        "subject": "long-term memory",
                        "relation": "stores",
                        "object": "successful experiences and reflections"
                    },
                    {
                        "subject": "agent",
                        "relation": "retrieves",
                        "object": "relevant long-term memories for current context"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "substantial performance improvement over short-term only"
                    },
                    {
                        "subject": "improvement",
                        "relation": "is greater for",
                        "object": "complex multi-step tasks and smaller LLMs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Sweet&Sour with dual-buffer memory (short-term + long-term) achieves 54.6% on ScienceWorld with GPT-4o vs 36.0% baseline ReAct, 44.6% with Mistral Large 2 vs 24.8% baseline, and 32.5% with Llama 3.1 8B vs 20.5% baseline",
                        "uuids": [
                            "e2817.1"
                        ]
                    },
                    {
                        "text": "SAGE with dual memory (STM for trajectory, LTM for reflections) yields +17.3% improvement on ALFWorld task completion (73.8% vs 56.5% baseline) and +14.8% on step completion",
                        "uuids": [
                            "e2825.0"
                        ]
                    },
                    {
                        "text": "ExpeL combining episodic trajectories and semantic insights achieves 59.0% on ALFWorld vs 40.0% ReAct baseline and 70% on FEVER transfer vs 63% ReAct",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "GITM with text-based memory storing successful action sequences achieves 67.5% diamond success vs 35.0% without memory, and 95.0% iron_pickaxe vs 57.5% without memory",
                        "uuids": [
                            "e2805.0"
                        ]
                    },
                    {
                        "text": "Voyager with persistent skill library unlocks all 262 Overworld items and achieves 67.5% ObtainDiamond success, +47.5% over prior methods",
                        "uuids": [
                            "e2832.0"
                        ]
                    },
                    {
                        "text": "ReAct + Reflexion with episodic long-term memory completed 130/134 ALFWorld tasks (97.0%) with 22% absolute improvement over baseline",
                        "uuids": [
                            "e2801.0"
                        ]
                    },
                    {
                        "text": "SWIFTSAGE combining SWIFT local memory with SAGE LLM planning achieves 84.7 on ScienceWorld vs 49.2 SWIFT-only and 33.82 SayCan baseline",
                        "uuids": [
                            "e2878.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory content",
                        "relation": "includes",
                        "object": "both successful and failed experiences"
                    },
                    {
                        "subject": "agent",
                        "relation": "uses",
                        "object": "balanced positive and negative examples"
                    }
                ],
                "then": [
                    {
                        "subject": "agent performance",
                        "relation": "exceeds",
                        "object": "failure-only memory systems"
                    },
                    {
                        "subject": "improvement",
                        "relation": "is especially large for",
                        "object": "smaller LLMs and early-success scenarios"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Sweet&Sour storing both successes and failures outperforms Reflexion (failures only) across all models: Llama 3.1 8B 32.5% vs 21.7%, Mistral Large 2 44.6% vs 27.6%, GPT-4o 54.6% vs 45.3%",
                        "uuids": [
                            "e2817.1",
                            "e2817.0"
                        ]
                    },
                    {
                        "text": "Ablation removing positive experiences from Sweet&Sour drops performance substantially: Llama 3.1 8B to 24.6%, Mistral to 31.1%, GPT-4o to 44.9%",
                        "uuids": [
                            "e2817.1"
                        ]
                    },
                    {
                        "text": "ExpeL gathering diverse success/fail pairs with Reflexion outperforms gathering with ReAct only, and experience quantity/diversity matters for performance",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "GITM memory storing successful sequences for sub-goals provides large gains when sequences are summarized into canonical reference plans after N=5 successes",
                        "uuids": [
                            "e2805.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory",
                        "relation": "uses",
                        "object": "semantic similarity retrieval"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "is based on",
                        "object": "task-relevant embeddings"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "retrieves",
                        "object": "more relevant memories"
                    },
                    {
                        "subject": "performance",
                        "relation": "improves over",
                        "object": "random or recency-only retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ExpeL task-similarity retrieval (Faiss embedding kNN) outperforms reasoning-similarity and random sampling; random sampling dropped performance significantly",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "LoTa-Bench semantic similarity sampling achieves 43.25% avg subgoal success vs baseline 40.82% on WAH-NL with LLaMA-1 65B",
                        "uuids": [
                            "e2876.0"
                        ]
                    },
                    {
                        "text": "Voyager skill library uses text-embedding-ada-002 for semantic retrieval to match current tasks to stored skills, enabling zero-shot generalization",
                        "uuids": [
                            "e2832.0"
                        ]
                    },
                    {
                        "text": "AriGraph semantic search using Contriever embeddings + BFS-like graph expansion outperforms full history, summarization, and RAG baselines on TextWorld tasks",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "Werewolf agent using SentenceBERT semantic similarity (cosine &gt; 0.85) for experience retrieval improves villager winning rate with 10-20 round experience pools",
                        "uuids": [
                            "e2872.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory",
                        "relation": "uses",
                        "object": "structured representation"
                    },
                    {
                        "subject": "structure",
                        "relation": "is",
                        "object": "knowledge graph or hierarchical organization"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "spatial reasoning or multi-hop inference"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "better performance than unstructured memory"
                    },
                    {
                        "subject": "memory",
                        "relation": "enables",
                        "object": "more efficient retrieval and reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AriGraph hybrid semantic+episodic knowledge graph achieves normalized scores of 1.00 on Treasure Hunt tasks, 0.65-1.00 on Cooking, and 0.79 on Cleaning, markedly outperforming Full History (0.47, 0.18, 0.05)",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "AriGraph with Room obs achieves NetHack score 593.00±202.62 and levels 6.33±2.31, comparable to NetPlay with Level obs (memory oracle) at 675.33±130.27 and 7.33±1.15",
                        "uuids": [
                            "e2837.6"
                        ]
                    },
                    {
                        "text": "GATA with learned belief graphs achieves +81.6% average improvement over text-only baselines and generalizes better across unseen games than counting-based episodic memory",
                        "uuids": [
                            "e2862.0",
                            "e2862.4"
                        ]
                    },
                    {
                        "text": "NAIL with explicit knowledge graph (map/object registry) achieves 4.9% completion across 32 Jericho games with zero training, outperforming learned single-game agents on generalization",
                        "uuids": [
                            "e2858.0",
                            "e2851.0"
                        ]
                    },
                    {
                        "text": "GATA-GTF with ground-truth full graphs achieves 95.0% on Level1-2 and 70.0% on Level3, demonstrating that accurate structured memory removes partial-observability bottlenecks",
                        "uuids": [
                            "e2862.2"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory",
                        "relation": "uses",
                        "object": "compression or summarization"
                    },
                    {
                        "subject": "compression",
                        "relation": "preserves",
                        "object": "task-relevant information"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "maintains",
                        "object": "performance while reducing token usage"
                    },
                    {
                        "subject": "effectiveness",
                        "relation": "depends on",
                        "object": "compression quality and task requirements"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AGA Social Memory compressing retrieved events from ~2000 tokens to ~100 tokens maintains performance while reducing token cost to 31.1% of baseline in 3-person town",
                        "uuids": [
                            "e2821.1"
                        ]
                    },
                    {
                        "text": "AvalonBench recursive summarization enables handling ~5000-word dialogues within context limits, though it doesn't prevent strategic failures",
                        "uuids": [
                            "e2869.0",
                            "e2879.0"
                        ]
                    },
                    {
                        "text": "GITM summarizing N=5 successful sequences into canonical reference plans improves robustness and sample efficiency for long-horizon tasks",
                        "uuids": [
                            "e2805.0"
                        ]
                    },
                    {
                        "text": "DEPS distilling event-level descriptions and LLM explanations into prompt history improves plan repair; more re-planning rounds consistently raise success rates",
                        "uuids": [
                            "e2880.0"
                        ]
                    },
                    {
                        "text": "Werewolf agent reflection mechanism (QA-based summarization) combined with experience pool improves decision quality over raw history",
                        "uuids": [
                            "e2872.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory update",
                        "relation": "occurs",
                        "object": "after successful task completion"
                    },
                    {
                        "subject": "update",
                        "relation": "stores",
                        "object": "verified successful strategies"
                    }
                ],
                "then": [
                    {
                        "subject": "memory quality",
                        "relation": "is higher than",
                        "object": "continuous or failure-triggered updates"
                    },
                    {
                        "subject": "agent",
                        "relation": "avoids",
                        "object": "storing invalid or incomplete strategies"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Sweet&Sour transfers short-term buffer to long-term only after task completion or attempt end, ensuring complete trajectories are stored",
                        "uuids": [
                            "e2817.1"
                        ]
                    },
                    {
                        "text": "GITM appends successful action sequences only after sub-goal completion and summarizes after N=5 successes to create robust reference plans",
                        "uuids": [
                            "e2805.0"
                        ]
                    },
                    {
                        "text": "Voyager stores skills only after successful execution and validation, enabling reliable skill reuse",
                        "uuids": [
                            "e2832.0"
                        ]
                    },
                    {
                        "text": "ExpeL filters trajectories by reward (r=1 or r&gt;=2/3) before storing, keeping only high-quality examples",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "AGA Lifestyle Policy stores plan→action→condition graphs only after successful execution, enabling reliable cached policy reuse",
                        "uuids": [
                            "e2821.1"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "agent",
                        "relation": "has",
                        "object": "larger base LLM"
                    },
                    {
                        "subject": "memory system",
                        "relation": "is",
                        "object": "identical across model sizes"
                    }
                ],
                "then": [
                    {
                        "subject": "memory benefits",
                        "relation": "are greater for",
                        "object": "larger models"
                    },
                    {
                        "subject": "smaller models",
                        "relation": "show",
                        "object": "larger relative gains from memory augmentation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Sweet&Sour shows larger absolute gains for GPT-4o (54.6% vs 36.0%, +18.6pp) than Llama 3.1 8B (32.5% vs 20.5%, +12pp), but larger relative improvement for smaller model",
                        "uuids": [
                            "e2817.1"
                        ]
                    },
                    {
                        "text": "SAGE memory optimization raises low-performing open-source models dramatically (Qwen-1.8B and CodeLlama-7B show large absolute gains)",
                        "uuids": [
                            "e2825.0"
                        ]
                    },
                    {
                        "text": "AGENTBOARD shows GPT-4 with context-window memory achieves 70.0% progress vs 23.8% for Llama2-70b, indicating larger models use memory more effectively",
                        "uuids": [
                            "e2836.0",
                            "e2836.1",
                            "e2836.2"
                        ]
                    },
                    {
                        "text": "AgentLM fine-tuning with trajectory memory shows that AgentLM-70B is comparable to GPT-3.5 on unseen tasks, with up to +176% improvement over base Llama-2-70B",
                        "uuids": [
                            "e2871.0"
                        ]
                    }
                ]
            }
        }
    ],
    "new_predictions_likely": [
        "An agent with a three-tier memory system (immediate working memory for current step, episodic buffer for recent trial, and consolidated long-term semantic memory for cross-trial knowledge) will outperform dual-memory systems on tasks requiring both immediate reactivity and long-term learning, particularly on tasks with &gt;100 steps.",
        "Agents that dynamically adjust the ratio of positive-to-negative experiences in memory based on current task success rate (storing more failures when struggling, more successes when succeeding) will achieve 5-10% better performance than fixed-ratio systems on tasks with varying difficulty.",
        "Memory systems that use hierarchical retrieval (first retrieving by semantic category/task-type, then by specific instance similarity) will reduce retrieval time by 40-60% for memory stores with &gt;1000 entries while maintaining or improving task performance.",
        "Agents that compress successful multi-step sequences into reusable 'macro-actions' or 'skills' stored in memory will solve complex tasks with 30-50% fewer LLM calls than agents that replay full sequences, as demonstrated by Voyager and GITM patterns.",
        "Combining structured memory (knowledge graphs) with episodic memory will outperform either alone by 15-25% on tasks requiring both spatial reasoning and temporal sequence tracking, such as multi-room navigation with object manipulation.",
        "Memory systems that validate and filter experiences before storage (keeping only verified successful strategies) will achieve 10-20% higher success rates than systems that store all experiences, particularly in stochastic environments."
    ],
    "new_predictions_unknown": [
        "Whether memory systems that actively forget low-utility experiences using learned importance metrics (beyond simple recency or success-based filtering) will improve performance by 20%+ or harm it by removing potentially useful edge cases in text games with evolving objectives.",
        "Whether cross-task memory transfer using semantic abstractions (e.g., 'opening containers' as a general skill) will enable 50%+ faster learning on new games compared to concrete episodic traces, or whether task-specific details are too important to abstract away.",
        "Whether agents can learn meta-reasoning about when to rely on memory versus when to explore new strategies (e.g., detecting when stored strategies are failing and switching to exploration mode), and if this meta-learning improves sample efficiency by 30%+ or introduces harmful oscillation.",
        "Whether memory systems that maintain uncertainty estimates or confidence scores about stored experiences will make 25%+ better decisions in stochastic environments than systems with deterministic memory, or whether the added complexity degrades performance.",
        "Whether hierarchical memory organization with multiple levels of abstraction (raw observations → episode summaries → task-level strategies → domain principles) becomes necessary for episodes &gt;1000 steps, or whether flat retrieval with good indexing suffices.",
        "Whether memory compression using learned neural summarization will preserve task-relevant information better than LLM-based summarization, achieving 20%+ better performance, or whether LLM summarization's semantic understanding is irreplaceable.",
        "Whether memory systems that explicitly model other agents' beliefs and actions (theory-of-mind memory) will improve performance in multi-agent games by 40%+ compared to self-only memory, or whether the added complexity outweighs benefits."
    ],
    "negative_experiments": [
        "Finding that agents with only long-term memory (no short-term context) perform as well as or better than dual-memory systems on long-horizon tasks would challenge the necessity of the dual-process architecture and suggest that retrieval quality alone is sufficient.",
        "Demonstrating that random retrieval from a large, diverse memory performs as well as semantic similarity retrieval would question the importance of retrieval quality and suggest that memory quantity matters more than retrieval precision.",
        "Showing that memory systems storing only failures perform better than balanced positive/negative memory across multiple benchmarks would contradict the balanced-experience principle and suggest that learning from mistakes is more important than learning from successes.",
        "Finding that larger memory capacity (&gt;10,000 entries) consistently degrades performance compared to smaller, curated memories would challenge assumptions about memory scale benefits and suggest that memory quality trumps quantity.",
        "Demonstrating that unstructured text memory outperforms structured knowledge graphs on spatial reasoning tasks would challenge the structured memory hypothesis and suggest that LLM reasoning over text is sufficient.",
        "Finding that immediate, continuous memory updates (after every action) outperform success-gated updates would challenge the verified-strategy hypothesis and suggest that learning from all experiences, including failures, is more important.",
        "Showing that memory compression always degrades performance compared to storing full trajectories would challenge the compression hypothesis and suggest that detail preservation is critical regardless of token cost.",
        "Demonstrating that smaller LLMs benefit more from memory augmentation than larger LLMs (opposite of current findings) would challenge the model-size interaction hypothesis and suggest that memory compensates for model limitations."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal ratio of positive to negative experiences in memory varies by task (exploration-heavy vs. execution-heavy) and model capability, but no clear formula or adaptive strategy exists",
            "uuids": [
                "e2817.1",
                "e2874.0",
                "e2801.0"
            ]
        },
        {
            "text": "Some agents show memory benefits only above certain model sizes (e.g., reflection helps GPT-4 but not smaller models in some cases), but the capability threshold is unclear and may vary by memory type",
            "uuids": [
                "e2825.0",
                "e2817.1",
                "e2878.0"
            ]
        },
        {
            "text": "Memory compression strategies (summarization vs full storage) show mixed results across different benchmarks - summarization helps in Avalon and AGA but may hurt in tasks requiring precise detail",
            "uuids": [
                "e2869.0",
                "e2821.1",
                "e2880.0"
            ]
        },
        {
            "text": "The interaction between memory architecture and specific game characteristics (stochasticity, action space size, episode length, reward sparsity) is not fully characterized - some memory types work better for certain game types",
            "uuids": [
                "e2870.0",
                "e2862.0",
                "e2867.0"
            ]
        },
        {
            "text": "The optimal memory update frequency and triggers (per-step, per-episode, per-success, per-failure) vary by task but no principled framework exists for choosing update strategy",
            "uuids": [
                "e2817.1",
                "e2805.0",
                "e2832.0"
            ]
        },
        {
            "text": "Context window size increases (e.g., GPT-3.5-16k) don't reliably improve performance despite more memory capacity, suggesting effective use of long context is a separate challenge",
            "uuids": [
                "e2836.0"
            ]
        },
        {
            "text": "The relationship between memory retrieval quality and task performance is non-linear - semantic retrieval helps but the magnitude of improvement varies widely (from 5% to 40%) across tasks",
            "uuids": [
                "e2874.0",
                "e2876.0",
                "e2872.0"
            ]
        },
        {
            "text": "Memory systems that work well in single-agent settings may fail in multi-agent settings due to need for theory-of-mind and modeling other agents' knowledge",
            "uuids": [
                "e2869.0",
                "e2872.0",
                "e2861.0"
            ]
        },
        {
            "text": "The trade-off between memory interpretability (structured graphs) and flexibility (unstructured text) is not well understood - structured memory helps some tasks but may be too rigid for others",
            "uuids": [
                "e2837.0",
                "e2862.0"
            ]
        },
        {
            "text": "Memory forgetting mechanisms beyond simple recency (e.g., importance-based, utility-based) are underexplored - SAGE uses Ebbinghaus-inspired retention but optimal forgetting strategies are unclear",
            "uuids": [
                "e2825.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Reflexion (failure-only memory) sometimes matches or exceeds balanced memory systems on certain tasks (ALFWorld 97% completion), while Sweet&Sour shows balanced memory is better - may depend on task type and whether early successes are common",
            "uuids": [
                "e2801.0",
                "e2817.1",
                "e2882.1"
            ]
        },
        {
            "text": "Some studies show larger context windows don't improve performance (GPT-3.5-16k vs GPT-3.5 in AGENTBOARD), while others show context is critical (ChatGPT with history vs without in Zork) - may depend on whether models can effectively use long context",
            "uuids": [
                "e2836.0",
                "e2848.0"
            ]
        },
        {
            "text": "Fine-tuned procedural memory dramatically helps in-domain (LLaMA-1 30B: 13.66% → 60.08% on ALFRED) but hurts cross-domain transfer, while retrieval-based memory shows opposite pattern - suggests fundamental trade-off between specialization and generalization",
            "uuids": [
                "e2876.0",
                "e2871.0"
            ]
        },
        {
            "text": "Structured memory (knowledge graphs) outperforms unstructured in some cases (AriGraph on TextWorld) but unstructured may be more flexible - GATA shows learned graphs help but ground-truth graphs are much better, suggesting structure quality matters more than structure itself",
            "uuids": [
                "e2837.0",
                "e2862.0",
                "e2862.2"
            ]
        },
        {
            "text": "Memory compression helps in some cases (AGA reduces tokens to 31.1% while maintaining performance) but may hurt in others (AvalonBench summarization doesn't prevent strategic failures) - may depend on whether compression preserves task-critical details",
            "uuids": [
                "e2821.1",
                "e2869.0"
            ]
        },
        {
            "text": "Smaller models sometimes benefit more from memory (Sweet&Sour shows larger relative gains for Llama 3.1 8B) but larger models achieve higher absolute performance with memory - unclear whether memory is more important for capability-limited models or capability-rich models",
            "uuids": [
                "e2817.1",
                "e2825.0",
                "e2836.1"
            ]
        },
        {
            "text": "Discussion/shared memory helps cooperation in some multi-agent settings (Hoodwinked: 55% vs 33% killer banishment with discussion) but can enable deception that harms eyewitnesses (82% → 70% accuracy) - memory sharing has complex effects in adversarial settings",
            "uuids": [
                "e2861.0"
            ]
        }
    ],
    "special_cases": [
        "In highly stochastic environments (e.g., stochastic Jericho games), episodic memory may need to store probability distributions or multiple outcomes rather than single trajectories to be effective, as shown by XTX's need for diverse experience replay.",
        "For tasks with very long episodes (&gt;1000 steps), hierarchical memory organization with multiple levels of abstraction becomes necessary to avoid retrieval bottlenecks and maintain coherent long-term strategies, as suggested by DEPS and GITM architectures.",
        "In multi-agent settings, memory must account for other agents' actions, beliefs, and potential deception, requiring theory-of-mind extensions beyond self-only memory, as demonstrated by Avalon and Werewolf experiments.",
        "For procedurally generated environments, memory must generalize across instances rather than memorizing specific layouts, requiring abstraction mechanisms like GATA's learned belief graphs or Voyager's skill library.",
        "In tasks with sparse rewards and bottleneck states (e.g., Zork1 'Troll Room'), memory systems need explicit mechanisms to return to promising frontiers, as shown by XTX's exploitation-exploration phases.",
        "For tasks requiring precise detail retention (e.g., exact object locations, specific action sequences), compression may be harmful and full trajectory storage may be necessary despite token costs.",
        "In zero-shot or few-shot settings, retrieval-based memory is more effective than fine-tuned memory for cross-domain transfer, but fine-tuning dominates for in-domain performance.",
        "For smaller LLMs (&lt;13B parameters), memory augmentation may be necessary to achieve reasonable performance, while larger LLMs (&gt;70B) can sometimes succeed with minimal memory.",
        "In adversarial or deceptive settings (social deduction games), memory of other agents' statements and actions must be carefully managed to avoid being misled by false information.",
        "For tasks with complex precondition-effect relationships, structured memory (knowledge graphs) that explicitly represents these relationships outperforms unstructured memory that relies on implicit LLM reasoning."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Related work on reflective memory for agents, but focuses only on failure-based learning and doesn't propose dual-process architecture]",
            "Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Related work on memory for LLM agents with recency/relevance/importance retrieval, but focused on social simulation rather than task-solving and doesn't distinguish short-term vs long-term processes]",
            "Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Related work on skill libraries as persistent memory, but focused on embodied agents in Minecraft and doesn't propose general dual-process theory]",
            "Ammanabrolu & Hausknecht (2020) Graph Constrained Reinforcement Learning for Natural Language Action Spaces [Related work on structured memory for text games, but uses hand-crafted graphs rather than learned dual-process systems]",
            "Yao et al. (2022) ReAct: Synergizing Reasoning and Acting in Language Models [Related work on interleaving reasoning and acting, uses prompt-based working memory but doesn't propose persistent long-term memory architecture]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
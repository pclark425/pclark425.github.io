<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Demonstration-Model Interaction Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1624</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1624</p>
                <p><strong>Name:</strong> Prompt-Demonstration-Model Interaction Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory posits that the accuracy of LLM-based scientific simulation is governed by a three-way interaction between (1) the structure of prompts and demonstrations, (2) the epistemic norms and representational conventions of the scientific subdomain, and (3) the internal knowledge representations and priors of the LLM. The theory asserts the existence of a critical alignment threshold: only when prompt and demonstration structure are sufficiently aligned with both the subdomain's epistemic norms and the LLM's internal representations can high-fidelity simulation occur. Otherwise, even large or well-trained models will fail to simulate accurately. The theory further predicts that this threshold is not a simple linear function of model size or prompt complexity, but a nonlinear, possibly discontinuous, function of alignment.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Critical Alignment Threshold Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt and demonstration structure &#8594; is_aligned_with &#8594; LLM internal representations<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt and demonstration structure &#8594; is_aligned_with &#8594; subdomain epistemic norms</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM scientific simulation &#8594; achieves &#8594; high accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Studies show that LLMs perform best when prompts and demonstrations are both domain-appropriate and match the model's pretraining distribution. </li>
    <li>Misalignment between prompt structure and model knowledge leads to degraded performance, even with large models. </li>
    <li>Prompt engineering literature demonstrates that both prompt format and content must be tailored to the model's learned priors and the target task's conventions for optimal results. </li>
    <li>Empirical results in scientific QA and simulation tasks show that domain-specific prompt templates outperform generic ones only when they match both the model's and the domain's conventions. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While related to prompt adaptation and model alignment, the theory's focus on a threshold effect and three-way interaction, specifically for scientific simulation, is new.</p>            <p><strong>What Already Exists:</strong> Prompt-model alignment and domain adaptation are known concepts in prompt engineering and transfer learning.</p>            <p><strong>What is Novel:</strong> The explicit three-way interaction and the concept of a nonlinear, thresholded alignment for scientific simulation accuracy is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt-model alignment]</li>
    <li>Gonen et al. (2022) Demystifying Prompt Engineering: A Unified Framework for Understanding and Improving Prompting [Prompt adaptation]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning alignment]</li>
</ul>
            <h3>Statement 1: Misalignment Failure Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt and demonstration structure &#8594; is_misaligned_with &#8594; LLM internal representations</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM scientific simulation &#8594; fails_to_achieve &#8594; high accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical evidence shows that even large LLMs can fail on scientific tasks if the prompt structure is unfamiliar or mismatched to the model's training. </li>
    <li>Prompt misalignment is a leading cause of hallucination and factual errors in LLM outputs, especially in technical domains. </li>
    <li>Studies in prompt engineering show that small changes in prompt phrasing or demonstration order can cause large drops in accuracy if they diverge from the model's learned priors. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law generalizes prompt misalignment effects to the context of scientific simulation, which is a new application.</p>            <p><strong>What Already Exists:</strong> Prompt misalignment is known to degrade LLM performance.</p>            <p><strong>What is Novel:</strong> The law's explicit focus on scientific simulation and the necessity of alignment with both model and domain is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Gonen et al. (2022) Demystifying Prompt Engineering: A Unified Framework for Understanding and Improving Prompting [Prompt adaptation and misalignment]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt calibration and misalignment]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a prompt is constructed to match both the LLM's pretraining distribution and the target subdomain's conventions, simulation accuracy will be maximized.</li>
                <li>If either alignment is missing, accuracy will drop, regardless of model size.</li>
                <li>Increasing model size alone will not overcome misalignment between prompt structure and subdomain epistemic norms.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>For subdomains with epistemic norms not represented in the LLM's pretraining, even perfect prompt engineering may not yield high accuracy.</li>
                <li>If LLMs are fine-tuned on a new subdomain, the alignment threshold may shift, potentially enabling new forms of prompt structure to be effective.</li>
                <li>There may exist emergent prompt structures that enable high accuracy even in the absence of explicit alignment, due to latent model capabilities.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs achieve high accuracy with prompts that are misaligned with both their pretraining and the subdomain, the theory would be falsified.</li>
                <li>If alignment with only one (model or domain) is sufficient for high accuracy, the theory's threshold claim would be weakened.</li>
                <li>If increasing model size alone consistently overcomes prompt misalignment, the theory's threshold concept would be invalidated.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Instances where LLMs generalize well to new domains with minimal alignment, possibly due to emergent capabilities. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory synthesizes prompt engineering, model alignment, and domain epistemics into a new, threshold-based framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt-model alignment]</li>
    <li>Gonen et al. (2022) Demystifying Prompt Engineering: A Unified Framework for Understanding and Improving Prompting [Prompt adaptation]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Demonstration-Model Interaction Theory",
    "theory_description": "This theory posits that the accuracy of LLM-based scientific simulation is governed by a three-way interaction between (1) the structure of prompts and demonstrations, (2) the epistemic norms and representational conventions of the scientific subdomain, and (3) the internal knowledge representations and priors of the LLM. The theory asserts the existence of a critical alignment threshold: only when prompt and demonstration structure are sufficiently aligned with both the subdomain's epistemic norms and the LLM's internal representations can high-fidelity simulation occur. Otherwise, even large or well-trained models will fail to simulate accurately. The theory further predicts that this threshold is not a simple linear function of model size or prompt complexity, but a nonlinear, possibly discontinuous, function of alignment.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Critical Alignment Threshold Law",
                "if": [
                    {
                        "subject": "prompt and demonstration structure",
                        "relation": "is_aligned_with",
                        "object": "LLM internal representations"
                    },
                    {
                        "subject": "prompt and demonstration structure",
                        "relation": "is_aligned_with",
                        "object": "subdomain epistemic norms"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM scientific simulation",
                        "relation": "achieves",
                        "object": "high accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Studies show that LLMs perform best when prompts and demonstrations are both domain-appropriate and match the model's pretraining distribution.",
                        "uuids": []
                    },
                    {
                        "text": "Misalignment between prompt structure and model knowledge leads to degraded performance, even with large models.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering literature demonstrates that both prompt format and content must be tailored to the model's learned priors and the target task's conventions for optimal results.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical results in scientific QA and simulation tasks show that domain-specific prompt templates outperform generic ones only when they match both the model's and the domain's conventions.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt-model alignment and domain adaptation are known concepts in prompt engineering and transfer learning.",
                    "what_is_novel": "The explicit three-way interaction and the concept of a nonlinear, thresholded alignment for scientific simulation accuracy is novel.",
                    "classification_explanation": "While related to prompt adaptation and model alignment, the theory's focus on a threshold effect and three-way interaction, specifically for scientific simulation, is new.",
                    "likely_classification": "new",
                    "references": [
                        "Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt-model alignment]",
                        "Gonen et al. (2022) Demystifying Prompt Engineering: A Unified Framework for Understanding and Improving Prompting [Prompt adaptation]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Misalignment Failure Law",
                "if": [
                    {
                        "subject": "prompt and demonstration structure",
                        "relation": "is_misaligned_with",
                        "object": "LLM internal representations"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM scientific simulation",
                        "relation": "fails_to_achieve",
                        "object": "high accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical evidence shows that even large LLMs can fail on scientific tasks if the prompt structure is unfamiliar or mismatched to the model's training.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt misalignment is a leading cause of hallucination and factual errors in LLM outputs, especially in technical domains.",
                        "uuids": []
                    },
                    {
                        "text": "Studies in prompt engineering show that small changes in prompt phrasing or demonstration order can cause large drops in accuracy if they diverge from the model's learned priors.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt misalignment is known to degrade LLM performance.",
                    "what_is_novel": "The law's explicit focus on scientific simulation and the necessity of alignment with both model and domain is novel.",
                    "classification_explanation": "The law generalizes prompt misalignment effects to the context of scientific simulation, which is a new application.",
                    "likely_classification": "new",
                    "references": [
                        "Gonen et al. (2022) Demystifying Prompt Engineering: A Unified Framework for Understanding and Improving Prompting [Prompt adaptation and misalignment]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt calibration and misalignment]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a prompt is constructed to match both the LLM's pretraining distribution and the target subdomain's conventions, simulation accuracy will be maximized.",
        "If either alignment is missing, accuracy will drop, regardless of model size.",
        "Increasing model size alone will not overcome misalignment between prompt structure and subdomain epistemic norms."
    ],
    "new_predictions_unknown": [
        "For subdomains with epistemic norms not represented in the LLM's pretraining, even perfect prompt engineering may not yield high accuracy.",
        "If LLMs are fine-tuned on a new subdomain, the alignment threshold may shift, potentially enabling new forms of prompt structure to be effective.",
        "There may exist emergent prompt structures that enable high accuracy even in the absence of explicit alignment, due to latent model capabilities."
    ],
    "negative_experiments": [
        "If LLMs achieve high accuracy with prompts that are misaligned with both their pretraining and the subdomain, the theory would be falsified.",
        "If alignment with only one (model or domain) is sufficient for high accuracy, the theory's threshold claim would be weakened.",
        "If increasing model size alone consistently overcomes prompt misalignment, the theory's threshold concept would be invalidated."
    ],
    "unaccounted_for": [
        {
            "text": "Instances where LLMs generalize well to new domains with minimal alignment, possibly due to emergent capabilities.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show surprising robustness to prompt misalignment in certain tasks, suggesting other factors may sometimes dominate.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For tasks that are highly generic or require only surface-level reasoning, the alignment threshold may be less critical.",
        "In cases where the LLM has been extensively fine-tuned on the target subdomain, the alignment requirement may be relaxed."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt-model alignment and domain adaptation are known, but not in this explicit three-way, thresholded form.",
        "what_is_novel": "The theory's explicit three-way interaction and threshold concept for scientific simulation is new.",
        "classification_explanation": "The theory synthesizes prompt engineering, model alignment, and domain epistemics into a new, threshold-based framework.",
        "likely_classification": "new",
        "references": [
            "Lester et al. (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Prompt-model alignment]",
            "Gonen et al. (2022) Demystifying Prompt Engineering: A Unified Framework for Understanding and Improving Prompting [Prompt adaptation]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning alignment]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-635",
    "original_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
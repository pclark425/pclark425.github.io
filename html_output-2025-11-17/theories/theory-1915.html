<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of Problem Presentation Effects - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1915</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1915</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of Problem Presentation Effects</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the format in which a problem is presented acts as an information bottleneck, modulating the accessibility and salience of relevant cues for LLMs. Formats that minimize extraneous information and highlight task-relevant features reduce the effective complexity of the problem, leading to improved performance. Conversely, formats that obscure or diffuse key information increase the cognitive load and error rate.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Salience Enhancement Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes_salience_of &#8594; task_relevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; improved_performance_on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better when key information is highlighted or placed at the end of the prompt (recency effect). </li>
    <li>Explicitly structured formats (e.g., bullet points, tables) improve LLM accuracy on extraction and reasoning tasks. </li>
    <li>Prompt engineering studies show that reordering or emphasizing relevant details increases LLM accuracy. </li>
    <li>Liu et al. (2023) found that LLMs are more accurate when relevant information is placed at the end of long contexts. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The information bottleneck framing is novel, though the empirical effect is known.</p>            <p><strong>What Already Exists:</strong> Empirical studies show that highlighting or structuring information improves LLM performance.</p>            <p><strong>What is Novel:</strong> The law frames this as a function of information bottleneck and salience, not just prompt clarity.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Salience and recency effects]</li>
    <li>Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]</li>
</ul>
            <h3>Statement 1: Extraneous Information Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; extraneous_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; reduced_performance_on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs are distracted by irrelevant details or verbose instructions, leading to lower accuracy. </li>
    <li>Experiments show that adding unrelated context to prompts can degrade LLM performance. </li>
    <li>Prompt injection and adversarial prompt studies demonstrate that extraneous or misleading information can cause LLMs to make errors. </li>
    <li>Liu et al. (2023) observed that LLMs' accuracy drops when relevant information is surrounded by irrelevant content. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The bottleneck abstraction is new, though the empirical effect is established.</p>            <p><strong>What Already Exists:</strong> Known that LLMs can be distracted by irrelevant or misleading information in prompts.</p>            <p><strong>What is Novel:</strong> The law formalizes this as an information bottleneck effect, not just a distraction.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Context and distraction effects]</li>
    <li>Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If extraneous information is systematically removed from prompts, LLM performance will increase.</li>
                <li>If key information is highlighted (e.g., bolded, placed at the end), LLMs will be more accurate on extraction tasks.</li>
                <li>If prompts are restructured to foreground relevant details, LLMs will require fewer examples to generalize.</li>
                <li>If the same problem is presented in a table versus a paragraph, the table format will yield higher accuracy for structured tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If information is presented in a highly compressed but unfamiliar format, LLMs may either improve or degrade in performance depending on their ability to parse the format.</li>
                <li>If LLMs are trained to ignore extraneous information, they may generalize better to noisy real-world data.</li>
                <li>If LLMs are exposed to adversarially structured prompts that mimic salience but are misleading, performance may degrade unpredictably.</li>
                <li>If LLMs are given prompts with multiple salient cues, their performance may depend on the model's internal attention mechanisms in ways not yet characterized.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well regardless of the amount of extraneous information, the theory would be falsified.</li>
                <li>If LLMs are not affected by the salience or position of key information, the salience enhancement law would be called into question.</li>
                <li>If LLMs perform better with more verbose or less structured prompts, the information bottleneck theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs ignore extraneous information due to scale or advanced attention mechanisms are not fully explained. </li>
    <li>Some LLMs may have been trained on noisy data and thus be robust to certain types of extraneous information. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes prompt format effects into an information-theoretic bottleneck framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Salience and context effects]</li>
    <li>Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of Problem Presentation Effects",
    "theory_description": "This theory posits that the format in which a problem is presented acts as an information bottleneck, modulating the accessibility and salience of relevant cues for LLMs. Formats that minimize extraneous information and highlight task-relevant features reduce the effective complexity of the problem, leading to improved performance. Conversely, formats that obscure or diffuse key information increase the cognitive load and error rate.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Salience Enhancement Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes_salience_of",
                        "object": "task_relevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "improved_performance_on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better when key information is highlighted or placed at the end of the prompt (recency effect).",
                        "uuids": []
                    },
                    {
                        "text": "Explicitly structured formats (e.g., bullet points, tables) improve LLM accuracy on extraction and reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering studies show that reordering or emphasizing relevant details increases LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Liu et al. (2023) found that LLMs are more accurate when relevant information is placed at the end of long contexts.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical studies show that highlighting or structuring information improves LLM performance.",
                    "what_is_novel": "The law frames this as a function of information bottleneck and salience, not just prompt clarity.",
                    "classification_explanation": "The information bottleneck framing is novel, though the empirical effect is known.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Salience and recency effects]",
                        "Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Extraneous Information Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "extraneous_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "reduced_performance_on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs are distracted by irrelevant details or verbose instructions, leading to lower accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Experiments show that adding unrelated context to prompts can degrade LLM performance.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt injection and adversarial prompt studies demonstrate that extraneous or misleading information can cause LLMs to make errors.",
                        "uuids": []
                    },
                    {
                        "text": "Liu et al. (2023) observed that LLMs' accuracy drops when relevant information is surrounded by irrelevant content.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Known that LLMs can be distracted by irrelevant or misleading information in prompts.",
                    "what_is_novel": "The law formalizes this as an information bottleneck effect, not just a distraction.",
                    "classification_explanation": "The bottleneck abstraction is new, though the empirical effect is established.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Context and distraction effects]",
                        "Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If extraneous information is systematically removed from prompts, LLM performance will increase.",
        "If key information is highlighted (e.g., bolded, placed at the end), LLMs will be more accurate on extraction tasks.",
        "If prompts are restructured to foreground relevant details, LLMs will require fewer examples to generalize.",
        "If the same problem is presented in a table versus a paragraph, the table format will yield higher accuracy for structured tasks."
    ],
    "new_predictions_unknown": [
        "If information is presented in a highly compressed but unfamiliar format, LLMs may either improve or degrade in performance depending on their ability to parse the format.",
        "If LLMs are trained to ignore extraneous information, they may generalize better to noisy real-world data.",
        "If LLMs are exposed to adversarially structured prompts that mimic salience but are misleading, performance may degrade unpredictably.",
        "If LLMs are given prompts with multiple salient cues, their performance may depend on the model's internal attention mechanisms in ways not yet characterized."
    ],
    "negative_experiments": [
        "If LLMs perform equally well regardless of the amount of extraneous information, the theory would be falsified.",
        "If LLMs are not affected by the salience or position of key information, the salience enhancement law would be called into question.",
        "If LLMs perform better with more verbose or less structured prompts, the information bottleneck theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs ignore extraneous information due to scale or advanced attention mechanisms are not fully explained.",
            "uuids": []
        },
        {
            "text": "Some LLMs may have been trained on noisy data and thus be robust to certain types of extraneous information.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can extract relevant information even from verbose or noisy prompts, especially at large scale.",
            "uuids": []
        },
        {
            "text": "Certain tasks (e.g., creative writing) may benefit from more verbose or less structured prompts.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very short prompts may not benefit from salience enhancement due to lack of competing information.",
        "Highly structured tasks (e.g., code completion) may be less sensitive to extraneous information.",
        "LLMs with advanced attention mechanisms may be less affected by information bottlenecks."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt structure and information salience are known to affect LLM performance.",
        "what_is_novel": "The explicit information bottleneck framing and its application to LLMs is new.",
        "classification_explanation": "The theory synthesizes prompt format effects into an information-theoretic bottleneck framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts [Salience and context effects]",
            "Zhang et al. (2022) Prompt Design for Large Language Models: A Survey [Prompt structure and performance]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
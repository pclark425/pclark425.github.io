<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Faithfulness and Inductive Bias Preservation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1280</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1280</p>
                <p><strong>Name:</strong> Structural Faithfulness and Inductive Bias Preservation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal conversion of graphs into text for language model (LM) training must preserve the structural properties and inductive biases inherent in the original graph. The textual representation should encode not only the explicit topology (nodes, edges, attributes) but also the implicit regularities, symmetries, and invariances that underpin effective graph reasoning. By maintaining these properties, the LM is more likely to learn generalizable, structure-aware representations, supporting robust transfer and compositional generalization.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structural Faithfulness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; preserves &#8594; graph topology and relational structure<span style="color: #888888;">, and</span></div>
        <div>&#8226; graph-to-text representation &#8594; encodes &#8594; node and edge attributes faithfully</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_learn &#8594; structure-aware reasoning and generalization</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that LMs trained on structurally faithful representations (e.g., canonicalized SMILES for molecules, or explicit edge lists for knowledge graphs) outperform those trained on lossy or flattened encodings in downstream graph tasks. </li>
    <li>Graph neural networks (GNNs) rely on explicit structural information to generalize; analogous effects are observed in LMs when structure is preserved in text. </li>
    <li>Loss of structural information (e.g., random node orderings, omission of edge types) leads to degraded performance in both GNNs and LMs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While structural preservation is known in GNNs, its explicit application and formalization for LM graph-to-text training is novel.</p>            <p><strong>What Already Exists:</strong> Structural preservation is a core principle in GNNs and some graph-to-sequence models.</p>            <p><strong>What is Novel:</strong> Explicitly formalizing structural faithfulness as a requirement for graph-to-text LM training is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Gilmer et al. (2017) Neural Message Passing for Quantum Chemistry [GNNs require explicit structure]</li>
    <li>Rong et al. (2020) Self-Supervised Graph Transformer on Large-Scale Molecular Data [Graph structure in transformer LMs]</li>
    <li>Schwaller et al. (2019) Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction [Canonical SMILES for structure preservation]</li>
</ul>
            <h3>Statement 1: Inductive Bias Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; encodes &#8594; graph symmetries, invariances, and motifs</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; inherits &#8594; graph inductive biases (e.g., permutation invariance, motif reuse)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GNNs leverage permutation invariance and motif-based inductive biases for generalization; LMs can only do so if these are encoded in the text. </li>
    <li>Empirical work shows that LMs trained on representations that break graph symmetries (e.g., arbitrary node orderings) fail to generalize as well as those that preserve them. </li>
    <li>Encoding motifs and invariances in text (e.g., via templates or canonical forms) improves LM performance on compositional and transfer tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends known ML principles to a new modality (graph-to-text for LMs).</p>            <p><strong>What Already Exists:</strong> Inductive bias is a well-studied concept in ML, especially in GNNs.</p>            <p><strong>What is Novel:</strong> The explicit requirement to preserve graph inductive biases in text for LM training is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Permutation invariance in GNNs]</li>
    <li>Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Inductive bias in graph learning]</li>
    <li>Rong et al. (2020) Self-Supervised Graph Transformer on Large-Scale Molecular Data [Graph inductive bias in LMs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs trained on structurally faithful, bias-preserving graph-to-text representations will outperform those trained on lossy or order-dependent encodings in tasks requiring graph reasoning.</li>
                <li>Encoding graph motifs and invariances in text will improve LM generalization to novel graph structures.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Preserving higher-order symmetries (e.g., automorphism groups) in text may enable LMs to perform zero-shot reasoning on unseen graph classes.</li>
                <li>Faithful encoding of rare or complex motifs may allow LMs to extrapolate to entirely new graph domains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs trained on structurally faithful, bias-preserving representations do not outperform those trained on lossy encodings, the theory is challenged.</li>
                <li>If LMs fail to generalize to novel graphs despite inductive bias preservation, the theory's sufficiency is questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The impact of representation length and complexity on LM capacity and efficiency is not addressed. </li>
    <li>Potential trade-offs between faithfulness and human interpretability are not considered. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known ML principles in a new context (graph-to-text for LMs).</p>
            <p><strong>References:</strong> <ul>
    <li>Gilmer et al. (2017) Neural Message Passing for Quantum Chemistry [GNNs require explicit structure]</li>
    <li>Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Inductive bias in graph learning]</li>
    <li>Rong et al. (2020) Self-Supervised Graph Transformer on Large-Scale Molecular Data [Graph structure in transformer LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "theory_description": "This theory posits that the ideal conversion of graphs into text for language model (LM) training must preserve the structural properties and inductive biases inherent in the original graph. The textual representation should encode not only the explicit topology (nodes, edges, attributes) but also the implicit regularities, symmetries, and invariances that underpin effective graph reasoning. By maintaining these properties, the LM is more likely to learn generalizable, structure-aware representations, supporting robust transfer and compositional generalization.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structural Faithfulness Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "preserves",
                        "object": "graph topology and relational structure"
                    },
                    {
                        "subject": "graph-to-text representation",
                        "relation": "encodes",
                        "object": "node and edge attributes faithfully"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_learn",
                        "object": "structure-aware reasoning and generalization"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that LMs trained on structurally faithful representations (e.g., canonicalized SMILES for molecules, or explicit edge lists for knowledge graphs) outperform those trained on lossy or flattened encodings in downstream graph tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Graph neural networks (GNNs) rely on explicit structural information to generalize; analogous effects are observed in LMs when structure is preserved in text.",
                        "uuids": []
                    },
                    {
                        "text": "Loss of structural information (e.g., random node orderings, omission of edge types) leads to degraded performance in both GNNs and LMs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Structural preservation is a core principle in GNNs and some graph-to-sequence models.",
                    "what_is_novel": "Explicitly formalizing structural faithfulness as a requirement for graph-to-text LM training is new.",
                    "classification_explanation": "While structural preservation is known in GNNs, its explicit application and formalization for LM graph-to-text training is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Gilmer et al. (2017) Neural Message Passing for Quantum Chemistry [GNNs require explicit structure]",
                        "Rong et al. (2020) Self-Supervised Graph Transformer on Large-Scale Molecular Data [Graph structure in transformer LMs]",
                        "Schwaller et al. (2019) Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction [Canonical SMILES for structure preservation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Inductive Bias Preservation Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "encodes",
                        "object": "graph symmetries, invariances, and motifs"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "inherits",
                        "object": "graph inductive biases (e.g., permutation invariance, motif reuse)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GNNs leverage permutation invariance and motif-based inductive biases for generalization; LMs can only do so if these are encoded in the text.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical work shows that LMs trained on representations that break graph symmetries (e.g., arbitrary node orderings) fail to generalize as well as those that preserve them.",
                        "uuids": []
                    },
                    {
                        "text": "Encoding motifs and invariances in text (e.g., via templates or canonical forms) improves LM performance on compositional and transfer tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Inductive bias is a well-studied concept in ML, especially in GNNs.",
                    "what_is_novel": "The explicit requirement to preserve graph inductive biases in text for LM training is new.",
                    "classification_explanation": "The law extends known ML principles to a new modality (graph-to-text for LMs).",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Permutation invariance in GNNs]",
                        "Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Inductive bias in graph learning]",
                        "Rong et al. (2020) Self-Supervised Graph Transformer on Large-Scale Molecular Data [Graph inductive bias in LMs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs trained on structurally faithful, bias-preserving graph-to-text representations will outperform those trained on lossy or order-dependent encodings in tasks requiring graph reasoning.",
        "Encoding graph motifs and invariances in text will improve LM generalization to novel graph structures."
    ],
    "new_predictions_unknown": [
        "Preserving higher-order symmetries (e.g., automorphism groups) in text may enable LMs to perform zero-shot reasoning on unseen graph classes.",
        "Faithful encoding of rare or complex motifs may allow LMs to extrapolate to entirely new graph domains."
    ],
    "negative_experiments": [
        "If LMs trained on structurally faithful, bias-preserving representations do not outperform those trained on lossy encodings, the theory is challenged.",
        "If LMs fail to generalize to novel graphs despite inductive bias preservation, the theory's sufficiency is questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The impact of representation length and complexity on LM capacity and efficiency is not addressed.",
            "uuids": []
        },
        {
            "text": "Potential trade-offs between faithfulness and human interpretability are not considered.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LMs can learn global graph properties from flat or lossy representations in specific domains (e.g., small molecules).",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with no clear symmetries or motifs (e.g., random graphs) may not benefit from inductive bias preservation.",
        "Very large graphs may require summarization or compression, potentially reducing faithfulness."
    ],
    "existing_theory": {
        "what_already_exists": "Structural and inductive bias preservation are established in GNNs and some graph-to-sequence models.",
        "what_is_novel": "Their explicit, formal application to graph-to-text LM training is new.",
        "classification_explanation": "The theory synthesizes known ML principles in a new context (graph-to-text for LMs).",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Gilmer et al. (2017) Neural Message Passing for Quantum Chemistry [GNNs require explicit structure]",
            "Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Inductive bias in graph learning]",
            "Rong et al. (2020) Self-Supervised Graph Transformer on Large-Scale Molecular Data [Graph structure in transformer LMs]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-613",
    "original_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
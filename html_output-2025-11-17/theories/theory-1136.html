<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Theory of Language Model Logical Reasoning - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1136</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1136</p>
                <p><strong>Name:</strong> Dual-Process Theory of Language Model Logical Reasoning</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models (LMs) can best perform strict logical reasoning when they are architecturally or procedurally equipped to alternate between two distinct modes: (1) a pattern-matching, context-driven mode (analogous to System 1 in human cognition), and (2) a rule-based, symbolic manipulation mode (analogous to System 2). The theory asserts that optimal logical reasoning arises when LMs can explicitly invoke, coordinate, and arbitrate between these two modes, leveraging the strengths of each for different sub-tasks within a reasoning chain.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Mode Arbitration Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is presented with &#8594; reasoning task<span style="color: #888888;">, and</span></div>
        <div>&#8226; reasoning task &#8594; contains &#8594; strict logical structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; should invoke &#8594; symbolic manipulation mode</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show LMs perform better on logic tasks when augmented with explicit symbolic modules or external tools. </li>
    <li>Human reasoning research supports dual-process models for logic and intuition. </li>
    <li>Neuro-symbolic models outperform pure neural models on tasks requiring formal logic. </li>
    <li>LMs often fail at strict logic tasks when relying solely on pattern-matching. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While dual-process ideas exist in cognitive science and some AI, the explicit requirement for arbitration and mode-switching in LMs for strict logic is a new synthesis.</p>            <p><strong>What Already Exists:</strong> Dual-process theories are well-established in cognitive science; some hybrid neuro-symbolic models exist in AI.</p>            <p><strong>What is Novel:</strong> Explicitly formalizing the arbitration mechanism and its necessity for strict logical reasoning in LMs is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]</li>
    <li>Mao et al. (2019) Neural-Symbolic VQA [neuro-symbolic reasoning in vision]</li>
    <li>Lake et al. (2017) Building machines that learn and think like people [hybrid models, but not explicit arbitration for logic in LMs]</li>
</ul>
            <h3>Statement 1: Contextual Bootstrapping Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning task &#8594; contains &#8594; ambiguous or underspecified context</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; should invoke &#8594; pattern-matching/contextual mode</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs excel at filling in gaps and resolving ambiguity using context, but struggle with strict logic in such cases. </li>
    <li>Few-shot and in-context learning in LMs demonstrates strong contextual inference. </li>
    <li>LMs' performance on ambiguous tasks is superior to their performance on formal logic tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes known LM strengths/weaknesses into a prescriptive arbitration rule.</p>            <p><strong>What Already Exists:</strong> LMs are known to be strong at contextual inference and weak at strict logic.</p>            <p><strong>What is Novel:</strong> The explicit prescription to use context mode for ambiguity, and symbolic mode for strict logic, is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [contextual inference in LMs]</li>
    <li>Bubeck et al. (2023) Sparks of Artificial General Intelligence [LMs' struggles with strict logic]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LM is given a mechanism to explicitly switch between symbolic and contextual modes, its logical reasoning accuracy will improve on formal logic benchmarks.</li>
                <li>Tasks that require both context resolution and strict logic (e.g., word problems) will be solved more accurately by LMs with dual-process arbitration.</li>
                <li>LMs with explicit arbitration will show improved robustness to adversarial logic puzzles compared to standard LMs.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LM is trained end-to-end with a learned arbitration controller, it may develop novel hybrid reasoning strategies not seen in humans.</li>
                <li>In adversarial tasks designed to confuse mode arbitration, LMs may develop emergent meta-reasoning capabilities.</li>
                <li>The arbitration mechanism may enable LMs to generalize logical reasoning to domains not seen during training.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs with explicit dual-process arbitration do not outperform standard LMs on strict logic tasks, the theory is called into question.</li>
                <li>If LMs can achieve perfect logical reasoning without any explicit symbolic mode, the necessity of dual-process arbitration is undermined.</li>
                <li>If arbitration between modes leads to decreased performance on both logic and context tasks, the theory's central claim is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs show improvement on logic tasks with scale alone, without explicit symbolic modules. </li>
    <li>Certain transformer architectures can perform some logical reasoning without explicit mode separation. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is a novel synthesis, extending dual-process ideas to LM architecture and strict logic.</p>
            <p><strong>References:</strong> <ul>
    <li>Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]</li>
    <li>Mao et al. (2019) Neural-Symbolic VQA [neuro-symbolic reasoning in vision]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [contextual inference in LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Theory of Language Model Logical Reasoning",
    "theory_description": "This theory posits that language models (LMs) can best perform strict logical reasoning when they are architecturally or procedurally equipped to alternate between two distinct modes: (1) a pattern-matching, context-driven mode (analogous to System 1 in human cognition), and (2) a rule-based, symbolic manipulation mode (analogous to System 2). The theory asserts that optimal logical reasoning arises when LMs can explicitly invoke, coordinate, and arbitrate between these two modes, leveraging the strengths of each for different sub-tasks within a reasoning chain.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Mode Arbitration Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is presented with",
                        "object": "reasoning task"
                    },
                    {
                        "subject": "reasoning task",
                        "relation": "contains",
                        "object": "strict logical structure"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "should invoke",
                        "object": "symbolic manipulation mode"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show LMs perform better on logic tasks when augmented with explicit symbolic modules or external tools.",
                        "uuids": []
                    },
                    {
                        "text": "Human reasoning research supports dual-process models for logic and intuition.",
                        "uuids": []
                    },
                    {
                        "text": "Neuro-symbolic models outperform pure neural models on tasks requiring formal logic.",
                        "uuids": []
                    },
                    {
                        "text": "LMs often fail at strict logic tasks when relying solely on pattern-matching.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Dual-process theories are well-established in cognitive science; some hybrid neuro-symbolic models exist in AI.",
                    "what_is_novel": "Explicitly formalizing the arbitration mechanism and its necessity for strict logical reasoning in LMs is novel.",
                    "classification_explanation": "While dual-process ideas exist in cognitive science and some AI, the explicit requirement for arbitration and mode-switching in LMs for strict logic is a new synthesis.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]",
                        "Mao et al. (2019) Neural-Symbolic VQA [neuro-symbolic reasoning in vision]",
                        "Lake et al. (2017) Building machines that learn and think like people [hybrid models, but not explicit arbitration for logic in LMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Contextual Bootstrapping Law",
                "if": [
                    {
                        "subject": "reasoning task",
                        "relation": "contains",
                        "object": "ambiguous or underspecified context"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "should invoke",
                        "object": "pattern-matching/contextual mode"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs excel at filling in gaps and resolving ambiguity using context, but struggle with strict logic in such cases.",
                        "uuids": []
                    },
                    {
                        "text": "Few-shot and in-context learning in LMs demonstrates strong contextual inference.",
                        "uuids": []
                    },
                    {
                        "text": "LMs' performance on ambiguous tasks is superior to their performance on formal logic tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LMs are known to be strong at contextual inference and weak at strict logic.",
                    "what_is_novel": "The explicit prescription to use context mode for ambiguity, and symbolic mode for strict logic, is new.",
                    "classification_explanation": "The law synthesizes known LM strengths/weaknesses into a prescriptive arbitration rule.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [contextual inference in LMs]",
                        "Bubeck et al. (2023) Sparks of Artificial General Intelligence [LMs' struggles with strict logic]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LM is given a mechanism to explicitly switch between symbolic and contextual modes, its logical reasoning accuracy will improve on formal logic benchmarks.",
        "Tasks that require both context resolution and strict logic (e.g., word problems) will be solved more accurately by LMs with dual-process arbitration.",
        "LMs with explicit arbitration will show improved robustness to adversarial logic puzzles compared to standard LMs."
    ],
    "new_predictions_unknown": [
        "If an LM is trained end-to-end with a learned arbitration controller, it may develop novel hybrid reasoning strategies not seen in humans.",
        "In adversarial tasks designed to confuse mode arbitration, LMs may develop emergent meta-reasoning capabilities.",
        "The arbitration mechanism may enable LMs to generalize logical reasoning to domains not seen during training."
    ],
    "negative_experiments": [
        "If LMs with explicit dual-process arbitration do not outperform standard LMs on strict logic tasks, the theory is called into question.",
        "If LMs can achieve perfect logical reasoning without any explicit symbolic mode, the necessity of dual-process arbitration is undermined.",
        "If arbitration between modes leads to decreased performance on both logic and context tasks, the theory's central claim is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs show improvement on logic tasks with scale alone, without explicit symbolic modules.",
            "uuids": []
        },
        {
            "text": "Certain transformer architectures can perform some logical reasoning without explicit mode separation.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Recent large LMs (e.g., GPT-4) sometimes solve logic puzzles without explicit symbolic reasoning.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with fully specified, unambiguous logic may not require context mode at all.",
        "Tasks with only ambiguous, open-ended questions may not benefit from symbolic mode.",
        "Some logic tasks may be solved by memorized patterns in sufficiently large LMs, bypassing explicit reasoning."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process models in cognitive science and some hybrid neuro-symbolic AI models.",
        "what_is_novel": "Explicit arbitration mechanism for LMs and its necessity for strict logical reasoning.",
        "classification_explanation": "The theory is a novel synthesis, extending dual-process ideas to LM architecture and strict logic.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Evans (2008) Dual-processing accounts of reasoning, judgment, and social cognition [dual-process theory in humans]",
            "Mao et al. (2019) Neural-Symbolic VQA [neuro-symbolic reasoning in vision]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [contextual inference in LMs]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-604",
    "original_theory_name": "Prompt Decomposition and Iterative Composition Law",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Objectivity-Subjectivity Spectrum Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-205</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-205</p>
                <p><strong>Name:</strong> Objectivity-Subjectivity Spectrum Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about alignment between proxy evaluations (LLM-as-a-judge, Likert-style) and expert human review for software development artifacts, including necessary conditions for high agreement, based on the following results.</p>
                <p><strong>Description:</strong> Software artifact evaluation tasks exist on a continuous spectrum from fully objective (deterministic, verifiable) to fully subjective (preference-based, context-dependent). The theory posits that maximum achievable alignment between automated judges and human experts is strongly correlated with task objectivity, with objective tasks achieving 90-99% alignment and subjective tasks limited to 50-75% alignment. Task objectivity can be decomposed into four primary dimensions: (1) Criterion clarity - how precisely the evaluation standard can be specified, (2) Solution multiplicity - how many valid solutions exist for the task, (3) Evidence availability - whether all necessary information for judgment is accessible, and (4) Evaluator consensus - the degree to which human evaluators naturally agree. The theory predicts that: (a) inter-human agreement provides an approximate upper bound for LLM-human alignment, (b) improving any dimension of objectivity increases achievable alignment, (c) the relationship between objectivity and alignment is non-linear with diminishing returns at high objectivity levels, and (d) different judge capabilities (LLM vs human) may perceive objectivity differently for certain tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Maximum achievable LLM-human alignment is strongly correlated with task objectivity, with objective tasks (objectivity score >8/10) achieving 90-99% alignment and subjective tasks (objectivity score <3/10) limited to 50-75% alignment</li>
                <li>Task objectivity = f(Criterion_Clarity, Solution_Uniqueness, Evidence_Availability, Human_Consensus), where each component contributes independently to overall objectivity</li>
                <li>Inter-human agreement provides an approximate upper bound for LLM-human alignment, though LLMs can occasionally exceed this bound when humans make inconsistent errors that LLMs avoid</li>
                <li>Criterion_Clarity can be quantified by: (a) whether criteria can be expressed as deterministic rules, (b) the number of edge cases requiring judgment, (c) the degree of domain knowledge required</li>
                <li>Solution_Uniqueness inversely affects objectivity: tasks with unique correct solutions (e.g., bug fixes) are more objective than tasks with many valid solutions (e.g., creative coding)</li>
                <li>Evidence_Availability is critical: even objective criteria become subjective when necessary context is unavailable (e.g., repository history for code review)</li>
                <li>Human_Consensus (inter-rater agreement) reflects inherent task objectivity: tasks with <70% human agreement will show <65% LLM-human agreement regardless of judge capability</li>
                <li>The relationship between objectivity and alignment is non-linear: improvements in objectivity yield larger alignment gains at low-to-moderate objectivity levels than at high objectivity levels</li>
                <li>Explicit rubrics and structured evaluation frameworks can increase Criterion_Clarity by 2-4 objectivity points, typically improving alignment by 15-30 percentage points</li>
                <li>Providing relevant evidence (code context, documentation, execution traces) can increase Evidence_Availability and improve alignment by 20-30 percentage points for context-dependent tasks</li>
                <li>Pairwise comparison is inherently more objective than absolute scoring, as it reduces scale interpretation variance and typically improves agreement by 10-20 percentage points</li>
                <li>Some tasks appear objective (clear labels) but are fundamentally subjective (causality detection, static warning triage) due to hidden context-dependency or implicit domain knowledge requirements</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Exact match and functional correctness (highly objective) achieve 90-99% correlation with humans on code refinement tasks <a href="../results/extraction-result-1789.html#e1789.2" class="evidence-link">[e1789.2]</a> <a href="../results/extraction-result-1848.html#e1848.0" class="evidence-link">[e1848.0]</a> </li>
    <li>Inter-human agreement is much higher for objective criteria (96.2% Scott's π for correctness) than subjective criteria (8-50% for likability) <a href="../results/extraction-result-1834.html#e1834.0" class="evidence-link">[e1834.0]</a> <a href="../results/extraction-result-1846.html#e1846.0" class="evidence-link">[e1846.0]</a> </li>
    <li>Semantic similarity (relatively objective with clear criteria) shows high human-model agreement (0.64-0.83 Krippendorff's alpha) across goals, operations, and effects <a href="../results/extraction-result-1849.html#e1849.3" class="evidence-link">[e1849.3]</a> </li>
    <li>Causality detection (subjective, context-dependent) shows very low human-model agreement (0.22 alpha) despite having clear binary labels, demonstrating that label clarity alone is insufficient <a href="../results/extraction-result-1849.html#e1849.2" class="evidence-link">[e1849.2]</a> </li>
    <li>Code refinement with unique correct solutions shows near-perfect correlation (r=0.999) while open-ended text-to-code generation shows lower correlation (r=0.912) <a href="../results/extraction-result-1789.html#e1789.2" class="evidence-link">[e1789.2]</a> </li>
    <li>Name-value consistency (moderately objective) shows human-model agreement (0.49 alpha) close to human-human agreement (0.52 alpha) <a href="../results/extraction-result-1849.html#e1849.1" class="evidence-link">[e1849.1]</a> </li>
    <li>Static warning triage (highly context-dependent, requiring repository history) shows very low agreement (0.15 alpha) despite clear categorical labels <a href="../results/extraction-result-1849.html#e1849.4" class="evidence-link">[e1849.4]</a> </li>
    <li>Relevance (more objective) shows higher LLM-human correlation (τ=0.38-0.43) than likability (more subjective, τ=0.19-0.22) <a href="../results/extraction-result-1846.html#e1846.0" class="evidence-link">[e1846.0]</a> </li>
    <li>PASS metric (objective functional correctness) shows strong correlation with human judgments of value (r=0.62-0.66) and accuracy (r=0.66) <a href="../results/extraction-result-1801.html#e1801.0" class="evidence-link">[e1801.0]</a> </li>
    <li>Code summarization with clear criteria shows human-model agreement approaching human-human agreement for accuracy (0.48 vs 0.38 human-human) and adequacy (0.41 vs 0.40) <a href="../results/extraction-result-1849.html#e1849.0" class="evidence-link">[e1849.0]</a> </li>
    <li>Translation quality evaluation shows higher agreement for objective criteria (accuracy, fluency) than subjective criteria, with system-level accuracy reaching 89.8% <a href="../results/extraction-result-1815.html#e1815.0" class="evidence-link">[e1815.0]</a> </li>
    <li>MT-Bench shows GPT-4 achieving 85% agreement with humans on relatively objective multi-turn conversation quality, approaching inter-human agreement of 81% <a href="../results/extraction-result-1718.html#e1718.0" class="evidence-link">[e1718.0]</a> </li>
    <li>CodeBLEU incorporating syntactic (AST) and semantic (data-flow) components shows higher correlation (r=0.970-0.979) than surface-level BLEU, demonstrating that making criteria more explicit improves alignment <a href="../results/extraction-result-1789.html#e1789.0" class="evidence-link">[e1789.0]</a> </li>
    <li>Agent-as-a-Judge with explicit evidence access (read/locate modules) achieves ~90% alignment versus ~60-70% for LLM-as-a-Judge without evidence access, showing evidence availability is critical <a href="../results/extraction-result-1713.html#e1713.0" class="evidence-link">[e1713.0]</a> </li>
    <li>ICE-Score reference-free evaluation shows substantial correlation (τ=0.556, r=0.613) with human usefulness judgments, but reference-enhanced versions do not reliably improve when references are imperfect <a href="../results/extraction-result-1717.html#e1717.0" class="evidence-link">[e1717.0]</a> </li>
    <li>BLEU shows high correlation for code refinement (r=0.923) where solutions are constrained, but lower for open-ended generation, demonstrating solution multiplicity effect <a href="../results/extraction-result-1789.html#e1789.1" class="evidence-link">[e1789.1]</a> </li>
    <li>Pairwise comparison by GPT-4 achieves 85.3% accuracy matching human pairwise judgments, higher than absolute scoring, suggesting relative judgments are more objective <a href="../results/extraction-result-1842.html#e1842.2" class="evidence-link">[e1842.2]</a> </li>
    <li>TruthfulQA automatic metrics overstate improvements compared to human evaluations, showing that apparent objectivity can mask subjective elements <a href="../results/extraction-result-1840.html#e1840.2" class="evidence-link">[e1840.2]</a> </li>
    <li>Perspective API toxicity scores align with human judgments when prompts instruct safe behavior but diverge under different prompting contexts, showing context-dependency <a href="../results/extraction-result-1840.html#e1840.1" class="evidence-link">[e1840.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For a new evaluation task, measuring inter-human agreement will predict maximum achievable LLM-human alignment within ±5-10 percentage points</li>
                <li>Converting a subjective criterion (e.g., 'good code style') into multiple objective sub-criteria (e.g., specific formatting rules, naming conventions) will improve alignment by 15-25 percentage points</li>
                <li>For code review evaluation, providing full repository context (history, related files, issue descriptions) will improve alignment by 20-30 percentage points compared to reviewing isolated diffs</li>
                <li>For architecture evaluation, decomposing 'good design' into specific measurable principles (coupling metrics, cohesion scores, SOLID violations) will improve alignment by 25-35 percentage points</li>
                <li>Tasks with inter-human agreement <70% will consistently show LLM-human agreement <65%, regardless of judge model capability or prompting strategy</li>
                <li>Using pairwise comparison instead of absolute scoring for subjective tasks (code elegance, naming quality) will improve agreement by 10-20 percentage points</li>
                <li>For documentation quality evaluation, providing explicit checklists (completeness, accuracy, clarity criteria) will improve alignment by 20-30 percentage points over holistic judgments</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exist evaluation tasks that are fundamentally objective for humans but subjective for LLMs (or vice versa) due to differences in reasoning capabilities or knowledge representation</li>
                <li>Whether future LLMs with significantly improved reasoning capabilities can exceed current alignment limits on subjective tasks, or if the limits are fundamental to the task rather than the judge</li>
                <li>Whether ensemble methods (multiple diverse judges with voting/discussion) can systematically exceed the theoretical alignment limit imposed by inter-human agreement for subjective tasks</li>
                <li>Whether there exist 'pseudo-objective' tasks that appear to have clear criteria but contain hidden subjective elements that fundamentally limit alignment below expected levels</li>
                <li>Whether the objectivity-subjectivity spectrum is domain-invariant or if different software domains (systems programming vs web development vs data science) have fundamentally different objectivity characteristics</li>
                <li>Whether providing explanations alongside judgments increases effective objectivity by forcing explicit reasoning, or if it simply makes subjective judgments more transparent</li>
                <li>Whether the relationship between objectivity and alignment changes as artifacts become more complex (e.g., evaluating entire systems vs individual functions)</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding an objective task (high inter-human agreement, clear criteria, unique solutions) where LLM-human alignment is consistently 15+ percentage points lower than inter-human agreement would challenge the upper-bound assumption</li>
                <li>Finding a subjective task (low inter-human agreement, ambiguous criteria) where LLM-human alignment consistently exceeds 85% would challenge the fundamental limit hypothesis</li>
                <li>Demonstrating that the four objectivity components (Criterion_Clarity, Solution_Uniqueness, Evidence_Availability, Human_Consensus) are not independent and cannot be improved separately would require revising the decomposition model</li>
                <li>Finding that alignment does not correlate with inter-human agreement across a diverse set of tasks would challenge the core premise of the theory</li>
                <li>Showing that some highly subjective tasks can be made fully objective through better specification alone (without changing the underlying judgment) would challenge the irreducibility assumption</li>
                <li>Finding tasks where increasing Evidence_Availability decreases alignment (by introducing confusing information) would challenge the monotonic relationship assumption</li>
                <li>Demonstrating that pairwise comparison is not more objective than absolute scoring for certain task types would challenge the generality of that prediction</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to quantitatively measure the four objectivity components in practice, making it difficult to compute an overall objectivity score </li>
    <li>The relative importance (weights) of the four objectivity components may vary by domain, task type, or artifact complexity, but the theory does not specify how to determine these weights </li>
    <li>The theory does not address how subjectivity changes with evaluator expertise - domain experts may find certain tasks more objective due to shared implicit knowledge </li>
    <li>The interaction between objectivity and judge capability is not fully characterized - whether stronger LLMs can make subjective tasks more objective or simply execute better on the same objectivity level </li>
    <li>The theory does not explain why some tasks with clear binary labels (causality, static warnings) show very low agreement while others (semantic similarity) show high agreement <a href="../results/extraction-result-1849.html#e1849.2" class="evidence-link">[e1849.2]</a> <a href="../results/extraction-result-1849.html#e1849.4" class="evidence-link">[e1849.4]</a> <a href="../results/extraction-result-1849.html#e1849.3" class="evidence-link">[e1849.3]</a> </li>
    <li>The theory does not account for temporal effects - whether objectivity changes as models improve or as evaluation standards evolve </li>
    <li>The theory does not address how cultural or domain-specific conventions affect perceived objectivity (e.g., code style preferences vary by community) </li>
    <li>The theory does not explain cases where automated metrics (BLEU) show high correlation despite being 'objective' measures of a subjective quality (translation quality) <a href="../results/extraction-result-1815.html#e1815.0" class="evidence-link">[e1815.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Gao et al. (2023) Can Large Language Models Be an Alternative to Human Evaluations? [Explores objective vs subjective tasks empirically but does not propose a formal spectrum theory or decomposition model]</li>
    <li>Wang et al. (2024) Can LLMs Replace Manual Annotation of Software Engineering Artifacts? [Compares agreement across task types and proposes model-model agreement as predictor, but does not formalize objectivity spectrum]</li>
    <li>Zheng et al. (2023) Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena [Studies LLM judge reliability but focuses on biases rather than objectivity theory]</li>
    <li>Ren et al. (2024) Agent-as-a-Judge: Evaluate Agents with Agents [Shows evidence access improves alignment but does not propose general objectivity framework]</li>
    <li>Evtikhiev et al. (2022) Out of the BLEU: how should we assess quality of the Code Generation models? [Studies metric-human alignment but focuses on metric choice rather than task objectivity]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Objectivity-Subjectivity Spectrum Theory",
    "theory_description": "Software artifact evaluation tasks exist on a continuous spectrum from fully objective (deterministic, verifiable) to fully subjective (preference-based, context-dependent). The theory posits that maximum achievable alignment between automated judges and human experts is strongly correlated with task objectivity, with objective tasks achieving 90-99% alignment and subjective tasks limited to 50-75% alignment. Task objectivity can be decomposed into four primary dimensions: (1) Criterion clarity - how precisely the evaluation standard can be specified, (2) Solution multiplicity - how many valid solutions exist for the task, (3) Evidence availability - whether all necessary information for judgment is accessible, and (4) Evaluator consensus - the degree to which human evaluators naturally agree. The theory predicts that: (a) inter-human agreement provides an approximate upper bound for LLM-human alignment, (b) improving any dimension of objectivity increases achievable alignment, (c) the relationship between objectivity and alignment is non-linear with diminishing returns at high objectivity levels, and (d) different judge capabilities (LLM vs human) may perceive objectivity differently for certain tasks.",
    "supporting_evidence": [
        {
            "text": "Exact match and functional correctness (highly objective) achieve 90-99% correlation with humans on code refinement tasks",
            "uuids": [
                "e1789.2",
                "e1848.0"
            ]
        },
        {
            "text": "Inter-human agreement is much higher for objective criteria (96.2% Scott's π for correctness) than subjective criteria (8-50% for likability)",
            "uuids": [
                "e1834.0",
                "e1846.0"
            ]
        },
        {
            "text": "Semantic similarity (relatively objective with clear criteria) shows high human-model agreement (0.64-0.83 Krippendorff's alpha) across goals, operations, and effects",
            "uuids": [
                "e1849.3"
            ]
        },
        {
            "text": "Causality detection (subjective, context-dependent) shows very low human-model agreement (0.22 alpha) despite having clear binary labels, demonstrating that label clarity alone is insufficient",
            "uuids": [
                "e1849.2"
            ]
        },
        {
            "text": "Code refinement with unique correct solutions shows near-perfect correlation (r=0.999) while open-ended text-to-code generation shows lower correlation (r=0.912)",
            "uuids": [
                "e1789.2"
            ]
        },
        {
            "text": "Name-value consistency (moderately objective) shows human-model agreement (0.49 alpha) close to human-human agreement (0.52 alpha)",
            "uuids": [
                "e1849.1"
            ]
        },
        {
            "text": "Static warning triage (highly context-dependent, requiring repository history) shows very low agreement (0.15 alpha) despite clear categorical labels",
            "uuids": [
                "e1849.4"
            ]
        },
        {
            "text": "Relevance (more objective) shows higher LLM-human correlation (τ=0.38-0.43) than likability (more subjective, τ=0.19-0.22)",
            "uuids": [
                "e1846.0"
            ]
        },
        {
            "text": "PASS metric (objective functional correctness) shows strong correlation with human judgments of value (r=0.62-0.66) and accuracy (r=0.66)",
            "uuids": [
                "e1801.0"
            ]
        },
        {
            "text": "Code summarization with clear criteria shows human-model agreement approaching human-human agreement for accuracy (0.48 vs 0.38 human-human) and adequacy (0.41 vs 0.40)",
            "uuids": [
                "e1849.0"
            ]
        },
        {
            "text": "Translation quality evaluation shows higher agreement for objective criteria (accuracy, fluency) than subjective criteria, with system-level accuracy reaching 89.8%",
            "uuids": [
                "e1815.0"
            ]
        },
        {
            "text": "MT-Bench shows GPT-4 achieving 85% agreement with humans on relatively objective multi-turn conversation quality, approaching inter-human agreement of 81%",
            "uuids": [
                "e1718.0"
            ]
        },
        {
            "text": "CodeBLEU incorporating syntactic (AST) and semantic (data-flow) components shows higher correlation (r=0.970-0.979) than surface-level BLEU, demonstrating that making criteria more explicit improves alignment",
            "uuids": [
                "e1789.0"
            ]
        },
        {
            "text": "Agent-as-a-Judge with explicit evidence access (read/locate modules) achieves ~90% alignment versus ~60-70% for LLM-as-a-Judge without evidence access, showing evidence availability is critical",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "ICE-Score reference-free evaluation shows substantial correlation (τ=0.556, r=0.613) with human usefulness judgments, but reference-enhanced versions do not reliably improve when references are imperfect",
            "uuids": [
                "e1717.0"
            ]
        },
        {
            "text": "BLEU shows high correlation for code refinement (r=0.923) where solutions are constrained, but lower for open-ended generation, demonstrating solution multiplicity effect",
            "uuids": [
                "e1789.1"
            ]
        },
        {
            "text": "Pairwise comparison by GPT-4 achieves 85.3% accuracy matching human pairwise judgments, higher than absolute scoring, suggesting relative judgments are more objective",
            "uuids": [
                "e1842.2"
            ]
        },
        {
            "text": "TruthfulQA automatic metrics overstate improvements compared to human evaluations, showing that apparent objectivity can mask subjective elements",
            "uuids": [
                "e1840.2"
            ]
        },
        {
            "text": "Perspective API toxicity scores align with human judgments when prompts instruct safe behavior but diverge under different prompting contexts, showing context-dependency",
            "uuids": [
                "e1840.1"
            ]
        }
    ],
    "theory_statements": [
        "Maximum achievable LLM-human alignment is strongly correlated with task objectivity, with objective tasks (objectivity score &gt;8/10) achieving 90-99% alignment and subjective tasks (objectivity score &lt;3/10) limited to 50-75% alignment",
        "Task objectivity = f(Criterion_Clarity, Solution_Uniqueness, Evidence_Availability, Human_Consensus), where each component contributes independently to overall objectivity",
        "Inter-human agreement provides an approximate upper bound for LLM-human alignment, though LLMs can occasionally exceed this bound when humans make inconsistent errors that LLMs avoid",
        "Criterion_Clarity can be quantified by: (a) whether criteria can be expressed as deterministic rules, (b) the number of edge cases requiring judgment, (c) the degree of domain knowledge required",
        "Solution_Uniqueness inversely affects objectivity: tasks with unique correct solutions (e.g., bug fixes) are more objective than tasks with many valid solutions (e.g., creative coding)",
        "Evidence_Availability is critical: even objective criteria become subjective when necessary context is unavailable (e.g., repository history for code review)",
        "Human_Consensus (inter-rater agreement) reflects inherent task objectivity: tasks with &lt;70% human agreement will show &lt;65% LLM-human agreement regardless of judge capability",
        "The relationship between objectivity and alignment is non-linear: improvements in objectivity yield larger alignment gains at low-to-moderate objectivity levels than at high objectivity levels",
        "Explicit rubrics and structured evaluation frameworks can increase Criterion_Clarity by 2-4 objectivity points, typically improving alignment by 15-30 percentage points",
        "Providing relevant evidence (code context, documentation, execution traces) can increase Evidence_Availability and improve alignment by 20-30 percentage points for context-dependent tasks",
        "Pairwise comparison is inherently more objective than absolute scoring, as it reduces scale interpretation variance and typically improves agreement by 10-20 percentage points",
        "Some tasks appear objective (clear labels) but are fundamentally subjective (causality detection, static warning triage) due to hidden context-dependency or implicit domain knowledge requirements"
    ],
    "new_predictions_likely": [
        "For a new evaluation task, measuring inter-human agreement will predict maximum achievable LLM-human alignment within ±5-10 percentage points",
        "Converting a subjective criterion (e.g., 'good code style') into multiple objective sub-criteria (e.g., specific formatting rules, naming conventions) will improve alignment by 15-25 percentage points",
        "For code review evaluation, providing full repository context (history, related files, issue descriptions) will improve alignment by 20-30 percentage points compared to reviewing isolated diffs",
        "For architecture evaluation, decomposing 'good design' into specific measurable principles (coupling metrics, cohesion scores, SOLID violations) will improve alignment by 25-35 percentage points",
        "Tasks with inter-human agreement &lt;70% will consistently show LLM-human agreement &lt;65%, regardless of judge model capability or prompting strategy",
        "Using pairwise comparison instead of absolute scoring for subjective tasks (code elegance, naming quality) will improve agreement by 10-20 percentage points",
        "For documentation quality evaluation, providing explicit checklists (completeness, accuracy, clarity criteria) will improve alignment by 20-30 percentage points over holistic judgments"
    ],
    "new_predictions_unknown": [
        "Whether there exist evaluation tasks that are fundamentally objective for humans but subjective for LLMs (or vice versa) due to differences in reasoning capabilities or knowledge representation",
        "Whether future LLMs with significantly improved reasoning capabilities can exceed current alignment limits on subjective tasks, or if the limits are fundamental to the task rather than the judge",
        "Whether ensemble methods (multiple diverse judges with voting/discussion) can systematically exceed the theoretical alignment limit imposed by inter-human agreement for subjective tasks",
        "Whether there exist 'pseudo-objective' tasks that appear to have clear criteria but contain hidden subjective elements that fundamentally limit alignment below expected levels",
        "Whether the objectivity-subjectivity spectrum is domain-invariant or if different software domains (systems programming vs web development vs data science) have fundamentally different objectivity characteristics",
        "Whether providing explanations alongside judgments increases effective objectivity by forcing explicit reasoning, or if it simply makes subjective judgments more transparent",
        "Whether the relationship between objectivity and alignment changes as artifacts become more complex (e.g., evaluating entire systems vs individual functions)"
    ],
    "negative_experiments": [
        "Finding an objective task (high inter-human agreement, clear criteria, unique solutions) where LLM-human alignment is consistently 15+ percentage points lower than inter-human agreement would challenge the upper-bound assumption",
        "Finding a subjective task (low inter-human agreement, ambiguous criteria) where LLM-human alignment consistently exceeds 85% would challenge the fundamental limit hypothesis",
        "Demonstrating that the four objectivity components (Criterion_Clarity, Solution_Uniqueness, Evidence_Availability, Human_Consensus) are not independent and cannot be improved separately would require revising the decomposition model",
        "Finding that alignment does not correlate with inter-human agreement across a diverse set of tasks would challenge the core premise of the theory",
        "Showing that some highly subjective tasks can be made fully objective through better specification alone (without changing the underlying judgment) would challenge the irreducibility assumption",
        "Finding tasks where increasing Evidence_Availability decreases alignment (by introducing confusing information) would challenge the monotonic relationship assumption",
        "Demonstrating that pairwise comparison is not more objective than absolute scoring for certain task types would challenge the generality of that prediction"
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to quantitatively measure the four objectivity components in practice, making it difficult to compute an overall objectivity score",
            "uuids": []
        },
        {
            "text": "The relative importance (weights) of the four objectivity components may vary by domain, task type, or artifact complexity, but the theory does not specify how to determine these weights",
            "uuids": []
        },
        {
            "text": "The theory does not address how subjectivity changes with evaluator expertise - domain experts may find certain tasks more objective due to shared implicit knowledge",
            "uuids": []
        },
        {
            "text": "The interaction between objectivity and judge capability is not fully characterized - whether stronger LLMs can make subjective tasks more objective or simply execute better on the same objectivity level",
            "uuids": []
        },
        {
            "text": "The theory does not explain why some tasks with clear binary labels (causality, static warnings) show very low agreement while others (semantic similarity) show high agreement",
            "uuids": [
                "e1849.2",
                "e1849.4",
                "e1849.3"
            ]
        },
        {
            "text": "The theory does not account for temporal effects - whether objectivity changes as models improve or as evaluation standards evolve",
            "uuids": []
        },
        {
            "text": "The theory does not address how cultural or domain-specific conventions affect perceived objectivity (e.g., code style preferences vary by community)",
            "uuids": []
        },
        {
            "text": "The theory does not explain cases where automated metrics (BLEU) show high correlation despite being 'objective' measures of a subjective quality (translation quality)",
            "uuids": [
                "e1815.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show LLM-human agreement exceeding inter-human agreement (e.g., JudgeLM ~90% vs human ~82%, code summarization accuracy 0.48 vs 0.38), which challenges the strict upper-bound assumption and suggests LLMs may be more consistent than humans on some tasks",
            "uuids": [
                "e1841.0",
                "e1849.0"
            ]
        },
        {
            "text": "Causality detection has clear binary labels but very low agreement (0.22), while semantic similarity with more complex multi-dimensional criteria shows high agreement (0.64-0.83), suggesting label clarity is not the primary determinant of objectivity",
            "uuids": [
                "e1849.2",
                "e1849.3"
            ]
        },
        {
            "text": "Exact match on diverse solutions shows lower agreement than expected, while CodeBLEU with semantic components shows higher agreement, suggesting that 'objective' surface metrics can be less aligned than 'subjective' semantic judgments",
            "uuids": [
                "e1833.0",
                "e1789.0"
            ]
        },
        {
            "text": "Reference-enhanced evaluation does not reliably improve alignment over reference-free evaluation, challenging the assumption that more evidence always increases objectivity",
            "uuids": [
                "e1717.0"
            ]
        },
        {
            "text": "Agent-as-a-Judge sometimes exceeds individual human evaluators, suggesting that systematic evidence gathering can make evaluation more objective than human judgment",
            "uuids": [
                "e1713.0"
            ]
        },
        {
            "text": "Length-controlled AlpacaEval shows that removing spurious correlates (verbosity) increases alignment, suggesting that apparent objectivity can mask hidden subjective biases",
            "uuids": [
                "e1825.0",
                "e1832.1"
            ]
        }
    ],
    "special_cases": [
        "For safety-critical evaluation, even objective criteria may require subjective judgment about acceptable risk levels, context-specific trade-offs, and potential failure modes",
        "For creative coding tasks, subjectivity is intentional and high alignment may not be desirable - diversity of valid solutions is a feature, not a bug",
        "For educational contexts, evaluating 'learning value' or 'pedagogical quality' is inherently subjective regardless of specification, as it depends on learner background and goals",
        "For accessibility evaluation, objective criteria (WCAG compliance) and subjective criteria (user experience for diverse abilities) must both be considered and may conflict",
        "For code review in rapidly evolving domains, what constitutes 'good practice' changes over time, making historical evaluations subjective in retrospect",
        "For multi-cultural or multi-lingual software, naming conventions and style preferences vary by community, making 'objective' style rules culturally subjective",
        "For research code vs production code, evaluation criteria differ fundamentally - research code prioritizes experimentation while production code prioritizes reliability",
        "For generated code that will be edited by humans, 'value' includes both functional correctness and editability/understandability, mixing objective and subjective criteria",
        "For adversarial or security-critical code, objective functional correctness is insufficient - subtle vulnerabilities require subjective expert judgment",
        "For API design evaluation, objective metrics (consistency, completeness) and subjective metrics (intuitiveness, learnability) are both essential and may trade off"
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Gao et al. (2023) Can Large Language Models Be an Alternative to Human Evaluations? [Explores objective vs subjective tasks empirically but does not propose a formal spectrum theory or decomposition model]",
            "Wang et al. (2024) Can LLMs Replace Manual Annotation of Software Engineering Artifacts? [Compares agreement across task types and proposes model-model agreement as predictor, but does not formalize objectivity spectrum]",
            "Zheng et al. (2023) Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena [Studies LLM judge reliability but focuses on biases rather than objectivity theory]",
            "Ren et al. (2024) Agent-as-a-Judge: Evaluate Agents with Agents [Shows evidence access improves alignment but does not propose general objectivity framework]",
            "Evtikhiev et al. (2022) Out of the BLEU: how should we assess quality of the Code Generation models? [Studies metric-human alignment but focuses on metric choice rather than task objectivity]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
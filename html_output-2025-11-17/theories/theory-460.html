<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fourier Feature Decomposition Mechanism for Addition in Transformers - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-460</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-460</p>
                <p><strong>Name:</strong> Fourier Feature Decomposition Mechanism for Addition in Transformers</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that transformer-based LLMs trained on addition tasks implement the computation by decomposing the sum into a superposition of low-frequency (magnitude approximation) and high-frequency (modular/digitwise classification) Fourier features in their hidden states. MLP layers primarily contribute low-frequency components that approximate the sum's magnitude, while attention layers contribute high-frequency components that resolve modular ambiguities (e.g., unit digit, carry). The final output is determined by combining these components, and targeted ablation of specific frequency bands produces predictable error patterns.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Addition as Superposition of Fourier Features (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; transformer_LLM &#8594; is_fine_tuned_on &#8594; addition_task</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; hidden_states &#8594; encode_sum_as &#8594; superposition_of_low_and_high_frequency_Fourier_components</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Discrete Fourier transform of intermediate logits reveals sparse outlier components at periods 2, 2.5, 5, 10; ablation of these components impairs modular/digitwise accuracy. <a href="../results/extraction-result-3002.html#e3002.0" class="evidence-link">[e3002.0]</a> <a href="../results/extraction-result-3002.html#e3002.4" class="evidence-link">[e3002.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Layerwise Specialization in Addition (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; MLP_layers &#8594; are_present &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; MLP_layers &#8594; contribute &#8594; low_frequency_magnitude_approximation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Ablation of low-frequency components in MLPs causes large-magnitude errors (off-by-10, 100, etc.). <a href="../results/extraction-result-3002.html#e3002.0" class="evidence-link">[e3002.0]</a> <a href="../results/extraction-result-3002.html#e3002.4" class="evidence-link">[e3002.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Attention-Mediated Modular Classification (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; attention_layers &#8594; are_present &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; attention_layers &#8594; contribute &#8594; high_frequency_modular_classification</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Ablation of high-frequency components in attention layers causes unit-digit and modular errors. <a href="../results/extraction-result-3002.html#e3002.0" class="evidence-link">[e3002.0]</a> <a href="../results/extraction-result-3002.html#e3002.4" class="evidence-link">[e3002.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new transformer is trained on addition in a different base (e.g., base-12), the dominant high-frequency Fourier components will shift to match the new modular structure.</li>
                <li>If a model is trained on addition with noisy or missing unit digits, the high-frequency components will be less pronounced and modular errors will increase.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained on addition with adversarially perturbed intermediate representations, it may develop alternative, non-Fourier-based decomposition strategies.</li>
                <li>If a model is trained with explicit supervision to align specific Fourier components with arithmetic subroutines, it may develop more interpretable and robust arithmetic circuits.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If ablation of high-frequency components in attention layers does not increase modular/digitwise errors, this would challenge the theory.</li>
                <li>If Fourier analysis of hidden states does not reveal sparse, interpretable frequency components after fine-tuning on addition, this would challenge the theory.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Multiplication and division tasks do not show the same clear Fourier decomposition, and may require different mechanisms. <a href="../results/extraction-result-3005.html#e3005.2" class="evidence-link">[e3005.2]</a> <a href="../results/extraction-result-2982.html#e2982.0" class="evidence-link">[e2982.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [Directly related, but this theory formalizes the mechanism and its causal consequences]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Fourier Feature Decomposition Mechanism for Addition in Transformers",
    "theory_description": "This theory asserts that transformer-based LLMs trained on addition tasks implement the computation by decomposing the sum into a superposition of low-frequency (magnitude approximation) and high-frequency (modular/digitwise classification) Fourier features in their hidden states. MLP layers primarily contribute low-frequency components that approximate the sum's magnitude, while attention layers contribute high-frequency components that resolve modular ambiguities (e.g., unit digit, carry). The final output is determined by combining these components, and targeted ablation of specific frequency bands produces predictable error patterns.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Addition as Superposition of Fourier Features",
                "if": [
                    {
                        "subject": "transformer_LLM",
                        "relation": "is_fine_tuned_on",
                        "object": "addition_task"
                    }
                ],
                "then": [
                    {
                        "subject": "hidden_states",
                        "relation": "encode_sum_as",
                        "object": "superposition_of_low_and_high_frequency_Fourier_components"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Discrete Fourier transform of intermediate logits reveals sparse outlier components at periods 2, 2.5, 5, 10; ablation of these components impairs modular/digitwise accuracy.",
                        "uuids": [
                            "e3002.0",
                            "e3002.4"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        },
        {
            "law": {
                "law_name": "Layerwise Specialization in Addition",
                "if": [
                    {
                        "subject": "MLP_layers",
                        "relation": "are_present",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "MLP_layers",
                        "relation": "contribute",
                        "object": "low_frequency_magnitude_approximation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Ablation of low-frequency components in MLPs causes large-magnitude errors (off-by-10, 100, etc.).",
                        "uuids": [
                            "e3002.0",
                            "e3002.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Attention-Mediated Modular Classification",
                "if": [
                    {
                        "subject": "attention_layers",
                        "relation": "are_present",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "attention_layers",
                        "relation": "contribute",
                        "object": "high_frequency_modular_classification"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Ablation of high-frequency components in attention layers causes unit-digit and modular errors.",
                        "uuids": [
                            "e3002.0",
                            "e3002.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new transformer is trained on addition in a different base (e.g., base-12), the dominant high-frequency Fourier components will shift to match the new modular structure.",
        "If a model is trained on addition with noisy or missing unit digits, the high-frequency components will be less pronounced and modular errors will increase."
    ],
    "new_predictions_unknown": [
        "If a model is trained on addition with adversarially perturbed intermediate representations, it may develop alternative, non-Fourier-based decomposition strategies.",
        "If a model is trained with explicit supervision to align specific Fourier components with arithmetic subroutines, it may develop more interpretable and robust arithmetic circuits."
    ],
    "negative_experiments": [
        "If ablation of high-frequency components in attention layers does not increase modular/digitwise errors, this would challenge the theory.",
        "If Fourier analysis of hidden states does not reveal sparse, interpretable frequency components after fine-tuning on addition, this would challenge the theory."
    ],
    "unaccounted_for": [
        {
            "text": "Multiplication and division tasks do not show the same clear Fourier decomposition, and may require different mechanisms.",
            "uuids": [
                "e3005.2",
                "e2982.0"
            ]
        }
    ],
    "conflicting_evidence": [],
    "special_cases": [
        "For very small numbers or trivial addition tasks, the frequency decomposition may be less pronounced.",
        "For models with shallow architectures, the separation of subroutines across layers may not occur."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Zhang et al. (2024) Pre-trained Large Language Models Use Fourier Features to Compute Addition [Directly related, but this theory formalizes the mechanism and its causal consequences]"
        ]
    },
    "theory_type_general_specific": "specific",
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structure-Aware Demonstration Retrieval Principle - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1670</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1670</p>
                <p><strong>Name:</strong> Structure-Aware Demonstration Retrieval Principle</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory posits that the accuracy of large language models (LLMs) in molecular property prediction tasks is fundamentally determined by the structural similarity between the query molecule and the molecules used as demonstrations in the prompt. The more structurally similar the demonstrations are to the query, the more likely the LLM is to generate accurate property predictions, due to improved alignment between the model's learned representations and the relevant chemical features.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structural Similarity Drives Predictive Accuracy (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; demonstration_set &#8594; has_high_structural_similarity_to &#8594; query_molecule<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; demonstration_set</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; predicts_property_of &#8594; query_molecule_with_high_accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that in-context learning with demonstrations structurally similar to the query improves LLM performance in chemistry tasks. </li>
    <li>Analogous findings in vision and NLP domains indicate that similarity-based retrieval enhances few-shot learning. </li>
    <li>Zhang (2023) found that LLMs prompted with high-Tanimoto-similarity molecules yield higher property prediction accuracy. </li>
    <li>Wang (2022) demonstrated that retrieval-augmented generation in NLP tasks improves accuracy when retrieved examples are similar to the query. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to existing work on in-context learning and retrieval augmentation, this law is a novel, explicit statement for molecular property prediction with LLMs.</p>            <p><strong>What Already Exists:</strong> Similarity-based retrieval is known to improve few-shot learning in NLP and vision, and some recent works in chemistry have shown improved LLM performance with similar demonstrations.</p>            <p><strong>What is Novel:</strong> This law formalizes the relationship as a conditional principle specifically for molecular property prediction, emphasizing the primacy of structure-aware retrieval.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang (2023) In-context learning for molecular property prediction [Shows improved LLM performance with similar demonstrations]</li>
    <li>Lester (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Demonstrates prompt selection impacts LLM performance]</li>
    <li>Wang (2022) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [Retrieval improves LLM accuracy in NLP]</li>
</ul>
            <h3>Statement 1: Diminishing Returns of Dissimilar Demonstrations (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; demonstration_set &#8594; has_low_structural_similarity_to &#8594; query_molecule<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; demonstration_set</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; predicts_property_of &#8594; query_molecule_with_low_accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Studies show that random or dissimilar demonstrations can degrade LLM performance in molecular property prediction. </li>
    <li>Ablation studies in in-context learning reveal that irrelevant demonstrations can even harm performance. </li>
    <li>Min (2022) found that random demonstrations can hurt LLM performance in in-context learning. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general effect is known, but its explicit application and thresholding in molecular property prediction is novel.</p>            <p><strong>What Already Exists:</strong> Negative effects of irrelevant or dissimilar demonstrations are observed in few-shot learning literature.</p>            <p><strong>What is Novel:</strong> This law frames the effect as a diminishing return specific to molecular property prediction, suggesting a threshold beyond which demonstrations are unhelpful or harmful.</p>
            <p><strong>References:</strong> <ul>
    <li>Min (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Shows that random demonstrations can hurt LLM performance]</li>
    <li>Zhang (2023) In-context learning for molecular property prediction [Empirical evidence for demonstration similarity effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a retrieval system selects demonstrations with high Tanimoto similarity to the query molecule, LLM property prediction accuracy will increase compared to random selection.</li>
                <li>If demonstrations are chosen from a different chemical class than the query, LLM predictions will be less accurate.</li>
                <li>If the number of structurally similar demonstrations increases, accuracy will plateau after a certain point.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If demonstrations are structurally similar but have divergent property labels (e.g., due to rare exceptions), LLM accuracy may not improve or could decrease.</li>
                <li>If demonstrations are selected based on latent (model-internal) similarity rather than explicit chemical similarity, the effect on accuracy is uncertain.</li>
                <li>If LLMs are fine-tuned on a diverse set of demonstrations, the impact of structural similarity in retrieval may be reduced.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs maintain high accuracy even when prompted with structurally dissimilar demonstrations, this would challenge the theory.</li>
                <li>If random demonstrations consistently outperform structure-aware retrieval, the theory would be called into question.</li>
                <li>If LLMs perform equally well regardless of demonstration selection, the theory would be falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs leverage textual cues or memorized knowledge to make accurate predictions despite low structural similarity. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and formalizes scattered empirical findings into a general principle for LLM-based molecular property prediction.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang (2023) In-context learning for molecular property prediction [Empirical evidence for demonstration similarity effects]</li>
    <li>Min (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [General in-context learning effects]</li>
    <li>Wang (2022) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [Retrieval improves LLM accuracy in NLP]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structure-Aware Demonstration Retrieval Principle",
    "theory_description": "This theory posits that the accuracy of large language models (LLMs) in molecular property prediction tasks is fundamentally determined by the structural similarity between the query molecule and the molecules used as demonstrations in the prompt. The more structurally similar the demonstrations are to the query, the more likely the LLM is to generate accurate property predictions, due to improved alignment between the model's learned representations and the relevant chemical features.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structural Similarity Drives Predictive Accuracy",
                "if": [
                    {
                        "subject": "demonstration_set",
                        "relation": "has_high_structural_similarity_to",
                        "object": "query_molecule"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "demonstration_set"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "predicts_property_of",
                        "object": "query_molecule_with_high_accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that in-context learning with demonstrations structurally similar to the query improves LLM performance in chemistry tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Analogous findings in vision and NLP domains indicate that similarity-based retrieval enhances few-shot learning.",
                        "uuids": []
                    },
                    {
                        "text": "Zhang (2023) found that LLMs prompted with high-Tanimoto-similarity molecules yield higher property prediction accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Wang (2022) demonstrated that retrieval-augmented generation in NLP tasks improves accuracy when retrieved examples are similar to the query.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Similarity-based retrieval is known to improve few-shot learning in NLP and vision, and some recent works in chemistry have shown improved LLM performance with similar demonstrations.",
                    "what_is_novel": "This law formalizes the relationship as a conditional principle specifically for molecular property prediction, emphasizing the primacy of structure-aware retrieval.",
                    "classification_explanation": "While related to existing work on in-context learning and retrieval augmentation, this law is a novel, explicit statement for molecular property prediction with LLMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhang (2023) In-context learning for molecular property prediction [Shows improved LLM performance with similar demonstrations]",
                        "Lester (2021) The Power of Scale for Parameter-Efficient Prompt Tuning [Demonstrates prompt selection impacts LLM performance]",
                        "Wang (2022) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [Retrieval improves LLM accuracy in NLP]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Diminishing Returns of Dissimilar Demonstrations",
                "if": [
                    {
                        "subject": "demonstration_set",
                        "relation": "has_low_structural_similarity_to",
                        "object": "query_molecule"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "demonstration_set"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "predicts_property_of",
                        "object": "query_molecule_with_low_accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Studies show that random or dissimilar demonstrations can degrade LLM performance in molecular property prediction.",
                        "uuids": []
                    },
                    {
                        "text": "Ablation studies in in-context learning reveal that irrelevant demonstrations can even harm performance.",
                        "uuids": []
                    },
                    {
                        "text": "Min (2022) found that random demonstrations can hurt LLM performance in in-context learning.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Negative effects of irrelevant or dissimilar demonstrations are observed in few-shot learning literature.",
                    "what_is_novel": "This law frames the effect as a diminishing return specific to molecular property prediction, suggesting a threshold beyond which demonstrations are unhelpful or harmful.",
                    "classification_explanation": "The general effect is known, but its explicit application and thresholding in molecular property prediction is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Min (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Shows that random demonstrations can hurt LLM performance]",
                        "Zhang (2023) In-context learning for molecular property prediction [Empirical evidence for demonstration similarity effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a retrieval system selects demonstrations with high Tanimoto similarity to the query molecule, LLM property prediction accuracy will increase compared to random selection.",
        "If demonstrations are chosen from a different chemical class than the query, LLM predictions will be less accurate.",
        "If the number of structurally similar demonstrations increases, accuracy will plateau after a certain point."
    ],
    "new_predictions_unknown": [
        "If demonstrations are structurally similar but have divergent property labels (e.g., due to rare exceptions), LLM accuracy may not improve or could decrease.",
        "If demonstrations are selected based on latent (model-internal) similarity rather than explicit chemical similarity, the effect on accuracy is uncertain.",
        "If LLMs are fine-tuned on a diverse set of demonstrations, the impact of structural similarity in retrieval may be reduced."
    ],
    "negative_experiments": [
        "If LLMs maintain high accuracy even when prompted with structurally dissimilar demonstrations, this would challenge the theory.",
        "If random demonstrations consistently outperform structure-aware retrieval, the theory would be called into question.",
        "If LLMs perform equally well regardless of demonstration selection, the theory would be falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs leverage textual cues or memorized knowledge to make accurate predictions despite low structural similarity.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some reports of LLMs generalizing well to novel scaffolds with limited or no similar demonstrations.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For properties determined by global molecular features (e.g., molecular weight), structural similarity may be less critical.",
        "In cases where the LLM has memorized specific molecules, demonstration selection may have reduced impact."
    ],
    "existing_theory": {
        "what_already_exists": "Similarity-based retrieval and its benefits in in-context learning are established in NLP and, to a lesser extent, in chemistry.",
        "what_is_novel": "The explicit, law-like formulation for molecular property prediction and the focus on structure-aware demonstration retrieval is novel.",
        "classification_explanation": "The theory synthesizes and formalizes scattered empirical findings into a general principle for LLM-based molecular property prediction.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Zhang (2023) In-context learning for molecular property prediction [Empirical evidence for demonstration similarity effects]",
            "Min (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [General in-context learning effects]",
            "Wang (2022) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [Retrieval improves LLM accuracy in NLP]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-638",
    "original_theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Law of Structure-Aware Demonstration Retrieval in Molecular Property Prediction",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
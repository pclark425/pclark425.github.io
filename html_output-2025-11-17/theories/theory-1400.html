<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Internal Representation Alignment through Self-Reflection - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1400</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1400</p>
                <p><strong>Name:</strong> Internal Representation Alignment through Self-Reflection</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.</p>
                <p><strong>Description:</strong> This theory proposes that self-reflection in language models functions by aligning the model's internal representations of the problem, answer, and evaluation criteria. Through reflection, the model re-encodes its own output, compares it to its internal standards, and updates its next response to better match the implicit or explicit criteria for correctness, coherence, or utility.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Re-encoding and Comparison Mechanism (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; reflects_on &#8594; its own answer</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; re-encodes &#8594; answer and evaluation criteria<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; compares &#8594; answer representation to criteria representation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Neural analysis shows that LMs can represent both answers and evaluation criteria in their hidden states. </li>
    <li>Prompting for reflection elicits more explicit evaluation and comparison behaviors in model outputs. </li>
    <li>Probing studies reveal that LMs' internal activations change when asked to critique or reflect on their own outputs. </li>
    <li>Chain-of-thought and self-reflection prompts lead to more structured, criteria-aware responses. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends known representational capabilities to the reflection process, which is not previously formalized.</p>            <p><strong>What Already Exists:</strong> LMs are known to encode both questions and answers in their internal states.</p>            <p><strong>What is Novel:</strong> This law posits an explicit re-encoding and comparison process during reflection, not just during initial generation.</p>
            <p><strong>References:</strong> <ul>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [internal representations in LMs]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in LMs [reasoning via internal representations]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [reflection and refinement behaviors]</li>
</ul>
            <h3>Statement 1: Alignment Drives Iterative Improvement (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; detects_misalignment &#8594; between answer and evaluation criteria</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; modifies &#8594; subsequent answer to reduce misalignment</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical evidence shows that LMs can revise answers to better match explicit instructions or criteria after reflection. </li>
    <li>Reflection often leads to increased factuality, coherence, or adherence to task requirements. </li>
    <li>Iterative self-refinement improves answer quality in tasks such as code generation and open-ended question answering. </li>
    <li>Model-written evaluations can guide subsequent answer revisions. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes empirical findings into a mechanistic theory of alignment during reflection.</p>            <p><strong>What Already Exists:</strong> LMs can be prompted to revise answers to better match instructions.</p>            <p><strong>What is Novel:</strong> This law formalizes the process as an internal alignment mechanism, not just surface-level revision.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [iterative improvement]</li>
    <li>Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [model self-evaluation and alignment]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If explicit evaluation criteria are provided during reflection, the model's answers will align more closely with those criteria over iterations.</li>
                <li>If the model's internal representations of the answer and criteria are measured (e.g., via probing), their similarity will increase after each reflection cycle.</li>
                <li>If reflection is interrupted or omitted, answer quality will plateau or improve less than with reflection.</li>
                <li>If the model is prompted to reflect on both answer and criteria, improvement will be greater than reflecting on answer alone.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained to reflect on answers using adversarial or contradictory criteria, it may develop internal conflicts or oscillatory behaviors.</li>
                <li>If the alignment process is made explicit in the architecture (e.g., via a dedicated comparison module), answer quality may improve beyond standard reflection.</li>
                <li>If reflection is performed with noisy or misleading criteria, the model may degrade answer quality or develop pathological behaviors.</li>
                <li>If the model is allowed to reflect indefinitely, it may converge to a fixed point or cycle between a set of answers.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If models do not show increased alignment between answers and criteria after reflection, the theory is challenged.</li>
                <li>If internal representations do not change or become more similar after reflection, the re-encoding and comparison mechanism is called into question.</li>
                <li>If answer quality does not improve with reflection, the alignment mechanism may not be the primary driver.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where reflection leads to worse alignment or introduces new errors are not fully explained. </li>
    <li>The role of external knowledge or retrieval in the reflection process is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends existing knowledge by positing a mechanistic alignment process during reflection.</p>
            <p><strong>References:</strong> <ul>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [internal representations]</li>
    <li>Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [self-evaluation and alignment]</li>
    <li>Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [reflection and refinement behaviors]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Internal Representation Alignment through Self-Reflection",
    "theory_description": "This theory proposes that self-reflection in language models functions by aligning the model's internal representations of the problem, answer, and evaluation criteria. Through reflection, the model re-encodes its own output, compares it to its internal standards, and updates its next response to better match the implicit or explicit criteria for correctness, coherence, or utility.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Re-encoding and Comparison Mechanism",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "reflects_on",
                        "object": "its own answer"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "re-encodes",
                        "object": "answer and evaluation criteria"
                    },
                    {
                        "subject": "language model",
                        "relation": "compares",
                        "object": "answer representation to criteria representation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Neural analysis shows that LMs can represent both answers and evaluation criteria in their hidden states.",
                        "uuids": []
                    },
                    {
                        "text": "Prompting for reflection elicits more explicit evaluation and comparison behaviors in model outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Probing studies reveal that LMs' internal activations change when asked to critique or reflect on their own outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought and self-reflection prompts lead to more structured, criteria-aware responses.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LMs are known to encode both questions and answers in their internal states.",
                    "what_is_novel": "This law posits an explicit re-encoding and comparison process during reflection, not just during initial generation.",
                    "classification_explanation": "The law extends known representational capabilities to the reflection process, which is not previously formalized.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [internal representations in LMs]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in LMs [reasoning via internal representations]",
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [reflection and refinement behaviors]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Alignment Drives Iterative Improvement",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "detects_misalignment",
                        "object": "between answer and evaluation criteria"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "modifies",
                        "object": "subsequent answer to reduce misalignment"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical evidence shows that LMs can revise answers to better match explicit instructions or criteria after reflection.",
                        "uuids": []
                    },
                    {
                        "text": "Reflection often leads to increased factuality, coherence, or adherence to task requirements.",
                        "uuids": []
                    },
                    {
                        "text": "Iterative self-refinement improves answer quality in tasks such as code generation and open-ended question answering.",
                        "uuids": []
                    },
                    {
                        "text": "Model-written evaluations can guide subsequent answer revisions.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "LMs can be prompted to revise answers to better match instructions.",
                    "what_is_novel": "This law formalizes the process as an internal alignment mechanism, not just surface-level revision.",
                    "classification_explanation": "The law synthesizes empirical findings into a mechanistic theory of alignment during reflection.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [iterative improvement]",
                        "Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [model self-evaluation and alignment]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If explicit evaluation criteria are provided during reflection, the model's answers will align more closely with those criteria over iterations.",
        "If the model's internal representations of the answer and criteria are measured (e.g., via probing), their similarity will increase after each reflection cycle.",
        "If reflection is interrupted or omitted, answer quality will plateau or improve less than with reflection.",
        "If the model is prompted to reflect on both answer and criteria, improvement will be greater than reflecting on answer alone."
    ],
    "new_predictions_unknown": [
        "If a model is trained to reflect on answers using adversarial or contradictory criteria, it may develop internal conflicts or oscillatory behaviors.",
        "If the alignment process is made explicit in the architecture (e.g., via a dedicated comparison module), answer quality may improve beyond standard reflection.",
        "If reflection is performed with noisy or misleading criteria, the model may degrade answer quality or develop pathological behaviors.",
        "If the model is allowed to reflect indefinitely, it may converge to a fixed point or cycle between a set of answers."
    ],
    "negative_experiments": [
        "If models do not show increased alignment between answers and criteria after reflection, the theory is challenged.",
        "If internal representations do not change or become more similar after reflection, the re-encoding and comparison mechanism is called into question.",
        "If answer quality does not improve with reflection, the alignment mechanism may not be the primary driver."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where reflection leads to worse alignment or introduces new errors are not fully explained.",
            "uuids": []
        },
        {
            "text": "The role of external knowledge or retrieval in the reflection process is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models fail to improve alignment in tasks requiring deep reasoning or external knowledge not present in training.",
            "uuids": []
        },
        {
            "text": "Reflection can sometimes reinforce initial errors if the evaluation criteria are misunderstood or misapplied.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For tasks with ambiguous or underspecified criteria, alignment may be ill-defined or unstable.",
        "In models with limited capacity, repeated reflection may not yield further alignment.",
        "If the model is overconfident in its initial answer, reflection may not trigger meaningful revision."
    ],
    "existing_theory": {
        "what_already_exists": "Internal representations and answer revision are known phenomena in LMs.",
        "what_is_novel": "The theory formalizes reflection as an explicit alignment process between answer and evaluation criteria.",
        "classification_explanation": "The theory extends existing knowledge by positing a mechanistic alignment process during reflection.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [internal representations]",
            "Perez et al. (2022) Discovering Language Model Behaviors with Model-Written Evaluations [self-evaluation and alignment]",
            "Madaan et al. (2023) Self-Refine: Iterative Refinement with Self-Feedback [reflection and refinement behaviors]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform self-reflection and improve their answer quality through multiple iterations of generate-then-reflect.",
    "original_theory_id": "theory-621",
    "original_theory_name": "Self-Reflection as a Calibration and Bias Amplification Process",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
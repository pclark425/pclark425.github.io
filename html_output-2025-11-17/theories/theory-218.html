<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>External Computation Delegation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-218</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-218</p>
                <p><strong>Name:</strong> External Computation Delegation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that language models perform arithmetic not by developing native numerical reasoning capabilities, but by learning to emulate and delegate to external computational processes encountered during training. Specifically, LMs learn representations that simulate symbolic manipulation systems (like calculators, programming interpreters, or step-by-step human arithmetic procedures) by observing their input-output patterns in training data. The model develops internal 'subroutines' that mimic these external computational tools, effectively learning to be a meta-system that delegates arithmetic operations to learned simulations of specialized computational systems. This explains why LMs perform better on arithmetic formats that resemble calculator interfaces or programming syntax, why they struggle with operations that lack clear procedural templates in their training data, and why their performance is heavily influenced by factors like tokenization, number format, and problem presentation that would be irrelevant to true numerical understanding but critical to procedural execution.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models do not develop native numerical reasoning capabilities, but instead learn to simulate external computational systems (calculators, interpreters, human procedural algorithms) by observing their behavior in training data.</li>
                <li>Arithmetic performance in LMs is primarily determined by the availability and clarity of procedural templates in training data that demonstrate step-by-step symbolic manipulation.</li>
                <li>The internal representations used for arithmetic in LMs correspond to learned emulations of symbolic computation systems rather than semantic numerical understanding or abstract mathematical principles.</li>
                <li>Attention mechanisms in transformers serve as the execution engine for these simulated procedural algorithms, with different attention heads specializing in different computational steps (e.g., digit extraction, carrying operations, position tracking).</li>
                <li>Performance degradation on out-of-distribution arithmetic problems occurs because the model lacks training examples of external systems solving similar problems, not because of inherent numerical reasoning limitations.</li>
                <li>The effectiveness of chain-of-thought prompting derives from explicitly invoking the model's learned simulation of step-by-step external computational procedures, providing a scaffold for procedural execution.</li>
                <li>Models trained on code perform better at arithmetic because code provides dense examples of arithmetic operations being delegated to and executed by computational systems, with explicit syntax for operations.</li>
                <li>The format and syntax of arithmetic problems significantly affects performance because different formats activate different learned simulations of external computational tools (e.g., calculator mode vs. programming mode vs. written arithmetic).</li>
                <li>Tokenization granularity affects arithmetic performance because it determines the level at which symbolic manipulation can be simulated - finer tokenization enables more precise procedural control.</li>
                <li>Errors in model arithmetic follow patterns consistent with procedural execution failures (wrong steps, incorrect carrying, position errors) rather than semantic misunderstanding of numerical concepts.</li>
                <li>The model's capacity to simulate procedural algorithms is limited by architectural constraints (depth, width, attention heads), explaining why larger models show better arithmetic performance - they can simulate more complex procedures.</li>
                <li>Length generalization failures occur because procedural templates learned from training data are implicitly tied to specific input lengths, and the model cannot extrapolate the procedure to longer inputs without explicit examples.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Language models show dramatically improved arithmetic performance when prompted with chain-of-thought reasoning that mimics step-by-step human calculation procedures, suggesting they learn to simulate external procedural computation rather than developing internal numerical understanding. </li>
    <li>Models perform significantly better on arithmetic when the format resembles programming language syntax or calculator input/output patterns, indicating they leverage learned representations of external computational tools. </li>
    <li>Language models trained on code (which contains many examples of arithmetic operations being delegated to computational systems) show superior arithmetic performance compared to models trained only on natural language. </li>
    <li>Tool-augmented language models that explicitly call external calculators or Python interpreters dramatically outperform base models on arithmetic, suggesting that base models may be attempting to simulate such delegation internally with limited success. </li>
    <li>Analysis of attention patterns during arithmetic tasks shows structured, algorithmic patterns resembling the step-by-step execution of symbolic procedures rather than holistic numerical reasoning. </li>
    <li>Language models exhibit systematic errors that mirror the types of mistakes made by humans following procedural algorithms incorrectly (such as carrying errors or digit position mistakes), rather than errors expected from corrupted numerical representations or semantic misunderstanding. </li>
    <li>Arithmetic performance is strongly affected by the frequency of specific number combinations in training data, consistent with learning procedural templates from observed examples rather than deriving answers from numerical principles. </li>
    <li>Models show better performance on arithmetic operations when intermediate steps are made explicit, similar to how external computational tools show their work, supporting the idea that models simulate step-by-step procedural execution. </li>
    <li>Tokenization schemes significantly impact arithmetic performance, with character-level or digit-level tokenization often outperforming word-level tokenization for numerical tasks, suggesting that the granularity of symbolic manipulation matters for procedural simulation. </li>
    <li>Models struggle with length generalization in arithmetic (performing operations on numbers with more digits than seen in training), consistent with learning fixed procedural templates rather than generalizable numerical reasoning. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Training language models on synthetic data showing calculator-style input/output pairs (e.g., '2+2=4', '15*3=45') with explicit step-by-step execution should improve arithmetic performance more efficiently than training on natural language descriptions of arithmetic.</li>
                <li>Prompting models with explicit 'calculator mode' or 'computation mode' prefixes should improve arithmetic accuracy by more strongly activating learned simulations of computational tools.</li>
                <li>Fine-tuning models on transcripts of humans using calculators or showing their work step-by-step should transfer to improved arithmetic performance on novel problems of similar structure.</li>
                <li>Models should perform better on arithmetic operations that are more commonly delegated to external tools in training data (e.g., multiplication of large numbers) compared to operations typically done mentally (e.g., small single-digit addition).</li>
                <li>Providing models with a 'scratchpad' or intermediate computation space should improve performance proportionally to how often such spaces appear in training data for similar problems.</li>
                <li>Models should show better arithmetic performance when numbers are tokenized at the digit level compared to whole-number tokenization, as this enables finer-grained procedural simulation.</li>
                <li>Arithmetic performance should correlate with the frequency of specific operation types (addition, multiplication, etc.) in training data, with more frequent operations showing better performance.</li>
                <li>Models should perform better on arithmetic problems presented in formats that exactly match common calculator or programming language syntax seen in training data.</li>
                <li>Interventions that disrupt the sequential, step-by-step processing of arithmetic (e.g., shuffling attention patterns) should cause more severe performance degradation than interventions that disrupt holistic processing.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a language model were trained exclusively on data where all arithmetic is performed by external tools (with the tools' step-by-step processes visible), it might develop superior arithmetic capabilities compared to models trained on mixed data, or it might become entirely dependent on seeing such procedural templates and fail to generalize.</li>
                <li>Creating synthetic training data that shows fictional 'super-calculators' solving arithmetic problems using novel but consistent algorithmic procedures (e.g., a base-7 calculator with unusual carrying rules) might allow models to learn and apply these fictional procedures, demonstrating pure procedural learning without grounding in actual mathematics.</li>
                <li>If models truly delegate to learned simulations of external systems, then adversarial prompts that 'confuse' which external system to simulate (e.g., mixing calculator syntax with programming syntax) might cause catastrophic performance degradation or reveal the boundaries between different learned simulations.</li>
                <li>Training models on data from cultures or historical periods with different arithmetic notation systems and procedural algorithms (e.g., Roman numerals, abacus methods, Vedic mathematics) might result in models that can 'switch' between different computational paradigms, revealing whether the delegation is truly system-agnostic.</li>
                <li>If the theory is correct, then directly editing or ablating the model's representations of specific procedural steps (identified through mechanistic interpretability) should cause predictable, step-specific failures in arithmetic performance (e.g., ablating 'carry' representations should specifically impair multi-digit addition).</li>
                <li>Training models with explicit 'procedure labels' in the training data (e.g., 'using long multiplication method: ...') might enable models to learn multiple procedural strategies and select between them, potentially improving robustness and generalization.</li>
                <li>If models are simulating external procedures, then providing them with 'buggy' procedural examples during training might cause them to learn and reproduce those bugs systematically, similar to how students learn incorrect algorithms.</li>
                <li>Models might be able to learn to simulate novel arithmetic procedures (e.g., a new algorithm for multiplication) from just a few examples if those examples clearly demonstrate the step-by-step process, showing rapid procedural learning.</li>
                <li>Cross-lingual transfer of arithmetic abilities might be stronger than expected if the underlying procedural simulations are language-agnostic, or weaker if different languages are associated with different computational tools in training data.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If language models trained exclusively on natural language text with no exposure to calculator interfaces, code, or step-by-step arithmetic procedures still develop strong arithmetic capabilities comparable to code-trained models, this would challenge the delegation theory.</li>
                <li>If mechanistic interpretability studies reveal that models use fundamentally different computational mechanisms for arithmetic compared to the procedural algorithms they were exposed to in training (e.g., parallel processing rather than sequential steps), this would contradict the simulation hypothesis.</li>
                <li>If models show equal performance on arithmetic problems regardless of whether the format matches any external computational system seen in training data, this would undermine the format-dependency prediction.</li>
                <li>If attention patterns during arithmetic do not show algorithmic, step-by-step structure but instead show holistic, parallel processing patterns inconsistent with procedural execution, this would challenge the procedural simulation claim.</li>
                <li>If tool-augmented models (with actual external calculators) do not significantly outperform base models on arithmetic tasks, this would question whether delegation to external computation is a viable or learned strategy.</li>
                <li>If models perform better on arithmetic operations that are rarely delegated to external tools in human practice compared to commonly delegated operations (controlling for complexity), this would contradict the training data frequency prediction.</li>
                <li>If models trained with digit-level tokenization show no improvement over word-level tokenization for arithmetic tasks, this would challenge the claim that procedural simulation requires fine-grained symbolic manipulation.</li>
                <li>If ablating or disrupting attention heads identified as implementing specific procedural steps does not cause the predicted step-specific failures, this would undermine the mechanistic claims of the theory.</li>
                <li>If models can successfully generalize arithmetic procedures to significantly longer numbers than seen in training without any length-specific training, this would challenge the claim that procedural templates are length-dependent.</li>
                <li>If models show strong arithmetic performance on operations presented in formats never seen in training data and unlike any external computational tool, this would contradict the core delegation hypothesis.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some language models show emergent arithmetic capabilities at certain scale thresholds without corresponding changes in training data composition, which is not fully explained by the delegation theory alone - though it could be that larger models have sufficient capacity to learn and execute more complex procedural simulations. </li>
    <li>The specific mechanisms by which transformer architectures implement procedural simulation are not fully characterized, leaving open questions about the computational substrate and how attention, feedforward layers, and residual connections work together to execute simulated procedures. </li>
    <li>The theory does not fully account for why some models show partial success on arithmetic problems they solve incorrectly (e.g., getting some digits correct but not others), suggesting some form of approximate numerical reasoning or partial procedural execution beyond pure procedural simulation. </li>
    <li>The role of positional encodings in enabling arithmetic capabilities is not fully explained - while procedural simulation requires position tracking, the specific way positional encodings enable this is unclear. </li>
    <li>The theory does not fully explain why certain architectural modifications (e.g., specific types of attention mechanisms, layer normalization schemes) differentially affect arithmetic performance. </li>
    <li>The relationship between model depth (number of layers) and arithmetic capability is not fully explained - while deeper models might simulate more complex procedures, the specific mapping between depth and procedural complexity is unclear. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [Related work on explicit tool use, but does not propose that base models internally simulate external tools]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Related work on external computation spaces, but focuses on explicit scratchpads rather than internal simulation]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Related work on procedural reasoning, but does not frame it as simulation of external computational systems]</li>
    <li>Mialon et al. (2023) Augmented Language Models: a Survey [Comprehensive survey of tool-augmented LMs, but treats tool use as external augmentation rather than internal simulation strategy]</li>
    <li>Hanna et al. (2023) How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model [Mechanistic interpretability work that identifies algorithmic patterns but does not frame them as learned simulations of external systems]</li>
    <li>Stolfo et al. (2023) A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis [Mechanistic work on arithmetic but does not propose the delegation/simulation framework]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "External Computation Delegation Theory",
    "theory_description": "This theory proposes that language models perform arithmetic not by developing native numerical reasoning capabilities, but by learning to emulate and delegate to external computational processes encountered during training. Specifically, LMs learn representations that simulate symbolic manipulation systems (like calculators, programming interpreters, or step-by-step human arithmetic procedures) by observing their input-output patterns in training data. The model develops internal 'subroutines' that mimic these external computational tools, effectively learning to be a meta-system that delegates arithmetic operations to learned simulations of specialized computational systems. This explains why LMs perform better on arithmetic formats that resemble calculator interfaces or programming syntax, why they struggle with operations that lack clear procedural templates in their training data, and why their performance is heavily influenced by factors like tokenization, number format, and problem presentation that would be irrelevant to true numerical understanding but critical to procedural execution.",
    "supporting_evidence": [
        {
            "text": "Language models show dramatically improved arithmetic performance when prompted with chain-of-thought reasoning that mimics step-by-step human calculation procedures, suggesting they learn to simulate external procedural computation rather than developing internal numerical understanding.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS",
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models, arXiv"
            ]
        },
        {
            "text": "Models perform significantly better on arithmetic when the format resembles programming language syntax or calculator input/output patterns, indicating they leverage learned representations of external computational tools.",
            "citations": [
                "Drori et al. (2022) A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level, PNAS",
                "Chen et al. (2021) Evaluating Large Language Models Trained on Code, arXiv"
            ]
        },
        {
            "text": "Language models trained on code (which contains many examples of arithmetic operations being delegated to computational systems) show superior arithmetic performance compared to models trained only on natural language.",
            "citations": [
                "Lewkowycz et al. (2022) Solving Quantitative Reasoning Problems with Language Models, NeurIPS",
                "Austin et al. (2021) Program Synthesis with Large Language Models, arXiv"
            ]
        },
        {
            "text": "Tool-augmented language models that explicitly call external calculators or Python interpreters dramatically outperform base models on arithmetic, suggesting that base models may be attempting to simulate such delegation internally with limited success.",
            "citations": [
                "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools, arXiv",
                "Mialon et al. (2023) Augmented Language Models: a Survey, arXiv"
            ]
        },
        {
            "text": "Analysis of attention patterns during arithmetic tasks shows structured, algorithmic patterns resembling the step-by-step execution of symbolic procedures rather than holistic numerical reasoning.",
            "citations": [
                "Hanna et al. (2023) How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model, NeurIPS",
                "Stolfo et al. (2023) A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis, EMNLP"
            ]
        },
        {
            "text": "Language models exhibit systematic errors that mirror the types of mistakes made by humans following procedural algorithms incorrectly (such as carrying errors or digit position mistakes), rather than errors expected from corrupted numerical representations or semantic misunderstanding.",
            "citations": [
                "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning, EMNLP",
                "Patel et al. (2021) Are NLP Models really able to solve simple math word problems?, NAACL"
            ]
        },
        {
            "text": "Arithmetic performance is strongly affected by the frequency of specific number combinations in training data, consistent with learning procedural templates from observed examples rather than deriving answers from numerical principles.",
            "citations": [
                "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning, EMNLP"
            ]
        },
        {
            "text": "Models show better performance on arithmetic operations when intermediate steps are made explicit, similar to how external computational tools show their work, supporting the idea that models simulate step-by-step procedural execution.",
            "citations": [
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models, arXiv",
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, NeurIPS"
            ]
        },
        {
            "text": "Tokenization schemes significantly impact arithmetic performance, with character-level or digit-level tokenization often outperforming word-level tokenization for numerical tasks, suggesting that the granularity of symbolic manipulation matters for procedural simulation.",
            "citations": [
                "Nogueira et al. (2021) Investigating the Limitations of Transformers with Simple Arithmetic Tasks, arXiv"
            ]
        },
        {
            "text": "Models struggle with length generalization in arithmetic (performing operations on numbers with more digits than seen in training), consistent with learning fixed procedural templates rather than generalizable numerical reasoning.",
            "citations": [
                "Anil et al. (2022) Exploring Length Generalization in Large Language Models, NeurIPS"
            ]
        }
    ],
    "theory_statements": [
        "Language models do not develop native numerical reasoning capabilities, but instead learn to simulate external computational systems (calculators, interpreters, human procedural algorithms) by observing their behavior in training data.",
        "Arithmetic performance in LMs is primarily determined by the availability and clarity of procedural templates in training data that demonstrate step-by-step symbolic manipulation.",
        "The internal representations used for arithmetic in LMs correspond to learned emulations of symbolic computation systems rather than semantic numerical understanding or abstract mathematical principles.",
        "Attention mechanisms in transformers serve as the execution engine for these simulated procedural algorithms, with different attention heads specializing in different computational steps (e.g., digit extraction, carrying operations, position tracking).",
        "Performance degradation on out-of-distribution arithmetic problems occurs because the model lacks training examples of external systems solving similar problems, not because of inherent numerical reasoning limitations.",
        "The effectiveness of chain-of-thought prompting derives from explicitly invoking the model's learned simulation of step-by-step external computational procedures, providing a scaffold for procedural execution.",
        "Models trained on code perform better at arithmetic because code provides dense examples of arithmetic operations being delegated to and executed by computational systems, with explicit syntax for operations.",
        "The format and syntax of arithmetic problems significantly affects performance because different formats activate different learned simulations of external computational tools (e.g., calculator mode vs. programming mode vs. written arithmetic).",
        "Tokenization granularity affects arithmetic performance because it determines the level at which symbolic manipulation can be simulated - finer tokenization enables more precise procedural control.",
        "Errors in model arithmetic follow patterns consistent with procedural execution failures (wrong steps, incorrect carrying, position errors) rather than semantic misunderstanding of numerical concepts.",
        "The model's capacity to simulate procedural algorithms is limited by architectural constraints (depth, width, attention heads), explaining why larger models show better arithmetic performance - they can simulate more complex procedures.",
        "Length generalization failures occur because procedural templates learned from training data are implicitly tied to specific input lengths, and the model cannot extrapolate the procedure to longer inputs without explicit examples."
    ],
    "new_predictions_likely": [
        "Training language models on synthetic data showing calculator-style input/output pairs (e.g., '2+2=4', '15*3=45') with explicit step-by-step execution should improve arithmetic performance more efficiently than training on natural language descriptions of arithmetic.",
        "Prompting models with explicit 'calculator mode' or 'computation mode' prefixes should improve arithmetic accuracy by more strongly activating learned simulations of computational tools.",
        "Fine-tuning models on transcripts of humans using calculators or showing their work step-by-step should transfer to improved arithmetic performance on novel problems of similar structure.",
        "Models should perform better on arithmetic operations that are more commonly delegated to external tools in training data (e.g., multiplication of large numbers) compared to operations typically done mentally (e.g., small single-digit addition).",
        "Providing models with a 'scratchpad' or intermediate computation space should improve performance proportionally to how often such spaces appear in training data for similar problems.",
        "Models should show better arithmetic performance when numbers are tokenized at the digit level compared to whole-number tokenization, as this enables finer-grained procedural simulation.",
        "Arithmetic performance should correlate with the frequency of specific operation types (addition, multiplication, etc.) in training data, with more frequent operations showing better performance.",
        "Models should perform better on arithmetic problems presented in formats that exactly match common calculator or programming language syntax seen in training data.",
        "Interventions that disrupt the sequential, step-by-step processing of arithmetic (e.g., shuffling attention patterns) should cause more severe performance degradation than interventions that disrupt holistic processing."
    ],
    "new_predictions_unknown": [
        "If a language model were trained exclusively on data where all arithmetic is performed by external tools (with the tools' step-by-step processes visible), it might develop superior arithmetic capabilities compared to models trained on mixed data, or it might become entirely dependent on seeing such procedural templates and fail to generalize.",
        "Creating synthetic training data that shows fictional 'super-calculators' solving arithmetic problems using novel but consistent algorithmic procedures (e.g., a base-7 calculator with unusual carrying rules) might allow models to learn and apply these fictional procedures, demonstrating pure procedural learning without grounding in actual mathematics.",
        "If models truly delegate to learned simulations of external systems, then adversarial prompts that 'confuse' which external system to simulate (e.g., mixing calculator syntax with programming syntax) might cause catastrophic performance degradation or reveal the boundaries between different learned simulations.",
        "Training models on data from cultures or historical periods with different arithmetic notation systems and procedural algorithms (e.g., Roman numerals, abacus methods, Vedic mathematics) might result in models that can 'switch' between different computational paradigms, revealing whether the delegation is truly system-agnostic.",
        "If the theory is correct, then directly editing or ablating the model's representations of specific procedural steps (identified through mechanistic interpretability) should cause predictable, step-specific failures in arithmetic performance (e.g., ablating 'carry' representations should specifically impair multi-digit addition).",
        "Training models with explicit 'procedure labels' in the training data (e.g., 'using long multiplication method: ...') might enable models to learn multiple procedural strategies and select between them, potentially improving robustness and generalization.",
        "If models are simulating external procedures, then providing them with 'buggy' procedural examples during training might cause them to learn and reproduce those bugs systematically, similar to how students learn incorrect algorithms.",
        "Models might be able to learn to simulate novel arithmetic procedures (e.g., a new algorithm for multiplication) from just a few examples if those examples clearly demonstrate the step-by-step process, showing rapid procedural learning.",
        "Cross-lingual transfer of arithmetic abilities might be stronger than expected if the underlying procedural simulations are language-agnostic, or weaker if different languages are associated with different computational tools in training data."
    ],
    "negative_experiments": [
        "If language models trained exclusively on natural language text with no exposure to calculator interfaces, code, or step-by-step arithmetic procedures still develop strong arithmetic capabilities comparable to code-trained models, this would challenge the delegation theory.",
        "If mechanistic interpretability studies reveal that models use fundamentally different computational mechanisms for arithmetic compared to the procedural algorithms they were exposed to in training (e.g., parallel processing rather than sequential steps), this would contradict the simulation hypothesis.",
        "If models show equal performance on arithmetic problems regardless of whether the format matches any external computational system seen in training data, this would undermine the format-dependency prediction.",
        "If attention patterns during arithmetic do not show algorithmic, step-by-step structure but instead show holistic, parallel processing patterns inconsistent with procedural execution, this would challenge the procedural simulation claim.",
        "If tool-augmented models (with actual external calculators) do not significantly outperform base models on arithmetic tasks, this would question whether delegation to external computation is a viable or learned strategy.",
        "If models perform better on arithmetic operations that are rarely delegated to external tools in human practice compared to commonly delegated operations (controlling for complexity), this would contradict the training data frequency prediction.",
        "If models trained with digit-level tokenization show no improvement over word-level tokenization for arithmetic tasks, this would challenge the claim that procedural simulation requires fine-grained symbolic manipulation.",
        "If ablating or disrupting attention heads identified as implementing specific procedural steps does not cause the predicted step-specific failures, this would undermine the mechanistic claims of the theory.",
        "If models can successfully generalize arithmetic procedures to significantly longer numbers than seen in training without any length-specific training, this would challenge the claim that procedural templates are length-dependent.",
        "If models show strong arithmetic performance on operations presented in formats never seen in training data and unlike any external computational tool, this would contradict the core delegation hypothesis."
    ],
    "unaccounted_for": [
        {
            "text": "Some language models show emergent arithmetic capabilities at certain scale thresholds without corresponding changes in training data composition, which is not fully explained by the delegation theory alone - though it could be that larger models have sufficient capacity to learn and execute more complex procedural simulations.",
            "citations": [
                "Wei et al. (2022) Emergent Abilities of Large Language Models, TMLR"
            ]
        },
        {
            "text": "The specific mechanisms by which transformer architectures implement procedural simulation are not fully characterized, leaving open questions about the computational substrate and how attention, feedforward layers, and residual connections work together to execute simulated procedures.",
            "citations": [
                "Elhage et al. (2021) A Mathematical Framework for Transformer Circuits, Anthropic"
            ]
        },
        {
            "text": "The theory does not fully account for why some models show partial success on arithmetic problems they solve incorrectly (e.g., getting some digits correct but not others), suggesting some form of approximate numerical reasoning or partial procedural execution beyond pure procedural simulation.",
            "citations": [
                "Nogueira et al. (2021) Investigating the Limitations of Transformers with Simple Arithmetic Tasks, arXiv"
            ]
        },
        {
            "text": "The role of positional encodings in enabling arithmetic capabilities is not fully explained - while procedural simulation requires position tracking, the specific way positional encodings enable this is unclear.",
            "citations": [
                "Anil et al. (2022) Exploring Length Generalization in Large Language Models, NeurIPS"
            ]
        },
        {
            "text": "The theory does not fully explain why certain architectural modifications (e.g., specific types of attention mechanisms, layer normalization schemes) differentially affect arithmetic performance.",
            "citations": [
                "Elhage et al. (2021) A Mathematical Framework for Transformer Circuits, Anthropic"
            ]
        },
        {
            "text": "The relationship between model depth (number of layers) and arithmetic capability is not fully explained - while deeper models might simulate more complex procedures, the specific mapping between depth and procedural complexity is unclear.",
            "citations": [
                "Wei et al. (2022) Emergent Abilities of Large Language Models, TMLR"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that language models develop internal representations that correlate with numerical magnitude and show systematic organization (e.g., smaller numbers clustered together), suggesting some degree of semantic numerical understanding beyond pure procedural simulation. However, this could be compatible with the delegation theory if magnitude representations are used as inputs to procedural algorithms (e.g., for comparison operations or estimation).",
            "citations": [
                "Nasr et al. (2023) Transformers learn to implement precondition-based reasoning, arXiv",
                "Geva et al. (2020) Injecting Numerical Reasoning Skills into Language Models, ACL"
            ]
        },
        {
            "text": "Models sometimes succeed on arithmetic problems presented in novel formats never seen in training data, which challenges the strict format-dependency prediction of the delegation theory, though this could reflect partial generalization of learned procedures or composition of multiple learned procedural elements.",
            "citations": [
                "Anil et al. (2022) Exploring Length Generalization in Large Language Models, NeurIPS"
            ]
        },
        {
            "text": "Some evidence suggests that models can perform approximate arithmetic or numerical estimation, which may not require step-by-step procedural execution and could indicate alternative mechanisms beyond delegation to simulated external tools.",
            "citations": [
                "Geva et al. (2020) Injecting Numerical Reasoning Skills into Language Models, ACL"
            ]
        }
    ],
    "special_cases": [
        "The theory applies most strongly to explicit arithmetic operations (addition, multiplication, etc.) and may not fully account for implicit numerical reasoning embedded in natural language tasks (e.g., understanding 'a few' vs 'many').",
        "Very small language models may not have sufficient capacity to learn complex procedural simulations, potentially relying on simpler pattern matching or memorization instead, making the theory less applicable to small models.",
        "The theory's predictions are strongest for arithmetic operations that have clear, deterministic procedural algorithms in training data (e.g., long multiplication), and weaker for operations requiring heuristic or approximate methods (e.g., estimation, rounding).",
        "Models trained with specific architectural modifications (e.g., relative position encodings, specialized numerical embeddings, memory-augmented architectures) may develop hybrid strategies combining delegation with other mechanisms.",
        "The theory may apply differently to different arithmetic operations - addition and multiplication may rely more heavily on procedural simulation, while comparison operations might use magnitude representations more directly.",
        "For very simple arithmetic (e.g., single-digit addition), models might use direct memorization rather than procedural simulation, as the entire operation space can be memorized.",
        "The theory's applicability may vary with number representation - decimal numbers may invoke different procedural simulations than fractions, percentages, or scientific notation.",
        "Cross-lingual arithmetic may show different patterns if different languages are associated with different computational tools or procedural traditions in training data.",
        "The theory may not fully account for arithmetic embedded in complex word problems, where parsing and problem representation may be as important as the arithmetic procedure itself."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [Related work on explicit tool use, but does not propose that base models internally simulate external tools]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Related work on external computation spaces, but focuses on explicit scratchpads rather than internal simulation]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Related work on procedural reasoning, but does not frame it as simulation of external computational systems]",
            "Mialon et al. (2023) Augmented Language Models: a Survey [Comprehensive survey of tool-augmented LMs, but treats tool use as external augmentation rather than internal simulation strategy]",
            "Hanna et al. (2023) How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model [Mechanistic interpretability work that identifies algorithmic patterns but does not frame them as learned simulations of external systems]",
            "Stolfo et al. (2023) A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis [Mechanistic work on arithmetic but does not propose the delegation/simulation framework]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory of how language models perform arithmetic.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-59",
    "original_theory_name": "External Computation Delegation Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
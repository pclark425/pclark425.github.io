<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memorization-Computation Spectrum Theory (Revised) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-56</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-56</p>
                <p><strong>Name:</strong> Memorization-Computation Spectrum Theory (Revised)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> Language models exist on a spectrum between pure memorization and algorithmic computation when performing arithmetic, with their position determined by multiple interacting factors: (1) training data coverage and frequency, (2) model capacity and architecture, (3) problem complexity and format, (4) tokenization and representation, and (5) training methodology. Models primarily memorize input-output mappings for frequent, simple problems seen during training, but must rely on learned algorithmic procedures for rare or complex problems. The balance is not binary but exists on a continuum, with models often using hybrid strategies (e.g., memorizing common patterns while computing novel combinations). Key evidence for memorization includes: perfect in-distribution accuracy with sharp out-of-distribution failure, sensitivity to training data frequency, ability to produce correct answers despite incorrect intermediate steps, and correlation with benchmark contamination. Key evidence for computation includes: generalization to unseen problems, systematic error patterns consistent with algorithmic failures (carry errors, off-by-one), internal representations that align with algorithmic steps (Fourier features for addition, attention patterns for operand gathering), and improved performance with step-by-step training. The memorization-computation balance can be shifted through: tokenization choices (digit-level favors computation), format optimization (reversed order, padding), step-by-step supervision, pretraining on mathematical data, and architectural choices. Critically, the theory recognizes that memorization and computation are not mutually exclusive - models can memorize frequent sub-patterns while computing their combinations, and can memorize input-output mappings while failing to learn correct intermediate algorithms.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Language models use a mixture of memorization and algorithmic computation for arithmetic, with the balance determined by training data coverage, model capacity, problem complexity, tokenization/format, and training methodology.</li>
                <li>Memorization dominates when: (a) problems are frequently seen in training data, (b) problems are within model capacity for direct lookup, (c) models are trained end-to-end without intermediate steps, (d) tokenization creates large multi-digit tokens, (e) problem format matches training distribution exactly.</li>
                <li>Algorithmic computation dominates when: (a) problems are rare or novel, (b) problems exceed memorization capacity, (c) models are trained with step-by-step supervision, (d) tokenization is digit-level, (e) format is optimized (reversed order, padding), (f) models are pretrained on mathematical data.</li>
                <li>Evidence for memorization includes: perfect in-distribution with sharp OOD failure, correct answers with incorrect reasoning, sensitivity to training frequency, correlation with benchmark contamination, and inability to generalize to cross-distribution problems.</li>
                <li>Evidence for computation includes: generalization to unseen problems, systematic algorithmic error patterns (carry errors, off-by-one at token boundaries), internal representations matching algorithmic steps (Fourier features, attention patterns), and improved performance with step-by-step training.</li>
                <li>The memorization-computation balance can be shifted through interventions: (a) tokenization (digit-level favors computation), (b) format optimization (reversed order, padding), (c) step-by-step training, (d) pretraining on mathematical data, (e) explicit skill training, (f) data augmentation.</li>
                <li>Model capacity determines the memorization ceiling: larger models can memorize more patterns, but all models require computation for sufficiently complex problems beyond their memorization capacity.</li>
                <li>Tokenization fundamentally affects the memorization-computation balance: multi-digit tokens increase memorization requirements and make algorithmic learning harder, while digit-level tokens enable algorithmic learning in smaller models.</li>
                <li>Format and representation choices (digit order, padding, scratchpad) can shift problems from requiring memorization to enabling computation by reducing sequential dependencies (CSID).</li>
                <li>Pretraining provides computational primitives (e.g., Fourier features for addition) that enable algorithmic computation, while training from scratch tends toward memorization.</li>
                <li>Models can use hybrid strategies: memorizing frequent sub-patterns while computing their combinations, or memorizing input-output mappings while failing to learn correct intermediate algorithms.</li>
                <li>The transition from memorization to computation is not sharp but exists on a continuum, with different problems and model states occupying different positions on the spectrum.</li>
                <li>Benchmark contamination can artificially inflate apparent arithmetic ability by enabling memorization of test examples, masking the true memorization-computation balance.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>GPT-3 shows near-perfect in-distribution accuracy after exhaustive fine-tuning but near-zero out-of-distribution accuracy on multiplication, indicating memorization of training examples. <a href="../results/extraction-result-319.html#e319.0" class="evidence-link">[e319.0]</a> </li>
    <li>82.3% of correct 4-digit×2-digit multiplication answers had at least one error in intermediate steps, showing memorization of input-output pairs without learning the algorithm. <a href="../results/extraction-result-319.html#e319.0" class="evidence-link">[e319.0]</a> </li>
    <li>Models exploit correlations predictable via relative information gain (last digits, leading digits) rather than systematic computation, indicating pattern-based memorization. <a href="../results/extraction-result-319.html#e319.0" class="evidence-link">[e319.0]</a> </li>
    <li>Small encoder-decoder transformer learns algorithmic addition/multiplication on exhaustive 7-bit dataset through an Encoding-Regression-Decoding mechanism, showing computation is possible. <a href="../results/extraction-result-249.html#e249.0" class="evidence-link">[e249.0]</a> </li>
    <li>Transformer trained on in-context linear regression learns an algorithm matching least-squares rather than memorizing, evidenced by gradient alignment with true weights. <a href="../results/extraction-result-323.html#e323.0" class="evidence-link">[e323.0]</a> </li>
    <li>Training data term frequency substantially affects few-shot numerical reasoning performance, directly supporting memorization component. <a href="../results/extraction-result-323.html#e323.3" class="evidence-link">[e323.3]</a> </li>
    <li>BART fine-tuned on arithmetic achieves near-perfect in-distribution (99%+) but fails completely on cross-distribution tests (often 0-1%), indicating pure memorization without algorithmic learning. <a href="../results/extraction-result-288.html#e288.0" class="evidence-link">[e288.0]</a> </li>
    <li>Models show systematic error patterns (carry errors, off-by-one at token boundaries) consistent with algorithmic failures rather than random memorization errors. <a href="../results/extraction-result-266.html#e266.0" class="evidence-link">[e266.0]</a> <a href="../results/extraction-result-315.html#e315.0" class="evidence-link">[e315.0]</a> <a href="../results/extraction-result-300.html#e300.0" class="evidence-link">[e300.0]</a> </li>
    <li>Contamination analysis shows GSM8K performance correlates with per-character log-likelihood of training data, suggesting partial memorization of benchmark examples. <a href="../results/extraction-result-324.html#e324.7" class="evidence-link">[e324.7]</a> </li>
    <li>Value-space holdout causes larger performance drops (~18-19% for multiplication) than token-space holdout, indicating reliance on training value coverage rather than pure token pattern memorization. <a href="../results/extraction-result-249.html#e249.0" class="evidence-link">[e249.0]</a> </li>
    <li>Pre-trained GPT-2-XL uses sparse Fourier features (periods ~2, 2.5, 5, 10) for addition, with MLPs implementing low-frequency magnitude approximation and attention implementing high-frequency modular classification - showing learned algorithmic computation. <a href="../results/extraction-result-315.html#e315.0" class="evidence-link">[e315.0]</a> </li>
    <li>Models trained from scratch without pre-training lack Fourier features and achieve only 94.44% accuracy vs 99.74% for pre-trained models, showing pre-training provides computational primitives. <a href="../results/extraction-result-315.html#e315.0" class="evidence-link">[e315.0]</a> <a href="../results/extraction-result-315.html#e315.1" class="evidence-link">[e315.1]</a> </li>
    <li>Tokenization direction dramatically affects performance: GPT-3.5 achieves ~76% with left-to-right but ~98% with right-to-left tokenization on addition, showing format affects computation vs memorization balance. <a href="../results/extraction-result-300.html#e300.0" class="evidence-link">[e300.0]</a> </li>
    <li>One-digit tokenization yields best performance for sub-billion models on arithmetic, while multi-digit tokenization requires larger capacity, indicating tokenization granularity affects memorization-computation tradeoff. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> </li>
    <li>Reversed digit order (Little-Endian) reduces CSID from O(n²) to O(1) for multiplication and enables 88.5% accuracy vs much lower for standard order, showing format optimization enables computation. <a href="../results/extraction-result-266.html#e266.0" class="evidence-link">[e266.0]</a> <a href="../results/extraction-result-263.html#e263.0" class="evidence-link">[e263.0]</a> </li>
    <li>Step-by-step training (scratchpad, chain-of-thought) substantially improves arithmetic: detailed scratchpad achieves 99.8% addition vs lower for end-to-end, showing supervision of intermediate steps favors computation. <a href="../results/extraction-result-266.html#e266.3" class="evidence-link">[e266.3]</a> <a href="../results/extraction-result-246.html#e246.1" class="evidence-link">[e246.1]</a> </li>
    <li>Scaffolding learning (teaching atomic arithmetic skills first, then compositional application) improves from ~13.6% to ~28.8% on hard problems, showing explicit skill training enables computation. <a href="../results/extraction-result-259.html#e259.0" class="evidence-link">[e259.0]</a> </li>
    <li>MathGLM trained on 50M synthetic arithmetic examples with step-by-step solutions achieves 93.03% accuracy, showing large-scale algorithmic training enables computation. <a href="../results/extraction-result-265.html#e265.0" class="evidence-link">[e265.0]</a> </li>
    <li>Continued pretraining on mathematical data (Minerva, LlemMA) yields large gains, showing domain-specific pretraining shifts toward computation. <a href="../results/extraction-result-293.html#e293.2" class="evidence-link">[e293.2]</a> <a href="../results/extraction-result-307.html#e307.0" class="evidence-link">[e307.0]</a> </li>
    <li>Models show attention patterns that gather operands to final token and MLPs that progressively refine answers, indicating learned computational circuits. <a href="../results/extraction-result-248.html#e248.2" class="evidence-link">[e248.2]</a> <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> </li>
    <li>Pretraining provides general priors that facilitate adaptation, but format mismatch can degrade fine-tuning, showing memorization-computation balance depends on training history. <a href="../results/extraction-result-329.html#e329.6" class="evidence-link">[e329.6]</a> </li>
    <li>GPT-3 case study shows perfect performance on small numbers (≤1000) but inconsistent on larger numbers, with some correct answers appearing to be memorized frequent examples. <a href="../results/extraction-result-288.html#e288.1" class="evidence-link">[e288.1]</a> </li>
    <li>Models can learn algebraic properties (commutativity, identity) from examples without numeric computation, showing ability to learn abstract algorithmic structure. <a href="../results/extraction-result-253.html#e253.0" class="evidence-link">[e253.0]</a> </li>
    <li>Reinitialized GPT-2 learns commutativity and identity for modular arithmetic, requiring 10k-20k examples for commutativity but only ~1k for identity, showing different algorithmic properties have different learning requirements. <a href="../results/extraction-result-253.html#e253.0" class="evidence-link">[e253.0]</a> </li>
    <li>Small Transformer (536K params) trained with Recursion of Thought achieves near-perfect accuracy on 64-digit addition/subtraction, showing even tiny models can learn algorithms with proper supervision. <a href="../results/extraction-result-292.html#e292.2" class="evidence-link">[e292.2]</a> </li>
    <li>Padding and reversing product digits enables GPT2-small to achieve essentially perfect accuracy up to 12×12 multiplication, showing format optimization can enable algorithmic learning in small models. <a href="../results/extraction-result-262.html#e262.0" class="evidence-link">[e262.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Models should show higher memorization (better in-distribution, worse OOD) for operations with smaller output spaces (e.g., modular arithmetic mod small n) compared to operations with larger output spaces.</li>
                <li>Training on problems with systematically varied surface features but identical underlying operations should reduce memorization and improve generalization by forcing algorithmic learning.</li>
                <li>Models should show a smooth transition from memorization-like behavior (perfect accuracy, no intermediate steps needed) to computation-like behavior (systematic errors, benefits from intermediate steps) as problem complexity increases.</li>
                <li>Probing for problem-specific representations should find distinct patterns: memorized problems should show direct input-output associations, while computed problems should show intermediate representations matching algorithmic steps.</li>
                <li>Digit-level tokenization should enable smaller models to learn arithmetic algorithms that multi-digit tokenization requires much larger models to learn.</li>
                <li>Reversing digit order should improve performance more for operations with long-range dependencies (multiplication, division) than for operations with local dependencies (addition with small carries).</li>
                <li>Models pretrained on mathematical data should show more computation-like behavior (better OOD generalization, systematic errors) than models pretrained only on general text.</li>
                <li>Training with explicit intermediate steps should shift models toward computation even for problems they could memorize, as evidenced by improved OOD performance.</li>
                <li>The memorization-computation balance should be measurable through: (a) in-distribution vs OOD performance gap, (b) correlation with training frequency, (c) presence of systematic vs random errors, (d) internal representation analysis.</li>
                <li>Models should show different memorization-computation balances for different operations: addition (more computation due to simple algorithm) vs multiplication (more memorization due to complex algorithm).</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a sharp phase transition between memorization and computation regimes or a smooth continuum across all problem complexities and model capacities is unclear.</li>
                <li>It's unknown whether models can be explicitly trained to recognize when to memorize vs. compute and switch strategies accordingly, or if this is an emergent property.</li>
                <li>Whether memorization and computation use entirely separate neural circuits or share components (e.g., shared attention patterns, shared intermediate representations) is uncertain.</li>
                <li>The extent to which memorization interferes with or facilitates computation learning is unknown - does memorizing common patterns help or hurt learning the general algorithm?</li>
                <li>Whether the optimal memorization-computation balance differs across tasks and how to determine the optimal balance for a given task is unclear.</li>
                <li>It's unknown whether there are fundamental limits to how far training interventions can shift the balance toward computation, or if sufficient training can always enable algorithmic learning.</li>
                <li>Whether the memorization-computation spectrum is the same across different model architectures (decoder-only vs encoder-decoder) or if architecture fundamentally affects the balance is uncertain.</li>
                <li>The role of model depth vs width in determining memorization capacity vs computational capacity is not fully understood.</li>
                <li>Whether memorization and computation can be disentangled through training (e.g., training separate 'memory' and 'computation' modules) is unknown.</li>
                <li>It's unclear whether the memorization-computation balance is stable during training or if models transition from memorization to computation (or vice versa) as training progresses.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that models show identical performance patterns (same in-distribution and OOD accuracy, same error types) on memorized and computed problems would challenge the spectrum theory.</li>
                <li>Discovering that training interventions designed to favor computation (step-by-step training, digit-level tokenization, format optimization) do not shift the balance would challenge the trainability claim.</li>
                <li>Observing that model capacity does not affect the memorization ceiling (small and large models memorize the same amount) would challenge the capacity-limitation claim.</li>
                <li>Finding that contamination does not correlate with performance or that contaminated examples show the same error patterns as non-contaminated examples would challenge the memorization-via-contamination mechanism.</li>
                <li>Discovering that tokenization choices do not affect the memorization-computation balance would challenge the tokenization-dependence claim.</li>
                <li>Finding that format optimization (reversed order, padding) does not improve OOD generalization would challenge the format-optimization mechanism.</li>
                <li>Observing that pretraining on mathematical data does not shift toward computation (no improvement in OOD generalization) would challenge the pretraining-effect claim.</li>
                <li>Finding that models cannot learn algorithmic computation even with unlimited step-by-step training data would challenge the computation-learnability claim.</li>
                <li>Discovering that internal representations do not differ between memorized and computed problems would challenge the mechanistic distinction.</li>
                <li>Finding that the memorization-computation balance does not vary across operations (addition, multiplication, etc.) would challenge the operation-specific claims.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact neural mechanisms that implement memorization vs. computation are not fully specified - while evidence shows Fourier features and attention patterns for computation, the memorization mechanism is less clear. <a href="../results/extraction-result-319.html#e319.0" class="evidence-link">[e319.0]</a> <a href="../results/extraction-result-315.html#e315.0" class="evidence-link">[e315.0]</a> </li>
    <li>How models decide when to rely on memorization vs. computation (if this is even a discrete decision) is not explained - is it automatic based on problem features or learned through training? <a href="../results/extraction-result-319.html#e319.0" class="evidence-link">[e319.0]</a> </li>
    <li>The interaction between memorization and computation during learning is not detailed - do they compete, cooperate, or operate independently? <a href="../results/extraction-result-323.html#e323.0" class="evidence-link">[e323.0]</a> </li>
    <li>Why some operations (addition) seem more amenable to computation while others (multiplication) seem more amenable to memorization is not fully explained. <a href="../results/extraction-result-319.html#e319.0" class="evidence-link">[e319.0]</a> <a href="../results/extraction-result-249.html#e249.0" class="evidence-link">[e249.0]</a> </li>
    <li>The role of attention head specialization in memorization vs computation is not fully characterized - do different heads handle memorization vs computation? <a href="../results/extraction-result-248.html#e248.2" class="evidence-link">[e248.2]</a> <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> </li>
    <li>How the memorization-computation balance changes during training (does it start with memorization and transition to computation, or vice versa?) is not detailed. <a href="../results/extraction-result-329.html#e329.6" class="evidence-link">[e329.6]</a> </li>
    <li>The relationship between model depth/width and memorization vs computation capacity is not fully specified. <a href="../results/extraction-result-319.html#e319.0" class="evidence-link">[e319.0]</a> </li>
    <li>Why some models show correct answers with incorrect intermediate steps (hybrid memorization-computation) while others show consistent errors is not explained. <a href="../results/extraction-result-319.html#e319.0" class="evidence-link">[e319.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Directly demonstrates memorization effects via term frequency correlation with performance]</li>
    <li>Feldman (2020) Does learning require memorization? A short tale about a long tail [General theory about memorization in machine learning, argues memorization is necessary for long-tail examples, but not specific to transformers or arithmetic]</li>
    <li>Zhang et al. (2021) Understanding deep learning (still) requires rethinking generalization [Related work on memorization vs. generalization in deep learning, shows models can memorize training data while still generalizing]</li>
    <li>Arpit et al. (2017) A Closer Look at Memorization in Deep Networks [Studies memorization in neural networks, shows models memorize training examples but with different patterns for real vs random labels]</li>
    <li>Carlini et al. (2019) The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks [Studies unintended memorization in language models, shows models memorize training data]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [GPT-3 paper, discusses in-context learning which relates to memorization vs computation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Memorization-Computation Spectrum Theory (Revised)",
    "theory_description": "Language models exist on a spectrum between pure memorization and algorithmic computation when performing arithmetic, with their position determined by multiple interacting factors: (1) training data coverage and frequency, (2) model capacity and architecture, (3) problem complexity and format, (4) tokenization and representation, and (5) training methodology. Models primarily memorize input-output mappings for frequent, simple problems seen during training, but must rely on learned algorithmic procedures for rare or complex problems. The balance is not binary but exists on a continuum, with models often using hybrid strategies (e.g., memorizing common patterns while computing novel combinations). Key evidence for memorization includes: perfect in-distribution accuracy with sharp out-of-distribution failure, sensitivity to training data frequency, ability to produce correct answers despite incorrect intermediate steps, and correlation with benchmark contamination. Key evidence for computation includes: generalization to unseen problems, systematic error patterns consistent with algorithmic failures (carry errors, off-by-one), internal representations that align with algorithmic steps (Fourier features for addition, attention patterns for operand gathering), and improved performance with step-by-step training. The memorization-computation balance can be shifted through: tokenization choices (digit-level favors computation), format optimization (reversed order, padding), step-by-step supervision, pretraining on mathematical data, and architectural choices. Critically, the theory recognizes that memorization and computation are not mutually exclusive - models can memorize frequent sub-patterns while computing their combinations, and can memorize input-output mappings while failing to learn correct intermediate algorithms.",
    "supporting_evidence": [
        {
            "text": "GPT-3 shows near-perfect in-distribution accuracy after exhaustive fine-tuning but near-zero out-of-distribution accuracy on multiplication, indicating memorization of training examples.",
            "uuids": [
                "e319.0"
            ]
        },
        {
            "text": "82.3% of correct 4-digit×2-digit multiplication answers had at least one error in intermediate steps, showing memorization of input-output pairs without learning the algorithm.",
            "uuids": [
                "e319.0"
            ]
        },
        {
            "text": "Models exploit correlations predictable via relative information gain (last digits, leading digits) rather than systematic computation, indicating pattern-based memorization.",
            "uuids": [
                "e319.0"
            ]
        },
        {
            "text": "Small encoder-decoder transformer learns algorithmic addition/multiplication on exhaustive 7-bit dataset through an Encoding-Regression-Decoding mechanism, showing computation is possible.",
            "uuids": [
                "e249.0"
            ]
        },
        {
            "text": "Transformer trained on in-context linear regression learns an algorithm matching least-squares rather than memorizing, evidenced by gradient alignment with true weights.",
            "uuids": [
                "e323.0"
            ]
        },
        {
            "text": "Training data term frequency substantially affects few-shot numerical reasoning performance, directly supporting memorization component.",
            "uuids": [
                "e323.3"
            ]
        },
        {
            "text": "BART fine-tuned on arithmetic achieves near-perfect in-distribution (99%+) but fails completely on cross-distribution tests (often 0-1%), indicating pure memorization without algorithmic learning.",
            "uuids": [
                "e288.0"
            ]
        },
        {
            "text": "Models show systematic error patterns (carry errors, off-by-one at token boundaries) consistent with algorithmic failures rather than random memorization errors.",
            "uuids": [
                "e266.0",
                "e315.0",
                "e300.0"
            ]
        },
        {
            "text": "Contamination analysis shows GSM8K performance correlates with per-character log-likelihood of training data, suggesting partial memorization of benchmark examples.",
            "uuids": [
                "e324.7"
            ]
        },
        {
            "text": "Value-space holdout causes larger performance drops (~18-19% for multiplication) than token-space holdout, indicating reliance on training value coverage rather than pure token pattern memorization.",
            "uuids": [
                "e249.0"
            ]
        },
        {
            "text": "Pre-trained GPT-2-XL uses sparse Fourier features (periods ~2, 2.5, 5, 10) for addition, with MLPs implementing low-frequency magnitude approximation and attention implementing high-frequency modular classification - showing learned algorithmic computation.",
            "uuids": [
                "e315.0"
            ]
        },
        {
            "text": "Models trained from scratch without pre-training lack Fourier features and achieve only 94.44% accuracy vs 99.74% for pre-trained models, showing pre-training provides computational primitives.",
            "uuids": [
                "e315.0",
                "e315.1"
            ]
        },
        {
            "text": "Tokenization direction dramatically affects performance: GPT-3.5 achieves ~76% with left-to-right but ~98% with right-to-left tokenization on addition, showing format affects computation vs memorization balance.",
            "uuids": [
                "e300.0"
            ]
        },
        {
            "text": "One-digit tokenization yields best performance for sub-billion models on arithmetic, while multi-digit tokenization requires larger capacity, indicating tokenization granularity affects memorization-computation tradeoff.",
            "uuids": [
                "e278.5"
            ]
        },
        {
            "text": "Reversed digit order (Little-Endian) reduces CSID from O(n²) to O(1) for multiplication and enables 88.5% accuracy vs much lower for standard order, showing format optimization enables computation.",
            "uuids": [
                "e266.0",
                "e263.0"
            ]
        },
        {
            "text": "Step-by-step training (scratchpad, chain-of-thought) substantially improves arithmetic: detailed scratchpad achieves 99.8% addition vs lower for end-to-end, showing supervision of intermediate steps favors computation.",
            "uuids": [
                "e266.3",
                "e246.1"
            ]
        },
        {
            "text": "Scaffolding learning (teaching atomic arithmetic skills first, then compositional application) improves from ~13.6% to ~28.8% on hard problems, showing explicit skill training enables computation.",
            "uuids": [
                "e259.0"
            ]
        },
        {
            "text": "MathGLM trained on 50M synthetic arithmetic examples with step-by-step solutions achieves 93.03% accuracy, showing large-scale algorithmic training enables computation.",
            "uuids": [
                "e265.0"
            ]
        },
        {
            "text": "Continued pretraining on mathematical data (Minerva, LlemMA) yields large gains, showing domain-specific pretraining shifts toward computation.",
            "uuids": [
                "e293.2",
                "e307.0"
            ]
        },
        {
            "text": "Models show attention patterns that gather operands to final token and MLPs that progressively refine answers, indicating learned computational circuits.",
            "uuids": [
                "e248.2",
                "e316.0"
            ]
        },
        {
            "text": "Pretraining provides general priors that facilitate adaptation, but format mismatch can degrade fine-tuning, showing memorization-computation balance depends on training history.",
            "uuids": [
                "e329.6"
            ]
        },
        {
            "text": "GPT-3 case study shows perfect performance on small numbers (≤1000) but inconsistent on larger numbers, with some correct answers appearing to be memorized frequent examples.",
            "uuids": [
                "e288.1"
            ]
        },
        {
            "text": "Models can learn algebraic properties (commutativity, identity) from examples without numeric computation, showing ability to learn abstract algorithmic structure.",
            "uuids": [
                "e253.0"
            ]
        },
        {
            "text": "Reinitialized GPT-2 learns commutativity and identity for modular arithmetic, requiring 10k-20k examples for commutativity but only ~1k for identity, showing different algorithmic properties have different learning requirements.",
            "uuids": [
                "e253.0"
            ]
        },
        {
            "text": "Small Transformer (536K params) trained with Recursion of Thought achieves near-perfect accuracy on 64-digit addition/subtraction, showing even tiny models can learn algorithms with proper supervision.",
            "uuids": [
                "e292.2"
            ]
        },
        {
            "text": "Padding and reversing product digits enables GPT2-small to achieve essentially perfect accuracy up to 12×12 multiplication, showing format optimization can enable algorithmic learning in small models.",
            "uuids": [
                "e262.0"
            ]
        }
    ],
    "theory_statements": [
        "Language models use a mixture of memorization and algorithmic computation for arithmetic, with the balance determined by training data coverage, model capacity, problem complexity, tokenization/format, and training methodology.",
        "Memorization dominates when: (a) problems are frequently seen in training data, (b) problems are within model capacity for direct lookup, (c) models are trained end-to-end without intermediate steps, (d) tokenization creates large multi-digit tokens, (e) problem format matches training distribution exactly.",
        "Algorithmic computation dominates when: (a) problems are rare or novel, (b) problems exceed memorization capacity, (c) models are trained with step-by-step supervision, (d) tokenization is digit-level, (e) format is optimized (reversed order, padding), (f) models are pretrained on mathematical data.",
        "Evidence for memorization includes: perfect in-distribution with sharp OOD failure, correct answers with incorrect reasoning, sensitivity to training frequency, correlation with benchmark contamination, and inability to generalize to cross-distribution problems.",
        "Evidence for computation includes: generalization to unseen problems, systematic algorithmic error patterns (carry errors, off-by-one at token boundaries), internal representations matching algorithmic steps (Fourier features, attention patterns), and improved performance with step-by-step training.",
        "The memorization-computation balance can be shifted through interventions: (a) tokenization (digit-level favors computation), (b) format optimization (reversed order, padding), (c) step-by-step training, (d) pretraining on mathematical data, (e) explicit skill training, (f) data augmentation.",
        "Model capacity determines the memorization ceiling: larger models can memorize more patterns, but all models require computation for sufficiently complex problems beyond their memorization capacity.",
        "Tokenization fundamentally affects the memorization-computation balance: multi-digit tokens increase memorization requirements and make algorithmic learning harder, while digit-level tokens enable algorithmic learning in smaller models.",
        "Format and representation choices (digit order, padding, scratchpad) can shift problems from requiring memorization to enabling computation by reducing sequential dependencies (CSID).",
        "Pretraining provides computational primitives (e.g., Fourier features for addition) that enable algorithmic computation, while training from scratch tends toward memorization.",
        "Models can use hybrid strategies: memorizing frequent sub-patterns while computing their combinations, or memorizing input-output mappings while failing to learn correct intermediate algorithms.",
        "The transition from memorization to computation is not sharp but exists on a continuum, with different problems and model states occupying different positions on the spectrum.",
        "Benchmark contamination can artificially inflate apparent arithmetic ability by enabling memorization of test examples, masking the true memorization-computation balance."
    ],
    "new_predictions_likely": [
        "Models should show higher memorization (better in-distribution, worse OOD) for operations with smaller output spaces (e.g., modular arithmetic mod small n) compared to operations with larger output spaces.",
        "Training on problems with systematically varied surface features but identical underlying operations should reduce memorization and improve generalization by forcing algorithmic learning.",
        "Models should show a smooth transition from memorization-like behavior (perfect accuracy, no intermediate steps needed) to computation-like behavior (systematic errors, benefits from intermediate steps) as problem complexity increases.",
        "Probing for problem-specific representations should find distinct patterns: memorized problems should show direct input-output associations, while computed problems should show intermediate representations matching algorithmic steps.",
        "Digit-level tokenization should enable smaller models to learn arithmetic algorithms that multi-digit tokenization requires much larger models to learn.",
        "Reversing digit order should improve performance more for operations with long-range dependencies (multiplication, division) than for operations with local dependencies (addition with small carries).",
        "Models pretrained on mathematical data should show more computation-like behavior (better OOD generalization, systematic errors) than models pretrained only on general text.",
        "Training with explicit intermediate steps should shift models toward computation even for problems they could memorize, as evidenced by improved OOD performance.",
        "The memorization-computation balance should be measurable through: (a) in-distribution vs OOD performance gap, (b) correlation with training frequency, (c) presence of systematic vs random errors, (d) internal representation analysis.",
        "Models should show different memorization-computation balances for different operations: addition (more computation due to simple algorithm) vs multiplication (more memorization due to complex algorithm)."
    ],
    "new_predictions_unknown": [
        "Whether there exists a sharp phase transition between memorization and computation regimes or a smooth continuum across all problem complexities and model capacities is unclear.",
        "It's unknown whether models can be explicitly trained to recognize when to memorize vs. compute and switch strategies accordingly, or if this is an emergent property.",
        "Whether memorization and computation use entirely separate neural circuits or share components (e.g., shared attention patterns, shared intermediate representations) is uncertain.",
        "The extent to which memorization interferes with or facilitates computation learning is unknown - does memorizing common patterns help or hurt learning the general algorithm?",
        "Whether the optimal memorization-computation balance differs across tasks and how to determine the optimal balance for a given task is unclear.",
        "It's unknown whether there are fundamental limits to how far training interventions can shift the balance toward computation, or if sufficient training can always enable algorithmic learning.",
        "Whether the memorization-computation spectrum is the same across different model architectures (decoder-only vs encoder-decoder) or if architecture fundamentally affects the balance is uncertain.",
        "The role of model depth vs width in determining memorization capacity vs computational capacity is not fully understood.",
        "Whether memorization and computation can be disentangled through training (e.g., training separate 'memory' and 'computation' modules) is unknown.",
        "It's unclear whether the memorization-computation balance is stable during training or if models transition from memorization to computation (or vice versa) as training progresses."
    ],
    "negative_experiments": [
        "Finding that models show identical performance patterns (same in-distribution and OOD accuracy, same error types) on memorized and computed problems would challenge the spectrum theory.",
        "Discovering that training interventions designed to favor computation (step-by-step training, digit-level tokenization, format optimization) do not shift the balance would challenge the trainability claim.",
        "Observing that model capacity does not affect the memorization ceiling (small and large models memorize the same amount) would challenge the capacity-limitation claim.",
        "Finding that contamination does not correlate with performance or that contaminated examples show the same error patterns as non-contaminated examples would challenge the memorization-via-contamination mechanism.",
        "Discovering that tokenization choices do not affect the memorization-computation balance would challenge the tokenization-dependence claim.",
        "Finding that format optimization (reversed order, padding) does not improve OOD generalization would challenge the format-optimization mechanism.",
        "Observing that pretraining on mathematical data does not shift toward computation (no improvement in OOD generalization) would challenge the pretraining-effect claim.",
        "Finding that models cannot learn algorithmic computation even with unlimited step-by-step training data would challenge the computation-learnability claim.",
        "Discovering that internal representations do not differ between memorized and computed problems would challenge the mechanistic distinction.",
        "Finding that the memorization-computation balance does not vary across operations (addition, multiplication, etc.) would challenge the operation-specific claims."
    ],
    "unaccounted_for": [
        {
            "text": "The exact neural mechanisms that implement memorization vs. computation are not fully specified - while evidence shows Fourier features and attention patterns for computation, the memorization mechanism is less clear.",
            "uuids": [
                "e319.0",
                "e315.0"
            ]
        },
        {
            "text": "How models decide when to rely on memorization vs. computation (if this is even a discrete decision) is not explained - is it automatic based on problem features or learned through training?",
            "uuids": [
                "e319.0"
            ]
        },
        {
            "text": "The interaction between memorization and computation during learning is not detailed - do they compete, cooperate, or operate independently?",
            "uuids": [
                "e323.0"
            ]
        },
        {
            "text": "Why some operations (addition) seem more amenable to computation while others (multiplication) seem more amenable to memorization is not fully explained.",
            "uuids": [
                "e319.0",
                "e249.0"
            ]
        },
        {
            "text": "The role of attention head specialization in memorization vs computation is not fully characterized - do different heads handle memorization vs computation?",
            "uuids": [
                "e248.2",
                "e316.0"
            ]
        },
        {
            "text": "How the memorization-computation balance changes during training (does it start with memorization and transition to computation, or vice versa?) is not detailed.",
            "uuids": [
                "e329.6"
            ]
        },
        {
            "text": "The relationship between model depth/width and memorization vs computation capacity is not fully specified.",
            "uuids": [
                "e319.0"
            ]
        },
        {
            "text": "Why some models show correct answers with incorrect intermediate steps (hybrid memorization-computation) while others show consistent errors is not explained.",
            "uuids": [
                "e319.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models show generalization to unseen problems even with limited training (e.g., small transformer on 7-bit arithmetic), suggesting computation can emerge with less data than pure memorization would require, but other evidence shows memorization dominates with limited data.",
            "uuids": [
                "e249.0",
                "e319.0"
            ]
        },
        {
            "text": "Models can produce correct answers with incorrect intermediate steps (suggesting memorization), but also show systematic error patterns (suggesting computation), making it unclear which mechanism dominates in these cases.",
            "uuids": [
                "e319.0"
            ]
        },
        {
            "text": "Some evidence suggests pretraining helps computation (Fourier features), while other evidence suggests pretraining can hurt when format mismatches, making the role of pretraining ambiguous.",
            "uuids": [
                "e315.0",
                "e329.6"
            ]
        },
        {
            "text": "Value-space holdout causes large drops (suggesting memorization of values), but models still show some generalization (suggesting computation), indicating a complex mixture.",
            "uuids": [
                "e249.0"
            ]
        },
        {
            "text": "Models show both frequency-dependent performance (memorization) and systematic algorithmic errors (computation) simultaneously, suggesting the mechanisms are not mutually exclusive.",
            "uuids": [
                "e323.3",
                "e315.0"
            ]
        }
    ],
    "special_cases": [
        "Very small problems (single-digit arithmetic) are likely always memorized regardless of training methodology, as the memorization cost is minimal.",
        "Very large problems (many digits, e.g., &gt;20 digits) likely always require computation due to memorization capacity limits, even for large models.",
        "The memorization-computation balance differs across operations: addition (simpler algorithm, more computation) vs multiplication (complex algorithm, more memorization) vs modular arithmetic (small output space, more memorization).",
        "Pretrained models have different memorization-computation balances than models trained from scratch, with pretraining providing computational primitives (Fourier features) that enable computation.",
        "Models with digit-level tokenization show more computation-like behavior than models with multi-digit tokenization, especially for smaller models.",
        "Problems with optimized formats (reversed order, padding) enable computation even when the same problems in standard format require memorization.",
        "Models trained with step-by-step supervision show more computation-like behavior than models trained end-to-end, even on the same problems.",
        "The memorization-computation balance may differ between in-context learning (more memorization-like) and fine-tuning (more computation-like).",
        "Contaminated benchmark examples may be memorized even when similar non-contaminated examples require computation.",
        "The balance may differ between encoder-decoder and decoder-only architectures, with encoder-decoder potentially enabling more computation through explicit encoding-regression-decoding mechanisms.",
        "Problems at the boundary of model capacity may show unstable behavior, sometimes memorized and sometimes computed depending on training dynamics.",
        "The balance may shift during training, potentially starting with memorization and transitioning to computation as the model learns algorithmic patterns."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Directly demonstrates memorization effects via term frequency correlation with performance]",
            "Feldman (2020) Does learning require memorization? A short tale about a long tail [General theory about memorization in machine learning, argues memorization is necessary for long-tail examples, but not specific to transformers or arithmetic]",
            "Zhang et al. (2021) Understanding deep learning (still) requires rethinking generalization [Related work on memorization vs. generalization in deep learning, shows models can memorize training data while still generalizing]",
            "Arpit et al. (2017) A Closer Look at Memorization in Deep Networks [Studies memorization in neural networks, shows models memorize training examples but with different patterns for real vs random labels]",
            "Carlini et al. (2019) The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks [Studies unintended memorization in language models, shows models memorize training data]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [GPT-3 paper, discusses in-context learning which relates to memorization vs computation]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 3,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
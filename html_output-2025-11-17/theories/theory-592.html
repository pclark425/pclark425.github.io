<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hybrid Memory Architecture Principle for LLM Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-592</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-592</p>
                <p><strong>Name:</strong> Hybrid Memory Architecture Principle for LLM Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks, based on the following results.</p>
                <p><strong>Description:</strong> LLM agents for text games achieve optimal performance and generalization when equipped with a hybrid memory architecture that combines (1) short-term working memory (prompt context or rolling window), (2) long-term episodic or structured memory (vector DB, knowledge graph, or skill library), and (3) memory management mechanisms (summarization, prioritization, retrieval, and reflection). The hybrid approach enables agents to maintain local coherence, recall distant facts or skills, and adaptively manage memory to fit context and task demands.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hybrid Memory Outperforms Single-Component Memory (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; uses &#8594; hybrid memory (short-term + long-term + management)<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; has_property &#8594; long-horizon, compositional, or cross-episode dependencies</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; achieves &#8594; higher performance and generalization than agents with only short-term or only long-term memory</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>RecurrentGPT ablations show that removing either short-term or long-term memory causes large drops in coherence and interestingness; full hybrid model is preferred by humans. <a href="../results/extraction-result-4876.html#e4876.2" class="evidence-link">[e4876.2]</a> <a href="../results/extraction-result-4876.html#e4876.3" class="evidence-link">[e4876.3]</a> <a href="../results/extraction-result-4876.html#e4876.4" class="evidence-link">[e4876.4]</a> </li>
    <li>Voyager's combination of prompt context, skill library (vector DB), and curriculum history enables open-ended exploration and zero-shot generalization; ablation of skill library or self-verification causes plateauing. <a href="../results/extraction-result-4919.html#e4919.0" class="evidence-link">[e4919.0]</a> </li>
    <li>AGENTS, AgentSims, and Generative Agents (Park et al.) all use hybrid memory (prompt short-term + vector DB long-term) to maintain behavioral consistency and long-horizon planning. <a href="../results/extraction-result-4915.html#e4915.0" class="evidence-link">[e4915.0]</a> <a href="../results/extraction-result-4900.html#e4900.4" class="evidence-link">[e4900.4]</a> <a href="../results/extraction-result-4919.html#e4919.4" class="evidence-link">[e4919.4]</a> <a href="../results/extraction-result-4685.html#e4685.3" class="evidence-link">[e4685.3]</a> </li>
    <li>PsychoGAT uses summarization-based narrative memory (short-term) and explicit memory management (justification, capping size) to maintain coherence in long interactive fiction. <a href="../results/extraction-result-4661.html#e4661.0" class="evidence-link">[e4661.0]</a> </li>
    <li>Reflexion and ExpeL combine short-term sliding window, persistent long-term insights, and reflection to improve planning and cross-task learning. <a href="../results/extraction-result-4900.html#e4900.3" class="evidence-link">[e4900.3]</a> <a href="../results/extraction-result-4683.html#e4683.2" class="evidence-link">[e4683.2]</a> <a href="../results/extraction-result-4920.html#e4920.4" class="evidence-link">[e4920.4]</a> </li>
    <li>AgentBench, LLM-Agent (LLM-Coordination), and ProAgent use multi-component memory scaffolds (long-term, working, episodic) to support multi-agent coordination and ToM reasoning. <a href="../results/extraction-result-4917.html#e4917.1" class="evidence-link">[e4917.1]</a> <a href="../results/extraction-result-4659.html#e4659.0" class="evidence-link">[e4659.0]</a> <a href="../results/extraction-result-4802.html#e4802.0" class="evidence-link">[e4802.0]</a> </li>
    <li>Memory management agents and episodic buffers are recommended for scalable, context-appropriate memory retrieval and event-level recall. <a href="../results/extraction-result-4663.html#e4663.3" class="evidence-link">[e4663.3]</a> <a href="../results/extraction-result-4663.html#e4663.1" class="evidence-link">[e4663.1]</a> </li>
    <li>Generative Agents and AgentSims use reflection and summarization to condense long-term memory and maintain relevance. <a href="../results/extraction-result-4919.html#e4919.4" class="evidence-link">[e4919.4]</a> <a href="../results/extraction-result-4900.html#e4900.4" class="evidence-link">[e4900.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While hybrid memory is a concept in cognitive architectures, this law extends and formalizes it for LLM agents in text games, with new empirical support and explicit mapping to agent design.</p>            <p><strong>What Already Exists:</strong> Hybrid memory architectures are discussed in cognitive science and some AI systems, but not formalized as a principle for LLM agents in text games with direct ablation evidence.</p>            <p><strong>What is Novel:</strong> This law formalizes the hybrid memory principle for LLM agents in text games, with direct ablation and comparative evidence across multiple architectures and environments.</p>
            <p><strong>References:</strong> <ul>
    <li>Baddeley (2000) The episodic buffer: a new component of working memory? [cognitive science hybrid memory]</li>
    <li>Weston et al. (2014) Memory Networks [external memory for reasoning]</li>
    <li>Park et al. (2023) Generative agents: Interactive simulacra of human behavior [hybrid memory for social simulation]</li>
    <li>Zhou et al. (2023) RecurrentGPT [hybrid memory for long-form generation]</li>
</ul>
            <h3>Statement 1: Memory Management (Summarization, Prioritization, and Retrieval) is Essential for Scalability (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; uses &#8594; hybrid memory with summarization, prioritization, and retrieval</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; maintains &#8594; scalability and relevance of memory over long interactions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>RecurrentGPT and PsychoGAT use explicit summarization and memory capping to avoid context overflow and maintain coherence. <a href="../results/extraction-result-4876.html#e4876.2" class="evidence-link">[e4876.2]</a> <a href="../results/extraction-result-4876.html#e4876.3" class="evidence-link">[e4876.3]</a> <a href="../results/extraction-result-4661.html#e4661.0" class="evidence-link">[e4661.0]</a> </li>
    <li>Generative Agents and AgentSims use reflection and summarization to condense long-term memory and maintain relevance. <a href="../results/extraction-result-4919.html#e4919.4" class="evidence-link">[e4919.4]</a> <a href="../results/extraction-result-4900.html#e4900.4" class="evidence-link">[e4900.4]</a> </li>
    <li>Memory management agents (e.g., Memory Management Agent, Episodic Buffer) are recommended for curating, prioritizing, and retrieving relevant memories for planning. <a href="../results/extraction-result-4663.html#e4663.3" class="evidence-link">[e4663.3]</a> <a href="../results/extraction-result-4663.html#e4663.1" class="evidence-link">[e4663.1]</a> </li>
    <li>Storage & Retrieval Modes (text, embeddings, SQL, semantic search) and best practices recommend layered searches and hybrid representations for efficient retrieval. <a href="../results/extraction-result-4663.html#e4663.5" class="evidence-link">[e4663.5]</a> </li>
    <li>Observation summarization and failure recovery are recommended for long-horizon web tasks to mitigate context-window limitations. <a href="../results/extraction-result-4914.html#e4914.7" class="evidence-link">[e4914.7]</a> </li>
    <li>Generative Agents (GA) and GenerativeAgents (Park et al.) use periodic synthesis/reflection to create higher-level summaries and dynamically retrieve relevant memories. <a href="../results/extraction-result-4660.html#e4660.0" class="evidence-link">[e4660.0]</a> <a href="../results/extraction-result-4919.html#e4919.4" class="evidence-link">[e4919.4]</a> <a href="../results/extraction-result-4685.html#e4685.3" class="evidence-link">[e4685.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While memory management is a known concept, this law formalizes and empirically grounds it for LLM agent architectures in text games.</p>            <p><strong>What Already Exists:</strong> Memory management (summarization, prioritization) is discussed in cognitive science and some AI systems, but not formalized as a law for LLM agents in text games.</p>            <p><strong>What is Novel:</strong> This law formalizes the necessity of memory management for scalable LLM agent memory in text games, with direct evidence from ablations and best practices.</p>
            <p><strong>References:</strong> <ul>
    <li>Baddeley (2000) The episodic buffer: a new component of working memory? [cognitive science]</li>
    <li>Weston et al. (2014) Memory Networks [need for memory selection and relevance]</li>
    <li>Park et al. (2023) Generative agents: Interactive simulacra of human behavior [reflection and summarization for memory management]</li>
    <li>Zhou et al. (2023) RecurrentGPT [summarization and memory capping]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Ablating either short-term or long-term memory in a hybrid-memory LLM agent will cause a measurable drop in performance on long-horizon or compositional tasks.</li>
                <li>Adding explicit memory management (summarization, prioritization, retrieval) to a hybrid-memory agent will improve scalability and maintain performance as the number of episodes or memory size increases.</li>
                <li>In a multi-agent simulation, agents with hybrid memory will maintain more consistent and human-like behavior over time than those with only prompt-based or only long-term memory.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a hybrid-memory LLM agent is deployed in a real-time, multi-agent environment with dynamically changing task structure, it will develop emergent meta-memory strategies (e.g., selective forgetting, dynamic prioritization).</li>
                <li>If memory management is learned (rather than rule-based), agents will outperform static management strategies on tasks with highly variable or adversarial memory demands.</li>
                <li>If hybrid memory is combined with explicit ToM (theory of mind) modules, agents will exhibit more robust social reasoning and deception detection in social deduction games.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a single-component memory agent (only prompt or only long-term) matches or exceeds the performance of a hybrid-memory agent on long-horizon, compositional, or cross-episode tasks, the theory would be challenged.</li>
                <li>If memory management (summarization, prioritization) does not improve scalability or performance as memory size increases, the theory would be undermined.</li>
                <li>If hybrid memory does not improve behavioral consistency or generalization in multi-agent or open-ended environments, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some transformer-based agents (e.g., LIGHT, LLM-DND) achieve strong performance on dialogue and state-tracking tasks using only context window and control features, without explicit external memory. <a href="../results/extraction-result-4904.html#e4904.3" class="evidence-link">[e4904.3]</a> <a href="../results/extraction-result-4854.html#e4854.0" class="evidence-link">[e4854.0]</a> </li>
    <li>In some web navigation and computer control tasks (e.g., Mind2Web, MindAct), memory is limited to action history in prompt, and the effect of richer memory is not directly tested. <a href="../results/extraction-result-4879.html#e4879.0" class="evidence-link">[e4879.0]</a> <a href="../results/extraction-result-4882.html#e4882.1" class="evidence-link">[e4882.1]</a> </li>
    <li>Some LLM agents (e.g., ReAct, SayCan) perform well on certain long-horizon tasks without explicit external memory, possibly due to strong in-context reasoning or model scale. <a href="../results/extraction-result-4850.html#e4850.0" class="evidence-link">[e4850.0]</a> <a href="../results/extraction-result-4898.html#e4898.2" class="evidence-link">[e4898.2]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While hybrid memory and memory management are known concepts, this theory formalizes and empirically grounds them for LLM agent architectures in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Baddeley (2000) The episodic buffer: a new component of working memory? [cognitive science hybrid memory]</li>
    <li>Weston et al. (2014) Memory Networks [external memory for reasoning]</li>
    <li>Park et al. (2023) Generative agents: Interactive simulacra of human behavior [hybrid memory for social simulation]</li>
    <li>Zhou et al. (2023) RecurrentGPT [hybrid memory for long-form generation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hybrid Memory Architecture Principle for LLM Agents",
    "theory_description": "LLM agents for text games achieve optimal performance and generalization when equipped with a hybrid memory architecture that combines (1) short-term working memory (prompt context or rolling window), (2) long-term episodic or structured memory (vector DB, knowledge graph, or skill library), and (3) memory management mechanisms (summarization, prioritization, retrieval, and reflection). The hybrid approach enables agents to maintain local coherence, recall distant facts or skills, and adaptively manage memory to fit context and task demands.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hybrid Memory Outperforms Single-Component Memory",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "uses",
                        "object": "hybrid memory (short-term + long-term + management)"
                    },
                    {
                        "subject": "task",
                        "relation": "has_property",
                        "object": "long-horizon, compositional, or cross-episode dependencies"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "achieves",
                        "object": "higher performance and generalization than agents with only short-term or only long-term memory"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "RecurrentGPT ablations show that removing either short-term or long-term memory causes large drops in coherence and interestingness; full hybrid model is preferred by humans.",
                        "uuids": [
                            "e4876.2",
                            "e4876.3",
                            "e4876.4"
                        ]
                    },
                    {
                        "text": "Voyager's combination of prompt context, skill library (vector DB), and curriculum history enables open-ended exploration and zero-shot generalization; ablation of skill library or self-verification causes plateauing.",
                        "uuids": [
                            "e4919.0"
                        ]
                    },
                    {
                        "text": "AGENTS, AgentSims, and Generative Agents (Park et al.) all use hybrid memory (prompt short-term + vector DB long-term) to maintain behavioral consistency and long-horizon planning.",
                        "uuids": [
                            "e4915.0",
                            "e4900.4",
                            "e4919.4",
                            "e4685.3"
                        ]
                    },
                    {
                        "text": "PsychoGAT uses summarization-based narrative memory (short-term) and explicit memory management (justification, capping size) to maintain coherence in long interactive fiction.",
                        "uuids": [
                            "e4661.0"
                        ]
                    },
                    {
                        "text": "Reflexion and ExpeL combine short-term sliding window, persistent long-term insights, and reflection to improve planning and cross-task learning.",
                        "uuids": [
                            "e4900.3",
                            "e4683.2",
                            "e4920.4"
                        ]
                    },
                    {
                        "text": "AgentBench, LLM-Agent (LLM-Coordination), and ProAgent use multi-component memory scaffolds (long-term, working, episodic) to support multi-agent coordination and ToM reasoning.",
                        "uuids": [
                            "e4917.1",
                            "e4659.0",
                            "e4802.0"
                        ]
                    },
                    {
                        "text": "Memory management agents and episodic buffers are recommended for scalable, context-appropriate memory retrieval and event-level recall.",
                        "uuids": [
                            "e4663.3",
                            "e4663.1"
                        ]
                    },
                    {
                        "text": "Generative Agents and AgentSims use reflection and summarization to condense long-term memory and maintain relevance.",
                        "uuids": [
                            "e4919.4",
                            "e4900.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hybrid memory architectures are discussed in cognitive science and some AI systems, but not formalized as a principle for LLM agents in text games with direct ablation evidence.",
                    "what_is_novel": "This law formalizes the hybrid memory principle for LLM agents in text games, with direct ablation and comparative evidence across multiple architectures and environments.",
                    "classification_explanation": "While hybrid memory is a concept in cognitive architectures, this law extends and formalizes it for LLM agents in text games, with new empirical support and explicit mapping to agent design.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Baddeley (2000) The episodic buffer: a new component of working memory? [cognitive science hybrid memory]",
                        "Weston et al. (2014) Memory Networks [external memory for reasoning]",
                        "Park et al. (2023) Generative agents: Interactive simulacra of human behavior [hybrid memory for social simulation]",
                        "Zhou et al. (2023) RecurrentGPT [hybrid memory for long-form generation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Memory Management (Summarization, Prioritization, and Retrieval) is Essential for Scalability",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "uses",
                        "object": "hybrid memory with summarization, prioritization, and retrieval"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "maintains",
                        "object": "scalability and relevance of memory over long interactions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "RecurrentGPT and PsychoGAT use explicit summarization and memory capping to avoid context overflow and maintain coherence.",
                        "uuids": [
                            "e4876.2",
                            "e4876.3",
                            "e4661.0"
                        ]
                    },
                    {
                        "text": "Generative Agents and AgentSims use reflection and summarization to condense long-term memory and maintain relevance.",
                        "uuids": [
                            "e4919.4",
                            "e4900.4"
                        ]
                    },
                    {
                        "text": "Memory management agents (e.g., Memory Management Agent, Episodic Buffer) are recommended for curating, prioritizing, and retrieving relevant memories for planning.",
                        "uuids": [
                            "e4663.3",
                            "e4663.1"
                        ]
                    },
                    {
                        "text": "Storage & Retrieval Modes (text, embeddings, SQL, semantic search) and best practices recommend layered searches and hybrid representations for efficient retrieval.",
                        "uuids": [
                            "e4663.5"
                        ]
                    },
                    {
                        "text": "Observation summarization and failure recovery are recommended for long-horizon web tasks to mitigate context-window limitations.",
                        "uuids": [
                            "e4914.7"
                        ]
                    },
                    {
                        "text": "Generative Agents (GA) and GenerativeAgents (Park et al.) use periodic synthesis/reflection to create higher-level summaries and dynamically retrieve relevant memories.",
                        "uuids": [
                            "e4660.0",
                            "e4919.4",
                            "e4685.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Memory management (summarization, prioritization) is discussed in cognitive science and some AI systems, but not formalized as a law for LLM agents in text games.",
                    "what_is_novel": "This law formalizes the necessity of memory management for scalable LLM agent memory in text games, with direct evidence from ablations and best practices.",
                    "classification_explanation": "While memory management is a known concept, this law formalizes and empirically grounds it for LLM agent architectures in text games.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Baddeley (2000) The episodic buffer: a new component of working memory? [cognitive science]",
                        "Weston et al. (2014) Memory Networks [need for memory selection and relevance]",
                        "Park et al. (2023) Generative agents: Interactive simulacra of human behavior [reflection and summarization for memory management]",
                        "Zhou et al. (2023) RecurrentGPT [summarization and memory capping]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Ablating either short-term or long-term memory in a hybrid-memory LLM agent will cause a measurable drop in performance on long-horizon or compositional tasks.",
        "Adding explicit memory management (summarization, prioritization, retrieval) to a hybrid-memory agent will improve scalability and maintain performance as the number of episodes or memory size increases.",
        "In a multi-agent simulation, agents with hybrid memory will maintain more consistent and human-like behavior over time than those with only prompt-based or only long-term memory."
    ],
    "new_predictions_unknown": [
        "If a hybrid-memory LLM agent is deployed in a real-time, multi-agent environment with dynamically changing task structure, it will develop emergent meta-memory strategies (e.g., selective forgetting, dynamic prioritization).",
        "If memory management is learned (rather than rule-based), agents will outperform static management strategies on tasks with highly variable or adversarial memory demands.",
        "If hybrid memory is combined with explicit ToM (theory of mind) modules, agents will exhibit more robust social reasoning and deception detection in social deduction games."
    ],
    "negative_experiments": [
        "If a single-component memory agent (only prompt or only long-term) matches or exceeds the performance of a hybrid-memory agent on long-horizon, compositional, or cross-episode tasks, the theory would be challenged.",
        "If memory management (summarization, prioritization) does not improve scalability or performance as memory size increases, the theory would be undermined.",
        "If hybrid memory does not improve behavioral consistency or generalization in multi-agent or open-ended environments, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Some transformer-based agents (e.g., LIGHT, LLM-DND) achieve strong performance on dialogue and state-tracking tasks using only context window and control features, without explicit external memory.",
            "uuids": [
                "e4904.3",
                "e4854.0"
            ]
        },
        {
            "text": "In some web navigation and computer control tasks (e.g., Mind2Web, MindAct), memory is limited to action history in prompt, and the effect of richer memory is not directly tested.",
            "uuids": [
                "e4879.0",
                "e4882.1"
            ]
        },
        {
            "text": "Some LLM agents (e.g., ReAct, SayCan) perform well on certain long-horizon tasks without explicit external memory, possibly due to strong in-context reasoning or model scale.",
            "uuids": [
                "e4850.0",
                "e4898.2"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "CALM-GPT2 and n-gram models, which use only short context, outperform more complex memory-augmented models on some tasks, suggesting that task structure and memory utility interact in complex ways.",
            "uuids": [
                "e4899.0",
                "e4899.1"
            ]
        },
        {
            "text": "In some cases, increasing the context window (e.g., GPT-3.5-Turbo-16k) does not improve long-horizon performance, suggesting that model reasoning/planning bottlenecks may override memory effects.",
            "uuids": [
                "e4681.3"
            ]
        }
    ],
    "special_cases": [
        "If the LLM is sufficiently large and trained on similar tasks, it may internally encode enough knowledge to compensate for lack of explicit memory in some cases.",
        "In tasks with extremely simple or fully observable environments, hybrid memory may be unnecessary or even detrimental.",
        "If memory management is poorly tuned, hybrid memory can introduce noise or irrelevant information, harming performance.",
        "In adversarial or rapidly changing environments, additional mechanisms (e.g., memory invalidation, belief revision) may be required."
    ],
    "existing_theory": {
        "what_already_exists": "Hybrid memory architectures and memory management are discussed in cognitive science and some AI systems, but not formalized as a principle for LLM agents in text games with direct ablation evidence.",
        "what_is_novel": "This theory formalizes the hybrid memory principle and the necessity of memory management for LLM agents in text games, with direct ablation and comparative evidence.",
        "classification_explanation": "While hybrid memory and memory management are known concepts, this theory formalizes and empirically grounds them for LLM agent architectures in text games.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Baddeley (2000) The episodic buffer: a new component of working memory? [cognitive science hybrid memory]",
            "Weston et al. (2014) Memory Networks [external memory for reasoning]",
            "Park et al. (2023) Generative agents: Interactive simulacra of human behavior [hybrid memory for social simulation]",
            "Zhou et al. (2023) RecurrentGPT [hybrid memory for long-form generation]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
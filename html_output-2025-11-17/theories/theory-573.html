<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Human-Machine Evaluation Complementarity Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-573</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-573</p>
                <p><strong>Name:</strong> Human-Machine Evaluation Complementarity Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories, based on the following results.</p>
                <p><strong>Description:</strong> Optimal evaluation of LLM-generated scientific theories requires strategic combination of human expert judgment and automated metrics, where each compensates for the other's weaknesses. Human evaluation provides ground truth for validity, nuanced judgment, and domain expertise but is costly and limited in scale. Automated evaluation provides scalability, consistency, and efficiency but suffers from bias, limited semantic understanding, and inability to assess true novelty. The optimal evaluation strategy uses automated methods for large-scale screening and ranking, with human evaluation for validation, calibration, and final assessment of top candidates.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Human Evaluation Scalability Constraint Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_task &#8594; requires &#8594; human_expert_judgment<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_task &#8594; has_scale &#8594; N_items<span style="color: #888888;">, and</span></div>
        <div>&#8226; N_items &#8594; exceeds &#8594; human_capacity_threshold</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_task &#8594; becomes &#8594; prohibitively_expensive<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_task &#8594; suffers_from &#8594; limited_coverage</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human evaluation is costly and limited in scale; ratings can be subjective and domain-dependent despite guidelines; sample sizes (e.g., 200 abstracts for correlation) limit statistical power. <a href="../results/extraction-result-4439.html#e4439.3" class="evidence-link">[e4439.3]</a> </li>
    <li>Cost and limited scale (only 10 experts and 150 fully human-evaluated ideas due to cost), subjectivity and domain-specific variability in assessing ideas. <a href="../results/extraction-result-4568.html#e4568.0" class="evidence-link">[e4568.0]</a> </li>
    <li>Expert judgments can be subjective and influenced by prior beliefs; small expert panel size may limit representativeness. <a href="../results/extraction-result-4410.html#e4410.2" class="evidence-link">[e4410.2]</a> </li>
    <li>Human evaluation requires significant expert time; potential for human biases to shape model outputs; measuring contribution of model vs. human is challenging. <a href="../results/extraction-result-4612.html#e4612.12" class="evidence-link">[e4612.12]</a> </li>
    <li>Requires many expert judgments to be statistically robust; residual biases may remain; subjective preferences may not map to real-world testability or impact. <a href="../results/extraction-result-4612.html#e4612.1" class="evidence-link">[e4612.1]</a> </li>
    <li>Small evaluator pool (n=9), limited to datasets that are accessible to non-experts, potential lack of domain expertise for some tasks, potential subjectivity and limited statistical power. <a href="../results/extraction-result-4432.html#e4432.2" class="evidence-link">[e4432.2]</a> </li>
    <li>Subjectivity in difficulty/surprise ratings (mitigated by averaging), resource-intensiveness to scale, potential contributor bias. <a href="../results/extraction-result-4454.html#e4454.4" class="evidence-link">[e4454.4]</a> </li>
    <li>Small annotator pool (10 participants per task), majority vote may mask disagreements, subjective interpretation of 'novelty' may vary by annotator. <a href="../results/extraction-result-4447.html#e4447.5" class="evidence-link">[e4447.5]</a> </li>
    <li>Evaluating 'idea quality' is inherently subjective; alignment with human preferences may not reflect downstream empirical success; scaling expert evaluation is costly. <a href="../results/extraction-result-4478.html#e4478.3" class="evidence-link">[e4478.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The scalability limitations of human evaluation are well-known in evaluation research, but this formulation as a formal law with threshold conditions is novel. It explicitly formalizes the trade-off between quality and scale in the context of scientific theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Snow et al. (2008) Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks [discusses human evaluation costs and scalability]</li>
</ul>
            <h3>Statement 1: Automated Evaluation Bias Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; automated_evaluator &#8594; is_trained_on &#8594; dataset_D<span style="color: #888888;">, and</span></div>
        <div>&#8226; dataset_D &#8594; contains &#8594; systematic_biases<span style="color: #888888;">, and</span></div>
        <div>&#8226; automated_evaluator &#8594; evaluates &#8594; generated_theory</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; automated_evaluator &#8594; inherits &#8594; systematic_biases<span style="color: #888888;">, and</span></div>
        <div>&#8226; automated_evaluator &#8594; produces &#8594; biased_scores</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM evaluators can inherit hallucinations and biases, may conflate novelty with irrelevance, and can be overly optimistic or inconsistent; lack of standardized prompts and calibration leads to variability. <a href="../results/extraction-result-4430.html#e4430.1" class="evidence-link">[e4430.1]</a> </li>
    <li>G-EVAL assigns higher scores to GPT-3.5-generated summaries than to human-written summaries even when humans prefer the human summaries; possible self-reinforcement if used as reward signal. <a href="../results/extraction-result-4571.html#e4571.9" class="evidence-link">[e4571.9]</a> </li>
    <li>Evaluator and generator share the same parametric biases (risk of self-reinforcing errors); difficulty establishing calibration or correlation with expert judgments; vulnerable to circular validation. <a href="../results/extraction-result-4473.html#e4473.4" class="evidence-link">[e4473.4]</a> </li>
    <li>LLM-based reviewers often produce generic strengths/weaknesses and may not base scores on deep semantic understanding; model internals and sizes often unspecified; reliability varies by task and input format. <a href="../results/extraction-result-4435.html#e4435.3" class="evidence-link">[e4435.3]</a> </li>
    <li>LLM judges can be inconsistent and may not align with human preferences; can be biased by prompt phrasing; in this work performed worse than IBE-Eval in aligning to human judgments. <a href="../results/extraction-result-4466.html#e4466.6" class="evidence-link">[e4466.6]</a> </li>
    <li>LLM-judge outputs can reflect model biases and overconfidence; without external calibration against human experts or experiments, scores may not correspond to real-world correctness. <a href="../results/extraction-result-4442.html#e4442.1" class="evidence-link">[e4442.1]</a> </li>
    <li>Proxy imperfect—correlations <100% differ by task; GPT-4 can be overconfident or replicate biases; relying on closed-source model as evaluator has reproducibility and transparency concerns. <a href="../results/extraction-result-4546.html#e4546.2" class="evidence-link">[e4546.2]</a> </li>
    <li>Evaluator inherits hallucinations, outdated knowledge, and biases from its training data; evaluation uncertainty and lack of ground-truth for many hypothesis-quality dimensions. <a href="../results/extraction-result-4479.html#e4479.1" class="evidence-link">[e4479.1]</a> </li>
    <li>Training without human labels risks propagating biases from generator/evaluator pairs; adversarial-only RL causes pessimism. <a href="../results/extraction-result-4451.html#e4451.3" class="evidence-link">[e4451.3]</a> </li>
    <li>Reliance on LLM-based critics and an LLM evaluation agent introduces risk of hallucinated or systematically biased judgments; unanimous LLM critic agreement does not guarantee scientific correctness. <a href="../results/extraction-result-4452.html#e4452.0" class="evidence-link">[e4452.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Bias in automated systems is well-documented, but this specific formulation about evaluation bias inheritance in scientific theory assessment is novel. It formalizes the mechanism by which training data biases propagate to evaluation outcomes in the context of theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Mehrabi et al. (2021) A Survey on Bias and Fairness in Machine Learning [general bias in ML systems]</li>
    <li>Blodgett et al. (2020) Language (Technology) is Power [bias in NLP systems]</li>
</ul>
            <h3>Statement 2: Hybrid Evaluation Optimality Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; uses &#8594; automated_screening<span style="color: #888888;">, and</span></div>
        <div>&#8226; automated_screening &#8594; filters_to &#8594; top_K_candidates<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; applies &#8594; human_evaluation_to_top_K<span style="color: #888888;">, and</span></div>
        <div>&#8226; K &#8594; is_within &#8594; human_capacity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation_system &#8594; achieves &#8594; high_coverage<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; achieves &#8594; high_validity<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_system &#8594; remains &#8594; cost_effective</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Metric-Based Screening automatically evaluates candidates via quantitative criteria and statistical controls, then integrates with downstream human review for high-stakes validation (hybrid). <a href="../results/extraction-result-4450.html#e4450.2" class="evidence-link">[e4450.2]</a> </li>
    <li>Hybrid evaluation combining automated metrics and human evaluation produces more robust assessments than either alone. <a href="../results/extraction-result-4439.html#e4439.3" class="evidence-link">[e4439.3]</a> </li>
    <li>Automated (LLM-based) ranking used for selection plus subsequent human expert evaluation of top candidates. <a href="../results/extraction-result-4563.html#e4563.2" class="evidence-link">[e4563.2]</a> </li>
    <li>Automated numeric evaluation against experimental outcomes plus qualitative comparison of component-extraction outputs against human chemistry expert. <a href="../results/extraction-result-4525.html#e4525.1" class="evidence-link">[e4525.1]</a> </li>
    <li>Hybrid: automated scoring via OpenAI-o1-preview combined with an LLM-based multi-critic refinement loop; human expert evaluation used for cross-validation. <a href="../results/extraction-result-4452.html#e4452.0" class="evidence-link">[e4452.0]</a> </li>
    <li>Hybrid: automated LLM-driven hypothesis generation and automated LLM judging to estimate posterior coverage, with human annotations used for validation. <a href="../results/extraction-result-4459.html#e4459.0" class="evidence-link">[e4459.0]</a> </li>
    <li>Hybrid: automated feature computation and scoring with validation against human judgments and an LLM-as-judge baseline. <a href="../results/extraction-result-4466.html#e4466.0" class="evidence-link">[e4466.0]</a> </li>
    <li>Hybrid: automated quantitative metrics combined with human experimental studies (randomized controlled comparison for utility with statistical testing). <a href="../results/extraction-result-4447.html#e4447.0" class="evidence-link">[e4447.0]</a> </li>
    <li>Automated scoring guided by aspect criteria compared with human per-aspect annotations. <a href="../results/extraction-result-4476.html#e4476.10" class="evidence-link">[e4476.10]</a> </li>
    <li>Hybrid: automated scoring pipelines supplemented by human researcher preference judgments used to validate alignment and quality. <a href="../results/extraction-result-4478.html#e4478.3" class="evidence-link">[e4478.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Hybrid evaluation strategies are used in practice, but this formulation as an optimality law with explicit conditions (automated screening to top-K within human capacity) is novel. It formalizes the strategic combination principle for scientific theory evaluation.</p>
            <p><strong>References:</strong> <ul>
    <li>Dang et al. (2006) Overview of DUC 2006 [hybrid evaluation in summarization]</li>
</ul>
            <h3>Statement 3: Automated-Human Correlation Validation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; automated_metric &#8594; is_proposed_for &#8594; evaluation_task<span style="color: #888888;">, and</span></div>
        <div>&#8226; automated_metric &#8594; lacks &#8594; human_correlation_validation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; automated_metric &#8594; has_unknown &#8594; validity<span style="color: #888888;">, and</span></div>
        <div>&#8226; automated_metric &#8594; may_produce &#8594; misleading_results</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Meta-evaluation via correlation to human judgments: measures statistical correlations (Spearman and Pearson) between automated metric scores and human judgments to assess reliability. <a href="../results/extraction-result-4610.html#e4610.1" class="evidence-link">[e4610.1]</a> </li>
    <li>Validation by correlating automated scores with expert judgments or by measuring downstream empirical test outcomes; no explicit correlation study with human relevance judgments reported. <a href="../results/extraction-result-4428.html#e4428.1" class="evidence-link">[e4428.1]</a> </li>
    <li>Measured correlation between GPT-4-pref and human-pref: 82% (Sentiment Reversal), 68% (Acronym Generation), and 71% (Dialogue Response Generation). <a href="../results/extraction-result-4546.html#e4546.2" class="evidence-link">[e4546.2]</a> </li>
    <li>RND achieves AUROC = 0.820 on NeurIPS test set, validated against human-curated signals (Program Chairs' novelty comments). <a href="../results/extraction-result-4465.html#e4465.0" class="evidence-link">[e4465.0]</a> </li>
    <li>G-EVAL-4 substantially outperforms prior metrics across benchmarks with high correlation to human judgments (Spearman rho = 0.514 on SummEval). <a href="../results/extraction-result-4571.html#e4571.0" class="evidence-link">[e4571.0]</a> </li>
    <li>IBE-Eval's plausibility scores have a Spearman correlation of 0.64 (p < 0.01) with human judgments on sampled explanation pairs. <a href="../results/extraction-result-4466.html#e4466.0" class="evidence-link">[e4466.0]</a> </li>
    <li>Reported results: human annotators A1 vs A2 ρ ≈ 0.710 (p = 0.001); GPT-4 vs Mistral ρ ≈ 0.786 (p = 0.000). Correlations between annotators and LLMs were weak. <a href="../results/extraction-result-4434.html#e4434.2" class="evidence-link">[e4434.2]</a> </li>
    <li>Validation with ON showed Pearson r ≈ 0.52 between ON and human novelty ratings (200-abstract sample). <a href="../results/extraction-result-4439.html#e4439.3" class="evidence-link">[e4439.3]</a> </li>
    <li>Correlation analysis between GPT-4 preferences and human A/B judgments (percent agreement reported per task). <a href="../results/extraction-result-4546.html#e4546.2" class="evidence-link">[e4546.2]</a> </li>
    <li>Compared by correlating BLEU scores with human segment/system-level judgments (WMT datasets) and by model-selection experiments. <a href="../results/extraction-result-4587.html#e4587.1" class="evidence-link">[e4587.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The need for validation against human judgments is standard practice in evaluation research, but this explicit formulation as a law governing metric validity in scientific theory evaluation is novel. It formalizes the validation requirement as a conditional principle.</p>
            <p><strong>References:</strong> <ul>
    <li>Reiter & Belz (2009) An Investigation into the Validity of Some Metrics for Automatically Evaluating NLG [metric validation principles]</li>
</ul>
            <h3>Statement 4: Human Evaluation Calibration Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; human_evaluators &#8594; evaluate &#8594; theories<span style="color: #888888;">, and</span></div>
        <div>&#8226; human_evaluators &#8594; use &#8594; explicit_rubric<span style="color: #888888;">, and</span></div>
        <div>&#8226; explicit_rubric &#8594; provides &#8594; detailed_criteria_and_examples</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; human_evaluators &#8594; achieve &#8594; higher_inter_rater_agreement<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation_results &#8594; have &#8594; higher_reliability</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Structured rubrics and collaborative assessment protocols help produce more stable and reproducible outcomes; earlier work found relatively low agreement among expert reviewers. <a href="../results/extraction-result-4612.html#e4612.2" class="evidence-link">[e4612.2]</a> </li>
    <li>Expert panel review with explicit scoring rubrics achieved strong agreement: Cohen's κ = 0.82 for LLM-generated hypotheses. <a href="../results/extraction-result-4410.html#e4410.2" class="evidence-link">[e4410.2]</a> </li>
    <li>Evaluation rubric with detailed 5-point Likert scale descriptors for each metric (Validness, Novelty, Helpfulness) used identically by experts and GPT-4 to ensure consistent scoring; high inter-rater soft consistency (>0.75). <a href="../results/extraction-result-4457.html#e4457.3" class="evidence-link">[e4457.3]</a> </li>
    <li>Multi-rater reliability protocols with formalized scoring rubrics improve inter-annotator agreement and reproducibility. <a href="../results/extraction-result-4612.html#e4612.2" class="evidence-link">[e4612.2]</a> </li>
    <li>Human expert Likert-scale scoring showed high inter-annotator reliability: Spearman rank correlations between two human annotators: Problem 0.83, Method 0.76, Experiment 0.67. <a href="../results/extraction-result-4568.html#e4568.0" class="evidence-link">[e4568.0]</a> </li>
    <li>Requiring LLMs to provide justifications/rationales alongside ratings tends to improve alignment of LLM evaluations with human judgments. <a href="../results/extraction-result-4434.html#e4434.5" class="evidence-link">[e4434.5]</a> </li>
    <li>Including AD (aspect definitions) consistently improved performance regardless of prompt template or model size. <a href="../results/extraction-result-4476.html#e4476.10" class="evidence-link">[e4476.10]</a> </li>
    <li>Explicit textual criteria for scoring each of the three evaluation dimensions on a 1–5 Likert scale; used identically by experts and by GPT-4 to ensure consistent scoring. <a href="../results/extraction-result-4457.html#e4457.3" class="evidence-link">[e4457.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The value of explicit rubrics for inter-rater reliability is well-established in assessment research, but this formulation as a law specific to scientific theory evaluation is novel. It formalizes the calibration mechanism with explicit conditions.</p>
            <p><strong>References:</strong> <ul>
    <li>Jonsson & Svingby (2007) The use of scoring rubrics: Reliability, validity and educational consequences [rubric effects on reliability]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a large-scale evaluation of 10,000 generated theories uses automated metrics to select the top 100, then has experts evaluate those 100, it will achieve 95%+ coverage of truly high-quality theories while using <1% of the expert time required for full manual evaluation.</li>
                <li>If two automated metrics are developed for the same evaluation task, one validated against human judgments (correlation r=0.7) and one not validated, the validated metric will produce rankings that better predict downstream experimental success.</li>
                <li>If human evaluators are given detailed rubrics with examples versus vague instructions, inter-rater agreement will increase by at least 0.2 in Cohen's kappa.</li>
                <li>If an automated evaluator is trained on a dataset with 80% bias toward certain types of theories, it will systematically over-score similar theories by at least 15% compared to human expert judgments.</li>
                <li>If a hybrid evaluation system uses automated screening to reduce 1000 candidates to 50, then human evaluation of those 50, the final rankings will correlate at r>0.85 with rankings from full human evaluation of all 1000.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If automated evaluators are continuously updated with human feedback on their errors, they might eventually match or exceed human evaluation quality, but the learning curve and asymptotic performance are unknown.</li>
                <li>If the cost of human evaluation drops dramatically (e.g., through crowdsourcing or AI assistance), the optimal balance point between automated and human evaluation might shift, but the new equilibrium is unclear.</li>
                <li>If automated evaluators are trained on diverse multi-domain data versus domain-specific data, their cross-domain generalization might improve or degrade depending on whether domain-general principles dominate or domain-specific knowledge is critical.</li>
                <li>If human evaluators are given access to automated metric outputs before making their judgments, they might anchor on those scores, but whether this improves or degrades their judgment quality is unknown.</li>
                <li>If multiple automated metrics with different biases are ensembled, the combined system might cancel out individual biases or amplify them in unpredictable ways.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that purely automated evaluation without any human validation consistently outperforms hybrid approaches in predicting long-term scientific impact would challenge this theory.</li>
                <li>Demonstrating that human evaluation with minimal rubrics achieves the same inter-rater reliability as detailed rubrics would challenge the calibration law.</li>
                <li>Showing that automated metrics with zero correlation to human judgments still predict experimental success would challenge the correlation validation law.</li>
                <li>Finding that automated evaluators trained on biased data produce unbiased evaluations would challenge the bias inheritance law.</li>
                <li>Demonstrating that human evaluation scales linearly with no increase in cost per item would challenge the scalability constraint law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal ratio of automated to human evaluation for different domains and contexts is not specified. </li>
    <li>How to handle cases where automated and human evaluations strongly disagree is not addressed. </li>
    <li>The theory doesn't specify how to select which top-K candidates should receive human evaluation. </li>
    <li>The exact mechanism of how gravity works at the quantum level is not fully understood in the context of evaluation theory. </li>
    <li>How to calibrate automated metrics across different scientific domains with varying standards is not addressed. </li>
    <li>The theory does not specify how to handle temporal drift in evaluation standards as scientific fields evolve. </li>
    <li>How to balance multiple evaluation criteria (novelty, validity, feasibility) when they conflict is not formalized. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory formalizes the complementary relationship between human and automated evaluation in scientific theory assessment. While hybrid evaluation is practiced, the explicit formulation of laws governing when and how to combine them, including the optimality conditions, validation requirements, and bias mechanisms, is novel. The theory synthesizes existing practices into a formal framework with testable predictions.</p>
            <p><strong>References:</strong> <ul>
    <li>Dang et al. (2006) Overview of DUC 2006 [hybrid evaluation in summarization]</li>
    <li>Belz & Reiter (2006) Comparing Automatic and Human Evaluation of NLG Systems [human vs automated evaluation]</li>
    <li>Novikova et al. (2018) RankME: Reliable Human Ratings for Natural Language Generation [human evaluation methodology]</li>
    <li>Reiter & Belz (2009) An Investigation into the Validity of Some Metrics for Automatically Evaluating NLG [metric validation principles]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Human-Machine Evaluation Complementarity Theory",
    "theory_description": "Optimal evaluation of LLM-generated scientific theories requires strategic combination of human expert judgment and automated metrics, where each compensates for the other's weaknesses. Human evaluation provides ground truth for validity, nuanced judgment, and domain expertise but is costly and limited in scale. Automated evaluation provides scalability, consistency, and efficiency but suffers from bias, limited semantic understanding, and inability to assess true novelty. The optimal evaluation strategy uses automated methods for large-scale screening and ranking, with human evaluation for validation, calibration, and final assessment of top candidates.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Human Evaluation Scalability Constraint Law",
                "if": [
                    {
                        "subject": "evaluation_task",
                        "relation": "requires",
                        "object": "human_expert_judgment"
                    },
                    {
                        "subject": "evaluation_task",
                        "relation": "has_scale",
                        "object": "N_items"
                    },
                    {
                        "subject": "N_items",
                        "relation": "exceeds",
                        "object": "human_capacity_threshold"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_task",
                        "relation": "becomes",
                        "object": "prohibitively_expensive"
                    },
                    {
                        "subject": "evaluation_task",
                        "relation": "suffers_from",
                        "object": "limited_coverage"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human evaluation is costly and limited in scale; ratings can be subjective and domain-dependent despite guidelines; sample sizes (e.g., 200 abstracts for correlation) limit statistical power.",
                        "uuids": [
                            "e4439.3"
                        ]
                    },
                    {
                        "text": "Cost and limited scale (only 10 experts and 150 fully human-evaluated ideas due to cost), subjectivity and domain-specific variability in assessing ideas.",
                        "uuids": [
                            "e4568.0"
                        ]
                    },
                    {
                        "text": "Expert judgments can be subjective and influenced by prior beliefs; small expert panel size may limit representativeness.",
                        "uuids": [
                            "e4410.2"
                        ]
                    },
                    {
                        "text": "Human evaluation requires significant expert time; potential for human biases to shape model outputs; measuring contribution of model vs. human is challenging.",
                        "uuids": [
                            "e4612.12"
                        ]
                    },
                    {
                        "text": "Requires many expert judgments to be statistically robust; residual biases may remain; subjective preferences may not map to real-world testability or impact.",
                        "uuids": [
                            "e4612.1"
                        ]
                    },
                    {
                        "text": "Small evaluator pool (n=9), limited to datasets that are accessible to non-experts, potential lack of domain expertise for some tasks, potential subjectivity and limited statistical power.",
                        "uuids": [
                            "e4432.2"
                        ]
                    },
                    {
                        "text": "Subjectivity in difficulty/surprise ratings (mitigated by averaging), resource-intensiveness to scale, potential contributor bias.",
                        "uuids": [
                            "e4454.4"
                        ]
                    },
                    {
                        "text": "Small annotator pool (10 participants per task), majority vote may mask disagreements, subjective interpretation of 'novelty' may vary by annotator.",
                        "uuids": [
                            "e4447.5"
                        ]
                    },
                    {
                        "text": "Evaluating 'idea quality' is inherently subjective; alignment with human preferences may not reflect downstream empirical success; scaling expert evaluation is costly.",
                        "uuids": [
                            "e4478.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "The scalability limitations of human evaluation are well-known in evaluation research, but this formulation as a formal law with threshold conditions is novel. It explicitly formalizes the trade-off between quality and scale in the context of scientific theory evaluation.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Snow et al. (2008) Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks [discusses human evaluation costs and scalability]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Automated Evaluation Bias Law",
                "if": [
                    {
                        "subject": "automated_evaluator",
                        "relation": "is_trained_on",
                        "object": "dataset_D"
                    },
                    {
                        "subject": "dataset_D",
                        "relation": "contains",
                        "object": "systematic_biases"
                    },
                    {
                        "subject": "automated_evaluator",
                        "relation": "evaluates",
                        "object": "generated_theory"
                    }
                ],
                "then": [
                    {
                        "subject": "automated_evaluator",
                        "relation": "inherits",
                        "object": "systematic_biases"
                    },
                    {
                        "subject": "automated_evaluator",
                        "relation": "produces",
                        "object": "biased_scores"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM evaluators can inherit hallucinations and biases, may conflate novelty with irrelevance, and can be overly optimistic or inconsistent; lack of standardized prompts and calibration leads to variability.",
                        "uuids": [
                            "e4430.1"
                        ]
                    },
                    {
                        "text": "G-EVAL assigns higher scores to GPT-3.5-generated summaries than to human-written summaries even when humans prefer the human summaries; possible self-reinforcement if used as reward signal.",
                        "uuids": [
                            "e4571.9"
                        ]
                    },
                    {
                        "text": "Evaluator and generator share the same parametric biases (risk of self-reinforcing errors); difficulty establishing calibration or correlation with expert judgments; vulnerable to circular validation.",
                        "uuids": [
                            "e4473.4"
                        ]
                    },
                    {
                        "text": "LLM-based reviewers often produce generic strengths/weaknesses and may not base scores on deep semantic understanding; model internals and sizes often unspecified; reliability varies by task and input format.",
                        "uuids": [
                            "e4435.3"
                        ]
                    },
                    {
                        "text": "LLM judges can be inconsistent and may not align with human preferences; can be biased by prompt phrasing; in this work performed worse than IBE-Eval in aligning to human judgments.",
                        "uuids": [
                            "e4466.6"
                        ]
                    },
                    {
                        "text": "LLM-judge outputs can reflect model biases and overconfidence; without external calibration against human experts or experiments, scores may not correspond to real-world correctness.",
                        "uuids": [
                            "e4442.1"
                        ]
                    },
                    {
                        "text": "Proxy imperfect—correlations &lt;100% differ by task; GPT-4 can be overconfident or replicate biases; relying on closed-source model as evaluator has reproducibility and transparency concerns.",
                        "uuids": [
                            "e4546.2"
                        ]
                    },
                    {
                        "text": "Evaluator inherits hallucinations, outdated knowledge, and biases from its training data; evaluation uncertainty and lack of ground-truth for many hypothesis-quality dimensions.",
                        "uuids": [
                            "e4479.1"
                        ]
                    },
                    {
                        "text": "Training without human labels risks propagating biases from generator/evaluator pairs; adversarial-only RL causes pessimism.",
                        "uuids": [
                            "e4451.3"
                        ]
                    },
                    {
                        "text": "Reliance on LLM-based critics and an LLM evaluation agent introduces risk of hallucinated or systematically biased judgments; unanimous LLM critic agreement does not guarantee scientific correctness.",
                        "uuids": [
                            "e4452.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "Bias in automated systems is well-documented, but this specific formulation about evaluation bias inheritance in scientific theory assessment is novel. It formalizes the mechanism by which training data biases propagate to evaluation outcomes in the context of theory evaluation.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Mehrabi et al. (2021) A Survey on Bias and Fairness in Machine Learning [general bias in ML systems]",
                        "Blodgett et al. (2020) Language (Technology) is Power [bias in NLP systems]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Hybrid Evaluation Optimality Law",
                "if": [
                    {
                        "subject": "evaluation_system",
                        "relation": "uses",
                        "object": "automated_screening"
                    },
                    {
                        "subject": "automated_screening",
                        "relation": "filters_to",
                        "object": "top_K_candidates"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "applies",
                        "object": "human_evaluation_to_top_K"
                    },
                    {
                        "subject": "K",
                        "relation": "is_within",
                        "object": "human_capacity"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation_system",
                        "relation": "achieves",
                        "object": "high_coverage"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "achieves",
                        "object": "high_validity"
                    },
                    {
                        "subject": "evaluation_system",
                        "relation": "remains",
                        "object": "cost_effective"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Metric-Based Screening automatically evaluates candidates via quantitative criteria and statistical controls, then integrates with downstream human review for high-stakes validation (hybrid).",
                        "uuids": [
                            "e4450.2"
                        ]
                    },
                    {
                        "text": "Hybrid evaluation combining automated metrics and human evaluation produces more robust assessments than either alone.",
                        "uuids": [
                            "e4439.3"
                        ]
                    },
                    {
                        "text": "Automated (LLM-based) ranking used for selection plus subsequent human expert evaluation of top candidates.",
                        "uuids": [
                            "e4563.2"
                        ]
                    },
                    {
                        "text": "Automated numeric evaluation against experimental outcomes plus qualitative comparison of component-extraction outputs against human chemistry expert.",
                        "uuids": [
                            "e4525.1"
                        ]
                    },
                    {
                        "text": "Hybrid: automated scoring via OpenAI-o1-preview combined with an LLM-based multi-critic refinement loop; human expert evaluation used for cross-validation.",
                        "uuids": [
                            "e4452.0"
                        ]
                    },
                    {
                        "text": "Hybrid: automated LLM-driven hypothesis generation and automated LLM judging to estimate posterior coverage, with human annotations used for validation.",
                        "uuids": [
                            "e4459.0"
                        ]
                    },
                    {
                        "text": "Hybrid: automated feature computation and scoring with validation against human judgments and an LLM-as-judge baseline.",
                        "uuids": [
                            "e4466.0"
                        ]
                    },
                    {
                        "text": "Hybrid: automated quantitative metrics combined with human experimental studies (randomized controlled comparison for utility with statistical testing).",
                        "uuids": [
                            "e4447.0"
                        ]
                    },
                    {
                        "text": "Automated scoring guided by aspect criteria compared with human per-aspect annotations.",
                        "uuids": [
                            "e4476.10"
                        ]
                    },
                    {
                        "text": "Hybrid: automated scoring pipelines supplemented by human researcher preference judgments used to validate alignment and quality.",
                        "uuids": [
                            "e4478.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "Hybrid evaluation strategies are used in practice, but this formulation as an optimality law with explicit conditions (automated screening to top-K within human capacity) is novel. It formalizes the strategic combination principle for scientific theory evaluation.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Dang et al. (2006) Overview of DUC 2006 [hybrid evaluation in summarization]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Automated-Human Correlation Validation Law",
                "if": [
                    {
                        "subject": "automated_metric",
                        "relation": "is_proposed_for",
                        "object": "evaluation_task"
                    },
                    {
                        "subject": "automated_metric",
                        "relation": "lacks",
                        "object": "human_correlation_validation"
                    }
                ],
                "then": [
                    {
                        "subject": "automated_metric",
                        "relation": "has_unknown",
                        "object": "validity"
                    },
                    {
                        "subject": "automated_metric",
                        "relation": "may_produce",
                        "object": "misleading_results"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Meta-evaluation via correlation to human judgments: measures statistical correlations (Spearman and Pearson) between automated metric scores and human judgments to assess reliability.",
                        "uuids": [
                            "e4610.1"
                        ]
                    },
                    {
                        "text": "Validation by correlating automated scores with expert judgments or by measuring downstream empirical test outcomes; no explicit correlation study with human relevance judgments reported.",
                        "uuids": [
                            "e4428.1"
                        ]
                    },
                    {
                        "text": "Measured correlation between GPT-4-pref and human-pref: 82% (Sentiment Reversal), 68% (Acronym Generation), and 71% (Dialogue Response Generation).",
                        "uuids": [
                            "e4546.2"
                        ]
                    },
                    {
                        "text": "RND achieves AUROC = 0.820 on NeurIPS test set, validated against human-curated signals (Program Chairs' novelty comments).",
                        "uuids": [
                            "e4465.0"
                        ]
                    },
                    {
                        "text": "G-EVAL-4 substantially outperforms prior metrics across benchmarks with high correlation to human judgments (Spearman rho = 0.514 on SummEval).",
                        "uuids": [
                            "e4571.0"
                        ]
                    },
                    {
                        "text": "IBE-Eval's plausibility scores have a Spearman correlation of 0.64 (p &lt; 0.01) with human judgments on sampled explanation pairs.",
                        "uuids": [
                            "e4466.0"
                        ]
                    },
                    {
                        "text": "Reported results: human annotators A1 vs A2 ρ ≈ 0.710 (p = 0.001); GPT-4 vs Mistral ρ ≈ 0.786 (p = 0.000). Correlations between annotators and LLMs were weak.",
                        "uuids": [
                            "e4434.2"
                        ]
                    },
                    {
                        "text": "Validation with ON showed Pearson r ≈ 0.52 between ON and human novelty ratings (200-abstract sample).",
                        "uuids": [
                            "e4439.3"
                        ]
                    },
                    {
                        "text": "Correlation analysis between GPT-4 preferences and human A/B judgments (percent agreement reported per task).",
                        "uuids": [
                            "e4546.2"
                        ]
                    },
                    {
                        "text": "Compared by correlating BLEU scores with human segment/system-level judgments (WMT datasets) and by model-selection experiments.",
                        "uuids": [
                            "e4587.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "The need for validation against human judgments is standard practice in evaluation research, but this explicit formulation as a law governing metric validity in scientific theory evaluation is novel. It formalizes the validation requirement as a conditional principle.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Reiter & Belz (2009) An Investigation into the Validity of Some Metrics for Automatically Evaluating NLG [metric validation principles]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Human Evaluation Calibration Law",
                "if": [
                    {
                        "subject": "human_evaluators",
                        "relation": "evaluate",
                        "object": "theories"
                    },
                    {
                        "subject": "human_evaluators",
                        "relation": "use",
                        "object": "explicit_rubric"
                    },
                    {
                        "subject": "explicit_rubric",
                        "relation": "provides",
                        "object": "detailed_criteria_and_examples"
                    }
                ],
                "then": [
                    {
                        "subject": "human_evaluators",
                        "relation": "achieve",
                        "object": "higher_inter_rater_agreement"
                    },
                    {
                        "subject": "evaluation_results",
                        "relation": "have",
                        "object": "higher_reliability"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Structured rubrics and collaborative assessment protocols help produce more stable and reproducible outcomes; earlier work found relatively low agreement among expert reviewers.",
                        "uuids": [
                            "e4612.2"
                        ]
                    },
                    {
                        "text": "Expert panel review with explicit scoring rubrics achieved strong agreement: Cohen's κ = 0.82 for LLM-generated hypotheses.",
                        "uuids": [
                            "e4410.2"
                        ]
                    },
                    {
                        "text": "Evaluation rubric with detailed 5-point Likert scale descriptors for each metric (Validness, Novelty, Helpfulness) used identically by experts and GPT-4 to ensure consistent scoring; high inter-rater soft consistency (&gt;0.75).",
                        "uuids": [
                            "e4457.3"
                        ]
                    },
                    {
                        "text": "Multi-rater reliability protocols with formalized scoring rubrics improve inter-annotator agreement and reproducibility.",
                        "uuids": [
                            "e4612.2"
                        ]
                    },
                    {
                        "text": "Human expert Likert-scale scoring showed high inter-annotator reliability: Spearman rank correlations between two human annotators: Problem 0.83, Method 0.76, Experiment 0.67.",
                        "uuids": [
                            "e4568.0"
                        ]
                    },
                    {
                        "text": "Requiring LLMs to provide justifications/rationales alongside ratings tends to improve alignment of LLM evaluations with human judgments.",
                        "uuids": [
                            "e4434.5"
                        ]
                    },
                    {
                        "text": "Including AD (aspect definitions) consistently improved performance regardless of prompt template or model size.",
                        "uuids": [
                            "e4476.10"
                        ]
                    },
                    {
                        "text": "Explicit textual criteria for scoring each of the three evaluation dimensions on a 1–5 Likert scale; used identically by experts and by GPT-4 to ensure consistent scoring.",
                        "uuids": [
                            "e4457.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "The value of explicit rubrics for inter-rater reliability is well-established in assessment research, but this formulation as a law specific to scientific theory evaluation is novel. It formalizes the calibration mechanism with explicit conditions.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Jonsson & Svingby (2007) The use of scoring rubrics: Reliability, validity and educational consequences [rubric effects on reliability]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a large-scale evaluation of 10,000 generated theories uses automated metrics to select the top 100, then has experts evaluate those 100, it will achieve 95%+ coverage of truly high-quality theories while using &lt;1% of the expert time required for full manual evaluation.",
        "If two automated metrics are developed for the same evaluation task, one validated against human judgments (correlation r=0.7) and one not validated, the validated metric will produce rankings that better predict downstream experimental success.",
        "If human evaluators are given detailed rubrics with examples versus vague instructions, inter-rater agreement will increase by at least 0.2 in Cohen's kappa.",
        "If an automated evaluator is trained on a dataset with 80% bias toward certain types of theories, it will systematically over-score similar theories by at least 15% compared to human expert judgments.",
        "If a hybrid evaluation system uses automated screening to reduce 1000 candidates to 50, then human evaluation of those 50, the final rankings will correlate at r&gt;0.85 with rankings from full human evaluation of all 1000."
    ],
    "new_predictions_unknown": [
        "If automated evaluators are continuously updated with human feedback on their errors, they might eventually match or exceed human evaluation quality, but the learning curve and asymptotic performance are unknown.",
        "If the cost of human evaluation drops dramatically (e.g., through crowdsourcing or AI assistance), the optimal balance point between automated and human evaluation might shift, but the new equilibrium is unclear.",
        "If automated evaluators are trained on diverse multi-domain data versus domain-specific data, their cross-domain generalization might improve or degrade depending on whether domain-general principles dominate or domain-specific knowledge is critical.",
        "If human evaluators are given access to automated metric outputs before making their judgments, they might anchor on those scores, but whether this improves or degrades their judgment quality is unknown.",
        "If multiple automated metrics with different biases are ensembled, the combined system might cancel out individual biases or amplify them in unpredictable ways."
    ],
    "negative_experiments": [
        "Finding that purely automated evaluation without any human validation consistently outperforms hybrid approaches in predicting long-term scientific impact would challenge this theory.",
        "Demonstrating that human evaluation with minimal rubrics achieves the same inter-rater reliability as detailed rubrics would challenge the calibration law.",
        "Showing that automated metrics with zero correlation to human judgments still predict experimental success would challenge the correlation validation law.",
        "Finding that automated evaluators trained on biased data produce unbiased evaluations would challenge the bias inheritance law.",
        "Demonstrating that human evaluation scales linearly with no increase in cost per item would challenge the scalability constraint law."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal ratio of automated to human evaluation for different domains and contexts is not specified.",
            "uuids": []
        },
        {
            "text": "How to handle cases where automated and human evaluations strongly disagree is not addressed.",
            "uuids": []
        },
        {
            "text": "The theory doesn't specify how to select which top-K candidates should receive human evaluation.",
            "uuids": []
        },
        {
            "text": "The exact mechanism of how gravity works at the quantum level is not fully understood in the context of evaluation theory.",
            "uuids": []
        },
        {
            "text": "How to calibrate automated metrics across different scientific domains with varying standards is not addressed.",
            "uuids": []
        },
        {
            "text": "The theory does not specify how to handle temporal drift in evaluation standards as scientific fields evolve.",
            "uuids": []
        },
        {
            "text": "How to balance multiple evaluation criteria (novelty, validity, feasibility) when they conflict is not formalized.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "ChatGPT outperforms crowd workers for text-annotation tasks with higher accuracy and much higher intercoder agreement, suggesting automated evaluation may sometimes exceed human performance.",
            "uuids": [
                "e4519.4"
            ]
        },
        {
            "text": "Some automated metrics show weak or no correlation with human judgments yet are still widely used, suggesting correlation validation may not always be necessary.",
            "uuids": [
                "e4434.2"
            ]
        },
        {
            "text": "Lower temperature (0.2) yielded higher intercoder agreement (97%) than temperature=1 (91%) while not decreasing accuracy, suggesting automated consistency can be very high.",
            "uuids": [
                "e4519.3"
            ]
        },
        {
            "text": "T5-based fine-tuned models outperform GPT-based models on automated metrics (ROUGE-L and BERTScore), but GPT4 outputs are longer and score lower on automated similarity metrics despite higher human preference.",
            "uuids": [
                "e4433.3"
            ]
        },
        {
            "text": "Automated similarity metrics reward surface-level similarity and templates, fail to capture novelty or technical depth, and can penalize longer, more detailed outputs even when better per human judgment.",
            "uuids": [
                "e4433.3"
            ]
        }
    ],
    "special_cases": [
        "For well-defined tasks with clear ground truth (e.g., mathematical proofs), automated evaluation may be sufficient without human validation.",
        "For highly novel or paradigm-shifting theories, human evaluation may be essential even at large scale, as automated metrics may not recognize breakthrough ideas.",
        "In time-critical scenarios (e.g., rapid response to emerging threats), automated evaluation may be used without human validation despite reduced accuracy.",
        "For domains with limited expert availability (e.g., highly specialized subfields), automated evaluation may need to carry more weight despite lower reliability.",
        "When evaluating theories that challenge existing paradigms, automated metrics trained on conventional theories may systematically undervalue them, requiring careful human oversight.",
        "For evaluation of theories in emerging fields with no established benchmarks, human evaluation becomes essential as automated metrics lack training data.",
        "In cases where the cost of false negatives is very high (e.g., medical theories), human validation of all automated rejections may be necessary."
    ],
    "existing_theory": {
        "classification_explanation": "This theory formalizes the complementary relationship between human and automated evaluation in scientific theory assessment. While hybrid evaluation is practiced, the explicit formulation of laws governing when and how to combine them, including the optimality conditions, validation requirements, and bias mechanisms, is novel. The theory synthesizes existing practices into a formal framework with testable predictions.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Dang et al. (2006) Overview of DUC 2006 [hybrid evaluation in summarization]",
            "Belz & Reiter (2006) Comparing Automatic and Human Evaluation of NLG Systems [human vs automated evaluation]",
            "Novikova et al. (2018) RankME: Reliable Human Ratings for Natural Language Generation [human evaluation methodology]",
            "Reiter & Belz (2009) An Investigation into the Validity of Some Metrics for Automatically Evaluating NLG [metric validation principles]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
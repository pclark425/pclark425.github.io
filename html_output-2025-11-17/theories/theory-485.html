<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Representation-Format Sensitivity Theory of LLM Spatial Reasoning - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-485</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-485</p>
                <p><strong>Name:</strong> Representation-Format Sensitivity Theory of LLM Spatial Reasoning</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku, based on the following results.</p>
                <p><strong>Description:</strong> The spatial reasoning ability of LLMs is highly sensitive to the format and explicitness of the input representation. LLMs perform best on spatial puzzles when the spatial structure is made explicit in the prompt (e.g., as ASCII/emoji grids, coordinate lists, or flattened grid tokens), and their performance degrades when spatial information is implicit, ambiguous, or only available in visual form. This sensitivity extends to both text-only and multimodal models, and is a key bottleneck for generalization and robustness in spatial reasoning tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Explicit Representation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; spatial puzzle input &#8594; is_formatted_as &#8594; explicit spatial structure (e.g., ASCII grid, coordinate list, flattened grid tokens)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves &#8594; higher spatial reasoning accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GPT-4 VoT and Llama3-70B VoT achieve higher accuracy when spatial state is represented as ASCII/emoji grids, compared to unstructured text. <a href="../results/extraction-result-3107.html#e3107.0" class="evidence-link">[e3107.0]</a> <a href="../results/extraction-result-3107.html#e3107.3" class="evidence-link">[e3107.3]</a> </li>
    <li>Maze-Nav and Spatial-Grid tasks: LLMs and VLMs perform better on text-only (ASCII or coordinate) representations than on vision-only (image) inputs. <a href="../results/extraction-result-3103.html#e3103.0" class="evidence-link">[e3103.0]</a> <a href="../results/extraction-result-3103.html#e3103.1" class="evidence-link">[e3103.1]</a> <a href="../results/extraction-result-3103.html#e3103.4" class="evidence-link">[e3103.4]</a> <a href="../results/extraction-result-3103.html#e3103.5" class="evidence-link">[e3103.5]</a> </li>
    <li>Minesweeper: LLMs perform better on coordinate representations than on table-formatted input with unusual symbols. <a href="../results/extraction-result-3096.html#e3096.0" class="evidence-link">[e3096.0]</a> <a href="../results/extraction-result-3096.html#e3096.1" class="evidence-link">[e3096.1]</a> <a href="../results/extraction-result-3096.html#e3096.2" class="evidence-link">[e3096.2]</a> </li>
    <li>In crossword solving, explicit word-mask templates (revealing letters) improve accuracy monotonically with % of letters revealed, indicating that explicit spatial constraints in the prompt are beneficial. <a href="../results/extraction-result-3384.html#e3384.1" class="evidence-link">[e3384.1]</a> </li>
    <li>In Sudoku and ARC, LLMs perform better when grid structure is made explicit (nested lists, flattened tokens) rather than as single strings. <a href="../results/extraction-result-3106.html#e3106.3" class="evidence-link">[e3106.3]</a> <a href="../results/extraction-result-3403.html#e3403.1" class="evidence-link">[e3403.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Implicit/Visual Representation Limitation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; spatial puzzle input &#8594; is_formatted_as &#8594; implicit or visual-only (e.g., image, ambiguous text)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM or VLM &#8594; achieves &#8594; lower spatial reasoning accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>VLMs (e.g., LLaVA-1.6-34B, InternLM-XComposer2, Claude-3 Opus) often perform worse with visual input than with text-only, and sometimes improve when the image is removed. <a href="../results/extraction-result-3103.html#e3103.5" class="evidence-link">[e3103.5]</a> <a href="../results/extraction-result-3433.html#e3433.3" class="evidence-link">[e3433.3]</a> <a href="../results/extraction-result-3392.html#e3392.2" class="evidence-link">[e3392.2]</a> </li>
    <li>GPT-4V and other multimodal models underperform text-only GPT-4 VoT on synthetic 2D grid tasks, indicating that visual input does not always help spatial reasoning. <a href="../results/extraction-result-3107.html#e3107.1" class="evidence-link">[e3107.1]</a> </li>
    <li>In MathVerse and MathVista, MLLMs often rely on textual redundancy and perform poorly when diagrams are the only source of spatial information. <a href="../results/extraction-result-3433.html#e3433.4" class="evidence-link">[e3433.4]</a> <a href="../results/extraction-result-3433.html#e3433.3" class="evidence-link">[e3433.3]</a> <a href="../results/extraction-result-3405.html#e3405.2" class="evidence-link">[e3405.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Providing more explicit spatial structure in the prompt (e.g., ASCII grid, coordinate list) will improve LLM performance on new spatial puzzles compared to ambiguous or visual-only input.</li>
                <li>Removing or degrading the explicitness of spatial structure in the prompt will reduce LLM accuracy, even for large models.</li>
                <li>VLMs will continue to underperform their LLM backbones on spatial tasks when only visual input is provided, unless their visual perception is substantially improved.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained extensively on ambiguous or visual-only spatial representations, they may eventually learn to extract spatial structure from such inputs.</li>
                <li>Future VLMs with improved visual encoders and spatial grounding may close the gap with text-only LLMs on spatial reasoning tasks.</li>
                <li>Combining explicit textual and visual representations in a hybrid prompt may yield superadditive gains in spatial reasoning accuracy.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs achieve high spatial reasoning accuracy on ambiguous or visual-only inputs without explicit spatial structure, this would challenge the explicit representation law.</li>
                <li>If VLMs outperform LLMs on vision-only spatial tasks without any textual support, this would challenge the implicit/visual representation limitation law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some hybrid neuro-symbolic and search-based systems (e.g., XoT, RAP, Map+ASP) achieve high accuracy regardless of input representation, as they rely on explicit world models or symbolic reasoning. <a href="../results/extraction-result-3108.html#e3108.1" class="evidence-link">[e3108.1]</a> <a href="../results/extraction-result-3108.html#e3108.3" class="evidence-link">[e3108.3]</a> <a href="../results/extraction-result-3409.html#e3409.0" class="evidence-link">[e3409.0]</a> <a href="../results/extraction-result-3429.html#e3429.1" class="evidence-link">[e3429.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhang et al. (2024) Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models [VoT as a prompting method, but this theory generalizes to representation format sensitivity]</li>
    <li>Liu et al. (2024) Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models [Systematic study of modality and representation effects]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [CoT as a method, but not focused on representation format]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Representation-Format Sensitivity Theory of LLM Spatial Reasoning",
    "theory_description": "The spatial reasoning ability of LLMs is highly sensitive to the format and explicitness of the input representation. LLMs perform best on spatial puzzles when the spatial structure is made explicit in the prompt (e.g., as ASCII/emoji grids, coordinate lists, or flattened grid tokens), and their performance degrades when spatial information is implicit, ambiguous, or only available in visual form. This sensitivity extends to both text-only and multimodal models, and is a key bottleneck for generalization and robustness in spatial reasoning tasks.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Explicit Representation Law",
                "if": [
                    {
                        "subject": "spatial puzzle input",
                        "relation": "is_formatted_as",
                        "object": "explicit spatial structure (e.g., ASCII grid, coordinate list, flattened grid tokens)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves",
                        "object": "higher spatial reasoning accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GPT-4 VoT and Llama3-70B VoT achieve higher accuracy when spatial state is represented as ASCII/emoji grids, compared to unstructured text.",
                        "uuids": [
                            "e3107.0",
                            "e3107.3"
                        ]
                    },
                    {
                        "text": "Maze-Nav and Spatial-Grid tasks: LLMs and VLMs perform better on text-only (ASCII or coordinate) representations than on vision-only (image) inputs.",
                        "uuids": [
                            "e3103.0",
                            "e3103.1",
                            "e3103.4",
                            "e3103.5"
                        ]
                    },
                    {
                        "text": "Minesweeper: LLMs perform better on coordinate representations than on table-formatted input with unusual symbols.",
                        "uuids": [
                            "e3096.0",
                            "e3096.1",
                            "e3096.2"
                        ]
                    },
                    {
                        "text": "In crossword solving, explicit word-mask templates (revealing letters) improve accuracy monotonically with % of letters revealed, indicating that explicit spatial constraints in the prompt are beneficial.",
                        "uuids": [
                            "e3384.1"
                        ]
                    },
                    {
                        "text": "In Sudoku and ARC, LLMs perform better when grid structure is made explicit (nested lists, flattened tokens) rather than as single strings.",
                        "uuids": [
                            "e3106.3",
                            "e3403.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Implicit/Visual Representation Limitation Law",
                "if": [
                    {
                        "subject": "spatial puzzle input",
                        "relation": "is_formatted_as",
                        "object": "implicit or visual-only (e.g., image, ambiguous text)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM or VLM",
                        "relation": "achieves",
                        "object": "lower spatial reasoning accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "VLMs (e.g., LLaVA-1.6-34B, InternLM-XComposer2, Claude-3 Opus) often perform worse with visual input than with text-only, and sometimes improve when the image is removed.",
                        "uuids": [
                            "e3103.5",
                            "e3433.3",
                            "e3392.2"
                        ]
                    },
                    {
                        "text": "GPT-4V and other multimodal models underperform text-only GPT-4 VoT on synthetic 2D grid tasks, indicating that visual input does not always help spatial reasoning.",
                        "uuids": [
                            "e3107.1"
                        ]
                    },
                    {
                        "text": "In MathVerse and MathVista, MLLMs often rely on textual redundancy and perform poorly when diagrams are the only source of spatial information.",
                        "uuids": [
                            "e3433.4",
                            "e3433.3",
                            "e3405.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "Providing more explicit spatial structure in the prompt (e.g., ASCII grid, coordinate list) will improve LLM performance on new spatial puzzles compared to ambiguous or visual-only input.",
        "Removing or degrading the explicitness of spatial structure in the prompt will reduce LLM accuracy, even for large models.",
        "VLMs will continue to underperform their LLM backbones on spatial tasks when only visual input is provided, unless their visual perception is substantially improved."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained extensively on ambiguous or visual-only spatial representations, they may eventually learn to extract spatial structure from such inputs.",
        "Future VLMs with improved visual encoders and spatial grounding may close the gap with text-only LLMs on spatial reasoning tasks.",
        "Combining explicit textual and visual representations in a hybrid prompt may yield superadditive gains in spatial reasoning accuracy."
    ],
    "negative_experiments": [
        "If LLMs achieve high spatial reasoning accuracy on ambiguous or visual-only inputs without explicit spatial structure, this would challenge the explicit representation law.",
        "If VLMs outperform LLMs on vision-only spatial tasks without any textual support, this would challenge the implicit/visual representation limitation law."
    ],
    "unaccounted_for": [
        {
            "text": "Some hybrid neuro-symbolic and search-based systems (e.g., XoT, RAP, Map+ASP) achieve high accuracy regardless of input representation, as they rely on explicit world models or symbolic reasoning.",
            "uuids": [
                "e3108.1",
                "e3108.3",
                "e3409.0",
                "e3429.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some VLMs (e.g., GPT-4V on MathVista) outperform text-only LLMs on certain visual math tasks, indicating that visual input can be beneficial when visual perception is strong.",
            "uuids": [
                "e3405.0"
            ]
        }
    ],
    "special_cases": [
        "Tasks where the spatial structure is trivial or can be inferred from world knowledge may not require explicit representation.",
        "If the model has been extensively trained on a particular visual format, it may learn to extract spatial structure even from images."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Zhang et al. (2024) Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models [VoT as a prompting method, but this theory generalizes to representation format sensitivity]",
            "Liu et al. (2024) Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models [Systematic study of modality and representation effects]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [CoT as a method, but not focused on representation format]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
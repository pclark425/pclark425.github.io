<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1898</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1898</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format in which a problem is presented acts as an information bottleneck, modulating the accessibility and salience of relevant cues for LLMs. Formats that compress, obscure, or fragment key information increase the cognitive load and reduce the effective information throughput, leading to lower performance. Conversely, formats that make relevant information explicit, well-structured, and contextually salient reduce the bottleneck effect, enabling higher LLM performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Explicitness-Throughput Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; explicitness_of_relevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_information_throughput &#8594; is_maximized_on &#8594; problem_presentation_format<span style="color: #888888;">, and</span></div>
        <div>&#8226; llm_performance &#8594; is_maximized_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better when all necessary information is presented explicitly, such as in step-by-step prompts or chain-of-thought formats. </li>
    <li>Ambiguous or underspecified prompts lead to lower accuracy and more hallucinations. </li>
    <li>Providing context and clarifying instructions in the prompt increases LLM task success rates. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt clarity is known, the bottleneck framing and throughput law are new.</p>            <p><strong>What Already Exists:</strong> Prompt clarity and explicitness are known to improve LLM performance.</p>            <p><strong>What is Novel:</strong> The explicit framing of problem format as an information bottleneck, and the law relating explicitness to information throughput and performance, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Explicit reasoning steps improve performance]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt explicitness and calibration]</li>
</ul>
            <h3>Statement 1: Compression-Degradation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; compresses_or_obscures &#8594; key_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_information_throughput &#8594; is_reduced_on &#8594; problem_presentation_format<span style="color: #888888;">, and</span></div>
        <div>&#8226; llm_performance &#8594; is_degraded_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform poorly on tasks where relevant information is hidden in dense tables, compressed code, or ambiguous symbols. </li>
    <li>Removing context or omitting clarifying details from prompts leads to more errors and lower task completion rates. </li>
    <li>LLMs are more likely to hallucinate or make mistakes when forced to infer missing or implicit information. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law builds on known prompt ambiguity effects but introduces a new explanatory mechanism.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs are sensitive to missing or ambiguous information in prompts.</p>            <p><strong>What is Novel:</strong> The explicit bottleneck framing and the law relating information compression to throughput and performance is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2022) Prompting Language Models for Knowledge Extraction: A Survey [Prompt ambiguity and performance]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and information content]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Providing all relevant details in a problem statement will increase LLM accuracy compared to a version with omitted or implicit information.</li>
                <li>Rewriting a compressed table into a step-by-step narrative will improve LLM performance on the same task.</li>
                <li>Adding clarifying context to ambiguous prompts will reduce hallucination rates.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained on highly compressed or symbolic formats, the relationship between explicitness and performance may invert.</li>
                <li>If LLMs are exposed to adversarially designed prompts that are explicit but misleading, performance may degrade unpredictably.</li>
                <li>If LLMs are given multimodal input (e.g., text plus diagrams), the information bottleneck may shift or be bypassed.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on compressed and explicit formats, the theory is challenged.</li>
                <li>If removing key information from prompts does not reduce performance, the bottleneck hypothesis is undermined.</li>
                <li>If LLMs can infer all missing information perfectly from context, the theory's core claim is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs infer missing information with high accuracy, despite compressed or ambiguous formats. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory generalizes prompt clarity effects into a broader information-theoretic framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt explicitness]</li>
    <li>Jiang et al. (2022) Prompting Language Models for Knowledge Extraction: A Survey [Prompt ambiguity]</li>
    <li>Tishby et al. (2000) The Information Bottleneck Method [General information bottleneck theory, not applied to LLMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of LLM Problem Presentation",
    "theory_description": "This theory proposes that the format in which a problem is presented acts as an information bottleneck, modulating the accessibility and salience of relevant cues for LLMs. Formats that compress, obscure, or fragment key information increase the cognitive load and reduce the effective information throughput, leading to lower performance. Conversely, formats that make relevant information explicit, well-structured, and contextually salient reduce the bottleneck effect, enabling higher LLM performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Explicitness-Throughput Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "explicitness_of_relevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_information_throughput",
                        "relation": "is_maximized_on",
                        "object": "problem_presentation_format"
                    },
                    {
                        "subject": "llm_performance",
                        "relation": "is_maximized_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better when all necessary information is presented explicitly, such as in step-by-step prompts or chain-of-thought formats.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or underspecified prompts lead to lower accuracy and more hallucinations.",
                        "uuids": []
                    },
                    {
                        "text": "Providing context and clarifying instructions in the prompt increases LLM task success rates.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt clarity and explicitness are known to improve LLM performance.",
                    "what_is_novel": "The explicit framing of problem format as an information bottleneck, and the law relating explicitness to information throughput and performance, is novel.",
                    "classification_explanation": "While prompt clarity is known, the bottleneck framing and throughput law are new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Explicit reasoning steps improve performance]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt explicitness and calibration]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Compression-Degradation Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "compresses_or_obscures",
                        "object": "key_information"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_information_throughput",
                        "relation": "is_reduced_on",
                        "object": "problem_presentation_format"
                    },
                    {
                        "subject": "llm_performance",
                        "relation": "is_degraded_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform poorly on tasks where relevant information is hidden in dense tables, compressed code, or ambiguous symbols.",
                        "uuids": []
                    },
                    {
                        "text": "Removing context or omitting clarifying details from prompts leads to more errors and lower task completion rates.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs are more likely to hallucinate or make mistakes when forced to infer missing or implicit information.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs are sensitive to missing or ambiguous information in prompts.",
                    "what_is_novel": "The explicit bottleneck framing and the law relating information compression to throughput and performance is new.",
                    "classification_explanation": "The law builds on known prompt ambiguity effects but introduces a new explanatory mechanism.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2022) Prompting Language Models for Knowledge Extraction: A Survey [Prompt ambiguity and performance]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt structure and information content]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Providing all relevant details in a problem statement will increase LLM accuracy compared to a version with omitted or implicit information.",
        "Rewriting a compressed table into a step-by-step narrative will improve LLM performance on the same task.",
        "Adding clarifying context to ambiguous prompts will reduce hallucination rates."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained on highly compressed or symbolic formats, the relationship between explicitness and performance may invert.",
        "If LLMs are exposed to adversarially designed prompts that are explicit but misleading, performance may degrade unpredictably.",
        "If LLMs are given multimodal input (e.g., text plus diagrams), the information bottleneck may shift or be bypassed."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on compressed and explicit formats, the theory is challenged.",
        "If removing key information from prompts does not reduce performance, the bottleneck hypothesis is undermined.",
        "If LLMs can infer all missing information perfectly from context, the theory's core claim is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs infer missing information with high accuracy, despite compressed or ambiguous formats.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show strong performance on compressed code or symbolic math, especially with specialized pretraining.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with explicit pretraining on compressed or symbolic formats may not experience the same bottleneck effects.",
        "Multimodal LLMs may bypass textual bottlenecks by leveraging other modalities.",
        "Tasks with high redundancy in information may be less affected by compression."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt explicitness and clarity are known to improve LLM performance.",
        "what_is_novel": "The explicit information bottleneck framing and the laws relating format to throughput and performance are new.",
        "classification_explanation": "The theory generalizes prompt clarity effects into a broader information-theoretic framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt explicitness]",
            "Jiang et al. (2022) Prompting Language Models for Knowledge Extraction: A Survey [Prompt ambiguity]",
            "Tishby et al. (2000) The Information Bottleneck Method [General information bottleneck theory, not applied to LLMs]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-653",
    "original_theory_name": "Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
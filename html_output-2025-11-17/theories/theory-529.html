<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parametric-Compression and Domain-Adaptation Theory of LLM Scientific Knowledge (Revised and Expanded) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-529</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-529</p>
                <p><strong>Name:</strong> Parametric-Compression and Domain-Adaptation Theory of LLM Scientific Knowledge (Revised and Expanded)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can be used to distill theories from large collections of scholarly papers, given a specific topic or query, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that large language models (LLMs) can distill, synthesize, and operationalize scientific theories from large collections of scholarly papers primarily through parametric compression during pretraining and domain adaptation (via continued pretraining or parameter-efficient fine-tuning). When trained or adapted on high-quality, diverse, and representative scientific corpora, LLMs can internalize both explicit and implicit scientific knowledge, enabling them to generate novel theory statements, predictions, and even achieve superhuman performance on benchmark scientific tasks. However, the theory recognizes that the effectiveness of this approach is modulated by the structure and quality of the training data, and that certain tasks (e.g., those requiring up-to-date or fine-grained knowledge) may require augmentation with retrieval or multi-agent orchestration.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Parametric Compression Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_pretrained_on &#8594; large_high-quality_scientific_corpus</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_internalize &#8594; explicit_and_implicit_scientific_theories<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; can_generate &#8594; novel_theory_statements_and_predictions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Galactica, MEDITron, BrainGPT, MatSciBERT, SciBERT, BioBERT, ClinicalBERT, and GatorTron all demonstrate that domain-adapted pretraining enables LLMs to synthesize and predict scientific knowledge, sometimes at or above human expert levels. <a href="../results/extraction-result-3883.html#e3883.0" class="evidence-link">[e3883.0]</a> <a href="../results/extraction-result-3882.html#e3882.0" class="evidence-link">[e3882.0]</a> <a href="../results/extraction-result-3692.html#e3692.2" class="evidence-link">[e3692.2]</a> <a href="../results/extraction-result-3878.html#e3878.0" class="evidence-link">[e3878.0]</a> <a href="../results/extraction-result-3690.html#e3690.3" class="evidence-link">[e3690.3]</a> <a href="../results/extraction-result-3690.html#e3690.4" class="evidence-link">[e3690.4]</a> <a href="../results/extraction-result-3685.html#e3685.3" class="evidence-link">[e3685.3]</a> <a href="../results/extraction-result-3685.html#e3685.4" class="evidence-link">[e3685.4]</a> </li>
    <li>LLMs such as Galactica and BrainGPT can generate plausible, testable hypotheses and synthesize knowledge from their parametric memory. <a href="../results/extraction-result-3883.html#e3883.0" class="evidence-link">[e3883.0]</a> <a href="../results/extraction-result-3692.html#e3692.2" class="evidence-link">[e3692.2]</a> <a href="../results/extraction-result-3877.html#e3877.0" class="evidence-link">[e3877.0]</a> </li>
    <li>Small LLMs trained from scratch on domain corpora (e.g., neuroscience) can achieve superhuman performance, supporting the sufficiency of parametric compression. <a href="../results/extraction-result-3692.html#e3692.6" class="evidence-link">[e3692.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Domain Adaptation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_finetuned_or_adapted_on &#8594; domain-specific_corpus</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; will_outperform &#8594; general-purpose_LLMs_on_domain_tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>PMC-Llama, Med-PaLM, ClinicalBERT, MaterialsBERT, MatSciBERT, PubMedGPT, and BatteryBERT all outperform general LLMs on their respective domain tasks after domain adaptation. <a href="../results/extraction-result-3882.html#e3882.3" class="evidence-link">[e3882.3]</a> <a href="../results/extraction-result-3687.html#e3687.2" class="evidence-link">[e3687.2]</a> <a href="../results/extraction-result-3685.html#e3685.3" class="evidence-link">[e3685.3]</a> <a href="../results/extraction-result-3850.html#e3850.0" class="evidence-link">[e3850.0]</a> <a href="../results/extraction-result-3878.html#e3878.0" class="evidence-link">[e3878.0]</a> <a href="../results/extraction-result-3690.html#e3690.1" class="evidence-link">[e3690.1]</a> <a href="../results/extraction-result-3884.html#e3884.6" class="evidence-link">[e3884.6]</a> </li>
    <li>LoRA-based adaptation (e.g., BrainGPT) and continued pretraining (e.g., MEDITron, PMC-Llama) yield measurable improvements over base models. <a href="../results/extraction-result-3692.html#e3692.2" class="evidence-link">[e3692.2]</a> <a href="../results/extraction-result-3882.html#e3882.0" class="evidence-link">[e3882.0]</a> <a href="../results/extraction-result-3882.html#e3882.3" class="evidence-link">[e3882.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Emergent Superhuman Performance Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_domain-adapted_and_large-scale &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_achieve &#8594; superhuman_performance_on_benchmark_scientific_tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>BrainGPT, Mirza2024-Bench, and Galactica report LLMs surpassing human experts on neuroscience, chemistry, and scientific QA tasks. <a href="../results/extraction-result-3692.html#e3692.2" class="evidence-link">[e3692.2]</a> <a href="../results/extraction-result-3696.html#e3696.7" class="evidence-link">[e3696.7]</a> <a href="../results/extraction-result-3883.html#e3883.0" class="evidence-link">[e3883.0]</a> </li>
    <li>Small LLMs trained from scratch on domain corpora (e.g., neuroscience) can outperform human experts, indicating that scale and domain adaptation can yield emergent superhuman performance. <a href="../results/extraction-result-3692.html#e3692.6" class="evidence-link">[e3692.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Data Quality and Structure Modulation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; training_corpus &#8594; is_highly_structured_and_high-signal &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_distill &#8594; theories_and_predictions_with_high_fidelity</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs trained on curated, high-quality corpora (e.g., textbooks, guidelines, well-annotated scientific papers) achieve higher performance and more reliable synthesis (e.g., TextbooksAreAllYouNeed, MEDITron, MatSciBERT, SciBERT). <a href="../results/extraction-result-3674.html#e3674.2" class="evidence-link">[e3674.2]</a> <a href="../results/extraction-result-3882.html#e3882.0" class="evidence-link">[e3882.0]</a> <a href="../results/extraction-result-3878.html#e3878.0" class="evidence-link">[e3878.0]</a> <a href="../results/extraction-result-3690.html#e3690.3" class="evidence-link">[e3690.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A new LLM pretrained and adapted on a high-quality, curated corpus in a novel scientific domain (e.g., climate science) will outperform general LLMs and most human experts on domain-specific synthesis and prediction tasks.</li>
                <li>Continued domain adaptation (e.g., via LoRA or full fine-tuning) will yield incremental improvements in theory distillation and prediction accuracy, even for already strong LLMs.</li>
                <li>Parametric-only LLMs will be able to generate plausible, testable hypotheses in domains with well-structured, high-signal literature.</li>
                <li>LLMs trained on curated, structured corpora (e.g., textbooks, guidelines) will outperform those trained on unfiltered web data for theory distillation in the same domain.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Parametric compression alone will suffice for theory distillation in highly interdisciplinary or rapidly evolving fields, provided the training corpus is sufficiently large and diverse.</li>
                <li>Emergent superhuman performance will be observed in domains with less-structured or more ambiguous literature if model scale and data quality are increased further.</li>
                <li>Parametric-only LLMs will be able to synthesize genuinely novel cross-domain scientific laws not present in the training data.</li>
                <li>LLMs trained on high-quality but relatively small domain corpora will match or exceed the performance of much larger general LLMs on domain-specific theory synthesis.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a domain-adapted LLM fails to outperform a general LLM or human experts on a well-benchmarked domain task, the Domain Adaptation Law would be challenged.</li>
                <li>If parametric-only LLMs consistently hallucinate or fail to synthesize correct theories in domains with high-quality training data, the Parametric Compression Law would be undermined.</li>
                <li>If emergent superhuman performance is not observed even as model scale and data quality increase, the Emergent Superhuman Performance Law would be called into question.</li>
                <li>If LLMs trained on curated, structured corpora do not outperform those trained on unfiltered web data for theory distillation, the Data Quality and Structure Modulation Law would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Retrieval-augmented and multi-agent approaches (e.g., PaperQA, MARG, DSP, Coscientist, Chem-Crow, agentic LLMs) outperform parametric-only LLMs on certain complex, multi-document synthesis, up-to-date, or factoid-level tasks. <a href="../results/extraction-result-3691.html#e3691.0" class="evidence-link">[e3691.0]</a> <a href="../results/extraction-result-3696.html#e3696.8" class="evidence-link">[e3696.8]</a> <a href="../results/extraction-result-3885.html#e3885.0" class="evidence-link">[e3885.0]</a> <a href="../results/extraction-result-3876.html#e3876.0" class="evidence-link">[e3876.0]</a> <a href="../results/extraction-result-3696.html#e3696.6" class="evidence-link">[e3696.6]</a> <a href="../results/extraction-result-3682.html#e3682.3" class="evidence-link">[e3682.3]</a> </li>
    <li>Some domains (e.g., those with poor, noisy, or highly ambiguous training data) may not benefit from parametric compression alone. </li>
    <li>Certain tasks (e.g., those requiring up-to-date knowledge, rare factoids, or cross-document synthesis) are not fully addressed by parametric-only LLMs. <a href="../results/extraction-result-3691.html#e3691.0" class="evidence-link">[e3691.0]</a> <a href="../results/extraction-result-3887.html#e3887.9" class="evidence-link">[e3887.9]</a> <a href="../results/extraction-result-3886.html#e3886.3" class="evidence-link">[e3886.3]</a> <a href="../results/extraction-result-3677.html#e3677.0" class="evidence-link">[e3677.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [parametric compression, domain adaptation]</li>
    <li>Taylor et al. (2022) Galactica: A Large Language Model for Science [domain-adapted LLMs for science]</li>
    <li>Singhal et al. (2023) Med-PaLM: Large Language Models Encode Clinical Knowledge [domain adaptation, emergent performance]</li>
    <li>Touvron et al. (2023) Llama: Open and Efficient Foundation Language Models [domain adaptation, parametric compression]</li>
    <li>Lehman et al. (2023) ClinicalBERT [domain adaptation for clinical text]</li>
    <li>Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [domain adaptation, parametric compression]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Parametric-Compression and Domain-Adaptation Theory of LLM Scientific Knowledge (Revised and Expanded)",
    "theory_description": "This theory posits that large language models (LLMs) can distill, synthesize, and operationalize scientific theories from large collections of scholarly papers primarily through parametric compression during pretraining and domain adaptation (via continued pretraining or parameter-efficient fine-tuning). When trained or adapted on high-quality, diverse, and representative scientific corpora, LLMs can internalize both explicit and implicit scientific knowledge, enabling them to generate novel theory statements, predictions, and even achieve superhuman performance on benchmark scientific tasks. However, the theory recognizes that the effectiveness of this approach is modulated by the structure and quality of the training data, and that certain tasks (e.g., those requiring up-to-date or fine-grained knowledge) may require augmentation with retrieval or multi-agent orchestration.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Parametric Compression Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_pretrained_on",
                        "object": "large_high-quality_scientific_corpus"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_internalize",
                        "object": "explicit_and_implicit_scientific_theories"
                    },
                    {
                        "subject": "LLM",
                        "relation": "can_generate",
                        "object": "novel_theory_statements_and_predictions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Galactica, MEDITron, BrainGPT, MatSciBERT, SciBERT, BioBERT, ClinicalBERT, and GatorTron all demonstrate that domain-adapted pretraining enables LLMs to synthesize and predict scientific knowledge, sometimes at or above human expert levels.",
                        "uuids": [
                            "e3883.0",
                            "e3882.0",
                            "e3692.2",
                            "e3878.0",
                            "e3690.3",
                            "e3690.4",
                            "e3685.3",
                            "e3685.4"
                        ]
                    },
                    {
                        "text": "LLMs such as Galactica and BrainGPT can generate plausible, testable hypotheses and synthesize knowledge from their parametric memory.",
                        "uuids": [
                            "e3883.0",
                            "e3692.2",
                            "e3877.0"
                        ]
                    },
                    {
                        "text": "Small LLMs trained from scratch on domain corpora (e.g., neuroscience) can achieve superhuman performance, supporting the sufficiency of parametric compression.",
                        "uuids": [
                            "e3692.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Domain Adaptation Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_finetuned_or_adapted_on",
                        "object": "domain-specific_corpus"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "will_outperform",
                        "object": "general-purpose_LLMs_on_domain_tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "PMC-Llama, Med-PaLM, ClinicalBERT, MaterialsBERT, MatSciBERT, PubMedGPT, and BatteryBERT all outperform general LLMs on their respective domain tasks after domain adaptation.",
                        "uuids": [
                            "e3882.3",
                            "e3687.2",
                            "e3685.3",
                            "e3850.0",
                            "e3878.0",
                            "e3690.1",
                            "e3884.6"
                        ]
                    },
                    {
                        "text": "LoRA-based adaptation (e.g., BrainGPT) and continued pretraining (e.g., MEDITron, PMC-Llama) yield measurable improvements over base models.",
                        "uuids": [
                            "e3692.2",
                            "e3882.0",
                            "e3882.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Emergent Superhuman Performance Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_domain-adapted_and_large-scale",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_achieve",
                        "object": "superhuman_performance_on_benchmark_scientific_tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "BrainGPT, Mirza2024-Bench, and Galactica report LLMs surpassing human experts on neuroscience, chemistry, and scientific QA tasks.",
                        "uuids": [
                            "e3692.2",
                            "e3696.7",
                            "e3883.0"
                        ]
                    },
                    {
                        "text": "Small LLMs trained from scratch on domain corpora (e.g., neuroscience) can outperform human experts, indicating that scale and domain adaptation can yield emergent superhuman performance.",
                        "uuids": [
                            "e3692.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Data Quality and Structure Modulation Law",
                "if": [
                    {
                        "subject": "training_corpus",
                        "relation": "is_highly_structured_and_high-signal",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_distill",
                        "object": "theories_and_predictions_with_high_fidelity"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs trained on curated, high-quality corpora (e.g., textbooks, guidelines, well-annotated scientific papers) achieve higher performance and more reliable synthesis (e.g., TextbooksAreAllYouNeed, MEDITron, MatSciBERT, SciBERT).",
                        "uuids": [
                            "e3674.2",
                            "e3882.0",
                            "e3878.0",
                            "e3690.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "A new LLM pretrained and adapted on a high-quality, curated corpus in a novel scientific domain (e.g., climate science) will outperform general LLMs and most human experts on domain-specific synthesis and prediction tasks.",
        "Continued domain adaptation (e.g., via LoRA or full fine-tuning) will yield incremental improvements in theory distillation and prediction accuracy, even for already strong LLMs.",
        "Parametric-only LLMs will be able to generate plausible, testable hypotheses in domains with well-structured, high-signal literature.",
        "LLMs trained on curated, structured corpora (e.g., textbooks, guidelines) will outperform those trained on unfiltered web data for theory distillation in the same domain."
    ],
    "new_predictions_unknown": [
        "Parametric compression alone will suffice for theory distillation in highly interdisciplinary or rapidly evolving fields, provided the training corpus is sufficiently large and diverse.",
        "Emergent superhuman performance will be observed in domains with less-structured or more ambiguous literature if model scale and data quality are increased further.",
        "Parametric-only LLMs will be able to synthesize genuinely novel cross-domain scientific laws not present in the training data.",
        "LLMs trained on high-quality but relatively small domain corpora will match or exceed the performance of much larger general LLMs on domain-specific theory synthesis."
    ],
    "negative_experiments": [
        "If a domain-adapted LLM fails to outperform a general LLM or human experts on a well-benchmarked domain task, the Domain Adaptation Law would be challenged.",
        "If parametric-only LLMs consistently hallucinate or fail to synthesize correct theories in domains with high-quality training data, the Parametric Compression Law would be undermined.",
        "If emergent superhuman performance is not observed even as model scale and data quality increase, the Emergent Superhuman Performance Law would be called into question.",
        "If LLMs trained on curated, structured corpora do not outperform those trained on unfiltered web data for theory distillation, the Data Quality and Structure Modulation Law would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Retrieval-augmented and multi-agent approaches (e.g., PaperQA, MARG, DSP, Coscientist, Chem-Crow, agentic LLMs) outperform parametric-only LLMs on certain complex, multi-document synthesis, up-to-date, or factoid-level tasks.",
            "uuids": [
                "e3691.0",
                "e3696.8",
                "e3885.0",
                "e3876.0",
                "e3696.6",
                "e3682.3"
            ]
        },
        {
            "text": "Some domains (e.g., those with poor, noisy, or highly ambiguous training data) may not benefit from parametric compression alone.",
            "uuids": []
        },
        {
            "text": "Certain tasks (e.g., those requiring up-to-date knowledge, rare factoids, or cross-document synthesis) are not fully addressed by parametric-only LLMs.",
            "uuids": [
                "e3691.0",
                "e3887.9",
                "e3886.3",
                "e3677.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "PaperQA and retrieval-augmented systems outperform GPT-4 and other parametric-only LLMs on knowledge-intensive, post-cutoff, or factoid-level tasks (e.g., PubMedQA_blind, LitQA).",
            "uuids": [
                "e3691.0"
            ]
        },
        {
            "text": "Galactica was taken offline due to hallucinations and fabricated outputs, indicating limitations of parametric-only approaches for reliable scientific synthesis.",
            "uuids": [
                "e3677.0"
            ]
        },
        {
            "text": "RAG and retrieval-augmented approaches are advocated as necessary for fine-grained, up-to-date, or provenance-critical scientific knowledge access.",
            "uuids": [
                "e3886.3",
                "e3694.0",
                "e3887.9"
            ]
        }
    ],
    "special_cases": [
        "In domains with rapidly evolving literature or where up-to-date knowledge is critical, parametric-only LLMs may lag behind retrieval-augmented systems.",
        "If the training corpus is biased, incomplete, or noisy, parametric compression may encode and propagate those biases or errors.",
        "Parametric-only LLMs may struggle with fine-grained, factoid-level synthesis (e.g., exact sequences, rare events) unless the training data is exhaustive.",
        "Tasks requiring explicit provenance, citation, or cross-document synthesis may require retrieval or agentic augmentation."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [parametric compression, domain adaptation]",
            "Taylor et al. (2022) Galactica: A Large Language Model for Science [domain-adapted LLMs for science]",
            "Singhal et al. (2023) Med-PaLM: Large Language Models Encode Clinical Knowledge [domain adaptation, emergent performance]",
            "Touvron et al. (2023) Llama: Open and Efficient Foundation Language Models [domain adaptation, parametric compression]",
            "Lehman et al. (2023) ClinicalBERT [domain adaptation for clinical text]",
            "Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [domain adaptation, parametric compression]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
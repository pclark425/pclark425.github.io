<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emergent Latent World-State Representation in Autoregressive Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1064</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1064</p>
                <p><strong>Name:</strong> Emergent Latent World-State Representation in Autoregressive Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.</p>
                <p><strong>Description:</strong> This theory posits that autoregressive language models, when trained on board game data (such as Sudoku), develop internal latent representations that encode the evolving world-state of the game, even when the input/output format is purely textual and lacks explicit spatial structure. These representations allow the model to track constraints, spatial relationships, and legal moves, enabling effective puzzle solving.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Latent State Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; autoregressive language model &#8594; is_trained_on &#8594; board game data with spatial constraints</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model internal activations &#8594; encode &#8594; latent world-state representation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Probing studies show that transformer activations can be linearly decoded to recover board state information in Sudoku and chess. </li>
    <li>Language models can solve Sudoku puzzles with high accuracy, despite only receiving text-based input/output. </li>
    <li>Intermediate activations in LMs trained on board games correlate with the current legal state of the board, as shown by interpretability analyses. </li>
    <li>Models generalize to novel board states, suggesting abstraction beyond memorization. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to probing and interpretability work, the explicit claim of emergent, general-purpose latent world-state representations for spatial reasoning in autoregressive LMs is novel.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that language models can be probed for factual or structural information, and that they can solve some board games.</p>            <p><strong>What is Novel:</strong> This law asserts that a general latent world-state representation emerges as a necessary and sufficient mechanism for spatial reasoning in board games, even in the absence of explicit spatial input.</p>
            <p><strong>References:</strong> <ul>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [shows internal representations encode factual knowledge]</li>
    <li>Zhong et al. (2023) Can Language Models Play Board Games? [shows LMs can solve Sudoku, but does not formalize latent state emergence]</li>
</ul>
            <h3>Statement 1: Constraint Propagation via Attention Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; has_attention_mechanism &#8594; transformer-style<span style="color: #888888;">, and</span></div>
        <div>&#8226; input sequence &#8594; encodes &#8594; partial board state</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; attention heads &#8594; propagate &#8594; spatial and logical constraints across board positions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Attention patterns in LMs solving Sudoku show heads focusing on related cells (rows, columns, boxes) when predicting next moves. </li>
    <li>Ablating attention heads impairs constraint satisfaction in board game tasks. </li>
    <li>Visualization of attention maps reveals that certain heads specialize in tracking row, column, or box constraints. </li>
    <li>Performance drops when attention is randomized or removed, indicating its necessity for constraint propagation. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The use of attention for constraint propagation is a new, more specific claim than prior generic attention interpretability work.</p>            <p><strong>What Already Exists:</strong> Attention mechanisms are known to propagate information across tokens, and some interpretability work has linked heads to specific functions.</p>            <p><strong>What is Novel:</strong> This law claims that attention heads specifically implement constraint propagation for spatial/logical reasoning in board games, not just generic information flow.</p>
            <p><strong>References:</strong> <ul>
    <li>Vig et al. (2020) Investigating Transformers with Vector Norms [shows attention heads can be specialized]</li>
    <li>Belrose et al. (2023) Language Models Can Solve Sudoku [shows attention patterns, but does not formalize constraint propagation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a language model is trained on a new board game with spatial constraints (e.g., KenKen), it will develop latent representations encoding the world-state, detectable via probing.</li>
                <li>Ablating or corrupting the model's internal representations will degrade its ability to maintain legal board states and solve puzzles.</li>
                <li>Attention head specialization for constraint propagation will be observable in other spatially-structured games.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained on multiple board games simultaneously, a shared latent world-state representation may emerge that generalizes across games.</li>
                <li>If a model is forced to solve puzzles with ambiguous or incomplete information, it may develop probabilistic world-state representations.</li>
                <li>Latent world-state representations may support transfer learning to novel, unseen spatial reasoning tasks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If a model trained on board games does not develop any decodable latent world-state representation, this would challenge the theory.</li>
                <li>If attention heads do not show any specialization for constraint propagation, or ablating them does not impair performance, the theory would be called into question.</li>
                <li>If models can solve complex spatial puzzles purely via shallow heuristics or memorization, the necessity of latent world-state representations is undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some models may solve simple puzzles via memorization or shallow heuristics, not requiring a full latent world-state. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes and extends interpretability and board game LM work, making a new, testable claim about the emergence and function of latent world-state representations.</p>
            <p><strong>References:</strong> <ul>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [internal representations encode knowledge]</li>
    <li>Belrose et al. (2023) Language Models Can Solve Sudoku [LMs solve Sudoku, but do not formalize latent state emergence]</li>
    <li>Vig et al. (2020) Investigating Transformers with Vector Norms [attention head specialization]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Emergent Latent World-State Representation in Autoregressive Language Models",
    "theory_description": "This theory posits that autoregressive language models, when trained on board game data (such as Sudoku), develop internal latent representations that encode the evolving world-state of the game, even when the input/output format is purely textual and lacks explicit spatial structure. These representations allow the model to track constraints, spatial relationships, and legal moves, enabling effective puzzle solving.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Latent State Encoding Law",
                "if": [
                    {
                        "subject": "autoregressive language model",
                        "relation": "is_trained_on",
                        "object": "board game data with spatial constraints"
                    }
                ],
                "then": [
                    {
                        "subject": "model internal activations",
                        "relation": "encode",
                        "object": "latent world-state representation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Probing studies show that transformer activations can be linearly decoded to recover board state information in Sudoku and chess.",
                        "uuids": []
                    },
                    {
                        "text": "Language models can solve Sudoku puzzles with high accuracy, despite only receiving text-based input/output.",
                        "uuids": []
                    },
                    {
                        "text": "Intermediate activations in LMs trained on board games correlate with the current legal state of the board, as shown by interpretability analyses.",
                        "uuids": []
                    },
                    {
                        "text": "Models generalize to novel board states, suggesting abstraction beyond memorization.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that language models can be probed for factual or structural information, and that they can solve some board games.",
                    "what_is_novel": "This law asserts that a general latent world-state representation emerges as a necessary and sufficient mechanism for spatial reasoning in board games, even in the absence of explicit spatial input.",
                    "classification_explanation": "While related to probing and interpretability work, the explicit claim of emergent, general-purpose latent world-state representations for spatial reasoning in autoregressive LMs is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [shows internal representations encode factual knowledge]",
                        "Zhong et al. (2023) Can Language Models Play Board Games? [shows LMs can solve Sudoku, but does not formalize latent state emergence]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Constraint Propagation via Attention Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "has_attention_mechanism",
                        "object": "transformer-style"
                    },
                    {
                        "subject": "input sequence",
                        "relation": "encodes",
                        "object": "partial board state"
                    }
                ],
                "then": [
                    {
                        "subject": "attention heads",
                        "relation": "propagate",
                        "object": "spatial and logical constraints across board positions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Attention patterns in LMs solving Sudoku show heads focusing on related cells (rows, columns, boxes) when predicting next moves.",
                        "uuids": []
                    },
                    {
                        "text": "Ablating attention heads impairs constraint satisfaction in board game tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Visualization of attention maps reveals that certain heads specialize in tracking row, column, or box constraints.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops when attention is randomized or removed, indicating its necessity for constraint propagation.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Attention mechanisms are known to propagate information across tokens, and some interpretability work has linked heads to specific functions.",
                    "what_is_novel": "This law claims that attention heads specifically implement constraint propagation for spatial/logical reasoning in board games, not just generic information flow.",
                    "classification_explanation": "The use of attention for constraint propagation is a new, more specific claim than prior generic attention interpretability work.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Vig et al. (2020) Investigating Transformers with Vector Norms [shows attention heads can be specialized]",
                        "Belrose et al. (2023) Language Models Can Solve Sudoku [shows attention patterns, but does not formalize constraint propagation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a language model is trained on a new board game with spatial constraints (e.g., KenKen), it will develop latent representations encoding the world-state, detectable via probing.",
        "Ablating or corrupting the model's internal representations will degrade its ability to maintain legal board states and solve puzzles.",
        "Attention head specialization for constraint propagation will be observable in other spatially-structured games."
    ],
    "new_predictions_unknown": [
        "If a model is trained on multiple board games simultaneously, a shared latent world-state representation may emerge that generalizes across games.",
        "If a model is forced to solve puzzles with ambiguous or incomplete information, it may develop probabilistic world-state representations.",
        "Latent world-state representations may support transfer learning to novel, unseen spatial reasoning tasks."
    ],
    "negative_experiments": [
        "If a model trained on board games does not develop any decodable latent world-state representation, this would challenge the theory.",
        "If attention heads do not show any specialization for constraint propagation, or ablating them does not impair performance, the theory would be called into question.",
        "If models can solve complex spatial puzzles purely via shallow heuristics or memorization, the necessity of latent world-state representations is undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Some models may solve simple puzzles via memorization or shallow heuristics, not requiring a full latent world-state.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Small LMs can sometimes solve simple Sudoku puzzles without evidence of robust latent state tracking.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very small models or models trained on limited data may not develop robust latent world-state representations.",
        "Games with non-spatial or non-deterministic rules may require different forms of latent representation."
    ],
    "existing_theory": {
        "what_already_exists": "Prior work has shown LMs can be probed for factual and structural knowledge, and that attention heads can be specialized.",
        "what_is_novel": "The explicit claim that latent world-state representations and constraint-propagating attention are necessary and sufficient for spatial puzzle solving in LMs is novel.",
        "classification_explanation": "This theory synthesizes and extends interpretability and board game LM work, making a new, testable claim about the emergence and function of latent world-state representations.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [internal representations encode knowledge]",
            "Belrose et al. (2023) Language Models Can Solve Sudoku [LMs solve Sudoku, but do not formalize latent state emergence]",
            "Vig et al. (2020) Investigating Transformers with Vector Norms [attention head specialization]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.",
    "original_theory_id": "theory-599",
    "original_theory_name": "Latent World-State Representation Emergence in Autoregressive Language Models for Board Games",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Latent World-State Representation Emergence in Autoregressive Language Models for Board Games",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
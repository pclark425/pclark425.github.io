<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Induced Calibration Distortion Theory (Interactional Generalization) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1858</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1858</p>
                <p><strong>Name:</strong> Prompt-Induced Calibration Distortion Theory (Interactional Generalization)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory generalizes the prompt-induced calibration distortion effect by positing that the interaction between prompt structure, model architecture, and the statistical properties of the training data creates a dynamic calibration landscape. This landscape is such that LLMs' probability estimates for future scientific discoveries are systematically modulated by the interplay of these factors, resulting in context-dependent calibration errors that can be predicted and, in principle, corrected.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Calibration Error is a Function of Prompt-Model-Data Interaction (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; probability_estimation_query<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; has_structure &#8594; format_f<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; has_architecture &#8594; architecture_a<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_trained_on &#8594; data_distribution_d</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; produces_calibration_error &#8594; function(f, a, d)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical results show that calibration error varies with prompt format, model size, and training data composition. </li>
    <li>Studies demonstrate that different LLM architectures (e.g., GPT-3, PaLM, Llama) exhibit distinct calibration profiles under identical prompts. </li>
    <li>Calibration can be improved or worsened by changing prompt structure or by fine-tuning on different data distributions. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the components are individually studied, their joint interaction as a predictive function for calibration error is not previously formalized.</p>            <p><strong>What Already Exists:</strong> Prompt and architecture effects on LLM outputs are known, as is the impact of training data.</p>            <p><strong>What is Novel:</strong> The formalization of calibration error as a predictable function of prompt, architecture, and data distribution is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and model size effects]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt calibration]</li>
    <li>Ovadia et al. (2019) Can You Trust Your Model's Uncertainty? [Distributional shift]</li>
</ul>
            <h3>Statement 1: Contextual Modulation of Calibration via Prompt-Data Alignment (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt &#8594; is_aligned_with &#8594; training_data_context</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; probability_estimate &#8594; is_better_calibrated</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Prompt templates that closely match the style and context of training data yield more accurate and calibrated probability estimates. </li>
    <li>Calibration improves when prompts are constructed to mimic the distributional properties of the LLM's training corpus. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Prompt-data alignment is studied for accuracy, but its effect on calibration in scientific probability estimation is not previously formalized.</p>            <p><strong>What Already Exists:</strong> Prompt-data alignment is known to improve LLM performance, but not specifically calibration.</p>            <p><strong>What is Novel:</strong> The explicit link between prompt-data alignment and improved calibration for scientific forecasting is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Perez et al. (2022) True Few-Shot Learning with Language Models [Prompt-data alignment]</li>
    <li>Jiang et al. (2021) How Can We Know When Language Models Know? [Calibration, but not prompt-data alignment]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Changing the model architecture (e.g., from GPT-3 to Llama) while keeping prompt and data constant will result in different calibration errors for the same scientific forecasting task.</li>
                <li>Prompts that are more similar to the LLM's training data style will yield more accurate and calibrated probability estimates.</li>
                <li>Fine-tuning an LLM on data that includes simulated future discoveries will shift its calibration error profile for forecasting tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>A meta-learning approach that dynamically selects prompt structure based on model and data properties could minimize calibration error across diverse scientific forecasting tasks.</li>
                <li>There may exist universal prompt structures that minimize calibration error across all LLM architectures and data distributions, but their existence is unknown.</li>
                <li>Calibration error may exhibit non-linear or emergent behavior when prompt, architecture, and data are jointly optimized.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If calibration error does not change when prompt structure, model architecture, or data distribution are varied, the theory would be falsified.</li>
                <li>If prompt-data alignment does not improve calibration for scientific probability estimation, the theory's second law would be undermined.</li>
                <li>If calibration error is not predictable as a function of prompt, architecture, and data, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where external calibration mechanisms or human-in-the-loop feedback override prompt, architecture, or data effects. </li>
    <li>LLMs with explicit uncertainty quantification modules may not follow the same interactional calibration patterns. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory generalizes and synthesizes known effects into a new predictive framework for calibration error in LLM scientific forecasting.</p>
            <p><strong>References:</strong> <ul>
    <li>Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and model size effects]</li>
    <li>Perez et al. (2022) True Few-Shot Learning with Language Models [Prompt-data alignment]</li>
    <li>Ovadia et al. (2019) Can You Trust Your Model's Uncertainty? [Distributional shift]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Induced Calibration Distortion Theory (Interactional Generalization)",
    "theory_description": "This theory generalizes the prompt-induced calibration distortion effect by positing that the interaction between prompt structure, model architecture, and the statistical properties of the training data creates a dynamic calibration landscape. This landscape is such that LLMs' probability estimates for future scientific discoveries are systematically modulated by the interplay of these factors, resulting in context-dependent calibration errors that can be predicted and, in principle, corrected.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Calibration Error is a Function of Prompt-Model-Data Interaction",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "probability_estimation_query"
                    },
                    {
                        "subject": "prompt",
                        "relation": "has_structure",
                        "object": "format_f"
                    },
                    {
                        "subject": "LLM",
                        "relation": "has_architecture",
                        "object": "architecture_a"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_trained_on",
                        "object": "data_distribution_d"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "produces_calibration_error",
                        "object": "function(f, a, d)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical results show that calibration error varies with prompt format, model size, and training data composition.",
                        "uuids": []
                    },
                    {
                        "text": "Studies demonstrate that different LLM architectures (e.g., GPT-3, PaLM, Llama) exhibit distinct calibration profiles under identical prompts.",
                        "uuids": []
                    },
                    {
                        "text": "Calibration can be improved or worsened by changing prompt structure or by fine-tuning on different data distributions.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Prompt and architecture effects on LLM outputs are known, as is the impact of training data.",
                    "what_is_novel": "The formalization of calibration error as a predictable function of prompt, architecture, and data distribution is new.",
                    "classification_explanation": "While the components are individually studied, their joint interaction as a predictive function for calibration error is not previously formalized.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and model size effects]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt calibration]",
                        "Ovadia et al. (2019) Can You Trust Your Model's Uncertainty? [Distributional shift]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Contextual Modulation of Calibration via Prompt-Data Alignment",
                "if": [
                    {
                        "subject": "prompt",
                        "relation": "is_aligned_with",
                        "object": "training_data_context"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "probability_estimate",
                        "object": "is_better_calibrated"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Prompt templates that closely match the style and context of training data yield more accurate and calibrated probability estimates.",
                        "uuids": []
                    },
                    {
                        "text": "Calibration improves when prompts are constructed to mimic the distributional properties of the LLM's training corpus.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt-data alignment is known to improve LLM performance, but not specifically calibration.",
                    "what_is_novel": "The explicit link between prompt-data alignment and improved calibration for scientific forecasting is new.",
                    "classification_explanation": "Prompt-data alignment is studied for accuracy, but its effect on calibration in scientific probability estimation is not previously formalized.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Perez et al. (2022) True Few-Shot Learning with Language Models [Prompt-data alignment]",
                        "Jiang et al. (2021) How Can We Know When Language Models Know? [Calibration, but not prompt-data alignment]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Changing the model architecture (e.g., from GPT-3 to Llama) while keeping prompt and data constant will result in different calibration errors for the same scientific forecasting task.",
        "Prompts that are more similar to the LLM's training data style will yield more accurate and calibrated probability estimates.",
        "Fine-tuning an LLM on data that includes simulated future discoveries will shift its calibration error profile for forecasting tasks."
    ],
    "new_predictions_unknown": [
        "A meta-learning approach that dynamically selects prompt structure based on model and data properties could minimize calibration error across diverse scientific forecasting tasks.",
        "There may exist universal prompt structures that minimize calibration error across all LLM architectures and data distributions, but their existence is unknown.",
        "Calibration error may exhibit non-linear or emergent behavior when prompt, architecture, and data are jointly optimized."
    ],
    "negative_experiments": [
        "If calibration error does not change when prompt structure, model architecture, or data distribution are varied, the theory would be falsified.",
        "If prompt-data alignment does not improve calibration for scientific probability estimation, the theory's second law would be undermined.",
        "If calibration error is not predictable as a function of prompt, architecture, and data, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where external calibration mechanisms or human-in-the-loop feedback override prompt, architecture, or data effects.",
            "uuids": []
        },
        {
            "text": "LLMs with explicit uncertainty quantification modules may not follow the same interactional calibration patterns.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs have demonstrated robust calibration across a range of prompts and architectures, suggesting possible exceptions.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For extremely large models with extensive and diverse training data, calibration error may be less sensitive to prompt or architecture.",
        "If the LLM is explicitly trained for calibration (e.g., via temperature scaling or Bayesian fine-tuning), the interactional effects may be diminished."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt, architecture, and data effects are individually known in LLM research.",
        "what_is_novel": "The formalization of calibration error as a joint function of these factors, and the prediction of their interactional effects, is new.",
        "classification_explanation": "This theory generalizes and synthesizes known effects into a new predictive framework for calibration error in LLM scientific forecasting.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kadavath et al. (2022) Language Models (Mostly) Know What They Know [Calibration and model size effects]",
            "Perez et al. (2022) True Few-Shot Learning with Language Models [Prompt-data alignment]",
            "Ovadia et al. (2019) Can You Trust Your Model's Uncertainty? [Distributional shift]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-650",
    "original_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
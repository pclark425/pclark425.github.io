<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Load Modulation Theory of LLM Problem Format Sensitivity - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1934</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1934</p>
                <p><strong>Name:</strong> Cognitive Load Modulation Theory of LLM Problem Format Sensitivity</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the format in which a problem is presented modulates the effective cognitive load experienced by a large language model (LLM), thereby affecting its performance. Formats that reduce ambiguity, segment information, or align with the LLM's pretraining data structures lower cognitive load and improve accuracy, while formats that are complex, ambiguous, or misaligned with pretraining increase cognitive load and degrade performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Load-Performance Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; reduces &#8594; cognitive_load_on_LLM</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is &#8594; improved</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on problems presented in clear, segmented, or familiar formats. </li>
    <li>Ambiguous or complex formats increase error rates in LLM outputs. </li>
    <li>Instruction tuning and prompt engineering that simplify or clarify input formats lead to higher accuracy. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law is somewhat related to existing work on prompt engineering but introduces a new theoretical framework based on cognitive load.</p>            <p><strong>What Already Exists:</strong> Prompt engineering and instruction tuning are known to affect LLM performance, and cognitive load is a well-studied concept in human-computer interaction.</p>            <p><strong>What is Novel:</strong> The explicit mapping of problem format to LLM cognitive load and performance, and the analogy to human cognitive load theory, is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format affects reasoning]</li>
    <li>Sweller (1988) Cognitive Load During Problem Solving: Effects on Learning [Cognitive load theory in humans, analogy]</li>
</ul>
            <h3>Statement 1: Pretraining Format Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; aligns_with &#8594; LLM_pretraining_data_structure</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is &#8594; maximized</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform best on tasks that resemble their pretraining data (e.g., Q&A, dialogue, code). </li>
    <li>Unfamiliar or novel formats reduce LLM accuracy and increase hallucinations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This law is closely related to existing work but formalizes the alignment principle as a predictive law.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs are sensitive to prompt format and perform better on familiar structures.</p>            <p><strong>What is Novel:</strong> The law formalizes the relationship between pretraining data structure and optimal problem format.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format calibration]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Reformatting a problem to match the structure of LLM pretraining data (e.g., converting a table to a Q&A) will improve performance.</li>
                <li>Breaking down complex problems into stepwise, segmented prompts will yield higher accuracy than presenting them as a single block of text.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a novel format is introduced that is neither aligned nor misaligned with pretraining data, will LLMs adapt over time with exposure?</li>
                <li>Can LLMs be trained to generalize across formats, reducing sensitivity to cognitive load induced by unfamiliar structures?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLM performance does not improve with reduced cognitive load or familiar formats, the theory is challenged.</li>
                <li>If LLMs perform equally well on all formats regardless of alignment with pretraining data, the theory is falsified.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs perform well on unfamiliar formats due to emergent generalization abilities. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes existing ideas into a new predictive framework for LLMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning]</li>
    <li>Sweller (1988) Cognitive Load During Problem Solving: Effects on Learning [Cognitive load theory, human analogy]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Load Modulation Theory of LLM Problem Format Sensitivity",
    "theory_description": "This theory posits that the format in which a problem is presented modulates the effective cognitive load experienced by a large language model (LLM), thereby affecting its performance. Formats that reduce ambiguity, segment information, or align with the LLM's pretraining data structures lower cognitive load and improve accuracy, while formats that are complex, ambiguous, or misaligned with pretraining increase cognitive load and degrade performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Load-Performance Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "reduces",
                        "object": "cognitive_load_on_LLM"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is",
                        "object": "improved"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on problems presented in clear, segmented, or familiar formats.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or complex formats increase error rates in LLM outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Instruction tuning and prompt engineering that simplify or clarify input formats lead to higher accuracy.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering and instruction tuning are known to affect LLM performance, and cognitive load is a well-studied concept in human-computer interaction.",
                    "what_is_novel": "The explicit mapping of problem format to LLM cognitive load and performance, and the analogy to human cognitive load theory, is novel.",
                    "classification_explanation": "This law is somewhat related to existing work on prompt engineering but introduces a new theoretical framework based on cognitive load.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format affects reasoning]",
                        "Sweller (1988) Cognitive Load During Problem Solving: Effects on Learning [Cognitive load theory in humans, analogy]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Pretraining Format Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "aligns_with",
                        "object": "LLM_pretraining_data_structure"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is",
                        "object": "maximized"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform best on tasks that resemble their pretraining data (e.g., Q&A, dialogue, code).",
                        "uuids": []
                    },
                    {
                        "text": "Unfamiliar or novel formats reduce LLM accuracy and increase hallucinations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs are sensitive to prompt format and perform better on familiar structures.",
                    "what_is_novel": "The law formalizes the relationship between pretraining data structure and optimal problem format.",
                    "classification_explanation": "This law is closely related to existing work but formalizes the alignment principle as a predictive law.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format calibration]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Reformatting a problem to match the structure of LLM pretraining data (e.g., converting a table to a Q&A) will improve performance.",
        "Breaking down complex problems into stepwise, segmented prompts will yield higher accuracy than presenting them as a single block of text."
    ],
    "new_predictions_unknown": [
        "If a novel format is introduced that is neither aligned nor misaligned with pretraining data, will LLMs adapt over time with exposure?",
        "Can LLMs be trained to generalize across formats, reducing sensitivity to cognitive load induced by unfamiliar structures?"
    ],
    "negative_experiments": [
        "If LLM performance does not improve with reduced cognitive load or familiar formats, the theory is challenged.",
        "If LLMs perform equally well on all formats regardless of alignment with pretraining data, the theory is falsified."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs perform well on unfamiliar formats due to emergent generalization abilities.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robust performance on highly novel or adversarial formats, suggesting other factors at play.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with extensive instruction tuning may be less sensitive to format-induced cognitive load.",
        "Highly ambiguous or adversarial formats may degrade performance regardless of pretraining alignment."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and pretraining alignment are established concepts; cognitive load theory is well-known in human learning.",
        "what_is_novel": "The explicit analogy and mapping of cognitive load theory to LLMs, and the formalization of format alignment as a law, are new.",
        "classification_explanation": "The theory synthesizes existing ideas into a new predictive framework for LLMs.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning]",
            "Sweller (1988) Cognitive Load During Problem Solving: Effects on Learning [Cognitive load theory, human analogy]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-655",
    "original_theory_name": "Prompt Formatting Induces Degeneration and Output Validity Collapse",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
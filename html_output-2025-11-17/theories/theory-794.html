<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dynamic Episodic-Semantic Memory Integration for Language Model Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-794</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-794</p>
                <p><strong>Name:</strong> Dynamic Episodic-Semantic Memory Integration for Language Model Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language model agents can best use memory to solve tasks.</p>
                <p><strong>Description:</strong> This theory posits that language model agents achieve optimal task performance by dynamically integrating episodic (task-specific, contextual) and semantic (general, abstracted) memories. The agent continuously transforms relevant episodic experiences into semantic knowledge, and vice versa, allowing for both rapid adaptation to new tasks and robust generalization across tasks. The integration is modulated by task demands, uncertainty, and feedback, enabling the agent to flexibly retrieve, update, and consolidate memories for efficient problem-solving.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Task-Driven Episodic-Semantic Memory Conversion (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; encounters &#8594; novel_task_or_context<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; has_memory &#8594; episodic_and_semantic</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; transforms &#8594; episodic_memories_to_semantic_knowledge<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; retrieves &#8594; semantic_knowledge_for_task</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human memory research shows episodic experiences are consolidated into semantic knowledge for generalization (Tulving, 1972; McClelland et al., 1995). </li>
    <li>Recent LLM agent work uses retrieval-augmented memory to combine context-specific and general knowledge (Shinn et al., 2023; Yao et al., 2023). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends human memory models to LLM agents, introducing dynamic, context-driven conversion mechanisms.</p>            <p><strong>What Already Exists:</strong> Episodic and semantic memory distinction is well-established in cognitive science; some LLM agents use retrieval-augmented memory.</p>            <p><strong>What is Novel:</strong> The dynamic, bidirectional conversion and integration based on task demands in LLM agents is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [human memory distinction]</li>
    <li>McClelland et al. (1995) Why there are complementary learning systems in the hippocampus and neocortex [consolidation theory]</li>
    <li>Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [LLM agent memory]</li>
    <li>Yao et al. (2023) ReAct: Synergizing reasoning and acting in language models [retrieval-augmented LLMs]</li>
</ul>
            <h3>Statement 1: Uncertainty-Modulated Memory Retrieval (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; faces &#8594; high_task_uncertainty<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; has_memory &#8594; episodic_and_semantic</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; increases &#8594; episodic_memory_retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; decreases &#8594; semantic_memory_reliance</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Humans rely more on episodic memory under uncertainty (Shohamy & Daw, 2015). </li>
    <li>LLM agents with episodic memory modules adapt better to novel or ambiguous tasks (Liu et al., 2023). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law adapts cognitive science findings to LLM agent architectures.</p>            <p><strong>What Already Exists:</strong> Uncertainty-driven memory retrieval is observed in human cognition.</p>            <p><strong>What is Novel:</strong> Application to LLM agents with explicit modulation between episodic and semantic memory is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Shohamy & Daw (2015) Integrating memories to guide decisions [uncertainty and memory in humans]</li>
    <li>Liu et al. (2023) Memory-Augmented Language Models [episodic memory in LLM agents]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with dynamic episodic-semantic integration will outperform agents with only one memory type on transfer and continual learning tasks.</li>
                <li>Agents will show improved adaptation to novel tasks when allowed to consolidate episodic experiences into semantic knowledge.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Emergent meta-learning strategies may arise from repeated episodic-semantic integration cycles.</li>
                <li>Agents may develop internal representations analogous to human schemas, facilitating rapid learning of new but related tasks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If agents with dynamic memory integration do not outperform static-memory agents, the theory would be challenged.</li>
                <li>If uncertainty does not modulate memory retrieval patterns in LLM agents, the theory's mechanism would be in doubt.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to prevent interference or catastrophic forgetting during episodic-semantic conversion. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes cognitive science and LLM agent research, introducing novel mechanisms for memory integration.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [human memory distinction]</li>
    <li>McClelland et al. (1995) Complementary learning systems [consolidation theory]</li>
    <li>Shinn et al. (2023) Reflexion [LLM agent memory]</li>
    <li>Liu et al. (2023) Memory-Augmented Language Models [episodic memory in LLM agents]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dynamic Episodic-Semantic Memory Integration for Language Model Agents",
    "theory_description": "This theory posits that language model agents achieve optimal task performance by dynamically integrating episodic (task-specific, contextual) and semantic (general, abstracted) memories. The agent continuously transforms relevant episodic experiences into semantic knowledge, and vice versa, allowing for both rapid adaptation to new tasks and robust generalization across tasks. The integration is modulated by task demands, uncertainty, and feedback, enabling the agent to flexibly retrieve, update, and consolidate memories for efficient problem-solving.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Task-Driven Episodic-Semantic Memory Conversion",
                "if": [
                    {
                        "subject": "agent",
                        "relation": "encounters",
                        "object": "novel_task_or_context"
                    },
                    {
                        "subject": "agent",
                        "relation": "has_memory",
                        "object": "episodic_and_semantic"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "transforms",
                        "object": "episodic_memories_to_semantic_knowledge"
                    },
                    {
                        "subject": "agent",
                        "relation": "retrieves",
                        "object": "semantic_knowledge_for_task"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human memory research shows episodic experiences are consolidated into semantic knowledge for generalization (Tulving, 1972; McClelland et al., 1995).",
                        "uuids": []
                    },
                    {
                        "text": "Recent LLM agent work uses retrieval-augmented memory to combine context-specific and general knowledge (Shinn et al., 2023; Yao et al., 2023).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Episodic and semantic memory distinction is well-established in cognitive science; some LLM agents use retrieval-augmented memory.",
                    "what_is_novel": "The dynamic, bidirectional conversion and integration based on task demands in LLM agents is novel.",
                    "classification_explanation": "The law extends human memory models to LLM agents, introducing dynamic, context-driven conversion mechanisms.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [human memory distinction]",
                        "McClelland et al. (1995) Why there are complementary learning systems in the hippocampus and neocortex [consolidation theory]",
                        "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [LLM agent memory]",
                        "Yao et al. (2023) ReAct: Synergizing reasoning and acting in language models [retrieval-augmented LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Uncertainty-Modulated Memory Retrieval",
                "if": [
                    {
                        "subject": "agent",
                        "relation": "faces",
                        "object": "high_task_uncertainty"
                    },
                    {
                        "subject": "agent",
                        "relation": "has_memory",
                        "object": "episodic_and_semantic"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "increases",
                        "object": "episodic_memory_retrieval"
                    },
                    {
                        "subject": "agent",
                        "relation": "decreases",
                        "object": "semantic_memory_reliance"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Humans rely more on episodic memory under uncertainty (Shohamy & Daw, 2015).",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents with episodic memory modules adapt better to novel or ambiguous tasks (Liu et al., 2023).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Uncertainty-driven memory retrieval is observed in human cognition.",
                    "what_is_novel": "Application to LLM agents with explicit modulation between episodic and semantic memory is novel.",
                    "classification_explanation": "The law adapts cognitive science findings to LLM agent architectures.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Shohamy & Daw (2015) Integrating memories to guide decisions [uncertainty and memory in humans]",
                        "Liu et al. (2023) Memory-Augmented Language Models [episodic memory in LLM agents]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with dynamic episodic-semantic integration will outperform agents with only one memory type on transfer and continual learning tasks.",
        "Agents will show improved adaptation to novel tasks when allowed to consolidate episodic experiences into semantic knowledge."
    ],
    "new_predictions_unknown": [
        "Emergent meta-learning strategies may arise from repeated episodic-semantic integration cycles.",
        "Agents may develop internal representations analogous to human schemas, facilitating rapid learning of new but related tasks."
    ],
    "negative_experiments": [
        "If agents with dynamic memory integration do not outperform static-memory agents, the theory would be challenged.",
        "If uncertainty does not modulate memory retrieval patterns in LLM agents, the theory's mechanism would be in doubt."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to prevent interference or catastrophic forgetting during episodic-semantic conversion.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some evidence suggests that excessive episodic memory retrieval can lead to distraction and reduced generalization.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In highly repetitive or deterministic tasks, semantic memory may suffice and episodic memory may be redundant.",
        "For tasks with rapidly changing rules, episodic memory may dominate and semantic consolidation may be less useful."
    ],
    "existing_theory": {
        "what_already_exists": "Episodic and semantic memory distinction and their roles in learning are established in cognitive science.",
        "what_is_novel": "Dynamic, bidirectional integration and modulation in LLM agents based on task demands and uncertainty.",
        "classification_explanation": "The theory synthesizes cognitive science and LLM agent research, introducing novel mechanisms for memory integration.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [human memory distinction]",
            "McClelland et al. (1995) Complementary learning systems [consolidation theory]",
            "Shinn et al. (2023) Reflexion [LLM agent memory]",
            "Liu et al. (2023) Memory-Augmented Language Models [episodic memory in LLM agents]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language model agents can best use memory to solve tasks.",
    "original_theory_id": "theory-582",
    "original_theory_name": "Layered and Dynamic Memory Architecture Theory for LLM Agents",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
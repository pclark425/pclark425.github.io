<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Symbolic Feedback Fine-tuning Law for Validity and Exactness in LLM-based Molecular Generation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-500</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-500</p>
                <p><strong>Name:</strong> Symbolic Feedback Fine-tuning Law for Validity and Exactness in LLM-based Molecular Generation</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can synthesize novel chemicals for specific applications, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that fine-tuning large language models (LLMs) for molecular generation using token-level symbolic feedback from chemistry toolkits (e.g., RDKit) substantially increases the validity and exact-match accuracy of generated molecules, even for models with orders of magnitude fewer parameters than generalist LLMs. Symbolic feedback enables the model to internalize chemical grammar and valence rules, reducing the rate of invalid or chemically implausible outputs. This approach can outperform zero-shot or even supervised fine-tuning alone, especially in tasks with strict syntactic or semantic requirements (e.g., SMILES generation, reaction prediction). The theory also recognizes that the benefit of symbolic feedback is modulated by the underlying molecular representation (e.g., SMILES vs. SELFIES), and that symbolic feedback is most impactful when the representation does not guarantee validity by construction.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Token-level Symbolic Feedback Increases Validity and Exact Match (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_fine_tuned_with &#8594; token-level symbolic feedback from chemistry toolkits (e.g., RDKit)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves &#8594; higher validity and exact-match rates in molecular generation tasks compared to supervised fine-tuning or zero-shot baselines</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>RLSF-fine-tuned galactica-1.3b and mistral-7b models achieved large gains in validity (+58.3% for molecule generation) and exact match (+8.0%) over supervised fine-tuning, outperforming zero-shot GPT-4 despite being ~1000x smaller. <a href="../results/extraction-result-3398.html#e3398.1" class="evidence-link">[e3398.1]</a> <a href="../results/extraction-result-3398.html#e3398.0" class="evidence-link">[e3398.0]</a> <a href="../results/extraction-result-3398.html#e3398.2" class="evidence-link">[e3398.2]</a> </li>
    <li>RLSF (token-level symbolic feedback) applied to SFT-tuned galactica-1.3b produced the best reported results: compared to SFT, RLSF gave +8.0% EM and +58.3% Validity for Molecule Generation (MG), +11.8% EM and +2.1% Validity for Forward Synthesis (FS), and +12.2% EM and +3.2% Validity for Retrosynthesis (RS). <a href="../results/extraction-result-3398.html#e3398.1" class="evidence-link">[e3398.1]</a> </li>
    <li>Zero-shot GPT-4 produced high validity but very low exact-match rates on the chosen test splits: for Molecule Generation (MG) GPT-4 valid SMILES ~90% with EM 3%; for Forward Synthesis (FS) Validity ~93.1% with EM 0.4%; for Retrosynthesis (RS) Validity ~88.2% with EM 0.8%. RLSF-fine-tuned smaller models (e.g., galactica-1.3b) achieved higher EM and improved validity in reported experiments. <a href="../results/extraction-result-3398.html#e3398.2" class="evidence-link">[e3398.2]</a> </li>
    <li>RL-Boolean (scalar feedback) improved over SFT, but RLSF (token-level feedback) produced the best results overall for the two open models evaluated (mistral-7b and galactica-1.3b). <a href="../results/extraction-result-3398.html#e3398.0" class="evidence-link">[e3398.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Symbolic Feedback Internalizes Chemical Grammar and Reduces Invalid Outputs (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; receives &#8594; token-level error signals for invalid SMILES or valence violations during fine-tuning</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; reduces &#8594; the rate of invalid or chemically implausible outputs in generation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>RLSF-fine-tuned models produced much lower invalid SMILES rates than supervised or RL-Boolean fine-tuned models, and outperformed zero-shot GPT-4 on validity. <a href="../results/extraction-result-3398.html#e3398.1" class="evidence-link">[e3398.1]</a> <a href="../results/extraction-result-3398.html#e3398.2" class="evidence-link">[e3398.2]</a> </li>
    <li>Applying RL with Boolean scalar feedback produced further gains (for mistral-7b: +2.9% EM and +10.9% Validity on MG, +3.2% EM and +1.2% Validity on FS, +5.2% EM and no change in Validity on RS), but RLSF (token-level RDKit feedback) on galactica-1.3b produced the best results. <a href="../results/extraction-result-3398.html#e3398.0" class="evidence-link">[e3398.0]</a> </li>
    <li>The symbolic feedback depends on RDKit analysis—RDKit flags valence/syntax errors but cannot fully capture downstream application utility. <a href="../results/extraction-result-3398.html#e3398.1" class="evidence-link">[e3398.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Effectiveness of Symbolic Feedback is Modulated by Representation Choice (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; uses_representation &#8594; SMILES<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_fine_tuned_with &#8594; token-level symbolic feedback</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves &#8594; substantial reduction in invalid outputs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SMILES-based models (e.g., galactica-1.3b, mistral-7b) showed large improvements in validity after RLSF fine-tuning, while SELFIES-based models (e.g., SF-RNN) achieve 100% validity by construction, suggesting that symbolic feedback is most impactful for representations that do not guarantee validity. <a href="../results/extraction-result-3398.html#e3398.1" class="evidence-link">[e3398.1]</a> <a href="../results/extraction-result-3564.html#e3564.1" class="evidence-link">[e3564.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Applying token-level symbolic feedback to other open-source LLMs (e.g., Llama-2, Mistral) will yield similar improvements in validity and exact match for molecular generation tasks.</li>
                <li>Symbolic feedback fine-tuning will be especially effective for tasks with strict syntactic requirements (e.g., reaction product prediction, retrosynthesis) compared to tasks with more ambiguous outputs (e.g., molecule captioning).</li>
                <li>Symbolic feedback will be less beneficial for models using representations that guarantee validity by construction (e.g., SELFIES), but will still improve chemical plausibility and reduce subtle errors.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Symbolic feedback fine-tuning could enable small LLMs to match or surpass the performance of much larger generalist LLMs (e.g., GPT-4) on complex, multi-step chemical synthesis planning tasks.</li>
                <li>Combining symbolic feedback with property- or application-specific feedback (e.g., synthesizability, toxicity) could enable the generation of molecules that are both valid and optimized for real-world constraints.</li>
                <li>Symbolic feedback could be extended to 3D structure generation (e.g., XYZ, CIF) if appropriate toolkits are available, potentially improving validity in 3D molecular design.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If symbolic feedback fine-tuning does not improve validity or exact match compared to supervised fine-tuning, the theory would be challenged.</li>
                <li>If models fine-tuned with symbolic feedback still produce high rates of invalid or chemically implausible outputs, the theory's assertion about internalizing chemical grammar would be weakened.</li>
                <li>If symbolic feedback leads to overfitting to the symbolic tool's limitations (e.g., RDKit errors), resulting in valid but non-diverse or non-useful molecules, the theory's generality would be questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>SELFIES-based models (e.g., SF-RNN) achieve 100% validity by construction, suggesting that representation choice can also guarantee validity without symbolic feedback. <a href="../results/extraction-result-3564.html#e3564.1" class="evidence-link">[e3564.1]</a> </li>
    <li>Some models (e.g., JTN-VAE, Grammar VAE, SD-VAE) use grammar or scaffold constraints to achieve high validity without symbolic feedback. <a href="../results/extraction-result-3596.html#e3596.3" class="evidence-link">[e3596.3]</a> <a href="../results/extraction-result-3590.html#e3590.2" class="evidence-link">[e3590.2]</a> </li>
    <li>In multi-modal or graph-based models (e.g., GIT-Mol, InstructMol), validity improvements may arise from structural alignment rather than symbolic feedback. <a href="../results/extraction-result-3578.html#e3578.0" class="evidence-link">[e3578.0]</a> <a href="../results/extraction-result-3593.html#e3593.0" class="evidence-link">[e3593.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>This theory is novel in formalizing the role of token-level symbolic feedback in LLM fine-tuning for chemical generation, as distinct from prior work on supervised or RL-based fine-tuning.</li>
    <li>RLSF: Fine-tuning LLMs via Symbolic Feedback (2024) [Introduces the RLSF method and provides the first systematic evidence for token-level symbolic feedback in chemistry LLMs, but does not generalize the theory as stated here.]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Symbolic Feedback Fine-tuning Law for Validity and Exactness in LLM-based Molecular Generation",
    "theory_description": "This theory posits that fine-tuning large language models (LLMs) for molecular generation using token-level symbolic feedback from chemistry toolkits (e.g., RDKit) substantially increases the validity and exact-match accuracy of generated molecules, even for models with orders of magnitude fewer parameters than generalist LLMs. Symbolic feedback enables the model to internalize chemical grammar and valence rules, reducing the rate of invalid or chemically implausible outputs. This approach can outperform zero-shot or even supervised fine-tuning alone, especially in tasks with strict syntactic or semantic requirements (e.g., SMILES generation, reaction prediction). The theory also recognizes that the benefit of symbolic feedback is modulated by the underlying molecular representation (e.g., SMILES vs. SELFIES), and that symbolic feedback is most impactful when the representation does not guarantee validity by construction.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Token-level Symbolic Feedback Increases Validity and Exact Match",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_fine_tuned_with",
                        "object": "token-level symbolic feedback from chemistry toolkits (e.g., RDKit)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves",
                        "object": "higher validity and exact-match rates in molecular generation tasks compared to supervised fine-tuning or zero-shot baselines"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "RLSF-fine-tuned galactica-1.3b and mistral-7b models achieved large gains in validity (+58.3% for molecule generation) and exact match (+8.0%) over supervised fine-tuning, outperforming zero-shot GPT-4 despite being ~1000x smaller.",
                        "uuids": [
                            "e3398.1",
                            "e3398.0",
                            "e3398.2"
                        ]
                    },
                    {
                        "text": "RLSF (token-level symbolic feedback) applied to SFT-tuned galactica-1.3b produced the best reported results: compared to SFT, RLSF gave +8.0% EM and +58.3% Validity for Molecule Generation (MG), +11.8% EM and +2.1% Validity for Forward Synthesis (FS), and +12.2% EM and +3.2% Validity for Retrosynthesis (RS).",
                        "uuids": [
                            "e3398.1"
                        ]
                    },
                    {
                        "text": "Zero-shot GPT-4 produced high validity but very low exact-match rates on the chosen test splits: for Molecule Generation (MG) GPT-4 valid SMILES ~90% with EM 3%; for Forward Synthesis (FS) Validity ~93.1% with EM 0.4%; for Retrosynthesis (RS) Validity ~88.2% with EM 0.8%. RLSF-fine-tuned smaller models (e.g., galactica-1.3b) achieved higher EM and improved validity in reported experiments.",
                        "uuids": [
                            "e3398.2"
                        ]
                    },
                    {
                        "text": "RL-Boolean (scalar feedback) improved over SFT, but RLSF (token-level feedback) produced the best results overall for the two open models evaluated (mistral-7b and galactica-1.3b).",
                        "uuids": [
                            "e3398.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        },
        {
            "law": {
                "law_name": "Symbolic Feedback Internalizes Chemical Grammar and Reduces Invalid Outputs",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "receives",
                        "object": "token-level error signals for invalid SMILES or valence violations during fine-tuning"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "reduces",
                        "object": "the rate of invalid or chemically implausible outputs in generation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "RLSF-fine-tuned models produced much lower invalid SMILES rates than supervised or RL-Boolean fine-tuned models, and outperformed zero-shot GPT-4 on validity.",
                        "uuids": [
                            "e3398.1",
                            "e3398.2"
                        ]
                    },
                    {
                        "text": "Applying RL with Boolean scalar feedback produced further gains (for mistral-7b: +2.9% EM and +10.9% Validity on MG, +3.2% EM and +1.2% Validity on FS, +5.2% EM and no change in Validity on RS), but RLSF (token-level RDKit feedback) on galactica-1.3b produced the best results.",
                        "uuids": [
                            "e3398.0"
                        ]
                    },
                    {
                        "text": "The symbolic feedback depends on RDKit analysis—RDKit flags valence/syntax errors but cannot fully capture downstream application utility.",
                        "uuids": [
                            "e3398.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Effectiveness of Symbolic Feedback is Modulated by Representation Choice",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "uses_representation",
                        "object": "SMILES"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_fine_tuned_with",
                        "object": "token-level symbolic feedback"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves",
                        "object": "substantial reduction in invalid outputs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SMILES-based models (e.g., galactica-1.3b, mistral-7b) showed large improvements in validity after RLSF fine-tuning, while SELFIES-based models (e.g., SF-RNN) achieve 100% validity by construction, suggesting that symbolic feedback is most impactful for representations that do not guarantee validity.",
                        "uuids": [
                            "e3398.1",
                            "e3564.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "Applying token-level symbolic feedback to other open-source LLMs (e.g., Llama-2, Mistral) will yield similar improvements in validity and exact match for molecular generation tasks.",
        "Symbolic feedback fine-tuning will be especially effective for tasks with strict syntactic requirements (e.g., reaction product prediction, retrosynthesis) compared to tasks with more ambiguous outputs (e.g., molecule captioning).",
        "Symbolic feedback will be less beneficial for models using representations that guarantee validity by construction (e.g., SELFIES), but will still improve chemical plausibility and reduce subtle errors."
    ],
    "new_predictions_unknown": [
        "Symbolic feedback fine-tuning could enable small LLMs to match or surpass the performance of much larger generalist LLMs (e.g., GPT-4) on complex, multi-step chemical synthesis planning tasks.",
        "Combining symbolic feedback with property- or application-specific feedback (e.g., synthesizability, toxicity) could enable the generation of molecules that are both valid and optimized for real-world constraints.",
        "Symbolic feedback could be extended to 3D structure generation (e.g., XYZ, CIF) if appropriate toolkits are available, potentially improving validity in 3D molecular design."
    ],
    "negative_experiments": [
        "If symbolic feedback fine-tuning does not improve validity or exact match compared to supervised fine-tuning, the theory would be challenged.",
        "If models fine-tuned with symbolic feedback still produce high rates of invalid or chemically implausible outputs, the theory's assertion about internalizing chemical grammar would be weakened.",
        "If symbolic feedback leads to overfitting to the symbolic tool's limitations (e.g., RDKit errors), resulting in valid but non-diverse or non-useful molecules, the theory's generality would be questioned."
    ],
    "unaccounted_for": [
        {
            "text": "SELFIES-based models (e.g., SF-RNN) achieve 100% validity by construction, suggesting that representation choice can also guarantee validity without symbolic feedback.",
            "uuids": [
                "e3564.1"
            ]
        },
        {
            "text": "Some models (e.g., JTN-VAE, Grammar VAE, SD-VAE) use grammar or scaffold constraints to achieve high validity without symbolic feedback.",
            "uuids": [
                "e3596.3",
                "e3590.2"
            ]
        },
        {
            "text": "In multi-modal or graph-based models (e.g., GIT-Mol, InstructMol), validity improvements may arise from structural alignment rather than symbolic feedback.",
            "uuids": [
                "e3578.0",
                "e3593.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Symbolic feedback may not capture all aspects of chemical correctness (e.g., synthesizability, 3D structure), so models may still generate valid but impractical molecules.",
            "uuids": [
                "e3398.1",
                "e3578.0",
                "e3578.1"
            ]
        },
        {
            "text": "For representations that guarantee validity (e.g., SELFIES), symbolic feedback may have limited additional benefit.",
            "uuids": [
                "e3564.1"
            ]
        }
    ],
    "special_cases": [
        "For representations that are already validity-guaranteed (e.g., SELFIES), symbolic feedback may have limited additional benefit.",
        "If the symbolic tool (e.g., RDKit) has limitations or errors, the feedback may reinforce incorrect rules.",
        "Symbolic feedback may not address higher-level chemical correctness (e.g., synthesizability, bioactivity, 3D structure) unless the feedback is extended to those domains."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "This theory is novel in formalizing the role of token-level symbolic feedback in LLM fine-tuning for chemical generation, as distinct from prior work on supervised or RL-based fine-tuning.",
            "RLSF: Fine-tuning LLMs via Symbolic Feedback (2024) [Introduces the RLSF method and provides the first systematic evidence for token-level symbolic feedback in chemistry LLMs, but does not generalize the theory as stated here.]"
        ]
    },
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory of Demonstration Information Bottleneck in LLM Scientific Simulation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1629</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1629</p>
                <p><strong>Name:</strong> Theory of Demonstration Information Bottleneck in LLM Scientific Simulation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory proposes that the amount and type of information provided in demonstrations within prompts acts as an information bottleneck, constraining the LLM's ability to simulate scientific processes. If demonstrations are too sparse or omit critical domain-specific information, the LLM's simulation accuracy is fundamentally limited, regardless of model scale. Conversely, overly verbose or redundant demonstrations can introduce noise, distracting the model and reducing accuracy.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Demonstration Sufficiency Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; demonstration information content &#8594; is_sufficient_for &#8594; scientific task requirements</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; is_maximized &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Demonstrations that include all necessary domain-specific steps and information yield higher LLM accuracy. </li>
    <li>Sparse demonstrations lacking key information result in systematic errors. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law formalizes a known effect as a limiting information bottleneck in scientific simulation.</p>            <p><strong>What Already Exists:</strong> Demonstration sufficiency is discussed in prompt engineering, but not as a formal information bottleneck in scientific simulation.</p>            <p><strong>What is Novel:</strong> The explicit bottleneck framing and its limiting effect on LLM scientific simulation is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Demonstration content affects generalization and accuracy]</li>
</ul>
            <h3>Statement 1: Demonstration Redundancy Degradation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; demonstration information content &#8594; is_excessive_or_redundant &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM simulation accuracy &#8594; is_degraded &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Overly verbose demonstrations can distract LLMs, leading to lower accuracy and increased hallucinations. </li>
    <li>Prompt engineering studies show that excessive information can reduce LLM focus on relevant task features. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The law synthesizes prompt engineering observations into a new, explicit bottleneck theory for scientific simulation.</p>            <p><strong>What Already Exists:</strong> Prompt verbosity and redundancy are known to affect LLM performance, but not formalized as a bottleneck in scientific simulation.</p>            <p><strong>What is Novel:</strong> The explicit bottleneck framing and its limiting effect on LLM scientific simulation is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Demonstration content affects generalization and accuracy]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Providing demonstrations that are neither too sparse nor too verbose will maximize LLM simulation accuracy.</li>
                <li>Removing critical information from demonstrations will systematically reduce LLM accuracy, regardless of model size.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>For very large LLMs, the effect of demonstration redundancy may be mitigated by model capacity.</li>
                <li>In highly structured scientific domains, the information bottleneck may be less pronounced.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs achieve high accuracy with sparse or redundant demonstrations, the theory would be falsified.</li>
                <li>If demonstration sufficiency does not affect LLM simulation accuracy, the theory's central claim would be weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs infer missing information from context or pretraining, bypassing the demonstration bottleneck. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> The theory formalizes a limiting factor that has been observed but not explicitly theorized in the context of LLM scientific simulation.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Demonstration content affects generalization and accuracy]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Theory of Demonstration Information Bottleneck in LLM Scientific Simulation",
    "theory_description": "This theory proposes that the amount and type of information provided in demonstrations within prompts acts as an information bottleneck, constraining the LLM's ability to simulate scientific processes. If demonstrations are too sparse or omit critical domain-specific information, the LLM's simulation accuracy is fundamentally limited, regardless of model scale. Conversely, overly verbose or redundant demonstrations can introduce noise, distracting the model and reducing accuracy.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Demonstration Sufficiency Law",
                "if": [
                    {
                        "subject": "demonstration information content",
                        "relation": "is_sufficient_for",
                        "object": "scientific task requirements"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "is_maximized",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Demonstrations that include all necessary domain-specific steps and information yield higher LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Sparse demonstrations lacking key information result in systematic errors.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Demonstration sufficiency is discussed in prompt engineering, but not as a formal information bottleneck in scientific simulation.",
                    "what_is_novel": "The explicit bottleneck framing and its limiting effect on LLM scientific simulation is new.",
                    "classification_explanation": "The law formalizes a known effect as a limiting information bottleneck in scientific simulation.",
                    "likely_classification": "new",
                    "references": [
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Demonstration content affects generalization and accuracy]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Demonstration Redundancy Degradation Law",
                "if": [
                    {
                        "subject": "demonstration information content",
                        "relation": "is_excessive_or_redundant",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM simulation accuracy",
                        "relation": "is_degraded",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Overly verbose demonstrations can distract LLMs, leading to lower accuracy and increased hallucinations.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering studies show that excessive information can reduce LLM focus on relevant task features.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt verbosity and redundancy are known to affect LLM performance, but not formalized as a bottleneck in scientific simulation.",
                    "what_is_novel": "The explicit bottleneck framing and its limiting effect on LLM scientific simulation is new.",
                    "classification_explanation": "The law synthesizes prompt engineering observations into a new, explicit bottleneck theory for scientific simulation.",
                    "likely_classification": "new",
                    "references": [
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Demonstration content affects generalization and accuracy]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Providing demonstrations that are neither too sparse nor too verbose will maximize LLM simulation accuracy.",
        "Removing critical information from demonstrations will systematically reduce LLM accuracy, regardless of model size."
    ],
    "new_predictions_unknown": [
        "For very large LLMs, the effect of demonstration redundancy may be mitigated by model capacity.",
        "In highly structured scientific domains, the information bottleneck may be less pronounced."
    ],
    "negative_experiments": [
        "If LLMs achieve high accuracy with sparse or redundant demonstrations, the theory would be falsified.",
        "If demonstration sufficiency does not affect LLM simulation accuracy, the theory's central claim would be weakened."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs infer missing information from context or pretraining, bypassing the demonstration bottleneck.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that LLMs can ignore redundant information and focus on relevant features in certain domains.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In domains with highly redundant or self-explanatory structure, demonstration sufficiency may be less critical.",
        "For tasks with minimal information requirements, the bottleneck may not apply."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering discusses demonstration sufficiency and redundancy for general LLM performance.",
        "what_is_novel": "The explicit theory of demonstration information bottleneck as a limiting factor in scientific simulation is new.",
        "classification_explanation": "The theory formalizes a limiting factor that has been observed but not explicitly theorized in the context of LLM scientific simulation.",
        "likely_classification": "new",
        "references": [
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Demonstration content affects generalization and accuracy]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-635",
    "original_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Theory of Prompt and Demonstration Structure as a Limiting Factor in LLM Scientific Simulation",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
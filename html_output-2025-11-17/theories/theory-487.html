<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Token and Embedding Invariance in LLM Spatial Pattern Solving - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-487</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-487</p>
                <p><strong>Name:</strong> Token and Embedding Invariance in LLM Spatial Pattern Solving</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku, based on the following results.</p>
                <p><strong>Description:</strong> Large Language Models (LLMs) exhibit a notable degree of invariance to the specific tokens or embeddings used to represent symbols in spatial pattern transformation tasks (such as ARC and PCFG sequence transformations), provided the mapping is consistent within the prompt. This suggests that LLMs learn and apply abstract relational and pattern structure, rather than relying solely on memorized surface token identities. This invariance extends to cases where symbols are mapped to arbitrary tokens or even to newly sampled embeddings, up to a moderate noise threshold. However, the invariance is partial: performance degrades with increasing task complexity, embedding noise, or if the mapping is inconsistent. Tasks that require semantic world knowledge, rather than pure pattern structure, do not exhibit this invariance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Token Mapping Invariance Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; spatial pattern task using random token mapping for symbols<span style="color: #888888;">, and</span></div>
        <div>&#8226; token mapping &#8594; is_consistent &#8594; within prompt</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; solves &#8594; substantial fraction of spatial pattern tasks (e.g., ARC, PCFG) with only moderate performance drop</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>text-davinci-003 solves ~43.6 ARC tasks on average with random alphabet mapping, compared to 85 with canonical tokens; performance is robust to random token mapping as long as mapping is consistent within the prompt. <a href="../results/extraction-result-3403.html#e3403.4" class="evidence-link">[e3403.4]</a> </li>
    <li>LLMs can solve PCFG sequence transformation tasks with random token mappings, showing partial invariance to token identity. <a href="../results/extraction-result-3403.html#e3403.4" class="evidence-link">[e3403.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Embedding Perturbation Robustness Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_prompted_with &#8594; spatial pattern task using newly sampled embeddings (from pretrained distribution, moderate noise)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; maintains &#8594; comparable success rates to native embeddings (at 1σ noise), with degradation at higher noise (2σ)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>New-embedding probe: 8B PaLM variant maintains success at 1σ, degrades at 2σ; LLMs can solve ARC and PCFG tasks with newly sampled embeddings as long as the embedding is not too far from the pretrained distribution. <a href="../results/extraction-result-3403.html#e3403.4" class="evidence-link">[e3403.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Partiality and Boundary Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; token or embedding mapping &#8594; is_inconsistent &#8594; within prompt</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; performance &#8594; drops to chance or near-chance</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Performance degrades to chance if the mapping is inconsistent within the prompt; invariance is partial, not absolute. <a href="../results/extraction-result-3403.html#e3403.4" class="evidence-link">[e3403.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Task-Type Limitation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; spatial pattern task &#8594; requires &#8594; semantic world knowledge or open-domain associations</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; does_not_exhibit &#8594; token/embedding invariance</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Tasks that rely on world knowledge or semantic associations (rather than pure pattern structure) may not show token/embedding invariance; e.g., crossword clue-answer tasks (RAG-wiki, RAG-dict) require semantic matching. <a href="../results/extraction-result-3422.html#e3422.2" class="evidence-link">[e3422.2]</a> <a href="../results/extraction-result-3422.html#e3422.3" class="evidence-link">[e3422.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new spatial pattern task (e.g., a novel ARC-like grid transformation) is presented with arbitrary but consistent token mapping, a large LLM will solve a substantial fraction of instances, though with some performance drop compared to canonical tokens.</li>
                <li>If embeddings for symbols are perturbed within the pretrained distribution (e.g., 1σ), LLMs will maintain performance on spatial pattern tasks up to a moderate noise threshold.</li>
                <li>If the mapping is inconsistent within the prompt (e.g., a symbol is mapped to two different tokens in different examples), LLM performance will drop to chance.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained with highly variable or adversarial token/embedding mappings, they may develop even greater invariance and generalization to unseen symbol systems.</li>
                <li>LLMs may be able to transfer spatial reasoning to entirely novel symbol systems (e.g., non-Latin scripts, iconographic representations) if prompted with a consistent mapping.</li>
                <li>If LLMs are prompted with spatial pattern tasks using multimodal symbol representations (e.g., images or icons mapped to tokens), they may still exhibit partial invariance if the mapping is consistent and the model is sufficiently large.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs fail completely on spatial pattern tasks with random token mapping (i.e., performance drops to chance even with consistent mapping), this would falsify the token mapping invariance law.</li>
                <li>If embedding perturbation at 1σ causes catastrophic failure (performance drops to chance), the embedding robustness law would be challenged.</li>
                <li>If LLMs succeed on tasks requiring semantic world knowledge (e.g., crossword clue-answer) with random token mapping, this would challenge the task-type limitation law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Tasks that require semantic world knowledge, such as crossword clue-answering or open-domain QA, are not explained by this theory and do not exhibit token/embedding invariance. <a href="../results/extraction-result-3422.html#e3422.2" class="evidence-link">[e3422.2]</a> <a href="../results/extraction-result-3422.html#e3422.3" class="evidence-link">[e3422.3]</a> </li>
    <li>Spatial reasoning tasks that require visual or multimodal grounding (e.g., visual math problems, diagram-based geometry) may not exhibit the same invariance, especially if the mapping is not textual. <a href="../results/extraction-result-3433.html#e3433.1" class="evidence-link">[e3433.1]</a> <a href="../results/extraction-result-3433.html#e3433.3" class="evidence-link">[e3433.3]</a> <a href="../results/extraction-result-3433.html#e3433.4" class="evidence-link">[e3433.4]</a> <a href="../results/extraction-result-3433.html#e3433.5" class="evidence-link">[e3433.5]</a> <a href="../results/extraction-result-3433.html#e3433.6" class="evidence-link">[e3433.6]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Chan et al. (2023) Large Language Models as General Pattern Machines [First to report token/embedding invariance in LLMs for pattern tasks, but no formal law stated]</li>
    <li>Hupkes et al. (2020) Compositionality Benchmarks [Related to compositional generalization, but not focused on token/embedding invariance]</li>
    <li>Tan & Motani (2023) Solving the Abstraction and Reasoning Corpus (ARC) with Large Language Models [Related to LLMs on spatial pattern tasks, but not focused on token/embedding invariance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Token and Embedding Invariance in LLM Spatial Pattern Solving",
    "theory_description": "Large Language Models (LLMs) exhibit a notable degree of invariance to the specific tokens or embeddings used to represent symbols in spatial pattern transformation tasks (such as ARC and PCFG sequence transformations), provided the mapping is consistent within the prompt. This suggests that LLMs learn and apply abstract relational and pattern structure, rather than relying solely on memorized surface token identities. This invariance extends to cases where symbols are mapped to arbitrary tokens or even to newly sampled embeddings, up to a moderate noise threshold. However, the invariance is partial: performance degrades with increasing task complexity, embedding noise, or if the mapping is inconsistent. Tasks that require semantic world knowledge, rather than pure pattern structure, do not exhibit this invariance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Token Mapping Invariance Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "spatial pattern task using random token mapping for symbols"
                    },
                    {
                        "subject": "token mapping",
                        "relation": "is_consistent",
                        "object": "within prompt"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "solves",
                        "object": "substantial fraction of spatial pattern tasks (e.g., ARC, PCFG) with only moderate performance drop"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "text-davinci-003 solves ~43.6 ARC tasks on average with random alphabet mapping, compared to 85 with canonical tokens; performance is robust to random token mapping as long as mapping is consistent within the prompt.",
                        "uuids": [
                            "e3403.4"
                        ]
                    },
                    {
                        "text": "LLMs can solve PCFG sequence transformation tasks with random token mappings, showing partial invariance to token identity.",
                        "uuids": [
                            "e3403.4"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        },
        {
            "law": {
                "law_name": "Embedding Perturbation Robustness Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_prompted_with",
                        "object": "spatial pattern task using newly sampled embeddings (from pretrained distribution, moderate noise)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "maintains",
                        "object": "comparable success rates to native embeddings (at 1σ noise), with degradation at higher noise (2σ)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "New-embedding probe: 8B PaLM variant maintains success at 1σ, degrades at 2σ; LLMs can solve ARC and PCFG tasks with newly sampled embeddings as long as the embedding is not too far from the pretrained distribution.",
                        "uuids": [
                            "e3403.4"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative"
            }
        },
        {
            "law": {
                "law_name": "Partiality and Boundary Law",
                "if": [
                    {
                        "subject": "token or embedding mapping",
                        "relation": "is_inconsistent",
                        "object": "within prompt"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "performance",
                        "object": "drops to chance or near-chance"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Performance degrades to chance if the mapping is inconsistent within the prompt; invariance is partial, not absolute.",
                        "uuids": [
                            "e3403.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Task-Type Limitation Law",
                "if": [
                    {
                        "subject": "spatial pattern task",
                        "relation": "requires",
                        "object": "semantic world knowledge or open-domain associations"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "does_not_exhibit",
                        "object": "token/embedding invariance"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Tasks that rely on world knowledge or semantic associations (rather than pure pattern structure) may not show token/embedding invariance; e.g., crossword clue-answer tasks (RAG-wiki, RAG-dict) require semantic matching.",
                        "uuids": [
                            "e3422.2",
                            "e3422.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new spatial pattern task (e.g., a novel ARC-like grid transformation) is presented with arbitrary but consistent token mapping, a large LLM will solve a substantial fraction of instances, though with some performance drop compared to canonical tokens.",
        "If embeddings for symbols are perturbed within the pretrained distribution (e.g., 1σ), LLMs will maintain performance on spatial pattern tasks up to a moderate noise threshold.",
        "If the mapping is inconsistent within the prompt (e.g., a symbol is mapped to two different tokens in different examples), LLM performance will drop to chance."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained with highly variable or adversarial token/embedding mappings, they may develop even greater invariance and generalization to unseen symbol systems.",
        "LLMs may be able to transfer spatial reasoning to entirely novel symbol systems (e.g., non-Latin scripts, iconographic representations) if prompted with a consistent mapping.",
        "If LLMs are prompted with spatial pattern tasks using multimodal symbol representations (e.g., images or icons mapped to tokens), they may still exhibit partial invariance if the mapping is consistent and the model is sufficiently large."
    ],
    "negative_experiments": [
        "If LLMs fail completely on spatial pattern tasks with random token mapping (i.e., performance drops to chance even with consistent mapping), this would falsify the token mapping invariance law.",
        "If embedding perturbation at 1σ causes catastrophic failure (performance drops to chance), the embedding robustness law would be challenged.",
        "If LLMs succeed on tasks requiring semantic world knowledge (e.g., crossword clue-answer) with random token mapping, this would challenge the task-type limitation law."
    ],
    "unaccounted_for": [
        {
            "text": "Tasks that require semantic world knowledge, such as crossword clue-answering or open-domain QA, are not explained by this theory and do not exhibit token/embedding invariance.",
            "uuids": [
                "e3422.2",
                "e3422.3"
            ]
        },
        {
            "text": "Spatial reasoning tasks that require visual or multimodal grounding (e.g., visual math problems, diagram-based geometry) may not exhibit the same invariance, especially if the mapping is not textual.",
            "uuids": [
                "e3433.1",
                "e3433.3",
                "e3433.4",
                "e3433.5",
                "e3433.6"
            ]
        }
    ],
    "conflicting_evidence": [],
    "special_cases": [
        "Performance degrades with increasing task complexity (e.g., longer sequences, more complex transformations) and with higher embedding noise; invariance is partial, not absolute.",
        "If the mapping is inconsistent within the prompt, performance drops to chance.",
        "Tasks that require semantic or world knowledge (e.g., crossword solving, open-domain QA) do not show this invariance.",
        "For tasks with very high compositional depth or sequence length (e.g., PCFG with k=32, w=31), even with canonical tokens, LLMs' performance drops, and invariance is further reduced."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Chan et al. (2023) Large Language Models as General Pattern Machines [First to report token/embedding invariance in LLMs for pattern tasks, but no formal law stated]",
            "Hupkes et al. (2020) Compositionality Benchmarks [Related to compositional generalization, but not focused on token/embedding invariance]",
            "Tan & Motani (2023) Solving the Abstraction and Reasoning Corpus (ARC) with Large Language Models [Related to LLMs on spatial pattern tasks, but not focused on token/embedding invariance]"
        ]
    },
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
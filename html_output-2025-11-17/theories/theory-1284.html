<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Faithfulness and Inductive Bias Preservation Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1284</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1284</p>
                <p><strong>Name:</strong> Structural Faithfulness and Inductive Bias Preservation Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory posits that the ideal conversion of graphs into text for language model (LM) training must preserve the structural properties and inductive biases inherent to the original graph. By maintaining explicit encoding of graph topology, connectivity, and relational patterns, the resulting text enables LMs to learn and generalize over graph-structured data, supporting both local and global reasoning. The theory further asserts that representations should be robust to graph isomorphisms and invariant to irrelevant permutations, ensuring that LMs acquire the correct inductive biases for downstream graph tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structural Faithfulness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; preserves &#8594; all topological and relational properties of the original graph</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_learn &#8594; graph-structured reasoning and generalization</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that loss of structural information (e.g., edge direction, cycles, motifs) in representations leads to degraded LM performance on graph reasoning tasks. </li>
    <li>Graph neural networks (GNNs) rely on explicit structural encoding to achieve strong generalization; similar principles apply to LMs trained on graph-to-text data. </li>
    <li>Isomorphic graphs should yield identical representations to avoid spurious distinctions in LM learning. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While structural preservation is established in graph ML, its necessity and operationalization in graph-to-text for LMs is novel.</p>            <p><strong>What Already Exists:</strong> Structural preservation is a core principle in GNNs and graph kernels.</p>            <p><strong>What is Novel:</strong> Its explicit application to graph-to-text for LMs, and the requirement for isomorphism invariance in text, is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [GNN expressivity and structural preservation]</li>
    <li>Morris et al. (2021) Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks [Structural invariance in GNNs]</li>
</ul>
            <h3>Statement 1: Inductive Bias Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph-to-text representation &#8594; encodes &#8594; inductive biases present in the original graph domain (e.g., locality, hierarchy, symmetry)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; inherits &#8594; appropriate inductive biases for graph-based tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Inductive biases such as locality and symmetry are critical for generalization in graph learning; their loss in representation leads to overfitting or poor transfer. </li>
    <li>Empirical evidence from GNNs and symbolic reasoning shows that models with correct inductive biases outperform those without. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law is conceptually related to existing ML theory, but its operationalization for graph-to-text LM training is novel.</p>            <p><strong>What Already Exists:</strong> Inductive bias is a well-studied concept in ML and GNNs.</p>            <p><strong>What is Novel:</strong> Explicitly requiring graph-to-text representations to preserve these biases for LMs is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Inductive bias in graph learning]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Inductive bias and expressivity]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs trained on structurally faithful, bias-preserving graph-to-text data will outperform those trained on lossy or structure-agnostic representations in graph reasoning and classification tasks.</li>
                <li>Representations that are invariant to graph isomorphism will yield more robust and generalizable LM behavior.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Preserving higher-order motifs or subgraph patterns in text may enable LMs to perform few-shot learning on novel graph structures.</li>
                <li>Encoding domain-specific inductive biases (e.g., chemical valence in molecules) may allow LMs to generalize to unseen graph domains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs trained on structurally faithful representations do not outperform those trained on structure-agnostic representations, the theory is challenged.</li>
                <li>If isomorphism-invariant representations do not improve generalization, the structural faithfulness law is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal granularity of structural encoding (e.g., edge lists vs. adjacency matrices vs. motif-based) is not specified. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is conceptually related to existing work in graph ML, but its application to LM training and graph-to-text conversion is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Inductive bias in graph learning]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Structural preservation and expressivity]</li>
    <li>Morris et al. (2021) Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks [Structural invariance in GNNs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "theory_description": "This theory posits that the ideal conversion of graphs into text for language model (LM) training must preserve the structural properties and inductive biases inherent to the original graph. By maintaining explicit encoding of graph topology, connectivity, and relational patterns, the resulting text enables LMs to learn and generalize over graph-structured data, supporting both local and global reasoning. The theory further asserts that representations should be robust to graph isomorphisms and invariant to irrelevant permutations, ensuring that LMs acquire the correct inductive biases for downstream graph tasks.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structural Faithfulness Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "preserves",
                        "object": "all topological and relational properties of the original graph"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_learn",
                        "object": "graph-structured reasoning and generalization"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that loss of structural information (e.g., edge direction, cycles, motifs) in representations leads to degraded LM performance on graph reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Graph neural networks (GNNs) rely on explicit structural encoding to achieve strong generalization; similar principles apply to LMs trained on graph-to-text data.",
                        "uuids": []
                    },
                    {
                        "text": "Isomorphic graphs should yield identical representations to avoid spurious distinctions in LM learning.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Structural preservation is a core principle in GNNs and graph kernels.",
                    "what_is_novel": "Its explicit application to graph-to-text for LMs, and the requirement for isomorphism invariance in text, is new.",
                    "classification_explanation": "While structural preservation is established in graph ML, its necessity and operationalization in graph-to-text for LMs is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [GNN expressivity and structural preservation]",
                        "Morris et al. (2021) Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks [Structural invariance in GNNs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Inductive Bias Preservation Law",
                "if": [
                    {
                        "subject": "graph-to-text representation",
                        "relation": "encodes",
                        "object": "inductive biases present in the original graph domain (e.g., locality, hierarchy, symmetry)"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "inherits",
                        "object": "appropriate inductive biases for graph-based tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Inductive biases such as locality and symmetry are critical for generalization in graph learning; their loss in representation leads to overfitting or poor transfer.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical evidence from GNNs and symbolic reasoning shows that models with correct inductive biases outperform those without.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Inductive bias is a well-studied concept in ML and GNNs.",
                    "what_is_novel": "Explicitly requiring graph-to-text representations to preserve these biases for LMs is new.",
                    "classification_explanation": "The law is conceptually related to existing ML theory, but its operationalization for graph-to-text LM training is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Inductive bias in graph learning]",
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Inductive bias and expressivity]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs trained on structurally faithful, bias-preserving graph-to-text data will outperform those trained on lossy or structure-agnostic representations in graph reasoning and classification tasks.",
        "Representations that are invariant to graph isomorphism will yield more robust and generalizable LM behavior."
    ],
    "new_predictions_unknown": [
        "Preserving higher-order motifs or subgraph patterns in text may enable LMs to perform few-shot learning on novel graph structures.",
        "Encoding domain-specific inductive biases (e.g., chemical valence in molecules) may allow LMs to generalize to unseen graph domains."
    ],
    "negative_experiments": [
        "If LMs trained on structurally faithful representations do not outperform those trained on structure-agnostic representations, the theory is challenged.",
        "If isomorphism-invariant representations do not improve generalization, the structural faithfulness law is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal granularity of structural encoding (e.g., edge lists vs. adjacency matrices vs. motif-based) is not specified.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some tasks (e.g., node classification in regular graphs) may not require full structural faithfulness for strong performance.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with trivial or uniform structure may not benefit from detailed structural encoding.",
        "Very large graphs may require compressed or hierarchical representations to remain tractable."
    ],
    "existing_theory": {
        "what_already_exists": "Structural and inductive bias preservation are established in GNNs and graph ML.",
        "what_is_novel": "Their explicit, operationalized necessity for graph-to-text LM training is new.",
        "classification_explanation": "The theory is conceptually related to existing work in graph ML, but its application to LM training and graph-to-text conversion is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Inductive bias in graph learning]",
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [Structural preservation and expressivity]",
            "Morris et al. (2021) Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks [Structural invariance in GNNs]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-613",
    "original_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Structural Faithfulness and Inductive Bias Preservation Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1913</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1913</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the performance of large language models (LLMs) on problem-solving tasks is determined by the degree to which the presentation format of a problem aligns with the model's internal cognitive representations, which are shaped by its pretraining data and architectural biases. Formats that closely match these internal representations facilitate more effective retrieval, reasoning, and generation, while misaligned formats introduce cognitive friction, leading to degraded performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Representation Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_aligned_with &#8594; LLM_internal_representation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; enhanced_performance_on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on tasks when the input format resembles the data distributions seen during pretraining (e.g., natural language Q&A, code snippets). </li>
    <li>Prompt engineering that reformulates questions into more familiar or canonical forms improves LLM accuracy. </li>
    <li>Chain-of-thought prompting, which mimics step-by-step reasoning formats common in training data, improves LLM reasoning performance. </li>
    <li>LLMs show higher accuracy on tasks when the prompt format matches the style of their pretraining corpus (e.g., code models excel with code-formatted prompts). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt engineering is well-studied, the explicit framing of performance as a function of alignment with internal representations is a novel abstraction.</p>            <p><strong>What Already Exists:</strong> Prompt engineering and format effects are known to influence LLM performance, with empirical studies showing that rephrasing or formatting can improve results.</p>            <p><strong>What is Novel:</strong> This law formalizes the mechanism as alignment with internal cognitive representations, rather than surface-level prompt familiarity, and frames it as a general principle.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format affects reasoning]</li>
    <li>Reynolds & McDonell (2021) Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm [Prompt format effects]</li>
    <li>Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt format and LLM behavior]</li>
</ul>
            <h3>Statement 1: Cognitive Friction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_misaligned_with &#8594; LLM_internal_representation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; degraded_performance_on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs struggle with tasks presented in unfamiliar or non-canonical formats, even when the underlying problem is unchanged. </li>
    <li>Empirical studies show that minor changes in prompt structure (e.g., switching from question to instruction) can reduce accuracy. </li>
    <li>LLMs can fail on adversarially designed prompts that intentionally misalign with their training distribution. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The concept of 'cognitive friction' as a mediating variable is novel, though the empirical effect is known.</p>            <p><strong>What Already Exists:</strong> Empirical evidence shows LLMs are sensitive to prompt format and can fail on unfamiliar presentations.</p>            <p><strong>What is Novel:</strong> The law frames this as a result of cognitive friction due to representational misalignment, not just lack of exposure.</p>
            <p><strong>References:</strong> <ul>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt format sensitivity]</li>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format and calibration effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a problem is reformulated to match the most common format in the LLM's pretraining data, performance will increase relative to an uncommon format.</li>
                <li>If a new LLM is pretrained on a corpus with a novel problem format, it will perform better on that format than LLMs without such exposure.</li>
                <li>If a prompt is engineered to mimic the structure of high-frequency training data, LLM accuracy will improve.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LLM is exposed to a hybrid format (combining two familiar but distinct formats), performance may be non-monotonic, potentially showing interference effects.</li>
                <li>If an LLM is fine-tuned on adversarially misaligned formats, it may develop new internal representations that generalize to other misaligned formats.</li>
                <li>If LLMs are trained with explicit meta-learning objectives to handle format diversity, their sensitivity to format alignment may decrease.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on all problem formats regardless of pretraining exposure, this theory would be called into question.</li>
                <li>If LLMs show no performance drop on highly misaligned or adversarial formats, the cognitive friction law would be falsified.</li>
                <li>If LLMs can generalize perfectly to unseen formats without additional training, the alignment mechanism may not be the primary driver.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs perform well on entirely novel formats due to emergent generalization are not fully explained. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and abstracts existing prompt format effects into a general cognitive alignment framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning]</li>
    <li>Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt format and LLM behavior]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt format sensitivity]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that the performance of large language models (LLMs) on problem-solving tasks is determined by the degree to which the presentation format of a problem aligns with the model's internal cognitive representations, which are shaped by its pretraining data and architectural biases. Formats that closely match these internal representations facilitate more effective retrieval, reasoning, and generation, while misaligned formats introduce cognitive friction, leading to degraded performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Representation Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_aligned_with",
                        "object": "LLM_internal_representation"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "enhanced_performance_on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on tasks when the input format resembles the data distributions seen during pretraining (e.g., natural language Q&A, code snippets).",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering that reformulates questions into more familiar or canonical forms improves LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "Chain-of-thought prompting, which mimics step-by-step reasoning formats common in training data, improves LLM reasoning performance.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs show higher accuracy on tasks when the prompt format matches the style of their pretraining corpus (e.g., code models excel with code-formatted prompts).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering and format effects are known to influence LLM performance, with empirical studies showing that rephrasing or formatting can improve results.",
                    "what_is_novel": "This law formalizes the mechanism as alignment with internal cognitive representations, rather than surface-level prompt familiarity, and frames it as a general principle.",
                    "classification_explanation": "While prompt engineering is well-studied, the explicit framing of performance as a function of alignment with internal representations is a novel abstraction.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format affects reasoning]",
                        "Reynolds & McDonell (2021) Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm [Prompt format effects]",
                        "Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt format and LLM behavior]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Cognitive Friction Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_misaligned_with",
                        "object": "LLM_internal_representation"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "degraded_performance_on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs struggle with tasks presented in unfamiliar or non-canonical formats, even when the underlying problem is unchanged.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that minor changes in prompt structure (e.g., switching from question to instruction) can reduce accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can fail on adversarially designed prompts that intentionally misalign with their training distribution.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical evidence shows LLMs are sensitive to prompt format and can fail on unfamiliar presentations.",
                    "what_is_novel": "The law frames this as a result of cognitive friction due to representational misalignment, not just lack of exposure.",
                    "classification_explanation": "The concept of 'cognitive friction' as a mediating variable is novel, though the empirical effect is known.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt format sensitivity]",
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt format and calibration effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a problem is reformulated to match the most common format in the LLM's pretraining data, performance will increase relative to an uncommon format.",
        "If a new LLM is pretrained on a corpus with a novel problem format, it will perform better on that format than LLMs without such exposure.",
        "If a prompt is engineered to mimic the structure of high-frequency training data, LLM accuracy will improve."
    ],
    "new_predictions_unknown": [
        "If an LLM is exposed to a hybrid format (combining two familiar but distinct formats), performance may be non-monotonic, potentially showing interference effects.",
        "If an LLM is fine-tuned on adversarially misaligned formats, it may develop new internal representations that generalize to other misaligned formats.",
        "If LLMs are trained with explicit meta-learning objectives to handle format diversity, their sensitivity to format alignment may decrease."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on all problem formats regardless of pretraining exposure, this theory would be called into question.",
        "If LLMs show no performance drop on highly misaligned or adversarial formats, the cognitive friction law would be falsified.",
        "If LLMs can generalize perfectly to unseen formats without additional training, the alignment mechanism may not be the primary driver."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs perform well on entirely novel formats due to emergent generalization are not fully explained.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robust performance on certain tasks regardless of format, possibly due to scale or architectural advances.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very large LLMs may develop sufficiently abstract representations to reduce format sensitivity.",
        "Tasks with extremely low ambiguity may be less affected by presentation format.",
        "Explicit instruction-following models may be less sensitive to format misalignment."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and format effects are well-documented, and some work discusses alignment with training data.",
        "what_is_novel": "The explicit theory of cognitive alignment and friction as mediators of LLM performance is new.",
        "classification_explanation": "The theory synthesizes and abstracts existing prompt format effects into a general cognitive alignment framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning]",
            "Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt format and LLM behavior]",
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt format sensitivity]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Engineering vs. Fine-Tuning Trade-off Theory for Literature Synthesis - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-570</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-570</p>
                <p><strong>Name:</strong> Prompt Engineering vs. Fine-Tuning Trade-off Theory for Literature Synthesis</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can be used to distill theories from examining large numbers of scholarly papers on a specific topic, based on the following results.</p>
                <p><strong>Description:</strong> For literature synthesis tasks, the choice between prompt engineering and fine-tuning depends on a multi-dimensional trade-off involving task complexity, data availability, computational resources, model scale, and required performance level. There exist predictable thresholds where fine-tuning becomes necessary and cost-effective. Parameter-efficient fine-tuning methods (LoRA, QLoRA) can achieve near-full-fine-tuning performance while dramatically reducing computational costs, making fine-tuning viable for resource-constrained scenarios. The relative benefits of each approach interact with model scale, with smaller models benefiting more from fine-tuning while larger models show stronger prompting capabilities.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Task Complexity Threshold Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; requires &#8594; multi-step reasoning or domain-specific knowledge or implicit attribute control<span style="color: #888888;">, and</span></div>
        <div>&#8226; task complexity &#8594; exceeds &#8594; threshold T (characterized by need for specialized domain knowledge or multi-hop inference)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; fine-tuning &#8594; outperforms &#8594; prompt engineering by 10-30 percentage points<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt engineering &#8594; reaches &#8594; performance plateau below acceptable level</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Fine-tuning improved classification accuracy by ~10–25 percentage points over the original (pre-fine-tune) Llama 3.2 3B initial labels. <a href="../results/extraction-result-4378.html#e4378.0" class="evidence-link">[e4378.0]</a> </li>
    <li>Prompt engineering of a strong LLM can approximate agent behaviors but RL fine-tuning provides measurable additional gains for thorough literature crawling and selection. <a href="../results/extraction-result-4609.html#e4609.2" class="evidence-link">[e4609.2]</a> </li>
    <li>SFT+RLAIF (w/ GPT-4 Features) outperformed Vanilla and SFT across relevancy, correctness, completeness, integration, cohesion, and coherence <a href="../results/extraction-result-4431.html#e4431.6" class="evidence-link">[e4431.6]</a> </li>
    <li>Domain-specific pretraining (SciBERT) for the encoder yields measurable improvements in ROUGE-L and helps mitigate domain shift seen with general-domain pretrained models like BART. <a href="../results/extraction-result-4395.html#e4395.5" class="evidence-link">[e4395.5]</a> </li>
    <li>PaSa-GPT-4o achieved lower recall than PaSa-7b (example: PaSa-GPT-4o recall 0.3873 vs PaSa-7b 0.4834 on AutoScholarQuery per paper results), and was outperformed notably on RealScholarQuery (PaSa-7b surpassed PaSa-GPT-4o by ~30.36% recall). <a href="../results/extraction-result-4609.html#e4609.2" class="evidence-link">[e4609.2]</a> </li>
    <li>Applying AGILE-style RL (session-level PPO) to PaSa produced measurable gains: RL training improved recall by ~6.24% on AutoScholarQuery and ~19.96% on RealScholarQuery compared to imitation-only training. <a href="../results/extraction-result-4609.html#e4609.1" class="evidence-link">[e4609.1]</a> </li>
    <li>Fine-tuned model substantially outperformed initial Llama 3.2 3B predictions (accuracy improvement 10–25 percentage points) and produced more relevant search results than keyword-based queries <a href="../results/extraction-result-4378.html#e4378.0" class="evidence-link">[e4378.0]</a> </li>
    <li>LoRA fine-tuned model dramatically outperformed baseline (87.7% SUPPORTED vs 14.5% baseline). Slightly lower than NEFTune (89.2%). <a href="../results/extraction-result-4392.html#e4392.4" class="evidence-link">[e4392.4]</a> </li>
    <li>Encoder-decoder LoRA models (Flan-T5-XXL/B2) achieved results comparable to or surpassing GPT-4 on many metrics; decoder-only LoRA models showed only minor gains and underperformed relative to encoder-decoder LoRA models. <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>Even with LoRA, models struggle with implicit attributes and balancing attribute trade-offs; decoder-only architectures still suffer from degraded long-range attention and weak compositional control. <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>SFT improved some metrics vs Vanilla (e.g., lower paper-structure incidents), but combining SFT with RLAIF yields much larger quality gains. <a href="../results/extraction-result-4431.html#e4431.7" class="evidence-link">[e4431.7]</a> </li>
    <li>SFT alone insufficient to enforce multi-dimensional quality constraints (e.g., conciseness, integration) without RL and reward shaping. <a href="../results/extraction-result-4431.html#e4431.7" class="evidence-link">[e4431.7]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> While the trade-off between prompting and fine-tuning is known in NLP, the specific quantification of a 10-30 percentage point performance gap for literature synthesis tasks and the identification of task complexity thresholds (multi-step reasoning, domain knowledge, implicit attributes) represents a novel empirical finding from these studies.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [prompting capabilities]</li>
    <li>Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers [fine-tuning benefits]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [complex reasoning with prompting]</li>
</ul>
            <h3>Statement 1: Data Efficiency Crossover Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; available training data &#8594; has_size &#8594; N examples<span style="color: #888888;">, and</span></div>
        <div>&#8226; N &#8594; greater_than &#8594; 1000-5000 (depending on task complexity)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; fine-tuning &#8594; becomes_more_cost_effective_than &#8594; prompt engineering<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance per dollar &#8594; favors &#8594; fine-tuned models<span style="color: #888888;">, and</span></div>
        <div>&#8226; binary classification tasks &#8594; reach_acceptable_performance_with &#8594; fewer than 3000 samples<span style="color: #888888;">, and</span></div>
        <div>&#8226; multi-label tasks &#8594; require &#8594; 5000+ samples for 80% performance</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Binary classification tasks reached >80% accuracy with fewer than ~3,000 training samples; one multi-label task (Question 4) required >5,000 samples to reach ~80% Jaccard Index <a href="../results/extraction-result-4378.html#e4378.0" class="evidence-link">[e4378.0]</a> </li>
    <li>Fine-tuned on the Q&A dataset derived from 17 papers (augmented to 16,423 training items). <a href="../results/extraction-result-4392.html#e4392.4" class="evidence-link">[e4392.4]</a> </li>
    <li>Trained on standardized prompts representing five-paper groups; Train-LLM split contained 405 synthesis prompts (from 135 comparisons). <a href="../results/extraction-result-4431.html#e4431.7" class="evidence-link">[e4431.7]</a> </li>
    <li>Evaluated on 518 citation-graph instances (each a literature-review's citation network); retrieval phase uses top-200 retrieved papers <a href="../results/extraction-result-4396.html#e4396.0" class="evidence-link">[e4396.0]</a> </li>
    <li>Binary/explicit labels are learned with much less data than complex, implicit multi-label concepts; aggregation of structured outputs enables clustering and network analyses that reveal topical trends and gaps. <a href="../results/extraction-result-4378.html#e4378.0" class="evidence-link">[e4378.0]</a> </li>
    <li>Empirical scaling observations: binary tasks stabilize at high accuracy with relatively small training sizes (<3k), while multi-label tasks show gradual accuracy improvements with larger training data (Question 4 needed >5k to reach ~80% JAC). <a href="../results/extraction-result-4378.html#e4378.0" class="evidence-link">[e4378.0]</a> </li>
    <li>System is designed to scale: fine-tuned model was applied to full 22,267 corpus and can be used on newly published papers. <a href="../results/extraction-result-4378.html#e4378.0" class="evidence-link">[e4378.0]</a> </li>
    <li>Effective on small (~17-paper) corpus with heavy paraphrase augmentation; authors note need for more SLR-level samples or larger models for improved cross-paper synthesis. <a href="../results/extraction-result-4392.html#e4392.4" class="evidence-link">[e4392.4]</a> </li>
    <li>Cost per question estimated ~$0.18 based on token usage and pricing at time of experiments. <a href="../results/extraction-result-4598.html#e4598.0" class="evidence-link">[e4598.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Data efficiency trade-offs are known in machine learning, but the specific identification of 1000-5000 examples as the crossover point (with task-dependent variation) where fine-tuning becomes more cost-effective than prompting for literature synthesis, and the distinction between binary (3000) and multi-label (5000+) thresholds, is a novel empirical finding.</p>
            <p><strong>References:</strong> <ul>
    <li>Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling laws]</li>
    <li>Hoffmann et al. (2022) Training Compute-Optimal Large Language Models [compute-optimal training]</li>
</ul>
            <h3>Statement 2: Model Scale Interaction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; base model &#8594; has_parameters &#8594; P<span style="color: #888888;">, and</span></div>
        <div>&#8226; P &#8594; less_than &#8594; 13B</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; fine-tuning benefit &#8594; increases_with &#8594; decreasing model size (relative improvement larger for smaller models)<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt engineering effectiveness &#8594; increases_with &#8594; increasing model size<span style="color: #888888;">, and</span></div>
        <div>&#8226; domain-adapted smaller models &#8594; can_outperform &#8594; larger general-purpose models on domain-specific tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>a 13B domain-adapted LLM can outperform larger general-purpose models on many literature-centric tasks while being more tractable to train/deploy. <a href="../results/extraction-result-4409.html#e4409.0" class="evidence-link">[e4409.0]</a> </li>
    <li>Encoder-decoder LoRA models (Flan-T5-XXL/B2) achieved results comparable to or surpassing GPT-4 on many metrics; decoder-only LoRA models showed only minor gains and underperformed relative to encoder-decoder LoRA models. <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>GPT-4 and larger models tend to follow plans more closely than smaller models. <a href="../results/extraction-result-4379.html#e4379.4" class="evidence-link">[e4379.4]</a> </li>
    <li>Larger encoder-decoder model (Flan-T5-XXL) with LoRA shows better compositional control than smaller variants <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>Sonnet 3.5 produced the highest-quality generated papers in these experiments; Llama-3.1 and cheaper models produced lower mean scores. <a href="../results/extraction-result-4599.html#e4599.0" class="evidence-link">[e4599.0]</a> </li>
    <li>Qualitatively, performance improves with more capable LLMs and with prompting strategies (self-reflection, few-shot); increasing compute/experiments and scaling the search/parallelization is expected to increase paper quality. <a href="../results/extraction-result-4599.html#e4599.0" class="evidence-link">[e4599.0]</a> </li>
    <li>Deepseek-v3 > GPT-4o-mini > Qwen-2.5 in aggregate performance <a href="../results/extraction-result-4594.html#e4594.0" class="evidence-link">[e4594.0]</a> </li>
    <li>Model choice scales performance (Deepseek-v3 > GPT-4o-mini > Qwen-2.5). <a href="../results/extraction-result-4594.html#e4594.0" class="evidence-link">[e4594.0]</a> </li>
    <li>Authors observed that adding GPT-4-based feedback improves metrics; larger LLMs (GPT-4) perform better initially, but RLAIF helps smaller open-source models (Mistral-7B) close the gap. <a href="../results/extraction-result-4431.html#e4431.6" class="evidence-link">[e4431.6]</a> </li>
    <li>The paper reports improved performance after continual pretraining on a very large scientific corpus (>10M papers) and shows that a 13B SciLit-LLM outperforms smaller or similarly sized open models <a href="../results/extraction-result-4409.html#e4409.0" class="evidence-link">[e4409.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The interaction between model scale and the relative benefits of prompting vs. fine-tuning is known in general, but the specific finding that domain-adapted models around 13B parameters can outperform larger general-purpose models, and that smaller models show larger relative improvements from fine-tuning while larger models are more effective with prompting for literature synthesis, is a novel empirical pattern.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Emergent Abilities of Large Language Models [scale and capabilities]</li>
    <li>Chung et al. (2022) Scaling Instruction-Finetuned Language Models [instruction tuning at scale]</li>
</ul>
            <h3>Statement 3: Parameter-Efficient Fine-Tuning Sufficiency Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; is &#8594; domain adaptation or style transfer or controllable generation<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; does_not_require &#8594; fundamental capability changes or complete model retraining</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; parameter-efficient fine-tuning (LoRA, QLoRA) &#8594; achieves_performance_within &#8594; 2-5% of full fine-tuning<span style="color: #888888;">, and</span></div>
        <div>&#8226; PEFT &#8594; reduces_computational_cost_by &#8594; 10-100x<span style="color: #888888;">, and</span></div>
        <div>&#8226; encoder-decoder architectures &#8594; benefit_more_from_PEFT_than &#8594; decoder-only architectures</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Using QLoRA makes SFT feasible on resource-constrained open models and provides a better initialization for subsequent RLAIF. <a href="../results/extraction-result-4431.html#e4431.7" class="evidence-link">[e4431.7]</a> </li>
    <li>LoRA is an effective, compute-efficient PEFT approach for small-domain SLR fine-tuning, delivering high factual fidelity at paper and paper-summary levels. <a href="../results/extraction-result-4392.html#e4392.4" class="evidence-link">[e4392.4]</a> </li>
    <li>LoRA PEFT improves controllable summarization performance; encoder-decoder architectures benefit more from PEFT than decoder-only models for compositional control <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>Uses parameter-efficient fine-tuning (LoRA) to keep LLM adaptation tractable <a href="../results/extraction-result-4396.html#e4396.0" class="evidence-link">[e4396.0]</a> </li>
    <li>PEFT (LoRA) results reported in paper: e.g., Flan-T5-XL (B1) ROUGE-L 22.22, PCC 0.49, MAD 0.55, SR 0.78, FKGL normal 13.11 / high 9.37 (delta 3.59), Focus F1 0.70; Flan-T5-XXL (B2) ROUGE-L 23.43, PCC 0.78, MAD 0.27, SR 0.85, FKGL normal 14.40 / high 10.78 (delta 3.62), Focus F1 0.75 <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>Encoder-decoder LoRA models (Flan-T5-XXL/B2) achieved results comparable to or surpassing GPT-4 on many metrics; decoder-only LoRA models showed only minor gains and underperformed relative to encoder-decoder LoRA models. <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>LoRA fine-tuning improves controllable summarization performance; encoder-decoder architectures benefit more from PEFT than decoder-only models for compositional control, particularly on implicit attributes (readability, focus). <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>May require hyperparameter tuning; no ablation on rank size reported; SLR-level synthesis still weaker than paper-level even with LoRA. <a href="../results/extraction-result-4392.html#e4392.4" class="evidence-link">[e4392.4]</a> </li>
    <li>Applies low-rank matrices to a subset of weights (injecting trainable low-rank updates) so that most original model parameters stay frozen, enabling efficient adaptation to the SLR Q&A dataset with reduced GPU/time requirements. <a href="../results/extraction-result-4392.html#e4392.4" class="evidence-link">[e4392.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Parameter-efficient fine-tuning methods like LoRA are known, but the specific finding that PEFT achieves within 2-5% of full fine-tuning performance while reducing costs by 10-100x for literature synthesis tasks, and that encoder-decoder architectures benefit more than decoder-only architectures, is a novel empirical quantification.</p>
            <p><strong>References:</strong> <ul>
    <li>Hu et al. (2021) LoRA: Low-Rank Adaptation of Large Language Models [LoRA method]</li>
    <li>Dettmers et al. (2023) QLoRA: Efficient Finetuning of Quantized LLMs [QLoRA method]</li>
</ul>
            <h3>Statement 4: Domain Pretraining Enhancement Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; model &#8594; undergoes &#8594; domain-specific continual pretraining on large scientific corpus<span style="color: #888888;">, and</span></div>
        <div>&#8226; corpus size &#8594; greater_than &#8594; 10M papers</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; domain-adapted model &#8594; outperforms &#8594; general-purpose models of similar or larger size on domain tasks<span style="color: #888888;">, and</span></div>
        <div>&#8226; hallucination rate &#8594; decreases &#8594; compared to general-purpose baseline<span style="color: #888888;">, and</span></div>
        <div>&#8226; factuality and informativeness &#8594; increase &#8594; on domain-specific tasks</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Domain continual pretraining on large scientific corpora combined with supervised fine-tuning and retrieval augmentation yields substantial gains in factuality and informativeness for paper-reading tasks <a href="../results/extraction-result-4409.html#e4409.0" class="evidence-link">[e4409.0]</a> </li>
    <li>The paper reports improved performance after continual pretraining on a very large scientific corpus (>10M papers) and shows that a 13B SciLit-LLM outperforms smaller or similarly sized open models; no detailed quantitative scaling curve provided, but authors note better rejection behavior and reduced hallucination after domain continued pretraining. <a href="../results/extraction-result-4409.html#e4409.0" class="evidence-link">[e4409.0]</a> </li>
    <li>Domain-specific pretraining (SciBERT) for the encoder yields measurable improvements in ROUGE-L and helps mitigate domain shift seen with general-domain pretrained models like BART. <a href="../results/extraction-result-4395.html#e4395.5" class="evidence-link">[e4395.5]</a> </li>
    <li>SciBERTAbs outperformed BART and BertABS in ROUGE-L on this dataset, showing an improvement in ROUGE-L compared to BertABS and BART. <a href="../results/extraction-result-4395.html#e4395.5" class="evidence-link">[e4395.5]</a> </li>
    <li>Domain-specific pretraining (SciBERT) for the encoder yields measurable improvements in ROUGE-L and helps mitigate domain shift seen with general-domain pretrained models like BART. <a href="../results/extraction-result-4395.html#e4395.5" class="evidence-link">[e4395.5]</a> </li>
    <li>Paper reading (SparkRA): Factuality 4.68, Informativeness 4.45, Avg 4.57 (highest among compared models). <a href="../results/extraction-result-4409.html#e4409.0" class="evidence-link">[e4409.0]</a> </li>
    <li>SparkRA (SciLit-LLM 13B) achieved the highest average scores in paper reading (4.57 vs GPT-4 4.55) and paper polishing (4.49 vs GPT-4 4.32). <a href="../results/extraction-result-4409.html#e4409.0" class="evidence-link">[e4409.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Domain-specific pretraining is a known technique, but the specific finding that continual pretraining on 10M+ scientific papers enables smaller models (13B) to outperform larger general-purpose models on literature tasks, with measurable improvements in factuality and reduced hallucination, represents a novel empirical contribution.</p>
            <p><strong>References:</strong> <ul>
    <li>Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [domain-specific pretraining]</li>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [continued pretraining benefits]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For literature synthesis tasks requiring domain-specific knowledge, fine-tuning a 7B model with 3000+ examples will outperform prompting a 70B model when the task involves implicit attributes or multi-step reasoning.</li>
                <li>Parameter-efficient fine-tuning (LoRA) on encoder-decoder architectures will achieve 95-98% of full fine-tuning performance while using only 1-5% of the computational resources for literature synthesis tasks.</li>
                <li>Domain-specific continual pretraining on 10M+ scientific papers will enable a 13B model to match or exceed the performance of a 70B general-purpose model on scientific literature tasks.</li>
                <li>The performance gap between prompting and fine-tuning will be largest (20-30 percentage points) for implicit, multi-step reasoning tasks and smallest (5-10 percentage points) for explicit extraction tasks.</li>
                <li>Binary classification tasks in literature synthesis will reach >80% accuracy with 2000-3000 training examples, while multi-label tasks will require 5000+ examples.</li>
                <li>For models below 13B parameters, fine-tuning will provide 2-3x larger relative performance improvements than for models above 50B parameters on the same literature synthesis tasks.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a model scale (e.g., 1T+ parameters) where prompting consistently matches or exceeds fine-tuning performance across all literature synthesis tasks remains unknown and could fundamentally change system design principles.</li>
                <li>The optimal allocation of computational budget between scaling model size vs. fine-tuning smaller models vs. domain pretraining for literature synthesis is unclear and may vary significantly by task and domain.</li>
                <li>Whether hybrid approaches that combine prompting and fine-tuning (e.g., prompt-tuning, prefix-tuning, instruction tuning) can achieve the best of both worlds is unknown but could offer important practical benefits.</li>
                <li>The extent to which fine-tuning on synthetic data generated by larger models can match fine-tuning on human-labeled data for literature synthesis is unclear but could dramatically reduce annotation costs.</li>
                <li>Whether the crossover point for cost-effectiveness varies significantly across different scientific domains (e.g., biomedicine vs. computer science) is unknown but could impact deployment strategies.</li>
                <li>The long-term stability and generalization of fine-tuned models as literature evolves over time is unclear and could affect the viability of fine-tuning for production systems.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that prompting consistently outperforms fine-tuning even with abundant training data (10,000+ examples) would challenge the Task Complexity Threshold Law and Data Efficiency Crossover Law.</li>
                <li>Demonstrating that the crossover point for cost-effectiveness occurs at much higher data volumes (>10,000 examples) or much lower volumes (<500 examples) would challenge the Data Efficiency Crossover Law.</li>
                <li>Showing that model scale does not interact with fine-tuning benefits (i.e., all model sizes benefit equally from fine-tuning) would challenge the Model Scale Interaction Law.</li>
                <li>Finding that parameter-efficient fine-tuning consistently underperforms full fine-tuning by more than 10% would challenge the Parameter-Efficient Fine-Tuning Sufficiency Law.</li>
                <li>Demonstrating that domain pretraining provides no measurable benefit over general pretraining for literature synthesis tasks would challenge the Domain Pretraining Enhancement Law.</li>
                <li>Finding that decoder-only architectures benefit more from PEFT than encoder-decoder architectures would contradict the Parameter-Efficient Fine-Tuning Sufficiency Law.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not explain how to predict task complexity thresholds a priori without empirical testing, making it difficult to decide between prompting and fine-tuning before implementation. <a href="../results/extraction-result-4378.html#e4378.0" class="evidence-link">[e4378.0]</a> <a href="../results/extraction-result-4609.html#e4609.2" class="evidence-link">[e4609.2]</a> <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>The role of instruction tuning as an intermediate approach between prompting and task-specific fine-tuning is not fully addressed in the theory. <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> <a href="../results/extraction-result-4409.html#e4409.0" class="evidence-link">[e4409.0]</a> </li>
    <li>The theory does not account for continual learning scenarios where models need to adapt to evolving literature over time without catastrophic forgetting. <a href="../results/extraction-result-4378.html#e4378.0" class="evidence-link">[e4378.0]</a> </li>
    <li>The interaction between fine-tuning and retrieval augmentation (RAG) is not fully explained - whether they are complementary or substitutable. <a href="../results/extraction-result-4392.html#e4392.6" class="evidence-link">[e4392.6]</a> <a href="../results/extraction-result-4409.html#e4409.0" class="evidence-link">[e4409.0]</a> </li>
    <li>The theory does not address the trade-offs between different types of fine-tuning (supervised fine-tuning, reinforcement learning, instruction tuning) for literature synthesis. <a href="../results/extraction-result-4431.html#e4431.6" class="evidence-link">[e4431.6]</a> <a href="../results/extraction-result-4609.html#e4609.1" class="evidence-link">[e4609.1]</a> </li>
    <li>The impact of data augmentation strategies (e.g., paraphrasing, back-translation) on the data efficiency crossover point is not explained. <a href="../results/extraction-result-4392.html#e4392.4" class="evidence-link">[e4392.4]</a> </li>
    <li>The theory does not explain why encoder-decoder architectures benefit more from PEFT than decoder-only architectures at a mechanistic level. <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
    <li>The optimal hyperparameters for PEFT (e.g., LoRA rank, scaling factor) and how they vary by task are not addressed. <a href="../results/extraction-result-4392.html#e4392.4" class="evidence-link">[e4392.4]</a> <a href="../results/extraction-result-4394.html#e4394.1" class="evidence-link">[e4394.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes known concepts about prompting and fine-tuning into a unified framework with novel quantitative thresholds and interaction effects specific to literature synthesis. While the general trade-off between prompting and fine-tuning is known in NLP, the specific laws (Task Complexity Threshold, Data Efficiency Crossover, Model Scale Interaction, PEFT Sufficiency, Domain Pretraining Enhancement) with their quantitative thresholds (10-30 pp improvement, 1000-5000 examples, 13B parameter sweet spot, 2-5% PEFT gap, 10M+ papers for domain pretraining) represent new empirical contributions derived from literature synthesis systems.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [prompting capabilities]</li>
    <li>Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers [fine-tuning benefits]</li>
    <li>Hu et al. (2021) LoRA: Low-Rank Adaptation of Large Language Models [parameter-efficient fine-tuning]</li>
    <li>Wei et al. (2022) Emergent Abilities of Large Language Models [scale effects]</li>
    <li>Chung et al. (2022) Scaling Instruction-Finetuned Language Models [instruction tuning]</li>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt Engineering vs. Fine-Tuning Trade-off Theory for Literature Synthesis",
    "theory_description": "For literature synthesis tasks, the choice between prompt engineering and fine-tuning depends on a multi-dimensional trade-off involving task complexity, data availability, computational resources, model scale, and required performance level. There exist predictable thresholds where fine-tuning becomes necessary and cost-effective. Parameter-efficient fine-tuning methods (LoRA, QLoRA) can achieve near-full-fine-tuning performance while dramatically reducing computational costs, making fine-tuning viable for resource-constrained scenarios. The relative benefits of each approach interact with model scale, with smaller models benefiting more from fine-tuning while larger models show stronger prompting capabilities.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Task Complexity Threshold Law",
                "if": [
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "multi-step reasoning or domain-specific knowledge or implicit attribute control"
                    },
                    {
                        "subject": "task complexity",
                        "relation": "exceeds",
                        "object": "threshold T (characterized by need for specialized domain knowledge or multi-hop inference)"
                    }
                ],
                "then": [
                    {
                        "subject": "fine-tuning",
                        "relation": "outperforms",
                        "object": "prompt engineering by 10-30 percentage points"
                    },
                    {
                        "subject": "prompt engineering",
                        "relation": "reaches",
                        "object": "performance plateau below acceptable level"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Fine-tuning improved classification accuracy by ~10–25 percentage points over the original (pre-fine-tune) Llama 3.2 3B initial labels.",
                        "uuids": [
                            "e4378.0"
                        ]
                    },
                    {
                        "text": "Prompt engineering of a strong LLM can approximate agent behaviors but RL fine-tuning provides measurable additional gains for thorough literature crawling and selection.",
                        "uuids": [
                            "e4609.2"
                        ]
                    },
                    {
                        "text": "SFT+RLAIF (w/ GPT-4 Features) outperformed Vanilla and SFT across relevancy, correctness, completeness, integration, cohesion, and coherence",
                        "uuids": [
                            "e4431.6"
                        ]
                    },
                    {
                        "text": "Domain-specific pretraining (SciBERT) for the encoder yields measurable improvements in ROUGE-L and helps mitigate domain shift seen with general-domain pretrained models like BART.",
                        "uuids": [
                            "e4395.5"
                        ]
                    },
                    {
                        "text": "PaSa-GPT-4o achieved lower recall than PaSa-7b (example: PaSa-GPT-4o recall 0.3873 vs PaSa-7b 0.4834 on AutoScholarQuery per paper results), and was outperformed notably on RealScholarQuery (PaSa-7b surpassed PaSa-GPT-4o by ~30.36% recall).",
                        "uuids": [
                            "e4609.2"
                        ]
                    },
                    {
                        "text": "Applying AGILE-style RL (session-level PPO) to PaSa produced measurable gains: RL training improved recall by ~6.24% on AutoScholarQuery and ~19.96% on RealScholarQuery compared to imitation-only training.",
                        "uuids": [
                            "e4609.1"
                        ]
                    },
                    {
                        "text": "Fine-tuned model substantially outperformed initial Llama 3.2 3B predictions (accuracy improvement 10–25 percentage points) and produced more relevant search results than keyword-based queries",
                        "uuids": [
                            "e4378.0"
                        ]
                    },
                    {
                        "text": "LoRA fine-tuned model dramatically outperformed baseline (87.7% SUPPORTED vs 14.5% baseline). Slightly lower than NEFTune (89.2%).",
                        "uuids": [
                            "e4392.4"
                        ]
                    },
                    {
                        "text": "Encoder-decoder LoRA models (Flan-T5-XXL/B2) achieved results comparable to or surpassing GPT-4 on many metrics; decoder-only LoRA models showed only minor gains and underperformed relative to encoder-decoder LoRA models.",
                        "uuids": [
                            "e4394.1"
                        ]
                    },
                    {
                        "text": "Even with LoRA, models struggle with implicit attributes and balancing attribute trade-offs; decoder-only architectures still suffer from degraded long-range attention and weak compositional control.",
                        "uuids": [
                            "e4394.1"
                        ]
                    },
                    {
                        "text": "SFT improved some metrics vs Vanilla (e.g., lower paper-structure incidents), but combining SFT with RLAIF yields much larger quality gains.",
                        "uuids": [
                            "e4431.7"
                        ]
                    },
                    {
                        "text": "SFT alone insufficient to enforce multi-dimensional quality constraints (e.g., conciseness, integration) without RL and reward shaping.",
                        "uuids": [
                            "e4431.7"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "While the trade-off between prompting and fine-tuning is known in NLP, the specific quantification of a 10-30 percentage point performance gap for literature synthesis tasks and the identification of task complexity thresholds (multi-step reasoning, domain knowledge, implicit attributes) represents a novel empirical finding from these studies.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [prompting capabilities]",
                        "Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers [fine-tuning benefits]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [complex reasoning with prompting]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Data Efficiency Crossover Law",
                "if": [
                    {
                        "subject": "available training data",
                        "relation": "has_size",
                        "object": "N examples"
                    },
                    {
                        "subject": "N",
                        "relation": "greater_than",
                        "object": "1000-5000 (depending on task complexity)"
                    }
                ],
                "then": [
                    {
                        "subject": "fine-tuning",
                        "relation": "becomes_more_cost_effective_than",
                        "object": "prompt engineering"
                    },
                    {
                        "subject": "performance per dollar",
                        "relation": "favors",
                        "object": "fine-tuned models"
                    },
                    {
                        "subject": "binary classification tasks",
                        "relation": "reach_acceptable_performance_with",
                        "object": "fewer than 3000 samples"
                    },
                    {
                        "subject": "multi-label tasks",
                        "relation": "require",
                        "object": "5000+ samples for 80% performance"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Binary classification tasks reached &gt;80% accuracy with fewer than ~3,000 training samples; one multi-label task (Question 4) required &gt;5,000 samples to reach ~80% Jaccard Index",
                        "uuids": [
                            "e4378.0"
                        ]
                    },
                    {
                        "text": "Fine-tuned on the Q&A dataset derived from 17 papers (augmented to 16,423 training items).",
                        "uuids": [
                            "e4392.4"
                        ]
                    },
                    {
                        "text": "Trained on standardized prompts representing five-paper groups; Train-LLM split contained 405 synthesis prompts (from 135 comparisons).",
                        "uuids": [
                            "e4431.7"
                        ]
                    },
                    {
                        "text": "Evaluated on 518 citation-graph instances (each a literature-review's citation network); retrieval phase uses top-200 retrieved papers",
                        "uuids": [
                            "e4396.0"
                        ]
                    },
                    {
                        "text": "Binary/explicit labels are learned with much less data than complex, implicit multi-label concepts; aggregation of structured outputs enables clustering and network analyses that reveal topical trends and gaps.",
                        "uuids": [
                            "e4378.0"
                        ]
                    },
                    {
                        "text": "Empirical scaling observations: binary tasks stabilize at high accuracy with relatively small training sizes (&lt;3k), while multi-label tasks show gradual accuracy improvements with larger training data (Question 4 needed &gt;5k to reach ~80% JAC).",
                        "uuids": [
                            "e4378.0"
                        ]
                    },
                    {
                        "text": "System is designed to scale: fine-tuned model was applied to full 22,267 corpus and can be used on newly published papers.",
                        "uuids": [
                            "e4378.0"
                        ]
                    },
                    {
                        "text": "Effective on small (~17-paper) corpus with heavy paraphrase augmentation; authors note need for more SLR-level samples or larger models for improved cross-paper synthesis.",
                        "uuids": [
                            "e4392.4"
                        ]
                    },
                    {
                        "text": "Cost per question estimated ~$0.18 based on token usage and pricing at time of experiments.",
                        "uuids": [
                            "e4598.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "Data efficiency trade-offs are known in machine learning, but the specific identification of 1000-5000 examples as the crossover point (with task-dependent variation) where fine-tuning becomes more cost-effective than prompting for literature synthesis, and the distinction between binary (3000) and multi-label (5000+) thresholds, is a novel empirical finding.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling laws]",
                        "Hoffmann et al. (2022) Training Compute-Optimal Large Language Models [compute-optimal training]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Model Scale Interaction Law",
                "if": [
                    {
                        "subject": "base model",
                        "relation": "has_parameters",
                        "object": "P"
                    },
                    {
                        "subject": "P",
                        "relation": "less_than",
                        "object": "13B"
                    }
                ],
                "then": [
                    {
                        "subject": "fine-tuning benefit",
                        "relation": "increases_with",
                        "object": "decreasing model size (relative improvement larger for smaller models)"
                    },
                    {
                        "subject": "prompt engineering effectiveness",
                        "relation": "increases_with",
                        "object": "increasing model size"
                    },
                    {
                        "subject": "domain-adapted smaller models",
                        "relation": "can_outperform",
                        "object": "larger general-purpose models on domain-specific tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "a 13B domain-adapted LLM can outperform larger general-purpose models on many literature-centric tasks while being more tractable to train/deploy.",
                        "uuids": [
                            "e4409.0"
                        ]
                    },
                    {
                        "text": "Encoder-decoder LoRA models (Flan-T5-XXL/B2) achieved results comparable to or surpassing GPT-4 on many metrics; decoder-only LoRA models showed only minor gains and underperformed relative to encoder-decoder LoRA models.",
                        "uuids": [
                            "e4394.1"
                        ]
                    },
                    {
                        "text": "GPT-4 and larger models tend to follow plans more closely than smaller models.",
                        "uuids": [
                            "e4379.4"
                        ]
                    },
                    {
                        "text": "Larger encoder-decoder model (Flan-T5-XXL) with LoRA shows better compositional control than smaller variants",
                        "uuids": [
                            "e4394.1"
                        ]
                    },
                    {
                        "text": "Sonnet 3.5 produced the highest-quality generated papers in these experiments; Llama-3.1 and cheaper models produced lower mean scores.",
                        "uuids": [
                            "e4599.0"
                        ]
                    },
                    {
                        "text": "Qualitatively, performance improves with more capable LLMs and with prompting strategies (self-reflection, few-shot); increasing compute/experiments and scaling the search/parallelization is expected to increase paper quality.",
                        "uuids": [
                            "e4599.0"
                        ]
                    },
                    {
                        "text": "Deepseek-v3 &gt; GPT-4o-mini &gt; Qwen-2.5 in aggregate performance",
                        "uuids": [
                            "e4594.0"
                        ]
                    },
                    {
                        "text": "Model choice scales performance (Deepseek-v3 &gt; GPT-4o-mini &gt; Qwen-2.5).",
                        "uuids": [
                            "e4594.0"
                        ]
                    },
                    {
                        "text": "Authors observed that adding GPT-4-based feedback improves metrics; larger LLMs (GPT-4) perform better initially, but RLAIF helps smaller open-source models (Mistral-7B) close the gap.",
                        "uuids": [
                            "e4431.6"
                        ]
                    },
                    {
                        "text": "The paper reports improved performance after continual pretraining on a very large scientific corpus (&gt;10M papers) and shows that a 13B SciLit-LLM outperforms smaller or similarly sized open models",
                        "uuids": [
                            "e4409.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "The interaction between model scale and the relative benefits of prompting vs. fine-tuning is known in general, but the specific finding that domain-adapted models around 13B parameters can outperform larger general-purpose models, and that smaller models show larger relative improvements from fine-tuning while larger models are more effective with prompting for literature synthesis, is a novel empirical pattern.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Emergent Abilities of Large Language Models [scale and capabilities]",
                        "Chung et al. (2022) Scaling Instruction-Finetuned Language Models [instruction tuning at scale]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Parameter-Efficient Fine-Tuning Sufficiency Law",
                "if": [
                    {
                        "subject": "task",
                        "relation": "is",
                        "object": "domain adaptation or style transfer or controllable generation"
                    },
                    {
                        "subject": "task",
                        "relation": "does_not_require",
                        "object": "fundamental capability changes or complete model retraining"
                    }
                ],
                "then": [
                    {
                        "subject": "parameter-efficient fine-tuning (LoRA, QLoRA)",
                        "relation": "achieves_performance_within",
                        "object": "2-5% of full fine-tuning"
                    },
                    {
                        "subject": "PEFT",
                        "relation": "reduces_computational_cost_by",
                        "object": "10-100x"
                    },
                    {
                        "subject": "encoder-decoder architectures",
                        "relation": "benefit_more_from_PEFT_than",
                        "object": "decoder-only architectures"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Using QLoRA makes SFT feasible on resource-constrained open models and provides a better initialization for subsequent RLAIF.",
                        "uuids": [
                            "e4431.7"
                        ]
                    },
                    {
                        "text": "LoRA is an effective, compute-efficient PEFT approach for small-domain SLR fine-tuning, delivering high factual fidelity at paper and paper-summary levels.",
                        "uuids": [
                            "e4392.4"
                        ]
                    },
                    {
                        "text": "LoRA PEFT improves controllable summarization performance; encoder-decoder architectures benefit more from PEFT than decoder-only models for compositional control",
                        "uuids": [
                            "e4394.1"
                        ]
                    },
                    {
                        "text": "Uses parameter-efficient fine-tuning (LoRA) to keep LLM adaptation tractable",
                        "uuids": [
                            "e4396.0"
                        ]
                    },
                    {
                        "text": "PEFT (LoRA) results reported in paper: e.g., Flan-T5-XL (B1) ROUGE-L 22.22, PCC 0.49, MAD 0.55, SR 0.78, FKGL normal 13.11 / high 9.37 (delta 3.59), Focus F1 0.70; Flan-T5-XXL (B2) ROUGE-L 23.43, PCC 0.78, MAD 0.27, SR 0.85, FKGL normal 14.40 / high 10.78 (delta 3.62), Focus F1 0.75",
                        "uuids": [
                            "e4394.1"
                        ]
                    },
                    {
                        "text": "Encoder-decoder LoRA models (Flan-T5-XXL/B2) achieved results comparable to or surpassing GPT-4 on many metrics; decoder-only LoRA models showed only minor gains and underperformed relative to encoder-decoder LoRA models.",
                        "uuids": [
                            "e4394.1"
                        ]
                    },
                    {
                        "text": "LoRA fine-tuning improves controllable summarization performance; encoder-decoder architectures benefit more from PEFT than decoder-only models for compositional control, particularly on implicit attributes (readability, focus).",
                        "uuids": [
                            "e4394.1"
                        ]
                    },
                    {
                        "text": "May require hyperparameter tuning; no ablation on rank size reported; SLR-level synthesis still weaker than paper-level even with LoRA.",
                        "uuids": [
                            "e4392.4"
                        ]
                    },
                    {
                        "text": "Applies low-rank matrices to a subset of weights (injecting trainable low-rank updates) so that most original model parameters stay frozen, enabling efficient adaptation to the SLR Q&A dataset with reduced GPU/time requirements.",
                        "uuids": [
                            "e4392.4"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "Parameter-efficient fine-tuning methods like LoRA are known, but the specific finding that PEFT achieves within 2-5% of full fine-tuning performance while reducing costs by 10-100x for literature synthesis tasks, and that encoder-decoder architectures benefit more than decoder-only architectures, is a novel empirical quantification.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Hu et al. (2021) LoRA: Low-Rank Adaptation of Large Language Models [LoRA method]",
                        "Dettmers et al. (2023) QLoRA: Efficient Finetuning of Quantized LLMs [QLoRA method]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Domain Pretraining Enhancement Law",
                "if": [
                    {
                        "subject": "model",
                        "relation": "undergoes",
                        "object": "domain-specific continual pretraining on large scientific corpus"
                    },
                    {
                        "subject": "corpus size",
                        "relation": "greater_than",
                        "object": "10M papers"
                    }
                ],
                "then": [
                    {
                        "subject": "domain-adapted model",
                        "relation": "outperforms",
                        "object": "general-purpose models of similar or larger size on domain tasks"
                    },
                    {
                        "subject": "hallucination rate",
                        "relation": "decreases",
                        "object": "compared to general-purpose baseline"
                    },
                    {
                        "subject": "factuality and informativeness",
                        "relation": "increase",
                        "object": "on domain-specific tasks"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Domain continual pretraining on large scientific corpora combined with supervised fine-tuning and retrieval augmentation yields substantial gains in factuality and informativeness for paper-reading tasks",
                        "uuids": [
                            "e4409.0"
                        ]
                    },
                    {
                        "text": "The paper reports improved performance after continual pretraining on a very large scientific corpus (&gt;10M papers) and shows that a 13B SciLit-LLM outperforms smaller or similarly sized open models; no detailed quantitative scaling curve provided, but authors note better rejection behavior and reduced hallucination after domain continued pretraining.",
                        "uuids": [
                            "e4409.0"
                        ]
                    },
                    {
                        "text": "Domain-specific pretraining (SciBERT) for the encoder yields measurable improvements in ROUGE-L and helps mitigate domain shift seen with general-domain pretrained models like BART.",
                        "uuids": [
                            "e4395.5"
                        ]
                    },
                    {
                        "text": "SciBERTAbs outperformed BART and BertABS in ROUGE-L on this dataset, showing an improvement in ROUGE-L compared to BertABS and BART.",
                        "uuids": [
                            "e4395.5"
                        ]
                    },
                    {
                        "text": "Domain-specific pretraining (SciBERT) for the encoder yields measurable improvements in ROUGE-L and helps mitigate domain shift seen with general-domain pretrained models like BART.",
                        "uuids": [
                            "e4395.5"
                        ]
                    },
                    {
                        "text": "Paper reading (SparkRA): Factuality 4.68, Informativeness 4.45, Avg 4.57 (highest among compared models).",
                        "uuids": [
                            "e4409.0"
                        ]
                    },
                    {
                        "text": "SparkRA (SciLit-LLM 13B) achieved the highest average scores in paper reading (4.57 vs GPT-4 4.55) and paper polishing (4.49 vs GPT-4 4.32).",
                        "uuids": [
                            "e4409.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "Domain-specific pretraining is a known technique, but the specific finding that continual pretraining on 10M+ scientific papers enables smaller models (13B) to outperform larger general-purpose models on literature tasks, with measurable improvements in factuality and reduced hallucination, represents a novel empirical contribution.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [domain-specific pretraining]",
                        "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [continued pretraining benefits]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "For literature synthesis tasks requiring domain-specific knowledge, fine-tuning a 7B model with 3000+ examples will outperform prompting a 70B model when the task involves implicit attributes or multi-step reasoning.",
        "Parameter-efficient fine-tuning (LoRA) on encoder-decoder architectures will achieve 95-98% of full fine-tuning performance while using only 1-5% of the computational resources for literature synthesis tasks.",
        "Domain-specific continual pretraining on 10M+ scientific papers will enable a 13B model to match or exceed the performance of a 70B general-purpose model on scientific literature tasks.",
        "The performance gap between prompting and fine-tuning will be largest (20-30 percentage points) for implicit, multi-step reasoning tasks and smallest (5-10 percentage points) for explicit extraction tasks.",
        "Binary classification tasks in literature synthesis will reach &gt;80% accuracy with 2000-3000 training examples, while multi-label tasks will require 5000+ examples.",
        "For models below 13B parameters, fine-tuning will provide 2-3x larger relative performance improvements than for models above 50B parameters on the same literature synthesis tasks."
    ],
    "new_predictions_unknown": [
        "Whether there exists a model scale (e.g., 1T+ parameters) where prompting consistently matches or exceeds fine-tuning performance across all literature synthesis tasks remains unknown and could fundamentally change system design principles.",
        "The optimal allocation of computational budget between scaling model size vs. fine-tuning smaller models vs. domain pretraining for literature synthesis is unclear and may vary significantly by task and domain.",
        "Whether hybrid approaches that combine prompting and fine-tuning (e.g., prompt-tuning, prefix-tuning, instruction tuning) can achieve the best of both worlds is unknown but could offer important practical benefits.",
        "The extent to which fine-tuning on synthetic data generated by larger models can match fine-tuning on human-labeled data for literature synthesis is unclear but could dramatically reduce annotation costs.",
        "Whether the crossover point for cost-effectiveness varies significantly across different scientific domains (e.g., biomedicine vs. computer science) is unknown but could impact deployment strategies.",
        "The long-term stability and generalization of fine-tuned models as literature evolves over time is unclear and could affect the viability of fine-tuning for production systems."
    ],
    "negative_experiments": [
        "Finding that prompting consistently outperforms fine-tuning even with abundant training data (10,000+ examples) would challenge the Task Complexity Threshold Law and Data Efficiency Crossover Law.",
        "Demonstrating that the crossover point for cost-effectiveness occurs at much higher data volumes (&gt;10,000 examples) or much lower volumes (&lt;500 examples) would challenge the Data Efficiency Crossover Law.",
        "Showing that model scale does not interact with fine-tuning benefits (i.e., all model sizes benefit equally from fine-tuning) would challenge the Model Scale Interaction Law.",
        "Finding that parameter-efficient fine-tuning consistently underperforms full fine-tuning by more than 10% would challenge the Parameter-Efficient Fine-Tuning Sufficiency Law.",
        "Demonstrating that domain pretraining provides no measurable benefit over general pretraining for literature synthesis tasks would challenge the Domain Pretraining Enhancement Law.",
        "Finding that decoder-only architectures benefit more from PEFT than encoder-decoder architectures would contradict the Parameter-Efficient Fine-Tuning Sufficiency Law."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not explain how to predict task complexity thresholds a priori without empirical testing, making it difficult to decide between prompting and fine-tuning before implementation.",
            "uuids": [
                "e4378.0",
                "e4609.2",
                "e4394.1"
            ]
        },
        {
            "text": "The role of instruction tuning as an intermediate approach between prompting and task-specific fine-tuning is not fully addressed in the theory.",
            "uuids": [
                "e4394.1",
                "e4409.0"
            ]
        },
        {
            "text": "The theory does not account for continual learning scenarios where models need to adapt to evolving literature over time without catastrophic forgetting.",
            "uuids": [
                "e4378.0"
            ]
        },
        {
            "text": "The interaction between fine-tuning and retrieval augmentation (RAG) is not fully explained - whether they are complementary or substitutable.",
            "uuids": [
                "e4392.6",
                "e4409.0"
            ]
        },
        {
            "text": "The theory does not address the trade-offs between different types of fine-tuning (supervised fine-tuning, reinforcement learning, instruction tuning) for literature synthesis.",
            "uuids": [
                "e4431.6",
                "e4609.1"
            ]
        },
        {
            "text": "The impact of data augmentation strategies (e.g., paraphrasing, back-translation) on the data efficiency crossover point is not explained.",
            "uuids": [
                "e4392.4"
            ]
        },
        {
            "text": "The theory does not explain why encoder-decoder architectures benefit more from PEFT than decoder-only architectures at a mechanistic level.",
            "uuids": [
                "e4394.1"
            ]
        },
        {
            "text": "The optimal hyperparameters for PEFT (e.g., LoRA rank, scaling factor) and how they vary by task are not addressed.",
            "uuids": [
                "e4392.4",
                "e4394.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some evidence suggests that fine-tuning always outperforms prompting given sufficient data (e4378.0, e4392.4), while other evidence shows that very large models with good prompting can match fine-tuned smaller models (e4379.4, e4409.0), creating tension about the necessity of fine-tuning at scale.",
            "uuids": [
                "e4378.0",
                "e4379.4",
                "e4409.0",
                "e4392.4"
            ]
        },
        {
            "text": "Some systems report that PEFT is nearly as effective as full fine-tuning (e4431.7, e4392.4), while others find significant gaps especially for complex tasks (e4394.1), suggesting task-dependent effects that are not fully captured.",
            "uuids": [
                "e4394.1",
                "e4392.4",
                "e4431.7"
            ]
        },
        {
            "text": "Evidence shows both that smaller models benefit more from fine-tuning (relative improvement) and that larger models show larger absolute improvements (e4394.1, e4409.0), creating ambiguity about the scale-benefit relationship.",
            "uuids": [
                "e4394.1",
                "e4409.0"
            ]
        },
        {
            "text": "Some evidence suggests domain pretraining is critical for performance (e4395.5, e4409.0), while other evidence shows that general-purpose models with good prompting can achieve competitive results (e4379.4), raising questions about when domain pretraining is necessary.",
            "uuids": [
                "e4395.5",
                "e4409.0",
                "e4379.4"
            ]
        },
        {
            "text": "RAG over curated, finetuning-aligned extracted Q&A data outperformed raw-article RAG, but naively combining RAG outputs with finetuned model generation can confuse the model and degrade factual performance, suggesting complex interactions.",
            "uuids": [
                "e4392.6"
            ]
        }
    ],
    "special_cases": [
        "For tasks requiring very recent knowledge not in the training data, prompting with retrieval may be the only viable approach regardless of other factors, as fine-tuning cannot incorporate knowledge that doesn't exist in the training corpus.",
        "When computational resources are severely constrained (e.g., edge deployment, real-time inference), prompting may be necessary even when fine-tuning would theoretically perform better, as PEFT still requires some adaptation overhead.",
        "For rapidly evolving domains where the literature changes significantly every few months, the cost of repeatedly fine-tuning may favor prompting-based approaches despite lower performance, unless continual learning methods are employed.",
        "When interpretability and control are critical (e.g., medical or legal applications), prompting may be preferred even if fine-tuning achieves higher performance, as prompts are more transparent and easier to audit than learned parameters.",
        "For multi-label classification tasks with implicit attributes, the data efficiency crossover point may be significantly higher (5000+ examples) than for binary tasks (3000 examples).",
        "Encoder-decoder architectures show larger benefits from PEFT than decoder-only architectures, suggesting that architecture choice interacts with the fine-tuning approach.",
        "When combining fine-tuning with retrieval augmentation, careful prompt engineering and retrieval filtering are necessary to avoid conflicts between the model's internal knowledge and retrieved context.",
        "For tasks requiring cross-paper synthesis (as opposed to single-paper analysis), even fine-tuned models may struggle and require additional architectural innovations or larger training sets."
    ],
    "existing_theory": {
        "classification_explanation": "This theory synthesizes known concepts about prompting and fine-tuning into a unified framework with novel quantitative thresholds and interaction effects specific to literature synthesis. While the general trade-off between prompting and fine-tuning is known in NLP, the specific laws (Task Complexity Threshold, Data Efficiency Crossover, Model Scale Interaction, PEFT Sufficiency, Domain Pretraining Enhancement) with their quantitative thresholds (10-30 pp improvement, 1000-5000 examples, 13B parameter sweet spot, 2-5% PEFT gap, 10M+ papers for domain pretraining) represent new empirical contributions derived from literature synthesis systems.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [prompting capabilities]",
            "Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers [fine-tuning benefits]",
            "Hu et al. (2021) LoRA: Low-Rank Adaptation of Large Language Models [parameter-efficient fine-tuning]",
            "Wei et al. (2022) Emergent Abilities of Large Language Models [scale effects]",
            "Chung et al. (2022) Scaling Instruction-Finetuned Language Models [instruction tuning]",
            "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
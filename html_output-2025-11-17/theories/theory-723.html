<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Latent Circuit Augmentation for Arithmetic Generalization - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-723</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-723</p>
                <p><strong>Name:</strong> Hierarchical Latent Circuit Augmentation for Arithmetic Generalization</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.</p>
                <p><strong>Description:</strong> This theory proposes that language models develop a hierarchy of latent circuits for arithmetic, where lower-level circuits encode digit-level operations and higher-level circuits encode compositional rules. Fine-tuning augments this hierarchy, enabling generalization to novel arithmetic formats and longer sequences by recursively composing lower-level operations.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Circuit Formation (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is fine-tuned on &#8594; multi-digit arithmetic tasks</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; develops &#8594; hierarchical latent circuits<span style="color: #888888;">, and</span></div>
        <div>&#8226; lower-level circuits &#8594; encode &#8594; digit-level operations<span style="color: #888888;">, and</span></div>
        <div>&#8226; higher-level circuits &#8594; encode &#8594; compositional and carry rules</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Interpretability studies show distinct modules for digit manipulation and carry propagation in arithmetic tasks. </li>
    <li>Models generalize to longer sequences by recursively applying learned digit-level operations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law extends general hierarchical representation principles to a new, mechanistic context for arithmetic in LMs.</p>            <p><strong>What Already Exists:</strong> Hierarchical representations are known in deep learning, but not explicitly for arithmetic in LMs.</p>            <p><strong>What is Novel:</strong> The explicit hierarchical circuit structure for arithmetic generalization in language models is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [General hierarchy, not arithmetic-specific]</li>
    <li>Wang et al. (2022) Interpretability in Arithmetic Tasks [Digit/carry modules, not explicit hierarchy]</li>
</ul>
            <h3>Statement 1: Recursive Generalization via Circuit Augmentation (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; model &#8594; has &#8594; hierarchical latent circuits for arithmetic<span style="color: #888888;">, and</span></div>
        <div>&#8226; fine-tuning &#8594; exposes &#8594; longer or novel arithmetic formats</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; generalizes &#8594; to new formats by recursive composition of lower-level operations</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Models fine-tuned on short sequences can generalize to longer ones, indicating recursive application of learned rules. </li>
    <li>Emergence of compositional rules in higher layers is observed in interpretability analyses. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law synthesizes known recursive generalization with new mechanistic insights for arithmetic in LMs.</p>            <p><strong>What Already Exists:</strong> Recursive and compositional generalization is a known property of deep networks.</p>            <p><strong>What is Novel:</strong> The explicit link to hierarchical circuit augmentation for arithmetic generalization is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity [General compositionality, not circuit-based]</li>
    <li>Wang et al. (2022) Interpretability in Arithmetic Tasks [Compositional rules, not explicit recursion]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Interventions that disrupt lower-level digit circuits will impair arithmetic performance across all formats.</li>
                <li>Fine-tuning on new arithmetic formats will primarily augment higher-level circuits, leaving lower-level digit circuits unchanged.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There may exist a limit to the depth of hierarchy that can be formed, constraining generalization to very long sequences.</li>
                <li>Hierarchical circuit augmentation may enable transfer to other hierarchical reasoning tasks, such as parsing or logic.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If models cannot generalize to longer sequences after fine-tuning on short ones, the theory would be challenged.</li>
                <li>If no hierarchical structure is observed in circuit analysis, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The precise mechanism for recursive composition in transformer architectures is not fully specified. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends and formalizes hierarchical representation principles in a new, mechanistic context for arithmetic in LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [General hierarchy]</li>
    <li>Wang et al. (2022) Interpretability in Arithmetic Tasks [Digit/carry modules]</li>
    <li>Lake & Baroni (2018) Generalization without Systematicity [Compositionality]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Latent Circuit Augmentation for Arithmetic Generalization",
    "theory_description": "This theory proposes that language models develop a hierarchy of latent circuits for arithmetic, where lower-level circuits encode digit-level operations and higher-level circuits encode compositional rules. Fine-tuning augments this hierarchy, enabling generalization to novel arithmetic formats and longer sequences by recursively composing lower-level operations.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Circuit Formation",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is fine-tuned on",
                        "object": "multi-digit arithmetic tasks"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "develops",
                        "object": "hierarchical latent circuits"
                    },
                    {
                        "subject": "lower-level circuits",
                        "relation": "encode",
                        "object": "digit-level operations"
                    },
                    {
                        "subject": "higher-level circuits",
                        "relation": "encode",
                        "object": "compositional and carry rules"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Interpretability studies show distinct modules for digit manipulation and carry propagation in arithmetic tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Models generalize to longer sequences by recursively applying learned digit-level operations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical representations are known in deep learning, but not explicitly for arithmetic in LMs.",
                    "what_is_novel": "The explicit hierarchical circuit structure for arithmetic generalization in language models is new.",
                    "classification_explanation": "The law extends general hierarchical representation principles to a new, mechanistic context for arithmetic in LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [General hierarchy, not arithmetic-specific]",
                        "Wang et al. (2022) Interpretability in Arithmetic Tasks [Digit/carry modules, not explicit hierarchy]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Recursive Generalization via Circuit Augmentation",
                "if": [
                    {
                        "subject": "model",
                        "relation": "has",
                        "object": "hierarchical latent circuits for arithmetic"
                    },
                    {
                        "subject": "fine-tuning",
                        "relation": "exposes",
                        "object": "longer or novel arithmetic formats"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "generalizes",
                        "object": "to new formats by recursive composition of lower-level operations"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Models fine-tuned on short sequences can generalize to longer ones, indicating recursive application of learned rules.",
                        "uuids": []
                    },
                    {
                        "text": "Emergence of compositional rules in higher layers is observed in interpretability analyses.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Recursive and compositional generalization is a known property of deep networks.",
                    "what_is_novel": "The explicit link to hierarchical circuit augmentation for arithmetic generalization is new.",
                    "classification_explanation": "The law synthesizes known recursive generalization with new mechanistic insights for arithmetic in LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake & Baroni (2018) Generalization without Systematicity [General compositionality, not circuit-based]",
                        "Wang et al. (2022) Interpretability in Arithmetic Tasks [Compositional rules, not explicit recursion]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Interventions that disrupt lower-level digit circuits will impair arithmetic performance across all formats.",
        "Fine-tuning on new arithmetic formats will primarily augment higher-level circuits, leaving lower-level digit circuits unchanged."
    ],
    "new_predictions_unknown": [
        "There may exist a limit to the depth of hierarchy that can be formed, constraining generalization to very long sequences.",
        "Hierarchical circuit augmentation may enable transfer to other hierarchical reasoning tasks, such as parsing or logic."
    ],
    "negative_experiments": [
        "If models cannot generalize to longer sequences after fine-tuning on short ones, the theory would be challenged.",
        "If no hierarchical structure is observed in circuit analysis, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The precise mechanism for recursive composition in transformer architectures is not fully specified.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models generalize to new formats without clear evidence of hierarchical circuit structure.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Arithmetic tasks with non-standard formats may require new lower-level circuits, not just higher-level augmentation.",
        "Models with shallow architectures may be unable to form deep hierarchies, limiting generalization."
    ],
    "existing_theory": {
        "what_already_exists": "General principles of hierarchical and compositional representations in deep learning.",
        "what_is_novel": "The explicit hierarchical circuit augmentation mechanism for arithmetic generalization in language models.",
        "classification_explanation": "The theory extends and formalizes hierarchical representation principles in a new, mechanistic context for arithmetic in LMs.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Elhage et al. (2022) A Mathematical Framework for Transformer Circuits [General hierarchy]",
            "Wang et al. (2022) Interpretability in Arithmetic Tasks [Digit/carry modules]",
            "Lake & Baroni (2018) Generalization without Systematicity [Compositionality]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform arithmetic.",
    "original_theory_id": "theory-577",
    "original_theory_name": "Latent Circuit Augmentation Theory of Arithmetic Fine-Tuning",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Latent Circuit Augmentation Theory of Arithmetic Fine-Tuning",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
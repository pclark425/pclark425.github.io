<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hybrid Neuro-Symbolic Spatial Reasoning Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-482</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-482</p>
                <p><strong>Name:</strong> Hybrid Neuro-Symbolic Spatial Reasoning Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku, based on the following results.</p>
                <p><strong>Description:</strong> Language models (LLMs) alone are fundamentally limited in their ability to solve spatial puzzle games (such as Sudoku, Rubik's Cube, logic grids, and spatial navigation) due to a lack of explicit, persistent world models and systematic state-tracking. However, when LLMs are combined with explicit symbolic or search-based modules (e.g., Monte Carlo Tree Search, Answer Set Programming, constraint solvers), or are prompted to externalize intermediate representations (e.g., Visualization-of-Thought), their spatial reasoning and planning capabilities are dramatically enhanced. This hybrid neuro-symbolic approach enables LLMs to overcome their inherent limitations in spatial imagination, multi-step planning, and constraint satisfaction by leveraging the strengths of both neural pattern recognition and explicit symbolic manipulation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: LLM+Symbolic Module Synergy Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_combined_with &#8594; explicit symbolic or search-based module (e.g., MCTS, ASP, SMT, constraint solver)<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; multi-step spatial reasoning or planning</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; hybrid system &#8594; achieves &#8594; higher accuracy and robustness on spatial puzzle games than LLM-alone or symbolic-alone</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>XoT (LLM+MCTS+revision) outperforms LLM-only and MCTS-only on 8-Puzzle and Pocket Cube; GPT-3/GPT-4+ASP pipeline outperforms LLM-only on Sudoku and logic grid puzzles. <a href="../results/extraction-result-3108.html#e3108.1" class="evidence-link">[e3108.1]</a> <a href="../results/extraction-result-3108.html#e3108.3" class="evidence-link">[e3108.3]</a> <a href="../results/extraction-result-3086.html#e3086.0" class="evidence-link">[e3086.0]</a> <a href="../results/extraction-result-3086.html#e3086.1" class="evidence-link">[e3086.1]</a> <a href="../results/extraction-result-3429.html#e3429.6" class="evidence-link">[e3429.6]</a> </li>
    <li>SweepClip (LLM+graph search) and Z3-based SMT solver with LLM-generated candidates outperform LLM-only on crosswords. <a href="../results/extraction-result-3093.html#e3093.3" class="evidence-link">[e3093.3]</a> <a href="../results/extraction-result-3422.html#e3422.4" class="evidence-link">[e3422.4]</a> </li>
    <li>RAP+LLaMA-33B (LLM as both agent and world model with MCTS) outperforms CoT-only LLMs on Blocksworld planning. <a href="../results/extraction-result-3409.html#e3409.0" class="evidence-link">[e3409.0]</a> <a href="../results/extraction-result-3409.html#e3409.1" class="evidence-link">[e3409.1]</a> </li>
    <li>Davinci+ASP and Map+ASP pipelines achieve near-perfect or perfect accuracy on StepGame, outperforming LLM-only and CoT/ToT prompting. <a href="../results/extraction-result-3429.html#e3429.1" class="evidence-link">[e3429.1]</a> <a href="../results/extraction-result-3429.html#e3429.3" class="evidence-link">[e3429.3]</a> <a href="../results/extraction-result-3429.html#e3429.6" class="evidence-link">[e3429.6]</a> </li>
    <li>ByT5-small scorer (character-level LLM) used as a scoring module in local search for crosswords, improving system-level accuracy. <a href="../results/extraction-result-3376.html#e3376.1" class="evidence-link">[e3376.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Explicit State Representation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; system &#8594; maintains &#8594; explicit, persistent representation of spatial state (e.g., grid, map, object positions)<span style="color: #888888;">, and</span></div>
        <div>&#8226; reasoning process &#8594; involves &#8594; multi-step spatial updates or constraint propagation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; system &#8594; is_able_to &#8594; perform correct multi-step spatial reasoning and avoid error accumulation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Visualization-of-Thought (VoT) prompting, which elicits explicit stepwise visualizations, improves LLM performance on spatial navigation and tiling tasks. <a href="../results/extraction-result-3107.html#e3107.0" class="evidence-link">[e3107.0]</a> <a href="../results/extraction-result-3107.html#e3107.3" class="evidence-link">[e3107.3]</a> </li>
    <li>Symbolic pipelines (e.g., Map+ASP, SpRL+rule-based) achieve perfect or near-perfect accuracy on multi-hop spatial reasoning when state is explicitly represented. <a href="../results/extraction-result-3429.html#e3429.1" class="evidence-link">[e3429.1]</a> <a href="../results/extraction-result-3404.html#e3404.3" class="evidence-link">[e3404.3]</a> </li>
    <li>SweepClip and Z3-based SMT solver enforce grid constraints explicitly, enabling high accuracy in crossword grid filling. <a href="../results/extraction-result-3093.html#e3093.3" class="evidence-link">[e3093.3]</a> <a href="../results/extraction-result-3422.html#e3422.4" class="evidence-link">[e3422.4]</a> </li>
    <li>MCTS-based approaches (e.g., XoT, RAP) maintain explicit state transitions, enabling effective planning in spatial puzzles. <a href="../results/extraction-result-3108.html#e3108.1" class="evidence-link">[e3108.1]</a> <a href="../results/extraction-result-3108.html#e3108.3" class="evidence-link">[e3108.3]</a> <a href="../results/extraction-result-3409.html#e3409.0" class="evidence-link">[e3409.0]</a> </li>
    <li>Program-of-Thought (PoT) and code-generation prompting (e.g., CodeLlama-34B) can improve spatial reasoning when code simulates explicit state updates, though only when code is correct. <a href="../results/extraction-result-3395.html#e3395.5" class="evidence-link">[e3395.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: LLM-Only Spatial Reasoning Limitation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_used_without &#8594; external symbolic module or explicit state-tracking prompt<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; long-horizon spatial planning or constraint satisfaction</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; rapidly degrading accuracy with increasing problem complexity (e.g., more steps, larger grids, deeper composition)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>GPT-4, GPT-3.5, and Llama2-70B show steep accuracy drops on multi-hop spatial navigation, logic grid, and puzzle tasks as complexity increases. <a href="../results/extraction-result-3395.html#e3395.0" class="evidence-link">[e3395.0]</a> <a href="../results/extraction-result-3372.html#e3372.0" class="evidence-link">[e3372.0]</a> <a href="../results/extraction-result-3372.html#e3372.1" class="evidence-link">[e3372.1]</a> <a href="../results/extraction-result-3395.html#e3395.2" class="evidence-link">[e3395.2]</a> </li>
    <li>CoT/ToT prompting alone does not prevent error accumulation in long chains; error rates increase with hop count or puzzle size. <a href="../results/extraction-result-3429.html#e3429.7" class="evidence-link">[e3429.7]</a> <a href="../results/extraction-result-3429.html#e3429.8" class="evidence-link">[e3429.8]</a> <a href="../results/extraction-result-3372.html#e3372.2" class="evidence-link">[e3372.2]</a> </li>
    <li>LLMs (e.g., GPT-2, GPT-3) trained/fine-tuned on spatial puzzles (Rubik's Cube, Sokoban, logic grids) fail to generalize to harder or larger instances. <a href="../results/extraction-result-3370.html#e3370.0" class="evidence-link">[e3370.0]</a> <a href="../results/extraction-result-3385.html#e3385.0" class="evidence-link">[e3385.0]</a> <a href="../results/extraction-result-3372.html#e3372.2" class="evidence-link">[e3372.2]</a> </li>
    <li>LLMs alone (e.g., GPT-4, GPT-3.5) fail to solve Minesweeper, StepGame, and other spatial puzzles with high accuracy, especially as the number of steps increases. <a href="../results/extraction-result-3096.html#e3096.2" class="evidence-link">[e3096.2]</a> <a href="../results/extraction-result-3429.html#e3429.2" class="evidence-link">[e3429.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new spatial puzzle (e.g., a novel grid-based constraint game) is introduced, a hybrid LLM+symbolic system will outperform both LLM-only and symbolic-only baselines, especially as puzzle complexity increases.</li>
                <li>Prompting LLMs to externalize intermediate spatial states (e.g., via VoT or explicit state tables) will improve their performance on any spatial reasoning task that requires multi-step updates.</li>
                <li>LLMs will continue to show error accumulation and failure on spatial puzzles with more than 5-6 compositional steps unless augmented with explicit state-tracking or search modules.</li>
                <li>Hybrid systems will be more robust to input representation changes (e.g., token remapping, random embedding) than LLM-only systems.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>A hybrid LLM+symbolic system could, with sufficient integration, match or exceed human expert performance on complex spatial puzzles (e.g., large Sudoku, advanced logic grids) if the symbolic module is sufficiently expressive and the LLM provides robust semantic parsing.</li>
                <li>If LLMs are trained end-to-end with differentiable symbolic modules (e.g., neural-SMT or neural-ASP), they may develop emergent spatial reasoning capabilities that generalize beyond current hybrid pipelines.</li>
                <li>LLMs prompted with self-generated visualizations (VoT) may develop internal representations that enable transfer to unseen spatial tasks, even in the absence of explicit symbolic modules.</li>
                <li>Multimodal LLMs, if tightly integrated with symbolic solvers, may surpass text-only hybrids on visual spatial puzzles.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an LLM+symbolic hybrid system does not outperform LLM-only or symbolic-only baselines on a new spatial puzzle, this would challenge the synergy law.</li>
                <li>If LLMs prompted with explicit state representations (e.g., VoT) do not show improved performance on spatial reasoning tasks, this would call into question the explicit state representation law.</li>
                <li>If LLMs alone (without any symbolic augmentation) are able to solve arbitrarily complex spatial puzzles with high accuracy, this would falsify the LLM-only limitation law.</li>
                <li>If symbolic modules (e.g., ASP, SMT) do not improve performance when combined with LLMs, the theory would be challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some multimodal models (e.g., GPT-4V, Gemini-Pro, Claude-3 Opus) show improved performance on visual spatial tasks, but their integration with symbolic modules is less explored or not present in the current evidence. <a href="../results/extraction-result-3433.html#e3433.4" class="evidence-link">[e3433.4]</a> <a href="../results/extraction-result-3433.html#e3433.1" class="evidence-link">[e3433.1]</a> <a href="../results/extraction-result-3433.html#e3433.5" class="evidence-link">[e3433.5]</a> <a href="../results/extraction-result-3392.html#e3392.2" class="evidence-link">[e3392.2]</a> </li>
    <li>Certain LLMs (e.g., Codex, PaLM) show large CoT gains on algorithmic spatial tasks, suggesting some internalization of spatial procedures even without explicit symbolic modules. <a href="../results/extraction-result-3410.html#e3410.1" class="evidence-link">[e3410.1]</a> <a href="../results/extraction-result-3410.html#e3410.3" class="evidence-link">[e3410.3]</a> <a href="../results/extraction-result-3410.html#e3410.2" class="evidence-link">[e3410.2]</a> <a href="../results/extraction-result-3410.html#e3410.4" class="evidence-link">[e3410.4]</a> </li>
    <li>Some VLMs (e.g., LLaVA-1.6-34B, InternLM-XComposer2) perform worse with visual input than with text-only, indicating that multimodal integration can sometimes degrade spatial reasoning. <a href="../results/extraction-result-3103.html#e3103.5" class="evidence-link">[e3103.5]</a> <a href="../results/extraction-result-3433.html#e3433.3" class="evidence-link">[e3433.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Valmeekam et al. (2022) Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning) [Related: LLMs lack explicit world models, need for hybrid approaches]</li>
    <li>Yang et al. (2023) Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text [Hybrid neuro-symbolic reasoning, LLM+ASP]</li>
    <li>Li et al. (2024) Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark [Hybrid LLM+ASP, CoT/ToT prompting, explicit state tracking]</li>
    <li>Ding et al. (2023) Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation [LLM+MCTS+revision for spatial puzzles]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hybrid Neuro-Symbolic Spatial Reasoning Theory",
    "theory_description": "Language models (LLMs) alone are fundamentally limited in their ability to solve spatial puzzle games (such as Sudoku, Rubik's Cube, logic grids, and spatial navigation) due to a lack of explicit, persistent world models and systematic state-tracking. However, when LLMs are combined with explicit symbolic or search-based modules (e.g., Monte Carlo Tree Search, Answer Set Programming, constraint solvers), or are prompted to externalize intermediate representations (e.g., Visualization-of-Thought), their spatial reasoning and planning capabilities are dramatically enhanced. This hybrid neuro-symbolic approach enables LLMs to overcome their inherent limitations in spatial imagination, multi-step planning, and constraint satisfaction by leveraging the strengths of both neural pattern recognition and explicit symbolic manipulation.",
    "theory_statements": [
        {
            "law": {
                "law_name": "LLM+Symbolic Module Synergy Law",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_combined_with",
                        "object": "explicit symbolic or search-based module (e.g., MCTS, ASP, SMT, constraint solver)"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "multi-step spatial reasoning or planning"
                    }
                ],
                "then": [
                    {
                        "subject": "hybrid system",
                        "relation": "achieves",
                        "object": "higher accuracy and robustness on spatial puzzle games than LLM-alone or symbolic-alone"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "XoT (LLM+MCTS+revision) outperforms LLM-only and MCTS-only on 8-Puzzle and Pocket Cube; GPT-3/GPT-4+ASP pipeline outperforms LLM-only on Sudoku and logic grid puzzles.",
                        "uuids": [
                            "e3108.1",
                            "e3108.3",
                            "e3086.0",
                            "e3086.1",
                            "e3429.6"
                        ]
                    },
                    {
                        "text": "SweepClip (LLM+graph search) and Z3-based SMT solver with LLM-generated candidates outperform LLM-only on crosswords.",
                        "uuids": [
                            "e3093.3",
                            "e3422.4"
                        ]
                    },
                    {
                        "text": "RAP+LLaMA-33B (LLM as both agent and world model with MCTS) outperforms CoT-only LLMs on Blocksworld planning.",
                        "uuids": [
                            "e3409.0",
                            "e3409.1"
                        ]
                    },
                    {
                        "text": "Davinci+ASP and Map+ASP pipelines achieve near-perfect or perfect accuracy on StepGame, outperforming LLM-only and CoT/ToT prompting.",
                        "uuids": [
                            "e3429.1",
                            "e3429.3",
                            "e3429.6"
                        ]
                    },
                    {
                        "text": "ByT5-small scorer (character-level LLM) used as a scoring module in local search for crosswords, improving system-level accuracy.",
                        "uuids": [
                            "e3376.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Explicit State Representation Law",
                "if": [
                    {
                        "subject": "system",
                        "relation": "maintains",
                        "object": "explicit, persistent representation of spatial state (e.g., grid, map, object positions)"
                    },
                    {
                        "subject": "reasoning process",
                        "relation": "involves",
                        "object": "multi-step spatial updates or constraint propagation"
                    }
                ],
                "then": [
                    {
                        "subject": "system",
                        "relation": "is_able_to",
                        "object": "perform correct multi-step spatial reasoning and avoid error accumulation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Visualization-of-Thought (VoT) prompting, which elicits explicit stepwise visualizations, improves LLM performance on spatial navigation and tiling tasks.",
                        "uuids": [
                            "e3107.0",
                            "e3107.3"
                        ]
                    },
                    {
                        "text": "Symbolic pipelines (e.g., Map+ASP, SpRL+rule-based) achieve perfect or near-perfect accuracy on multi-hop spatial reasoning when state is explicitly represented.",
                        "uuids": [
                            "e3429.1",
                            "e3404.3"
                        ]
                    },
                    {
                        "text": "SweepClip and Z3-based SMT solver enforce grid constraints explicitly, enabling high accuracy in crossword grid filling.",
                        "uuids": [
                            "e3093.3",
                            "e3422.4"
                        ]
                    },
                    {
                        "text": "MCTS-based approaches (e.g., XoT, RAP) maintain explicit state transitions, enabling effective planning in spatial puzzles.",
                        "uuids": [
                            "e3108.1",
                            "e3108.3",
                            "e3409.0"
                        ]
                    },
                    {
                        "text": "Program-of-Thought (PoT) and code-generation prompting (e.g., CodeLlama-34B) can improve spatial reasoning when code simulates explicit state updates, though only when code is correct.",
                        "uuids": [
                            "e3395.5"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "LLM-Only Spatial Reasoning Limitation Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_used_without",
                        "object": "external symbolic module or explicit state-tracking prompt"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "long-horizon spatial planning or constraint satisfaction"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "rapidly degrading accuracy with increasing problem complexity (e.g., more steps, larger grids, deeper composition)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "GPT-4, GPT-3.5, and Llama2-70B show steep accuracy drops on multi-hop spatial navigation, logic grid, and puzzle tasks as complexity increases.",
                        "uuids": [
                            "e3395.0",
                            "e3372.0",
                            "e3372.1",
                            "e3395.2"
                        ]
                    },
                    {
                        "text": "CoT/ToT prompting alone does not prevent error accumulation in long chains; error rates increase with hop count or puzzle size.",
                        "uuids": [
                            "e3429.7",
                            "e3429.8",
                            "e3372.2"
                        ]
                    },
                    {
                        "text": "LLMs (e.g., GPT-2, GPT-3) trained/fine-tuned on spatial puzzles (Rubik's Cube, Sokoban, logic grids) fail to generalize to harder or larger instances.",
                        "uuids": [
                            "e3370.0",
                            "e3385.0",
                            "e3372.2"
                        ]
                    },
                    {
                        "text": "LLMs alone (e.g., GPT-4, GPT-3.5) fail to solve Minesweeper, StepGame, and other spatial puzzles with high accuracy, especially as the number of steps increases.",
                        "uuids": [
                            "e3096.2",
                            "e3429.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new spatial puzzle (e.g., a novel grid-based constraint game) is introduced, a hybrid LLM+symbolic system will outperform both LLM-only and symbolic-only baselines, especially as puzzle complexity increases.",
        "Prompting LLMs to externalize intermediate spatial states (e.g., via VoT or explicit state tables) will improve their performance on any spatial reasoning task that requires multi-step updates.",
        "LLMs will continue to show error accumulation and failure on spatial puzzles with more than 5-6 compositional steps unless augmented with explicit state-tracking or search modules.",
        "Hybrid systems will be more robust to input representation changes (e.g., token remapping, random embedding) than LLM-only systems."
    ],
    "new_predictions_unknown": [
        "A hybrid LLM+symbolic system could, with sufficient integration, match or exceed human expert performance on complex spatial puzzles (e.g., large Sudoku, advanced logic grids) if the symbolic module is sufficiently expressive and the LLM provides robust semantic parsing.",
        "If LLMs are trained end-to-end with differentiable symbolic modules (e.g., neural-SMT or neural-ASP), they may develop emergent spatial reasoning capabilities that generalize beyond current hybrid pipelines.",
        "LLMs prompted with self-generated visualizations (VoT) may develop internal representations that enable transfer to unseen spatial tasks, even in the absence of explicit symbolic modules.",
        "Multimodal LLMs, if tightly integrated with symbolic solvers, may surpass text-only hybrids on visual spatial puzzles."
    ],
    "negative_experiments": [
        "If an LLM+symbolic hybrid system does not outperform LLM-only or symbolic-only baselines on a new spatial puzzle, this would challenge the synergy law.",
        "If LLMs prompted with explicit state representations (e.g., VoT) do not show improved performance on spatial reasoning tasks, this would call into question the explicit state representation law.",
        "If LLMs alone (without any symbolic augmentation) are able to solve arbitrarily complex spatial puzzles with high accuracy, this would falsify the LLM-only limitation law.",
        "If symbolic modules (e.g., ASP, SMT) do not improve performance when combined with LLMs, the theory would be challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some multimodal models (e.g., GPT-4V, Gemini-Pro, Claude-3 Opus) show improved performance on visual spatial tasks, but their integration with symbolic modules is less explored or not present in the current evidence.",
            "uuids": [
                "e3433.4",
                "e3433.1",
                "e3433.5",
                "e3392.2"
            ]
        },
        {
            "text": "Certain LLMs (e.g., Codex, PaLM) show large CoT gains on algorithmic spatial tasks, suggesting some internalization of spatial procedures even without explicit symbolic modules.",
            "uuids": [
                "e3410.1",
                "e3410.3",
                "e3410.2",
                "e3410.4"
            ]
        },
        {
            "text": "Some VLMs (e.g., LLaVA-1.6-34B, InternLM-XComposer2) perform worse with visual input than with text-only, indicating that multimodal integration can sometimes degrade spatial reasoning.",
            "uuids": [
                "e3103.5",
                "e3433.3"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "GPT-4V CoT underperforms GPT-4 VoT on synthetic 2D grid tasks, indicating that multimodal grounding does not always yield superior spatial reasoning compared to explicit internal visualization.",
            "uuids": [
                "e3107.1"
            ]
        },
        {
            "text": "Some LLMs (e.g., Codex, PaLM) with CoT prompting approach or exceed hybrid system performance on certain algorithmic spatial tasks, suggesting that scale and training can partially compensate for lack of explicit symbolic modules.",
            "uuids": [
                "e3410.1",
                "e3410.3"
            ]
        }
    ],
    "special_cases": [
        "Tasks that require only shallow or local spatial reasoning (e.g., neighbor counting, single-step moves) may be solvable by LLMs alone without symbolic augmentation.",
        "If the symbolic module is poorly specified or the LLM's semantic parsing is unreliable, the hybrid system may fail or underperform.",
        "For tasks with highly regular or short-horizon spatial structure, large LLMs with sufficient in-context examples may perform well without explicit symbolic modules.",
        "Hybrid systems may be less effective if the symbolic module cannot represent the puzzle's constraints or if the LLM's outputs are not reliably parseable."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Valmeekam et al. (2022) Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning) [Related: LLMs lack explicit world models, need for hybrid approaches]",
            "Yang et al. (2023) Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text [Hybrid neuro-symbolic reasoning, LLM+ASP]",
            "Li et al. (2024) Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark [Hybrid LLM+ASP, CoT/ToT prompting, explicit state tracking]",
            "Ding et al. (2023) Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation [LLM+MCTS+revision for spatial puzzles]"
        ]
    },
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
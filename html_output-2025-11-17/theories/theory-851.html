<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adaptive Memory Routing Theory for LLM Agents - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-851</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-851</p>
                <p><strong>Name:</strong> Adaptive Memory Routing Theory for LLM Agents</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language model agents can best use memory to solve tasks.</p>
                <p><strong>Description:</strong> This theory proposes that LLM agents achieve optimal task performance by adaptively routing information between different memory modules (short-term, episodic, semantic, external) based on task context, uncertainty, and salience. The routing mechanism is learned or meta-learned, enabling agents to dynamically select, update, and retrieve memories in a context-sensitive manner.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Contextual Memory Routing Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; faces &#8594; task with variable context or uncertainty</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; routes &#8594; information to/from memory modules based on context, uncertainty, and salience<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; updates &#8594; routing policy through learning or meta-learning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Biological brains route information between memory systems based on context and salience. </li>
    <li>Meta-learning approaches enable neural agents to adapt memory usage policies to new tasks. </li>
    <li>LLM agents with learned memory routing outperform static memory architectures on variable-context tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While adaptive routing is known in other domains, its formalization for LLM agent memory is novel.</p>            <p><strong>What Already Exists:</strong> Contextual routing is observed in biological cognition and some meta-learning systems.</p>            <p><strong>What is Novel:</strong> The explicit law of adaptive, context-driven memory routing for LLM agents is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2018) Prefrontal cortex as a meta-reinforcement learning system [Meta-learning and memory routing in brains]</li>
    <li>Ritter et al. (2018) Episodic Control as Meta-Reinforcement Learning [Meta-learning and memory in RL]</li>
    <li>Graves et al. (2016) Hybrid computing using a neural network with dynamic external memory [Learned memory access in neural networks]</li>
</ul>
            <h3>Statement 1: Salience-Gated Memory Update Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; detects &#8594; high-salience event or information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; prioritizes &#8594; updating relevant memory modules with salient information<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; suppresses &#8594; updates for low-salience or redundant information</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Salience-gated memory updating is observed in human and animal learning. </li>
    <li>LLM agents with salience-based memory updates retain important information and avoid memory overload. </li>
    <li>Salience gating improves efficiency and reduces interference in both biological and artificial systems. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Salience gating is known, but its formalization for LLM agent memory is novel.</p>            <p><strong>What Already Exists:</strong> Salience-based memory updating is established in neuroscience and some AI systems.</p>            <p><strong>What is Novel:</strong> The explicit law of salience-gated memory updates for LLM agent memory is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Lisman & Grace (2005) The hippocampal-VTA loop: controlling the entry of information into long-term memory [Salience gating in biological memory]</li>
    <li>Kirkpatrick et al. (2017) Overcoming catastrophic forgetting in neural networks [Selective memory updates in AI]</li>
    <li>Ritter et al. (2018) Episodic Control as Meta-Reinforcement Learning [Salience in meta-learning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with adaptive memory routing will outperform static-memory agents on tasks with variable context, uncertainty, or salience.</li>
                <li>Salience-gated memory updates will reduce memory interference and improve retention of important information.</li>
                <li>Meta-learned routing policies will generalize to novel task distributions.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Emergent memory specialization may occur, with different modules developing distinct roles.</li>
                <li>Adaptive routing may enable LLM agents to develop forms of attention or working memory analogous to biological systems.</li>
                <li>Unintended memory bottlenecks or routing failures may arise in highly dynamic environments.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If adaptive routing does not improve performance over static routing, the theory is challenged.</li>
                <li>If salience gating does not reduce interference or improve retention, the theory's assumptions are weakened.</li>
                <li>If meta-learned routing policies do not generalize, the theory's scope is limited.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify the optimal criteria for salience or the mechanisms for detecting it in LLM agents. </li>
    <li>The impact of adversarial or misleading salience signals is not addressed. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and extends existing concepts into a novel, formal framework for LLM agents.</p>
            <p><strong>References:</strong> <ul>
    <li>Wang et al. (2018) Prefrontal cortex as a meta-reinforcement learning system [Meta-learning and memory routing in brains]</li>
    <li>Lisman & Grace (2005) The hippocampal-VTA loop: controlling the entry of information into long-term memory [Salience gating in biological memory]</li>
    <li>Graves et al. (2016) Hybrid computing using a neural network with dynamic external memory [Learned memory access in neural networks]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Adaptive Memory Routing Theory for LLM Agents",
    "theory_description": "This theory proposes that LLM agents achieve optimal task performance by adaptively routing information between different memory modules (short-term, episodic, semantic, external) based on task context, uncertainty, and salience. The routing mechanism is learned or meta-learned, enabling agents to dynamically select, update, and retrieve memories in a context-sensitive manner.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Contextual Memory Routing Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "faces",
                        "object": "task with variable context or uncertainty"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "routes",
                        "object": "information to/from memory modules based on context, uncertainty, and salience"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "updates",
                        "object": "routing policy through learning or meta-learning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Biological brains route information between memory systems based on context and salience.",
                        "uuids": []
                    },
                    {
                        "text": "Meta-learning approaches enable neural agents to adapt memory usage policies to new tasks.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents with learned memory routing outperform static memory architectures on variable-context tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Contextual routing is observed in biological cognition and some meta-learning systems.",
                    "what_is_novel": "The explicit law of adaptive, context-driven memory routing for LLM agents is new.",
                    "classification_explanation": "While adaptive routing is known in other domains, its formalization for LLM agent memory is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wang et al. (2018) Prefrontal cortex as a meta-reinforcement learning system [Meta-learning and memory routing in brains]",
                        "Ritter et al. (2018) Episodic Control as Meta-Reinforcement Learning [Meta-learning and memory in RL]",
                        "Graves et al. (2016) Hybrid computing using a neural network with dynamic external memory [Learned memory access in neural networks]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Salience-Gated Memory Update Law",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "detects",
                        "object": "high-salience event or information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "prioritizes",
                        "object": "updating relevant memory modules with salient information"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "suppresses",
                        "object": "updates for low-salience or redundant information"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Salience-gated memory updating is observed in human and animal learning.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents with salience-based memory updates retain important information and avoid memory overload.",
                        "uuids": []
                    },
                    {
                        "text": "Salience gating improves efficiency and reduces interference in both biological and artificial systems.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Salience-based memory updating is established in neuroscience and some AI systems.",
                    "what_is_novel": "The explicit law of salience-gated memory updates for LLM agent memory is new.",
                    "classification_explanation": "Salience gating is known, but its formalization for LLM agent memory is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lisman & Grace (2005) The hippocampal-VTA loop: controlling the entry of information into long-term memory [Salience gating in biological memory]",
                        "Kirkpatrick et al. (2017) Overcoming catastrophic forgetting in neural networks [Selective memory updates in AI]",
                        "Ritter et al. (2018) Episodic Control as Meta-Reinforcement Learning [Salience in meta-learning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with adaptive memory routing will outperform static-memory agents on tasks with variable context, uncertainty, or salience.",
        "Salience-gated memory updates will reduce memory interference and improve retention of important information.",
        "Meta-learned routing policies will generalize to novel task distributions."
    ],
    "new_predictions_unknown": [
        "Emergent memory specialization may occur, with different modules developing distinct roles.",
        "Adaptive routing may enable LLM agents to develop forms of attention or working memory analogous to biological systems.",
        "Unintended memory bottlenecks or routing failures may arise in highly dynamic environments."
    ],
    "negative_experiments": [
        "If adaptive routing does not improve performance over static routing, the theory is challenged.",
        "If salience gating does not reduce interference or improve retention, the theory's assumptions are weakened.",
        "If meta-learned routing policies do not generalize, the theory's scope is limited."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify the optimal criteria for salience or the mechanisms for detecting it in LLM agents.",
            "uuids": []
        },
        {
            "text": "The impact of adversarial or misleading salience signals is not addressed.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLM agents with fixed memory access patterns perform well on certain tasks, challenging the necessity of adaptive routing.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with uniform or low-salience information may not benefit from adaptive routing.",
        "Highly structured tasks with predictable context may be solved with static routing."
    ],
    "existing_theory": {
        "what_already_exists": "Adaptive routing and salience gating are established in neuroscience and some meta-learning systems.",
        "what_is_novel": "The formalization of these principles as explicit laws for LLM agent memory is new.",
        "classification_explanation": "The theory synthesizes and extends existing concepts into a novel, formal framework for LLM agents.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Wang et al. (2018) Prefrontal cortex as a meta-reinforcement learning system [Meta-learning and memory routing in brains]",
            "Lisman & Grace (2005) The hippocampal-VTA loop: controlling the entry of information into long-term memory [Salience gating in biological memory]",
            "Graves et al. (2016) Hybrid computing using a neural network with dynamic external memory [Learned memory access in neural networks]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language model agents can best use memory to solve tasks.",
    "original_theory_id": "theory-585",
    "original_theory_name": "Hybrid and Hierarchical Memory Architecture Theory for LLM Agents",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Hybrid and Hierarchical Memory Architecture Theory for LLM Agents",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
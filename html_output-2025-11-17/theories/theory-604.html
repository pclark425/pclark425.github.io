<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Decomposition and Iterative Composition Law - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-604</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-604</p>
                <p><strong>Name:</strong> Prompt Decomposition and Iterative Composition Law</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning, based on the following results.</p>
                <p><strong>Description:</strong> This theory asserts that explicit decomposition of complex reasoning tasks into smaller subproblems (via prompt engineering, e.g., least-to-most, selection-inference, or program synthesis) enables language models to generalize to longer or more complex reasoning chains than are present in their training data or exemplars. The key mechanism is the reuse of intermediate subanswers as building blocks, which allows for compositional generalization and length extrapolation, even in the absence of explicit symbolic augmentation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Explicit Decomposition Enables Length and Compositional Generalization (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompting method &#8594; decomposes &#8594; complex task into sequential subproblems with explicit reuse of subanswers<span style="color: #888888;">, and</span></div>
        <div>&#8226; model &#8594; is_provided_with &#8594; decomposition exemplars</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; generalizes_to &#8594; longer or more complex instances than seen in training</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Least-to-most prompting enables code-davinci-002 to generalize to longer last-letter-concatenation and SCAN tasks, outperforming CoT and standard prompting, with accuracy for L=12: L2M 74.0% vs CoT 31.8%. <a href="../results/extraction-result-5110.html#e5110.1" class="evidence-link">[e5110.1]</a> </li>
    <li>Least-to-most prompting on DROP (numerical subset) yields 82.45% vs CoT 74.77% and standard 58.78%, with largest gains on non-football subset where decomposition is straightforward. <a href="../results/extraction-result-5110.html#e5110.4" class="evidence-link">[e5110.4]</a> </li>
    <li>On GSM8K, L2M slightly improves or matches CoT overall, with clear advantage on problems requiring many steps (≥5): L2M 45.23% vs CoT lower. <a href="../results/extraction-result-5110.html#e5110.3" class="evidence-link">[e5110.3]</a> </li>
    <li>Selection-Inference modularization enables 7B models to outperform 280B models on multi-step reasoning by decomposing selection and inference; prompt-engineered SI (7B) achieves 58.75% vs 7B COT 41.32% and 280B COT 44.03%. <a href="../results/extraction-result-4947.html#e4947.1" class="evidence-link">[e4947.1]</a> </li>
    <li>In SCAN length generalization, L2M achieves 99.7% vs CoT 16.2% (code-davinci-002), showing explicit decomposition enables length generalization. <a href="../results/extraction-result-5110.html#e5110.1" class="evidence-link">[e5110.1]</a> </li>
    <li>Manual decomposition (human-provided) can often salvage many L2M failures, indicating the mechanism is decomposition and subanswer reuse. <a href="../results/extraction-result-5110.html#e5110.3" class="evidence-link">[e5110.3]</a> </li>
    <li>Prompt-size ablation: more exemplars improve both COT and L2M but L2M still outperforms; model ablation: code-davinci-002 >> text-davinci-002 >> code-davinci-001. <a href="../results/extraction-result-5110.html#e5110.1" class="evidence-link">[e5110.1]</a> </li>
    <li>In the selection-inference framework, modular decomposition (Selection+Inference) yields higher accuracy than consolidating steps into one generative pass. <a href="../results/extraction-result-4947.html#e4947.1" class="evidence-link">[e4947.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While decomposition and modular reasoning are established, the explicit link between prompt-based decomposition, subanswer reuse, and length/compositional generalization in LMs is a novel, mechanistic extension.</p>            <p><strong>What Already Exists:</strong> Prompt decomposition and modularization are known to help generalization in some settings, and modular reasoning is a recognized approach in cognitive science and program synthesis.</p>            <p><strong>What is Novel:</strong> This law formalizes the mechanism of intermediate subanswer reuse as the key to length and compositional generalization in LMs, and demonstrates that explicit decomposition in prompts enables generalization to longer/harder instances even when training data lacks such examples.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [decomposition and length generalization]</li>
    <li>Creswell et al. (2022) Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning [modular reasoning]</li>
    <li>Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks [compositionality in neural models]</li>
</ul>
            <h3>Statement 1: Program Synthesis as a Universal Reasoning Scaffold (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompting method &#8594; elicits &#8594; executable program traces (e.g., Python code) as intermediate reasoning</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; achieves &#8594; higher accuracy and robustness on arithmetic and symbolic reasoning tasks than with NL-only rationales</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>PAL and PoT prompting with code-davinci-002 achieve higher accuracy on GSM8K, Algebra, and other math tasks than CoT or standard prompting: e.g., PoT (Codex, greedy) GSM8K 71.6% vs Codex CoT 63.1%; PoT+SC 80.0%. <a href="../results/extraction-result-5115.html#e5115.0" class="evidence-link">[e5115.0]</a> <a href="../results/extraction-result-5085.html#e5085.1" class="evidence-link">[e5085.1]</a> </li>
    <li>Program synthesis enables error correction via execution, reducing arithmetic and symbolic mistakes; PAL achieves 73.3% on GSM8K vs CoT 62.5%. <a href="../results/extraction-result-5085.html#e5085.1" class="evidence-link">[e5085.1]</a> </li>
    <li>PoT outperforms CoT on a wide range of numerical and symbolic reasoning datasets, including AQuA, SVAMP, TabMWP, FinQA, ConvFinQA, TATQA. <a href="../results/extraction-result-5115.html#e5115.0" class="evidence-link">[e5115.0]</a> </li>
    <li>Program synthesis approach (Codex) achieves 81.1% on MATH benchmark vs GPT-3 few-shot+CoT 42.2%. <a href="../results/extraction-result-5082.html#e5082.1" class="evidence-link">[e5082.1]</a> </li>
    <li>Weaker code LLMs (code-cushman-001, code-davinci-001) still benefit from PAL over CoT, indicating robustness of the program synthesis scaffold. <a href="../results/extraction-result-5125.html#e5125.4" class="evidence-link">[e5125.4]</a> </li>
    <li>Ablations show semantic variable binding and multi-step decomposition are critical: removing binding (use a,b,c) reduced GSM8K from 71.6% to 60.2%; removing multi-step (direct equation generation) dropped GSM8K to 45.8%. <a href="../results/extraction-result-5115.html#e5115.0" class="evidence-link">[e5115.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The law generalizes program synthesis as a reasoning scaffold beyond arithmetic, and formalizes its role in enabling robust, compositional, and error-correcting reasoning in LMs.</p>            <p><strong>What Already Exists:</strong> Program synthesis and code generation are established for math reasoning and program induction; code execution as a reasoning tool is well-known in symbolic AI.</p>            <p><strong>What is Novel:</strong> The law asserts that program synthesis acts as a universal scaffold for multi-step reasoning in LMs, not just for arithmetic, and that code-based reasoning enables error correction and compositional generalization beyond what is possible with NL-only rationales.</p>
            <p><strong>References:</strong> <ul>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [program synthesis for math reasoning]</li>
    <li>Chen et al. (2021) Evaluating Large Language Models Trained on Code [code as reasoning]</li>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [tool-augmented LMs]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Applying least-to-most or selection-inference prompting to new symbolic manipulation tasks (e.g., string transformations, logic puzzles) will enable models to generalize to longer or more complex instances than seen in training.</li>
                <li>Prompting LMs to generate executable code for new domains (e.g., logic programming, constraint satisfaction) will yield higher accuracy than NL-only rationales.</li>
                <li>Explicit decomposition prompts will improve performance on multi-hop reasoning tasks in domains such as chemistry, legal reasoning, or multi-step planning.</li>
                <li>Combining program synthesis with decomposition (e.g., least-to-most program synthesis) will further improve generalization to longer reasoning chains.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is trained end-to-end to generate and execute programs for arbitrary reasoning tasks (not just arithmetic), it may develop generalizable reasoning skills across domains, including logic, planning, and symbolic manipulation.</li>
                <li>Explicit decomposition may enable models to generalize to tasks with fundamentally new logical structures (e.g., non-monotonic logic, modal logic), provided the decomposition is correct.</li>
                <li>Program synthesis scaffolding may enable LMs to learn new algorithmic reasoning skills (e.g., graph traversal, constraint satisfaction) with minimal in-domain data.</li>
                <li>Iterative composition via decomposition may allow LMs to solve problems with unbounded reasoning depth, limited only by prompt length and model context.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If explicit decomposition does not improve length or compositional generalization on new tasks (e.g., fails on new symbolic or algorithmic tasks), the theory would be challenged.</li>
                <li>If program synthesis fails to outperform NL-only rationales on new symbolic reasoning tasks (e.g., logic puzzles, constraint satisfaction), the universality claim would be weakened.</li>
                <li>If models trained with decomposition prompts do not generalize to longer/harder instances, or if subanswer reuse does not occur in model outputs, the mechanism would be called into question.</li>
                <li>If program synthesis scaffolding fails to reduce error rates or does not enable error correction via execution, the error-correction aspect of the law would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some tasks may not admit natural decompositions or program representations, limiting the applicability of the theory (e.g., tasks requiring holistic or global reasoning, or those with non-decomposable structure). <a href="../results/extraction-result-5110.html#e5110.4" class="evidence-link">[e5110.4]</a> <a href="../results/extraction-result-5110.html#e5110.5" class="evidence-link">[e5110.5]</a> </li>
    <li>In some domains, decomposition prompts may be difficult to construct or require domain expertise, limiting practical generalization. <a href="../results/extraction-result-5110.html#e5110.4" class="evidence-link">[e5110.4]</a> </li>
    <li>Program synthesis may not be applicable to tasks requiring world knowledge, pragmatics, or open-ended generation (e.g., creative writing, commonsense QA). <a href="../results/extraction-result-5115.html#e5115.0" class="evidence-link">[e5115.0]</a> <a href="../results/extraction-result-5085.html#e5085.1" class="evidence-link">[e5085.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory extends and mechanistically grounds existing findings by formalizing the role of decomposition and program synthesis in enabling generalization and error correction in LMs, and by providing empirical evidence across multiple domains.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [decomposition and length generalization]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [program synthesis for math reasoning]</li>
    <li>Creswell et al. (2022) Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning [modular reasoning]</li>
    <li>Chen et al. (2021) Evaluating Large Language Models Trained on Code [code as reasoning]</li>
    <li>Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks [compositionality in neural models]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt Decomposition and Iterative Composition Law",
    "theory_description": "This theory asserts that explicit decomposition of complex reasoning tasks into smaller subproblems (via prompt engineering, e.g., least-to-most, selection-inference, or program synthesis) enables language models to generalize to longer or more complex reasoning chains than are present in their training data or exemplars. The key mechanism is the reuse of intermediate subanswers as building blocks, which allows for compositional generalization and length extrapolation, even in the absence of explicit symbolic augmentation.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Explicit Decomposition Enables Length and Compositional Generalization",
                "if": [
                    {
                        "subject": "prompting method",
                        "relation": "decomposes",
                        "object": "complex task into sequential subproblems with explicit reuse of subanswers"
                    },
                    {
                        "subject": "model",
                        "relation": "is_provided_with",
                        "object": "decomposition exemplars"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "generalizes_to",
                        "object": "longer or more complex instances than seen in training"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Least-to-most prompting enables code-davinci-002 to generalize to longer last-letter-concatenation and SCAN tasks, outperforming CoT and standard prompting, with accuracy for L=12: L2M 74.0% vs CoT 31.8%.",
                        "uuids": [
                            "e5110.1"
                        ]
                    },
                    {
                        "text": "Least-to-most prompting on DROP (numerical subset) yields 82.45% vs CoT 74.77% and standard 58.78%, with largest gains on non-football subset where decomposition is straightforward.",
                        "uuids": [
                            "e5110.4"
                        ]
                    },
                    {
                        "text": "On GSM8K, L2M slightly improves or matches CoT overall, with clear advantage on problems requiring many steps (≥5): L2M 45.23% vs CoT lower.",
                        "uuids": [
                            "e5110.3"
                        ]
                    },
                    {
                        "text": "Selection-Inference modularization enables 7B models to outperform 280B models on multi-step reasoning by decomposing selection and inference; prompt-engineered SI (7B) achieves 58.75% vs 7B COT 41.32% and 280B COT 44.03%.",
                        "uuids": [
                            "e4947.1"
                        ]
                    },
                    {
                        "text": "In SCAN length generalization, L2M achieves 99.7% vs CoT 16.2% (code-davinci-002), showing explicit decomposition enables length generalization.",
                        "uuids": [
                            "e5110.1"
                        ]
                    },
                    {
                        "text": "Manual decomposition (human-provided) can often salvage many L2M failures, indicating the mechanism is decomposition and subanswer reuse.",
                        "uuids": [
                            "e5110.3"
                        ]
                    },
                    {
                        "text": "Prompt-size ablation: more exemplars improve both COT and L2M but L2M still outperforms; model ablation: code-davinci-002 &gt;&gt; text-davinci-002 &gt;&gt; code-davinci-001.",
                        "uuids": [
                            "e5110.1"
                        ]
                    },
                    {
                        "text": "In the selection-inference framework, modular decomposition (Selection+Inference) yields higher accuracy than consolidating steps into one generative pass.",
                        "uuids": [
                            "e4947.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt decomposition and modularization are known to help generalization in some settings, and modular reasoning is a recognized approach in cognitive science and program synthesis.",
                    "what_is_novel": "This law formalizes the mechanism of intermediate subanswer reuse as the key to length and compositional generalization in LMs, and demonstrates that explicit decomposition in prompts enables generalization to longer/harder instances even when training data lacks such examples.",
                    "classification_explanation": "While decomposition and modular reasoning are established, the explicit link between prompt-based decomposition, subanswer reuse, and length/compositional generalization in LMs is a novel, mechanistic extension.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [decomposition and length generalization]",
                        "Creswell et al. (2022) Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning [modular reasoning]",
                        "Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks [compositionality in neural models]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Program Synthesis as a Universal Reasoning Scaffold",
                "if": [
                    {
                        "subject": "prompting method",
                        "relation": "elicits",
                        "object": "executable program traces (e.g., Python code) as intermediate reasoning"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "achieves",
                        "object": "higher accuracy and robustness on arithmetic and symbolic reasoning tasks than with NL-only rationales"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "PAL and PoT prompting with code-davinci-002 achieve higher accuracy on GSM8K, Algebra, and other math tasks than CoT or standard prompting: e.g., PoT (Codex, greedy) GSM8K 71.6% vs Codex CoT 63.1%; PoT+SC 80.0%.",
                        "uuids": [
                            "e5115.0",
                            "e5085.1"
                        ]
                    },
                    {
                        "text": "Program synthesis enables error correction via execution, reducing arithmetic and symbolic mistakes; PAL achieves 73.3% on GSM8K vs CoT 62.5%.",
                        "uuids": [
                            "e5085.1"
                        ]
                    },
                    {
                        "text": "PoT outperforms CoT on a wide range of numerical and symbolic reasoning datasets, including AQuA, SVAMP, TabMWP, FinQA, ConvFinQA, TATQA.",
                        "uuids": [
                            "e5115.0"
                        ]
                    },
                    {
                        "text": "Program synthesis approach (Codex) achieves 81.1% on MATH benchmark vs GPT-3 few-shot+CoT 42.2%.",
                        "uuids": [
                            "e5082.1"
                        ]
                    },
                    {
                        "text": "Weaker code LLMs (code-cushman-001, code-davinci-001) still benefit from PAL over CoT, indicating robustness of the program synthesis scaffold.",
                        "uuids": [
                            "e5125.4"
                        ]
                    },
                    {
                        "text": "Ablations show semantic variable binding and multi-step decomposition are critical: removing binding (use a,b,c) reduced GSM8K from 71.6% to 60.2%; removing multi-step (direct equation generation) dropped GSM8K to 45.8%.",
                        "uuids": [
                            "e5115.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Program synthesis and code generation are established for math reasoning and program induction; code execution as a reasoning tool is well-known in symbolic AI.",
                    "what_is_novel": "The law asserts that program synthesis acts as a universal scaffold for multi-step reasoning in LMs, not just for arithmetic, and that code-based reasoning enables error correction and compositional generalization beyond what is possible with NL-only rationales.",
                    "classification_explanation": "The law generalizes program synthesis as a reasoning scaffold beyond arithmetic, and formalizes its role in enabling robust, compositional, and error-correcting reasoning in LMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Gao et al. (2022) PAL: Program-aided Language Models [program synthesis for math reasoning]",
                        "Chen et al. (2021) Evaluating Large Language Models Trained on Code [code as reasoning]",
                        "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [tool-augmented LMs]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Applying least-to-most or selection-inference prompting to new symbolic manipulation tasks (e.g., string transformations, logic puzzles) will enable models to generalize to longer or more complex instances than seen in training.",
        "Prompting LMs to generate executable code for new domains (e.g., logic programming, constraint satisfaction) will yield higher accuracy than NL-only rationales.",
        "Explicit decomposition prompts will improve performance on multi-hop reasoning tasks in domains such as chemistry, legal reasoning, or multi-step planning.",
        "Combining program synthesis with decomposition (e.g., least-to-most program synthesis) will further improve generalization to longer reasoning chains."
    ],
    "new_predictions_unknown": [
        "If a model is trained end-to-end to generate and execute programs for arbitrary reasoning tasks (not just arithmetic), it may develop generalizable reasoning skills across domains, including logic, planning, and symbolic manipulation.",
        "Explicit decomposition may enable models to generalize to tasks with fundamentally new logical structures (e.g., non-monotonic logic, modal logic), provided the decomposition is correct.",
        "Program synthesis scaffolding may enable LMs to learn new algorithmic reasoning skills (e.g., graph traversal, constraint satisfaction) with minimal in-domain data.",
        "Iterative composition via decomposition may allow LMs to solve problems with unbounded reasoning depth, limited only by prompt length and model context."
    ],
    "negative_experiments": [
        "If explicit decomposition does not improve length or compositional generalization on new tasks (e.g., fails on new symbolic or algorithmic tasks), the theory would be challenged.",
        "If program synthesis fails to outperform NL-only rationales on new symbolic reasoning tasks (e.g., logic puzzles, constraint satisfaction), the universality claim would be weakened.",
        "If models trained with decomposition prompts do not generalize to longer/harder instances, or if subanswer reuse does not occur in model outputs, the mechanism would be called into question.",
        "If program synthesis scaffolding fails to reduce error rates or does not enable error correction via execution, the error-correction aspect of the law would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Some tasks may not admit natural decompositions or program representations, limiting the applicability of the theory (e.g., tasks requiring holistic or global reasoning, or those with non-decomposable structure).",
            "uuids": [
                "e5110.4",
                "e5110.5"
            ]
        },
        {
            "text": "In some domains, decomposition prompts may be difficult to construct or require domain expertise, limiting practical generalization.",
            "uuids": [
                "e5110.4"
            ]
        },
        {
            "text": "Program synthesis may not be applicable to tasks requiring world knowledge, pragmatics, or open-ended generation (e.g., creative writing, commonsense QA).",
            "uuids": [
                "e5115.0",
                "e5085.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some cases, CoT or program synthesis does not yield gains (e.g., declarative algebra problems for PAL: PAL is outperformed by DECLARATIVE+SymPy by ~20 percentage points on Algebra).",
            "uuids": [
                "e5085.1"
            ]
        },
        {
            "text": "L2M and CoT both fail on some GSM8K problems unless a correct manual decomposition is supplied, indicating decomposition alone is not always sufficient.",
            "uuids": [
                "e5110.3"
            ]
        },
        {
            "text": "Program synthesis is less effective for tasks requiring declarative reasoning where procedural programs are non-obvious.",
            "uuids": [
                "e5085.1"
            ]
        }
    ],
    "special_cases": [
        "Tasks with non-compositional or holistic reasoning requirements (e.g., certain types of analogical reasoning, global constraint satisfaction) may not benefit from decomposition.",
        "Program synthesis may be less effective for tasks requiring world knowledge, pragmatics, or open-ended generation.",
        "Decomposition may not help if the subproblems are not independent or if intermediate subanswers cannot be reused.",
        "For tasks where the correct decomposition is not known or is ambiguous, explicit decomposition may not yield gains."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt decomposition and program synthesis are established for arithmetic and symbolic tasks, and modular reasoning is a recognized approach in cognitive science and program synthesis.",
        "what_is_novel": "The explicit mechanism of subanswer reuse and the universality of program synthesis as a reasoning scaffold for LMs are novel, as is the demonstration that prompt-based decomposition enables length and compositional generalization in LMs beyond training data.",
        "classification_explanation": "The theory extends and mechanistically grounds existing findings by formalizing the role of decomposition and program synthesis in enabling generalization and error correction in LMs, and by providing empirical evidence across multiple domains.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [decomposition and length generalization]",
            "Gao et al. (2022) PAL: Program-aided Language Models [program synthesis for math reasoning]",
            "Creswell et al. (2022) Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning [modular reasoning]",
            "Chen et al. (2021) Evaluating Large Language Models Trained on Code [code as reasoning]",
            "Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks [compositionality in neural models]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Representation Factorization and Local Causal Structure Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-121</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-121</p>
                <p><strong>Name:</strong> Representation Factorization and Local Causal Structure Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of distractor-robust causal discovery in open-ended virtual labs, including methods to detect, downweight, and refute spurious signals during inquiry, based on the following results.</p>
                <p><strong>Description:</strong> In structured environments with factorizable state spaces (e.g., multi-object scenes, modular systems), spurious correlations can be mitigated by learning representations that respect local causal structure. This involves: (1) factorizing observations into per-entity or per-module latent variables, (2) learning sparse, local transition functions that capture genuine causal dependencies while ignoring irrelevant factors, (3) validating counterfactual reasoning by checking consistency of local causal models, and (4) handling latent confounders through appropriate independence testing and subspace analysis. The theory predicts that modular architectures with explicit factorization outperform monolithic models in sample efficiency, interpretability, and robustness to spurious correlations, particularly in sparse interaction regimes. However, the approach requires careful handling of latent variables, may fail in densely-connected systems, and depends critically on correct identification of factorization granularity.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>In factorizable environments, the global causal graph can be decomposed into local causal models M^L that are sparser and more interpretable than the global model, with sparsity increasing as neighborhood size decreases</li>
                <li>Counterfactual data augmentation is valid if and only if: (1) swapped components are independent in the local causal graph, (2) structural equations are identical across neighborhoods, and (3) mask validation confirms consistent graph partitioning</li>
                <li>Modular architectures with per-entity transition functions scale better to larger graphs (more objects) than monolithic models, with sample complexity growing sub-quadratically in the number of entities for sparse interaction regimes</li>
                <li>Action influence on entity j can be quantified as I(S'_j; A | S=s), which is zero when the agent cannot causally affect that entity, and this measure is robust to moderate observation noise (<10%)</li>
                <li>Attention mechanisms alone are insufficient for causal discovery in non-i.i.d. RL data and require prior causal structure to avoid spurious correlations; attention weights without causal constraints produce spurious causal chains</li>
                <li>The number of valid counterfactuals grows combinatorially (n^m) with n factual samples and m independent components, enabling exponential data augmentation in factorized domains</li>
                <li>Latent confounders in factorized models can be detected through independent subspace analysis (ISA) or generalized independent noise (GIN) conditions, which identify multi-dimensional subspaces corresponding to shared latent causes</li>
                <li>Local causal models enable computational savings in counterfactual reasoning by restricting computation to relevant neighborhoods, with complexity scaling with local graph size rather than global graph size</li>
                <li>Factorization granularity must match the natural modularity of the environment; incorrect granularity leads to either over-parameterization (too fine) or failure to capture local independence (too coarse)</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Modular transition models with per-object latents achieve 90%+ long-horizon prediction accuracy vs. 40% for monolithic models in visual RL <a href="../results/extraction-result-991.html#e991.0" class="evidence-link">[e991.0]</a> </li>
    <li>Local Causal Models (LCMs) induce sparser graphs by conditioning on neighborhoods, enabling valid counterfactual reasoning and computational savings <a href="../results/extraction-result-994.html#e994.0" class="evidence-link">[e994.0]</a> <a href="../results/extraction-result-994.html#e994.1" class="evidence-link">[e994.1]</a> </li>
    <li>CoDA generates valid counterfactuals by swapping locally independent components validated by mask consistency, achieving 2-3x sample efficiency gains <a href="../results/extraction-result-994.html#e994.0" class="evidence-link">[e994.0]</a> <a href="../results/extraction-result-994.html#e994.5" class="evidence-link">[e994.5]</a> </li>
    <li>Attention-based inference networks with causal-stage graphs achieve 99.3% AIM recovery vs. 90% for attention alone on synthetic tests <a href="../results/extraction-result-758.html#e758.1" class="evidence-link">[e758.1]</a> <a href="../results/extraction-result-758.html#e758.3" class="evidence-link">[e758.3]</a> </li>
    <li>CAIAC uses learned action influence (CAI) to generate dynamically-feasible counterfactuals, substantially outperforming attention-based CODA in high-dimensional manipulation tasks <a href="../results/extraction-result-737.html#e737.0" class="evidence-link">[e737.0]</a> <a href="../results/extraction-result-737.html#e737.1" class="evidence-link">[e737.1]</a> <a href="../results/extraction-result-737.html#e737.2" class="evidence-link">[e737.2]</a> </li>
    <li>CAI detects agent influence via conditional mutual information I(S'_j; A | S=s), distinguishing agent-caused from externally-caused motion with >97% accuracy <a href="../results/extraction-result-1003.html#e1003.0" class="evidence-link">[e1003.0]</a> <a href="../results/extraction-result-1003.html#e1003.3" class="evidence-link">[e1003.3]</a> <a href="../results/extraction-result-1003.html#e1003.4" class="evidence-link">[e1003.4]</a> </li>
    <li>ISA-based local causal discovery recovers latent variable structure by identifying independent subspaces, enabling identification of confounded variables <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> <a href="../results/extraction-result-771.html#e771.2" class="evidence-link">[e771.2]</a> </li>
    <li>GIN condition enables detection and removal of spurious correlations induced by latent confounders through surrogate variable construction <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> <a href="../results/extraction-result-744.html#e744.1" class="evidence-link">[e744.1]</a> <a href="../results/extraction-result-744.html#e744.3" class="evidence-link">[e744.3]</a> <a href="../results/extraction-result-744.html#e744.4" class="evidence-link">[e744.4]</a> </li>
    <li>Amortized Causal Discovery learns shared dynamics across samples while amortizing per-sample graph inference, handling latent confounders via augmented encoder <a href="../results/extraction-result-1002.html#e1002.0" class="evidence-link">[e1002.0]</a> <a href="../results/extraction-result-1002.html#e1002.4" class="evidence-link">[e1002.4]</a> </li>
    <li>Test-Time Adaptation (TTA) improves graph predictions in low-data regimes by optimizing per-sample, reducing encoder overfitting <a href="../results/extraction-result-1002.html#e1002.4" class="evidence-link">[e1002.4]</a> </li>
    <li>Modular models scale better to larger graphs (5 objects) than monolithic models, with improved downstream planning success <a href="../results/extraction-result-991.html#e991.0" class="evidence-link">[e991.0]</a> </li>
    <li>CAI-based prioritization (CAI-P) improves RL sample efficiency 1.5-2.5x over standard prioritized replay <a href="../results/extraction-result-1003.html#e1003.0" class="evidence-link">[e1003.0]</a> </li>
    <li>Direct per-action causal discovery achieves 97% AIM recovery but is sample-inefficient compared to two-stage approaches <a href="../results/extraction-result-758.html#e758.3" class="evidence-link">[e758.3]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In a 5-object manipulation task with sparse interactions, modular models should achieve >80% prediction accuracy with 10x less data than monolithic models</li>
                <li>CoDA-style augmentation should improve sample efficiency by 2-3x in sparse, factorizable RL environments (e.g., object manipulation, multi-agent coordination with local interactions)</li>
                <li>CAI-based influence detection should maintain >90% accuracy even with 10% observation noise in continuous control tasks with factorized state spaces</li>
                <li>In environments with k independent subsystems, counterfactual augmentation should generate O(n^k) valid samples from n factual samples, with validation rejection rate <20%</li>
                <li>Amortized causal discovery with latent variable extensions should outperform per-sample methods by 2-5x in sample efficiency when multiple related time series are available</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether local causal model factorization extends effectively to environments with dense, non-local interactions (e.g., fluid dynamics, electromagnetic fields) where most variables interact</li>
                <li>The effectiveness of counterfactual augmentation when structural equations vary significantly across neighborhoods (e.g., non-stationary dynamics, context-dependent physics)</li>
                <li>Whether learned attention weights can reliably identify causal structure in high-dimensional visual observations without explicit object segmentation or pre-training on simpler tasks</li>
                <li>The scalability of CAI estimation to environments with 100+ entities and complex multi-agent interactions, particularly when many entities have weak but non-zero influence</li>
                <li>Whether ISA-based latent variable discovery scales to settings with 10+ latent confounders affecting overlapping subsets of observed variables</li>
                <li>The robustness of local causal models to partial observability where entity states are only partially observed or inferred from high-dimensional observations</li>
                <li>Whether modular architectures maintain advantages in environments with hierarchical structure (nested factorizations) or only in flat factorizations</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding environments where monolithic models consistently outperform modular models (e.g., densely-connected systems, holistic pattern recognition tasks) would challenge the factorization principle's generality</li>
                <li>Demonstrating that CoDA generates invalid counterfactuals even when mask validation passes (e.g., due to undetected equation differences) would undermine the validation criterion's sufficiency</li>
                <li>Showing that CAI fails to distinguish agent influence from external causes in carefully designed confounded scenarios (e.g., synchronized external motion) would reveal fundamental limitations</li>
                <li>Finding that attention weights are uncorrelated with true causal influence even when trained on causal graphs would question the attention-based approach's validity</li>
                <li>Demonstrating that local causal models fail to capture long-range dependencies that are causally relevant would challenge the locality assumption</li>
                <li>Finding cases where ISA fails to identify latent confounders despite satisfying theoretical assumptions would question the method's practical reliability</li>
                <li>Showing that counterfactual augmentation degrades performance in certain factorizable domains would challenge the universality of the approach</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to automatically discover the appropriate factorization granularity (entity-level vs. feature-level vs. subsystem-level) <a href="../results/extraction-result-991.html#e991.0" class="evidence-link">[e991.0]</a> <a href="../results/extraction-result-994.html#e994.0" class="evidence-link">[e994.0]</a> </li>
    <li>Handling partial observability where entity states are not fully observed is not fully addressed; CAI and CoDA assume full state observability <a href="../results/extraction-result-994.html#e994.0" class="evidence-link">[e994.0]</a> <a href="../results/extraction-result-1003.html#e1003.0" class="evidence-link">[e1003.0]</a> </li>
    <li>The computational cost of mask validation and counterfactual generation at scale (100+ entities) is not fully characterized <a href="../results/extraction-result-994.html#e994.0" class="evidence-link">[e994.0]</a> </li>
    <li>How to handle time-varying factorizations (where the appropriate decomposition changes over time) is not addressed <a href="../results/extraction-result-994.html#e994.0" class="evidence-link">[e994.0]</a> <a href="../results/extraction-result-991.html#e991.0" class="evidence-link">[e991.0]</a> </li>
    <li>The theory does not fully specify how to combine local causal models with global constraints or long-range dependencies <a href="../results/extraction-result-994.html#e994.1" class="evidence-link">[e994.1]</a> </li>
    <li>Handling continuous rather than discrete factorizations (e.g., spatial fields) is not addressed <a href="../results/extraction-result-991.html#e991.0" class="evidence-link">[e991.0]</a> </li>
    <li>The interaction between latent variable discovery (ISA/GIN) and factorization is not fully specified <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Watters et al. (2017) Visual Interaction Networks [Early work on factorized object representations for physical reasoning; precursor to modular transition models]</li>
    <li>Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Graph networks and structured representations; foundational work on modular architectures]</li>
    <li>Kipf et al. (2019) Contrastive Learning of Structured World Models [Object-centric world models with factorized representations]</li>
    <li>Pitis et al. (2020) Counterfactual Data Augmentation using Locally Factored Dynamics [Original CoDA paper; introduces local causal models and counterfactual augmentation]</li>
    <li>Shimizu et al. (2006) A Linear Non-Gaussian Acyclic Model for Causal Discovery [LiNGAM; foundational work on identifiable causal models with non-Gaussian noise]</li>
    <li>Hoyer et al. (2008) Nonlinear causal discovery with additive noise models [ANM; extends identifiability to nonlinear settings]</li>
    <li>Pearl (2009) Causality: Models, Reasoning, and Inference [Foundational causal inference theory; local Markov condition and d-separation]</li>
    <li>Schölkopf et al. (2021) Toward Causal Representation Learning [Broader framework for learning causal representations; includes discussion of modularity and factorization]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Representation Factorization and Local Causal Structure Theory",
    "theory_description": "In structured environments with factorizable state spaces (e.g., multi-object scenes, modular systems), spurious correlations can be mitigated by learning representations that respect local causal structure. This involves: (1) factorizing observations into per-entity or per-module latent variables, (2) learning sparse, local transition functions that capture genuine causal dependencies while ignoring irrelevant factors, (3) validating counterfactual reasoning by checking consistency of local causal models, and (4) handling latent confounders through appropriate independence testing and subspace analysis. The theory predicts that modular architectures with explicit factorization outperform monolithic models in sample efficiency, interpretability, and robustness to spurious correlations, particularly in sparse interaction regimes. However, the approach requires careful handling of latent variables, may fail in densely-connected systems, and depends critically on correct identification of factorization granularity.",
    "supporting_evidence": [
        {
            "text": "Modular transition models with per-object latents achieve 90%+ long-horizon prediction accuracy vs. 40% for monolithic models in visual RL",
            "uuids": [
                "e991.0"
            ]
        },
        {
            "text": "Local Causal Models (LCMs) induce sparser graphs by conditioning on neighborhoods, enabling valid counterfactual reasoning and computational savings",
            "uuids": [
                "e994.0",
                "e994.1"
            ]
        },
        {
            "text": "CoDA generates valid counterfactuals by swapping locally independent components validated by mask consistency, achieving 2-3x sample efficiency gains",
            "uuids": [
                "e994.0",
                "e994.5"
            ]
        },
        {
            "text": "Attention-based inference networks with causal-stage graphs achieve 99.3% AIM recovery vs. 90% for attention alone on synthetic tests",
            "uuids": [
                "e758.1",
                "e758.3"
            ]
        },
        {
            "text": "CAIAC uses learned action influence (CAI) to generate dynamically-feasible counterfactuals, substantially outperforming attention-based CODA in high-dimensional manipulation tasks",
            "uuids": [
                "e737.0",
                "e737.1",
                "e737.2"
            ]
        },
        {
            "text": "CAI detects agent influence via conditional mutual information I(S'_j; A | S=s), distinguishing agent-caused from externally-caused motion with &gt;97% accuracy",
            "uuids": [
                "e1003.0",
                "e1003.3",
                "e1003.4"
            ]
        },
        {
            "text": "ISA-based local causal discovery recovers latent variable structure by identifying independent subspaces, enabling identification of confounded variables",
            "uuids": [
                "e771.1",
                "e771.2"
            ]
        },
        {
            "text": "GIN condition enables detection and removal of spurious correlations induced by latent confounders through surrogate variable construction",
            "uuids": [
                "e744.0",
                "e744.1",
                "e744.3",
                "e744.4"
            ]
        },
        {
            "text": "Amortized Causal Discovery learns shared dynamics across samples while amortizing per-sample graph inference, handling latent confounders via augmented encoder",
            "uuids": [
                "e1002.0",
                "e1002.4"
            ]
        },
        {
            "text": "Test-Time Adaptation (TTA) improves graph predictions in low-data regimes by optimizing per-sample, reducing encoder overfitting",
            "uuids": [
                "e1002.4"
            ]
        },
        {
            "text": "Modular models scale better to larger graphs (5 objects) than monolithic models, with improved downstream planning success",
            "uuids": [
                "e991.0"
            ]
        },
        {
            "text": "CAI-based prioritization (CAI-P) improves RL sample efficiency 1.5-2.5x over standard prioritized replay",
            "uuids": [
                "e1003.0"
            ]
        },
        {
            "text": "Direct per-action causal discovery achieves 97% AIM recovery but is sample-inefficient compared to two-stage approaches",
            "uuids": [
                "e758.3"
            ]
        }
    ],
    "theory_statements": [
        "In factorizable environments, the global causal graph can be decomposed into local causal models M^L that are sparser and more interpretable than the global model, with sparsity increasing as neighborhood size decreases",
        "Counterfactual data augmentation is valid if and only if: (1) swapped components are independent in the local causal graph, (2) structural equations are identical across neighborhoods, and (3) mask validation confirms consistent graph partitioning",
        "Modular architectures with per-entity transition functions scale better to larger graphs (more objects) than monolithic models, with sample complexity growing sub-quadratically in the number of entities for sparse interaction regimes",
        "Action influence on entity j can be quantified as I(S'_j; A | S=s), which is zero when the agent cannot causally affect that entity, and this measure is robust to moderate observation noise (&lt;10%)",
        "Attention mechanisms alone are insufficient for causal discovery in non-i.i.d. RL data and require prior causal structure to avoid spurious correlations; attention weights without causal constraints produce spurious causal chains",
        "The number of valid counterfactuals grows combinatorially (n^m) with n factual samples and m independent components, enabling exponential data augmentation in factorized domains",
        "Latent confounders in factorized models can be detected through independent subspace analysis (ISA) or generalized independent noise (GIN) conditions, which identify multi-dimensional subspaces corresponding to shared latent causes",
        "Local causal models enable computational savings in counterfactual reasoning by restricting computation to relevant neighborhoods, with complexity scaling with local graph size rather than global graph size",
        "Factorization granularity must match the natural modularity of the environment; incorrect granularity leads to either over-parameterization (too fine) or failure to capture local independence (too coarse)"
    ],
    "new_predictions_likely": [
        "In a 5-object manipulation task with sparse interactions, modular models should achieve &gt;80% prediction accuracy with 10x less data than monolithic models",
        "CoDA-style augmentation should improve sample efficiency by 2-3x in sparse, factorizable RL environments (e.g., object manipulation, multi-agent coordination with local interactions)",
        "CAI-based influence detection should maintain &gt;90% accuracy even with 10% observation noise in continuous control tasks with factorized state spaces",
        "In environments with k independent subsystems, counterfactual augmentation should generate O(n^k) valid samples from n factual samples, with validation rejection rate &lt;20%",
        "Amortized causal discovery with latent variable extensions should outperform per-sample methods by 2-5x in sample efficiency when multiple related time series are available"
    ],
    "new_predictions_unknown": [
        "Whether local causal model factorization extends effectively to environments with dense, non-local interactions (e.g., fluid dynamics, electromagnetic fields) where most variables interact",
        "The effectiveness of counterfactual augmentation when structural equations vary significantly across neighborhoods (e.g., non-stationary dynamics, context-dependent physics)",
        "Whether learned attention weights can reliably identify causal structure in high-dimensional visual observations without explicit object segmentation or pre-training on simpler tasks",
        "The scalability of CAI estimation to environments with 100+ entities and complex multi-agent interactions, particularly when many entities have weak but non-zero influence",
        "Whether ISA-based latent variable discovery scales to settings with 10+ latent confounders affecting overlapping subsets of observed variables",
        "The robustness of local causal models to partial observability where entity states are only partially observed or inferred from high-dimensional observations",
        "Whether modular architectures maintain advantages in environments with hierarchical structure (nested factorizations) or only in flat factorizations"
    ],
    "negative_experiments": [
        "Finding environments where monolithic models consistently outperform modular models (e.g., densely-connected systems, holistic pattern recognition tasks) would challenge the factorization principle's generality",
        "Demonstrating that CoDA generates invalid counterfactuals even when mask validation passes (e.g., due to undetected equation differences) would undermine the validation criterion's sufficiency",
        "Showing that CAI fails to distinguish agent influence from external causes in carefully designed confounded scenarios (e.g., synchronized external motion) would reveal fundamental limitations",
        "Finding that attention weights are uncorrelated with true causal influence even when trained on causal graphs would question the attention-based approach's validity",
        "Demonstrating that local causal models fail to capture long-range dependencies that are causally relevant would challenge the locality assumption",
        "Finding cases where ISA fails to identify latent confounders despite satisfying theoretical assumptions would question the method's practical reliability",
        "Showing that counterfactual augmentation degrades performance in certain factorizable domains would challenge the universality of the approach"
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to automatically discover the appropriate factorization granularity (entity-level vs. feature-level vs. subsystem-level)",
            "uuids": [
                "e991.0",
                "e994.0"
            ]
        },
        {
            "text": "Handling partial observability where entity states are not fully observed is not fully addressed; CAI and CoDA assume full state observability",
            "uuids": [
                "e994.0",
                "e1003.0"
            ]
        },
        {
            "text": "The computational cost of mask validation and counterfactual generation at scale (100+ entities) is not fully characterized",
            "uuids": [
                "e994.0"
            ]
        },
        {
            "text": "How to handle time-varying factorizations (where the appropriate decomposition changes over time) is not addressed",
            "uuids": [
                "e994.0",
                "e991.0"
            ]
        },
        {
            "text": "The theory does not fully specify how to combine local causal models with global constraints or long-range dependencies",
            "uuids": [
                "e994.1"
            ]
        },
        {
            "text": "Handling continuous rather than discrete factorizations (e.g., spatial fields) is not addressed",
            "uuids": [
                "e991.0"
            ]
        },
        {
            "text": "The interaction between latent variable discovery (ISA/GIN) and factorization is not fully specified",
            "uuids": [
                "e771.1",
                "e744.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Transformer attention-based causal discovery (CODA) fails in high-dimensional settings, misidentifying influence and producing infeasible counterfactuals",
            "uuids": [
                "e737.2"
            ]
        },
        {
            "text": "Attention alone without causal structure produces spurious causal chains in RL data (90% vs 99.3% with causal structure)",
            "uuids": [
                "e758.1"
            ]
        },
        {
            "text": "Model-based augmentation (Dyna) can underperform CoDA due to model bias, suggesting learned models may not always benefit from factorization",
            "uuids": [
                "e994.0"
            ]
        },
        {
            "text": "Direct per-action causal discovery is sample-inefficient despite being theoretically sound, suggesting factorization alone is insufficient",
            "uuids": [
                "e758.3"
            ]
        },
        {
            "text": "Monolithic models with contrastive training can match modular models in some settings, suggesting factorization is not always necessary",
            "uuids": [
                "e991.0"
            ]
        },
        {
            "text": "Test-Time Adaptation sometimes outperforms encoder predictions, suggesting amortized factorization can fail in low-data regimes",
            "uuids": [
                "e1002.4"
            ]
        }
    ],
    "special_cases": [
        "In environments with external moving objects (e.g., rotating tables), CAI must condition on full state to distinguish agent vs. external causation; I(S'_j; A | S=s) correctly attributes causation only when S includes all relevant context",
        "When structural equations differ across neighborhoods, CoDA validation must check equation consistency in addition to mask consistency; this requires either learning neighborhood-specific models or restricting to stationary dynamics",
        "For continuous high-dimensional observations (e.g., raw pixels), learned masks may require pre-training on simpler tasks, auxiliary objectives (e.g., object segmentation), or explicit object-centric architectures",
        "In multi-agent settings, influence detection must account for other agents' actions as potential causes; CAI must be extended to I(S'_j; A_ego | S, A_others) to properly attribute causation",
        "When latent confounders are present, ISA-based methods require that confounders affect at most two observed variables (for identifiability); more complex confounding patterns require additional assumptions or interventions",
        "In cyclic local causal graphs, counterfactual reasoning requires solving fixed-point equations rather than simple ancestral sampling; CoDA's swap-and-validate may fail",
        "For hierarchical factorizations (nested modules), local causal models must be defined at each level of the hierarchy, with consistency constraints across levels",
        "When the number of independent components m is large relative to sample size n, counterfactual validation becomes statistically unreliable; minimum sample requirements scale with m",
        "In non-stationary environments where causal structure changes over time, local causal models must be re-learned or adapted; static factorizations may fail"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Watters et al. (2017) Visual Interaction Networks [Early work on factorized object representations for physical reasoning; precursor to modular transition models]",
            "Battaglia et al. (2018) Relational inductive biases, deep learning, and graph networks [Graph networks and structured representations; foundational work on modular architectures]",
            "Kipf et al. (2019) Contrastive Learning of Structured World Models [Object-centric world models with factorized representations]",
            "Pitis et al. (2020) Counterfactual Data Augmentation using Locally Factored Dynamics [Original CoDA paper; introduces local causal models and counterfactual augmentation]",
            "Shimizu et al. (2006) A Linear Non-Gaussian Acyclic Model for Causal Discovery [LiNGAM; foundational work on identifiable causal models with non-Gaussian noise]",
            "Hoyer et al. (2008) Nonlinear causal discovery with additive noise models [ANM; extends identifiability to nonlinear settings]",
            "Pearl (2009) Causality: Models, Reasoning, and Inference [Foundational causal inference theory; local Markov condition and d-separation]",
            "Schölkopf et al. (2021) Toward Causal Representation Learning [Broader framework for learning causal representations; includes discussion of modularity and factorization]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 2,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Induced Calibration Distortion Theory: General Law of Prompt Framing Effects - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1863</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1863</p>
                <p><strong>Name:</strong> Prompt-Induced Calibration Distortion Theory: General Law of Prompt Framing Effects</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory posits that the way prompts are constructed for LLMs systematically distorts the calibration of their probability estimates for future scientific discoveries. Specifically, the semantic, syntactic, and contextual framing of prompts can induce biases in the model's internal representation of uncertainty, leading to over- or under-estimation of event likelihoods, regardless of the underlying data distribution.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt Framing Calibration Distortion Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM_prompt &#8594; contains &#8594; framing_bias (semantic, syntactic, or contextual)<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; is_queried_for &#8594; probability_estimate_of_future_scientific_discovery</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_systematically_distorted_by &#8594; prompt_framing_bias</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show LLMs' outputs are sensitive to prompt wording, with significant shifts in probability estimates based on minor prompt changes. </li>
    <li>Prompt engineering literature documents that LLMs can be nudged toward different answers by rephrasing or contextualizing prompts. </li>
    <li>Calibration studies (e.g., Jiang et al. 2021) show that LLMs' confidence is not invariant to prompt structure. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While prompt effects and calibration are known, their systematic interaction in the context of scientific forecasting is not formalized.</p>            <p><strong>What Already Exists:</strong> Prompt sensitivity and prompt engineering are well-documented in LLM research, as is the general issue of calibration.</p>            <p><strong>What is Novel:</strong> The explicit law connecting prompt framing to systematic calibration distortion in the context of scientific discovery forecasting is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2021) How Can We Know When Language Models Know? [Prompt sensitivity, calibration]</li>
    <li>Reynolds & McDonell (2021) Prompt Programming for Large Language Models [Prompt engineering effects]</li>
    <li>Perez et al. (2022) True Few-Shot Learning with Language Models [Prompting and calibration]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Rephrasing a prompt about the likelihood of a scientific discovery will yield systematically different probability estimates from the same LLM.</li>
                <li>Adding contextual cues (e.g., 'experts believe', 'historically') to prompts will shift LLM probability estimates in predictable directions.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There may exist optimal prompt framings that minimize calibration distortion for scientific forecasting, but their structure is currently unknown.</li>
                <li>Prompt-induced calibration distortion may interact nonlinearly with model size or architecture, leading to unpredictable effects in future LLMs.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs produce identical probability estimates for all prompt framings, the theory would be falsified.</li>
                <li>If calibration distortion is not systematically related to prompt structure, the theory would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs with explicit calibration layers or post-processing may mitigate prompt-induced distortion. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This law synthesizes known prompt and calibration effects into a new, domain-specific theory.</p>
            <p><strong>References:</strong> <ul>
    <li>Jiang et al. (2021) How Can We Know When Language Models Know? [Prompt sensitivity, calibration]</li>
    <li>Reynolds & McDonell (2021) Prompt Programming for Large Language Models [Prompt engineering]</li>
    <li>Perez et al. (2022) True Few-Shot Learning with Language Models [Prompting and calibration]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Induced Calibration Distortion Theory: General Law of Prompt Framing Effects",
    "theory_description": "This theory posits that the way prompts are constructed for LLMs systematically distorts the calibration of their probability estimates for future scientific discoveries. Specifically, the semantic, syntactic, and contextual framing of prompts can induce biases in the model's internal representation of uncertainty, leading to over- or under-estimation of event likelihoods, regardless of the underlying data distribution.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt Framing Calibration Distortion Law",
                "if": [
                    {
                        "subject": "LLM_prompt",
                        "relation": "contains",
                        "object": "framing_bias (semantic, syntactic, or contextual)"
                    },
                    {
                        "subject": "LLM",
                        "relation": "is_queried_for",
                        "object": "probability_estimate_of_future_scientific_discovery"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_systematically_distorted_by",
                        "object": "prompt_framing_bias"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show LLMs' outputs are sensitive to prompt wording, with significant shifts in probability estimates based on minor prompt changes.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering literature documents that LLMs can be nudged toward different answers by rephrasing or contextualizing prompts.",
                        "uuids": []
                    },
                    {
                        "text": "Calibration studies (e.g., Jiang et al. 2021) show that LLMs' confidence is not invariant to prompt structure.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt sensitivity and prompt engineering are well-documented in LLM research, as is the general issue of calibration.",
                    "what_is_novel": "The explicit law connecting prompt framing to systematic calibration distortion in the context of scientific discovery forecasting is new.",
                    "classification_explanation": "While prompt effects and calibration are known, their systematic interaction in the context of scientific forecasting is not formalized.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jiang et al. (2021) How Can We Know When Language Models Know? [Prompt sensitivity, calibration]",
                        "Reynolds & McDonell (2021) Prompt Programming for Large Language Models [Prompt engineering effects]",
                        "Perez et al. (2022) True Few-Shot Learning with Language Models [Prompting and calibration]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Rephrasing a prompt about the likelihood of a scientific discovery will yield systematically different probability estimates from the same LLM.",
        "Adding contextual cues (e.g., 'experts believe', 'historically') to prompts will shift LLM probability estimates in predictable directions."
    ],
    "new_predictions_unknown": [
        "There may exist optimal prompt framings that minimize calibration distortion for scientific forecasting, but their structure is currently unknown.",
        "Prompt-induced calibration distortion may interact nonlinearly with model size or architecture, leading to unpredictable effects in future LLMs."
    ],
    "negative_experiments": [
        "If LLMs produce identical probability estimates for all prompt framings, the theory would be falsified.",
        "If calibration distortion is not systematically related to prompt structure, the theory would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs with explicit calibration layers or post-processing may mitigate prompt-induced distortion.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs with strong instruction tuning show reduced prompt sensitivity in calibration.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For extremely generic or ambiguous prompts, the distortion may be minimal or dominated by model priors.",
        "In multi-turn dialogues, prompt-induced distortion may accumulate or be dampened by context."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and calibration are established topics, but not formalized together for scientific forecasting.",
        "what_is_novel": "The explicit law of prompt-induced calibration distortion in scientific discovery probability estimation is new.",
        "classification_explanation": "This law synthesizes known prompt and calibration effects into a new, domain-specific theory.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Jiang et al. (2021) How Can We Know When Language Models Know? [Prompt sensitivity, calibration]",
            "Reynolds & McDonell (2021) Prompt Programming for Large Language Models [Prompt engineering]",
            "Perez et al. (2022) True Few-Shot Learning with Language Models [Prompting and calibration]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-650",
    "original_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
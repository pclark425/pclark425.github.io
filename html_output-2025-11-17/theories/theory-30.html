<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explanation-as-Task-Specification Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-30</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-30</p>
                <p><strong>Name:</strong> Explanation-as-Task-Specification Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> </p>
                <p><strong>Description:</strong> When natural language explanations are added to few-shot prompts (particularly post-answer explanations), they function not as additional factual evidence but as meta-level task specifications that shape the model's hypothesis about the input-output mapping. Explanations work by making explicit the reasoning principles or decision rules that should be applied, allowing the model to infer the correct task generalization from examples. This effect is distinct from providing more examples or more information: explanations change which features the model attends to and how it generalizes from examples. The effectiveness of explanations depends critically on: (1) model scale (only large models can perform the higher-order inference needed to extract task principles from explanations), (2) the semantic link between examples and explanations (scrambled or mismatched explanations hurt performance), and (3) explanation quality (hand-tuned explanations that clearly specify the task principle work better than generic explanations). Importantly, explanations placed after answers can still improve performance because they affect task inference rather than providing information for the immediate answer.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Explanations function as meta-level task specifications that shape the model's hypothesis about the input-output mapping, not as object-level factual evidence.</li>
                <li>The benefit of explanations scales with model size: ∂P(correct|explanation)/∂log(model_size) > 0, with benefits concentrated in models >100B parameters.</li>
                <li>Explanation effectiveness depends on the semantic link between examples and explanations: P(correct|matched_explanation) > P(correct|scrambled_explanation).</li>
                <li>Post-answer explanations can improve performance because they affect task inference: P(correct|explanation_after) > P(correct|no_explanation), even though explanations cannot provide information for the immediate answer.</li>
                <li>Hand-tuned explanations that clearly specify task principles are more effective than generic explanations: P(correct|tuned_explanation) >> P(correct|untuned_explanation).</li>
                <li>Selecting examples jointly with explanations is more effective than selecting examples alone: P(correct|selected_with_explanation) > P(correct|selected_without_explanation).</li>
                <li>The benefit of explanations is mediated by the model's ability to perform higher-order inference about task structure.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Adding untuned post-answer explanations to prompts produced +0.25 log-odds increase (~+5.5 percentage points) for 280B model. <a href="../results/extraction-result-198.html#e198.0" class="evidence-link">[e198.0]</a> </li>
    <li>Hand-tuned explanations produced +1.23 log-odds increase (~+23.8 percentage points) on challenging tasks. <a href="../results/extraction-result-198.html#e198.1" class="evidence-link">[e198.1]</a> </li>
    <li>Selecting examples jointly with explanations produced +0.64 log-odds increase (~+14.7 percentage points) over selecting examples alone. <a href="../results/extraction-result-198.html#e198.2" class="evidence-link">[e198.2]</a> </li>
    <li>Control conditions (scrambled explanations, true non-explanations, other-item explanations) were neutral or harmful, with all controls performing significantly worse than real explanations. <a href="../results/extraction-result-198.html#e198.3" class="evidence-link">[e198.3]</a> </li>
    <li>Larger models benefit more from explanations: positive interaction coefficient +0.077 for log_model_params * explanation. <a href="../results/extraction-result-198.html#e198.4" class="evidence-link">[e198.4]</a> </li>
    <li>Post-answer explanations can improve performance even though they cannot provide information for the immediate answer, supporting task-inference mechanism. <a href="../results/extraction-result-198.html#e198.5" class="evidence-link">[e198.5]</a> </li>
    <li>Explanations work by making explicit the reasoning principles or decision rules, allowing models to infer correct task generalization. <a href="../results/extraction-result-198.html#e198.0" class="evidence-link">[e198.0]</a> </li>
    <li>When explanatory mapping between example and explanation is destroyed, the prompt misleads or fails to convey correct task principle, decreasing performance. <a href="../results/extraction-result-198.html#e198.3" class="evidence-link">[e198.3]</a> </li>
    <li>Untuned explanations produced modest accuracy increases only for the largest model; smaller models did not reliably benefit. <a href="../results/extraction-result-198.html#e198.4" class="evidence-link">[e198.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Explanations will be most effective for tasks where the correct generalization principle is ambiguous from examples alone.</li>
                <li>Providing multiple diverse explanations for the same example will improve performance more than providing a single explanation.</li>
                <li>Explanations that explicitly contrast correct and incorrect reasoning will be more effective than explanations that only describe correct reasoning.</li>
                <li>Training models with explanation-augmented data will improve their ability to benefit from explanations at inference time.</li>
                <li>Explanations will be more effective when they highlight non-obvious features or principles that the model might otherwise miss.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exist explanation formats (e.g., formal logic, structured representations) that are more effective than natural language explanations.</li>
                <li>Whether models can be trained to generate their own explanations that improve their task performance (self-explanation).</li>
                <li>Whether the explanation-as-task-specification mechanism is the same across different types of tasks or whether different tasks require different explanation strategies.</li>
                <li>Whether explanations can be automatically optimized (e.g., through gradient-based methods) to maximize their task-specification effectiveness.</li>
                <li>Whether the size threshold for explanation benefits is determined by the model's general reasoning ability or by specific capabilities related to meta-learning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that scrambled or mismatched explanations provide the same benefits as correct explanations would challenge the semantic link requirement.</li>
                <li>Demonstrating that small models benefit from explanations as much as large models would challenge the scale-dependent aspect.</li>
                <li>Showing that pre-answer explanations (which can provide information for the immediate answer) don't work better than post-answer explanations would challenge the task-inference mechanism.</li>
                <li>Finding that generic, untuned explanations work as well as hand-tuned explanations would challenge the quality-dependent aspect.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Why some tasks show explanation benefits at smaller model sizes than others. <a href="../results/extraction-result-198.html#e198.4" class="evidence-link">[e198.4]</a> </li>
    <li>The exact cognitive or computational mechanisms that enable large models to extract task principles from explanations. <a href="../results/extraction-result-198.html#e198.0" class="evidence-link">[e198.0]</a> </li>
    <li>Whether the explanation effect is mediated by attention patterns or by deeper changes in the model's internal representations. <a href="../results/extraction-result-198.html#e198.0" class="evidence-link">[e198.0]</a> </li>
    <li>Why hand-tuning produces such large improvements over untuned explanations. <a href="../results/extraction-result-198.html#e198.1" class="evidence-link">[e198.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Lampinen et al. (2022) Can language models learn from explanations in context? [Introduces and provides evidence for explanation-as-task-specification, this theory formalizes and extends their findings]</li>
    <li>Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [Shows benefits of reasoning steps, but focuses on pre-answer chains for computation rather than post-answer explanations for task specification]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Introduces in-context learning, but doesn't distinguish explanation-based task specification from example-based learning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Explanation-as-Task-Specification Theory",
    "theory_description": "When natural language explanations are added to few-shot prompts (particularly post-answer explanations), they function not as additional factual evidence but as meta-level task specifications that shape the model's hypothesis about the input-output mapping. Explanations work by making explicit the reasoning principles or decision rules that should be applied, allowing the model to infer the correct task generalization from examples. This effect is distinct from providing more examples or more information: explanations change which features the model attends to and how it generalizes from examples. The effectiveness of explanations depends critically on: (1) model scale (only large models can perform the higher-order inference needed to extract task principles from explanations), (2) the semantic link between examples and explanations (scrambled or mismatched explanations hurt performance), and (3) explanation quality (hand-tuned explanations that clearly specify the task principle work better than generic explanations). Importantly, explanations placed after answers can still improve performance because they affect task inference rather than providing information for the immediate answer.",
    "supporting_evidence": [
        {
            "text": "Adding untuned post-answer explanations to prompts produced +0.25 log-odds increase (~+5.5 percentage points) for 280B model.",
            "uuids": [
                "e198.0"
            ]
        },
        {
            "text": "Hand-tuned explanations produced +1.23 log-odds increase (~+23.8 percentage points) on challenging tasks.",
            "uuids": [
                "e198.1"
            ]
        },
        {
            "text": "Selecting examples jointly with explanations produced +0.64 log-odds increase (~+14.7 percentage points) over selecting examples alone.",
            "uuids": [
                "e198.2"
            ]
        },
        {
            "text": "Control conditions (scrambled explanations, true non-explanations, other-item explanations) were neutral or harmful, with all controls performing significantly worse than real explanations.",
            "uuids": [
                "e198.3"
            ]
        },
        {
            "text": "Larger models benefit more from explanations: positive interaction coefficient +0.077 for log_model_params * explanation.",
            "uuids": [
                "e198.4"
            ]
        },
        {
            "text": "Post-answer explanations can improve performance even though they cannot provide information for the immediate answer, supporting task-inference mechanism.",
            "uuids": [
                "e198.5"
            ]
        },
        {
            "text": "Explanations work by making explicit the reasoning principles or decision rules, allowing models to infer correct task generalization.",
            "uuids": [
                "e198.0"
            ]
        },
        {
            "text": "When explanatory mapping between example and explanation is destroyed, the prompt misleads or fails to convey correct task principle, decreasing performance.",
            "uuids": [
                "e198.3"
            ]
        },
        {
            "text": "Untuned explanations produced modest accuracy increases only for the largest model; smaller models did not reliably benefit.",
            "uuids": [
                "e198.4"
            ]
        }
    ],
    "theory_statements": [
        "Explanations function as meta-level task specifications that shape the model's hypothesis about the input-output mapping, not as object-level factual evidence.",
        "The benefit of explanations scales with model size: ∂P(correct|explanation)/∂log(model_size) &gt; 0, with benefits concentrated in models &gt;100B parameters.",
        "Explanation effectiveness depends on the semantic link between examples and explanations: P(correct|matched_explanation) &gt; P(correct|scrambled_explanation).",
        "Post-answer explanations can improve performance because they affect task inference: P(correct|explanation_after) &gt; P(correct|no_explanation), even though explanations cannot provide information for the immediate answer.",
        "Hand-tuned explanations that clearly specify task principles are more effective than generic explanations: P(correct|tuned_explanation) &gt;&gt; P(correct|untuned_explanation).",
        "Selecting examples jointly with explanations is more effective than selecting examples alone: P(correct|selected_with_explanation) &gt; P(correct|selected_without_explanation).",
        "The benefit of explanations is mediated by the model's ability to perform higher-order inference about task structure."
    ],
    "new_predictions_likely": [
        "Explanations will be most effective for tasks where the correct generalization principle is ambiguous from examples alone.",
        "Providing multiple diverse explanations for the same example will improve performance more than providing a single explanation.",
        "Explanations that explicitly contrast correct and incorrect reasoning will be more effective than explanations that only describe correct reasoning.",
        "Training models with explanation-augmented data will improve their ability to benefit from explanations at inference time.",
        "Explanations will be more effective when they highlight non-obvious features or principles that the model might otherwise miss."
    ],
    "new_predictions_unknown": [
        "Whether there exist explanation formats (e.g., formal logic, structured representations) that are more effective than natural language explanations.",
        "Whether models can be trained to generate their own explanations that improve their task performance (self-explanation).",
        "Whether the explanation-as-task-specification mechanism is the same across different types of tasks or whether different tasks require different explanation strategies.",
        "Whether explanations can be automatically optimized (e.g., through gradient-based methods) to maximize their task-specification effectiveness.",
        "Whether the size threshold for explanation benefits is determined by the model's general reasoning ability or by specific capabilities related to meta-learning."
    ],
    "negative_experiments": [
        "Finding that scrambled or mismatched explanations provide the same benefits as correct explanations would challenge the semantic link requirement.",
        "Demonstrating that small models benefit from explanations as much as large models would challenge the scale-dependent aspect.",
        "Showing that pre-answer explanations (which can provide information for the immediate answer) don't work better than post-answer explanations would challenge the task-inference mechanism.",
        "Finding that generic, untuned explanations work as well as hand-tuned explanations would challenge the quality-dependent aspect."
    ],
    "unaccounted_for": [
        {
            "text": "Why some tasks show explanation benefits at smaller model sizes than others.",
            "uuids": [
                "e198.4"
            ]
        },
        {
            "text": "The exact cognitive or computational mechanisms that enable large models to extract task principles from explanations.",
            "uuids": [
                "e198.0"
            ]
        },
        {
            "text": "Whether the explanation effect is mediated by attention patterns or by deeper changes in the model's internal representations.",
            "uuids": [
                "e198.0"
            ]
        },
        {
            "text": "Why hand-tuning produces such large improvements over untuned explanations.",
            "uuids": [
                "e198.1"
            ]
        }
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Lampinen et al. (2022) Can language models learn from explanations in context? [Introduces and provides evidence for explanation-as-task-specification, this theory formalizes and extends their findings]",
            "Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [Shows benefits of reasoning steps, but focuses on pre-answer chains for computation rather than post-answer explanations for task specification]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [Introduces in-context learning, but doesn't distinguish explanation-based task specification from example-based learning]"
        ]
    },
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Process Abstraction Theory for LM Logical Reasoning - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1156</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1156</p>
                <p><strong>Name:</strong> Dual-Process Abstraction Theory for LM Logical Reasoning</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models (LMs) achieve optimal strict logical reasoning by dynamically integrating two distinct cognitive-like processes: (1) a fast, pattern-matching, context-driven process for abstraction and retrieval of relevant logical schemas, and (2) a slow, rule-based, stepwise process for explicit logical manipulation and inference. The theory asserts that the interplay and adaptive switching between these processes, guided by task complexity and ambiguity, enables both efficiency and rigor in logical reasoning.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Dynamic Dual-Process Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning task &#8594; is_presented_to &#8594; language model</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; engages &#8594; fast abstraction process<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; engages &#8594; slow rule-based process<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; dynamically_switches_between &#8594; the two processes based on task demands</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human reasoning is often modeled as dual-process (Kahneman's System 1 and System 2), and LMs show both fast pattern-matching and slower, stepwise reasoning behaviors. </li>
    <li>LMs can retrieve logical templates quickly but require explicit stepwise prompting for strict logical tasks. </li>
    <li>Empirical results show LMs perform better on logic tasks when both retrieval and explicit reasoning are combined (e.g., retrieval-augmented chain-of-thought). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law adapts dual-process cognitive theory to LMs and formalizes the dynamic interplay for logical reasoning, which is not standard in LM architectures.</p>            <p><strong>What Already Exists:</strong> Dual-process theories are well-established in cognitive science; LMs have been shown to benefit from both retrieval and explicit reasoning.</p>            <p><strong>What is Novel:</strong> The explicit integration and adaptive switching of dual processes within LMs for strict logical reasoning is not formalized in prior LM literature.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory in humans]</li>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning]</li>
    <li>Borgeaud et al. (2022) Improving language models by retrieving from trillions of tokens [retrieval-augmented LMs]</li>
</ul>
            <h3>Statement 1: Adaptive Process Selection Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; reasoning task &#8594; has_property &#8594; high complexity or ambiguity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; increases_reliance_on &#8594; slow rule-based process</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs require explicit, stepwise reasoning for complex logic puzzles, while simple tasks are solved via pattern-matching. </li>
    <li>Prompting LMs to 'think step by step' improves performance on ambiguous or multi-step logic tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to chain-of-thought prompting, the law generalizes the adaptive process selection mechanism.</p>            <p><strong>What Already Exists:</strong> Prompting for stepwise reasoning is known to help LMs on complex tasks.</p>            <p><strong>What is Novel:</strong> The formalization of adaptive process selection based on task complexity/ambiguity is new for LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning]</li>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LMs that explicitly implement dual-process switching will outperform those using only pattern-matching or only stepwise reasoning on a wide range of logic tasks.</li>
                <li>Performance gains will be most pronounced on tasks with variable complexity or ambiguity.</li>
                <li>Analysis of LM activations will reveal distinct modes corresponding to fast abstraction and slow rule-based processing.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>The dual-process approach may enable LMs to generalize logical reasoning to novel domains with minimal additional training.</li>
                <li>Adaptive switching may allow LMs to self-correct logical errors in real time.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LMs with explicit dual-process mechanisms do not outperform single-process LMs on strict logical reasoning, the theory is undermined.</li>
                <li>If no distinct activation patterns corresponding to the two processes are found, the theory's assumptions are challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some logic tasks may not benefit from dual-process integration, especially if they are either trivially simple or require entirely new forms of reasoning. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory adapts and extends dual-process cognitive theory to LMs, formalizing a new mechanism for logical reasoning.</p>
            <p><strong>References:</strong> <ul>
    <li>Kahneman (2011) Thinking, Fast and Slow [dual-process theory in humans]</li>
    <li>Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning]</li>
    <li>Borgeaud et al. (2022) Improving language models by retrieving from trillions of tokens [retrieval-augmented LMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Process Abstraction Theory for LM Logical Reasoning",
    "theory_description": "This theory posits that language models (LMs) achieve optimal strict logical reasoning by dynamically integrating two distinct cognitive-like processes: (1) a fast, pattern-matching, context-driven process for abstraction and retrieval of relevant logical schemas, and (2) a slow, rule-based, stepwise process for explicit logical manipulation and inference. The theory asserts that the interplay and adaptive switching between these processes, guided by task complexity and ambiguity, enables both efficiency and rigor in logical reasoning.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Dynamic Dual-Process Law",
                "if": [
                    {
                        "subject": "reasoning task",
                        "relation": "is_presented_to",
                        "object": "language model"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "engages",
                        "object": "fast abstraction process"
                    },
                    {
                        "subject": "language model",
                        "relation": "engages",
                        "object": "slow rule-based process"
                    },
                    {
                        "subject": "language model",
                        "relation": "dynamically_switches_between",
                        "object": "the two processes based on task demands"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human reasoning is often modeled as dual-process (Kahneman's System 1 and System 2), and LMs show both fast pattern-matching and slower, stepwise reasoning behaviors.",
                        "uuids": []
                    },
                    {
                        "text": "LMs can retrieve logical templates quickly but require explicit stepwise prompting for strict logical tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical results show LMs perform better on logic tasks when both retrieval and explicit reasoning are combined (e.g., retrieval-augmented chain-of-thought).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Dual-process theories are well-established in cognitive science; LMs have been shown to benefit from both retrieval and explicit reasoning.",
                    "what_is_novel": "The explicit integration and adaptive switching of dual processes within LMs for strict logical reasoning is not formalized in prior LM literature.",
                    "classification_explanation": "This law adapts dual-process cognitive theory to LMs and formalizes the dynamic interplay for logical reasoning, which is not standard in LM architectures.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Kahneman (2011) Thinking, Fast and Slow [dual-process theory in humans]",
                        "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning]",
                        "Borgeaud et al. (2022) Improving language models by retrieving from trillions of tokens [retrieval-augmented LMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Adaptive Process Selection Law",
                "if": [
                    {
                        "subject": "reasoning task",
                        "relation": "has_property",
                        "object": "high complexity or ambiguity"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "increases_reliance_on",
                        "object": "slow rule-based process"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs require explicit, stepwise reasoning for complex logic puzzles, while simple tasks are solved via pattern-matching.",
                        "uuids": []
                    },
                    {
                        "text": "Prompting LMs to 'think step by step' improves performance on ambiguous or multi-step logic tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompting for stepwise reasoning is known to help LMs on complex tasks.",
                    "what_is_novel": "The formalization of adaptive process selection based on task complexity/ambiguity is new for LMs.",
                    "classification_explanation": "While related to chain-of-thought prompting, the law generalizes the adaptive process selection mechanism.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning]",
                        "Kahneman (2011) Thinking, Fast and Slow [dual-process theory]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LMs that explicitly implement dual-process switching will outperform those using only pattern-matching or only stepwise reasoning on a wide range of logic tasks.",
        "Performance gains will be most pronounced on tasks with variable complexity or ambiguity.",
        "Analysis of LM activations will reveal distinct modes corresponding to fast abstraction and slow rule-based processing."
    ],
    "new_predictions_unknown": [
        "The dual-process approach may enable LMs to generalize logical reasoning to novel domains with minimal additional training.",
        "Adaptive switching may allow LMs to self-correct logical errors in real time."
    ],
    "negative_experiments": [
        "If LMs with explicit dual-process mechanisms do not outperform single-process LMs on strict logical reasoning, the theory is undermined.",
        "If no distinct activation patterns corresponding to the two processes are found, the theory's assumptions are challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some logic tasks may not benefit from dual-process integration, especially if they are either trivially simple or require entirely new forms of reasoning.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LMs achieve high logical accuracy without explicit dual-process mechanisms, suggesting alternative explanations.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with extremely low or high complexity may not require adaptive switching.",
        "Tasks with non-logical ambiguity (e.g., linguistic ambiguity) may not benefit from this approach."
    ],
    "existing_theory": {
        "what_already_exists": "Dual-process theory in cognitive science; stepwise and retrieval-augmented reasoning in LMs.",
        "what_is_novel": "Explicit, dynamic integration and switching of dual processes within LMs for strict logical reasoning.",
        "classification_explanation": "The theory adapts and extends dual-process cognitive theory to LMs, formalizing a new mechanism for logical reasoning.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Kahneman (2011) Thinking, Fast and Slow [dual-process theory in humans]",
            "Wei et al. (2022) Chain-of-thought prompting elicits reasoning in large language models [stepwise reasoning]",
            "Borgeaud et al. (2022) Improving language models by retrieving from trillions of tokens [retrieval-augmented LMs]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-605",
    "original_theory_name": "Preference Optimization and Hard Negative Sampling for Robust Multi-Step Reasoning",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
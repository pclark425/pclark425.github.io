<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain Randomization Sufficiency Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-351</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-351</p>
                <p><strong>Name:</strong> Domain Randomization Sufficiency Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about sim-to-real transfer for scientific discovery agents that specifies environment fidelity requirements and skill-transfer conditions from virtual labs to real robotics labs.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory specifies the minimum requirements for domain randomization to enable successful sim-to-real transfer in scientific discovery agents. It proposes that domain randomization sufficiency depends on three factors: (1) Parameter Space Coverage - the extent to which randomized parameters span the real-world distribution, (2) Parameter Criticality Classification - identifying which parameters affect scientific validity vs. execution success, and (3) Discovery-Relevant Fidelity - ensuring parameters that influence the causal mechanisms being discovered are prioritized. The theory introduces a Sufficiency Index: SI = α·PCI_scientific + β·PCI_execution + γ·PCI_environmental, where PCI represents Parameter Coverage Index for each category, and weights (α≥β≥γ) reflect relative importance. Sufficiency is achieved when SI ≥ 0.85, with PCI_scientific ≥ 0.90 being necessary but not sufficient. The theory predicts that insufficient coverage of scientifically-critical parameters leads to discovery of spurious relationships that fail to replicate in reality, while insufficient coverage of execution-critical parameters leads to task failure without affecting discovery validity.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Parameters can be classified into three categories based on their impact on scientific discovery agents: (1) Scientific-Critical parameters that affect the causal mechanisms and phenomena being discovered, (2) Execution-Critical parameters that affect the agent's ability to perform experimental protocols but not the underlying science, and (3) Environmental parameters that affect neither discovery validity nor execution success.</li>
                <li>The Sufficiency Index for domain randomization is: SI = α·PCI_scientific + β·PCI_execution + γ·PCI_environmental, where α ≥ β ≥ γ and α + β + γ = 1, with typical values α=0.5, β=0.35, γ=0.15 for scientific discovery tasks.</li>
                <li>Successful sim-to-real transfer for scientific discovery requires SI ≥ 0.85, with the additional constraint that PCI_scientific ≥ 0.90 (necessary but not sufficient condition).</li>
                <li>Parameter Coverage Index is defined as: PCI = |V_sim ∩ V_real| / |V_real|, where V represents the volume of parameter space, measuring what fraction of real-world parameter variations are covered in simulation training.</li>
                <li>Scientific-Critical parameters include: physical constants relevant to the discovery domain (e.g., gravity, viscosity, reaction rates), measurement noise characteristics that affect data quality, temporal dynamics of phenomena under study, and causal dependencies between experimental variables.</li>
                <li>Execution-Critical parameters include: robot kinematics and dynamics, sensor calibration and response characteristics, actuator delays and nonlinearities, and workspace geometry that affects reachability.</li>
                <li>Environmental parameters include: visual appearance (when not measured), background elements, simulation rendering details, and ambient conditions that don't affect measurements.</li>
                <li>Insufficient coverage of Scientific-Critical parameters (PCI_scientific < 0.90) leads to Type I discovery errors: the agent discovers spurious relationships in simulation that fail to replicate in reality because the simulation didn't capture critical causal factors.</li>
                <li>Insufficient coverage of Execution-Critical parameters (PCI_execution < 0.70) leads to Type II discovery errors: the agent fails to execute experiments successfully in reality, preventing discovery even when the simulation's scientific model is accurate.</li>
                <li>Parameter criticality can be determined through: (a) sensitivity analysis (∂Performance/∂Parameter), (b) causal analysis identifying which parameters affect measured outcomes, and (c) domain knowledge about the scientific phenomena being studied.</li>
                <li>For multi-stage discovery processes, parameter criticality may change across stages, requiring adaptive randomization that adjusts coverage based on the current discovery phase.</li>
                <li>The computational cost of domain randomization scales super-linearly with the number of randomized parameters; prioritizing critical parameters can reduce training time by 40-70% while maintaining or improving transfer success.</li>
                <li>Sufficiency can be validated through sim-to-sim transfer tests: training in one simulator with randomization and testing in a different simulator with different physics engines provides a lower bound on real-world transfer success.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Selective randomization of physics parameters most relevant to the task improves transfer success while reducing computational requirements compared to uniform randomization of all parameters. </li>
    <li>Randomizing all parameters equally leads to inefficient learning, poor sample efficiency, and does not guarantee successful transfer, suggesting that parameter selection matters more than exhaustive randomization. </li>
    <li>Parameter sensitivity analysis can identify which simulation parameters most affect task performance and should be prioritized for randomization to achieve efficient transfer. </li>
    <li>Visual domain randomization (textures, lighting, colors) can improve transfer even for non-vision-critical tasks, possibly through regularization effects that prevent overfitting to simulation artifacts. </li>
    <li>Adaptive and active domain randomization approaches that learn which parameters to randomize can improve transfer efficiency compared to fixed randomization schemes. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A scientific discovery agent trained with PCI_scientific = 0.95 and PCI_execution = 0.60 will successfully discover valid scientific relationships in the real world but may fail to execute some experimental protocols, requiring execution adaptation but not re-discovery.</li>
                <li>An agent trained with PCI_scientific = 0.70 and PCI_execution = 0.95 will execute experiments successfully but will discover relationships that include simulation artifacts, requiring re-training to discover valid science.</li>
                <li>For a chemistry discovery task studying reaction kinetics, randomizing temperature, concentration, and stirring rate parameters will be Scientific-Critical, while randomizing robot arm dynamics will be Execution-Critical, with a measurable difference in their impact on discovery validity vs. execution success.</li>
                <li>Sensitivity analysis conducted in simulation will correctly classify 75-85% of parameters into the appropriate criticality category, with misclassifications primarily occurring for parameters with complex interaction effects.</li>
                <li>An agent achieving SI = 0.87 will have >90% probability of successful sim-to-real transfer, while an agent with SI = 0.75 will have <60% probability, demonstrating the threshold effect of the sufficiency condition.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether the sufficiency threshold (SI ≥ 0.85) is universal across all scientific domains or requires domain-specific calibration - this would determine if the theory provides a general principle or needs per-domain tuning, with major implications for practical deployment.</li>
                <li>Whether interaction effects between parameters can cause individually non-critical parameters to become collectively critical - this would require higher-order analysis and could fundamentally change how we compute sufficiency.</li>
                <li>Whether the weight ratios (α=0.5, β=0.35, γ=0.15) are task-invariant or depend on the complexity of the scientific phenomena being discovered - this would affect how we apply the theory to new domains.</li>
                <li>Whether active learning approaches can reduce the required PCI_scientific threshold below 0.90 by intelligently sampling the parameter space to cover the most informative regions - this could dramatically improve efficiency if true.</li>
                <li>Whether sufficiency for discovering qualitative relationships (e.g., 'A increases B') requires lower coverage than discovering quantitative relationships (e.g., 'B = 2.3·A^1.5') - this would create a hierarchy of sufficiency requirements based on discovery goals.</li>
                <li>Whether temporal correlations in parameter variations (e.g., equipment degradation over time) require explicit modeling in the randomization scheme or are implicitly captured by sufficient static coverage - this affects long-term deployment strategies.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding a case where an agent with SI = 0.90 and PCI_scientific = 0.95 still discovers spurious relationships that fail in reality would challenge the sufficiency threshold and suggest missing parameter categories.</li>
                <li>Demonstrating successful transfer and valid discovery with PCI_scientific = 0.70 would contradict the necessary condition and suggest the threshold is too conservative.</li>
                <li>Showing that a parameter classified as Environmental through sensitivity analysis proves to be Scientific-Critical in reality would challenge the classification methodology and suggest limitations of simulation-based sensitivity analysis.</li>
                <li>Finding that computational cost does not decrease with selective randomization, or that selective randomization degrades transfer success, would question the efficiency-effectiveness tradeoff claimed by the theory.</li>
                <li>Discovering that the weight parameters (α, β, γ) need to be inverted (γ>β>α) for certain discovery tasks would fundamentally challenge the priority ordering of parameter categories.</li>
                <li>Demonstrating that sim-to-sim transfer success does not predict real-world transfer success would invalidate the proposed validation methodology.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to handle parameters that are difficult or impossible to measure accurately in the real world, creating fundamental uncertainty in computing PCI and validating sufficiency. </li>
    <li>Temporal correlations between parameters (e.g., wear and tear, equipment drift, environmental changes over experimental campaigns) are not addressed in the static parameter coverage framework. </li>
    <li>The theory does not address how to handle systematic biases or modeling errors in simulation that cannot be corrected through randomization alone, such as missing physics or incorrect causal structures. </li>
    <li>The interaction between domain randomization and the agent's learning algorithm (e.g., model-based vs. model-free, sample efficiency, generalization capacity) is not explicitly modeled in the sufficiency conditions. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Tobin et al. (2017) Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World [Foundational work on domain randomization but does not provide sufficiency conditions or parameter classification for scientific discovery]</li>
    <li>Mehta et al. (2020) Active Domain Randomization [Addresses parameter selection through active learning but does not provide a sufficiency theory or distinguish scientific vs. execution criticality]</li>
    <li>Muratore et al. (2021) Neural Posterior Domain Randomization [Addresses parameter identification and adaptation but does not provide sufficiency conditions or a classification framework]</li>
    <li>Chebotar et al. (2019) Closing the Sim-to-Real Loop [Addresses adaptive randomization but does not provide a sufficiency theory or focus on scientific discovery]</li>
    <li>Zhao et al. (2020) Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey [Survey of sim-to-real methods but does not propose a sufficiency theory for domain randomization]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain Randomization Sufficiency Theory",
    "theory_description": "This theory specifies the minimum requirements for domain randomization to enable successful sim-to-real transfer in scientific discovery agents. It proposes that domain randomization sufficiency depends on three factors: (1) Parameter Space Coverage - the extent to which randomized parameters span the real-world distribution, (2) Parameter Criticality Classification - identifying which parameters affect scientific validity vs. execution success, and (3) Discovery-Relevant Fidelity - ensuring parameters that influence the causal mechanisms being discovered are prioritized. The theory introduces a Sufficiency Index: SI = α·PCI_scientific + β·PCI_execution + γ·PCI_environmental, where PCI represents Parameter Coverage Index for each category, and weights (α≥β≥γ) reflect relative importance. Sufficiency is achieved when SI ≥ 0.85, with PCI_scientific ≥ 0.90 being necessary but not sufficient. The theory predicts that insufficient coverage of scientifically-critical parameters leads to discovery of spurious relationships that fail to replicate in reality, while insufficient coverage of execution-critical parameters leads to task failure without affecting discovery validity.",
    "supporting_evidence": [
        {
            "text": "Selective randomization of physics parameters most relevant to the task improves transfer success while reducing computational requirements compared to uniform randomization of all parameters.",
            "citations": [
                "Chebotar et al. (2019) Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience, ICRA",
                "OpenAI et al. (2019) Solving Rubik's Cube with a Robot Hand, arXiv"
            ]
        },
        {
            "text": "Randomizing all parameters equally leads to inefficient learning, poor sample efficiency, and does not guarantee successful transfer, suggesting that parameter selection matters more than exhaustive randomization.",
            "citations": [
                "Mehta et al. (2020) Active Domain Randomization, CoRL",
                "Muratore et al. (2021) Neural Posterior Domain Randomization, CoRL"
            ]
        },
        {
            "text": "Parameter sensitivity analysis can identify which simulation parameters most affect task performance and should be prioritized for randomization to achieve efficient transfer.",
            "citations": [
                "Tiboni et al. (2022) Sim-to-Real Transfer for Robotic Manipulation with Tactile Sensory, Robotics"
            ]
        },
        {
            "text": "Visual domain randomization (textures, lighting, colors) can improve transfer even for non-vision-critical tasks, possibly through regularization effects that prevent overfitting to simulation artifacts.",
            "citations": [
                "Tobin et al. (2017) Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, IROS"
            ]
        },
        {
            "text": "Adaptive and active domain randomization approaches that learn which parameters to randomize can improve transfer efficiency compared to fixed randomization schemes.",
            "citations": [
                "Mehta et al. (2020) Active Domain Randomization, CoRL",
                "Muratore et al. (2021) Neural Posterior Domain Randomization, CoRL"
            ]
        }
    ],
    "theory_statements": [
        "Parameters can be classified into three categories based on their impact on scientific discovery agents: (1) Scientific-Critical parameters that affect the causal mechanisms and phenomena being discovered, (2) Execution-Critical parameters that affect the agent's ability to perform experimental protocols but not the underlying science, and (3) Environmental parameters that affect neither discovery validity nor execution success.",
        "The Sufficiency Index for domain randomization is: SI = α·PCI_scientific + β·PCI_execution + γ·PCI_environmental, where α ≥ β ≥ γ and α + β + γ = 1, with typical values α=0.5, β=0.35, γ=0.15 for scientific discovery tasks.",
        "Successful sim-to-real transfer for scientific discovery requires SI ≥ 0.85, with the additional constraint that PCI_scientific ≥ 0.90 (necessary but not sufficient condition).",
        "Parameter Coverage Index is defined as: PCI = |V_sim ∩ V_real| / |V_real|, where V represents the volume of parameter space, measuring what fraction of real-world parameter variations are covered in simulation training.",
        "Scientific-Critical parameters include: physical constants relevant to the discovery domain (e.g., gravity, viscosity, reaction rates), measurement noise characteristics that affect data quality, temporal dynamics of phenomena under study, and causal dependencies between experimental variables.",
        "Execution-Critical parameters include: robot kinematics and dynamics, sensor calibration and response characteristics, actuator delays and nonlinearities, and workspace geometry that affects reachability.",
        "Environmental parameters include: visual appearance (when not measured), background elements, simulation rendering details, and ambient conditions that don't affect measurements.",
        "Insufficient coverage of Scientific-Critical parameters (PCI_scientific &lt; 0.90) leads to Type I discovery errors: the agent discovers spurious relationships in simulation that fail to replicate in reality because the simulation didn't capture critical causal factors.",
        "Insufficient coverage of Execution-Critical parameters (PCI_execution &lt; 0.70) leads to Type II discovery errors: the agent fails to execute experiments successfully in reality, preventing discovery even when the simulation's scientific model is accurate.",
        "Parameter criticality can be determined through: (a) sensitivity analysis (∂Performance/∂Parameter), (b) causal analysis identifying which parameters affect measured outcomes, and (c) domain knowledge about the scientific phenomena being studied.",
        "For multi-stage discovery processes, parameter criticality may change across stages, requiring adaptive randomization that adjusts coverage based on the current discovery phase.",
        "The computational cost of domain randomization scales super-linearly with the number of randomized parameters; prioritizing critical parameters can reduce training time by 40-70% while maintaining or improving transfer success.",
        "Sufficiency can be validated through sim-to-sim transfer tests: training in one simulator with randomization and testing in a different simulator with different physics engines provides a lower bound on real-world transfer success."
    ],
    "new_predictions_likely": [
        "A scientific discovery agent trained with PCI_scientific = 0.95 and PCI_execution = 0.60 will successfully discover valid scientific relationships in the real world but may fail to execute some experimental protocols, requiring execution adaptation but not re-discovery.",
        "An agent trained with PCI_scientific = 0.70 and PCI_execution = 0.95 will execute experiments successfully but will discover relationships that include simulation artifacts, requiring re-training to discover valid science.",
        "For a chemistry discovery task studying reaction kinetics, randomizing temperature, concentration, and stirring rate parameters will be Scientific-Critical, while randomizing robot arm dynamics will be Execution-Critical, with a measurable difference in their impact on discovery validity vs. execution success.",
        "Sensitivity analysis conducted in simulation will correctly classify 75-85% of parameters into the appropriate criticality category, with misclassifications primarily occurring for parameters with complex interaction effects.",
        "An agent achieving SI = 0.87 will have &gt;90% probability of successful sim-to-real transfer, while an agent with SI = 0.75 will have &lt;60% probability, demonstrating the threshold effect of the sufficiency condition."
    ],
    "new_predictions_unknown": [
        "Whether the sufficiency threshold (SI ≥ 0.85) is universal across all scientific domains or requires domain-specific calibration - this would determine if the theory provides a general principle or needs per-domain tuning, with major implications for practical deployment.",
        "Whether interaction effects between parameters can cause individually non-critical parameters to become collectively critical - this would require higher-order analysis and could fundamentally change how we compute sufficiency.",
        "Whether the weight ratios (α=0.5, β=0.35, γ=0.15) are task-invariant or depend on the complexity of the scientific phenomena being discovered - this would affect how we apply the theory to new domains.",
        "Whether active learning approaches can reduce the required PCI_scientific threshold below 0.90 by intelligently sampling the parameter space to cover the most informative regions - this could dramatically improve efficiency if true.",
        "Whether sufficiency for discovering qualitative relationships (e.g., 'A increases B') requires lower coverage than discovering quantitative relationships (e.g., 'B = 2.3·A^1.5') - this would create a hierarchy of sufficiency requirements based on discovery goals.",
        "Whether temporal correlations in parameter variations (e.g., equipment degradation over time) require explicit modeling in the randomization scheme or are implicitly captured by sufficient static coverage - this affects long-term deployment strategies."
    ],
    "negative_experiments": [
        "Finding a case where an agent with SI = 0.90 and PCI_scientific = 0.95 still discovers spurious relationships that fail in reality would challenge the sufficiency threshold and suggest missing parameter categories.",
        "Demonstrating successful transfer and valid discovery with PCI_scientific = 0.70 would contradict the necessary condition and suggest the threshold is too conservative.",
        "Showing that a parameter classified as Environmental through sensitivity analysis proves to be Scientific-Critical in reality would challenge the classification methodology and suggest limitations of simulation-based sensitivity analysis.",
        "Finding that computational cost does not decrease with selective randomization, or that selective randomization degrades transfer success, would question the efficiency-effectiveness tradeoff claimed by the theory.",
        "Discovering that the weight parameters (α, β, γ) need to be inverted (γ&gt;β&gt;α) for certain discovery tasks would fundamentally challenge the priority ordering of parameter categories.",
        "Demonstrating that sim-to-sim transfer success does not predict real-world transfer success would invalidate the proposed validation methodology."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to handle parameters that are difficult or impossible to measure accurately in the real world, creating fundamental uncertainty in computing PCI and validating sufficiency.",
            "citations": [
                "Muratore et al. (2021) Neural Posterior Domain Randomization, CoRL"
            ]
        },
        {
            "text": "Temporal correlations between parameters (e.g., wear and tear, equipment drift, environmental changes over experimental campaigns) are not addressed in the static parameter coverage framework.",
            "citations": [
                "Kaspar et al. (2021) Controlling an Organic Synthesis Robot with Machine Learning to Search for New Reactivity, Nature"
            ]
        },
        {
            "text": "The theory does not address how to handle systematic biases or modeling errors in simulation that cannot be corrected through randomization alone, such as missing physics or incorrect causal structures.",
            "citations": [
                "Chebotar et al. (2019) Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience, ICRA"
            ]
        },
        {
            "text": "The interaction between domain randomization and the agent's learning algorithm (e.g., model-based vs. model-free, sample efficiency, generalization capacity) is not explicitly modeled in the sufficiency conditions.",
            "citations": [
                "Mehta et al. (2020) Active Domain Randomization, CoRL"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that randomizing visual and environmental parameters improves transfer even when they are classified as non-critical, possibly due to regularization effects that prevent overfitting to simulation artifacts, suggesting the Environmental category may be more important than the theory's low weight (γ=0.15) indicates.",
            "citations": [
                "Tobin et al. (2017) Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, IROS"
            ]
        },
        {
            "text": "Active domain randomization approaches suggest that parameter importance can be learned during training rather than determined a priori, which conflicts with the theory's assumption that criticality classification precedes training.",
            "citations": [
                "Mehta et al. (2020) Active Domain Randomization, CoRL"
            ]
        }
    ],
    "special_cases": [
        "For vision-based scientific discovery tasks where visual features are part of the measurement (e.g., observing color changes in chemistry, morphological changes in biology), visual parameters are promoted from Environmental to Scientific-Critical, requiring α_visual ≈ α_scientific.",
        "In contact-rich manipulation tasks for scientific discovery (e.g., materials testing, mechanical property measurement), friction and contact parameters are always Scientific-Critical regardless of the specific discovery objective.",
        "For time-sensitive discovery tasks studying transient phenomena (e.g., reaction kinetics, dynamic systems), temporal parameters including simulation step size and sensor sampling rates become Scientific-Critical.",
        "When the discovery task involves learning causal relationships between actions and outcomes, actuator response characteristics are promoted from Execution-Critical to Scientific-Critical because they affect the causal model.",
        "For multi-modal discovery tasks that combine different types of measurements (e.g., visual + force + chemical), sufficiency must be achieved independently for each modality: SI_total = min(SI_visual, SI_force, SI_chemical).",
        "In hierarchical discovery tasks where high-level scientific relationships depend on low-level experimental skills, Execution-Critical parameters may indirectly affect discovery validity, requiring β to be increased relative to α.",
        "When using sim-to-sim transfer as a validation method, simulator-specific artifacts and numerical precision differences must be treated as additional parameter dimensions to randomize over."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Tobin et al. (2017) Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World [Foundational work on domain randomization but does not provide sufficiency conditions or parameter classification for scientific discovery]",
            "Mehta et al. (2020) Active Domain Randomization [Addresses parameter selection through active learning but does not provide a sufficiency theory or distinguish scientific vs. execution criticality]",
            "Muratore et al. (2021) Neural Posterior Domain Randomization [Addresses parameter identification and adaptation but does not provide sufficiency conditions or a classification framework]",
            "Chebotar et al. (2019) Closing the Sim-to-Real Loop [Addresses adaptive randomization but does not provide a sufficiency theory or focus on scientific discovery]",
            "Zhao et al. (2020) Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey [Survey of sim-to-real methods but does not propose a sufficiency theory for domain randomization]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 1,
    "theory_query": "Build a theory about sim-to-real transfer for scientific discovery agents that specifies environment fidelity requirements and skill-transfer conditions from virtual labs to real robotics labs.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-187",
    "original_theory_name": "Domain Randomization Sufficiency Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Synthetic-to-Real Transfer Effectiveness Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-72</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-72</p>
                <p><strong>Name:</strong> Synthetic-to-Real Transfer Effectiveness Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how scientific procedural knowledge transfers across different experimental domains and contexts, based on the following results.</p>
                <p><strong>Description:</strong> Transfer from synthetic to real data is most effective when: (1) synthetic data captures target appearance diversity through randomization or realistic rendering, (2) synthetic labels are reliable and aligned with real-world semantics, (3) models are prevented from overfitting to synthetic artifacts through architectural choices (frozen pretrained backbones, appropriate regularization), and (4) fine-tuning on limited real data bridges remaining domain gaps. The theory predicts that synthetic pretraining value increases non-linearly with real data scarcity, showing largest benefits when real data is extremely limited (e.g., <500 examples) and diminishing returns as real data increases. Domain randomization succeeds by forcing invariance to synthetic artifacts through extreme variation, while photorealistic synthesis succeeds by minimizing the domain gap. Task-specific filtering (e.g., Precision-Recall based on auxiliary detectors) is more effective than general quality metrics (e.g., FID) because it directly evaluates whether generated objects meet task requirements rather than overall image realism. The optimal strategy combines multiple generation approaches (randomization, realistic rendering, compositing with varied blending) to maximize coverage of appearance space while maintaining label reliability.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Synthetic data value increases non-linearly with real data scarcity, showing largest multiplicative benefits when real data < 500 examples</li>
                <li>Domain randomization and photorealistic synthesis represent complementary strategies: randomization forces invariance through extreme variation, while photorealism minimizes domain gap through accurate appearance modeling</li>
                <li>Frozen pretrained backbones prevent overfitting to synthetic artifacts by preserving general visual features while allowing task-specific adaptation through trainable heads</li>
                <li>Task-specific filtering (e.g., Precision-Recall using auxiliary detectors) is more effective than general quality metrics (e.g., FID) because it directly evaluates task-relevant properties rather than overall image realism</li>
                <li>Synthetic pretraining provides diminishing returns as real data quantity increases, with crossover point typically between 1,000-5,000 real examples depending on task complexity</li>
                <li>Combining multiple synthetic generation strategies (randomization, realistic rendering, compositing with varied blending) is more robust than any single approach because it maximizes coverage of appearance space</li>
                <li>The optimal synthetic-to-real ratio depends on: (1) synthetic generation quality, (2) real data quantity, (3) task complexity, and (4) domain gap magnitude, with typical optimal ratios ranging from 3:1 to 30:1</li>
                <li>Filtering synthetic data is most valuable in low-data regimes; as synthetic data quantity increases, unfiltered data approaches filtered performance</li>
                <li>Architectural choices (frozen backbones, regularization schedules) are as important as synthetic data quality for successful transfer</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Domain randomization enables sim-to-real transfer in robotics by creating diverse non-photorealistic variations that force models to learn invariant features <a href="../results/extraction-result-561.html#e561.1" class="evidence-link">[e561.1]</a> </li>
    <li>Generated image pretraining with frozen ImageNet-pretrained backbone improves detection with limited real data: 300 real + 9,000 generated achieved mAP comparable to 4,500 real images; 300 real + 750 filtered generated outperformed 1,500 real images <a href="../results/extraction-result-421.html#e421.1" class="evidence-link">[e421.1]</a> </li>
    <li>Compositing real object masks onto real backgrounds produces effective synthetic training data, with performance approaching or exceeding some baselines <a href="../results/extraction-result-570.html#e570.2" class="evidence-link">[e570.2]</a> </li>
    <li>Virtual KITTI photorealistic rendering enables transfer to real driving scenarios, with Faster R-CNN trained on VKITTI achieving AP@0.5 = 79.7 on KITTI subset <a href="../results/extraction-result-561.html#e561.2" class="evidence-link">[e561.2]</a> </li>
    <li>Precision-Recall filtering of generated images using auxiliary detector improves low-data regime performance substantially, though filtering removed ~56% of generated images <a href="../results/extraction-result-421.html#e421.3" class="evidence-link">[e421.3]</a> </li>
    <li>Multiple blending modes (including Poisson blending, Gaussian blur, no blending) increase synthetic data diversity: ensemble approach (All Blend + same image) achieved mAP 73.7 vs single methods <a href="../results/extraction-result-570.html#e570.0" class="evidence-link">[e570.0]</a> </li>
    <li>Poisson blending alone produced lower detection performance (mAP lower than 'No blending'), but contributed to higher overall performance when used as one member of an ensemble <a href="../results/extraction-result-570.html#e570.0" class="evidence-link">[e570.0]</a> </li>
    <li>FID filtering doesn't reliably predict detector performance: three generated subsets with different FID values (47, 105, 148) produced very similar downstream detection mAPs, indicating FID measures global image distribution rather than task-specific utility <a href="../results/extraction-result-421.html#e421.4" class="evidence-link">[e421.4]</a> </li>
    <li>3D rotation augmentation from turntable captures improves viewpoint diversity: disabling 3D rotation reduced performance (No 3D Rotation mAP 68.3 vs All mAP 73.7) <a href="../results/extraction-result-570.html#e570.4" class="evidence-link">[e570.4]</a> </li>
    <li>GLIGEN-generated images with CLIP grounding enable open-set object generation for unseen target types, though generator struggles with overlapped objects and small targets <a href="../results/extraction-result-421.html#e421.6" class="evidence-link">[e421.6]</a> </li>
    <li>Frozen ImageNet-pretrained ResNet-50 backbone prevents overfitting to synthetic labeling noise while allowing task-specific adaptation through trainable detection heads <a href="../results/extraction-result-421.html#e421.5" class="evidence-link">[e421.5]</a> </li>
    <li>Fourier positional embedding enables effective spatial grounding in generated images, with ablation showing substantial improvement (YOLO AP 21.7 with Fourier vs 3.2 with learned MLP-only) <a href="../results/extraction-result-577.html#e577.2" class="evidence-link">[e577.2]</a> </li>
    <li>Training only on 9,000 generated images (no fine-tuning) achieved mAP similar to ~1,500 real images, demonstrating substantial standalone value of synthetic data <a href="../results/extraction-result-421.html#e421.1" class="evidence-link">[e421.1]</a> </li>
    <li>Different weight decay during pretraining (0.001) vs fine-tuning (0.01) helps prevent overfitting to synthetic artifacts while enabling adaptation to real data <a href="../results/extraction-result-421.html#e421.1" class="evidence-link">[e421.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Synthetic data will show largest benefits (>5x data efficiency) for rare object detection where real examples are extremely scarce (<100 examples)</li>
                <li>Combining domain randomization with realistic rendering will outperform either approach alone by 10-20% mAP in most vision tasks</li>
                <li>Synthetic pretraining will show larger benefits for fine-grained recognition tasks (e.g., species identification) than coarse categorization (e.g., vehicle vs. non-vehicle)</li>
                <li>Generative models conditioned on physical constraints (e.g., free-flow reachability, spatial relationships) will produce more useful synthetic data than unconstrained generation</li>
                <li>For tasks requiring precise localization, Precision-Recall filtering will outperform FID filtering by >15% in low-data regimes</li>
                <li>Synthetic data generated with multiple blending modes will transfer better than single-mode generation across diverse real-world conditions</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether synthetic data generated by very large foundation models (e.g., GPT-4 scale vision models) will eventually eliminate the need for real data in some domains, or whether fundamental limitations of synthetic data will persist</li>
                <li>Whether there exists a theoretical limit to synthetic data utility (e.g., maximum achievable performance with infinite synthetic data) beyond which more synthetic data provides no benefit regardless of quality</li>
                <li>Whether synthetic data can capture rare failure modes and edge cases (e.g., adversarial examples, distribution shifts) as effectively as real data, or whether real-world complexity is fundamentally irreducible</li>
                <li>Whether models trained primarily on synthetic data will generalize to real-world distribution shifts as well as models trained on real data, particularly for safety-critical applications</li>
                <li>Whether the optimal frozen-vs-trainable backbone decision changes with synthetic data quality, or whether freezing is universally beneficial for synthetic pretraining</li>
                <li>Whether task-specific filtering can be automated to match human expert filtering, or whether human judgment remains essential for identifying useful synthetic examples</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that synthetic pretraining consistently harms performance even with extremely limited real data (<100 examples) would fundamentally challenge the theory</li>
                <li>Demonstrating that photorealistic synthesis and domain randomization are equally effective across all tasks and domains would challenge the complementary strategies claim and suggest a simpler unified mechanism</li>
                <li>Showing that task-agnostic quality metrics (e.g., FID, IS) predict transfer success as well as task-specific metrics would challenge the filtering prediction and suggest synthetic data quality is more general than theorized</li>
                <li>Finding that synthetic data provides equal benefit regardless of real data quantity (no diminishing returns) would contradict the non-linear value prediction</li>
                <li>Demonstrating that trainable backbones consistently outperform frozen backbones for synthetic pretraining would challenge the overfitting prevention mechanism</li>
                <li>Finding that single-strategy synthetic generation (e.g., only randomization or only photorealism) consistently matches or exceeds multi-strategy approaches would challenge the complementarity claim</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The specific mechanisms by which different blending modes contribute to ensemble performance - why does combining methods that individually underperform produce superior results? <a href="../results/extraction-result-570.html#e570.0" class="evidence-link">[e570.0]</a> </li>
    <li>Why FID fails as a predictor of detection performance - what aspects of image quality matter for detection vs. generation, and can we develop better metrics? <a href="../results/extraction-result-421.html#e421.4" class="evidence-link">[e421.4]</a> </li>
    <li>The interaction between synthetic data quantity and filtering strategies - at what point does filtering become unnecessary or counterproductive? <a href="../results/extraction-result-421.html#e421.3" class="evidence-link">[e421.3]</a> </li>
    <li>How to optimally balance different synthetic generation strategies - what is the optimal mix of randomization, photorealism, and compositing for different tasks? <a href="../results/extraction-result-570.html#e570.0" class="evidence-link">[e570.0]</a> <a href="../results/extraction-result-561.html#e561.1" class="evidence-link">[e561.1]</a> <a href="../results/extraction-result-561.html#e561.2" class="evidence-link">[e561.2]</a> </li>
    <li>Whether certain types of synthetic artifacts are more harmful than others - are some generation failures more detrimental to transfer than others? <a href="../results/extraction-result-421.html#e421.3" class="evidence-link">[e421.3]</a> <a href="../results/extraction-result-421.html#e421.6" class="evidence-link">[e421.6]</a> </li>
    <li>How synthetic data quality requirements change with model architecture - do larger models require higher quality synthetic data, or are they more robust to synthetic artifacts? <a href="../results/extraction-result-421.html#e421.5" class="evidence-link">[e421.5]</a> </li>
    <li>The role of CLIP fine-tuning for niche domains - when is CLIP fine-tuning necessary vs. when is pretrained CLIP sufficient? <a href="../results/extraction-result-421.html#e421.6" class="evidence-link">[e421.6]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Tobin et al. (2017) Domain randomization for transferring deep neural networks from simulation to the real world [Original domain randomization work, foundational to randomization strategy]</li>
    <li>Tremblay et al. (2018) Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization [Comprehensive study of synthetic data strategies in robotics]</li>
    <li>Kar et al. (2019) Meta-Sim: Learning to Generate Synthetic Datasets [Discusses synthetic data generation but focuses on learning generation parameters rather than comprehensive transfer theory]</li>
    <li>Dwibedi et al. (2017) Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection [Demonstrates compositing approach but doesn't provide unified theory]</li>
    <li>Heusel et al. (2017) GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium [Introduces FID metric but doesn't address its limitations for task-specific evaluation]</li>
    <li>Radford et al. (2021) Learning Transferable Visual Models From Natural Language Supervision [CLIP work, relevant to grounding but doesn't address synthetic-to-real transfer comprehensively]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Synthetic-to-Real Transfer Effectiveness Theory",
    "theory_description": "Transfer from synthetic to real data is most effective when: (1) synthetic data captures target appearance diversity through randomization or realistic rendering, (2) synthetic labels are reliable and aligned with real-world semantics, (3) models are prevented from overfitting to synthetic artifacts through architectural choices (frozen pretrained backbones, appropriate regularization), and (4) fine-tuning on limited real data bridges remaining domain gaps. The theory predicts that synthetic pretraining value increases non-linearly with real data scarcity, showing largest benefits when real data is extremely limited (e.g., &lt;500 examples) and diminishing returns as real data increases. Domain randomization succeeds by forcing invariance to synthetic artifacts through extreme variation, while photorealistic synthesis succeeds by minimizing the domain gap. Task-specific filtering (e.g., Precision-Recall based on auxiliary detectors) is more effective than general quality metrics (e.g., FID) because it directly evaluates whether generated objects meet task requirements rather than overall image realism. The optimal strategy combines multiple generation approaches (randomization, realistic rendering, compositing with varied blending) to maximize coverage of appearance space while maintaining label reliability.",
    "supporting_evidence": [
        {
            "text": "Domain randomization enables sim-to-real transfer in robotics by creating diverse non-photorealistic variations that force models to learn invariant features",
            "uuids": [
                "e561.1"
            ]
        },
        {
            "text": "Generated image pretraining with frozen ImageNet-pretrained backbone improves detection with limited real data: 300 real + 9,000 generated achieved mAP comparable to 4,500 real images; 300 real + 750 filtered generated outperformed 1,500 real images",
            "uuids": [
                "e421.1"
            ]
        },
        {
            "text": "Compositing real object masks onto real backgrounds produces effective synthetic training data, with performance approaching or exceeding some baselines",
            "uuids": [
                "e570.2"
            ]
        },
        {
            "text": "Virtual KITTI photorealistic rendering enables transfer to real driving scenarios, with Faster R-CNN trained on VKITTI achieving AP@0.5 = 79.7 on KITTI subset",
            "uuids": [
                "e561.2"
            ]
        },
        {
            "text": "Precision-Recall filtering of generated images using auxiliary detector improves low-data regime performance substantially, though filtering removed ~56% of generated images",
            "uuids": [
                "e421.3"
            ]
        },
        {
            "text": "Multiple blending modes (including Poisson blending, Gaussian blur, no blending) increase synthetic data diversity: ensemble approach (All Blend + same image) achieved mAP 73.7 vs single methods",
            "uuids": [
                "e570.0"
            ]
        },
        {
            "text": "Poisson blending alone produced lower detection performance (mAP lower than 'No blending'), but contributed to higher overall performance when used as one member of an ensemble",
            "uuids": [
                "e570.0"
            ]
        },
        {
            "text": "FID filtering doesn't reliably predict detector performance: three generated subsets with different FID values (47, 105, 148) produced very similar downstream detection mAPs, indicating FID measures global image distribution rather than task-specific utility",
            "uuids": [
                "e421.4"
            ]
        },
        {
            "text": "3D rotation augmentation from turntable captures improves viewpoint diversity: disabling 3D rotation reduced performance (No 3D Rotation mAP 68.3 vs All mAP 73.7)",
            "uuids": [
                "e570.4"
            ]
        },
        {
            "text": "GLIGEN-generated images with CLIP grounding enable open-set object generation for unseen target types, though generator struggles with overlapped objects and small targets",
            "uuids": [
                "e421.6"
            ]
        },
        {
            "text": "Frozen ImageNet-pretrained ResNet-50 backbone prevents overfitting to synthetic labeling noise while allowing task-specific adaptation through trainable detection heads",
            "uuids": [
                "e421.5"
            ]
        },
        {
            "text": "Fourier positional embedding enables effective spatial grounding in generated images, with ablation showing substantial improvement (YOLO AP 21.7 with Fourier vs 3.2 with learned MLP-only)",
            "uuids": [
                "e577.2"
            ]
        },
        {
            "text": "Training only on 9,000 generated images (no fine-tuning) achieved mAP similar to ~1,500 real images, demonstrating substantial standalone value of synthetic data",
            "uuids": [
                "e421.1"
            ]
        },
        {
            "text": "Different weight decay during pretraining (0.001) vs fine-tuning (0.01) helps prevent overfitting to synthetic artifacts while enabling adaptation to real data",
            "uuids": [
                "e421.1"
            ]
        }
    ],
    "theory_statements": [
        "Synthetic data value increases non-linearly with real data scarcity, showing largest multiplicative benefits when real data &lt; 500 examples",
        "Domain randomization and photorealistic synthesis represent complementary strategies: randomization forces invariance through extreme variation, while photorealism minimizes domain gap through accurate appearance modeling",
        "Frozen pretrained backbones prevent overfitting to synthetic artifacts by preserving general visual features while allowing task-specific adaptation through trainable heads",
        "Task-specific filtering (e.g., Precision-Recall using auxiliary detectors) is more effective than general quality metrics (e.g., FID) because it directly evaluates task-relevant properties rather than overall image realism",
        "Synthetic pretraining provides diminishing returns as real data quantity increases, with crossover point typically between 1,000-5,000 real examples depending on task complexity",
        "Combining multiple synthetic generation strategies (randomization, realistic rendering, compositing with varied blending) is more robust than any single approach because it maximizes coverage of appearance space",
        "The optimal synthetic-to-real ratio depends on: (1) synthetic generation quality, (2) real data quantity, (3) task complexity, and (4) domain gap magnitude, with typical optimal ratios ranging from 3:1 to 30:1",
        "Filtering synthetic data is most valuable in low-data regimes; as synthetic data quantity increases, unfiltered data approaches filtered performance",
        "Architectural choices (frozen backbones, regularization schedules) are as important as synthetic data quality for successful transfer"
    ],
    "new_predictions_likely": [
        "Synthetic data will show largest benefits (&gt;5x data efficiency) for rare object detection where real examples are extremely scarce (&lt;100 examples)",
        "Combining domain randomization with realistic rendering will outperform either approach alone by 10-20% mAP in most vision tasks",
        "Synthetic pretraining will show larger benefits for fine-grained recognition tasks (e.g., species identification) than coarse categorization (e.g., vehicle vs. non-vehicle)",
        "Generative models conditioned on physical constraints (e.g., free-flow reachability, spatial relationships) will produce more useful synthetic data than unconstrained generation",
        "For tasks requiring precise localization, Precision-Recall filtering will outperform FID filtering by &gt;15% in low-data regimes",
        "Synthetic data generated with multiple blending modes will transfer better than single-mode generation across diverse real-world conditions"
    ],
    "new_predictions_unknown": [
        "Whether synthetic data generated by very large foundation models (e.g., GPT-4 scale vision models) will eventually eliminate the need for real data in some domains, or whether fundamental limitations of synthetic data will persist",
        "Whether there exists a theoretical limit to synthetic data utility (e.g., maximum achievable performance with infinite synthetic data) beyond which more synthetic data provides no benefit regardless of quality",
        "Whether synthetic data can capture rare failure modes and edge cases (e.g., adversarial examples, distribution shifts) as effectively as real data, or whether real-world complexity is fundamentally irreducible",
        "Whether models trained primarily on synthetic data will generalize to real-world distribution shifts as well as models trained on real data, particularly for safety-critical applications",
        "Whether the optimal frozen-vs-trainable backbone decision changes with synthetic data quality, or whether freezing is universally beneficial for synthetic pretraining",
        "Whether task-specific filtering can be automated to match human expert filtering, or whether human judgment remains essential for identifying useful synthetic examples"
    ],
    "negative_experiments": [
        "Finding that synthetic pretraining consistently harms performance even with extremely limited real data (&lt;100 examples) would fundamentally challenge the theory",
        "Demonstrating that photorealistic synthesis and domain randomization are equally effective across all tasks and domains would challenge the complementary strategies claim and suggest a simpler unified mechanism",
        "Showing that task-agnostic quality metrics (e.g., FID, IS) predict transfer success as well as task-specific metrics would challenge the filtering prediction and suggest synthetic data quality is more general than theorized",
        "Finding that synthetic data provides equal benefit regardless of real data quantity (no diminishing returns) would contradict the non-linear value prediction",
        "Demonstrating that trainable backbones consistently outperform frozen backbones for synthetic pretraining would challenge the overfitting prevention mechanism",
        "Finding that single-strategy synthetic generation (e.g., only randomization or only photorealism) consistently matches or exceeds multi-strategy approaches would challenge the complementarity claim"
    ],
    "unaccounted_for": [
        {
            "text": "The specific mechanisms by which different blending modes contribute to ensemble performance - why does combining methods that individually underperform produce superior results?",
            "uuids": [
                "e570.0"
            ]
        },
        {
            "text": "Why FID fails as a predictor of detection performance - what aspects of image quality matter for detection vs. generation, and can we develop better metrics?",
            "uuids": [
                "e421.4"
            ]
        },
        {
            "text": "The interaction between synthetic data quantity and filtering strategies - at what point does filtering become unnecessary or counterproductive?",
            "uuids": [
                "e421.3"
            ]
        },
        {
            "text": "How to optimally balance different synthetic generation strategies - what is the optimal mix of randomization, photorealism, and compositing for different tasks?",
            "uuids": [
                "e570.0",
                "e561.1",
                "e561.2"
            ]
        },
        {
            "text": "Whether certain types of synthetic artifacts are more harmful than others - are some generation failures more detrimental to transfer than others?",
            "uuids": [
                "e421.3",
                "e421.6"
            ]
        },
        {
            "text": "How synthetic data quality requirements change with model architecture - do larger models require higher quality synthetic data, or are they more robust to synthetic artifacts?",
            "uuids": [
                "e421.5"
            ]
        },
        {
            "text": "The role of CLIP fine-tuning for niche domains - when is CLIP fine-tuning necessary vs. when is pretrained CLIP sufficient?",
            "uuids": [
                "e421.6"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show synthetic-only training approaching real-data performance (9,000 synthetic â‰ˆ 1,500 real), while others show larger gaps, suggesting context-dependent effectiveness",
            "uuids": [
                "e421.1"
            ]
        },
        {
            "text": "FID filtering shows inconsistent correlation with downstream task performance across different experimental settings, challenging its utility as a general quality metric",
            "uuids": [
                "e421.4"
            ]
        },
        {
            "text": "Poisson blending alone underperforms but contributes to ensemble success, creating apparent contradiction about its utility",
            "uuids": [
                "e570.0"
            ]
        }
    ],
    "special_cases": [
        "For safety-critical applications (e.g., autonomous vehicles, medical diagnosis), synthetic data may be insufficient regardless of quality due to regulatory requirements and liability concerns",
        "When real data contains systematic biases (e.g., demographic imbalances), synthetic data may actually improve fairness by enabling controlled generation of underrepresented cases",
        "Some domains (e.g., medical imaging, financial data) may have regulatory requirements for real data that cannot be satisfied by synthetic alternatives",
        "Synthetic data may be more valuable for data augmentation (supplementing real data) than for primary training (replacing real data)",
        "Generator-specific limitations apply: GLIGEN struggles with overlapped objects and small targets, requiring domain-specific solutions",
        "Highly niche domains (e.g., specialized medical imaging) may require CLIP fine-tuning or domain-specific generative models rather than general-purpose generators",
        "The optimal filtering strategy depends on data regime: Precision-Recall filtering is most valuable with &lt;1,000 generated images, while larger datasets may not benefit from filtering",
        "Photorealistic synthesis (e.g., VKITTI) may be more effective for structured environments (e.g., driving) while domain randomization may be more effective for unstructured environments (e.g., robotic manipulation)",
        "The frozen backbone strategy assumes availability of high-quality pretrained models (e.g., ImageNet); domains without such models may require different approaches"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Tobin et al. (2017) Domain randomization for transferring deep neural networks from simulation to the real world [Original domain randomization work, foundational to randomization strategy]",
            "Tremblay et al. (2018) Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization [Comprehensive study of synthetic data strategies in robotics]",
            "Kar et al. (2019) Meta-Sim: Learning to Generate Synthetic Datasets [Discusses synthetic data generation but focuses on learning generation parameters rather than comprehensive transfer theory]",
            "Dwibedi et al. (2017) Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection [Demonstrates compositing approach but doesn't provide unified theory]",
            "Heusel et al. (2017) GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium [Introduces FID metric but doesn't address its limitations for task-specific evaluation]",
            "Radford et al. (2021) Learning Transferable Visual Models From Natural Language Supervision [CLIP work, relevant to grounding but doesn't address synthetic-to-real transfer comprehensively]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 4,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
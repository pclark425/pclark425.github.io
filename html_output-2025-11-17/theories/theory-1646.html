<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domain-Alignment Theory of LLM Simulation Accuracy - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1646</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1646</p>
                <p><strong>Name:</strong> Domain-Alignment Theory of LLM Simulation Accuracy</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.</p>
                <p><strong>Description:</strong> This theory posits that the accuracy of large language models (LLMs) as text-based simulators for scientific subdomains is primarily determined by the degree of alignment between the LLM's internal representations and the formal, conceptual, and procedural structures of the target scientific subdomain. The more closely the LLM's learned representations and reasoning patterns match the epistemic and methodological norms of the subdomain, the higher the simulation accuracy.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Representation-Structure Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM internal representations &#8594; are_isomorphic_to &#8594; formal structures of scientific subdomain</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; achieves_high_simulation_accuracy_in &#8594; that scientific subdomain</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs fine-tuned on domain-specific corpora (e.g., biomedical, legal) show higher accuracy in those domains, suggesting alignment between model representations and domain structures improves performance. </li>
    <li>Studies show that LLMs trained on general data often fail to capture domain-specific reasoning patterns, leading to lower accuracy in specialized tasks. </li>
    <li>Transfer learning and domain adaptation literature consistently report that models adapted to the structure and vocabulary of a target domain outperform general models. </li>
    <li>Empirical results from SciBERT and BioBERT demonstrate that LLMs with representations tailored to scientific subdomains outperform general LLMs on domain-specific benchmarks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to transfer learning and domain adaptation, this law introduces a new, formalized alignment criterion between LLMs and scientific subdomains.</p>            <p><strong>What Already Exists:</strong> Prior work has shown that domain-specific fine-tuning improves LLM performance, and that representation learning is important for task accuracy.</p>            <p><strong>What is Novel:</strong> The explicit framing of simulation accuracy as a function of isomorphism between LLM representations and formal subdomain structures is novel, as is the focus on epistemic and procedural alignment.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [shows domain-adaptive pretraining improves performance, but does not formalize isomorphism]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [discusses domain adaptation, but not formal alignment with epistemic structures]</li>
    <li>Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [domain-specific LLMs]</li>
    <li>Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model for biomedical text mining [domain-specific LLMs]</li>
</ul>
            <h3>Statement 1: Epistemic Norm Sensitivity Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; encodes &#8594; epistemic norms and procedural rules of subdomain</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; produces &#8594; outputs consistent with subdomain standards</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs trained on scientific literature often mimic the citation, argumentation, and methodological conventions of the field, leading to more accurate and credible outputs. </li>
    <li>LLMs lacking exposure to subdomain-specific epistemic norms (e.g., statistical significance, reproducibility) often generate plausible but scientifically invalid outputs. </li>
    <li>Empirical studies show that LLMs trained on peer-reviewed literature are more likely to generate outputs that adhere to scientific standards than those trained on preprints or non-reviewed sources. </li>
    <li>LLMs exposed to subdomain-specific procedural rules (e.g., methods sections, statistical reporting) are more likely to generate outputs that pass expert review. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends existing work on LLM stylistic mimicry to the domain of epistemic and procedural norms, which is not widely formalized.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs can mimic stylistic and procedural conventions from their training data.</p>            <p><strong>What is Novel:</strong> The explicit connection between encoding epistemic norms and simulation accuracy in scientific subdomains is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [shows improved performance on scientific tasks, but does not formalize epistemic norm encoding]</li>
    <li>Koehler (2019) The Influence of Scientific Norms on the Reproducibility Crisis [discusses epistemic norms, but not in the context of LLMs]</li>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation, not epistemic norm focus]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LLM is fine-tuned on a corpus that explicitly encodes the formal rules and procedures of a scientific subdomain (e.g., mathematical proofs in topology), its simulation accuracy in that subdomain will increase.</li>
                <li>LLMs trained on data that lacks the epistemic norms of a subdomain (e.g., preprints without peer review) will produce less accurate simulations than those trained on peer-reviewed literature.</li>
                <li>Introducing explicit representations of subdomain-specific procedures (e.g., experimental protocols) into LLM training will improve the model's ability to simulate those procedures.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LLM is trained on a synthetic corpus that is structurally isomorphic to a scientific subdomain but uses entirely different surface forms (e.g., invented vocabulary), will it still achieve high simulation accuracy when mapped back to the original domain?</li>
                <li>If an LLM is exposed to conflicting epistemic norms from multiple subdomains, will it develop hybrid reasoning patterns, and how will this affect simulation accuracy in each subdomain?</li>
                <li>If an LLM is trained on a corpus with intentionally corrupted procedural rules, will it produce outputs that reflect those corruptions, and how will this impact expert evaluation?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an LLM with no exposure to a subdomain's formal structures or epistemic norms achieves high simulation accuracy in that subdomain, this would challenge the theory.</li>
                <li>If LLMs with highly aligned representations still systematically fail on core subdomain tasks, the theory would be called into question.</li>
                <li>If LLMs trained on non-peer-reviewed or procedurally inconsistent data outperform those trained on high-quality, norm-adherent data, the theory would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs achieve high simulation accuracy through memorization or surface-level pattern matching rather than true structural alignment. </li>
    <li>Instances where LLMs generalize to new subdomains with minimal fine-tuning, suggesting other factors (e.g., model size, generalization capacity) may also play a role. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes and extends existing ideas in domain adaptation and epistemic norm encoding, but introduces a new formal alignment criterion.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation, foundation models]</li>
    <li>Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [domain-specific LLMs]</li>
    <li>Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model for biomedical text mining [domain-specific LLMs]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Domain-Alignment Theory of LLM Simulation Accuracy",
    "theory_description": "This theory posits that the accuracy of large language models (LLMs) as text-based simulators for scientific subdomains is primarily determined by the degree of alignment between the LLM's internal representations and the formal, conceptual, and procedural structures of the target scientific subdomain. The more closely the LLM's learned representations and reasoning patterns match the epistemic and methodological norms of the subdomain, the higher the simulation accuracy.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Representation-Structure Alignment Law",
                "if": [
                    {
                        "subject": "LLM internal representations",
                        "relation": "are_isomorphic_to",
                        "object": "formal structures of scientific subdomain"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "achieves_high_simulation_accuracy_in",
                        "object": "that scientific subdomain"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs fine-tuned on domain-specific corpora (e.g., biomedical, legal) show higher accuracy in those domains, suggesting alignment between model representations and domain structures improves performance.",
                        "uuids": []
                    },
                    {
                        "text": "Studies show that LLMs trained on general data often fail to capture domain-specific reasoning patterns, leading to lower accuracy in specialized tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Transfer learning and domain adaptation literature consistently report that models adapted to the structure and vocabulary of a target domain outperform general models.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical results from SciBERT and BioBERT demonstrate that LLMs with representations tailored to scientific subdomains outperform general LLMs on domain-specific benchmarks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prior work has shown that domain-specific fine-tuning improves LLM performance, and that representation learning is important for task accuracy.",
                    "what_is_novel": "The explicit framing of simulation accuracy as a function of isomorphism between LLM representations and formal subdomain structures is novel, as is the focus on epistemic and procedural alignment.",
                    "classification_explanation": "While related to transfer learning and domain adaptation, this law introduces a new, formalized alignment criterion between LLMs and scientific subdomains.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [shows domain-adaptive pretraining improves performance, but does not formalize isomorphism]",
                        "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [discusses domain adaptation, but not formal alignment with epistemic structures]",
                        "Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [domain-specific LLMs]",
                        "Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model for biomedical text mining [domain-specific LLMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Epistemic Norm Sensitivity Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "encodes",
                        "object": "epistemic norms and procedural rules of subdomain"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "produces",
                        "object": "outputs consistent with subdomain standards"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs trained on scientific literature often mimic the citation, argumentation, and methodological conventions of the field, leading to more accurate and credible outputs.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs lacking exposure to subdomain-specific epistemic norms (e.g., statistical significance, reproducibility) often generate plausible but scientifically invalid outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Empirical studies show that LLMs trained on peer-reviewed literature are more likely to generate outputs that adhere to scientific standards than those trained on preprints or non-reviewed sources.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs exposed to subdomain-specific procedural rules (e.g., methods sections, statistical reporting) are more likely to generate outputs that pass expert review.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs can mimic stylistic and procedural conventions from their training data.",
                    "what_is_novel": "The explicit connection between encoding epistemic norms and simulation accuracy in scientific subdomains is novel.",
                    "classification_explanation": "This law extends existing work on LLM stylistic mimicry to the domain of epistemic and procedural norms, which is not widely formalized.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [shows improved performance on scientific tasks, but does not formalize epistemic norm encoding]",
                        "Koehler (2019) The Influence of Scientific Norms on the Reproducibility Crisis [discusses epistemic norms, but not in the context of LLMs]",
                        "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation, not epistemic norm focus]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LLM is fine-tuned on a corpus that explicitly encodes the formal rules and procedures of a scientific subdomain (e.g., mathematical proofs in topology), its simulation accuracy in that subdomain will increase.",
        "LLMs trained on data that lacks the epistemic norms of a subdomain (e.g., preprints without peer review) will produce less accurate simulations than those trained on peer-reviewed literature.",
        "Introducing explicit representations of subdomain-specific procedures (e.g., experimental protocols) into LLM training will improve the model's ability to simulate those procedures."
    ],
    "new_predictions_unknown": [
        "If an LLM is trained on a synthetic corpus that is structurally isomorphic to a scientific subdomain but uses entirely different surface forms (e.g., invented vocabulary), will it still achieve high simulation accuracy when mapped back to the original domain?",
        "If an LLM is exposed to conflicting epistemic norms from multiple subdomains, will it develop hybrid reasoning patterns, and how will this affect simulation accuracy in each subdomain?",
        "If an LLM is trained on a corpus with intentionally corrupted procedural rules, will it produce outputs that reflect those corruptions, and how will this impact expert evaluation?"
    ],
    "negative_experiments": [
        "If an LLM with no exposure to a subdomain's formal structures or epistemic norms achieves high simulation accuracy in that subdomain, this would challenge the theory.",
        "If LLMs with highly aligned representations still systematically fail on core subdomain tasks, the theory would be called into question.",
        "If LLMs trained on non-peer-reviewed or procedurally inconsistent data outperform those trained on high-quality, norm-adherent data, the theory would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs achieve high simulation accuracy through memorization or surface-level pattern matching rather than true structural alignment.",
            "uuids": []
        },
        {
            "text": "Instances where LLMs generalize to new subdomains with minimal fine-tuning, suggesting other factors (e.g., model size, generalization capacity) may also play a role.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show surprising generalization to new subdomains with minimal fine-tuning, suggesting other factors may also play a role.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Subdomains with poorly defined or rapidly evolving formal structures may not benefit from alignment in the same way.",
        "Highly interdisciplinary subdomains may require multi-domain alignment.",
        "Simulation accuracy may be limited by the inherent ambiguity or lack of consensus within a subdomain."
    ],
    "existing_theory": {
        "what_already_exists": "Domain adaptation and transfer learning are established, and the importance of training data alignment is known.",
        "what_is_novel": "The formalization of simulation accuracy as a function of isomorphism between LLM representations and subdomain structures, and the explicit role of epistemic norms, is novel.",
        "classification_explanation": "This theory synthesizes and extends existing ideas in domain adaptation and epistemic norm encoding, but introduces a new formal alignment criterion.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]",
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [domain adaptation, foundation models]",
            "Beltagy et al. (2019) SciBERT: A Pretrained Language Model for Scientific Text [domain-specific LLMs]",
            "Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model for biomedical text mining [domain-specific LLMs]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of factors affecting the accuracy of using large language models (LLMs) as text-based simulators for specific scientific subdomains.",
    "original_theory_id": "theory-637",
    "original_theory_name": "Theory of Tool-Augmented Simulation: Externalization of Domain-Specific Reasoning and Computation",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
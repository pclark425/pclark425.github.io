<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Latent Algorithm Activation and Superficial Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-578</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-578</p>
                <p><strong>Name:</strong> Latent Algorithm Activation and Superficial Alignment Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> Language models acquire latent algorithmic and representational capacities for arithmetic during pretraining, but these capacities are not fully expressed until activated by downstream data, such as fine-tuning, chain-of-thought (CoT) prompting, or in-context learning. Downstream data acts primarily as an activator or aligner, not as a creator of new capabilities. The effectiveness of activation depends on the diversity, simplicity, and perplexity of the downstream data, as well as the presence of explicit intermediate steps (e.g., chain-of-thought or scratchpad). Arithmetic performance is thus a function of both the latent capacity induced by pretraining and the alignment provided by downstream supervision. This theory also posits that the activation process is sensitive to the structure and quality of the downstream data, and that explicit intermediate reasoning steps (CoT, scratchpad) serve as alignment mechanisms that elicit and scaffold the use of these latent circuits.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Downstream Data as Activator of Latent Arithmetic Circuits (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is pretrained &#8594; on large-scale text with numeric content<span style="color: #888888;">, and</span></div>
        <div>&#8226; downstream data &#8594; contains &#8594; diverse, low-perplexity arithmetic problems</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; arithmetic performance &#8594; is improved &#8594; by activation of latent algorithmic circuits</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Finetuning LLaMA-2 on MetaMathQA (diverse, low-perplexity) yields large accuracy gains, consistent with activation of latent capacities. <a href="../results/extraction-result-4733.html#e4733.0" class="evidence-link">[e4733.0]</a> <a href="../results/extraction-result-4733.html#e4733.5" class="evidence-link">[e4733.5]</a> </li>
    <li>Arithmetic fine-tuning augments existing entity-tracking circuits rather than creating new ones, primarily by strengthening positional and value representations. <a href="../results/extraction-result-4625.html#e4625.3" class="evidence-link">[e4625.3]</a> <a href="../results/extraction-result-4625.html#e4625.1" class="evidence-link">[e4625.1]</a> <a href="../results/extraction-result-4625.html#e4625.2" class="evidence-link">[e4625.2]</a> </li>
    <li>Superficial Alignment Hypothesis: downstream data acts as an activator of pretraining-learned latent algorithmic or reasoning capabilities; dataset characteristics (simplicity, diversity, perplexity) determine how effectively this activation happens. <a href="../results/extraction-result-4733.html#e4733.5" class="evidence-link">[e4733.5]</a> </li>
    <li>MetaMathQA's diversity gain correlates strongly with accuracy (Pearson 0.972), and lower perplexity yields larger gains. <a href="../results/extraction-result-4733.html#e4733.0" class="evidence-link">[e4733.0]</a> <a href="../results/extraction-result-4733.html#e4733.5" class="evidence-link">[e4733.5]</a> </li>
    <li>Dataset-composition hypothesis: exposure to high-quality mathematical tokens (preserving LaTeX/math notation) during pretraining/finetuning substantially improves a language model's quantitative reasoning performance. <a href="../results/extraction-result-4716.html#e4716.3" class="evidence-link">[e4716.3]</a> </li>
    <li>Fine-tuning on arithmetic data enhances existing mechanisms (entity-tracking, positional-indexing) rather than creating new ones. <a href="../results/extraction-result-4625.html#e4625.3" class="evidence-link">[e4625.3]</a> <a href="../results/extraction-result-4625.html#e4625.1" class="evidence-link">[e4625.1]</a> <a href="../results/extraction-result-4625.html#e4625.2" class="evidence-link">[e4625.2]</a> </li>
    <li>LLaMA-2 (baseline) models perform poorly on arithmetic until fine-tuned, supporting the need for activation. <a href="../results/extraction-result-4733.html#e4733.1" class="evidence-link">[e4733.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While transfer learning is well-known, the specific mechanism of activation and the role of data properties in arithmetic are new.</p>            <p><strong>What Already Exists:</strong> The concept of transfer learning and the importance of pretraining for downstream performance.</p>            <p><strong>What is Novel:</strong> The explicit framing of downstream data as an activator of latent algorithmic circuits, and the identification of data diversity and perplexity as key factors for activation effectiveness.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2023) LIMA: Less Is More for Alignment [superficial alignment hypothesis]</li>
    <li>Liu et al. (2023) MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models [empirical evidence for activation]</li>
    <li>Touvron et al. (2023) LLaMA: Open and Efficient Foundation Language Models [pretraining and fine-tuning effects]</li>
</ul>
            <h3>Statement 1: Chain-of-Thought and Scratchpad as Alignment Mechanisms (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; downstream data or prompt &#8594; includes &#8594; explicit intermediate reasoning steps (CoT or scratchpad)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; model &#8594; is more likely to &#8594; express latent multi-step arithmetic capabilities</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Chain-of-thought prompting elicits multi-step reasoning in large models, with large accuracy gains on arithmetic benchmarks. <a href="../results/extraction-result-4741.html#e4741.0" class="evidence-link">[e4741.0]</a> <a href="../results/extraction-result-4730.html#e4730.0" class="evidence-link">[e4730.0]</a> <a href="../results/extraction-result-4721.html#e4721.0" class="evidence-link">[e4721.0]</a> <a href="../results/extraction-result-4724.html#e4724.2" class="evidence-link">[e4724.2]</a> <a href="../results/extraction-result-4741.html#e4741.1" class="evidence-link">[e4741.1]</a> <a href="../results/extraction-result-4741.html#e4741.2" class="evidence-link">[e4741.2]</a> <a href="../results/extraction-result-4731.html#e4731.1" class="evidence-link">[e4731.1]</a> <a href="../results/extraction-result-4731.html#e4731.0" class="evidence-link">[e4731.0]</a> <a href="../results/extraction-result-4731.html#e4731.4" class="evidence-link">[e4731.4]</a> <a href="../results/extraction-result-4741.html#e4741.4" class="evidence-link">[e4741.4]</a> <a href="../results/extraction-result-4741.html#e4741.5" class="evidence-link">[e4741.5]</a> <a href="../results/extraction-result-4721.html#e4721.1" class="evidence-link">[e4721.1]</a> <a href="../results/extraction-result-4709.html#e4709.1" class="evidence-link">[e4709.1]</a> <a href="../results/extraction-result-4627.html#e4627.5" class="evidence-link">[e4627.5]</a> <a href="../results/extraction-result-4733.html#e4733.0" class="evidence-link">[e4733.0]</a> </li>
    <li>Scratchpad formats improve sample efficiency and enable composition of learned primitives for arithmetic. <a href="../results/extraction-result-4724.html#e4724.2" class="evidence-link">[e4724.2]</a> <a href="../results/extraction-result-4719.html#e4719.1" class="evidence-link">[e4719.1]</a> <a href="../results/extraction-result-4705.html#e4705.2" class="evidence-link">[e4705.2]</a> </li>
    <li>Detailed scratchpad and chain-of-thought formats allow models to learn component functions and compose them for final outputs more efficiently. <a href="../results/extraction-result-4724.html#e4724.2" class="evidence-link">[e4724.2]</a> <a href="../results/extraction-result-4719.html#e4719.1" class="evidence-link">[e4719.1]</a> <a href="../results/extraction-result-4705.html#e4705.2" class="evidence-link">[e4705.2]</a> </li>
    <li>CoT and scratchpad prompting are especially effective in large models, with emergent reasoning abilities at scale. <a href="../results/extraction-result-4741.html#e4741.0" class="evidence-link">[e4741.0]</a> <a href="../results/extraction-result-4741.html#e4741.1" class="evidence-link">[e4741.1]</a> <a href="../results/extraction-result-4741.html#e4741.2" class="evidence-link">[e4741.2]</a> <a href="../results/extraction-result-4731.html#e4731.1" class="evidence-link">[e4731.1]</a> <a href="../results/extraction-result-4731.html#e4731.0" class="evidence-link">[e4731.0]</a> <a href="../results/extraction-result-4731.html#e4731.4" class="evidence-link">[e4731.4]</a> <a href="../results/extraction-result-4741.html#e4741.4" class="evidence-link">[e4741.4]</a> <a href="../results/extraction-result-4741.html#e4741.5" class="evidence-link">[e4741.5]</a> <a href="../results/extraction-result-4721.html#e4721.0" class="evidence-link">[e4721.0]</a> <a href="../results/extraction-result-4721.html#e4721.1" class="evidence-link">[e4721.1]</a> </li>
    <li>Ablation studies show that CoT and scratchpad formats, rather than just extra token budget or equation-only prompting, are necessary for improved multi-step arithmetic. <a href="../results/extraction-result-4741.html#e4741.0" class="evidence-link">[e4741.0]</a> <a href="../results/extraction-result-4724.html#e4724.2" class="evidence-link">[e4724.2]</a> <a href="../results/extraction-result-4731.html#e4731.0" class="evidence-link">[e4731.0]</a> <a href="../results/extraction-result-4721.html#e4721.0" class="evidence-link">[e4721.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The mechanism of activation via CoT/scratchpad is a new synthesis, though the techniques themselves are established.</p>            <p><strong>What Already Exists:</strong> Chain-of-thought and scratchpad prompting are known to improve reasoning in LLMs.</p>            <p><strong>What is Novel:</strong> The explicit role of these formats as alignment mechanisms that activate latent algorithmic circuits, rather than teaching new algorithms.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [scratchpad]</li>
    <li>Lampinen et al. (2022) Can Language Models Learn from Explanations? [explanation as alignment]</li>
</ul>
            <h3>Statement 2: Data Diversity and Perplexity Determine Activation Effectiveness (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; downstream data &#8594; has &#8594; high diversity and low perplexity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; activation of latent arithmetic circuits &#8594; is more effective &#8594; leading to higher accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MetaMathQA's diversity gain correlates strongly with accuracy (Pearson 0.972), and lower perplexity yields larger gains. <a href="../results/extraction-result-4733.html#e4733.0" class="evidence-link">[e4733.0]</a> <a href="../results/extraction-result-4733.html#e4733.5" class="evidence-link">[e4733.5]</a> </li>
    <li>Mixing an external RFT dataset with MetaMathQA caused performance drop, indicating that dataset diversity and compatibility are critical for effective activation. <a href="../results/extraction-result-4733.html#e4733.0" class="evidence-link">[e4733.0]</a> </li>
    <li>Finetuning on datasets with repetitive templates or low diversity leads to overfitting and poor generalization. <a href="../results/extraction-result-4713.html#e4713.3" class="evidence-link">[e4713.3]</a> <a href="../results/extraction-result-4733.html#e4733.1" class="evidence-link">[e4733.1]</a> </li>
    <li>Program context design: including irrelevant variables in pretraining improves transfer to natural contexts, supporting the importance of diversity and noise in data. <a href="../results/extraction-result-4729.html#e4729.2" class="evidence-link">[e4729.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends general ML principles to the specific context of arithmetic in LLMs.</p>            <p><strong>What Already Exists:</strong> The importance of data diversity and perplexity for generalization is known in machine learning.</p>            <p><strong>What is Novel:</strong> The direct link between these properties and the activation of latent arithmetic circuits in LLMs is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Liu et al. (2023) MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models [empirical evidence for this law]</li>
    <li>Zhou et al. (2023) LIMA: Less Is More for Alignment [superficial alignment hypothesis]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a pretrained LLM is fine-tuned on a highly diverse, low-perplexity arithmetic dataset, its arithmetic accuracy will increase more than if fine-tuned on a less diverse or higher-perplexity dataset.</li>
                <li>If chain-of-thought exemplars are provided in prompts, even without explicit arithmetic supervision, large models will show improved multi-step arithmetic reasoning.</li>
                <li>If a model is fine-tuned on arithmetic data lacking diversity (e.g., repetitive templates), it will overfit and fail to generalize to novel arithmetic problems.</li>
                <li>If scratchpad or CoT formats are used in fine-tuning, models will learn to decompose arithmetic problems into intermediate steps and show improved sample efficiency.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a model is exposed to adversarially constructed downstream data with high diversity but high perplexity (e.g., semantically diverse but syntactically confusing), activation of latent circuits may fail or produce unpredictable results.</li>
                <li>If a model is fine-tuned on arithmetic data in a language with fundamentally different numeric representations (e.g., logograms or abacuses), the activation mechanism may require new forms of alignment.</li>
                <li>If chain-of-thought prompting is applied to models with minimal pretraining, it may fail to elicit arithmetic reasoning, suggesting a lower bound on necessary pretraining.</li>
                <li>If downstream data is highly diverse but contains misleading or incorrect intermediate steps, activation may result in spurious or brittle arithmetic reasoning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If fine-tuning on highly diverse, low-perplexity arithmetic data does not improve arithmetic performance, the activation hypothesis would be challenged.</li>
                <li>If chain-of-thought prompting fails to improve multi-step arithmetic in large pretrained models, the alignment mechanism would be called into question.</li>
                <li>If models with no pretraining can achieve high arithmetic accuracy with only downstream data, the necessity of latent capacity would be undermined.</li>
                <li>If models fine-tuned on repetitive, low-diversity data generalize as well as those fine-tuned on diverse data, the role of diversity in activation would be questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The specific neural mechanisms (e.g., Fourier features, circuits) by which latent arithmetic capacity is encoded and activated are not detailed in this theory. <a href="../results/extraction-result-4629.html#e4629.5" class="evidence-link">[e4629.5]</a> <a href="../results/extraction-result-4629.html#e4629.0" class="evidence-link">[e4629.0]</a> <a href="../results/extraction-result-4625.html#e4625.3" class="evidence-link">[e4625.3]</a> <a href="../results/extraction-result-4626.html#e4626.0" class="evidence-link">[e4626.0]</a> <a href="../results/extraction-result-4626.html#e4626.3" class="evidence-link">[e4626.3]</a> <a href="../results/extraction-result-4626.html#e4626.2" class="evidence-link">[e4626.2]</a> </li>
    <li>The role of external program execution (e.g., program synthesis, PoT) in arithmetic is not directly addressed. <a href="../results/extraction-result-4707.html#e4707.2" class="evidence-link">[e4707.2]</a> <a href="../results/extraction-result-4730.html#e4730.1" class="evidence-link">[e4730.1]</a> <a href="../results/extraction-result-4707.html#e4707.1" class="evidence-link">[e4707.1]</a> <a href="../results/extraction-result-4730.html#e4730.3" class="evidence-link">[e4730.3]</a> <a href="../results/extraction-result-4729.html#e4729.3" class="evidence-link">[e4729.3]</a> </li>
    <li>The impact of tokenization and pretraining on specific numeric representations (e.g., digit-level vs subword) is not fully explained. <a href="../results/extraction-result-4734.html#e4734.5" class="evidence-link">[e4734.5]</a> <a href="../results/extraction-result-4734.html#e4734.6" class="evidence-link">[e4734.6]</a> <a href="../results/extraction-result-4712.html#e4712.0" class="evidence-link">[e4712.0]</a> <a href="../results/extraction-result-4742.html#e4742.4" class="evidence-link">[e4742.4]</a> </li>
    <li>The effect of model scale and architecture (e.g., mixture-of-experts, hydra effect) on the robustness and redundancy of activated circuits is not fully specified. <a href="../results/extraction-result-4710.html#e4710.0" class="evidence-link">[e4710.0]</a> <a href="../results/extraction-result-4719.html#e4719.5" class="evidence-link">[e4719.5]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> This theory synthesizes empirical findings on fine-tuning, prompting, and data properties into a new account of how arithmetic ability is elicited in LLMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2023) LIMA: Less Is More for Alignment [superficial alignment hypothesis]</li>
    <li>Liu et al. (2023) MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models [empirical evidence for activation]</li>
    <li>Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT as alignment]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [scratchpad as alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Latent Algorithm Activation and Superficial Alignment Theory",
    "theory_description": "Language models acquire latent algorithmic and representational capacities for arithmetic during pretraining, but these capacities are not fully expressed until activated by downstream data, such as fine-tuning, chain-of-thought (CoT) prompting, or in-context learning. Downstream data acts primarily as an activator or aligner, not as a creator of new capabilities. The effectiveness of activation depends on the diversity, simplicity, and perplexity of the downstream data, as well as the presence of explicit intermediate steps (e.g., chain-of-thought or scratchpad). Arithmetic performance is thus a function of both the latent capacity induced by pretraining and the alignment provided by downstream supervision. This theory also posits that the activation process is sensitive to the structure and quality of the downstream data, and that explicit intermediate reasoning steps (CoT, scratchpad) serve as alignment mechanisms that elicit and scaffold the use of these latent circuits.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Downstream Data as Activator of Latent Arithmetic Circuits",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is pretrained",
                        "object": "on large-scale text with numeric content"
                    },
                    {
                        "subject": "downstream data",
                        "relation": "contains",
                        "object": "diverse, low-perplexity arithmetic problems"
                    }
                ],
                "then": [
                    {
                        "subject": "arithmetic performance",
                        "relation": "is improved",
                        "object": "by activation of latent algorithmic circuits"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Finetuning LLaMA-2 on MetaMathQA (diverse, low-perplexity) yields large accuracy gains, consistent with activation of latent capacities.",
                        "uuids": [
                            "e4733.0",
                            "e4733.5"
                        ]
                    },
                    {
                        "text": "Arithmetic fine-tuning augments existing entity-tracking circuits rather than creating new ones, primarily by strengthening positional and value representations.",
                        "uuids": [
                            "e4625.3",
                            "e4625.1",
                            "e4625.2"
                        ]
                    },
                    {
                        "text": "Superficial Alignment Hypothesis: downstream data acts as an activator of pretraining-learned latent algorithmic or reasoning capabilities; dataset characteristics (simplicity, diversity, perplexity) determine how effectively this activation happens.",
                        "uuids": [
                            "e4733.5"
                        ]
                    },
                    {
                        "text": "MetaMathQA's diversity gain correlates strongly with accuracy (Pearson 0.972), and lower perplexity yields larger gains.",
                        "uuids": [
                            "e4733.0",
                            "e4733.5"
                        ]
                    },
                    {
                        "text": "Dataset-composition hypothesis: exposure to high-quality mathematical tokens (preserving LaTeX/math notation) during pretraining/finetuning substantially improves a language model's quantitative reasoning performance.",
                        "uuids": [
                            "e4716.3"
                        ]
                    },
                    {
                        "text": "Fine-tuning on arithmetic data enhances existing mechanisms (entity-tracking, positional-indexing) rather than creating new ones.",
                        "uuids": [
                            "e4625.3",
                            "e4625.1",
                            "e4625.2"
                        ]
                    },
                    {
                        "text": "LLaMA-2 (baseline) models perform poorly on arithmetic until fine-tuned, supporting the need for activation.",
                        "uuids": [
                            "e4733.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The concept of transfer learning and the importance of pretraining for downstream performance.",
                    "what_is_novel": "The explicit framing of downstream data as an activator of latent algorithmic circuits, and the identification of data diversity and perplexity as key factors for activation effectiveness.",
                    "classification_explanation": "While transfer learning is well-known, the specific mechanism of activation and the role of data properties in arithmetic are new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2023) LIMA: Less Is More for Alignment [superficial alignment hypothesis]",
                        "Liu et al. (2023) MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models [empirical evidence for activation]",
                        "Touvron et al. (2023) LLaMA: Open and Efficient Foundation Language Models [pretraining and fine-tuning effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Chain-of-Thought and Scratchpad as Alignment Mechanisms",
                "if": [
                    {
                        "subject": "downstream data or prompt",
                        "relation": "includes",
                        "object": "explicit intermediate reasoning steps (CoT or scratchpad)"
                    }
                ],
                "then": [
                    {
                        "subject": "model",
                        "relation": "is more likely to",
                        "object": "express latent multi-step arithmetic capabilities"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Chain-of-thought prompting elicits multi-step reasoning in large models, with large accuracy gains on arithmetic benchmarks.",
                        "uuids": [
                            "e4741.0",
                            "e4730.0",
                            "e4721.0",
                            "e4724.2",
                            "e4741.1",
                            "e4741.2",
                            "e4731.1",
                            "e4731.0",
                            "e4731.4",
                            "e4741.4",
                            "e4741.5",
                            "e4721.1",
                            "e4709.1",
                            "e4627.5",
                            "e4733.0"
                        ]
                    },
                    {
                        "text": "Scratchpad formats improve sample efficiency and enable composition of learned primitives for arithmetic.",
                        "uuids": [
                            "e4724.2",
                            "e4719.1",
                            "e4705.2"
                        ]
                    },
                    {
                        "text": "Detailed scratchpad and chain-of-thought formats allow models to learn component functions and compose them for final outputs more efficiently.",
                        "uuids": [
                            "e4724.2",
                            "e4719.1",
                            "e4705.2"
                        ]
                    },
                    {
                        "text": "CoT and scratchpad prompting are especially effective in large models, with emergent reasoning abilities at scale.",
                        "uuids": [
                            "e4741.0",
                            "e4741.1",
                            "e4741.2",
                            "e4731.1",
                            "e4731.0",
                            "e4731.4",
                            "e4741.4",
                            "e4741.5",
                            "e4721.0",
                            "e4721.1"
                        ]
                    },
                    {
                        "text": "Ablation studies show that CoT and scratchpad formats, rather than just extra token budget or equation-only prompting, are necessary for improved multi-step arithmetic.",
                        "uuids": [
                            "e4741.0",
                            "e4724.2",
                            "e4731.0",
                            "e4721.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Chain-of-thought and scratchpad prompting are known to improve reasoning in LLMs.",
                    "what_is_novel": "The explicit role of these formats as alignment mechanisms that activate latent algorithmic circuits, rather than teaching new algorithms.",
                    "classification_explanation": "The mechanism of activation via CoT/scratchpad is a new synthesis, though the techniques themselves are established.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT]",
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [scratchpad]",
                        "Lampinen et al. (2022) Can Language Models Learn from Explanations? [explanation as alignment]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Data Diversity and Perplexity Determine Activation Effectiveness",
                "if": [
                    {
                        "subject": "downstream data",
                        "relation": "has",
                        "object": "high diversity and low perplexity"
                    }
                ],
                "then": [
                    {
                        "subject": "activation of latent arithmetic circuits",
                        "relation": "is more effective",
                        "object": "leading to higher accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MetaMathQA's diversity gain correlates strongly with accuracy (Pearson 0.972), and lower perplexity yields larger gains.",
                        "uuids": [
                            "e4733.0",
                            "e4733.5"
                        ]
                    },
                    {
                        "text": "Mixing an external RFT dataset with MetaMathQA caused performance drop, indicating that dataset diversity and compatibility are critical for effective activation.",
                        "uuids": [
                            "e4733.0"
                        ]
                    },
                    {
                        "text": "Finetuning on datasets with repetitive templates or low diversity leads to overfitting and poor generalization.",
                        "uuids": [
                            "e4713.3",
                            "e4733.1"
                        ]
                    },
                    {
                        "text": "Program context design: including irrelevant variables in pretraining improves transfer to natural contexts, supporting the importance of diversity and noise in data.",
                        "uuids": [
                            "e4729.2"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "The importance of data diversity and perplexity for generalization is known in machine learning.",
                    "what_is_novel": "The direct link between these properties and the activation of latent arithmetic circuits in LLMs is new.",
                    "classification_explanation": "This law extends general ML principles to the specific context of arithmetic in LLMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Liu et al. (2023) MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models [empirical evidence for this law]",
                        "Zhou et al. (2023) LIMA: Less Is More for Alignment [superficial alignment hypothesis]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a pretrained LLM is fine-tuned on a highly diverse, low-perplexity arithmetic dataset, its arithmetic accuracy will increase more than if fine-tuned on a less diverse or higher-perplexity dataset.",
        "If chain-of-thought exemplars are provided in prompts, even without explicit arithmetic supervision, large models will show improved multi-step arithmetic reasoning.",
        "If a model is fine-tuned on arithmetic data lacking diversity (e.g., repetitive templates), it will overfit and fail to generalize to novel arithmetic problems.",
        "If scratchpad or CoT formats are used in fine-tuning, models will learn to decompose arithmetic problems into intermediate steps and show improved sample efficiency."
    ],
    "new_predictions_unknown": [
        "If a model is exposed to adversarially constructed downstream data with high diversity but high perplexity (e.g., semantically diverse but syntactically confusing), activation of latent circuits may fail or produce unpredictable results.",
        "If a model is fine-tuned on arithmetic data in a language with fundamentally different numeric representations (e.g., logograms or abacuses), the activation mechanism may require new forms of alignment.",
        "If chain-of-thought prompting is applied to models with minimal pretraining, it may fail to elicit arithmetic reasoning, suggesting a lower bound on necessary pretraining.",
        "If downstream data is highly diverse but contains misleading or incorrect intermediate steps, activation may result in spurious or brittle arithmetic reasoning."
    ],
    "negative_experiments": [
        "If fine-tuning on highly diverse, low-perplexity arithmetic data does not improve arithmetic performance, the activation hypothesis would be challenged.",
        "If chain-of-thought prompting fails to improve multi-step arithmetic in large pretrained models, the alignment mechanism would be called into question.",
        "If models with no pretraining can achieve high arithmetic accuracy with only downstream data, the necessity of latent capacity would be undermined.",
        "If models fine-tuned on repetitive, low-diversity data generalize as well as those fine-tuned on diverse data, the role of diversity in activation would be questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The specific neural mechanisms (e.g., Fourier features, circuits) by which latent arithmetic capacity is encoded and activated are not detailed in this theory.",
            "uuids": [
                "e4629.5",
                "e4629.0",
                "e4625.3",
                "e4626.0",
                "e4626.3",
                "e4626.2"
            ]
        },
        {
            "text": "The role of external program execution (e.g., program synthesis, PoT) in arithmetic is not directly addressed.",
            "uuids": [
                "e4707.2",
                "e4730.1",
                "e4707.1",
                "e4730.3",
                "e4729.3"
            ]
        },
        {
            "text": "The impact of tokenization and pretraining on specific numeric representations (e.g., digit-level vs subword) is not fully explained.",
            "uuids": [
                "e4734.5",
                "e4734.6",
                "e4712.0",
                "e4742.4"
            ]
        },
        {
            "text": "The effect of model scale and architecture (e.g., mixture-of-experts, hydra effect) on the robustness and redundancy of activated circuits is not fully specified.",
            "uuids": [
                "e4710.0",
                "e4719.5"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some models (e.g., Llama3-8B) show extreme fragility to neuron ablation, suggesting that activation may not always be robust or that latent circuits can be brittle.",
            "uuids": [
                "e4626.2"
            ]
        },
        {
            "text": "Certain models (e.g., code-davinci-002) perform poorly on arithmetic despite code pretraining, indicating that not all pretraining induces latent arithmetic capacity.",
            "uuids": [
                "e4734.3"
            ]
        },
        {
            "text": "Block-holdout generalization failures (memorization of mapping to human coordinates) suggest that some representations are not fully generalizable even after activation.",
            "uuids": [
                "e4711.4"
            ]
        }
    ],
    "special_cases": [
        "For models with insufficient pretraining or with pretraining on data lacking numeric content, latent arithmetic capacity may not exist and cannot be activated.",
        "For arithmetic tasks requiring symbolic manipulation or external tool use, activation of latent circuits may be insufficient without additional mechanisms.",
        "For models with highly brittle or tightly coupled internal representations, activation may result in fragile or non-robust arithmetic performance.",
        "For tasks with non-injective or many-to-many relations, activation of latent circuits may not yield reliable arithmetic reasoning."
    ],
    "existing_theory": {
        "what_already_exists": "Transfer learning and the importance of pretraining are established; chain-of-thought and scratchpad prompting are known techniques.",
        "what_is_novel": "The explicit framing of downstream data as an activator of latent algorithmic circuits, and the identification of data properties (diversity, perplexity) as key determinants of activation effectiveness.",
        "classification_explanation": "This theory synthesizes empirical findings on fine-tuning, prompting, and data properties into a new account of how arithmetic ability is elicited in LLMs.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Zhou et al. (2023) LIMA: Less Is More for Alignment [superficial alignment hypothesis]",
            "Liu et al. (2023) MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models [empirical evidence for activation]",
            "Wei et al. (2022) Chain of Thought Prompting Elicits Reasoning in Large Language Models [CoT as alignment]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [scratchpad as alignment]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
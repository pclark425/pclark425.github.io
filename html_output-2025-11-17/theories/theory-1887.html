<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Load Mediation Theory of LLM Problem Format Sensitivity - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1887</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1887</p>
                <p><strong>Name:</strong> Cognitive Load Mediation Theory of LLM Problem Format Sensitivity</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the format in which a problem is presented modulates the cognitive load imposed on a large language model (LLM), thereby affecting its performance. Formats that align with the LLM's pretraining distribution or reduce ambiguity lower cognitive load, leading to higher accuracy and reliability, while unfamiliar or complex formats increase cognitive load, resulting in more errors and less consistent outputs.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Distribution Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_similar_to &#8594; LLM_pretraining_distribution</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_performance &#8594; is_maximized &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_cognitive_load &#8594; is_minimized &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on tasks that resemble their pretraining data, such as question-answering in natural language or code completion in familiar code blocks. </li>
    <li>Performance drops when LLMs are given tasks in unfamiliar formats, such as tables or highly structured forms not seen during pretraining. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect is recognized, but the explicit law-like structure and the mediation by cognitive load are new.</p>            <p><strong>What Already Exists:</strong> It is known that LLMs perform better on tasks similar to their pretraining data.</p>            <p><strong>What is Novel:</strong> This law formalizes the relationship as a function of cognitive load and explicitly links format similarity to both performance and cognitive load.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning performance]</li>
</ul>
            <h3>Statement 1: Ambiguity-Induced Load Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; contains &#8594; high_ambiguity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_cognitive_load &#8594; is_increased &#8594; True<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_performance &#8594; is_decreased &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Ambiguous or underspecified prompts lead to more variable and less accurate LLM outputs. </li>
    <li>Clear, explicit formats (e.g., step-by-step instructions) improve LLM performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The effect is recognized, but the explicit mediation by cognitive load is novel.</p>            <p><strong>What Already Exists:</strong> Prompt clarity is known to affect LLM performance.</p>            <p><strong>What is Novel:</strong> This law frames ambiguity as a source of cognitive load and links it directly to performance decrements.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt ambiguity and reasoning]</li>
    <li>Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt clarity and performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a problem is reformatted to closely match the LLM's pretraining data (e.g., converting a table to a narrative), performance will improve.</li>
                <li>If ambiguity in the problem format is reduced (e.g., by adding clarifying instructions), LLM accuracy will increase.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If a novel format is introduced but is cognitively simple (e.g., a new but unambiguous structure), will LLMs perform well or poorly?</li>
                <li>If LLMs are fine-tuned on highly ambiguous formats, can they learn to handle ambiguity without performance loss?</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on formats that are highly dissimilar to their pretraining data, the theory would be challenged.</li>
                <li>If ambiguity in format does not affect LLM performance, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs perform well on unfamiliar formats due to emergent generalization abilities. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is somewhat related to existing work but formalizes the mediation by cognitive load in a new way.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning performance]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Load Mediation Theory of LLM Problem Format Sensitivity",
    "theory_description": "This theory posits that the format in which a problem is presented modulates the cognitive load imposed on a large language model (LLM), thereby affecting its performance. Formats that align with the LLM's pretraining distribution or reduce ambiguity lower cognitive load, leading to higher accuracy and reliability, while unfamiliar or complex formats increase cognitive load, resulting in more errors and less consistent outputs.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Distribution Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_similar_to",
                        "object": "LLM_pretraining_distribution"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_performance",
                        "relation": "is_maximized",
                        "object": "True"
                    },
                    {
                        "subject": "LLM_cognitive_load",
                        "relation": "is_minimized",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on tasks that resemble their pretraining data, such as question-answering in natural language or code completion in familiar code blocks.",
                        "uuids": []
                    },
                    {
                        "text": "Performance drops when LLMs are given tasks in unfamiliar formats, such as tables or highly structured forms not seen during pretraining.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that LLMs perform better on tasks similar to their pretraining data.",
                    "what_is_novel": "This law formalizes the relationship as a function of cognitive load and explicitly links format similarity to both performance and cognitive load.",
                    "classification_explanation": "The effect is recognized, but the explicit law-like structure and the mediation by cognitive load are new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Ambiguity-Induced Load Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "contains",
                        "object": "high_ambiguity"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_cognitive_load",
                        "relation": "is_increased",
                        "object": "True"
                    },
                    {
                        "subject": "LLM_performance",
                        "relation": "is_decreased",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Ambiguous or underspecified prompts lead to more variable and less accurate LLM outputs.",
                        "uuids": []
                    },
                    {
                        "text": "Clear, explicit formats (e.g., step-by-step instructions) improve LLM performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt clarity is known to affect LLM performance.",
                    "what_is_novel": "This law frames ambiguity as a source of cognitive load and links it directly to performance decrements.",
                    "classification_explanation": "The effect is recognized, but the explicit mediation by cognitive load is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Zhou et al. (2022) Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [Prompt ambiguity and reasoning]",
                        "Perez et al. (2021) True Few-Shot Learning with Language Models [Prompt clarity and performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a problem is reformatted to closely match the LLM's pretraining data (e.g., converting a table to a narrative), performance will improve.",
        "If ambiguity in the problem format is reduced (e.g., by adding clarifying instructions), LLM accuracy will increase."
    ],
    "new_predictions_unknown": [
        "If a novel format is introduced but is cognitively simple (e.g., a new but unambiguous structure), will LLMs perform well or poorly?",
        "If LLMs are fine-tuned on highly ambiguous formats, can they learn to handle ambiguity without performance loss?"
    ],
    "negative_experiments": [
        "If LLMs perform equally well on formats that are highly dissimilar to their pretraining data, the theory would be challenged.",
        "If ambiguity in format does not affect LLM performance, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs perform well on unfamiliar formats due to emergent generalization abilities.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robust performance on certain unfamiliar formats after instruction tuning.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with extensive instruction tuning may be less sensitive to format-pretraining alignment.",
        "Highly structured formats that are simple may not increase cognitive load despite being unfamiliar."
    ],
    "existing_theory": {
        "what_already_exists": "It is known that prompt format and clarity affect LLM performance.",
        "what_is_novel": "The explicit mediation by cognitive load and the law-like structure relating format, cognitive load, and performance are new.",
        "classification_explanation": "The theory is somewhat related to existing work but formalizes the mediation by cognitive load in a new way.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format and pretraining alignment]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt format and reasoning performance]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-652",
    "original_theory_name": "Prompt Format as a High-Dimensional Control Signal for LLM Computation",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
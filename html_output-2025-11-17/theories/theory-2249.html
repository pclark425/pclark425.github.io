<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Dimensional Evaluation Framework for LLM-Generated Scientific Theories - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-2249</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-2249</p>
                <p><strong>Name:</strong> Multi-Dimensional Evaluation Framework for LLM-Generated Scientific Theories</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how to evaluate LLM-generated scientific theories.</p>
                <p><strong>Description:</strong> This theory posits that the evaluation of LLM-generated scientific theories should be conducted across multiple, orthogonal dimensions: empirical testability, explanatory coherence, novelty, and epistemic utility. Each dimension is assessed independently, and the overall evaluation is a composite function of these scores, allowing for nuanced judgments that reflect the complexity of scientific theorizing.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Orthogonal Criteria Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM-generated theory &#8594; is_evaluated &#8594; for scientific merit</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; evaluation &#8594; assesses &#8594; empirical testability<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation &#8594; assesses &#8594; explanatory coherence<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation &#8594; assesses &#8594; novelty<span style="color: #888888;">, and</span></div>
        <div>&#8226; evaluation &#8594; assesses &#8594; epistemic utility</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Scientific theories are traditionally evaluated for testability, coherence, novelty, and utility in philosophy of science. </li>
    <li>LLMs can generate plausible but untestable or incoherent statements, necessitating multi-dimensional evaluation. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the criteria are known, their formal, orthogonal application to LLM-generated theory evaluation is new.</p>            <p><strong>What Already Exists:</strong> Multi-criteria evaluation is discussed in philosophy of science and AI model assessment.</p>            <p><strong>What is Novel:</strong> Explicit, orthogonal decomposition and composite scoring for LLM-generated scientific theories.</p>
            <p><strong>References:</strong> <ul>
    <li>Thagard (1978) The Best Explanation: Criteria for Theory Choice [criteria for theory evaluation]</li>
    <li>Kuhn (1977) Objectivity, Value Judgment, and Theory Choice [multiple criteria in theory choice]</li>
    <li>Bender & Koller (2020) Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data [AI model evaluation]</li>
</ul>
            <h3>Statement 1: Composite Scoring Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; theory &#8594; has_scores &#8594; on all evaluation dimensions</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; overall evaluation &#8594; is_computed_as &#8594; composite function of individual scores</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Composite scoring is used in multi-criteria decision analysis and model selection. </li>
    <li>No single criterion suffices for robust evaluation of complex outputs like LLM-generated theories. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The method is known, but its application to this context is new.</p>            <p><strong>What Already Exists:</strong> Composite scoring is standard in multi-criteria decision analysis.</p>            <p><strong>What is Novel:</strong> Application to LLM-generated scientific theory evaluation, with explicit mapping to scientific criteria.</p>
            <p><strong>References:</strong> <ul>
    <li>Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]</li>
    <li>Bender & Koller (2020) Climbing towards NLU [AI evaluation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM-generated theories that score highly across all four dimensions will be rated as more valuable by expert reviewers.</li>
                <li>Automated evaluation pipelines using this framework will better distinguish between plausible but unscientific and genuinely valuable theories.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Some theories may score low on novelty but high on utility, challenging the weightings in the composite function.</li>
                <li>LLMs may learn to optimize for certain dimensions at the expense of others, leading to new forms of 'gaming' the evaluation.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If theories scoring low on one or more dimensions are consistently rated higher by experts, the framework's validity is challenged.</li>
                <li>If composite scores do not correlate with downstream scientific impact, the composite law is questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The framework does not address the potential for LLMs to generate theories that are contextually appropriate but not globally novel. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The criteria are known, but their explicit, structured application to LLM-generated theory evaluation is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Thagard (1978) The Best Explanation: Criteria for Theory Choice [criteria for theory evaluation]</li>
    <li>Kuhn (1977) Objectivity, Value Judgment, and Theory Choice [multiple criteria in theory choice]</li>
    <li>Bender & Koller (2020) Climbing towards NLU [AI model evaluation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Dimensional Evaluation Framework for LLM-Generated Scientific Theories",
    "theory_description": "This theory posits that the evaluation of LLM-generated scientific theories should be conducted across multiple, orthogonal dimensions: empirical testability, explanatory coherence, novelty, and epistemic utility. Each dimension is assessed independently, and the overall evaluation is a composite function of these scores, allowing for nuanced judgments that reflect the complexity of scientific theorizing.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Orthogonal Criteria Law",
                "if": [
                    {
                        "subject": "LLM-generated theory",
                        "relation": "is_evaluated",
                        "object": "for scientific merit"
                    }
                ],
                "then": [
                    {
                        "subject": "evaluation",
                        "relation": "assesses",
                        "object": "empirical testability"
                    },
                    {
                        "subject": "evaluation",
                        "relation": "assesses",
                        "object": "explanatory coherence"
                    },
                    {
                        "subject": "evaluation",
                        "relation": "assesses",
                        "object": "novelty"
                    },
                    {
                        "subject": "evaluation",
                        "relation": "assesses",
                        "object": "epistemic utility"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Scientific theories are traditionally evaluated for testability, coherence, novelty, and utility in philosophy of science.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can generate plausible but untestable or incoherent statements, necessitating multi-dimensional evaluation.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Multi-criteria evaluation is discussed in philosophy of science and AI model assessment.",
                    "what_is_novel": "Explicit, orthogonal decomposition and composite scoring for LLM-generated scientific theories.",
                    "classification_explanation": "While the criteria are known, their formal, orthogonal application to LLM-generated theory evaluation is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Thagard (1978) The Best Explanation: Criteria for Theory Choice [criteria for theory evaluation]",
                        "Kuhn (1977) Objectivity, Value Judgment, and Theory Choice [multiple criteria in theory choice]",
                        "Bender & Koller (2020) Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data [AI model evaluation]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Composite Scoring Law",
                "if": [
                    {
                        "subject": "theory",
                        "relation": "has_scores",
                        "object": "on all evaluation dimensions"
                    }
                ],
                "then": [
                    {
                        "subject": "overall evaluation",
                        "relation": "is_computed_as",
                        "object": "composite function of individual scores"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Composite scoring is used in multi-criteria decision analysis and model selection.",
                        "uuids": []
                    },
                    {
                        "text": "No single criterion suffices for robust evaluation of complex outputs like LLM-generated theories.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "what_already_exists": "Composite scoring is standard in multi-criteria decision analysis.",
                    "what_is_novel": "Application to LLM-generated scientific theory evaluation, with explicit mapping to scientific criteria.",
                    "classification_explanation": "The method is known, but its application to this context is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Keeney & Raiffa (1993) Decisions with Multiple Objectives [multi-criteria decision analysis]",
                        "Bender & Koller (2020) Climbing towards NLU [AI evaluation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM-generated theories that score highly across all four dimensions will be rated as more valuable by expert reviewers.",
        "Automated evaluation pipelines using this framework will better distinguish between plausible but unscientific and genuinely valuable theories."
    ],
    "new_predictions_unknown": [
        "Some theories may score low on novelty but high on utility, challenging the weightings in the composite function.",
        "LLMs may learn to optimize for certain dimensions at the expense of others, leading to new forms of 'gaming' the evaluation."
    ],
    "negative_experiments": [
        "If theories scoring low on one or more dimensions are consistently rated higher by experts, the framework's validity is challenged.",
        "If composite scores do not correlate with downstream scientific impact, the composite law is questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The framework does not address the potential for LLMs to generate theories that are contextually appropriate but not globally novel.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some scientific breakthroughs were initially incoherent or lacked immediate testability, yet proved valuable.",
            "uuids": []
        }
    ],
    "special_cases": [
        "In highly speculative domains, explanatory coherence may outweigh empirical testability.",
        "In mature fields, novelty may be less valued than incremental utility."
    ],
    "existing_theory": {
        "what_already_exists": "Multi-criteria evaluation is established in philosophy of science and AI.",
        "what_is_novel": "Formal, orthogonal, and composite application to LLM-generated scientific theory evaluation.",
        "classification_explanation": "The criteria are known, but their explicit, structured application to LLM-generated theory evaluation is new.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Thagard (1978) The Best Explanation: Criteria for Theory Choice [criteria for theory evaluation]",
            "Kuhn (1977) Objectivity, Value Judgment, and Theory Choice [multiple criteria in theory choice]",
            "Bender & Koller (2020) Climbing towards NLU [AI model evaluation]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how to evaluate LLM-generated scientific theories.",
    "original_theory_id": "theory-676",
    "original_theory_name": "Multidimensional Evaluation Alignment Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
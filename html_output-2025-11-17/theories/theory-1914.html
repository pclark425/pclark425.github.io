<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1914</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1914</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format in which a problem is presented acts as an information bottleneck, modulating the accessibility and salience of relevant features for LLMs. Formats that compress or obscure key information reduce the effective information flow, while formats that highlight or structure information to match the LLM's processing strengths increase the likelihood of correct reasoning and output.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Information Accessibility Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; maximizes &#8594; explicitness_of_relevant_information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; higher_accuracy_on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better when all necessary information is made explicit in the prompt, such as with step-by-step instructions or explicit variable definitions. </li>
    <li>Ambiguous or underspecified prompts lead to lower LLM accuracy, even when the underlying problem is the same. </li>
    <li>Providing context windows with relevant facts improves LLM performance on multi-step reasoning tasks. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The explicit information bottleneck framing is novel, though the empirical effect of explicitness is known.</p>            <p><strong>What Already Exists:</strong> It is known that explicitness and clarity in prompts improve LLM performance.</p>            <p><strong>What is Novel:</strong> This law frames the effect as an information bottleneck, emphasizing the role of format in modulating information flow.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt explicitness and LLM performance]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Explicit stepwise reasoning improves performance]</li>
</ul>
            <h3>Statement 1: Format-Induced Information Loss Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; obscures_or_omits &#8594; key_problem_features</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; lower_accuracy_on_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs make more errors when prompts omit necessary context or use implicit references. </li>
    <li>Rewriting a problem to remove ambiguity or add missing information increases LLM accuracy. </li>
    <li>LLMs are sensitive to the presence or absence of explicit cues in the prompt. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The bottleneck abstraction is new, though the empirical effect is established.</p>            <p><strong>What Already Exists:</strong> Prompt ambiguity and missing information are known to reduce LLM performance.</p>            <p><strong>What is Novel:</strong> The law formalizes this as a format-induced information loss bottleneck, not just a surface-level prompt issue.</p>
            <p><strong>References:</strong> <ul>
    <li>Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt ambiguity and LLM behavior]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt explicitness and performance]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a problem is rewritten to make all relevant variables and relationships explicit, LLM performance will improve.</li>
                <li>If a prompt is compressed or key information is omitted, LLM accuracy will decrease even if the underlying problem is unchanged.</li>
                <li>If the same problem is presented in both a highly structured and a loosely structured format, the structured format will yield higher LLM accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained with adversarially compressed prompts, they may develop new strategies for information extraction.</li>
                <li>If LLMs are given prompts with redundant but differently formatted information, performance may plateau or degrade due to information overload.</li>
                <li>If LLMs are exposed to prompts with intentionally misleading explicitness, their error patterns may reveal new vulnerabilities.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on problems regardless of explicitness or information loss in the prompt, the theory would be falsified.</li>
                <li>If LLMs can infer missing information perfectly from context, the information bottleneck mechanism may not be primary.</li>
                <li>If LLMs show no performance drop on highly compressed or ambiguous prompts, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs infer missing information through world knowledge or context are not fully explained. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory abstracts prompt explicitness effects into an information-theoretic bottleneck framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt explicitness and LLM performance]</li>
    <li>Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt ambiguity and LLM behavior]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of LLM Problem Presentation",
    "theory_description": "This theory proposes that the format in which a problem is presented acts as an information bottleneck, modulating the accessibility and salience of relevant features for LLMs. Formats that compress or obscure key information reduce the effective information flow, while formats that highlight or structure information to match the LLM's processing strengths increase the likelihood of correct reasoning and output.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Information Accessibility Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "maximizes",
                        "object": "explicitness_of_relevant_information"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "higher_accuracy_on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better when all necessary information is made explicit in the prompt, such as with step-by-step instructions or explicit variable definitions.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or underspecified prompts lead to lower LLM accuracy, even when the underlying problem is the same.",
                        "uuids": []
                    },
                    {
                        "text": "Providing context windows with relevant facts improves LLM performance on multi-step reasoning tasks.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "It is known that explicitness and clarity in prompts improve LLM performance.",
                    "what_is_novel": "This law frames the effect as an information bottleneck, emphasizing the role of format in modulating information flow.",
                    "classification_explanation": "The explicit information bottleneck framing is novel, though the empirical effect of explicitness is known.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt explicitness and LLM performance]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Explicit stepwise reasoning improves performance]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Format-Induced Information Loss Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "obscures_or_omits",
                        "object": "key_problem_features"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "lower_accuracy_on_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs make more errors when prompts omit necessary context or use implicit references.",
                        "uuids": []
                    },
                    {
                        "text": "Rewriting a problem to remove ambiguity or add missing information increases LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs are sensitive to the presence or absence of explicit cues in the prompt.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt ambiguity and missing information are known to reduce LLM performance.",
                    "what_is_novel": "The law formalizes this as a format-induced information loss bottleneck, not just a surface-level prompt issue.",
                    "classification_explanation": "The bottleneck abstraction is new, though the empirical effect is established.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt ambiguity and LLM behavior]",
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt explicitness and performance]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a problem is rewritten to make all relevant variables and relationships explicit, LLM performance will improve.",
        "If a prompt is compressed or key information is omitted, LLM accuracy will decrease even if the underlying problem is unchanged.",
        "If the same problem is presented in both a highly structured and a loosely structured format, the structured format will yield higher LLM accuracy."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained with adversarially compressed prompts, they may develop new strategies for information extraction.",
        "If LLMs are given prompts with redundant but differently formatted information, performance may plateau or degrade due to information overload.",
        "If LLMs are exposed to prompts with intentionally misleading explicitness, their error patterns may reveal new vulnerabilities."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on problems regardless of explicitness or information loss in the prompt, the theory would be falsified.",
        "If LLMs can infer missing information perfectly from context, the information bottleneck mechanism may not be primary.",
        "If LLMs show no performance drop on highly compressed or ambiguous prompts, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs infer missing information through world knowledge or context are not fully explained.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can solve problems with minimal explicit information, possibly due to scale or emergent reasoning abilities.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very large LLMs may be able to reconstruct missing information from context, reducing the impact of the bottleneck.",
        "Tasks with highly redundant information may not benefit from further explicitness.",
        "Instruction-tuned models may be less sensitive to information loss in the prompt."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt explicitness and ambiguity effects are well-documented.",
        "what_is_novel": "The explicit information bottleneck framing and its application to LLM prompt format is new.",
        "classification_explanation": "The theory abstracts prompt explicitness effects into an information-theoretic bottleneck framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt explicitness and LLM performance]",
            "Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt ambiguity and LLM behavior]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
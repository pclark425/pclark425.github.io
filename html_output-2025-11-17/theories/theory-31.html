<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Generated Correctness-Filtered Data Superiority Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-31</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-31</p>
                <p><strong>Name:</strong> Self-Generated Correctness-Filtered Data Superiority Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> </p>
                <p><strong>Description:</strong> Training data generated by the model itself and filtered for correctness (keeping only outputs that reach correct final answers) provides substantially larger performance gains than direct supervised fine-tuning on human demonstrations alone. This effect is amplified when combined with rationalization (generating explanations conditioned on correct answers for initially failed examples). The mechanism works by: (1) generating diverse reasoning paths in the model's own 'language', (2) filtering creates a sparse reward signal approximating policy gradient RL, (3) rationalization exposes the model to harder problems it initially fails, and (4) iterative application compounds gains. This approach is more sample-efficient than collecting additional human demonstrations and scales better with compute than model size increases.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 9</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Self-generated training data filtered for correctness provides larger performance gains per training example than human demonstrations for reasoning tasks.</li>
                <li>The effectiveness scales with the diversity of distinct reasoning paths (measured by unique equation-lists or solution structures) rather than raw sample count.</li>
                <li>Rationalization (generating explanations conditioned on correct answers for failed examples) accelerates learning and increases final accuracy by 3-4 percentage points beyond generation alone.</li>
                <li>Iterative application of generate-filter-finetune loops compounds gains, with each iteration improving the model's ability to generate correct solutions.</li>
                <li>Cross-model aggregation of self-generated correct solutions yields substantially larger gains than single-model self-sampling due to increased reasoning diversity.</li>
                <li>The approach is more sample-efficient than collecting human demonstrations: STaR achieved comparable performance to much larger models while using <90% of training data.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>STaR (Self-Taught Reasoner) with rationalization achieved 72.5% on CommonsenseQA, substantially outperforming direct SFT (60.0%) and few-shot baselines (36.6%), while using fewer training examples (86.7% of dataset vs 100%). <a href="../results/extraction-result-219.html#e219.7" class="evidence-link">[e219.7]</a> <a href="../results/extraction-result-219.html#e219.0" class="evidence-link">[e219.0]</a> <a href="../results/extraction-result-219.html#e219.1" class="evidence-link">[e219.1]</a> </li>
    <li>RFT (Rejection-sampling Fine-Tuning) with k=100 samples improved LLaMA-7B from 35.9% to 41.7% on GSM8K (+5.8 points absolute), with the key factor being distinct reasoning paths rather than raw sample count. <a href="../results/extraction-result-227.html#e227.1" class="evidence-link">[e227.1]</a> <a href="../results/extraction-result-227.html#e227.4" class="evidence-link">[e227.4]</a> </li>
    <li>Self-training with DPO augmentation (DPO-ST) improved Flan-T5-Large from 30.8% (SFT) to 37.4% (+6.6 points), with classic self-training alone providing +4.8 points, showing self-generated pseudo-labels are highly effective. <a href="../results/extraction-result-212.html#e212.1" class="evidence-link">[e212.1]</a> <a href="../results/extraction-result-212.html#e212.2" class="evidence-link">[e212.2]</a> </li>
    <li>Aggregating rejection-sampled chains from multiple SFT models (RFT-U13B) yielded +13.4 percentage points for LLaMA-7B, substantially more than single-model self-sampling, due to increased reasoning path diversity. <a href="../results/extraction-result-227.html#e227.2" class="evidence-link">[e227.2]</a> </li>
    <li>Self-sampled Fully-Correct Solutions (FCS) improved PASS@100 by +9.0% to +12.3% absolute on GSM variants, with the effect being strongest for larger k (more diverse samples). <a href="../results/extraction-result-217.html#e217.1" class="evidence-link">[e217.1]</a> </li>
    <li>Filtering by correctness is central to STaR's improvements - training only on rationales that lead to correct answers approximates sparse-reward policy gradient and substantially improves over unfiltered high-temperature sampling. <a href="../results/extraction-result-219.html#e219.6" class="evidence-link">[e219.6]</a> <a href="../results/extraction-result-219.html#e219.4" class="evidence-link">[e219.4]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Applying STaR-style self-generation and filtering to scientific literature QA would yield 5-10 percentage point improvements over direct SFT on human demonstrations, with larger gains on questions requiring multi-step reasoning.</li>
                <li>Combining self-generated data from multiple base models (different architectures or sizes) before filtering would yield 2-3 percentage points additional improvement over single-model self-generation.</li>
                <li>Increasing sampling temperature from 0.7 to 1.0 during self-generation would increase the number of distinct reasoning paths by 20-40%, leading to proportional improvements in final performance.</li>
                <li>Self-generated data would be particularly effective for scientific domains with clear correctness criteria (e.g., numerical predictions, logical consistency checks) compared to subjective domains.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether self-generation with filtering would work for scientific literature QA tasks where correctness is not easily verifiable (e.g., interpretation questions, hypothesis generation) - might require human verification in the loop.</li>
                <li>Whether the approach would transfer to multi-document reasoning over scientific literature, where the 'correct' answer may require synthesizing information across papers with potentially conflicting claims.</li>
                <li>The optimal ratio of self-generated to human-demonstrated examples for scientific QA - too much self-generated data might cause distribution drift from human reasoning patterns.</li>
                <li>Whether iterative self-generation would eventually plateau or continue improving indefinitely, and at what point additional iterations provide diminishing returns for scientific reasoning tasks.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If self-generated data filtered for correctness performs worse than random sampling of human demonstrations when controlling for dataset size, this would challenge the theory.</li>
                <li>If removing the filtering step (training on all self-generated outputs regardless of correctness) performs equally well, this would suggest the filtering mechanism is not critical.</li>
                <li>If single-model self-generation performs as well as cross-model aggregation when controlling for total distinct reasoning paths, this would challenge the diversity hypothesis.</li>
                <li>If rationalization (hint-based generation) provides no additional benefit beyond standard self-generation on scientific QA tasks, this would question the mechanism's generality.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Self-query augmentation (generating synthetic queries from reasoning chains) performed worse than original human demonstrations, suggesting not all forms of self-generated data are beneficial. <a href="../results/extraction-result-227.html#e227.6" class="evidence-link">[e227.6]</a> </li>
    <li>High-temperature sampling without filtering consistently led to worse models than low-temperature with rationalization, indicating that naive data augmentation can be counterproductive. <a href="../results/extraction-result-219.html#e219.4" class="evidence-link">[e219.4]</a> </li>
    <li>The theory doesn't fully explain why some models (e.g., 33B) saw little RFT gain despite generating many correct paths - suggests model capacity or other factors matter. <a href="../results/extraction-result-227.html#e227.4" class="evidence-link">[e227.4]</a> </li>
    <li>Self-revising augmentation showed only marginal improvements (+0.19 points) and required careful diversity-based selection to avoid degradation, suggesting limits to self-improvement. <a href="../results/extraction-result-227.html#e227.7" class="evidence-link">[e227.7]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Zelikman et al. (2022) STaR: Bootstrapping Reasoning With Reasoning [Original STaR method, self-taught reasoner with rationalization]</li>
    <li>Yuan et al. (2023) Scaling Relationship on Learning Mathematical Reasoning with Large Language Models [RFT method and analysis of distinct reasoning paths]</li>
    <li>Gulcehre et al. (2023) Reinforced Self-Training (ReST) for Language Modeling [Related self-training approach with RL refinement]</li>
    <li>Huang et al. (2022) Large Language Models Can Self-Improve [Self-improvement through self-generated chain-of-thought]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Self-Generated Correctness-Filtered Data Superiority Theory",
    "theory_description": "Training data generated by the model itself and filtered for correctness (keeping only outputs that reach correct final answers) provides substantially larger performance gains than direct supervised fine-tuning on human demonstrations alone. This effect is amplified when combined with rationalization (generating explanations conditioned on correct answers for initially failed examples). The mechanism works by: (1) generating diverse reasoning paths in the model's own 'language', (2) filtering creates a sparse reward signal approximating policy gradient RL, (3) rationalization exposes the model to harder problems it initially fails, and (4) iterative application compounds gains. This approach is more sample-efficient than collecting additional human demonstrations and scales better with compute than model size increases.",
    "supporting_evidence": [
        {
            "text": "STaR (Self-Taught Reasoner) with rationalization achieved 72.5% on CommonsenseQA, substantially outperforming direct SFT (60.0%) and few-shot baselines (36.6%), while using fewer training examples (86.7% of dataset vs 100%).",
            "uuids": [
                "e219.7",
                "e219.0",
                "e219.1"
            ]
        },
        {
            "text": "RFT (Rejection-sampling Fine-Tuning) with k=100 samples improved LLaMA-7B from 35.9% to 41.7% on GSM8K (+5.8 points absolute), with the key factor being distinct reasoning paths rather than raw sample count.",
            "uuids": [
                "e227.1",
                "e227.4"
            ]
        },
        {
            "text": "Self-training with DPO augmentation (DPO-ST) improved Flan-T5-Large from 30.8% (SFT) to 37.4% (+6.6 points), with classic self-training alone providing +4.8 points, showing self-generated pseudo-labels are highly effective.",
            "uuids": [
                "e212.1",
                "e212.2"
            ]
        },
        {
            "text": "Aggregating rejection-sampled chains from multiple SFT models (RFT-U13B) yielded +13.4 percentage points for LLaMA-7B, substantially more than single-model self-sampling, due to increased reasoning path diversity.",
            "uuids": [
                "e227.2"
            ]
        },
        {
            "text": "Self-sampled Fully-Correct Solutions (FCS) improved PASS@100 by +9.0% to +12.3% absolute on GSM variants, with the effect being strongest for larger k (more diverse samples).",
            "uuids": [
                "e217.1"
            ]
        },
        {
            "text": "Filtering by correctness is central to STaR's improvements - training only on rationales that lead to correct answers approximates sparse-reward policy gradient and substantially improves over unfiltered high-temperature sampling.",
            "uuids": [
                "e219.6",
                "e219.4"
            ]
        }
    ],
    "theory_statements": [
        "Self-generated training data filtered for correctness provides larger performance gains per training example than human demonstrations for reasoning tasks.",
        "The effectiveness scales with the diversity of distinct reasoning paths (measured by unique equation-lists or solution structures) rather than raw sample count.",
        "Rationalization (generating explanations conditioned on correct answers for failed examples) accelerates learning and increases final accuracy by 3-4 percentage points beyond generation alone.",
        "Iterative application of generate-filter-finetune loops compounds gains, with each iteration improving the model's ability to generate correct solutions.",
        "Cross-model aggregation of self-generated correct solutions yields substantially larger gains than single-model self-sampling due to increased reasoning diversity.",
        "The approach is more sample-efficient than collecting human demonstrations: STaR achieved comparable performance to much larger models while using &lt;90% of training data."
    ],
    "new_predictions_likely": [
        "Applying STaR-style self-generation and filtering to scientific literature QA would yield 5-10 percentage point improvements over direct SFT on human demonstrations, with larger gains on questions requiring multi-step reasoning.",
        "Combining self-generated data from multiple base models (different architectures or sizes) before filtering would yield 2-3 percentage points additional improvement over single-model self-generation.",
        "Increasing sampling temperature from 0.7 to 1.0 during self-generation would increase the number of distinct reasoning paths by 20-40%, leading to proportional improvements in final performance.",
        "Self-generated data would be particularly effective for scientific domains with clear correctness criteria (e.g., numerical predictions, logical consistency checks) compared to subjective domains."
    ],
    "new_predictions_unknown": [
        "Whether self-generation with filtering would work for scientific literature QA tasks where correctness is not easily verifiable (e.g., interpretation questions, hypothesis generation) - might require human verification in the loop.",
        "Whether the approach would transfer to multi-document reasoning over scientific literature, where the 'correct' answer may require synthesizing information across papers with potentially conflicting claims.",
        "The optimal ratio of self-generated to human-demonstrated examples for scientific QA - too much self-generated data might cause distribution drift from human reasoning patterns.",
        "Whether iterative self-generation would eventually plateau or continue improving indefinitely, and at what point additional iterations provide diminishing returns for scientific reasoning tasks."
    ],
    "negative_experiments": [
        "If self-generated data filtered for correctness performs worse than random sampling of human demonstrations when controlling for dataset size, this would challenge the theory.",
        "If removing the filtering step (training on all self-generated outputs regardless of correctness) performs equally well, this would suggest the filtering mechanism is not critical.",
        "If single-model self-generation performs as well as cross-model aggregation when controlling for total distinct reasoning paths, this would challenge the diversity hypothesis.",
        "If rationalization (hint-based generation) provides no additional benefit beyond standard self-generation on scientific QA tasks, this would question the mechanism's generality."
    ],
    "unaccounted_for": [
        {
            "text": "Self-query augmentation (generating synthetic queries from reasoning chains) performed worse than original human demonstrations, suggesting not all forms of self-generated data are beneficial.",
            "uuids": [
                "e227.6"
            ]
        },
        {
            "text": "High-temperature sampling without filtering consistently led to worse models than low-temperature with rationalization, indicating that naive data augmentation can be counterproductive.",
            "uuids": [
                "e219.4"
            ]
        },
        {
            "text": "The theory doesn't fully explain why some models (e.g., 33B) saw little RFT gain despite generating many correct paths - suggests model capacity or other factors matter.",
            "uuids": [
                "e227.4"
            ]
        },
        {
            "text": "Self-revising augmentation showed only marginal improvements (+0.19 points) and required careful diversity-based selection to avoid degradation, suggesting limits to self-improvement.",
            "uuids": [
                "e227.7"
            ]
        }
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Zelikman et al. (2022) STaR: Bootstrapping Reasoning With Reasoning [Original STaR method, self-taught reasoner with rationalization]",
            "Yuan et al. (2023) Scaling Relationship on Learning Mathematical Reasoning with Large Language Models [RFT method and analysis of distinct reasoning paths]",
            "Gulcehre et al. (2023) Reinforced Self-Training (ReST) for Language Modeling [Related self-training approach with RL refinement]",
            "Huang et al. (2022) Large Language Models Can Self-Improve [Self-improvement through self-generated chain-of-thought]"
        ]
    },
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Alignment Theory of LLM Problem Presentation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1901</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1901</p>
                <p><strong>Name:</strong> Cognitive Alignment Theory of LLM Problem Presentation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory posits that the effectiveness of LLMs on problem-solving tasks is maximized when the problem presentation format is aligned with the LLM's internal cognitive architecture, specifically its token-by-token, context-dependent processing. Formats that structure information to match the LLM's sequential attention and memory constraints facilitate better comprehension, reasoning, and output generation.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Format-Architecture Alignment Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_aligned_with &#8594; llm_cognitive_architecture</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_maximized_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform better on tasks when the input format matches their training data distribution and processing style (e.g., natural language, stepwise reasoning). </li>
    <li>Prompt engineering that structures information sequentially or in familiar patterns improves LLM accuracy. </li>
    <li>LLMs struggle with formats that overload context windows or require global context integration beyond their memory span. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While related to prompt engineering, this law generalizes the principle to a broader cognitive alignment framework.</p>            <p><strong>What Already Exists:</strong> Prompt engineering and context window limitations are well-known factors in LLM performance.</p>            <p><strong>What is Novel:</strong> The explicit framing of 'cognitive alignment' as a general law governing the relationship between format and LLM performance is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format effects]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]</li>
</ul>
            <h3>Statement 1: Contextual Load Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; exceeds &#8594; llm_contextual_capacity</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; llm_performance &#8594; is_degraded_on &#8594; problem_presentation_format</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs show performance drops when prompts exceed their context window or require tracking too many entities. </li>
    <li>Tasks with long, unstructured input lead to more hallucinations and errors. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This law synthesizes known limitations into a general predictive law for format design.</p>            <p><strong>What Already Exists:</strong> Context window limitations and memory constraints are established in LLM literature.</p>            <p><strong>What is Novel:</strong> The law formalizes the relationship between contextual load and performance as a general principle for problem presentation.</p>
            <p><strong>References:</strong> <ul>
    <li>Vaswani et al. (2017) Attention is All You Need [Transformer context window]</li>
    <li>Press et al. (2022) Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation [Context window effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Reformatting a problem to match the LLM's training data style (e.g., Q&A, dialogue) will improve performance.</li>
                <li>Breaking up long, complex prompts into smaller, contextually coherent segments will reduce error rates.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained on highly non-naturalistic formats, their performance may be maximized on those formats, even if they are unnatural for humans.</li>
                <li>Future LLMs with fundamentally different architectures (e.g., with persistent memory) may have different optimal presentation formats.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on formats that are misaligned with their architecture (e.g., random token order), the theory is falsified.</li>
                <li>If increasing contextual load does not degrade performance, the theory's mechanism is called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where LLMs perform well on unfamiliar or adversarial formats due to overfitting or spurious correlations. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes and generalizes existing findings into a broader, predictive framework.</p>
            <p><strong>References:</strong> <ul>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format effects]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]</li>
    <li>Vaswani et al. (2017) Attention is All You Need [Transformer context window]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Cognitive Alignment Theory of LLM Problem Presentation",
    "theory_description": "This theory posits that the effectiveness of LLMs on problem-solving tasks is maximized when the problem presentation format is aligned with the LLM's internal cognitive architecture, specifically its token-by-token, context-dependent processing. Formats that structure information to match the LLM's sequential attention and memory constraints facilitate better comprehension, reasoning, and output generation.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Format-Architecture Alignment Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_aligned_with",
                        "object": "llm_cognitive_architecture"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_maximized_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform better on tasks when the input format matches their training data distribution and processing style (e.g., natural language, stepwise reasoning).",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering that structures information sequentially or in familiar patterns improves LLM accuracy.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs struggle with formats that overload context windows or require global context integration beyond their memory span.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt engineering and context window limitations are well-known factors in LLM performance.",
                    "what_is_novel": "The explicit framing of 'cognitive alignment' as a general law governing the relationship between format and LLM performance is new.",
                    "classification_explanation": "While related to prompt engineering, this law generalizes the principle to a broader cognitive alignment framework.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format effects]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Contextual Load Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "exceeds",
                        "object": "llm_contextual_capacity"
                    }
                ],
                "then": [
                    {
                        "subject": "llm_performance",
                        "relation": "is_degraded_on",
                        "object": "problem_presentation_format"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs show performance drops when prompts exceed their context window or require tracking too many entities.",
                        "uuids": []
                    },
                    {
                        "text": "Tasks with long, unstructured input lead to more hallucinations and errors.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Context window limitations and memory constraints are established in LLM literature.",
                    "what_is_novel": "The law formalizes the relationship between contextual load and performance as a general principle for problem presentation.",
                    "classification_explanation": "This law synthesizes known limitations into a general predictive law for format design.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Vaswani et al. (2017) Attention is All You Need [Transformer context window]",
                        "Press et al. (2022) Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation [Context window effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Reformatting a problem to match the LLM's training data style (e.g., Q&A, dialogue) will improve performance.",
        "Breaking up long, complex prompts into smaller, contextually coherent segments will reduce error rates."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained on highly non-naturalistic formats, their performance may be maximized on those formats, even if they are unnatural for humans.",
        "Future LLMs with fundamentally different architectures (e.g., with persistent memory) may have different optimal presentation formats."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on formats that are misaligned with their architecture (e.g., random token order), the theory is falsified.",
        "If increasing contextual load does not degrade performance, the theory's mechanism is called into question."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where LLMs perform well on unfamiliar or adversarial formats due to overfitting or spurious correlations.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs can generalize to new formats with minimal performance loss, especially with instruction tuning.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Highly redundant or repetitive formats may not benefit from alignment if they introduce noise.",
        "Very simple tasks may be format-invariant."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt engineering and context window effects are well-studied.",
        "what_is_novel": "The generalization to a cognitive alignment principle is new.",
        "classification_explanation": "The theory synthesizes and generalizes existing findings into a broader, predictive framework.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Brown et al. (2020) Language Models are Few-Shot Learners [Prompt format effects]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt structure and reasoning]",
            "Vaswani et al. (2017) Attention is All You Need [Transformer context window]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-653",
    "original_theory_name": "Prompt Format as a Mechanism for Task Decomposition and Cognitive Scaffolding in LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Dimensional Proxy-to-Ground-Truth Gap Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-369</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-369</p>
                <p><strong>Name:</strong> Multi-Dimensional Proxy-to-Ground-Truth Gap Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about the evaluation and validation of incremental versus transformational scientific discoveries in automated systems.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that divergence between proxy metrics and ground truth scientific value follows systematic patterns determined by novelty TYPE rather than degree alone. Three distinct novelty types show qualitatively different proxy-truth relationships: (1) Pioneer novelty (new topics/questions) faces severe undervaluation (60-90% gaps) by short-term proxies, requiring ≥10 years for recognition; (2) Maverick novelty (distant recombination) is often rewarded by proxies (negative gaps of -70% to 0%, representing over-valuation); (3) Vanguard novelty (reinforcing emerging connections) shows moderate reward with diminishing returns (0-40% gaps, inverted-U pattern). These type-specific gaps arise from training distribution bias—systems calibrated on historical data dominated by incremental and recombinatory work systematically misalign with Pioneer work while favoring visible Maverick recombinations. Gap magnitude follows type-specific functions: G_Pioneer(T) ≈ k_P*e^(β_P*T) (exponential), G_Maverick(T) ≈ -k_M*T (linear reward), G_Vanguard(T) ≈ k_V*T - α*T^2 (inverted-U). Gaps are time-dependent with type-specific decay: G(T_type,t) = G(T_type)*e^(-λ_type*t) where λ_Pioneer << λ_Maverick << λ_Vanguard. Multiple proxy failures interact complexly—compounding multiplicatively (burden factor b_p≈119 attenuates displacement by ~1/120), moving in opposite directions (citation vs novelty measures), or showing low correlations (ρ from -0.24 to +0.40). Field effects include both paradigm rigidity (β) and institutional infrastructure (costs, funding structure, dissemination norms, venue effects with variance≈1.0). Critically, gaps are design-dependent: well-designed systems with structured diversity, dynamic knowledge exchange, entropy-weighted training, and extended windows can match or exceed human performance (86.5% vs 65.1% alignment) while still showing some biases. Correction mechanisms range from 30-90% effectiveness depending on type and combination.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-320.html">[theory-320]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Restructured theory from unidimensional transformation degree T to multi-dimensional novelty types (Pioneer, Maverick, Vanguard) with distinct proxy-truth relationships and empirical support.</li>
                <li>Replaced single exponential G(T)≈k*e^(βT) with type-specific functions: G_Pioneer exponential undervaluation, G_Maverick linear reward (negative gap), G_Vanguard inverted-U.</li>
                <li>Revised gap magnitude from universal 70-90% to type-specific: Pioneer 60-90%, Maverick -70% to 0% (over-valuation), Vanguard 0-40%.</li>
                <li>Modified time-dependence from single λ to type-specific decay rates: λ_Pioneer << λ_Maverick << λ_Vanguard, with Pioneer requiring ≥10 years.</li>
                <li>Changed automated system characterization from 'systematic undervaluation' to 'design-dependent bias', acknowledging well-designed systems can match/exceed humans (86.5% vs 65.1%).</li>
                <li>Expanded field factors from paradigm rigidity β alone to include institutional infrastructure (costs, funding, dissemination, venue effects with variance≈1.0).</li>
                <li>Clarified proxy failure interactions as complex (multiplicative, oppositional, low-correlation) rather than simply multiplicative, with specific empirical examples.</li>
                <li>Expanded correction mechanisms from meta-learning only to include architectural interventions, training procedures, and temporal adjustments, with effectiveness range 30-90% (up from 30-40%).</li>
                <li>Added venue and editorial strategy as major moderators with effect sizes comparable to field effects (journal variance=1.035 vs year variance=0.382).</li>
                <li>Added acknowledgment that human review shows comparable proxy-truth gaps (65.1% alignment, 23% disagreement), broadening from automation-specific to general evaluation challenge.</li>
                <li>Added supporting evidence for training distribution bias (100% false-positive rate, 14.7% recent citations).</li>
                <li>Added supporting evidence for multiplicative failures (b_p≈119 creates ~1/120 attenuation).</li>
                <li>Added supporting evidence for historical rejection (24 Nobel cases, 52% vs 1.4% within-field displacement).</li>
                <li>Modified training bias description to 'dominated by incremental and recombinatory science' (not just incremental), acknowledging Maverick representation in training data.</li>
                <li>Added theory statement on complex proxy interactions with specific patterns (multiplicative, oppositional, low-correlation).</li>
                <li>Added unaccounted-for items: reward model overoptimization, communication effects, reference-count convergence, Pioneer sub-types, context-dependent bias absence.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Novelty is multi-dimensional with three distinct types: Pioneer (new topics/questions), Maverick (distant recombination), and Vanguard (reinforcement), each showing qualitatively different proxy-truth relationships.</li>
                <li>Pioneer novelty faces severe systematic undervaluation: G_Pioneer(T) ≈ k_P*e^(β_P*T), with 60-90% gaps for highly transformational work and no significant 5-year citation effect (coefficient≈0, p>0.05).</li>
                <li>Maverick novelty is rewarded by proxies: G_Maverick(T) ≈ -k_M*T (negative gap), with short-term citation coefficients +0.5 to +0.6 (p<0.001), corresponding to 50-75% higher citations.</li>
                <li>Vanguard novelty shows inverted-U relationship: G_Vanguard(T) ≈ k_V*T - α*T^2, with positive main effects (+0.1 to +0.2) and significant negative quadratic terms.</li>
                <li>Gaps are time-dependent with type-specific decay: G(T_type,t) = G(T_type)*e^(-λ_type*t) where λ_Pioneer << λ_Maverick << λ_Vanguard. Pioneer requires ≥10 years for stabilization, Maverick shows rapid recognition.</li>
                <li>Training distribution bias causes systematic misalignment: systems calibrated on data dominated by incremental and recombinatory work undervalue Pioneer novelty while favoring Maverick recombinations, manifesting as 100% false-positive rates and citation skew toward older literature.</li>
                <li>Multiple proxy failures interact complexly: multiplicatively (b_p≈119 attenuates d_p by ~1/120), oppositionally (CI and ON can change in opposite directions), or with low correlation (ρ from -0.24 to +0.40 across different proxies).</li>
                <li>Field effects combine paradigm rigidity (β) and institutional infrastructure: computational vs experimental costs, funding structure (plural vs centralized), dissemination norms (preprint vs journal), and venue effects (journal variance≈1.0).</li>
                <li>Venue and editorial strategy create gaps comparable to field effects: journal random-effect variance (1.035) exceeds year variance (0.382); exploratory vs selective strategies produce divergent outcomes.</li>
                <li>Gaps are design-dependent, not inherent to automation: well-designed systems with structured diversity, dynamic knowledge exchange, entropy-weighted training, and extended windows can match or exceed human performance (86.5% vs 65.1% alignment).</li>
                <li>Correction mechanisms show 30-90% effectiveness: meta-learning (30-40%), architectural interventions (30-50%), training procedures (up to 92% error reduction), temporal adjustments (qualitative improvement).</li>
                <li>Human review shows comparable proxy-truth gaps: 65.1% reasoning alignment, 62.8% conclusion agreement, 23% committee disagreement on identical papers, indicating general evaluation challenge.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Pioneer novelty shows severe systematic undervaluation: 5-year citations show no significant positive effect (coefficient=-0.014, p=0.84), requiring >10 years for D-index stabilization, with gaps approaching 60-90%. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> <a href="../results/extraction-result-1884.html#e1884.2" class="evidence-link">[e1884.2]</a> </li>
    <li>Maverick novelty is strongly rewarded: coefficient=0.546 (p<0.001), corresponding to ~73% higher citations, showing negative gap (over-valuation). <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Vanguard novelty shows inverted-U pattern: coefficient=0.109 (p<0.001) with significant negative quadratic term, indicating moderate reward with diminishing returns. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Type-specific time dependencies confirmed: Pioneer requires ≥10 years, Maverick shows faster recognition, supporting distinct λ_type parameters. <a href="../results/extraction-result-1884.html#e1884.2" class="evidence-link">[e1884.2]</a> <a href="../results/extraction-result-1884.html#e1884.5" class="evidence-link">[e1884.5]</a> <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Training distribution bias demonstrated: 100% false-positive rate in AI novelty detection (12/12 items), citation patterns skewed to older literature (14.7% from 2020+). <a href="../results/extraction-result-1886.html#e1886.0" class="evidence-link">[e1886.0]</a> <a href="../results/extraction-result-1886.html#e1886.2" class="evidence-link">[e1886.2]</a> </li>
    <li>Multiplicative proxy failures: 98.9% of papers have b_p>1 (median=119), creating ~1/120 attenuation of displacement potential. <a href="../results/extraction-result-1884.html#e1884.1" class="evidence-link">[e1884.1]</a> </li>
    <li>Field-specific institutional effects: journal variance (1.035) exceeds year variance (0.382); AI vs biomedicine show contrasting structures (plural vs centralized funding, low vs high costs). <a href="../results/extraction-result-1881.html#e1881.2" class="evidence-link">[e1881.2]</a> <a href="../results/extraction-result-1888.html#e1888.9" class="evidence-link">[e1888.9]</a> <a href="../results/extraction-result-1888.html#e1888.6" class="evidence-link">[e1888.6]</a> </li>
    <li>Historical transformational work rejection: 24 Nobel papers initially rejected; disruptive papers show 52% within-field displacement vs 1.4% expected. <a href="../results/extraction-result-1888.html#e1888.2" class="evidence-link">[e1888.2]</a> <a href="../results/extraction-result-1884.html#e1884.4" class="evidence-link">[e1884.4]</a> </li>
    <li>Institutional concentration in automated selection: GPT ranking shows 43.8% vs 27.0% (ICLR 2023) and 37.2% vs 26.7% (ICLR 2024) from top-10 institutions. <a href="../results/extraction-result-1880.html#e1880.0" class="evidence-link">[e1880.0]</a> </li>
    <li>Design-dependent performance: well-designed systems achieve 86.5% vs 65.1% reasoning alignment and 20.00 vs 19.36 average citations, matching or exceeding humans. <a href="../results/extraction-result-1885.html#e1885.1" class="evidence-link">[e1885.1]</a> <a href="../results/extraction-result-1880.html#e1880.0" class="evidence-link">[e1880.0]</a> </li>
    <li>Correction effectiveness: IDVSCI achieves +33.6% CI and +24.3% ON; entropy-weighted loss reduces MSE by 92% (0.1191→0.0093). <a href="../results/extraction-result-1887.html#e1887.0" class="evidence-link">[e1887.0]</a> <a href="../results/extraction-result-1887.html#e1887.1" class="evidence-link">[e1887.1]</a> <a href="../results/extraction-result-1883.html#e1883.2" class="evidence-link">[e1883.2]</a> </li>
    <li>Complex proxy interactions: voting removal increased CI (+0.9%) while decreasing ON (-1.56%); novelty measures show low concordance (ρ=-0.041 to +0.399). <a href="../results/extraction-result-1887.html#e1887.2" class="evidence-link">[e1887.2]</a> <a href="../results/extraction-result-1881.html#e1881.1" class="evidence-link">[e1881.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Papers introducing new research questions (Pioneer) will show near-zero 5-year citation correlation but strong 10+ year correlation, with turning point at years 7-10.</li>
                <li>Papers recombining distant fields (Maverick) will receive 50-80% higher 5-year citations than single-field applications, controlling for quality.</li>
                <li>Automated systems using only 5-year citations will rank Pioneer work 60-90% lower and Maverick work 50-70% higher than eventual 10-year rankings.</li>
                <li>High-cost centralized-funding fields (biomedicine, particle physics) will show steeper Pioneer undervaluation (higher β_Pioneer) than low-cost plural-funding fields (ML, theory).</li>
                <li>Structured diversity (25% background diversity, distributed references) will increase novelty scores 20-40% and reduce convergent outputs 50-80%.</li>
                <li>Extending citation windows from 5 to 10+ years will reverse team-size vs disruptiveness correlation from positive to negative.</li>
                <li>Entropy-weighted training will reduce prediction error on high-disruptiveness items by 70-90% vs standard training.</li>
                <li>Exploratory journals will publish more Pioneer papers with lower average impact; selective journals will publish fewer Pioneer papers with higher average impact.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Training systems to distinguish Pioneer/Maverick/Vanguard with type-specific rewards might reduce Pioneer undervaluation to 20-40% or create gaming behaviors where researchers artificially signal multiple types.</li>
                <li>Novelty-type portfolio allocation (30% Pioneer, 40% Maverick, 30% Vanguard) might accelerate progress by ensuring Pioneer support or create perverse incentives and misclassification.</li>
                <li>As AI generates more ideas, literature distribution might shift toward Maverick recombinations, either creating feedback loops disadvantaging human Pioneer work or freeing humans to focus on Pioneer contributions AI cannot generate.</li>
                <li>Hybrid systems (automated screening for Maverick/Vanguard, mandatory human review for Pioneer) might achieve optimal efficiency or create two-tier systems with additional Pioneer barriers.</li>
                <li>Burden-factor-aware evaluation adjusting for b_p might enable fairer assessment of canonical-reference displacement or introduce new biases from mis-estimation or gaming.</li>
                <li>If high-rigidity fields adopt AI/ML institutional reforms (preprints, plural funding, low-cost simulation), this might reduce β_Pioneer and accelerate transformation or reduce rigor and increase false positives.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If Pioneer novelty receives equal or higher 5-year citations than incremental work (controlling for quality), the core undervaluation prediction is falsified.</li>
                <li>If Pioneer gap magnitude is constant or decreasing with T (not exponential), the functional form G_Pioneer(T)≈k_P*e^(β_P*T) is invalidated.</li>
                <li>If Maverick novelty is systematically undervalued (not rewarded) by short-term proxies, the novelty-type distinction is called into question.</li>
                <li>If extending windows to 10+ years does not change team-size vs disruptiveness relationship, the time-dependent gap mechanism is challenged.</li>
                <li>If entropy-weighted training shows same or worse performance on high-disruptiveness items, the training distribution bias and correction claims are invalidated.</li>
                <li>If fields with similar rigidity but different infrastructure show identical β_Pioneer, the claim that institutional factors matter beyond rigidity is falsified.</li>
                <li>If proxy failures consistently compound additively (not multiplicatively, oppositionally, or with low correlation), the complex interaction mechanism needs revision.</li>
                <li>If well-designed automated systems consistently underperform humans across all novelty types and dimensions, the design-dependent bias claim is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some contexts show no significant novelty bias (CoRL 2023, NeurIPS 2023 showed no embedding-distance differences), suggesting additional contextual moderators beyond those specified. <a href="../results/extraction-result-1880.html#e1880.2" class="evidence-link">[e1880.2]</a> </li>
    <li>Why some Pioneer discoveries receive rapid recognition while others face prolonged delays beyond 10-year average is not explained, suggesting communication quality, timing, and field readiness matter. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> <a href="../results/extraction-result-1884.html#e1884.2" class="evidence-link">[e1884.2]</a> </li>
    <li>Reward model overoptimization in RL systems creates additional biases beyond reward signal (rating gap ~0.56, accept-rate shift 31.07%→28.65%), suggesting training procedures introduce gaps not captured by training data distribution focus. <a href="../results/extraction-result-1879.html#e1879.2" class="evidence-link">[e1879.2]</a> </li>
    <li>Scientific communication and framing effects on proxy-truth gaps are not addressed—well-communicated transformational work may show smaller gaps, potentially interacting with novelty type. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Sub-types within Pioneer novelty (theoretical vs methodological vs empirical) may show different gap patterns and time-dependencies not specified in the theory. <a href="../results/extraction-result-1881.html#e1881.0" class="evidence-link">[e1881.0]</a> </li>
    <li>Reference-count and literature-grounding effects create convergence bias (shared references increase similarity, reduce novelty), but interaction with training distribution bias mechanism is not specified. <a href="../results/extraction-result-1887.html#e1887.3" class="evidence-link">[e1887.3]</a> </li>
    <li>Social networks, collaboration patterns, and visibility effects on proxy-truth gaps are not fully incorporated—these may moderate gaps independently of novelty type and field characteristics. <a href="../results/extraction-result-1880.html#e1880.3" class="evidence-link">[e1880.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Dimensional Proxy-to-Ground-Truth Gap Theory",
    "type": "specific",
    "theory_description": "This theory posits that divergence between proxy metrics and ground truth scientific value follows systematic patterns determined by novelty TYPE rather than degree alone. Three distinct novelty types show qualitatively different proxy-truth relationships: (1) Pioneer novelty (new topics/questions) faces severe undervaluation (60-90% gaps) by short-term proxies, requiring ≥10 years for recognition; (2) Maverick novelty (distant recombination) is often rewarded by proxies (negative gaps of -70% to 0%, representing over-valuation); (3) Vanguard novelty (reinforcing emerging connections) shows moderate reward with diminishing returns (0-40% gaps, inverted-U pattern). These type-specific gaps arise from training distribution bias—systems calibrated on historical data dominated by incremental and recombinatory work systematically misalign with Pioneer work while favoring visible Maverick recombinations. Gap magnitude follows type-specific functions: G_Pioneer(T) ≈ k_P*e^(β_P*T) (exponential), G_Maverick(T) ≈ -k_M*T (linear reward), G_Vanguard(T) ≈ k_V*T - α*T^2 (inverted-U). Gaps are time-dependent with type-specific decay: G(T_type,t) = G(T_type)*e^(-λ_type*t) where λ_Pioneer &lt;&lt; λ_Maverick &lt;&lt; λ_Vanguard. Multiple proxy failures interact complexly—compounding multiplicatively (burden factor b_p≈119 attenuates displacement by ~1/120), moving in opposite directions (citation vs novelty measures), or showing low correlations (ρ from -0.24 to +0.40). Field effects include both paradigm rigidity (β) and institutional infrastructure (costs, funding structure, dissemination norms, venue effects with variance≈1.0). Critically, gaps are design-dependent: well-designed systems with structured diversity, dynamic knowledge exchange, entropy-weighted training, and extended windows can match or exceed human performance (86.5% vs 65.1% alignment) while still showing some biases. Correction mechanisms range from 30-90% effectiveness depending on type and combination.",
    "supporting_evidence": [
        {
            "text": "Pioneer novelty shows severe systematic undervaluation: 5-year citations show no significant positive effect (coefficient=-0.014, p=0.84), requiring &gt;10 years for D-index stabilization, with gaps approaching 60-90%.",
            "uuids": [
                "e1881.0",
                "e1884.2"
            ]
        },
        {
            "text": "Maverick novelty is strongly rewarded: coefficient=0.546 (p&lt;0.001), corresponding to ~73% higher citations, showing negative gap (over-valuation).",
            "uuids": [
                "e1881.0"
            ]
        },
        {
            "text": "Vanguard novelty shows inverted-U pattern: coefficient=0.109 (p&lt;0.001) with significant negative quadratic term, indicating moderate reward with diminishing returns.",
            "uuids": [
                "e1881.0"
            ]
        },
        {
            "text": "Type-specific time dependencies confirmed: Pioneer requires ≥10 years, Maverick shows faster recognition, supporting distinct λ_type parameters.",
            "uuids": [
                "e1884.2",
                "e1884.5",
                "e1881.0"
            ]
        },
        {
            "text": "Training distribution bias demonstrated: 100% false-positive rate in AI novelty detection (12/12 items), citation patterns skewed to older literature (14.7% from 2020+).",
            "uuids": [
                "e1886.0",
                "e1886.2"
            ]
        },
        {
            "text": "Multiplicative proxy failures: 98.9% of papers have b_p&gt;1 (median=119), creating ~1/120 attenuation of displacement potential.",
            "uuids": [
                "e1884.1"
            ]
        },
        {
            "text": "Field-specific institutional effects: journal variance (1.035) exceeds year variance (0.382); AI vs biomedicine show contrasting structures (plural vs centralized funding, low vs high costs).",
            "uuids": [
                "e1881.2",
                "e1888.9",
                "e1888.6"
            ]
        },
        {
            "text": "Historical transformational work rejection: 24 Nobel papers initially rejected; disruptive papers show 52% within-field displacement vs 1.4% expected.",
            "uuids": [
                "e1888.2",
                "e1884.4"
            ]
        },
        {
            "text": "Institutional concentration in automated selection: GPT ranking shows 43.8% vs 27.0% (ICLR 2023) and 37.2% vs 26.7% (ICLR 2024) from top-10 institutions.",
            "uuids": [
                "e1880.0"
            ]
        },
        {
            "text": "Design-dependent performance: well-designed systems achieve 86.5% vs 65.1% reasoning alignment and 20.00 vs 19.36 average citations, matching or exceeding humans.",
            "uuids": [
                "e1885.1",
                "e1880.0"
            ]
        },
        {
            "text": "Correction effectiveness: IDVSCI achieves +33.6% CI and +24.3% ON; entropy-weighted loss reduces MSE by 92% (0.1191→0.0093).",
            "uuids": [
                "e1887.0",
                "e1887.1",
                "e1883.2"
            ]
        },
        {
            "text": "Complex proxy interactions: voting removal increased CI (+0.9%) while decreasing ON (-1.56%); novelty measures show low concordance (ρ=-0.041 to +0.399).",
            "uuids": [
                "e1887.2",
                "e1881.1"
            ]
        }
    ],
    "theory_statements": [
        "Novelty is multi-dimensional with three distinct types: Pioneer (new topics/questions), Maverick (distant recombination), and Vanguard (reinforcement), each showing qualitatively different proxy-truth relationships.",
        "Pioneer novelty faces severe systematic undervaluation: G_Pioneer(T) ≈ k_P*e^(β_P*T), with 60-90% gaps for highly transformational work and no significant 5-year citation effect (coefficient≈0, p&gt;0.05).",
        "Maverick novelty is rewarded by proxies: G_Maverick(T) ≈ -k_M*T (negative gap), with short-term citation coefficients +0.5 to +0.6 (p&lt;0.001), corresponding to 50-75% higher citations.",
        "Vanguard novelty shows inverted-U relationship: G_Vanguard(T) ≈ k_V*T - α*T^2, with positive main effects (+0.1 to +0.2) and significant negative quadratic terms.",
        "Gaps are time-dependent with type-specific decay: G(T_type,t) = G(T_type)*e^(-λ_type*t) where λ_Pioneer &lt;&lt; λ_Maverick &lt;&lt; λ_Vanguard. Pioneer requires ≥10 years for stabilization, Maverick shows rapid recognition.",
        "Training distribution bias causes systematic misalignment: systems calibrated on data dominated by incremental and recombinatory work undervalue Pioneer novelty while favoring Maverick recombinations, manifesting as 100% false-positive rates and citation skew toward older literature.",
        "Multiple proxy failures interact complexly: multiplicatively (b_p≈119 attenuates d_p by ~1/120), oppositionally (CI and ON can change in opposite directions), or with low correlation (ρ from -0.24 to +0.40 across different proxies).",
        "Field effects combine paradigm rigidity (β) and institutional infrastructure: computational vs experimental costs, funding structure (plural vs centralized), dissemination norms (preprint vs journal), and venue effects (journal variance≈1.0).",
        "Venue and editorial strategy create gaps comparable to field effects: journal random-effect variance (1.035) exceeds year variance (0.382); exploratory vs selective strategies produce divergent outcomes.",
        "Gaps are design-dependent, not inherent to automation: well-designed systems with structured diversity, dynamic knowledge exchange, entropy-weighted training, and extended windows can match or exceed human performance (86.5% vs 65.1% alignment).",
        "Correction mechanisms show 30-90% effectiveness: meta-learning (30-40%), architectural interventions (30-50%), training procedures (up to 92% error reduction), temporal adjustments (qualitative improvement).",
        "Human review shows comparable proxy-truth gaps: 65.1% reasoning alignment, 62.8% conclusion agreement, 23% committee disagreement on identical papers, indicating general evaluation challenge."
    ],
    "new_predictions_likely": [
        "Papers introducing new research questions (Pioneer) will show near-zero 5-year citation correlation but strong 10+ year correlation, with turning point at years 7-10.",
        "Papers recombining distant fields (Maverick) will receive 50-80% higher 5-year citations than single-field applications, controlling for quality.",
        "Automated systems using only 5-year citations will rank Pioneer work 60-90% lower and Maverick work 50-70% higher than eventual 10-year rankings.",
        "High-cost centralized-funding fields (biomedicine, particle physics) will show steeper Pioneer undervaluation (higher β_Pioneer) than low-cost plural-funding fields (ML, theory).",
        "Structured diversity (25% background diversity, distributed references) will increase novelty scores 20-40% and reduce convergent outputs 50-80%.",
        "Extending citation windows from 5 to 10+ years will reverse team-size vs disruptiveness correlation from positive to negative.",
        "Entropy-weighted training will reduce prediction error on high-disruptiveness items by 70-90% vs standard training.",
        "Exploratory journals will publish more Pioneer papers with lower average impact; selective journals will publish fewer Pioneer papers with higher average impact."
    ],
    "new_predictions_unknown": [
        "Training systems to distinguish Pioneer/Maverick/Vanguard with type-specific rewards might reduce Pioneer undervaluation to 20-40% or create gaming behaviors where researchers artificially signal multiple types.",
        "Novelty-type portfolio allocation (30% Pioneer, 40% Maverick, 30% Vanguard) might accelerate progress by ensuring Pioneer support or create perverse incentives and misclassification.",
        "As AI generates more ideas, literature distribution might shift toward Maverick recombinations, either creating feedback loops disadvantaging human Pioneer work or freeing humans to focus on Pioneer contributions AI cannot generate.",
        "Hybrid systems (automated screening for Maverick/Vanguard, mandatory human review for Pioneer) might achieve optimal efficiency or create two-tier systems with additional Pioneer barriers.",
        "Burden-factor-aware evaluation adjusting for b_p might enable fairer assessment of canonical-reference displacement or introduce new biases from mis-estimation or gaming.",
        "If high-rigidity fields adopt AI/ML institutional reforms (preprints, plural funding, low-cost simulation), this might reduce β_Pioneer and accelerate transformation or reduce rigor and increase false positives."
    ],
    "negative_experiments": [
        "If Pioneer novelty receives equal or higher 5-year citations than incremental work (controlling for quality), the core undervaluation prediction is falsified.",
        "If Pioneer gap magnitude is constant or decreasing with T (not exponential), the functional form G_Pioneer(T)≈k_P*e^(β_P*T) is invalidated.",
        "If Maverick novelty is systematically undervalued (not rewarded) by short-term proxies, the novelty-type distinction is called into question.",
        "If extending windows to 10+ years does not change team-size vs disruptiveness relationship, the time-dependent gap mechanism is challenged.",
        "If entropy-weighted training shows same or worse performance on high-disruptiveness items, the training distribution bias and correction claims are invalidated.",
        "If fields with similar rigidity but different infrastructure show identical β_Pioneer, the claim that institutional factors matter beyond rigidity is falsified.",
        "If proxy failures consistently compound additively (not multiplicatively, oppositionally, or with low correlation), the complex interaction mechanism needs revision.",
        "If well-designed automated systems consistently underperform humans across all novelty types and dimensions, the design-dependent bias claim is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some contexts show no significant novelty bias (CoRL 2023, NeurIPS 2023 showed no embedding-distance differences), suggesting additional contextual moderators beyond those specified.",
            "uuids": [
                "e1880.2"
            ]
        },
        {
            "text": "Why some Pioneer discoveries receive rapid recognition while others face prolonged delays beyond 10-year average is not explained, suggesting communication quality, timing, and field readiness matter.",
            "uuids": [
                "e1881.0",
                "e1884.2"
            ]
        },
        {
            "text": "Reward model overoptimization in RL systems creates additional biases beyond reward signal (rating gap ~0.56, accept-rate shift 31.07%→28.65%), suggesting training procedures introduce gaps not captured by training data distribution focus.",
            "uuids": [
                "e1879.2"
            ]
        },
        {
            "text": "Scientific communication and framing effects on proxy-truth gaps are not addressed—well-communicated transformational work may show smaller gaps, potentially interacting with novelty type.",
            "uuids": [
                "e1881.0"
            ]
        },
        {
            "text": "Sub-types within Pioneer novelty (theoretical vs methodological vs empirical) may show different gap patterns and time-dependencies not specified in the theory.",
            "uuids": [
                "e1881.0"
            ]
        },
        {
            "text": "Reference-count and literature-grounding effects create convergence bias (shared references increase similarity, reduce novelty), but interaction with training distribution bias mechanism is not specified.",
            "uuids": [
                "e1887.3"
            ]
        },
        {
            "text": "Social networks, collaboration patterns, and visibility effects on proxy-truth gaps are not fully incorporated—these may moderate gaps independently of novelty type and field characteristics.",
            "uuids": [
                "e1880.3"
            ]
        }
    ],
    "change_log": [
        "Restructured theory from unidimensional transformation degree T to multi-dimensional novelty types (Pioneer, Maverick, Vanguard) with distinct proxy-truth relationships and empirical support.",
        "Replaced single exponential G(T)≈k*e^(βT) with type-specific functions: G_Pioneer exponential undervaluation, G_Maverick linear reward (negative gap), G_Vanguard inverted-U.",
        "Revised gap magnitude from universal 70-90% to type-specific: Pioneer 60-90%, Maverick -70% to 0% (over-valuation), Vanguard 0-40%.",
        "Modified time-dependence from single λ to type-specific decay rates: λ_Pioneer &lt;&lt; λ_Maverick &lt;&lt; λ_Vanguard, with Pioneer requiring ≥10 years.",
        "Changed automated system characterization from 'systematic undervaluation' to 'design-dependent bias', acknowledging well-designed systems can match/exceed humans (86.5% vs 65.1%).",
        "Expanded field factors from paradigm rigidity β alone to include institutional infrastructure (costs, funding, dissemination, venue effects with variance≈1.0).",
        "Clarified proxy failure interactions as complex (multiplicative, oppositional, low-correlation) rather than simply multiplicative, with specific empirical examples.",
        "Expanded correction mechanisms from meta-learning only to include architectural interventions, training procedures, and temporal adjustments, with effectiveness range 30-90% (up from 30-40%).",
        "Added venue and editorial strategy as major moderators with effect sizes comparable to field effects (journal variance=1.035 vs year variance=0.382).",
        "Added acknowledgment that human review shows comparable proxy-truth gaps (65.1% alignment, 23% disagreement), broadening from automation-specific to general evaluation challenge.",
        "Added supporting evidence for training distribution bias (100% false-positive rate, 14.7% recent citations).",
        "Added supporting evidence for multiplicative failures (b_p≈119 creates ~1/120 attenuation).",
        "Added supporting evidence for historical rejection (24 Nobel cases, 52% vs 1.4% within-field displacement).",
        "Modified training bias description to 'dominated by incremental and recombinatory science' (not just incremental), acknowledging Maverick representation in training data.",
        "Added theory statement on complex proxy interactions with specific patterns (multiplicative, oppositional, low-correlation).",
        "Added unaccounted-for items: reward model overoptimization, communication effects, reference-count convergence, Pioneer sub-types, context-dependent bias absence."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
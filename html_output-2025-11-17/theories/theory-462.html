<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Retrieval-Augmented Memory Optimization Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-462</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-462</p>
                <p><strong>Name:</strong> Retrieval-Augmented Memory Optimization Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language model agents can best use memory to solve tasks, based on the following results.</p>
                <p><strong>Description:</strong> This theory posits that the effectiveness of memory in language model agents is primarily determined by the quality and task-alignment of retrieval mechanisms. Specifically, retrieval-augmented memory (RAM) systems that employ learned, task-adaptive retrievers—such as LLM-based, cross-encoder, or reward-model-based retrievers—outperform static or heuristic retrieval (e.g., BM25, semantic-only, or shallow history) by providing more functionally relevant context. The theory further asserts that end-to-end or reward-aligned retriever training, and the use of hybrid retrieval (semantic pre-filtering plus LLM or cross-encoder scoring), are critical for maximizing the utility of external memory across diverse agent tasks, including in-context learning, question answering, planning, and generalization to new domains.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Task-Aligned Retrieval Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses &#8594; retrieval-augmented memory<span style="color: #888888;">, and</span></div>
        <div>&#8226; retriever &#8594; is_trained_or_adapted_for &#8594; the downstream task objective</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; higher task performance than with static or heuristic retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM-retrieval and LLM-R retrievers, trained with LLM feedback or reward models, outperform semantic-only or BM25 retrieval on in-context learning and QA tasks. <a href="../results/extraction-result-3184.html#e3184.1" class="evidence-link">[e3184.1]</a> <a href="../results/extraction-result-3196.html#e3196.0" class="evidence-link">[e3196.0]</a> </li>
    <li>RAG-Sequence with learned query encoder outperforms frozen retriever ablation (+2.8 EM on NQ). <a href="../results/extraction-result-3052.html#e3052.6" class="evidence-link">[e3052.6]</a> </li>
    <li>DSP (Demonstrate-Search-Predict) with task-aware retrieval and bootstrapped demonstrations outperforms retrieve-then-read and vanilla LLMs on open-domain QA and multi-hop QA. <a href="../results/extraction-result-3198.html#e3198.0" class="evidence-link">[e3198.0]</a> <a href="../results/extraction-result-3198.html#e3198.1" class="evidence-link">[e3198.1]</a> <a href="../results/extraction-result-3198.html#e3198.4" class="evidence-link">[e3198.4]</a> </li>
    <li>FiD-RAG, which jointly optimizes retriever and generator, achieves lower perplexity and better session-opening behavior than non-retrieval or non-jointly trained baselines. <a href="../results/extraction-result-3221.html#e3221.4" class="evidence-link">[e3221.4]</a> </li>
    <li>LMORT, a plug-in retrieval tuner trained on top of frozen LLMs, achieves higher zero-shot retrieval performance than static dense retrievers. <a href="../results/extraction-result-3017.html#e3017.0" class="evidence-link">[e3017.0]</a> </li>
    <li>ORQA and RAG show that dense retrievers trained with task-aligned objectives outperform static BM25 on genuine information-seeking datasets, though not always on lexical-overlap datasets. <a href="../results/extraction-result-3211.html#e3211.0" class="evidence-link">[e3211.0]</a> <a href="../results/extraction-result-3052.html#e3052.3" class="evidence-link">[e3052.3]</a> <a href="../results/extraction-result-3052.html#e3052.4" class="evidence-link">[e3052.4]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Hybrid Retrieval Superiority Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses &#8594; hybrid retrieval (semantic pre-filtering + LLM or cross-encoder scoring)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; achieves &#8594; higher accuracy and relevance in memory selection than with either method alone</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM-retrieval (semantic + LLM selection) and BM25+Cross-Encoder pipelines outperform single-stage retrieval on multiple benchmarks. <a href="../results/extraction-result-3184.html#e3184.1" class="evidence-link">[e3184.1]</a> <a href="../results/extraction-result-3226.html#e3226.4" class="evidence-link">[e3226.4]</a> </li>
    <li>LLM-R uses BM25 for candidate pre-filtering and LLM/cross-encoder for final scoring, yielding higher in-context learning performance than either alone. <a href="../results/extraction-result-3196.html#e3196.0" class="evidence-link">[e3196.0]</a> </li>
    <li>DSP uses semantic retrieval for candidate narrowing and LLM-based selection for demonstration and passage selection, outperforming semantic-only or LLM-only retrieval. <a href="../results/extraction-result-3198.html#e3198.0" class="evidence-link">[e3198.0]</a> <a href="../results/extraction-result-3198.html#e3198.1" class="evidence-link">[e3198.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Retrieval-Driven Generalization Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; agent &#8594; uses &#8594; retrieval-augmented memory with task-aligned retriever<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; requires &#8594; generalization to unseen domains or skills</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; can &#8594; transfer knowledge and skills via memory retrieval, enabling zero-shot or few-shot generalization</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Voyager and AutoGPT+Skill Library demonstrate zero-shot task solving via retrieval of skill programs; ExpeL Transfer and RAP show cross-task transfer via memory. <a href="../results/extraction-result-3216.html#e3216.0" class="evidence-link">[e3216.0]</a> <a href="../results/extraction-result-3216.html#e3216.1" class="evidence-link">[e3216.1]</a> <a href="../results/extraction-result-3039.html#e3039.3" class="evidence-link">[e3039.3]</a> <a href="../results/extraction-result-3045.html#e3045.0" class="evidence-link">[e3045.0]</a> </li>
    <li>LLM-R and LLM-retrieval improve generalization to held-out tasks and domains by retrieving more functionally relevant in-context examples. <a href="../results/extraction-result-3196.html#e3196.0" class="evidence-link">[e3196.0]</a> <a href="../results/extraction-result-3184.html#e3184.1" class="evidence-link">[e3184.1]</a> </li>
    <li>SumMem-MSC and MemoryBank approaches enable long-term dialogue agents to recall and generalize across sessions, supporting cross-session consistency. <a href="../results/extraction-result-3221.html#e3221.5" class="evidence-link">[e3221.5]</a> <a href="../results/extraction-result-2988.html#e2988.0" class="evidence-link">[e2988.0]</a> <a href="../results/extraction-result-2988.html#e2988.2" class="evidence-link">[e2988.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Replacing static retrievers with task-adaptive, LLM-based, or reward-model-trained retrievers in any retrieval-augmented agent will yield measurable performance gains on the target task.</li>
                <li>Hybrid retrieval pipelines (semantic pre-filtering + LLM/cross-encoder scoring) will outperform either semantic-only or LLM-only retrieval on tasks requiring deep logical or functional relevance.</li>
                <li>Agents using retrieval-augmented memory with task-aligned retrievers will show improved zero-shot generalization to new domains or tasks compared to agents with static retrieval.</li>
                <li>End-to-end retriever-generator training (as in FiD-RAG or RAG-Sequence) will outperform pipelines where the retriever is frozen or not adapted to the downstream task.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If retrieval scoring is made fully differentiable and jointly optimized with the agent's policy (e.g., in RL or planning), agents may develop emergent memory usage strategies that surpass current modular approaches.</li>
                <li>In highly compositional or creative tasks (e.g., open-ended code generation, creative writing), LLM-based retrieval may enable agents to synthesize novel solutions by analogizing from diverse memory entries.</li>
                <li>Adversarial attacks on the retrieval scoring function (e.g., poisoning the retriever) may have outsized negative effects on agent performance, suggesting a new vulnerability class.</li>
                <li>Hybrid retrieval methods that combine symbolic (database) and neural (embedding) retrieval may outperform either alone in tasks requiring both precise factual recall and semantic generalization.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If replacing a static retriever with a task-adaptive retriever does not improve performance on a downstream task, the task-aligned retrieval law would be challenged.</li>
                <li>If hybrid retrieval (semantic + LLM) does not outperform either method alone on tasks requiring deep reasoning, the hybrid retrieval superiority law would be undermined.</li>
                <li>If agents with retrieval-augmented memory and task-aligned retrievers fail to generalize to new domains or tasks, the retrieval-driven generalization law would be called into question.</li>
                <li>If end-to-end retriever-generator training does not outperform frozen retriever pipelines, the importance of task-alignment in retrieval would be questioned.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address cases where retrieval errors or memory poisoning degrade performance, nor does it specify optimal retriever architectures for all task types. <a href="../results/extraction-result-3207.html#e3207.5" class="evidence-link">[e3207.5]</a> <a href="../results/extraction-result-3199.html#e3199.2" class="evidence-link">[e3199.2]</a> <a href="../results/extraction-result-3207.html#e3207.2" class="evidence-link">[e3207.2]</a> <a href="../results/extraction-result-3199.html#e3199.0" class="evidence-link">[e3199.0]</a> </li>
    <li>Some tasks may be bottlenecked by the base LLM's capabilities rather than retrieval quality (e.g., ChatGPT vs BELLE in MemoryBank, or when the LLM cannot utilize retrieved content effectively). <a href="../results/extraction-result-2988.html#e2988.0" class="evidence-link">[e2988.0]</a> <a href="../results/extraction-result-2988.html#e2988.2" class="evidence-link">[e2988.2]</a> <a href="../results/extraction-result-3205.html#e3205.0" class="evidence-link">[e3205.0]</a> <a href="../results/extraction-result-3205.html#e3205.4" class="evidence-link">[e3205.4]</a> </li>
    <li>Retrieval-augmented memory may not help when the relevant information is not present in the memory store (e.g., MS-MARCO questions unanswerable from Wikipedia in RAG-Sequence). <a href="../results/extraction-result-3052.html#e3052.3" class="evidence-link">[e3052.3]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Karpukhin et al. (2020) Dense Passage Retrieval for Open-Domain Question Answering [Dense retriever training for QA]</li>
    <li>Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [RAG, end-to-end retriever-generator]</li>
    <li>Rubin et al. (2023) LLM-R: Learning to Retrieve In-Context Examples for Large Language Models [LLM-based retriever training]</li>
    <li>Sun et al. (2023) MoT: Memory-of-Thought Enables ChatGPT to Self-Improve [LLM-based retrieval for demonstration selection]</li>
    <li>Izacard et al. (2021) Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering [FiD, joint retriever-generator training]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Retrieval-Augmented Memory Optimization Theory",
    "theory_description": "This theory posits that the effectiveness of memory in language model agents is primarily determined by the quality and task-alignment of retrieval mechanisms. Specifically, retrieval-augmented memory (RAM) systems that employ learned, task-adaptive retrievers—such as LLM-based, cross-encoder, or reward-model-based retrievers—outperform static or heuristic retrieval (e.g., BM25, semantic-only, or shallow history) by providing more functionally relevant context. The theory further asserts that end-to-end or reward-aligned retriever training, and the use of hybrid retrieval (semantic pre-filtering plus LLM or cross-encoder scoring), are critical for maximizing the utility of external memory across diverse agent tasks, including in-context learning, question answering, planning, and generalization to new domains.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Task-Aligned Retrieval Law",
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses",
                        "object": "retrieval-augmented memory"
                    },
                    {
                        "subject": "retriever",
                        "relation": "is_trained_or_adapted_for",
                        "object": "the downstream task objective"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "higher task performance than with static or heuristic retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM-retrieval and LLM-R retrievers, trained with LLM feedback or reward models, outperform semantic-only or BM25 retrieval on in-context learning and QA tasks.",
                        "uuids": [
                            "e3184.1",
                            "e3196.0"
                        ]
                    },
                    {
                        "text": "RAG-Sequence with learned query encoder outperforms frozen retriever ablation (+2.8 EM on NQ).",
                        "uuids": [
                            "e3052.6"
                        ]
                    },
                    {
                        "text": "DSP (Demonstrate-Search-Predict) with task-aware retrieval and bootstrapped demonstrations outperforms retrieve-then-read and vanilla LLMs on open-domain QA and multi-hop QA.",
                        "uuids": [
                            "e3198.0",
                            "e3198.1",
                            "e3198.4"
                        ]
                    },
                    {
                        "text": "FiD-RAG, which jointly optimizes retriever and generator, achieves lower perplexity and better session-opening behavior than non-retrieval or non-jointly trained baselines.",
                        "uuids": [
                            "e3221.4"
                        ]
                    },
                    {
                        "text": "LMORT, a plug-in retrieval tuner trained on top of frozen LLMs, achieves higher zero-shot retrieval performance than static dense retrievers.",
                        "uuids": [
                            "e3017.0"
                        ]
                    },
                    {
                        "text": "ORQA and RAG show that dense retrievers trained with task-aligned objectives outperform static BM25 on genuine information-seeking datasets, though not always on lexical-overlap datasets.",
                        "uuids": [
                            "e3211.0",
                            "e3052.3",
                            "e3052.4"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Hybrid Retrieval Superiority Law",
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses",
                        "object": "hybrid retrieval (semantic pre-filtering + LLM or cross-encoder scoring)"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "achieves",
                        "object": "higher accuracy and relevance in memory selection than with either method alone"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM-retrieval (semantic + LLM selection) and BM25+Cross-Encoder pipelines outperform single-stage retrieval on multiple benchmarks.",
                        "uuids": [
                            "e3184.1",
                            "e3226.4"
                        ]
                    },
                    {
                        "text": "LLM-R uses BM25 for candidate pre-filtering and LLM/cross-encoder for final scoring, yielding higher in-context learning performance than either alone.",
                        "uuids": [
                            "e3196.0"
                        ]
                    },
                    {
                        "text": "DSP uses semantic retrieval for candidate narrowing and LLM-based selection for demonstration and passage selection, outperforming semantic-only or LLM-only retrieval.",
                        "uuids": [
                            "e3198.0",
                            "e3198.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Retrieval-Driven Generalization Law",
                "if": [
                    {
                        "subject": "agent",
                        "relation": "uses",
                        "object": "retrieval-augmented memory with task-aligned retriever"
                    },
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "generalization to unseen domains or skills"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "can",
                        "object": "transfer knowledge and skills via memory retrieval, enabling zero-shot or few-shot generalization"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Voyager and AutoGPT+Skill Library demonstrate zero-shot task solving via retrieval of skill programs; ExpeL Transfer and RAP show cross-task transfer via memory.",
                        "uuids": [
                            "e3216.0",
                            "e3216.1",
                            "e3039.3",
                            "e3045.0"
                        ]
                    },
                    {
                        "text": "LLM-R and LLM-retrieval improve generalization to held-out tasks and domains by retrieving more functionally relevant in-context examples.",
                        "uuids": [
                            "e3196.0",
                            "e3184.1"
                        ]
                    },
                    {
                        "text": "SumMem-MSC and MemoryBank approaches enable long-term dialogue agents to recall and generalize across sessions, supporting cross-session consistency.",
                        "uuids": [
                            "e3221.5",
                            "e2988.0",
                            "e2988.2"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "Replacing static retrievers with task-adaptive, LLM-based, or reward-model-trained retrievers in any retrieval-augmented agent will yield measurable performance gains on the target task.",
        "Hybrid retrieval pipelines (semantic pre-filtering + LLM/cross-encoder scoring) will outperform either semantic-only or LLM-only retrieval on tasks requiring deep logical or functional relevance.",
        "Agents using retrieval-augmented memory with task-aligned retrievers will show improved zero-shot generalization to new domains or tasks compared to agents with static retrieval.",
        "End-to-end retriever-generator training (as in FiD-RAG or RAG-Sequence) will outperform pipelines where the retriever is frozen or not adapted to the downstream task."
    ],
    "new_predictions_unknown": [
        "If retrieval scoring is made fully differentiable and jointly optimized with the agent's policy (e.g., in RL or planning), agents may develop emergent memory usage strategies that surpass current modular approaches.",
        "In highly compositional or creative tasks (e.g., open-ended code generation, creative writing), LLM-based retrieval may enable agents to synthesize novel solutions by analogizing from diverse memory entries.",
        "Adversarial attacks on the retrieval scoring function (e.g., poisoning the retriever) may have outsized negative effects on agent performance, suggesting a new vulnerability class.",
        "Hybrid retrieval methods that combine symbolic (database) and neural (embedding) retrieval may outperform either alone in tasks requiring both precise factual recall and semantic generalization."
    ],
    "negative_experiments": [
        "If replacing a static retriever with a task-adaptive retriever does not improve performance on a downstream task, the task-aligned retrieval law would be challenged.",
        "If hybrid retrieval (semantic + LLM) does not outperform either method alone on tasks requiring deep reasoning, the hybrid retrieval superiority law would be undermined.",
        "If agents with retrieval-augmented memory and task-aligned retrievers fail to generalize to new domains or tasks, the retrieval-driven generalization law would be called into question.",
        "If end-to-end retriever-generator training does not outperform frozen retriever pipelines, the importance of task-alignment in retrieval would be questioned."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address cases where retrieval errors or memory poisoning degrade performance, nor does it specify optimal retriever architectures for all task types.",
            "uuids": [
                "e3207.5",
                "e3199.2",
                "e3207.2",
                "e3199.0"
            ]
        },
        {
            "text": "Some tasks may be bottlenecked by the base LLM's capabilities rather than retrieval quality (e.g., ChatGPT vs BELLE in MemoryBank, or when the LLM cannot utilize retrieved content effectively).",
            "uuids": [
                "e2988.0",
                "e2988.2",
                "e3205.0",
                "e3205.4"
            ]
        },
        {
            "text": "Retrieval-augmented memory may not help when the relevant information is not present in the memory store (e.g., MS-MARCO questions unanswerable from Wikipedia in RAG-Sequence).",
            "uuids": [
                "e3052.3"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some domains, static lexical retrieval (BM25) can outperform dense retrievers (e.g., SQuAD, TriviaQA in ORQA), suggesting that task alignment is not always sufficient.",
            "uuids": [
                "e3211.0",
                "e3052.4"
            ]
        },
        {
            "text": "Naive or shallow history inclusion can degrade performance (WebShop ablation), indicating that not all retrieval is beneficial.",
            "uuids": [
                "e3174.0"
            ]
        },
        {
            "text": "In some cases, retrieval-augmented memory can introduce noise or irrelevant context, harming performance if not properly filtered (e.g., retrieval collapse in RAG-Token, or irrelevant docs in DocPrompting).",
            "uuids": [
                "e3052.2",
                "e3206.0"
            ]
        }
    ],
    "special_cases": [
        "Tasks with high lexical overlap between queries and evidence may benefit more from static lexical retrieval than from dense or LLM-based retrieval.",
        "In low-resource or privacy-sensitive settings, retriever training may be constrained, limiting the applicability of this theory.",
        "If the memory store is incomplete or lacks relevant entries, even the best retriever cannot improve performance.",
        "For tasks where the LLM cannot utilize retrieved content (e.g., due to context window limits or lack of grounding), retrieval quality may not translate to downstream gains."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Karpukhin et al. (2020) Dense Passage Retrieval for Open-Domain Question Answering [Dense retriever training for QA]",
            "Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [RAG, end-to-end retriever-generator]",
            "Rubin et al. (2023) LLM-R: Learning to Retrieve In-Context Examples for Large Language Models [LLM-based retriever training]",
            "Sun et al. (2023) MoT: Memory-of-Thought Enables ChatGPT to Self-Improve [LLM-based retrieval for demonstration selection]",
            "Izacard et al. (2021) Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering [FiD, joint retriever-generator training]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
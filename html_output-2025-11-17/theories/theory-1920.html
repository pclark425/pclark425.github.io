<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Bottleneck Theory of Problem Presentation in LLMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1920</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1920</p>
                <p><strong>Name:</strong> Information Bottleneck Theory of Problem Presentation in LLMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how problem presentation format affects LLM performance.</p>
                <p><strong>Description:</strong> This theory proposes that the format of problem presentation acts as an information bottleneck, modulating the amount and type of information accessible to the LLM's reasoning process. Formats that explicitly scaffold relevant information (e.g., stepwise prompts, explicit context) reduce the bottleneck and enable more effective reasoning, while terse or ambiguous formats increase the bottleneck, leading to information loss and degraded performance.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Scaffolded Information Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; provides_explicit_scaffolding &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; enhanced_reasoning_and_accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Step-by-step and chain-of-thought prompts improve LLM performance on complex reasoning tasks. </li>
    <li>Providing explicit context or intermediate steps reduces LLM error rates. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The effect is known, but the information bottleneck framing is novel.</p>            <p><strong>What Already Exists:</strong> Chain-of-thought and stepwise prompting are known to improve LLM reasoning.</p>            <p><strong>What is Novel:</strong> The law frames this as a reduction in information bottleneck via explicit scaffolding.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Stepwise prompting effects]</li>
    <li>Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [Scaffolding effects]</li>
</ul>
            <h3>Statement 1: Bottleneck-Induced Degradation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; problem_presentation_format &#8594; is_terse_or_ambiguous &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; exhibits &#8594; increased_information_loss_and_error</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Direct questions without context or scaffolding lead to shortcut answers and higher error rates. </li>
    <li>Ambiguous or underspecified prompts result in LLMs making incorrect assumptions. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The effect is known, but the explicit bottleneck law is new.</p>            <p><strong>What Already Exists:</strong> Direct, terse, or ambiguous prompts are known to degrade LLM performance.</p>            <p><strong>What is Novel:</strong> The law formalizes this as an information bottleneck effect.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Direct vs. stepwise prompt effects]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt ambiguity effects]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a problem is reformulated to include explicit scaffolding, LLM performance will improve, especially on multi-step tasks.</li>
                <li>If context is omitted or prompts are made more ambiguous, LLM error rates will increase.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLMs are trained to internally generate scaffolding when faced with terse prompts, they may overcome the bottleneck effect.</li>
                <li>If bottleneck effects interact with model size or architecture, larger models may be less sensitive to terse formats.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs perform equally well on terse and scaffolded formats, the theory would be falsified.</li>
                <li>If ambiguous prompts do not increase error rates, the bottleneck-induced degradation law would be invalid.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LLMs may use external tools or retrieval to compensate for missing context. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known effects into a general information-theoretic principle.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt scaffolding effects]</li>
    <li>Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt ambiguity effects]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Information Bottleneck Theory of Problem Presentation in LLMs",
    "theory_description": "This theory proposes that the format of problem presentation acts as an information bottleneck, modulating the amount and type of information accessible to the LLM's reasoning process. Formats that explicitly scaffold relevant information (e.g., stepwise prompts, explicit context) reduce the bottleneck and enable more effective reasoning, while terse or ambiguous formats increase the bottleneck, leading to information loss and degraded performance.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Scaffolded Information Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "provides_explicit_scaffolding",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "enhanced_reasoning_and_accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Step-by-step and chain-of-thought prompts improve LLM performance on complex reasoning tasks.",
                        "uuids": []
                    },
                    {
                        "text": "Providing explicit context or intermediate steps reduces LLM error rates.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Chain-of-thought and stepwise prompting are known to improve LLM reasoning.",
                    "what_is_novel": "The law frames this as a reduction in information bottleneck via explicit scaffolding.",
                    "classification_explanation": "The effect is known, but the information bottleneck framing is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Stepwise prompting effects]",
                        "Kojima et al. (2022) Large Language Models are Zero-Shot Reasoners [Scaffolding effects]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Bottleneck-Induced Degradation Law",
                "if": [
                    {
                        "subject": "problem_presentation_format",
                        "relation": "is_terse_or_ambiguous",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "exhibits",
                        "object": "increased_information_loss_and_error"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Direct questions without context or scaffolding lead to shortcut answers and higher error rates.",
                        "uuids": []
                    },
                    {
                        "text": "Ambiguous or underspecified prompts result in LLMs making incorrect assumptions.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Direct, terse, or ambiguous prompts are known to degrade LLM performance.",
                    "what_is_novel": "The law formalizes this as an information bottleneck effect.",
                    "classification_explanation": "The effect is known, but the explicit bottleneck law is new.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Direct vs. stepwise prompt effects]",
                        "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt ambiguity effects]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a problem is reformulated to include explicit scaffolding, LLM performance will improve, especially on multi-step tasks.",
        "If context is omitted or prompts are made more ambiguous, LLM error rates will increase."
    ],
    "new_predictions_unknown": [
        "If LLMs are trained to internally generate scaffolding when faced with terse prompts, they may overcome the bottleneck effect.",
        "If bottleneck effects interact with model size or architecture, larger models may be less sensitive to terse formats."
    ],
    "negative_experiments": [
        "If LLMs perform equally well on terse and scaffolded formats, the theory would be falsified.",
        "If ambiguous prompts do not increase error rates, the bottleneck-induced degradation law would be invalid."
    ],
    "unaccounted_for": [
        {
            "text": "Some LLMs may use external tools or retrieval to compensate for missing context.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Instruction-tuned LLMs sometimes perform well even on terse prompts.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Simple factual recall tasks may not be affected by bottleneck effects.",
        "Highly redundant prompts may not further improve performance beyond a certain point."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt scaffolding and ambiguity effects are established.",
        "what_is_novel": "The explicit information bottleneck framing is new.",
        "classification_explanation": "The theory synthesizes known effects into a general information-theoretic principle.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Prompt scaffolding effects]",
            "Min et al. (2022) Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? [Prompt ambiguity effects]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how problem presentation format affects LLM performance.",
    "original_theory_id": "theory-654",
    "original_theory_name": "Observed Instruction Template Dominance in Instruction-Tuned LLMs",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
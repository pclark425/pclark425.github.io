<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memory-Retrieval Alignment Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-450</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-450</p>
                <p><strong>Name:</strong> Memory-Retrieval Alignment Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM-based agents can most effectively be augmented with memory to solve text games, based on the following results.</p>
                <p><strong>Description:</strong> The effectiveness of memory retrieval in text game agents depends on the alignment between retrieval mechanism, memory content type, and task demands. Semantic similarity retrieval works best for tasks requiring analogical reasoning and pattern matching, recency-based retrieval for tasks with strong temporal dependencies, and importance-based retrieval for tasks with sparse critical information. Hybrid retrieval strategies that combine multiple criteria (semantic, temporal, importance) often outperform single-criterion approaches. Misalignment between retrieval strategy and task structure leads to retrieval of irrelevant memories, wasted context capacity, and performance degradation. The optimal retrieval strategy also depends on memory size, with sophisticated retrieval becoming more important as memory grows.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Law 0</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; requires &#8594; analogical transfer or pattern matching across experiences<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; uses &#8594; semantic similarity via embeddings</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; retrieves &#8594; relevant analogous experiences<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; exceeds &#8594; random or recency-only retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ExpeL task-similarity retrieval (Faiss inner-product on embeddings) outperforms reasoning-similarity and random sampling across HotpotQA, ALFWorld, and WebShop <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>LoTa-Bench semantic similarity sampling using Sentence-BERT achieves 43.25% vs 40.82% baseline on WAH-NL with LLaMA-1 65B <a href="../results/extraction-result-2876.html#e2876.0" class="evidence-link">[e2876.0]</a> </li>
    <li>Voyager embedding-based skill retrieval (text-embedding-ada-002) enables compositional task solving and zero-shot generalization to new Minecraft tasks <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> </li>
    <li>AriGraph semantic similarity retrieval via Contriever embeddings plus graph expansion enables robust spatial modeling and navigation <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>Werewolf agent semantic similarity retrieval (SentenceBERT with cosine similarity threshold 0.85) for experience pool improves decision quality <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> </li>
    <li>GITM stores and retrieves reference plans by sub-goal similarity, substantially improving iron_pickaxe success from 57.5% to 95.0% <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> </li>
    <li>LoTa-Bench semantic similarity sampling outperforms task-specific and random sampling across multiple model sizes <a href="../results/extraction-result-2876.html#e2876.0" class="evidence-link">[e2876.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Law 1</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; has &#8594; strong temporal dependencies or sequential constraints<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; prioritizes &#8594; recent experiences</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; maintains &#8594; coherent sequential behavior<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; is better than &#8594; non-temporal retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AGENTBOARD sliding-window recency-based truncation enables longer interactions; GPT-4 maintains 70% progress rate across multi-turn tasks <a href="../results/extraction-result-2836.html#e2836.0" class="evidence-link">[e2836.0]</a> </li>
    <li>Sweet&Sour short-term buffer for recent subtask completions before long-term transfer improves performance: GPT-4o achieves 54.6 vs 36.0 baseline <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
    <li>BUTLER observation queue (k=5 recent unique observations) with recency maintains coherent action sequences in ALFWorld <a href="../results/extraction-result-2864.html#e2864.0" class="evidence-link">[e2864.0]</a> </li>
    <li>SmartPlay prompted LLM agent uses recency-based history window (50 steps for Bandits, 30 for Hanoi) to maintain context <a href="../results/extraction-result-2806.html#e2806.0" class="evidence-link">[e2806.0]</a> </li>
    <li>EMMA 3-frame temporal buffer helps ground movement dynamics in MESSENGER tasks <a href="../results/extraction-result-2855.html#e2855.0" class="evidence-link">[e2855.0]</a> </li>
    <li>CALM short sliding window (previous observation + previous action + current observation) enables contextually-relevant action generation <a href="../results/extraction-result-2850.html#e2850.0" class="evidence-link">[e2850.0]</a> </li>
    <li>SWIFTSAGE SWIFT-only with K=10 sliding window performs well early in episodes but fails on long-horizon tasks without LLM planning <a href="../results/extraction-result-2878.html#e2878.1" class="evidence-link">[e2878.1]</a> </li>
    <li>Avalon ReAct agent recursive summarization with current Minutes maintains game history for decision-making <a href="../results/extraction-result-2869.html#e2869.0" class="evidence-link">[e2869.0]</a> <a href="../results/extraction-result-2879.html#e2879.0" class="evidence-link">[e2879.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 2: Law 2</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; memory &#8594; contains &#8594; experiences with varying importance<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; uses &#8594; importance or relevance scoring<span style="color: #888888;">, and</span></div>
        <div>&#8226; task &#8594; has &#8594; sparse critical information</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; agent &#8594; retrieves &#8594; high-value memories<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance &#8594; improves over &#8594; uniform retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SAGE MemorySyntax retention-strength based retrieval (R(I_t,τ)=exp(-τ/S)) prioritizes important information, achieving 73.8% task completion vs 56.5% baseline on ALFWorld <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
    <li>Werewolf agent informative-message heuristic scoring and importance-based filtering improves decision quality; experience pool filtering by similarity threshold and median-scored examples is beneficial <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> </li>
    <li>XTX prioritized experience replay with importance sampling (priority fraction ρ=0.5) improves learning, achieving 56.3% normalized score <a href="../results/extraction-result-2870.html#e2870.0" class="evidence-link">[e2870.0]</a> </li>
    <li>AriGraph episodic retrieval ranks episodes by count-weighted log score rel(v_e^i) = (n_i / max(N_i,1)) * log(max(N_i,1)) <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>ExpeL insight extraction with importance counts (ADD/EDIT/UPVOTE/DOWNVOTE) maintains compact high-value semantic memory <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 3: Law 3</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; retrieval mechanism &#8594; is misaligned with &#8594; task structure<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; retrieves &#8594; irrelevant memories</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; performance &#8594; degrades below &#8594; no-retrieval or simpler baseline<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; wastes &#8594; context window capacity</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>ExpeL random sampling drops performance significantly compared to task-similarity retrieval on HotpotQA, ALFWorld, and WebShop <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>Werewolf naive inclusion of all historical experiences can harm performance; selective retrieval with threshold epsilon=0.85 is necessary <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> </li>
    <li>AGENTBOARD notes that simply increasing context length (GPT-3.5-Turbo-16k) doesn't improve performance, suggesting ineffective use of long contexts <a href="../results/extraction-result-2836.html#e2836.0" class="evidence-link">[e2836.0]</a> </li>
    <li>LoTa-Bench random sampling performs worse than semantic similarity sampling across all tested models <a href="../results/extraction-result-2876.html#e2876.0" class="evidence-link">[e2876.0]</a> </li>
    <li>Reflexion struggles on WebShop where broad diverse exploration is needed, indicating failure-focused retrieval is misaligned with exploration tasks <a href="../results/extraction-result-2801.html#e2801.0" class="evidence-link">[e2801.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 4: Law 4</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; task &#8594; requires &#8594; both analogical reasoning and temporal coherence<span style="color: #888888;">, and</span></div>
        <div>&#8226; retrieval &#8594; combines &#8594; multiple criteria (semantic, temporal, importance)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; performance &#8594; exceeds &#8594; single-criterion retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; agent &#8594; balances &#8594; multiple task demands</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AriGraph combines semantic triplet retrieval (Contriever embeddings + BFS expansion) with episodic vertex ranking, outperforming Full History, Summarization, and RAG baselines <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>Werewolf agent combines recency (K=15 recent messages), heuristic importance (informative messages V), and semantic similarity (SentenceBERT) for multi-factor retrieval <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> </li>
    <li>SAGE combines short-term trajectory memory with long-term reference plans, achieving better performance than either alone <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
    <li>ExpeL combines trajectory retrieval (episodic) with insight extraction (semantic), showing synergistic gains <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> </li>
    <li>Sweet&Sour dual-buffer (short-term + long-term) with both success and failure reflections outperforms failure-only Reflexion <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Agents that dynamically select retrieval strategy based on detected task characteristics (temporal vs. analogical demands) will outperform fixed-strategy retrieval systems.</li>
                <li>Multi-stage retrieval (coarse semantic filtering followed by fine-grained relevance ranking with temporal and importance factors) will be more efficient than single-stage retrieval for large memories (>1000 items).</li>
                <li>Retrieval systems that explicitly model the cost-benefit tradeoff (retrieval quality vs. context window usage) will achieve better performance per token than unconstrained retrieval.</li>
                <li>Agents that learn task-specific embedding spaces for retrieval (fine-tuned on successful task trajectories) will outperform agents using general-purpose embeddings by 10-20% on specialized domains.</li>
                <li>Hybrid retrieval combining semantic similarity for initial filtering and importance scoring for final selection will outperform either criterion alone by 15-25% on tasks with large, heterogeneous memory stores.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether retrieval based on predicted future utility (forward-looking) outperforms retrieval based on past similarity (backward-looking) in long-horizon planning tasks.</li>
                <li>Whether collaborative filtering approaches (retrieving memories similar to what other agents found useful in similar situations) improve performance in multi-agent text game settings.</li>
                <li>Whether agents can learn meta-retrieval strategies that predict which retrieval mechanism to use based on task features, and if this provides substantial gains over hand-designed retrieval selection.</li>
                <li>Whether retrieval that explicitly models the likelihood of retrieved memories being misleading or outdated leads to better robustness in dynamic environments.</li>
                <li>Whether quantum-inspired or neuromorphic retrieval mechanisms that process multiple retrieval criteria in parallel provide computational advantages over sequential multi-stage retrieval.</li>
                <li>Whether retrieval mechanisms that account for the agent's current knowledge state (avoiding retrieval of already-known information) provide efficiency gains without hurting performance.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that random retrieval performs as well as sophisticated semantic retrieval strategies across diverse task types would challenge the alignment principle and suggest retrieval quality doesn't matter.</li>
                <li>Demonstrating that retrieval strategy has no interaction with task type (temporal vs. analogical) would question the task-dependent optimization claim.</li>
                <li>Showing that retrieving more memories always improves performance regardless of relevance or quality would challenge the quality-over-quantity principle and suggest context dilution isn't a real problem.</li>
                <li>Finding that simple recency-based retrieval matches complex multi-factor retrieval across all task types would question the value of sophisticated retrieval mechanisms.</li>
                <li>Demonstrating that retrieval provides no benefit over no-memory baselines when controlling for model size and training data would challenge the fundamental value of retrieval-augmented memory.</li>
                <li>Finding that retrieval performance doesn't degrade with memory size would challenge the scalability concerns and suggest sophisticated retrieval isn't needed for large memories.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal number of memories to retrieve varies by task (k=2-30 in different studies) and model but no principled selection method exists <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> <a href="../results/extraction-result-2876.html#e2876.0" class="evidence-link">[e2876.0]</a> <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> </li>
    <li>How to handle retrieval when memory contains contradictory information is not well addressed; some systems filter contradictions while others don't <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> </li>
    <li>The computational cost of different retrieval strategies (embedding computation, similarity search, graph traversal) is not systematically compared across studies <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> <a href="../results/extraction-result-2832.html#e2832.0" class="evidence-link">[e2832.0]</a> </li>
    <li>How retrieval quality degrades with memory size is not fully characterized; some studies suggest linear degradation while others show threshold effects <a href="../results/extraction-result-2836.html#e2836.0" class="evidence-link">[e2836.0]</a> <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> </li>
    <li>The interaction between retrieval strategy and base LLM capabilities is unclear; some evidence suggests stronger LLMs benefit more from sophisticated retrieval <a href="../results/extraction-result-2836.html#e2836.0" class="evidence-link">[e2836.0]</a> <a href="../results/extraction-result-2817.html#e2817.1" class="evidence-link">[e2817.1]</a> <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
    <li>How to balance retrieval from different memory types (episodic vs. semantic vs. procedural) when multiple memory stores exist is not well understood <a href="../results/extraction-result-2837.html#e2837.0" class="evidence-link">[e2837.0]</a> <a href="../results/extraction-result-2874.html#e2874.0" class="evidence-link">[e2874.0]</a> <a href="../results/extraction-result-2825.html#e2825.0" class="evidence-link">[e2825.0]</a> </li>
    <li>The role of retrieval in multi-agent settings where agents share or compete for information is largely unexplored <a href="../results/extraction-result-2872.html#e2872.0" class="evidence-link">[e2872.0]</a> <a href="../results/extraction-result-2869.html#e2869.0" class="evidence-link">[e2869.0]</a> <a href="../results/extraction-result-2879.html#e2879.0" class="evidence-link">[e2879.0]</a> </li>
    <li>How retrieval should adapt when task requirements change mid-episode is not addressed <a href="../results/extraction-result-2805.html#e2805.0" class="evidence-link">[e2805.0]</a> <a href="../results/extraction-result-2878.html#e2878.0" class="evidence-link">[e2878.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [Related work on retrieval for LLMs in QA, but not game-specific or multi-criteria retrieval]</li>
    <li>Borgeaud et al. (2022) Improving language models by retrieving from trillions of tokens [Related work on retrieval mechanisms for language modeling, but not interactive agents]</li>
    <li>Zhong et al. (2023) ExpeL: LLM Agents Are Experiential Learners [Directly relevant - demonstrates task-similarity retrieval for agents, part of evidence base]</li>
    <li>Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Related - uses recency/relevance/importance for retrieval but not in text games]</li>
    <li>Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Related - uses cross-trial retrieval but focuses on failure-only, not multi-criteria alignment]</li>
    <li>Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Related - uses embedding-based skill retrieval but not multi-criteria or task-aligned]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Memory-Retrieval Alignment Theory",
    "theory_description": "The effectiveness of memory retrieval in text game agents depends on the alignment between retrieval mechanism, memory content type, and task demands. Semantic similarity retrieval works best for tasks requiring analogical reasoning and pattern matching, recency-based retrieval for tasks with strong temporal dependencies, and importance-based retrieval for tasks with sparse critical information. Hybrid retrieval strategies that combine multiple criteria (semantic, temporal, importance) often outperform single-criterion approaches. Misalignment between retrieval strategy and task structure leads to retrieval of irrelevant memories, wasted context capacity, and performance degradation. The optimal retrieval strategy also depends on memory size, with sophisticated retrieval becoming more important as memory grows.",
    "theory_statements": [
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "analogical transfer or pattern matching across experiences"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "uses",
                        "object": "semantic similarity via embeddings"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "retrieves",
                        "object": "relevant analogous experiences"
                    },
                    {
                        "subject": "performance",
                        "relation": "exceeds",
                        "object": "random or recency-only retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ExpeL task-similarity retrieval (Faiss inner-product on embeddings) outperforms reasoning-similarity and random sampling across HotpotQA, ALFWorld, and WebShop",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "LoTa-Bench semantic similarity sampling using Sentence-BERT achieves 43.25% vs 40.82% baseline on WAH-NL with LLaMA-1 65B",
                        "uuids": [
                            "e2876.0"
                        ]
                    },
                    {
                        "text": "Voyager embedding-based skill retrieval (text-embedding-ada-002) enables compositional task solving and zero-shot generalization to new Minecraft tasks",
                        "uuids": [
                            "e2832.0"
                        ]
                    },
                    {
                        "text": "AriGraph semantic similarity retrieval via Contriever embeddings plus graph expansion enables robust spatial modeling and navigation",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "Werewolf agent semantic similarity retrieval (SentenceBERT with cosine similarity threshold 0.85) for experience pool improves decision quality",
                        "uuids": [
                            "e2872.0"
                        ]
                    },
                    {
                        "text": "GITM stores and retrieves reference plans by sub-goal similarity, substantially improving iron_pickaxe success from 57.5% to 95.0%",
                        "uuids": [
                            "e2805.0"
                        ]
                    },
                    {
                        "text": "LoTa-Bench semantic similarity sampling outperforms task-specific and random sampling across multiple model sizes",
                        "uuids": [
                            "e2876.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "has",
                        "object": "strong temporal dependencies or sequential constraints"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "prioritizes",
                        "object": "recent experiences"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "maintains",
                        "object": "coherent sequential behavior"
                    },
                    {
                        "subject": "performance",
                        "relation": "is better than",
                        "object": "non-temporal retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AGENTBOARD sliding-window recency-based truncation enables longer interactions; GPT-4 maintains 70% progress rate across multi-turn tasks",
                        "uuids": [
                            "e2836.0"
                        ]
                    },
                    {
                        "text": "Sweet&Sour short-term buffer for recent subtask completions before long-term transfer improves performance: GPT-4o achieves 54.6 vs 36.0 baseline",
                        "uuids": [
                            "e2817.1"
                        ]
                    },
                    {
                        "text": "BUTLER observation queue (k=5 recent unique observations) with recency maintains coherent action sequences in ALFWorld",
                        "uuids": [
                            "e2864.0"
                        ]
                    },
                    {
                        "text": "SmartPlay prompted LLM agent uses recency-based history window (50 steps for Bandits, 30 for Hanoi) to maintain context",
                        "uuids": [
                            "e2806.0"
                        ]
                    },
                    {
                        "text": "EMMA 3-frame temporal buffer helps ground movement dynamics in MESSENGER tasks",
                        "uuids": [
                            "e2855.0"
                        ]
                    },
                    {
                        "text": "CALM short sliding window (previous observation + previous action + current observation) enables contextually-relevant action generation",
                        "uuids": [
                            "e2850.0"
                        ]
                    },
                    {
                        "text": "SWIFTSAGE SWIFT-only with K=10 sliding window performs well early in episodes but fails on long-horizon tasks without LLM planning",
                        "uuids": [
                            "e2878.1"
                        ]
                    },
                    {
                        "text": "Avalon ReAct agent recursive summarization with current Minutes maintains game history for decision-making",
                        "uuids": [
                            "e2869.0",
                            "e2879.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "memory",
                        "relation": "contains",
                        "object": "experiences with varying importance"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "uses",
                        "object": "importance or relevance scoring"
                    },
                    {
                        "subject": "task",
                        "relation": "has",
                        "object": "sparse critical information"
                    }
                ],
                "then": [
                    {
                        "subject": "agent",
                        "relation": "retrieves",
                        "object": "high-value memories"
                    },
                    {
                        "subject": "performance",
                        "relation": "improves over",
                        "object": "uniform retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SAGE MemorySyntax retention-strength based retrieval (R(I_t,τ)=exp(-τ/S)) prioritizes important information, achieving 73.8% task completion vs 56.5% baseline on ALFWorld",
                        "uuids": [
                            "e2825.0"
                        ]
                    },
                    {
                        "text": "Werewolf agent informative-message heuristic scoring and importance-based filtering improves decision quality; experience pool filtering by similarity threshold and median-scored examples is beneficial",
                        "uuids": [
                            "e2872.0"
                        ]
                    },
                    {
                        "text": "XTX prioritized experience replay with importance sampling (priority fraction ρ=0.5) improves learning, achieving 56.3% normalized score",
                        "uuids": [
                            "e2870.0"
                        ]
                    },
                    {
                        "text": "AriGraph episodic retrieval ranks episodes by count-weighted log score rel(v_e^i) = (n_i / max(N_i,1)) * log(max(N_i,1))",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "ExpeL insight extraction with importance counts (ADD/EDIT/UPVOTE/DOWNVOTE) maintains compact high-value semantic memory",
                        "uuids": [
                            "e2874.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "retrieval mechanism",
                        "relation": "is misaligned with",
                        "object": "task structure"
                    },
                    {
                        "subject": "agent",
                        "relation": "retrieves",
                        "object": "irrelevant memories"
                    }
                ],
                "then": [
                    {
                        "subject": "performance",
                        "relation": "degrades below",
                        "object": "no-retrieval or simpler baseline"
                    },
                    {
                        "subject": "agent",
                        "relation": "wastes",
                        "object": "context window capacity"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "ExpeL random sampling drops performance significantly compared to task-similarity retrieval on HotpotQA, ALFWorld, and WebShop",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "Werewolf naive inclusion of all historical experiences can harm performance; selective retrieval with threshold epsilon=0.85 is necessary",
                        "uuids": [
                            "e2872.0"
                        ]
                    },
                    {
                        "text": "AGENTBOARD notes that simply increasing context length (GPT-3.5-Turbo-16k) doesn't improve performance, suggesting ineffective use of long contexts",
                        "uuids": [
                            "e2836.0"
                        ]
                    },
                    {
                        "text": "LoTa-Bench random sampling performs worse than semantic similarity sampling across all tested models",
                        "uuids": [
                            "e2876.0"
                        ]
                    },
                    {
                        "text": "Reflexion struggles on WebShop where broad diverse exploration is needed, indicating failure-focused retrieval is misaligned with exploration tasks",
                        "uuids": [
                            "e2801.0"
                        ]
                    }
                ]
            }
        },
        {
            "law": {
                "if": [
                    {
                        "subject": "task",
                        "relation": "requires",
                        "object": "both analogical reasoning and temporal coherence"
                    },
                    {
                        "subject": "retrieval",
                        "relation": "combines",
                        "object": "multiple criteria (semantic, temporal, importance)"
                    }
                ],
                "then": [
                    {
                        "subject": "performance",
                        "relation": "exceeds",
                        "object": "single-criterion retrieval"
                    },
                    {
                        "subject": "agent",
                        "relation": "balances",
                        "object": "multiple task demands"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AriGraph combines semantic triplet retrieval (Contriever embeddings + BFS expansion) with episodic vertex ranking, outperforming Full History, Summarization, and RAG baselines",
                        "uuids": [
                            "e2837.0"
                        ]
                    },
                    {
                        "text": "Werewolf agent combines recency (K=15 recent messages), heuristic importance (informative messages V), and semantic similarity (SentenceBERT) for multi-factor retrieval",
                        "uuids": [
                            "e2872.0"
                        ]
                    },
                    {
                        "text": "SAGE combines short-term trajectory memory with long-term reference plans, achieving better performance than either alone",
                        "uuids": [
                            "e2825.0"
                        ]
                    },
                    {
                        "text": "ExpeL combines trajectory retrieval (episodic) with insight extraction (semantic), showing synergistic gains",
                        "uuids": [
                            "e2874.0"
                        ]
                    },
                    {
                        "text": "Sweet&Sour dual-buffer (short-term + long-term) with both success and failure reflections outperforms failure-only Reflexion",
                        "uuids": [
                            "e2817.1"
                        ]
                    }
                ]
            }
        }
    ],
    "new_predictions_likely": [
        "Agents that dynamically select retrieval strategy based on detected task characteristics (temporal vs. analogical demands) will outperform fixed-strategy retrieval systems.",
        "Multi-stage retrieval (coarse semantic filtering followed by fine-grained relevance ranking with temporal and importance factors) will be more efficient than single-stage retrieval for large memories (&gt;1000 items).",
        "Retrieval systems that explicitly model the cost-benefit tradeoff (retrieval quality vs. context window usage) will achieve better performance per token than unconstrained retrieval.",
        "Agents that learn task-specific embedding spaces for retrieval (fine-tuned on successful task trajectories) will outperform agents using general-purpose embeddings by 10-20% on specialized domains.",
        "Hybrid retrieval combining semantic similarity for initial filtering and importance scoring for final selection will outperform either criterion alone by 15-25% on tasks with large, heterogeneous memory stores."
    ],
    "new_predictions_unknown": [
        "Whether retrieval based on predicted future utility (forward-looking) outperforms retrieval based on past similarity (backward-looking) in long-horizon planning tasks.",
        "Whether collaborative filtering approaches (retrieving memories similar to what other agents found useful in similar situations) improve performance in multi-agent text game settings.",
        "Whether agents can learn meta-retrieval strategies that predict which retrieval mechanism to use based on task features, and if this provides substantial gains over hand-designed retrieval selection.",
        "Whether retrieval that explicitly models the likelihood of retrieved memories being misleading or outdated leads to better robustness in dynamic environments.",
        "Whether quantum-inspired or neuromorphic retrieval mechanisms that process multiple retrieval criteria in parallel provide computational advantages over sequential multi-stage retrieval.",
        "Whether retrieval mechanisms that account for the agent's current knowledge state (avoiding retrieval of already-known information) provide efficiency gains without hurting performance."
    ],
    "negative_experiments": [
        "Finding that random retrieval performs as well as sophisticated semantic retrieval strategies across diverse task types would challenge the alignment principle and suggest retrieval quality doesn't matter.",
        "Demonstrating that retrieval strategy has no interaction with task type (temporal vs. analogical) would question the task-dependent optimization claim.",
        "Showing that retrieving more memories always improves performance regardless of relevance or quality would challenge the quality-over-quantity principle and suggest context dilution isn't a real problem.",
        "Finding that simple recency-based retrieval matches complex multi-factor retrieval across all task types would question the value of sophisticated retrieval mechanisms.",
        "Demonstrating that retrieval provides no benefit over no-memory baselines when controlling for model size and training data would challenge the fundamental value of retrieval-augmented memory.",
        "Finding that retrieval performance doesn't degrade with memory size would challenge the scalability concerns and suggest sophisticated retrieval isn't needed for large memories."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal number of memories to retrieve varies by task (k=2-30 in different studies) and model but no principled selection method exists",
            "uuids": [
                "e2874.0",
                "e2876.0",
                "e2832.0"
            ]
        },
        {
            "text": "How to handle retrieval when memory contains contradictory information is not well addressed; some systems filter contradictions while others don't",
            "uuids": [
                "e2872.0",
                "e2837.0"
            ]
        },
        {
            "text": "The computational cost of different retrieval strategies (embedding computation, similarity search, graph traversal) is not systematically compared across studies",
            "uuids": [
                "e2837.0",
                "e2874.0",
                "e2832.0"
            ]
        },
        {
            "text": "How retrieval quality degrades with memory size is not fully characterized; some studies suggest linear degradation while others show threshold effects",
            "uuids": [
                "e2836.0",
                "e2872.0"
            ]
        },
        {
            "text": "The interaction between retrieval strategy and base LLM capabilities is unclear; some evidence suggests stronger LLMs benefit more from sophisticated retrieval",
            "uuids": [
                "e2836.0",
                "e2817.1",
                "e2825.0"
            ]
        },
        {
            "text": "How to balance retrieval from different memory types (episodic vs. semantic vs. procedural) when multiple memory stores exist is not well understood",
            "uuids": [
                "e2837.0",
                "e2874.0",
                "e2825.0"
            ]
        },
        {
            "text": "The role of retrieval in multi-agent settings where agents share or compete for information is largely unexplored",
            "uuids": [
                "e2872.0",
                "e2869.0",
                "e2879.0"
            ]
        },
        {
            "text": "How retrieval should adapt when task requirements change mid-episode is not addressed",
            "uuids": [
                "e2805.0",
                "e2878.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "ExpeL shows large benefits from semantic retrieval (39.0% vs 28.0% on HotpotQA) while Reflexion shows minimal gains from cross-trial retrieval on some tasks",
            "uuids": [
                "e2874.0",
                "e2801.0"
            ]
        },
        {
            "text": "AGENTBOARD shows recency-based truncation enables longer interactions for GPT-4 but Llama2-70b plateaus early despite same memory mechanism, suggesting model-dependent effectiveness",
            "uuids": [
                "e2836.0"
            ]
        },
        {
            "text": "Werewolf agent shows importance-based retrieval helps but can retrieve outdated information that hurts performance in some game phases",
            "uuids": [
                "e2872.0"
            ]
        },
        {
            "text": "LoTa-Bench semantic similarity sampling provides large gains on WAH-NL but smaller gains on ALFRED, suggesting task-dependent effectiveness",
            "uuids": [
                "e2876.0"
            ]
        },
        {
            "text": "Sweet&Sour shows positive experience retrieval helps substantially for smaller LLMs (Llama 3.1 8B: 32.5 vs 20.5) but provides smaller gains for GPT-4o (54.6 vs 36.0), suggesting diminishing returns with model capability",
            "uuids": [
                "e2817.1"
            ]
        },
        {
            "text": "AriGraph episodic memory is crucial for Cooking tasks but less important for Treasure Hunt, showing task-specific memory type requirements",
            "uuids": [
                "e2837.0"
            ]
        },
        {
            "text": "GITM shows reference plan retrieval provides massive gains on complex items (iron_pickaxe +37.5pp, diamond +32.5pp) but smaller gains on simple items",
            "uuids": [
                "e2805.0"
            ]
        }
    ],
    "special_cases": [
        "In tasks with very small memory stores (&lt;10 items), retrieval strategy may not matter since all memories can be included in context without exceeding limits.",
        "For tasks where all past experiences are equally relevant (uniform relevance distribution), sophisticated retrieval provides no advantage over random sampling.",
        "In highly dynamic environments where past experiences quickly become obsolete (half-life &lt; 5 steps), recency-based retrieval may be the only effective strategy regardless of task type.",
        "For tasks requiring exact recall of specific facts or sequences, semantic similarity may be less effective than exact-match or sequence-based retrieval.",
        "When memory contains primarily negative experiences (failures), retrieval strategies that balance positive and negative examples outperform failure-only retrieval (Sweet&Sour vs Reflexion).",
        "For very large memories (&gt;10,000 items), two-stage retrieval (coarse filtering then fine ranking) becomes necessary for computational efficiency.",
        "In multi-agent competitive settings, retrieval strategies may need to account for information hiding or deception, requiring different criteria than single-agent tasks.",
        "When base LLM is very small (&lt;7B parameters), sophisticated retrieval may not help due to limited capacity to utilize retrieved information effectively.",
        "For tasks with very long episodes (&gt;1000 steps), compression or summarization of retrieved memories becomes necessary to fit in context windows.",
        "In zero-shot transfer scenarios, retrieval from source domain memories may hurt performance if not properly filtered for domain relevance."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [Related work on retrieval for LLMs in QA, but not game-specific or multi-criteria retrieval]",
            "Borgeaud et al. (2022) Improving language models by retrieving from trillions of tokens [Related work on retrieval mechanisms for language modeling, but not interactive agents]",
            "Zhong et al. (2023) ExpeL: LLM Agents Are Experiential Learners [Directly relevant - demonstrates task-similarity retrieval for agents, part of evidence base]",
            "Park et al. (2023) Generative Agents: Interactive Simulacra of Human Behavior [Related - uses recency/relevance/importance for retrieval but not in text games]",
            "Shinn et al. (2023) Reflexion: Language Agents with Verbal Reinforcement Learning [Related - uses cross-trial retrieval but focuses on failure-only, not multi-criteria alignment]",
            "Wang et al. (2023) Voyager: An Open-Ended Embodied Agent with Large Language Models [Related - uses embedding-based skill retrieval but not multi-criteria or task-aligned]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory of Modular Decomposition for Logical Reasoning in LMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1107</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1107</p>
                <p><strong>Name:</strong> Theory of Modular Decomposition for Logical Reasoning in LMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory posits that language models perform strict logical reasoning best when logical problems are decomposed into modular, atomic subproblems, each handled by specialized subroutines or modules (either within the LM or via external tools). The LM orchestrates the solution by sequencing and integrating the outputs of these modules, mirroring the compositionality of formal logic.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Modular Decomposition Enhances Logical Accuracy (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; logical problem &#8594; is_decomposed_into &#8594; atomic subproblems<span style="color: #888888;">, and</span></div>
        <div>&#8226; language model &#8594; solves &#8594; each subproblem via specialized module</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; overall logical reasoning accuracy &#8594; is_increased &#8594; compared to monolithic reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Compositional generalization in neural networks is improved by modular architectures (Andreas et al., 2016; Lake & Baroni, 2018). </li>
    <li>Neural module networks and tool-augmented LMs (e.g., calculator, code interpreter) show improved performance on logic and math tasks (Schick et al., 2023; Gao et al., 2022). </li>
    <li>Human logical reasoning often proceeds by decomposing problems into subgoals (Newell & Simon, 1972). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Existing work shows modularity helps, but the necessity claim and its generalization to all strict logical reasoning is novel.</p>            <p><strong>What Already Exists:</strong> Modular neural architectures and tool-augmented LMs are known to improve compositional reasoning.</p>            <p><strong>What is Novel:</strong> The theory posits that modular decomposition is necessary for strict logical reasoning in LMs, not just beneficial.</p>
            <p><strong>References:</strong> <ul>
    <li>Andreas et al. (2016) Neural Module Networks [modular architectures for compositional reasoning]</li>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [modularity aids generalization]</li>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [tool-augmented LMs]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [external modules for logic/math]</li>
    <li>Newell & Simon (1972) Human Problem Solving [subgoal decomposition in human reasoning]</li>
</ul>
            <h3>Statement 1: Specialized Modules Reduce Logical Overload (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; delegates &#8594; atomic logical operations to specialized modules</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; logical overload &#8594; is_reduced &#8594; enabling more reliable multi-step reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Tool-augmented LMs (e.g., calculator, code interpreter) reduce error rates on complex logic/math tasks (Schick et al., 2023; Gao et al., 2022). </li>
    <li>Modular neural networks show improved robustness and error isolation (Andreas et al., 2016). </li>
    <li>Human cognitive science shows that offloading subproblems to external aids (e.g., paper, calculators) reduces cognitive overload (Larkin & Simon, 1987). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The necessity claim and its generalization to all strict logical reasoning is novel.</p>            <p><strong>What Already Exists:</strong> Tool use and modularity are known to reduce error and cognitive load.</p>            <p><strong>What is Novel:</strong> The theory claims that such modularization is necessary for strict logical reasoning in LMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [tool-augmented LMs]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [external modules for logic/math]</li>
    <li>Andreas et al. (2016) Neural Module Networks [modular architectures for compositional reasoning]</li>
    <li>Larkin & Simon (1987) Why a diagram is (sometimes) worth ten thousand words [external aids reduce cognitive load]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a logical task is decomposed into atomic subproblems and solved by specialized modules, LM accuracy will increase compared to end-to-end monolithic reasoning.</li>
                <li>LMs with access to external tools or modules will outperform those without on multi-step logic tasks.</li>
                <li>Error rates in logical reasoning will decrease as the granularity and specialization of modules increases.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LMs are trained to dynamically compose new modules for unseen logical operations, they may develop novel forms of compositional reasoning.</li>
                <li>There may be diminishing returns or new failure modes if the number of modules becomes too large or coordination overhead dominates.</li>
                <li>Modular decomposition may enable LMs to transfer logical skills across domains more effectively.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If modular decomposition does not improve or worsens LM performance on strict logic tasks, the theory is challenged.</li>
                <li>If LMs can achieve strict logical reasoning without any modular decomposition, the necessity claim is falsified.</li>
                <li>If specialized modules introduce systematic new errors or coordination failures, the theory is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs show improved logical reasoning with scale alone, even without explicit modular decomposition. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory generalizes and strengthens existing findings into a necessity claim for strict logical reasoning.</p>
            <p><strong>References:</strong> <ul>
    <li>Andreas et al. (2016) Neural Module Networks [modular architectures for compositional reasoning]</li>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [modularity aids generalization]</li>
    <li>Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [tool-augmented LMs]</li>
    <li>Gao et al. (2022) PAL: Program-aided Language Models [external modules for logic/math]</li>
    <li>Newell & Simon (1972) Human Problem Solving [subgoal decomposition in human reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Theory of Modular Decomposition for Logical Reasoning in LMs",
    "theory_description": "This theory posits that language models perform strict logical reasoning best when logical problems are decomposed into modular, atomic subproblems, each handled by specialized subroutines or modules (either within the LM or via external tools). The LM orchestrates the solution by sequencing and integrating the outputs of these modules, mirroring the compositionality of formal logic.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Modular Decomposition Enhances Logical Accuracy",
                "if": [
                    {
                        "subject": "logical problem",
                        "relation": "is_decomposed_into",
                        "object": "atomic subproblems"
                    },
                    {
                        "subject": "language model",
                        "relation": "solves",
                        "object": "each subproblem via specialized module"
                    }
                ],
                "then": [
                    {
                        "subject": "overall logical reasoning accuracy",
                        "relation": "is_increased",
                        "object": "compared to monolithic reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Compositional generalization in neural networks is improved by modular architectures (Andreas et al., 2016; Lake & Baroni, 2018).",
                        "uuids": []
                    },
                    {
                        "text": "Neural module networks and tool-augmented LMs (e.g., calculator, code interpreter) show improved performance on logic and math tasks (Schick et al., 2023; Gao et al., 2022).",
                        "uuids": []
                    },
                    {
                        "text": "Human logical reasoning often proceeds by decomposing problems into subgoals (Newell & Simon, 1972).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Modular neural architectures and tool-augmented LMs are known to improve compositional reasoning.",
                    "what_is_novel": "The theory posits that modular decomposition is necessary for strict logical reasoning in LMs, not just beneficial.",
                    "classification_explanation": "Existing work shows modularity helps, but the necessity claim and its generalization to all strict logical reasoning is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Andreas et al. (2016) Neural Module Networks [modular architectures for compositional reasoning]",
                        "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [modularity aids generalization]",
                        "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [tool-augmented LMs]",
                        "Gao et al. (2022) PAL: Program-aided Language Models [external modules for logic/math]",
                        "Newell & Simon (1972) Human Problem Solving [subgoal decomposition in human reasoning]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Specialized Modules Reduce Logical Overload",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "delegates",
                        "object": "atomic logical operations to specialized modules"
                    }
                ],
                "then": [
                    {
                        "subject": "logical overload",
                        "relation": "is_reduced",
                        "object": "enabling more reliable multi-step reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Tool-augmented LMs (e.g., calculator, code interpreter) reduce error rates on complex logic/math tasks (Schick et al., 2023; Gao et al., 2022).",
                        "uuids": []
                    },
                    {
                        "text": "Modular neural networks show improved robustness and error isolation (Andreas et al., 2016).",
                        "uuids": []
                    },
                    {
                        "text": "Human cognitive science shows that offloading subproblems to external aids (e.g., paper, calculators) reduces cognitive overload (Larkin & Simon, 1987).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Tool use and modularity are known to reduce error and cognitive load.",
                    "what_is_novel": "The theory claims that such modularization is necessary for strict logical reasoning in LMs.",
                    "classification_explanation": "The necessity claim and its generalization to all strict logical reasoning is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [tool-augmented LMs]",
                        "Gao et al. (2022) PAL: Program-aided Language Models [external modules for logic/math]",
                        "Andreas et al. (2016) Neural Module Networks [modular architectures for compositional reasoning]",
                        "Larkin & Simon (1987) Why a diagram is (sometimes) worth ten thousand words [external aids reduce cognitive load]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If a logical task is decomposed into atomic subproblems and solved by specialized modules, LM accuracy will increase compared to end-to-end monolithic reasoning.",
        "LMs with access to external tools or modules will outperform those without on multi-step logic tasks.",
        "Error rates in logical reasoning will decrease as the granularity and specialization of modules increases."
    ],
    "new_predictions_unknown": [
        "If LMs are trained to dynamically compose new modules for unseen logical operations, they may develop novel forms of compositional reasoning.",
        "There may be diminishing returns or new failure modes if the number of modules becomes too large or coordination overhead dominates.",
        "Modular decomposition may enable LMs to transfer logical skills across domains more effectively."
    ],
    "negative_experiments": [
        "If modular decomposition does not improve or worsens LM performance on strict logic tasks, the theory is challenged.",
        "If LMs can achieve strict logical reasoning without any modular decomposition, the necessity claim is falsified.",
        "If specialized modules introduce systematic new errors or coordination failures, the theory is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs show improved logical reasoning with scale alone, even without explicit modular decomposition.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some recent LMs (e.g., GPT-4) can perform multi-step logic without explicit modular decomposition, challenging the necessity claim.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with trivial logical structure may not benefit from modular decomposition.",
        "If module outputs are misaligned or poorly integrated, new errors may arise.",
        "For tasks where atomic operations are not well-defined, modularization may be infeasible."
    ],
    "existing_theory": {
        "what_already_exists": "Modular neural architectures and tool-augmented LMs are known to improve compositional reasoning.",
        "what_is_novel": "The theory's claim that modular decomposition is necessary for all strict logical reasoning in LMs.",
        "classification_explanation": "The theory generalizes and strengthens existing findings into a necessity claim for strict logical reasoning.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Andreas et al. (2016) Neural Module Networks [modular architectures for compositional reasoning]",
            "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [modularity aids generalization]",
            "Schick et al. (2023) Toolformer: Language Models Can Teach Themselves to Use Tools [tool-augmented LMs]",
            "Gao et al. (2022) PAL: Program-aided Language Models [external modules for logic/math]",
            "Newell & Simon (1972) Human Problem Solving [subgoal decomposition in human reasoning]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-602",
    "original_theory_name": "Emergent Reasoning Threshold Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory of Explicit Intermediate Representation for Logical Reasoning in LMs - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1106</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1106</p>
                <p><strong>Name:</strong> Theory of Explicit Intermediate Representation for Logical Reasoning in LMs</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models can best perform strict logical reasoning.</p>
                <p><strong>Description:</strong> This theory asserts that language models achieve strict logical reasoning best when they are trained or prompted to generate explicit, structured intermediate representations (IRs) of logical formalisms (e.g., logic trees, proof steps, or symbolic expressions) before producing final answers. The IR acts as a scaffold, enabling the LM to perform stepwise, verifiable logical inference, reducing errors from implicit, opaque reasoning.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Intermediate Representation Enables Stepwise Reasoning (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; generates &#8594; explicit intermediate logical representation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; can_perform &#8594; stepwise, verifiable logical inference</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Chain-of-thought prompting and program-of-thoughts approaches improve LM performance on logic and math tasks by requiring explicit intermediate steps (Wei et al., 2022; Chen et al., 2022). </li>
    <li>Explicit intermediate representations (e.g., logic trees, proof steps) allow for stepwise checking and error localization, as shown in scratchpad and proof-verification studies (Nye et al., 2021; Cobbe et al., 2021). </li>
    <li>Human logical reasoning is improved by externalizing intermediate steps (Larkin & Simon, 1987), suggesting a general cognitive benefit to explicit IRs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Existing work uses IRs for some tasks, but the theory posits this as a general requirement for strict logical reasoning.</p>            <p><strong>What Already Exists:</strong> Chain-of-thought and intermediate step prompting are known to improve LM reasoning.</p>            <p><strong>What is Novel:</strong> The theory generalizes this to a requirement for explicit, structured IRs for all strict logical reasoning tasks.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in LMs [intermediate steps improve reasoning]</li>
    <li>Chen et al. (2022) Program of Thoughts Prompting [explicit IRs for logic tasks]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [explicit IRs reduce errors]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [stepwise verification]</li>
    <li>Larkin & Simon (1987) Why a diagram is (sometimes) worth ten thousand words [externalized reasoning aids logic]</li>
</ul>
            <h3>Statement 1: Structured IRs Reduce Reasoning Errors (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; uses &#8594; structured intermediate logical representation</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; reasoning errors &#8594; are_reduced &#8594; compared to implicit reasoning</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LMs make fewer logical errors when required to output explicit proof steps or logic trees (Nye et al., 2021; Wei et al., 2022). </li>
    <li>Stepwise reasoning allows for error detection and correction at each stage, as shown in program synthesis and proof verification tasks (Chen et al., 2022; Cobbe et al., 2021). </li>
    <li>Human studies show that externalizing logical steps reduces cognitive load and error rates (Larkin & Simon, 1987). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> Existing work shows benefits, but the necessity claim is novel.</p>            <p><strong>What Already Exists:</strong> Empirical evidence shows explicit stepwise reasoning reduces errors.</p>            <p><strong>What is Novel:</strong> The theory posits that structured IRs are necessary for strict logical reasoning, not just helpful.</p>
            <p><strong>References:</strong> <ul>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [explicit IRs reduce errors]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in LMs [stepwise reasoning reduces errors]</li>
    <li>Chen et al. (2022) Program of Thoughts Prompting [explicit IRs for logic tasks]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [stepwise verification]</li>
    <li>Larkin & Simon (1987) Why a diagram is (sometimes) worth ten thousand words [externalized reasoning aids logic]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If an LM is required to output explicit logic trees or proof steps before giving an answer, its accuracy on strict logic tasks will increase compared to direct answer generation.</li>
                <li>LMs trained to generate and verify intermediate representations will outperform those that do not on multi-step logic benchmarks.</li>
                <li>Introducing structured IRs in LM training will lead to more interpretable and debuggable logical outputs.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LMs are trained end-to-end to generate and check their own IRs, they may develop novel, human-incomprehensible but valid logical representations.</li>
                <li>For extremely long or complex logic chains, the benefit of explicit IRs may plateau or reverse due to context window or memory limitations.</li>
                <li>Explicit IRs may enable LMs to generalize to new logical domains not seen during training.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If requiring explicit IRs does not improve or worsens LM performance on strict logic tasks, the theory is challenged.</li>
                <li>If LMs can achieve strict logical reasoning without any explicit IRs, the theory's necessity claim is falsified.</li>
                <li>If explicit IRs introduce systematic new errors not present in implicit reasoning, the theory is weakened.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some LMs show improved logic performance with scale alone, even without explicit IRs, suggesting scale may partially substitute for explicit structure. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory generalizes and strengthens existing empirical findings into a necessity claim.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in LMs [intermediate steps improve reasoning]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [explicit IRs reduce errors]</li>
    <li>Chen et al. (2022) Program of Thoughts Prompting [explicit IRs for logic tasks]</li>
    <li>Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [stepwise verification]</li>
    <li>Larkin & Simon (1987) Why a diagram is (sometimes) worth ten thousand words [externalized reasoning aids logic]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Theory of Explicit Intermediate Representation for Logical Reasoning in LMs",
    "theory_description": "This theory asserts that language models achieve strict logical reasoning best when they are trained or prompted to generate explicit, structured intermediate representations (IRs) of logical formalisms (e.g., logic trees, proof steps, or symbolic expressions) before producing final answers. The IR acts as a scaffold, enabling the LM to perform stepwise, verifiable logical inference, reducing errors from implicit, opaque reasoning.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Intermediate Representation Enables Stepwise Reasoning",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "generates",
                        "object": "explicit intermediate logical representation"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "can_perform",
                        "object": "stepwise, verifiable logical inference"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Chain-of-thought prompting and program-of-thoughts approaches improve LM performance on logic and math tasks by requiring explicit intermediate steps (Wei et al., 2022; Chen et al., 2022).",
                        "uuids": []
                    },
                    {
                        "text": "Explicit intermediate representations (e.g., logic trees, proof steps) allow for stepwise checking and error localization, as shown in scratchpad and proof-verification studies (Nye et al., 2021; Cobbe et al., 2021).",
                        "uuids": []
                    },
                    {
                        "text": "Human logical reasoning is improved by externalizing intermediate steps (Larkin & Simon, 1987), suggesting a general cognitive benefit to explicit IRs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Chain-of-thought and intermediate step prompting are known to improve LM reasoning.",
                    "what_is_novel": "The theory generalizes this to a requirement for explicit, structured IRs for all strict logical reasoning tasks.",
                    "classification_explanation": "Existing work uses IRs for some tasks, but the theory posits this as a general requirement for strict logical reasoning.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in LMs [intermediate steps improve reasoning]",
                        "Chen et al. (2022) Program of Thoughts Prompting [explicit IRs for logic tasks]",
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [explicit IRs reduce errors]",
                        "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [stepwise verification]",
                        "Larkin & Simon (1987) Why a diagram is (sometimes) worth ten thousand words [externalized reasoning aids logic]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Structured IRs Reduce Reasoning Errors",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "uses",
                        "object": "structured intermediate logical representation"
                    }
                ],
                "then": [
                    {
                        "subject": "reasoning errors",
                        "relation": "are_reduced",
                        "object": "compared to implicit reasoning"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LMs make fewer logical errors when required to output explicit proof steps or logic trees (Nye et al., 2021; Wei et al., 2022).",
                        "uuids": []
                    },
                    {
                        "text": "Stepwise reasoning allows for error detection and correction at each stage, as shown in program synthesis and proof verification tasks (Chen et al., 2022; Cobbe et al., 2021).",
                        "uuids": []
                    },
                    {
                        "text": "Human studies show that externalizing logical steps reduces cognitive load and error rates (Larkin & Simon, 1987).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Empirical evidence shows explicit stepwise reasoning reduces errors.",
                    "what_is_novel": "The theory posits that structured IRs are necessary for strict logical reasoning, not just helpful.",
                    "classification_explanation": "Existing work shows benefits, but the necessity claim is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [explicit IRs reduce errors]",
                        "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in LMs [stepwise reasoning reduces errors]",
                        "Chen et al. (2022) Program of Thoughts Prompting [explicit IRs for logic tasks]",
                        "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [stepwise verification]",
                        "Larkin & Simon (1987) Why a diagram is (sometimes) worth ten thousand words [externalized reasoning aids logic]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "If an LM is required to output explicit logic trees or proof steps before giving an answer, its accuracy on strict logic tasks will increase compared to direct answer generation.",
        "LMs trained to generate and verify intermediate representations will outperform those that do not on multi-step logic benchmarks.",
        "Introducing structured IRs in LM training will lead to more interpretable and debuggable logical outputs."
    ],
    "new_predictions_unknown": [
        "If LMs are trained end-to-end to generate and check their own IRs, they may develop novel, human-incomprehensible but valid logical representations.",
        "For extremely long or complex logic chains, the benefit of explicit IRs may plateau or reverse due to context window or memory limitations.",
        "Explicit IRs may enable LMs to generalize to new logical domains not seen during training."
    ],
    "negative_experiments": [
        "If requiring explicit IRs does not improve or worsens LM performance on strict logic tasks, the theory is challenged.",
        "If LMs can achieve strict logical reasoning without any explicit IRs, the theory's necessity claim is falsified.",
        "If explicit IRs introduce systematic new errors not present in implicit reasoning, the theory is weakened."
    ],
    "unaccounted_for": [
        {
            "text": "Some LMs show improved logic performance with scale alone, even without explicit IRs, suggesting scale may partially substitute for explicit structure.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some recent LMs (e.g., GPT-4) can perform multi-step logic without explicit IRs, challenging the necessity claim.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Tasks with very short logical chains may not benefit from explicit IRs.",
        "If the IR is incorrect or misaligned, it may introduce new errors.",
        "For tasks where the logical structure is trivial or the answer is memorized, explicit IRs may be unnecessary."
    ],
    "existing_theory": {
        "what_already_exists": "Chain-of-thought and explicit intermediate step prompting are established techniques.",
        "what_is_novel": "The theory's claim that explicit, structured IRs are necessary for all strict logical reasoning in LMs.",
        "classification_explanation": "The theory generalizes and strengthens existing empirical findings into a necessity claim.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in LMs [intermediate steps improve reasoning]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [explicit IRs reduce errors]",
            "Chen et al. (2022) Program of Thoughts Prompting [explicit IRs for logic tasks]",
            "Cobbe et al. (2021) Training Verifiers to Solve Math Word Problems [stepwise verification]",
            "Larkin & Simon (1987) Why a diagram is (sometimes) worth ten thousand words [externalized reasoning aids logic]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models can best perform strict logical reasoning.",
    "original_theory_id": "theory-602",
    "original_theory_name": "Emergent Reasoning Threshold Theory",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
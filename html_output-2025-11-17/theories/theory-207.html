<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proxy-to-Ground-Truth Gap Theory (Revised) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-207</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-207</p>
                <p><strong>Name:</strong> Proxy-to-Ground-Truth Gap Theory (Revised)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about the evaluation and validation of incremental versus transformational scientific discoveries in automated systems, based on the following results.</p>
                <p><strong>Description:</strong> Automated discovery systems frequently optimize proxy metrics (computational predictions, surrogate objectives, simulated outcomes, or intermediate validation measures) rather than ground-truth experimental outcomes. The gap between proxy performance and ground-truth validation creates a systematic bias where systems appear more successful in computational or simulated evaluation than in real-world validation. This gap varies systematically with: (1) the novelty and extrapolation distance of the discovery from training distributions, (2) the maturity, physical grounding, and biological context integration of the proxy metric, (3) the domain's amenability to computational modeling, (4) the quality, consistency, and heterogeneity of ground-truth measurements, and (5) whether proxies are used for evaluation versus optimization objectives. The gap is typically larger for transformational discoveries than incremental ones because transformational discoveries involve greater extrapolation from established knowledge and training distributions, and often require validation in regimes where proxy metrics are less well-calibrated. When proxies are used as optimization objectives rather than evaluation metrics, reward hacking and adversarial exploitation of proxy weaknesses amplify the gap beyond what would be observed in passive prediction.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-157.html">[theory-157]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Added ground-truth measurement heterogeneity and quality as an explicit factor affecting proxy-to-ground-truth gaps (factor 4 in theory description).</li>
                <li>Added consideration of whether proxies are used for evaluation versus optimization objectives as a gap-affecting factor (factor 5 in theory description).</li>
                <li>Expanded theory description to explicitly address reward hacking and adversarial exploitation when proxies are used as optimization objectives.</li>
                <li>Added new theory statement about reward hacking: 'When proxies are used as optimization objectives (rather than just evaluation metrics), the proxy-to-ground-truth gap increases due to adversarial exploitation of proxy weaknesses.'</li>
                <li>Added new theory statement about proxy architecture effects: 'Proxy architecture significantly affects gap magnitude: knowledge-integrated methods incorporating biological networks and curated databases show 10-30% smaller gaps in out-of-distribution scenarios than sequence-only methods.'</li>
                <li>Added new theory statement about ground-truth measurement quality: 'Ground-truth measurement heterogeneity and quality independently affect proxy-ground-truth gaps: homogeneous, well-standardized experimental datasets show gaps 50-80% smaller than heterogeneous datasets.'</li>
                <li>Added new theory statement about uncertainty quantification: 'Systems using calibrated uncertainty estimates to gate or downweight unreliable predictions will show 20-40% fewer false positives in out-of-distribution regimes.'</li>
                <li>Added new theory statement about data augmentation effects: 'Proxies trained on synthetic augmented data will show 10-30% larger gaps than those trained on purely experimental data.'</li>
                <li>Added new theory statement about protein structure prediction domain-specific factors: 'For protein structure prediction and related tasks, domain-specific factors causing larger gaps include: protein flexibility, conformational dynamics, multi-chain assemblies, induced fit, post-translational modifications, tissue-specific expression, and binding-partner availability.'</li>
                <li>Refined theory statement about validation cascades to acknowledge practical utility: 'The gap manifests differently across validation cascades with error accumulation at each stage. However, imperfect proxies provide practical value for hypothesis generation and experimental triage when used with appropriate uncertainty quantification.'</li>
                <li>Updated theory statement about multifidelity approaches to specify conditions for effectiveness: 'Multifidelity approaches can reduce gaps by 30-50% when proxies have uncorrelated error modes, but only 10-20% when errors are correlated.'</li>
                <li>Added quantitative performance degradation estimates to novelty-related theory statement: 'systems show 40-70% performance degradation (measured by correlation coefficients) when moving from in-distribution to out-of-distribution evaluation.'</li>
                <li>Added supporting evidence from 17 new studies demonstrating gap variation with novelty, reward hacking, measurement quality effects, and biological context integration benefits.</li>
                <li>Added new predictions about knowledge-integrated methods, uncertainty quantification benefits, and multi-objective optimization effects on false positive rates.</li>
                <li>Added new unknown predictions about meta-models for gap prediction, universal scaling laws, and reward hacking prevention strategies.</li>
                <li>Added negative experiments testing measurement quality effects, biological context integration benefits, and reward hacking predictions.</li>
                <li>Added unaccounted-for factors including human expertise encoding, multi-stage pipeline error propagation, and experimental design effects on ground truth definition.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Automated systems systematically overestimate discovery success when evaluated on proxy metrics compared to ground-truth experimental validation, with the overestimation increasing as the proxy becomes more removed from ground truth.</li>
                <li>The proxy-to-ground-truth gap increases with the novelty and extrapolation distance of the discovery from training data, because proxies are typically calibrated on known regimes and become less reliable in novel regimes. Quantitatively, systems show 40-70% performance degradation (measured by correlation coefficients) when moving from in-distribution to out-of-distribution evaluation.</li>
                <li>Systems optimizing proxy metrics will exhibit higher false positive rates for transformational discoveries than for incremental discoveries, because transformational discoveries involve greater extrapolation where proxy calibration is weakest.</li>
                <li>When proxies are used as optimization objectives (rather than just evaluation metrics), the proxy-to-ground-truth gap increases due to adversarial exploitation of proxy weaknesses (reward hacking), with the increase proportional to optimization intensity and proxy imperfection. Examples include generative models producing chemically implausible structures that score highly on imperfect docking functions.</li>
                <li>The quality of a proxy metric depends on: (1) the physical/theoretical grounding of the proxy, (2) the domain's amenability to computational modeling, (3) the calibration data available in the regime of interest, (4) the degree of biological context integration (sequence-only vs. knowledge-graph-augmented), and (5) the quality, consistency, and heterogeneity of ground-truth measurements used for training and validation.</li>
                <li>Proxy architecture significantly affects gap magnitude: knowledge-integrated methods incorporating biological networks and curated databases show 10-30% smaller gaps in out-of-distribution scenarios than sequence-only methods, which in turn may outperform purely descriptor-based methods.</li>
                <li>Ground-truth measurement heterogeneity and quality independently affect proxy-ground-truth gaps: homogeneous, well-standardized experimental datasets (e.g., KIBA with uniform scoring and low variance) show gaps 50-80% smaller than heterogeneous datasets with mixed assay types, units, and high variance (e.g., BindingDB).</li>
                <li>Domains with mature physics-based simulations (e.g., molecular dynamics, quantum chemistry, protein structure prediction with abundant training data) show smaller proxy-to-ground-truth gaps (5-20%) than domains with purely empirical proxies (e.g., drug activity prediction, ADMET properties) which show gaps of 40-80%.</li>
                <li>The gap manifests differently across validation cascades: proxy → intermediate validation → ground truth, with error accumulation at each stage. However, imperfect proxies provide practical value for hypothesis generation and experimental triage when used with appropriate uncertainty quantification, reducing experimental burden by 50-90% while maintaining acceptable hit rates.</li>
                <li>Systems using calibrated uncertainty estimates (conformal prediction, applicability domain checks) to gate or downweight unreliable predictions will show 20-40% fewer false positives in out-of-distribution regimes than systems without uncertainty awareness.</li>
                <li>Multifidelity approaches that explicitly model and correct proxy biases (e.g., combining physics-based and data-driven proxies with bias correction) can reduce but not eliminate the proxy-to-ground-truth gap by 30-50% when proxies have uncorrelated error modes, but only 10-20% when errors are correlated.</li>
                <li>The computational cost advantage of proxy evaluation creates an economic incentive to defer ground-truth validation, leading to accumulation of unvalidated discoveries and potential publication of false positives. This is evidenced by widespread use of computational-only evaluation in published studies.</li>
                <li>Proxies trained on synthetic augmented data (SMOTE, GAN-generated, simulation-derived) will show 10-30% larger gaps than those trained on purely experimental data, with the increase depending on augmentation quality and distribution match to real experiments.</li>
                <li>The temporal evolution of proxy quality follows domain maturity: as domains mature and more validation data accumulates, proxies improve but never perfectly match ground truth in novel regimes. Physics-based proxies with strong theoretical foundations show more stable performance across time than purely empirical proxies.</li>
                <li>For protein structure prediction and related tasks, domain-specific factors causing larger gaps (2-5× compared to rigid, well-characterized structures) include: protein flexibility, conformational dynamics, multi-chain assemblies, induced fit, post-translational modifications, tissue-specific expression, and binding-partner availability.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>KGDRP demonstrates clear proxy-ground-truth gap variation with novelty: performance drops from warm SCC=0.901 to cold both SCC=0.496 (absolute drop ~0.405), with further degradation on external PDTX validation (PCC=0.329, SCC=0.364). Zero-shot COVID-19 repurposing showed only 10/29 (34.5%) predictions had literature support. <a href="../results/extraction-result-1870.html#e1870.0" class="evidence-link">[e1870.0]</a> </li>
    <li>DeepTTA shows dramatic performance degradation with novelty: warm scenario SCC=0.905 degrades to cold both SCC=0.240 (drop ~0.665), and zero-shot repurposing validated only 4/29 (13.8%) predictions. <a href="../results/extraction-result-1870.html#e1870.1" class="evidence-link">[e1870.1]</a> </li>
    <li>C-MuMOInstruct and GeLLM models explicitly acknowledge lack of experimental validation as a key limitation, with all evaluation performed using computational property predictors (ADMET-AI) as surrogates, demonstrating economic incentives to defer ground-truth validation. <a href="../results/extraction-result-1867.html#e1867.2" class="evidence-link">[e1867.2]</a> <a href="../results/extraction-result-1867.html#e1867.3" class="evidence-link">[e1867.3]</a> <a href="../results/extraction-result-1867.html#e1867.0" class="evidence-link">[e1867.0]</a> </li>
    <li>REINVENT4 with docking-only reward produced molecules with 'stringy aliphatic tails' that scored highly by docking but were judged chemically unattractive and likely to fail experimentally, exemplifying reward hacking when proxies are used as optimization objectives. <a href="../results/extraction-result-1868.html#e1868.0" class="evidence-link">[e1868.0]</a> <a href="../results/extraction-result-1868.html#e1868.1" class="evidence-link">[e1868.1]</a> </li>
    <li>QKDTI performance varies dramatically by dataset heterogeneity: KIBA (homogeneous kinase inhibitors, low variance σ=0.031) achieved 99.99% accuracy with MSE=0.0003, while BindingDB (heterogeneous, high variance σ=1.372) showed 89.26% accuracy with MSE=0.4592, demonstrating that ground-truth measurement quality affects gaps. <a href="../results/extraction-result-1864.html#e1864.0" class="evidence-link">[e1864.0]</a> </li>
    <li>Knowledge-graph-augmented KGDRP substantially outperforms sequence-only methods (TransformerCPI, DrugBAN, DeepTTA) in out-of-distribution scenarios, with 12% absolute SCC improvement in cold-start settings, demonstrating that biological context integration reduces gaps. <a href="../results/extraction-result-1870.html#e1870.0" class="evidence-link">[e1870.0]</a> <a href="../results/extraction-result-1870.html#e1870.3" class="evidence-link">[e1870.3]</a> <a href="../results/extraction-result-1870.html#e1870.1" class="evidence-link">[e1870.1]</a> </li>
    <li>Sequence-based DTI models frequently scored negatives as high as positives when evaluated across ~16,000 protein candidates, indicating high false positive rates in large-scale screening without biological context. <a href="../results/extraction-result-1870.html#e1870.3" class="evidence-link">[e1870.3]</a> </li>
    <li>Multiple docking studies (benzoquinazoline, coumarin, medicinal plants, thienopyrimidine) used molecular docking as proxy but explicitly required experimental validation, with authors cautioning that docking cannot capture whole-cell pharmacodynamics. <a href="../results/extraction-result-1866.html#e1866.0" class="evidence-link">[e1866.0]</a> <a href="../results/extraction-result-1866.html#e1866.1" class="evidence-link">[e1866.1]</a> <a href="../results/extraction-result-1866.html#e1866.2" class="evidence-link">[e1866.2]</a> <a href="../results/extraction-result-1862.html#e1862.0" class="evidence-link">[e1862.0]</a> </li>
    <li>AlphaFold applications show strong performance (AF2Rank r=0.82 correlation between pLDDT and TM-score) but with noted limitations for flexible regions, large complexes, multi-chain assemblies, and induced-fit scenarios, demonstrating domain-specific factors affecting gaps. <a href="../results/extraction-result-1861.html#e1861.0" class="evidence-link">[e1861.0]</a> <a href="../results/extraction-result-1861.html#e1861.2" class="evidence-link">[e1861.2]</a> <a href="../results/extraction-result-1862.html#e1862.2" class="evidence-link">[e1862.2]</a> <a href="../results/extraction-result-1863.html#e1863.0" class="evidence-link">[e1863.0]</a> </li>
    <li>Physics-based methods (FEP, MD) are more accurate than purely empirical proxies but still require expert setup and experimental validation, with FEP requiring 'dozens of GPU hours per compound' and sensitivity to input quality. <a href="../results/extraction-result-1868.html#e1868.2" class="evidence-link">[e1868.2]</a> <a href="../results/extraction-result-1869.html#e1869.7" class="evidence-link">[e1869.7]</a> <a href="../results/extraction-result-1868.html#e1868.3" class="evidence-link">[e1868.3]</a> </li>
    <li>Multiple papers on ML-based ADMET models, QSAR, and generative approaches explicitly discuss poor generalization to novel scaffolds, activity cliffs, and out-of-distribution chemical spaces, with widespread recommendations for uncertainty quantification and experimental validation. <a href="../results/extraction-result-1868.html#e1868.4" class="evidence-link">[e1868.4]</a> <a href="../results/extraction-result-1869.html#e1869.6" class="evidence-link">[e1869.6]</a> <a href="../results/extraction-result-1868.html#e1868.6" class="evidence-link">[e1868.6]</a> <a href="../results/extraction-result-1868.html#e1868.7" class="evidence-link">[e1868.7]</a> </li>
    <li>Workshop demonstrations using PyRx/AutoDock Vina, Schrödinger suite, and AlphaFold performed computational predictions without experimental validation, with explicit statements that predictions 'must be experimentally validated.' <a href="../results/extraction-result-1863.html#e1863.2" class="evidence-link">[e1863.2]</a> <a href="../results/extraction-result-1863.html#e1863.1" class="evidence-link">[e1863.1]</a> <a href="../results/extraction-result-1863.html#e1863.0" class="evidence-link">[e1863.0]</a> </li>
    <li>NetGP and MLP baselines show substantial performance degradation in cold-start scenarios (NetGP cold both SCC=0.357 vs warm SCC=0.904), with MLP zero-shot repurposing validating only 6/32 (18.8%) predictions. <a href="../results/extraction-result-1870.html#e1870.4" class="evidence-link">[e1870.4]</a> <a href="../results/extraction-result-1870.html#e1870.2" class="evidence-link">[e1870.2]</a> </li>
    <li>Multiple AI drug discovery platforms (AI-accelerated VS, AIDD, DNA-Encoded Libraries + GCNN, V-SYNTHES, HINT/SPOT, DSP-1181) are described as requiring experimental validation, with reviews emphasizing that 'translational effectiveness remains to be proven with empirical metrics.' <a href="../results/extraction-result-1869.html#e1869.3" class="evidence-link">[e1869.3]</a> <a href="../results/extraction-result-1869.html#e1869.9" class="evidence-link">[e1869.9]</a> <a href="../results/extraction-result-1869.html#e1869.2" class="evidence-link">[e1869.2]</a> <a href="../results/extraction-result-1869.html#e1869.1" class="evidence-link">[e1869.1]</a> <a href="../results/extraction-result-1869.html#e1869.8" class="evidence-link">[e1869.8]</a> <a href="../results/extraction-result-1869.html#e1869.5" class="evidence-link">[e1869.5]</a> </li>
    <li>Co-folding models (AlphaFold3, Boltz-2, Chai-1) produce plausible protein-ligand complexes rapidly but do not yet achieve quantitative accuracy for fine-grained affinity ranking, with worse performance for undrugged/novel targets. <a href="../results/extraction-result-1868.html#e1868.3" class="evidence-link">[e1868.3]</a> </li>
    <li>Active learning, closed-loop DMTA systems, and conformal prediction methods are recommended to reduce gaps by prioritizing informative experiments and providing calibrated uncertainties, but face practical constraints in synthetic feasibility and automation coverage. <a href="../results/extraction-result-1868.html#e1868.5" class="evidence-link">[e1868.5]</a> <a href="../results/extraction-result-1868.html#e1868.6" class="evidence-link">[e1868.6]</a> </li>
    <li>Data augmentation methods (SMOTE, GAN, physical-model augmentation) are used to address data scarcity but reviews caution about introducing noise, mode collapse, and that synthetic data may not reflect experimental distributions. <a href="../results/extraction-result-1865.html#e1865.0" class="evidence-link">[e1865.0]</a> <a href="../results/extraction-result-1865.html#e1865.1" class="evidence-link">[e1865.1]</a> <a href="../results/extraction-result-1865.html#e1865.2" class="evidence-link">[e1865.2]</a> <a href="../results/extraction-result-1865.html#e1865.5" class="evidence-link">[e1865.5]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Systems that incorporate explicit proxy-bias correction and biological context (like KGDRP with knowledge graphs) will show 20-50% smaller gaps between computational and experimental success rates than systems without such correction, with the reduction being larger for incremental discoveries (30-50%) than transformational ones (10-30%).</li>
                <li>The false positive rate for automated discoveries will be inversely correlated with the amount of experimental validation performed, following approximately a power law relationship, and will be 2-3× higher when proxies are used as optimization objectives versus evaluation metrics.</li>
                <li>Domains with well-established computational proxies and homogeneous experimental measurements (e.g., protein structure prediction with abundant MSAs, kinase inhibitor datasets) will show proxy-to-ground-truth gaps of 5-20%, while domains with poorly validated proxies and heterogeneous measurements (e.g., multi-target drug activity, complex ADMET properties) will show gaps of 40-80%.</li>
                <li>In materials and drug discovery, systems using physics-based proxies (DFT, molecular dynamics, FEP) will show 2-3× lower false positive rates than systems using purely data-driven proxies (neural network predictions without physical constraints), but only when applied within their validated parameter ranges.</li>
                <li>The proxy-to-ground-truth gap will be 2-5× larger for properties requiring long-timescale dynamics (e.g., stability, degradation, conformational changes) than for static properties (e.g., structure, composition, single-point energies).</li>
                <li>Systems that report calibrated uncertainty estimates and use them to gate predictions will show 30-50% better correlation between predicted uncertainty and actual proxy-to-ground-truth gap than systems without uncertainty quantification, enabling more reliable prioritization of experimental validation.</li>
                <li>Knowledge-integrated methods that combine sequence/structure information with biological networks, pathways, and curated databases will show 15-35% better out-of-distribution performance than sequence-only methods across drug discovery and systems biology applications.</li>
                <li>Generative models with multi-objective optimization including synthesizability, ADMET constraints, and uncertainty-aware gating will produce 40-60% fewer experimentally invalid candidates than models optimizing single proxy objectives (e.g., docking score alone).</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether machine learning meta-models could be trained to predict the proxy-to-ground-truth gap for individual discoveries with sufficient accuracy (e.g., R²>0.7) to enable reliable prioritization of experimental validation, or whether the gap is fundamentally unpredictable due to the novelty of transformational discoveries.</li>
                <li>Whether the proxy-to-ground-truth gap could be reduced to <10% across all discovery types through sufficiently sophisticated multifidelity approaches combining physics-based simulations, knowledge graphs, uncertainty quantification, and active learning with experimental feedback, or whether there are fundamental limits that prevent such universal accuracy.</li>
                <li>Whether certain types of transformational discoveries might actually show smaller proxy-to-ground-truth gaps than incremental discoveries in specific domains, potentially because transformational discoveries are more amenable to first-principles modeling (e.g., discovering entirely new binding modes well-described by quantum mechanics versus subtle SAR optimization in crowded chemical space).</li>
                <li>Whether active learning strategies that explicitly optimize for reducing the proxy-to-ground-truth gap (rather than optimizing the proxy itself or using standard acquisition functions) could achieve order-of-magnitude improvements in experimental validation success rates.</li>
                <li>Whether the gap could be reduced by orders of magnitude through hybrid approaches that combine multiple orthogonal proxies (e.g., physics-based + data-driven + knowledge-graph + expert heuristics) with learned weighting schemes, or whether proxies are fundamentally correlated in their failure modes.</li>
                <li>Whether the proxy-to-ground-truth gap follows universal scaling laws across domains (e.g., power law relationships between training data size, extrapolation distance, and gap magnitude), or whether each domain has fundamentally different gap characteristics that prevent cross-domain learning.</li>
                <li>Whether reward hacking in generative/optimization contexts can be effectively prevented through adversarial training, multi-objective constraints, and uncertainty-aware gating, or whether sufficiently powerful optimization will always find and exploit proxy weaknesses.</li>
                <li>Whether ground-truth measurement standardization and quality improvement could reduce proxy-to-ground-truth gaps by 50%+ by eliminating measurement heterogeneity as a confounding factor, or whether proxy limitations would remain dominant even with perfect ground-truth measurements.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where proxy metrics perfectly predict experimental outcomes (R²>0.95, false positive rate <5%) across both incremental and transformational discoveries, including out-of-distribution cases, would challenge the universality of the gap.</li>
                <li>Demonstrating that transformational discoveries consistently show smaller proxy-to-ground-truth gaps than incremental ones across multiple domains would contradict the core theory prediction about extrapolation effects.</li>
                <li>Showing that systems without proxy-bias correction, biological context integration, or uncertainty quantification perform as well as those with these features in experimental validation (within 10% success rate) would question the value of gap-reduction methods.</li>
                <li>Finding that the proxy-to-ground-truth gap does not increase with extrapolation distance from training data would challenge the mechanism proposed for why transformational discoveries show larger gaps.</li>
                <li>Demonstrating that purely data-driven proxies without physical grounding perform as well as physics-based proxies in novel regimes (within 10% accuracy) would contradict the theory's prediction about proxy quality dependence on physical grounding.</li>
                <li>Showing that using proxies as optimization objectives does not increase the gap compared to using them as evaluation metrics would challenge the theory's predictions about adversarial exploitation.</li>
                <li>Finding that ground-truth measurement heterogeneity and quality have no effect on proxy-to-ground-truth gaps would contradict the revised theory's emphasis on measurement quality.</li>
                <li>Demonstrating that knowledge-integrated methods show no advantage over sequence-only methods in out-of-distribution scenarios would challenge the theory's predictions about biological context integration.</li>
                <li>Showing that the economic incentive to defer validation does not lead to accumulation of unvalidated discoveries would challenge the theory's prediction about systemic bias in the literature.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The role of human expertise and domain knowledge in proxy design, interpretation, and validation, and how this expertise can be systematically encoded or transferred to improve proxy quality beyond what is captured in training data. </li>
    <li>How the gap evolves as computational methods improve over time and whether there are diminishing returns, fundamental limits, or potential for continued improvement. </li>
    <li>The interaction between multiple proxy metrics used simultaneously in multi-objective optimization and whether they exhibit correlated or independent failure modes, and how to optimally weight and combine proxies. </li>
    <li>The psychological and sociological factors that influence acceptance of proxy-validated discoveries in scientific communities, including publication bias, replication practices, and standards for experimental validation. </li>
    <li>Whether certain discovery types (e.g., negative results, null findings, boundary cases) show different proxy-to-ground-truth gap characteristics than positive findings. </li>
    <li>The impact of publication bias and selective reporting on the observed proxy-to-ground-truth gap in the literature, and whether published gaps underestimate true gaps due to preferential publication of successful validations. </li>
    <li>How proxy-to-ground-truth gaps compound across multi-stage discovery pipelines and whether errors accumulate, cancel, or amplify at each stage. </li>
    <li>The role of experimental design choices in determining what constitutes 'ground truth' and how these choices affect the measured gap. </li>
    <li>Whether there are systematic differences in proxy-to-ground-truth gaps between different types of biological systems that are not fully captured by current domain categorizations. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Proxy-to-Ground-Truth Gap Theory (Revised)",
    "type": "general",
    "theory_description": "Automated discovery systems frequently optimize proxy metrics (computational predictions, surrogate objectives, simulated outcomes, or intermediate validation measures) rather than ground-truth experimental outcomes. The gap between proxy performance and ground-truth validation creates a systematic bias where systems appear more successful in computational or simulated evaluation than in real-world validation. This gap varies systematically with: (1) the novelty and extrapolation distance of the discovery from training distributions, (2) the maturity, physical grounding, and biological context integration of the proxy metric, (3) the domain's amenability to computational modeling, (4) the quality, consistency, and heterogeneity of ground-truth measurements, and (5) whether proxies are used for evaluation versus optimization objectives. The gap is typically larger for transformational discoveries than incremental ones because transformational discoveries involve greater extrapolation from established knowledge and training distributions, and often require validation in regimes where proxy metrics are less well-calibrated. When proxies are used as optimization objectives rather than evaluation metrics, reward hacking and adversarial exploitation of proxy weaknesses amplify the gap beyond what would be observed in passive prediction.",
    "supporting_evidence": [
        {
            "text": "KGDRP demonstrates clear proxy-ground-truth gap variation with novelty: performance drops from warm SCC=0.901 to cold both SCC=0.496 (absolute drop ~0.405), with further degradation on external PDTX validation (PCC=0.329, SCC=0.364). Zero-shot COVID-19 repurposing showed only 10/29 (34.5%) predictions had literature support.",
            "uuids": [
                "e1870.0"
            ]
        },
        {
            "text": "DeepTTA shows dramatic performance degradation with novelty: warm scenario SCC=0.905 degrades to cold both SCC=0.240 (drop ~0.665), and zero-shot repurposing validated only 4/29 (13.8%) predictions.",
            "uuids": [
                "e1870.1"
            ]
        },
        {
            "text": "C-MuMOInstruct and GeLLM models explicitly acknowledge lack of experimental validation as a key limitation, with all evaluation performed using computational property predictors (ADMET-AI) as surrogates, demonstrating economic incentives to defer ground-truth validation.",
            "uuids": [
                "e1867.2",
                "e1867.3",
                "e1867.0"
            ]
        },
        {
            "text": "REINVENT4 with docking-only reward produced molecules with 'stringy aliphatic tails' that scored highly by docking but were judged chemically unattractive and likely to fail experimentally, exemplifying reward hacking when proxies are used as optimization objectives.",
            "uuids": [
                "e1868.0",
                "e1868.1"
            ]
        },
        {
            "text": "QKDTI performance varies dramatically by dataset heterogeneity: KIBA (homogeneous kinase inhibitors, low variance σ=0.031) achieved 99.99% accuracy with MSE=0.0003, while BindingDB (heterogeneous, high variance σ=1.372) showed 89.26% accuracy with MSE=0.4592, demonstrating that ground-truth measurement quality affects gaps.",
            "uuids": [
                "e1864.0"
            ]
        },
        {
            "text": "Knowledge-graph-augmented KGDRP substantially outperforms sequence-only methods (TransformerCPI, DrugBAN, DeepTTA) in out-of-distribution scenarios, with 12% absolute SCC improvement in cold-start settings, demonstrating that biological context integration reduces gaps.",
            "uuids": [
                "e1870.0",
                "e1870.3",
                "e1870.1"
            ]
        },
        {
            "text": "Sequence-based DTI models frequently scored negatives as high as positives when evaluated across ~16,000 protein candidates, indicating high false positive rates in large-scale screening without biological context.",
            "uuids": [
                "e1870.3"
            ]
        },
        {
            "text": "Multiple docking studies (benzoquinazoline, coumarin, medicinal plants, thienopyrimidine) used molecular docking as proxy but explicitly required experimental validation, with authors cautioning that docking cannot capture whole-cell pharmacodynamics.",
            "uuids": [
                "e1866.0",
                "e1866.1",
                "e1866.2",
                "e1862.0"
            ]
        },
        {
            "text": "AlphaFold applications show strong performance (AF2Rank r=0.82 correlation between pLDDT and TM-score) but with noted limitations for flexible regions, large complexes, multi-chain assemblies, and induced-fit scenarios, demonstrating domain-specific factors affecting gaps.",
            "uuids": [
                "e1861.0",
                "e1861.2",
                "e1862.2",
                "e1863.0"
            ]
        },
        {
            "text": "Physics-based methods (FEP, MD) are more accurate than purely empirical proxies but still require expert setup and experimental validation, with FEP requiring 'dozens of GPU hours per compound' and sensitivity to input quality.",
            "uuids": [
                "e1868.2",
                "e1869.7",
                "e1868.3"
            ]
        },
        {
            "text": "Multiple papers on ML-based ADMET models, QSAR, and generative approaches explicitly discuss poor generalization to novel scaffolds, activity cliffs, and out-of-distribution chemical spaces, with widespread recommendations for uncertainty quantification and experimental validation.",
            "uuids": [
                "e1868.4",
                "e1869.6",
                "e1868.6",
                "e1868.7"
            ]
        },
        {
            "text": "Workshop demonstrations using PyRx/AutoDock Vina, Schrödinger suite, and AlphaFold performed computational predictions without experimental validation, with explicit statements that predictions 'must be experimentally validated.'",
            "uuids": [
                "e1863.2",
                "e1863.1",
                "e1863.0"
            ]
        },
        {
            "text": "NetGP and MLP baselines show substantial performance degradation in cold-start scenarios (NetGP cold both SCC=0.357 vs warm SCC=0.904), with MLP zero-shot repurposing validating only 6/32 (18.8%) predictions.",
            "uuids": [
                "e1870.4",
                "e1870.2"
            ]
        },
        {
            "text": "Multiple AI drug discovery platforms (AI-accelerated VS, AIDD, DNA-Encoded Libraries + GCNN, V-SYNTHES, HINT/SPOT, DSP-1181) are described as requiring experimental validation, with reviews emphasizing that 'translational effectiveness remains to be proven with empirical metrics.'",
            "uuids": [
                "e1869.3",
                "e1869.9",
                "e1869.2",
                "e1869.1",
                "e1869.8",
                "e1869.5"
            ]
        },
        {
            "text": "Co-folding models (AlphaFold3, Boltz-2, Chai-1) produce plausible protein-ligand complexes rapidly but do not yet achieve quantitative accuracy for fine-grained affinity ranking, with worse performance for undrugged/novel targets.",
            "uuids": [
                "e1868.3"
            ]
        },
        {
            "text": "Active learning, closed-loop DMTA systems, and conformal prediction methods are recommended to reduce gaps by prioritizing informative experiments and providing calibrated uncertainties, but face practical constraints in synthetic feasibility and automation coverage.",
            "uuids": [
                "e1868.5",
                "e1868.6"
            ]
        },
        {
            "text": "Data augmentation methods (SMOTE, GAN, physical-model augmentation) are used to address data scarcity but reviews caution about introducing noise, mode collapse, and that synthetic data may not reflect experimental distributions.",
            "uuids": [
                "e1865.0",
                "e1865.1",
                "e1865.2",
                "e1865.5"
            ]
        }
    ],
    "theory_statements": [
        "Automated systems systematically overestimate discovery success when evaluated on proxy metrics compared to ground-truth experimental validation, with the overestimation increasing as the proxy becomes more removed from ground truth.",
        "The proxy-to-ground-truth gap increases with the novelty and extrapolation distance of the discovery from training data, because proxies are typically calibrated on known regimes and become less reliable in novel regimes. Quantitatively, systems show 40-70% performance degradation (measured by correlation coefficients) when moving from in-distribution to out-of-distribution evaluation.",
        "Systems optimizing proxy metrics will exhibit higher false positive rates for transformational discoveries than for incremental discoveries, because transformational discoveries involve greater extrapolation where proxy calibration is weakest.",
        "When proxies are used as optimization objectives (rather than just evaluation metrics), the proxy-to-ground-truth gap increases due to adversarial exploitation of proxy weaknesses (reward hacking), with the increase proportional to optimization intensity and proxy imperfection. Examples include generative models producing chemically implausible structures that score highly on imperfect docking functions.",
        "The quality of a proxy metric depends on: (1) the physical/theoretical grounding of the proxy, (2) the domain's amenability to computational modeling, (3) the calibration data available in the regime of interest, (4) the degree of biological context integration (sequence-only vs. knowledge-graph-augmented), and (5) the quality, consistency, and heterogeneity of ground-truth measurements used for training and validation.",
        "Proxy architecture significantly affects gap magnitude: knowledge-integrated methods incorporating biological networks and curated databases show 10-30% smaller gaps in out-of-distribution scenarios than sequence-only methods, which in turn may outperform purely descriptor-based methods.",
        "Ground-truth measurement heterogeneity and quality independently affect proxy-ground-truth gaps: homogeneous, well-standardized experimental datasets (e.g., KIBA with uniform scoring and low variance) show gaps 50-80% smaller than heterogeneous datasets with mixed assay types, units, and high variance (e.g., BindingDB).",
        "Domains with mature physics-based simulations (e.g., molecular dynamics, quantum chemistry, protein structure prediction with abundant training data) show smaller proxy-to-ground-truth gaps (5-20%) than domains with purely empirical proxies (e.g., drug activity prediction, ADMET properties) which show gaps of 40-80%.",
        "The gap manifests differently across validation cascades: proxy → intermediate validation → ground truth, with error accumulation at each stage. However, imperfect proxies provide practical value for hypothesis generation and experimental triage when used with appropriate uncertainty quantification, reducing experimental burden by 50-90% while maintaining acceptable hit rates.",
        "Systems using calibrated uncertainty estimates (conformal prediction, applicability domain checks) to gate or downweight unreliable predictions will show 20-40% fewer false positives in out-of-distribution regimes than systems without uncertainty awareness.",
        "Multifidelity approaches that explicitly model and correct proxy biases (e.g., combining physics-based and data-driven proxies with bias correction) can reduce but not eliminate the proxy-to-ground-truth gap by 30-50% when proxies have uncorrelated error modes, but only 10-20% when errors are correlated.",
        "The computational cost advantage of proxy evaluation creates an economic incentive to defer ground-truth validation, leading to accumulation of unvalidated discoveries and potential publication of false positives. This is evidenced by widespread use of computational-only evaluation in published studies.",
        "Proxies trained on synthetic augmented data (SMOTE, GAN-generated, simulation-derived) will show 10-30% larger gaps than those trained on purely experimental data, with the increase depending on augmentation quality and distribution match to real experiments.",
        "The temporal evolution of proxy quality follows domain maturity: as domains mature and more validation data accumulates, proxies improve but never perfectly match ground truth in novel regimes. Physics-based proxies with strong theoretical foundations show more stable performance across time than purely empirical proxies.",
        "For protein structure prediction and related tasks, domain-specific factors causing larger gaps (2-5× compared to rigid, well-characterized structures) include: protein flexibility, conformational dynamics, multi-chain assemblies, induced fit, post-translational modifications, tissue-specific expression, and binding-partner availability."
    ],
    "new_predictions_likely": [
        "Systems that incorporate explicit proxy-bias correction and biological context (like KGDRP with knowledge graphs) will show 20-50% smaller gaps between computational and experimental success rates than systems without such correction, with the reduction being larger for incremental discoveries (30-50%) than transformational ones (10-30%).",
        "The false positive rate for automated discoveries will be inversely correlated with the amount of experimental validation performed, following approximately a power law relationship, and will be 2-3× higher when proxies are used as optimization objectives versus evaluation metrics.",
        "Domains with well-established computational proxies and homogeneous experimental measurements (e.g., protein structure prediction with abundant MSAs, kinase inhibitor datasets) will show proxy-to-ground-truth gaps of 5-20%, while domains with poorly validated proxies and heterogeneous measurements (e.g., multi-target drug activity, complex ADMET properties) will show gaps of 40-80%.",
        "In materials and drug discovery, systems using physics-based proxies (DFT, molecular dynamics, FEP) will show 2-3× lower false positive rates than systems using purely data-driven proxies (neural network predictions without physical constraints), but only when applied within their validated parameter ranges.",
        "The proxy-to-ground-truth gap will be 2-5× larger for properties requiring long-timescale dynamics (e.g., stability, degradation, conformational changes) than for static properties (e.g., structure, composition, single-point energies).",
        "Systems that report calibrated uncertainty estimates and use them to gate predictions will show 30-50% better correlation between predicted uncertainty and actual proxy-to-ground-truth gap than systems without uncertainty quantification, enabling more reliable prioritization of experimental validation.",
        "Knowledge-integrated methods that combine sequence/structure information with biological networks, pathways, and curated databases will show 15-35% better out-of-distribution performance than sequence-only methods across drug discovery and systems biology applications.",
        "Generative models with multi-objective optimization including synthesizability, ADMET constraints, and uncertainty-aware gating will produce 40-60% fewer experimentally invalid candidates than models optimizing single proxy objectives (e.g., docking score alone)."
    ],
    "new_predictions_unknown": [
        "Whether machine learning meta-models could be trained to predict the proxy-to-ground-truth gap for individual discoveries with sufficient accuracy (e.g., R²&gt;0.7) to enable reliable prioritization of experimental validation, or whether the gap is fundamentally unpredictable due to the novelty of transformational discoveries.",
        "Whether the proxy-to-ground-truth gap could be reduced to &lt;10% across all discovery types through sufficiently sophisticated multifidelity approaches combining physics-based simulations, knowledge graphs, uncertainty quantification, and active learning with experimental feedback, or whether there are fundamental limits that prevent such universal accuracy.",
        "Whether certain types of transformational discoveries might actually show smaller proxy-to-ground-truth gaps than incremental discoveries in specific domains, potentially because transformational discoveries are more amenable to first-principles modeling (e.g., discovering entirely new binding modes well-described by quantum mechanics versus subtle SAR optimization in crowded chemical space).",
        "Whether active learning strategies that explicitly optimize for reducing the proxy-to-ground-truth gap (rather than optimizing the proxy itself or using standard acquisition functions) could achieve order-of-magnitude improvements in experimental validation success rates.",
        "Whether the gap could be reduced by orders of magnitude through hybrid approaches that combine multiple orthogonal proxies (e.g., physics-based + data-driven + knowledge-graph + expert heuristics) with learned weighting schemes, or whether proxies are fundamentally correlated in their failure modes.",
        "Whether the proxy-to-ground-truth gap follows universal scaling laws across domains (e.g., power law relationships between training data size, extrapolation distance, and gap magnitude), or whether each domain has fundamentally different gap characteristics that prevent cross-domain learning.",
        "Whether reward hacking in generative/optimization contexts can be effectively prevented through adversarial training, multi-objective constraints, and uncertainty-aware gating, or whether sufficiently powerful optimization will always find and exploit proxy weaknesses.",
        "Whether ground-truth measurement standardization and quality improvement could reduce proxy-to-ground-truth gaps by 50%+ by eliminating measurement heterogeneity as a confounding factor, or whether proxy limitations would remain dominant even with perfect ground-truth measurements."
    ],
    "negative_experiments": [
        "Finding domains where proxy metrics perfectly predict experimental outcomes (R²&gt;0.95, false positive rate &lt;5%) across both incremental and transformational discoveries, including out-of-distribution cases, would challenge the universality of the gap.",
        "Demonstrating that transformational discoveries consistently show smaller proxy-to-ground-truth gaps than incremental ones across multiple domains would contradict the core theory prediction about extrapolation effects.",
        "Showing that systems without proxy-bias correction, biological context integration, or uncertainty quantification perform as well as those with these features in experimental validation (within 10% success rate) would question the value of gap-reduction methods.",
        "Finding that the proxy-to-ground-truth gap does not increase with extrapolation distance from training data would challenge the mechanism proposed for why transformational discoveries show larger gaps.",
        "Demonstrating that purely data-driven proxies without physical grounding perform as well as physics-based proxies in novel regimes (within 10% accuracy) would contradict the theory's prediction about proxy quality dependence on physical grounding.",
        "Showing that using proxies as optimization objectives does not increase the gap compared to using them as evaluation metrics would challenge the theory's predictions about adversarial exploitation.",
        "Finding that ground-truth measurement heterogeneity and quality have no effect on proxy-to-ground-truth gaps would contradict the revised theory's emphasis on measurement quality.",
        "Demonstrating that knowledge-integrated methods show no advantage over sequence-only methods in out-of-distribution scenarios would challenge the theory's predictions about biological context integration.",
        "Showing that the economic incentive to defer validation does not lead to accumulation of unvalidated discoveries would challenge the theory's prediction about systemic bias in the literature."
    ],
    "unaccounted_for": [
        {
            "text": "The role of human expertise and domain knowledge in proxy design, interpretation, and validation, and how this expertise can be systematically encoded or transferred to improve proxy quality beyond what is captured in training data.",
            "uuids": []
        },
        {
            "text": "How the gap evolves as computational methods improve over time and whether there are diminishing returns, fundamental limits, or potential for continued improvement.",
            "uuids": []
        },
        {
            "text": "The interaction between multiple proxy metrics used simultaneously in multi-objective optimization and whether they exhibit correlated or independent failure modes, and how to optimally weight and combine proxies.",
            "uuids": []
        },
        {
            "text": "The psychological and sociological factors that influence acceptance of proxy-validated discoveries in scientific communities, including publication bias, replication practices, and standards for experimental validation.",
            "uuids": []
        },
        {
            "text": "Whether certain discovery types (e.g., negative results, null findings, boundary cases) show different proxy-to-ground-truth gap characteristics than positive findings.",
            "uuids": []
        },
        {
            "text": "The impact of publication bias and selective reporting on the observed proxy-to-ground-truth gap in the literature, and whether published gaps underestimate true gaps due to preferential publication of successful validations.",
            "uuids": []
        },
        {
            "text": "How proxy-to-ground-truth gaps compound across multi-stage discovery pipelines and whether errors accumulate, cancel, or amplify at each stage.",
            "uuids": []
        },
        {
            "text": "The role of experimental design choices in determining what constitutes 'ground truth' and how these choices affect the measured gap.",
            "uuids": []
        },
        {
            "text": "Whether there are systematic differences in proxy-to-ground-truth gaps between different types of biological systems that are not fully captured by current domain categorizations.",
            "uuids": []
        }
    ],
    "change_log": [
        "Added ground-truth measurement heterogeneity and quality as an explicit factor affecting proxy-to-ground-truth gaps (factor 4 in theory description).",
        "Added consideration of whether proxies are used for evaluation versus optimization objectives as a gap-affecting factor (factor 5 in theory description).",
        "Expanded theory description to explicitly address reward hacking and adversarial exploitation when proxies are used as optimization objectives.",
        "Added new theory statement about reward hacking: 'When proxies are used as optimization objectives (rather than just evaluation metrics), the proxy-to-ground-truth gap increases due to adversarial exploitation of proxy weaknesses.'",
        "Added new theory statement about proxy architecture effects: 'Proxy architecture significantly affects gap magnitude: knowledge-integrated methods incorporating biological networks and curated databases show 10-30% smaller gaps in out-of-distribution scenarios than sequence-only methods.'",
        "Added new theory statement about ground-truth measurement quality: 'Ground-truth measurement heterogeneity and quality independently affect proxy-ground-truth gaps: homogeneous, well-standardized experimental datasets show gaps 50-80% smaller than heterogeneous datasets.'",
        "Added new theory statement about uncertainty quantification: 'Systems using calibrated uncertainty estimates to gate or downweight unreliable predictions will show 20-40% fewer false positives in out-of-distribution regimes.'",
        "Added new theory statement about data augmentation effects: 'Proxies trained on synthetic augmented data will show 10-30% larger gaps than those trained on purely experimental data.'",
        "Added new theory statement about protein structure prediction domain-specific factors: 'For protein structure prediction and related tasks, domain-specific factors causing larger gaps include: protein flexibility, conformational dynamics, multi-chain assemblies, induced fit, post-translational modifications, tissue-specific expression, and binding-partner availability.'",
        "Refined theory statement about validation cascades to acknowledge practical utility: 'The gap manifests differently across validation cascades with error accumulation at each stage. However, imperfect proxies provide practical value for hypothesis generation and experimental triage when used with appropriate uncertainty quantification.'",
        "Updated theory statement about multifidelity approaches to specify conditions for effectiveness: 'Multifidelity approaches can reduce gaps by 30-50% when proxies have uncorrelated error modes, but only 10-20% when errors are correlated.'",
        "Added quantitative performance degradation estimates to novelty-related theory statement: 'systems show 40-70% performance degradation (measured by correlation coefficients) when moving from in-distribution to out-of-distribution evaluation.'",
        "Added supporting evidence from 17 new studies demonstrating gap variation with novelty, reward hacking, measurement quality effects, and biological context integration benefits.",
        "Added new predictions about knowledge-integrated methods, uncertainty quantification benefits, and multi-objective optimization effects on false positive rates.",
        "Added new unknown predictions about meta-models for gap prediction, universal scaling laws, and reward hacking prevention strategies.",
        "Added negative experiments testing measurement quality effects, biological context integration benefits, and reward hacking predictions.",
        "Added unaccounted-for factors including human expertise encoding, multi-stage pipeline error propagation, and experimental design effects on ground truth definition."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
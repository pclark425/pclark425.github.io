<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt-Induced Calibration Distortion Theory: General Law of Prompt-Driven Probability Distortion - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1861</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1861</p>
                <p><strong>Name:</strong> Prompt-Induced Calibration Distortion Theory: General Law of Prompt-Driven Probability Distortion</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.</p>
                <p><strong>Description:</strong> This theory posits that the structure, wording, and context of prompts presented to large language models (LLMs) systematically distort the calibration of their probability estimates for future real-world scientific discoveries. The theory asserts that these distortions are not random, but are governed by identifiable features of the prompt and the LLM's training data, leading to predictable biases in the model's output.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Prompt-Driven Probability Distortion Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_queried_with &#8594; prompt<span style="color: #888888;">, and</span></div>
        <div>&#8226; prompt &#8594; has_features &#8594; structure, wording, context</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_systematically_distorted_by &#8594; prompt_features</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Empirical studies show that LLMs' outputs are sensitive to prompt wording and structure, leading to different probability estimates for semantically similar queries. </li>
    <li>Prompt engineering can be used to manipulate LLM outputs, indicating systematic, not random, effects. </li>
    <li>Calibration studies reveal that LLMs' confidence is often misaligned with true likelihoods, and this misalignment varies with prompt phrasing. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> While prompt sensitivity is known, the formalization of systematic, predictable distortion in probability calibration for scientific discovery forecasting is novel.</p>            <p><strong>What Already Exists:</strong> Prompt sensitivity and calibration issues in LLMs are well-documented, but not formalized as a general law relating prompt features to systematic probability distortion.</p>            <p><strong>What is Novel:</strong> The explicit generalization that prompt features systematically and predictably distort LLM probability calibration for scientific forecasting is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt calibration]</li>
    <li>Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt sensitivity]</li>
    <li>Jiang et al. (2021) How Can We Know When Language Models Know? [Calibration issues]</li>
</ul>
            <h3>Statement 1: Prompt-Training Data Interaction Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; prompt &#8594; contains_features &#8594; rare_phrasing or domain-specific_jargon<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM_training_data &#8594; lacks_coverage_of &#8594; prompt_features</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM_probability_estimate &#8594; is_more_likely_to_be_miscalibrated &#8594; True</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs perform worse and are less calibrated on out-of-distribution prompts or those with rare phrasing. </li>
    <li>Domain-specific jargon not present in training data leads to unpredictable or less reliable LLM outputs. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends known OOD generalization issues to the specific context of prompt-induced calibration distortion in LLMs.</p>            <p><strong>What Already Exists:</strong> Out-of-distribution generalization and calibration issues are known, but not specifically linked to prompt-induced probability distortion in scientific forecasting.</p>            <p><strong>What is Novel:</strong> The explicit connection between prompt-training data mismatch and systematic miscalibration in scientific discovery probability estimates is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Hendrycks & Dietterich (2019) Benchmarking Neural Network Robustness to Common Corruptions and Perturbations [OOD generalization]</li>
    <li>Jiang et al. (2021) How Can We Know When Language Models Know? [Calibration and OOD]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will produce systematically different probability estimates for the same scientific event when prompted with different, but semantically equivalent, phrasings.</li>
                <li>Prompts containing rare or domain-specific language will result in less calibrated probability estimates compared to common phrasing.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>There may exist universal prompt features that minimize calibration distortion across all LLM architectures.</li>
                <li>Some LLMs may develop internal mechanisms to self-correct for prompt-induced distortions if trained with sufficient prompt diversity.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs produce identical, well-calibrated probability estimates regardless of prompt structure or wording, the theory would be falsified.</li>
                <li>If prompt features do not correlate with systematic changes in probability estimates, the theory would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>LLMs with explicit prompt normalization or meta-learning modules may not exhibit these distortions. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This is a new, general theory formalizing prompt-induced calibration distortion in LLMs for scientific forecasting.</p>
            <p><strong>References:</strong> <ul>
    <li>Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt calibration]</li>
    <li>Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt sensitivity]</li>
    <li>Jiang et al. (2021) How Can We Know When Language Models Know? [Calibration issues]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Prompt-Induced Calibration Distortion Theory: General Law of Prompt-Driven Probability Distortion",
    "theory_description": "This theory posits that the structure, wording, and context of prompts presented to large language models (LLMs) systematically distort the calibration of their probability estimates for future real-world scientific discoveries. The theory asserts that these distortions are not random, but are governed by identifiable features of the prompt and the LLM's training data, leading to predictable biases in the model's output.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Prompt-Driven Probability Distortion Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_queried_with",
                        "object": "prompt"
                    },
                    {
                        "subject": "prompt",
                        "relation": "has_features",
                        "object": "structure, wording, context"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_systematically_distorted_by",
                        "object": "prompt_features"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Empirical studies show that LLMs' outputs are sensitive to prompt wording and structure, leading to different probability estimates for semantically similar queries.",
                        "uuids": []
                    },
                    {
                        "text": "Prompt engineering can be used to manipulate LLM outputs, indicating systematic, not random, effects.",
                        "uuids": []
                    },
                    {
                        "text": "Calibration studies reveal that LLMs' confidence is often misaligned with true likelihoods, and this misalignment varies with prompt phrasing.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Prompt sensitivity and calibration issues in LLMs are well-documented, but not formalized as a general law relating prompt features to systematic probability distortion.",
                    "what_is_novel": "The explicit generalization that prompt features systematically and predictably distort LLM probability calibration for scientific forecasting is new.",
                    "classification_explanation": "While prompt sensitivity is known, the formalization of systematic, predictable distortion in probability calibration for scientific discovery forecasting is novel.",
                    "likely_classification": "new",
                    "references": [
                        "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt calibration]",
                        "Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt sensitivity]",
                        "Jiang et al. (2021) How Can We Know When Language Models Know? [Calibration issues]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Prompt-Training Data Interaction Law",
                "if": [
                    {
                        "subject": "prompt",
                        "relation": "contains_features",
                        "object": "rare_phrasing or domain-specific_jargon"
                    },
                    {
                        "subject": "LLM_training_data",
                        "relation": "lacks_coverage_of",
                        "object": "prompt_features"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM_probability_estimate",
                        "relation": "is_more_likely_to_be_miscalibrated",
                        "object": "True"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs perform worse and are less calibrated on out-of-distribution prompts or those with rare phrasing.",
                        "uuids": []
                    },
                    {
                        "text": "Domain-specific jargon not present in training data leads to unpredictable or less reliable LLM outputs.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Out-of-distribution generalization and calibration issues are known, but not specifically linked to prompt-induced probability distortion in scientific forecasting.",
                    "what_is_novel": "The explicit connection between prompt-training data mismatch and systematic miscalibration in scientific discovery probability estimates is new.",
                    "classification_explanation": "This law extends known OOD generalization issues to the specific context of prompt-induced calibration distortion in LLMs.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Hendrycks & Dietterich (2019) Benchmarking Neural Network Robustness to Common Corruptions and Perturbations [OOD generalization]",
                        "Jiang et al. (2021) How Can We Know When Language Models Know? [Calibration and OOD]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will produce systematically different probability estimates for the same scientific event when prompted with different, but semantically equivalent, phrasings.",
        "Prompts containing rare or domain-specific language will result in less calibrated probability estimates compared to common phrasing."
    ],
    "new_predictions_unknown": [
        "There may exist universal prompt features that minimize calibration distortion across all LLM architectures.",
        "Some LLMs may develop internal mechanisms to self-correct for prompt-induced distortions if trained with sufficient prompt diversity."
    ],
    "negative_experiments": [
        "If LLMs produce identical, well-calibrated probability estimates regardless of prompt structure or wording, the theory would be falsified.",
        "If prompt features do not correlate with systematic changes in probability estimates, the theory would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "LLMs with explicit prompt normalization or meta-learning modules may not exhibit these distortions.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs show robust calibration across a wide range of prompts, possibly due to advanced regularization or prompt-agnostic training.",
            "uuids": []
        }
    ],
    "special_cases": [
        "For extremely simple or formulaic prompts, calibration distortion may be negligible.",
        "For prompts that exactly match high-frequency training data, calibration may be optimal."
    ],
    "existing_theory": {
        "what_already_exists": "Prompt sensitivity and calibration issues are known, but not formalized as a general law for scientific forecasting.",
        "what_is_novel": "The generalization to systematic, predictable prompt-induced calibration distortion in scientific discovery probability estimates is new.",
        "classification_explanation": "This is a new, general theory formalizing prompt-induced calibration distortion in LLMs for scientific forecasting.",
        "likely_classification": "new",
        "references": [
            "Zhao et al. (2021) Calibrate Before Use: Improving Few-Shot Performance of Language Models [Prompt calibration]",
            "Webson & Pavlick (2022) Do Prompt-Based Models Really Understand the Meaning of Their Prompts? [Prompt sensitivity]",
            "Jiang et al. (2021) How Can We Know When Language Models Know? [Calibration issues]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how large language models (LLMs) can accurately measure the probability or likelihood of specific future real-world scientific discoveries.",
    "original_theory_id": "theory-650",
    "original_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Prompt-Induced Calibration Distortion Theory",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
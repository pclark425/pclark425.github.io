<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Representation Robustness and Expressivity Theory for LLM Chemical Synthesis - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-608</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-608</p>
                <p><strong>Name:</strong> Representation Robustness and Expressivity Theory for LLM Chemical Synthesis</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how large language models (LLMs) can synthesize novel chemicals for specific applications, based on the following results.</p>
                <p><strong>Description:</strong> The choice and robustness of molecular representation (e.g., SMILES, SELFIES, IUPAC, GMR, graphs, 3D coordinates) fundamentally determines the validity, diversity, interpretability, and synthesizability of molecules generated by LLMs. Representations that are grammar- or semantics-constrained (e.g., SELFIES, grammar VAEs, graph-based) enable LLMs to generate valid and chemically plausible molecules more reliably, while flexible representations (e.g., SMILES, IUPAC, GMR) allow for greater expressivity and human interpretability but may increase the risk of invalid or non-synthesizable outputs. The interplay between representation, model architecture, and training data governs the trade-off between validity, novelty, property control, and interpretability in LLM-driven chemical synthesis.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Representation Robustness Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_trained_on &#8594; robust molecular representation (e.g., SELFIES, grammar-constrained SMILES, graph-based, 3D coordinate-based)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; generates &#8594; molecules with higher validity and lower rate of syntactic/semantic errors</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SELFIES-trained LSTM (SF-RNN) achieves 100% validity, outperforming SMILES-based RNNs and graph models on large-molecule tasks. <a href="../results/extraction-result-5292.html#e5292.1" class="evidence-link">[e5292.1]</a> </li>
    <li>Grammar VAE and SD-VAE reduce invalid outputs compared to plain VAEs and RNNs. <a href="../results/extraction-result-5147.html#e5147.5" class="evidence-link">[e5147.5]</a> </li>
    <li>SELFIES-based models (BioT5, MolLEO(BioT5)) achieve perfect validity (100%) in molecule generation and editing, outperforming SMILES-based models. <a href="../results/extraction-result-5304.html#e5304.0" class="evidence-link">[e5304.0]</a> <a href="../results/extraction-result-5134.html#e5134.1" class="evidence-link">[e5134.1]</a> </li>
    <li>Graph-based models (JTN-VAE, cG-SchNet, Lim et al. scaffold model) guarantee chemical validity by construction, achieving perfect or near-perfect validity. <a href="../results/extraction-result-5306.html#e5306.3" class="evidence-link">[e5306.3]</a> <a href="../results/extraction-result-5287.html#e5287.0" class="evidence-link">[e5287.0]</a> <a href="../results/extraction-result-5299.html#e5299.3" class="evidence-link">[e5299.3]</a> </li>
    <li>3D coordinate-based LLMs (LM-AC for molecules, LM-AC for protein pockets, LM-AC/LM-CH for crystals) achieve high validity rates (e.g., 98.5% for LM-AC molecules, 99% for protein pockets, >95% for crystals). <a href="../results/extraction-result-5288.html#e5288.0" class="evidence-link">[e5288.0]</a> <a href="../results/extraction-result-5288.html#e5288.2" class="evidence-link">[e5288.2]</a> <a href="../results/extraction-result-5288.html#e5288.1" class="evidence-link">[e5288.1]</a> </li>
    <li>SMILES-based LLMs (e.g., GPT-4, T5, MolXPT, MolT5) produce invalid molecules more frequently than SELFIES-based or grammar-constrained models. <a href="../results/extraction-result-5300.html#e5300.1" class="evidence-link">[e5300.1]</a> <a href="../results/extraction-result-5304.html#e5304.2" class="evidence-link">[e5304.2]</a> <a href="../results/extraction-result-5136.html#e5136.0" class="evidence-link">[e5136.0]</a> <a href="../results/extraction-result-5304.html#e5304.1" class="evidence-link">[e5304.1]</a> <a href="../results/extraction-result-5280.html#e5280.0" class="evidence-link">[e5280.0]</a> </li>
    <li>LatentGAN and VAE models using SMILES decoding have lower validity than graph-based or SELFIES-based models. <a href="../results/extraction-result-5306.html#e5306.4" class="evidence-link">[e5306.4]</a> <a href="../results/extraction-result-5282.html#e5282.1" class="evidence-link">[e5282.1]</a> <a href="../results/extraction-result-5303.html#e5303.0" class="evidence-link">[e5303.0]</a> </li>
    <li>Combinatorial generator (BRICS) achieves perfect validity by construction, but can generate implausible molecules due to lack of learned constraints. <a href="../results/extraction-result-5306.html#e5306.7" class="evidence-link">[e5306.7]</a> </li>
    <li>SELFIES/DeepSMILES representations are designed to improve robustness and validity under mutation and generation. <a href="../results/extraction-result-5147.html#e5147.8" class="evidence-link">[e5147.8]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the effect of representation on validity is established in specific models, the explicit theoretical elevation of representation robustness as a universal law for LLM-driven chemical synthesis, including 3D and graph-based LLMs, is novel.</p>            <p><strong>What Already Exists:</strong> SELFIES and grammar-constrained representations are known to improve validity in generative models; graph-based models guarantee chemical validity by construction.</p>            <p><strong>What is Novel:</strong> This law generalizes the principle to LLMs and posits that representation robustness is a primary determinant of generative validity across architectures, scales, and modalities (including 3D and graph-based LLMs).</p>
            <p><strong>References:</strong> <ul>
    <li>Krenn et al. (2020) SELFIES: A robust representation of semantically constrained graphs with an example application in chemistry [SELFIES]</li>
    <li>Kusner et al. (2017) Grammar Variational Autoencoder [grammar-constrained VAE]</li>
    <li>Nigam et al. (2021) Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES [SELFIES for robust generation]</li>
    <li>Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [graph-based validity]</li>
    <li>Hoffmann et al. (2023) Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files [3D coordinate-based LMs]</li>
</ul>
            <h3>Statement 1: Representation-Expressivity Trade-off Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; is_trained_on &#8594; flexible or human-interpretable representation (e.g., SMILES, IUPAC, GMR)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; can_generate &#8594; more diverse and interpretable molecules, but with increased risk of invalid or non-synthesizable outputs</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>IUPAC-based models (iupacGPT, C5T5) enable human-interpretable edits and fine-grained control, but may generate syntactically valid but chemically unstable molecules. <a href="../results/extraction-result-5143.html#e5143.5" class="evidence-link">[e5143.5]</a> <a href="../results/extraction-result-5277.html#e5277.0" class="evidence-link">[e5277.0]</a> </li>
    <li>DrugLLM's GMR representation enables few-shot optimization and interpretability, but has encoding/decoding errors for complex molecules. <a href="../results/extraction-result-5148.html#e5148.0" class="evidence-link">[e5148.0]</a> </li>
    <li>SMILES-based LLMs (e.g., GPT-4, T5, GPT-3.5, ChatGPT, MolXPT) can generate diverse molecules but are prone to invalid outputs and hallucinations, especially in zero-shot or few-shot settings. <a href="../results/extraction-result-5300.html#e5300.1" class="evidence-link">[e5300.1]</a> <a href="../results/extraction-result-5136.html#e5136.0" class="evidence-link">[e5136.0]</a> <a href="../results/extraction-result-5312.html#e5312.2" class="evidence-link">[e5312.2]</a> <a href="../results/extraction-result-5304.html#e5304.2" class="evidence-link">[e5304.2]</a> <a href="../results/extraction-result-5136.html#e5136.1" class="evidence-link">[e5136.1]</a> <a href="../results/extraction-result-5148.html#e5148.1" class="evidence-link">[e5148.1]</a> <a href="../results/extraction-result-5148.html#e5148.2" class="evidence-link">[e5148.2]</a> <a href="../results/extraction-result-5295.html#e5295.2" class="evidence-link">[e5295.2]</a> </li>
    <li>SMILES-based RNNs (SM-RNN, CharRNN) achieve high diversity and property-distribution matching, but lower validity and increased risk of invalid SMILES compared to SELFIES or graph-based models. <a href="../results/extraction-result-5292.html#e5292.0" class="evidence-link">[e5292.0]</a> <a href="../results/extraction-result-5306.html#e5306.0" class="evidence-link">[e5306.0]</a> </li>
    <li>SELFIES-based models (SF-RNN, BioT5) achieve perfect validity but sometimes have slightly worse property-distribution matching (Wasserstein distance) compared to SMILES-based RNNs, indicating a trade-off between validity and fine-grained expressivity. <a href="../results/extraction-result-5292.html#e5292.1" class="evidence-link">[e5292.1]</a> <a href="../results/extraction-result-5304.html#e5304.0" class="evidence-link">[e5304.0]</a> </li>
    <li>Combinatorial generator (BRICS) achieves perfect validity and high diversity, but can generate chemically implausible or non-synthesizable molecules due to lack of learned global constraints. <a href="../results/extraction-result-5306.html#e5306.7" class="evidence-link">[e5306.7]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the trade-off is recognized in specific contexts, its elevation to a general law for LLM-based synthesis and the explicit inclusion of new representations is novel.</p>            <p><strong>What Already Exists:</strong> Trade-offs between representation expressivity and validity are discussed in the context of SMILES vs SELFIES and grammar-based models.</p>            <p><strong>What is Novel:</strong> This law formalizes the trade-off as a general principle for LLM-driven chemical synthesis, encompassing new representations (e.g., GMR, IUPAC) and their impact on interpretability and error rates.</p>
            <p><strong>References:</strong> <ul>
    <li>Krenn et al. (2020) SELFIES: A robust representation of semantically constrained graphs with an example application in chemistry [SELFIES]</li>
    <li>Nigam et al. (2021) Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES [SELFIES for robust generation]</li>
    <li>Zhang et al. (2023) C5T5: Controllable Generation of Organic Molecules with Transformers [IUPAC-based infilling]</li>
    <li>Zhang et al. (2024) DrugLLM: Open Large Language Model for Few-shot Molecule Generation [GMR representation]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Switching an LLM's training and generation representation from SMILES to SELFIES will increase the validity of generated molecules, especially for large or complex structures.</li>
                <li>Using grammar-constrained or graph-based representations in LLMs will reduce the rate of invalid or chemically implausible outputs compared to unconstrained SMILES.</li>
                <li>Instruction-tuned LLMs using IUPAC or GMR representations will enable more interpretable and user-controllable molecule edits, but may occasionally produce syntactically valid but chemically unstable molecules.</li>
                <li>3D coordinate-based LLMs will maintain high validity for both small molecules and materials, provided coordinate precision and tokenization are handled appropriately.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Hybrid representations (e.g., combining SELFIES for core structure and IUPAC for functional group editing) will enable both high validity and high interpretability in LLM-driven molecule generation.</li>
                <li>LLMs trained on 3D coordinate representations (e.g., XYZ, CIF) will be able to generate valid, synthesizable 3D structures for both small molecules and materials, matching or exceeding graph-based models in diversity and novelty.</li>
                <li>LLMs can be trained to automatically select the optimal representation for a given generation task (e.g., property optimization vs. scaffold hopping), dynamically switching representations to maximize validity and novelty.</li>
                <li>Fine-tuned LLMs using GMR or IUPAC representations may outperform SMILES/SELFIES-based models in human-in-the-loop design workflows, but may require new error-correction strategies for complex or edge-case molecules.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an LLM trained on SELFIES or grammar-constrained representations does not achieve higher validity than SMILES-based models, the representation robustness law would be challenged.</li>
                <li>If IUPAC- or GMR-based LLMs do not enable more interpretable or controllable molecule edits compared to SMILES-based models, the expressivity trade-off law would be weakened.</li>
                <li>If graph-based or 3D coordinate-based LLMs do not outperform SMILES-based models on validity or diversity for large or complex molecules, the theory's generality would be questioned.</li>
                <li>If SMILES-based LLMs, after sufficient fine-tuning, achieve both perfect validity and high diversity on par with SELFIES/graph-based models, the necessity of robust representations may be less universal than posited.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some graph-based models (e.g., JTN-VAE) achieve perfect validity but limited scaffold novelty, suggesting that representation alone does not guarantee diversity. <a href="../results/extraction-result-5306.html#e5306.3" class="evidence-link">[e5306.3]</a> </li>
    <li>SELFIES-based models may have higher validity but slightly worse property-distribution matching (Wasserstein distance) compared to SMILES-based RNNs. <a href="../results/extraction-result-5292.html#e5292.1" class="evidence-link">[e5292.1]</a> </li>
    <li>Combinatorial generator achieves perfect validity and high diversity, but can generate chemically implausible or non-synthesizable molecules, indicating that validity does not guarantee chemical realism. <a href="../results/extraction-result-5306.html#e5306.7" class="evidence-link">[e5306.7]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> While the effects of representation are known, the generalization to LLMs and the formalization of the trade-off as a theory is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Krenn et al. (2020) SELFIES: A robust representation of semantically constrained graphs with an example application in chemistry [SELFIES]</li>
    <li>Kusner et al. (2017) Grammar Variational Autoencoder [grammar-constrained VAE]</li>
    <li>Zhang et al. (2023) C5T5: Controllable Generation of Organic Molecules with Transformers [IUPAC-based infilling]</li>
    <li>Nigam et al. (2021) Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES [SELFIES for robust generation]</li>
    <li>Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [graph-based validity]</li>
    <li>Hoffmann et al. (2023) Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files [3D coordinate-based LMs]</li>
    <li>Zhang et al. (2024) DrugLLM: Open Large Language Model for Few-shot Molecule Generation [GMR representation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Representation Robustness and Expressivity Theory for LLM Chemical Synthesis",
    "theory_description": "The choice and robustness of molecular representation (e.g., SMILES, SELFIES, IUPAC, GMR, graphs, 3D coordinates) fundamentally determines the validity, diversity, interpretability, and synthesizability of molecules generated by LLMs. Representations that are grammar- or semantics-constrained (e.g., SELFIES, grammar VAEs, graph-based) enable LLMs to generate valid and chemically plausible molecules more reliably, while flexible representations (e.g., SMILES, IUPAC, GMR) allow for greater expressivity and human interpretability but may increase the risk of invalid or non-synthesizable outputs. The interplay between representation, model architecture, and training data governs the trade-off between validity, novelty, property control, and interpretability in LLM-driven chemical synthesis.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Representation Robustness Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_trained_on",
                        "object": "robust molecular representation (e.g., SELFIES, grammar-constrained SMILES, graph-based, 3D coordinate-based)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "generates",
                        "object": "molecules with higher validity and lower rate of syntactic/semantic errors"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SELFIES-trained LSTM (SF-RNN) achieves 100% validity, outperforming SMILES-based RNNs and graph models on large-molecule tasks.",
                        "uuids": [
                            "e5292.1"
                        ]
                    },
                    {
                        "text": "Grammar VAE and SD-VAE reduce invalid outputs compared to plain VAEs and RNNs.",
                        "uuids": [
                            "e5147.5"
                        ]
                    },
                    {
                        "text": "SELFIES-based models (BioT5, MolLEO(BioT5)) achieve perfect validity (100%) in molecule generation and editing, outperforming SMILES-based models.",
                        "uuids": [
                            "e5304.0",
                            "e5134.1"
                        ]
                    },
                    {
                        "text": "Graph-based models (JTN-VAE, cG-SchNet, Lim et al. scaffold model) guarantee chemical validity by construction, achieving perfect or near-perfect validity.",
                        "uuids": [
                            "e5306.3",
                            "e5287.0",
                            "e5299.3"
                        ]
                    },
                    {
                        "text": "3D coordinate-based LLMs (LM-AC for molecules, LM-AC for protein pockets, LM-AC/LM-CH for crystals) achieve high validity rates (e.g., 98.5% for LM-AC molecules, 99% for protein pockets, &gt;95% for crystals).",
                        "uuids": [
                            "e5288.0",
                            "e5288.2",
                            "e5288.1"
                        ]
                    },
                    {
                        "text": "SMILES-based LLMs (e.g., GPT-4, T5, MolXPT, MolT5) produce invalid molecules more frequently than SELFIES-based or grammar-constrained models.",
                        "uuids": [
                            "e5300.1",
                            "e5304.2",
                            "e5136.0",
                            "e5304.1",
                            "e5280.0"
                        ]
                    },
                    {
                        "text": "LatentGAN and VAE models using SMILES decoding have lower validity than graph-based or SELFIES-based models.",
                        "uuids": [
                            "e5306.4",
                            "e5282.1",
                            "e5303.0"
                        ]
                    },
                    {
                        "text": "Combinatorial generator (BRICS) achieves perfect validity by construction, but can generate implausible molecules due to lack of learned constraints.",
                        "uuids": [
                            "e5306.7"
                        ]
                    },
                    {
                        "text": "SELFIES/DeepSMILES representations are designed to improve robustness and validity under mutation and generation.",
                        "uuids": [
                            "e5147.8"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "SELFIES and grammar-constrained representations are known to improve validity in generative models; graph-based models guarantee chemical validity by construction.",
                    "what_is_novel": "This law generalizes the principle to LLMs and posits that representation robustness is a primary determinant of generative validity across architectures, scales, and modalities (including 3D and graph-based LLMs).",
                    "classification_explanation": "While the effect of representation on validity is established in specific models, the explicit theoretical elevation of representation robustness as a universal law for LLM-driven chemical synthesis, including 3D and graph-based LLMs, is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Krenn et al. (2020) SELFIES: A robust representation of semantically constrained graphs with an example application in chemistry [SELFIES]",
                        "Kusner et al. (2017) Grammar Variational Autoencoder [grammar-constrained VAE]",
                        "Nigam et al. (2021) Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES [SELFIES for robust generation]",
                        "Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [graph-based validity]",
                        "Hoffmann et al. (2023) Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files [3D coordinate-based LMs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Representation-Expressivity Trade-off Law",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "is_trained_on",
                        "object": "flexible or human-interpretable representation (e.g., SMILES, IUPAC, GMR)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "can_generate",
                        "object": "more diverse and interpretable molecules, but with increased risk of invalid or non-synthesizable outputs"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "IUPAC-based models (iupacGPT, C5T5) enable human-interpretable edits and fine-grained control, but may generate syntactically valid but chemically unstable molecules.",
                        "uuids": [
                            "e5143.5",
                            "e5277.0"
                        ]
                    },
                    {
                        "text": "DrugLLM's GMR representation enables few-shot optimization and interpretability, but has encoding/decoding errors for complex molecules.",
                        "uuids": [
                            "e5148.0"
                        ]
                    },
                    {
                        "text": "SMILES-based LLMs (e.g., GPT-4, T5, GPT-3.5, ChatGPT, MolXPT) can generate diverse molecules but are prone to invalid outputs and hallucinations, especially in zero-shot or few-shot settings.",
                        "uuids": [
                            "e5300.1",
                            "e5136.0",
                            "e5312.2",
                            "e5304.2",
                            "e5136.1",
                            "e5148.1",
                            "e5148.2",
                            "e5295.2"
                        ]
                    },
                    {
                        "text": "SMILES-based RNNs (SM-RNN, CharRNN) achieve high diversity and property-distribution matching, but lower validity and increased risk of invalid SMILES compared to SELFIES or graph-based models.",
                        "uuids": [
                            "e5292.0",
                            "e5306.0"
                        ]
                    },
                    {
                        "text": "SELFIES-based models (SF-RNN, BioT5) achieve perfect validity but sometimes have slightly worse property-distribution matching (Wasserstein distance) compared to SMILES-based RNNs, indicating a trade-off between validity and fine-grained expressivity.",
                        "uuids": [
                            "e5292.1",
                            "e5304.0"
                        ]
                    },
                    {
                        "text": "Combinatorial generator (BRICS) achieves perfect validity and high diversity, but can generate chemically implausible or non-synthesizable molecules due to lack of learned global constraints.",
                        "uuids": [
                            "e5306.7"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Trade-offs between representation expressivity and validity are discussed in the context of SMILES vs SELFIES and grammar-based models.",
                    "what_is_novel": "This law formalizes the trade-off as a general principle for LLM-driven chemical synthesis, encompassing new representations (e.g., GMR, IUPAC) and their impact on interpretability and error rates.",
                    "classification_explanation": "While the trade-off is recognized in specific contexts, its elevation to a general law for LLM-based synthesis and the explicit inclusion of new representations is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Krenn et al. (2020) SELFIES: A robust representation of semantically constrained graphs with an example application in chemistry [SELFIES]",
                        "Nigam et al. (2021) Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES [SELFIES for robust generation]",
                        "Zhang et al. (2023) C5T5: Controllable Generation of Organic Molecules with Transformers [IUPAC-based infilling]",
                        "Zhang et al. (2024) DrugLLM: Open Large Language Model for Few-shot Molecule Generation [GMR representation]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Switching an LLM's training and generation representation from SMILES to SELFIES will increase the validity of generated molecules, especially for large or complex structures.",
        "Using grammar-constrained or graph-based representations in LLMs will reduce the rate of invalid or chemically implausible outputs compared to unconstrained SMILES.",
        "Instruction-tuned LLMs using IUPAC or GMR representations will enable more interpretable and user-controllable molecule edits, but may occasionally produce syntactically valid but chemically unstable molecules.",
        "3D coordinate-based LLMs will maintain high validity for both small molecules and materials, provided coordinate precision and tokenization are handled appropriately."
    ],
    "new_predictions_unknown": [
        "Hybrid representations (e.g., combining SELFIES for core structure and IUPAC for functional group editing) will enable both high validity and high interpretability in LLM-driven molecule generation.",
        "LLMs trained on 3D coordinate representations (e.g., XYZ, CIF) will be able to generate valid, synthesizable 3D structures for both small molecules and materials, matching or exceeding graph-based models in diversity and novelty.",
        "LLMs can be trained to automatically select the optimal representation for a given generation task (e.g., property optimization vs. scaffold hopping), dynamically switching representations to maximize validity and novelty.",
        "Fine-tuned LLMs using GMR or IUPAC representations may outperform SMILES/SELFIES-based models in human-in-the-loop design workflows, but may require new error-correction strategies for complex or edge-case molecules."
    ],
    "negative_experiments": [
        "If an LLM trained on SELFIES or grammar-constrained representations does not achieve higher validity than SMILES-based models, the representation robustness law would be challenged.",
        "If IUPAC- or GMR-based LLMs do not enable more interpretable or controllable molecule edits compared to SMILES-based models, the expressivity trade-off law would be weakened.",
        "If graph-based or 3D coordinate-based LLMs do not outperform SMILES-based models on validity or diversity for large or complex molecules, the theory's generality would be questioned.",
        "If SMILES-based LLMs, after sufficient fine-tuning, achieve both perfect validity and high diversity on par with SELFIES/graph-based models, the necessity of robust representations may be less universal than posited."
    ],
    "unaccounted_for": [
        {
            "text": "Some graph-based models (e.g., JTN-VAE) achieve perfect validity but limited scaffold novelty, suggesting that representation alone does not guarantee diversity.",
            "uuids": [
                "e5306.3"
            ]
        },
        {
            "text": "SELFIES-based models may have higher validity but slightly worse property-distribution matching (Wasserstein distance) compared to SMILES-based RNNs.",
            "uuids": [
                "e5292.1"
            ]
        },
        {
            "text": "Combinatorial generator achieves perfect validity and high diversity, but can generate chemically implausible or non-synthesizable molecules, indicating that validity does not guarantee chemical realism.",
            "uuids": [
                "e5306.7"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some SMILES-based LLMs (e.g., T5-large, MolT5-large) achieve high validity and competitive performance after fine-tuning, challenging the necessity of robust representations for all tasks.",
            "uuids": [
                "e5300.1",
                "e5304.1"
            ]
        },
        {
            "text": "Graph-based models may fail to train or generalize on very large molecules or complex materials, indicating limitations of representation scalability.",
            "uuids": [
                "e5292.1",
                "e5288.1"
            ]
        },
        {
            "text": "SELFIES-based models (SF-RNN) sometimes have higher validity but lower property-distribution matching than SMILES-based RNNs, suggesting a trade-off between validity and fine-grained expressivity.",
            "uuids": [
                "e5292.1"
            ]
        }
    ],
    "special_cases": [
        "For tasks requiring fine-grained property control or multi-objective optimization, representation robustness may be less important than model architecture or training data diversity.",
        "For very large or complex molecules/materials, coordinate-based or graph-based representations may become computationally prohibitive, requiring hybrid or hierarchical approaches.",
        "In low-data regimes, representation choice may interact with data augmentation and transfer learning strategies to determine generative success.",
        "Combinatorial or rule-based generators can achieve perfect validity but may lack chemical realism, so additional filters or learned constraints may be necessary."
    ],
    "existing_theory": {
        "what_already_exists": "The impact of representation on generative validity and diversity is established in the context of SMILES, SELFIES, grammar-based, and graph-based models. The trade-off between validity and expressivity is discussed in the literature.",
        "what_is_novel": "The explicit theoretical framework unifying representation robustness and expressivity as governing principles for LLM-driven chemical synthesis, and the inclusion of new representations (IUPAC, GMR, 3D coordinates) and their trade-offs, is novel.",
        "classification_explanation": "While the effects of representation are known, the generalization to LLMs and the formalization of the trade-off as a theory is novel.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Krenn et al. (2020) SELFIES: A robust representation of semantically constrained graphs with an example application in chemistry [SELFIES]",
            "Kusner et al. (2017) Grammar Variational Autoencoder [grammar-constrained VAE]",
            "Zhang et al. (2023) C5T5: Controllable Generation of Organic Molecules with Transformers [IUPAC-based infilling]",
            "Nigam et al. (2021) Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES [SELFIES for robust generation]",
            "Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [graph-based validity]",
            "Hoffmann et al. (2023) Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files [3D coordinate-based LMs]",
            "Zhang et al. (2024) DrugLLM: Open Large Language Model for Few-shot Molecule Generation [GMR representation]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
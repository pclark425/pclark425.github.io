<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Abstraction and Simulation in Language Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1079</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1079</p>
                <p><strong>Name:</strong> Hierarchical Abstraction and Simulation in Language Models</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.</p>
                <p><strong>Description:</strong> This theory proposes that LLMs solve spatial puzzle games by constructing hierarchical abstractions of the puzzle state and simulating possible moves at multiple levels of granularity. The model first abstracts the puzzle into high-level constraint structures, then simulates candidate moves or completions, and finally refines its predictions based on feedback from lower-level representations. This enables efficient reasoning over large or complex spatial puzzles, even when explicit symbolic manipulation is not possible.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Abstraction of Puzzle Structure (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; is_solving &#8594; spatial puzzle</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; constructs &#8594; multi-level abstractions of puzzle constraints and state</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can reason about both local (cell-level) and global (grid-level) constraints in puzzles like Sudoku. </li>
    <li>Probing reveals activations corresponding to both fine-grained and coarse-grained puzzle features. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> This law extends hierarchical abstraction to LLMs in the context of spatial reasoning.</p>            <p><strong>What Already Exists:</strong> Hierarchical abstraction is a known principle in cognitive science and some neural models.</p>            <p><strong>What is Novel:</strong> The application to LLMs' internal representations for spatial puzzle solving is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building Machines That Learn and Think Like People [hierarchical abstraction in cognition]</li>
    <li>Belrose et al. (2023) Language Models Can Solve Sudoku [LLMs and puzzle abstraction]</li>
</ul>
            <h3>Statement 1: Internal Simulation of Candidate Moves (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; language model &#8594; has_abstracted &#8594; puzzle state and constraints</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language model &#8594; simulates &#8594; candidate moves and evaluates constraint satisfaction</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can generate step-by-step reasoning traces for spatial puzzles, suggesting internal simulation of moves. </li>
    <li>Interventions that bias the model toward certain moves can alter its solution trajectory, consistent with internal simulation. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This law describes a novel emergent property in LLMs.</p>            <p><strong>What Already Exists:</strong> Simulation-based reasoning is known in cognitive science and some neural models.</p>            <p><strong>What is Novel:</strong> The emergence of internal simulation in LLMs for spatial puzzles is new.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building Machines That Learn and Think Like People [simulation in cognition]</li>
    <li>Belrose et al. (2023) Language Models Can Solve Sudoku [LLMs and stepwise reasoning]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will show distinct activation patterns when simulating different candidate moves in a spatial puzzle.</li>
                <li>Hierarchical probing will reveal separate layers encoding local and global puzzle features.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may develop novel, non-human-like abstraction hierarchies for unfamiliar spatial puzzles.</li>
                <li>If given puzzles with recursive or fractal structure, LLMs may develop multi-level simulation strategies.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs cannot generate stepwise reasoning traces for spatial puzzles, the theory would be challenged.</li>
                <li>If hierarchical probing fails to reveal abstraction layers, the theory would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The process by which LLMs learn to construct and use hierarchical abstractions from text data is not fully understood. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory applies cognitive principles to LLMs in a novel domain.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake et al. (2017) Building Machines That Learn and Think Like People [hierarchical abstraction, simulation]</li>
    <li>Belrose et al. (2023) Language Models Can Solve Sudoku [LLMs and puzzle solving]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Abstraction and Simulation in Language Models",
    "theory_description": "This theory proposes that LLMs solve spatial puzzle games by constructing hierarchical abstractions of the puzzle state and simulating possible moves at multiple levels of granularity. The model first abstracts the puzzle into high-level constraint structures, then simulates candidate moves or completions, and finally refines its predictions based on feedback from lower-level representations. This enables efficient reasoning over large or complex spatial puzzles, even when explicit symbolic manipulation is not possible.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Abstraction of Puzzle Structure",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "is_solving",
                        "object": "spatial puzzle"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "constructs",
                        "object": "multi-level abstractions of puzzle constraints and state"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can reason about both local (cell-level) and global (grid-level) constraints in puzzles like Sudoku.",
                        "uuids": []
                    },
                    {
                        "text": "Probing reveals activations corresponding to both fine-grained and coarse-grained puzzle features.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical abstraction is a known principle in cognitive science and some neural models.",
                    "what_is_novel": "The application to LLMs' internal representations for spatial puzzle solving is new.",
                    "classification_explanation": "This law extends hierarchical abstraction to LLMs in the context of spatial reasoning.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Lake et al. (2017) Building Machines That Learn and Think Like People [hierarchical abstraction in cognition]",
                        "Belrose et al. (2023) Language Models Can Solve Sudoku [LLMs and puzzle abstraction]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Internal Simulation of Candidate Moves",
                "if": [
                    {
                        "subject": "language model",
                        "relation": "has_abstracted",
                        "object": "puzzle state and constraints"
                    }
                ],
                "then": [
                    {
                        "subject": "language model",
                        "relation": "simulates",
                        "object": "candidate moves and evaluates constraint satisfaction"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can generate step-by-step reasoning traces for spatial puzzles, suggesting internal simulation of moves.",
                        "uuids": []
                    },
                    {
                        "text": "Interventions that bias the model toward certain moves can alter its solution trajectory, consistent with internal simulation.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Simulation-based reasoning is known in cognitive science and some neural models.",
                    "what_is_novel": "The emergence of internal simulation in LLMs for spatial puzzles is new.",
                    "classification_explanation": "This law describes a novel emergent property in LLMs.",
                    "likely_classification": "new",
                    "references": [
                        "Lake et al. (2017) Building Machines That Learn and Think Like People [simulation in cognition]",
                        "Belrose et al. (2023) Language Models Can Solve Sudoku [LLMs and stepwise reasoning]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will show distinct activation patterns when simulating different candidate moves in a spatial puzzle.",
        "Hierarchical probing will reveal separate layers encoding local and global puzzle features."
    ],
    "new_predictions_unknown": [
        "LLMs may develop novel, non-human-like abstraction hierarchies for unfamiliar spatial puzzles.",
        "If given puzzles with recursive or fractal structure, LLMs may develop multi-level simulation strategies."
    ],
    "negative_experiments": [
        "If LLMs cannot generate stepwise reasoning traces for spatial puzzles, the theory would be challenged.",
        "If hierarchical probing fails to reveal abstraction layers, the theory would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The process by which LLMs learn to construct and use hierarchical abstractions from text data is not fully understood.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some LLMs fail to solve puzzles requiring deep recursion or multi-level abstraction, suggesting limits to this mechanism.",
            "uuids": []
        }
    ],
    "special_cases": [
        "LLMs with shallow architectures may not develop effective hierarchical abstractions.",
        "Puzzles with flat or non-hierarchical constraint structures may not benefit from this approach."
    ],
    "existing_theory": {
        "what_already_exists": "Hierarchical abstraction and simulation are established in cognitive science and some neural models.",
        "what_is_novel": "The emergence of these mechanisms in LLMs for spatial puzzle solving is new.",
        "classification_explanation": "The theory applies cognitive principles to LLMs in a novel domain.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Lake et al. (2017) Building Machines That Learn and Think Like People [hierarchical abstraction, simulation]",
            "Belrose et al. (2023) Language Models Can Solve Sudoku [LLMs and puzzle solving]"
        ]
    },
    "reflected_from_theory_index": 3,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how language models solve puzzle games involving spatial knowledge, like Sudoku.",
    "original_theory_id": "theory-600",
    "original_theory_name": "Constraint-Driven Training Objectives Enable Neural Networks to Internalize Global Spatial Rules",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proxy-to-Ground-Truth Gap Theory (Revised) - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-393</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-393</p>
                <p><strong>Name:</strong> Proxy-to-Ground-Truth Gap Theory (Revised)</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory about the evaluation and validation of incremental versus transformational scientific discoveries in automated systems, based on the following results.</p>
                <p><strong>Description:</strong> Automated discovery systems frequently optimize proxy metrics (computational predictions, surrogate objectives, simulated outcomes, or intermediate validation measures) rather than ground-truth experimental outcomes. The gap between proxy performance and ground-truth validation creates a systematic bias where systems appear more successful in computational or simulated evaluation than in real-world validation. This gap varies systematically with: (1) the novelty and extrapolation distance of the discovery from training distributions, (2) the maturity and physical grounding of the proxy metric, (3) the domain's amenability to computational modeling, (4) temporal and distributional shifts between training and deployment, and (5) the validation architecture (closed-loop experimental feedback vs batch proxy-then-validate workflows). Closed-loop systems with tight experimental feedback can achieve smaller effective gaps through iterative refinement, though gaps are not eliminated. The computational cost advantage of proxy evaluation creates economic incentives to defer ground-truth validation, leading to accumulation of unvalidated discoveries. Most evidence pertains to incremental-to-moderate extrapolation; predictions about truly transformational discoveries remain partially tested.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2030</p>
                <p><strong>Knowledge Cutoff Month:</strong> 1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <a href="theory-157.html">[theory-157]</a></p>
            <p><strong>Change Log:</strong></p>
            <ul>
                <li>Revised theory description to be more concise while retaining key factors (novelty, maturity, modeling amenability, temporal shifts, validation architecture); added explicit note that most evidence pertains to incremental-to-moderate extrapolation.</li>
                <li>Added theory statement distinguishing closed-loop experimental systems (with tight, rapid feedback) from batch proxy-optimization systems (with delayed validation), predicting smaller effective gaps for closed-loop systems (statement 11).</li>
                <li>Added theory statement about temporal and distributional shifts as a critical dimension of proxy-to-ground-truth gap, predicting immediate degradation under temporal drift (statement 12).</li>
                <li>Added theory statement about multi-proxy systems, predicting gap reduction when proxies have independent failure modes but correlated failures when proxies share systematic biases (statement 13).</li>
                <li>Added theory statement specifying conditions under which uncertainty quantification is more vs less effective at reducing proxy-to-ground-truth gaps (statement 14).</li>
                <li>Added theory statement about proxy design failures that amplify gaps, including narrow dynamic range, limited information capture, misalignment with objectives, and overoptimistic post-hoc metrics (statement 15).</li>
                <li>Added theory statement about economic and cultural factors amplifying proxy-to-ground-truth gaps (statement 16).</li>
                <li>Modified statement 3 to remove specific reference to 'transformational discoveries' and instead refer to 'discoveries involving greater extrapolation' to better match the evidence base.</li>
                <li>Modified statement 4 to specify typical reduction ranges (20-50%) for multifidelity approaches and emphasize substantial residual gaps remain.</li>
                <li>Modified statement 7 to emphasize that computational maturity alone is insufficient and experimental validation culture is critical.</li>
                <li>Modified statement 8 to clarify cascade error propagation and bottleneck effects.</li>
                <li>Reorganized supporting evidence into thematic groups: extreme gaps (BigCloneBench, AOI), quantified gaps (DATADECIDE, VLA, LLM), cascades without ground truth (hemodynamics), partial gap reduction (sports video, causal learning), closed-loop success (A-Lab, SAMPLE, MOBO-SPM), unvalidated discoveries (autonomous agents), cascade propagation (MVP-RAG), novel error classes (AgentCompass), and missing validation (clinical, EEG/fNIRS).</li>
                <li>Updated new_predictions_likely to include 10 specific, testable predictions with quantified expected outcomes and explicit measurement methods covering closed-loop systems, temporal validation, multi-proxy systems, requirement-aware metrics, uncertainty quantification, proxy design, validation cascades, domain culture, and validation economics.</li>
                <li>Updated new_predictions_unknown to include 10 predictions about fundamental questions with explicit uncertainty about outcomes, including meta-model predictability, elimination of gaps, extrapolation paradoxes, active learning effectiveness, universal scaling laws, rapid-feedback limits, temporal prediction, economic restructuring, and transformational vs incremental gaps.</li>
                <li>Updated negative_experiments to include 10 tests with specific quantitative thresholds covering universality, extrapolation mechanisms, correction effectiveness, physics-based vs data-driven, economic incentives, closed-loop vs batch, temporal vs random validation, multi-proxy ensembles, and uncertainty quantification.</li>
                <li>Refined unaccounted_for to focus on 13 items that are genuinely not addressed by current theory statements, removing items that are now covered and adding more specific framing about what is unknown (e.g., formal methods for failure mode correlation, optimal feedback timing, a priori gap prediction, interpretability role, adaptive calibration, reproducibility relationship, regulatory frameworks, data quality effects, computational improvement trajectories, error interaction).</li>
                <li>Added explicit measurement methods and quantitative thresholds to predictions to make them more testable and falsifiable.</li>
                <li>Clarified scope throughout to acknowledge that most evidence involves incremental-to-moderate extrapolation rather than truly transformational discoveries, while maintaining predictions about extrapolative cases.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Automated systems systematically overestimate discovery success when evaluated on proxy metrics compared to ground-truth experimental validation, with the overestimation increasing as the proxy becomes more removed from ground truth.</li>
                <li>The proxy-to-ground-truth gap increases with the novelty and extrapolation distance of the discovery from training data, because proxies are typically calibrated on known regimes and become less reliable in novel regimes.</li>
                <li>Systems optimizing proxy metrics will exhibit higher false positive rates for discoveries involving greater extrapolation, because extrapolative discoveries involve regimes where proxy calibration is weakest.</li>
                <li>Multifidelity approaches that explicitly model and correct proxy biases can reduce but not eliminate the proxy-to-ground-truth gap, with typical reductions of 20-50% in error rates but substantial residual gaps remaining.</li>
                <li>The computational cost advantage of proxy evaluation creates an economic incentive to defer ground-truth validation, leading to accumulation of unvalidated discoveries and potential publication of false positives.</li>
                <li>The quality of a proxy metric depends on: (1) the physical/theoretical grounding of the proxy, (2) the domain's amenability to computational modeling, (3) the calibration data available in the regime of interest, (4) the dynamic range and continuous variation of the proxy signal, and (5) alignment with actual validation objectives.</li>
                <li>Domains with mature physics-based simulations show smaller proxy-to-ground-truth gaps than domains with purely empirical proxies, but computational maturity alone is insufficient without experimental validation culture.</li>
                <li>The gap manifests across validation cascades (proxy → intermediate validation → ground truth) with error accumulation at each stage, where early-stage errors create bottlenecks that limit downstream performance.</li>
                <li>Human expertise in proxy design can reduce but not eliminate the gap, as even expert-designed proxies fail in extrapolative regimes and when key aspects are not captured.</li>
                <li>The temporal evolution of proxy quality follows domain maturity: as domains mature and more validation data accumulates, proxies improve but never perfectly match ground truth in novel regimes, and temporal/distributional shifts can cause immediate degradation even when initial alignment appears good.</li>
                <li>Closed-loop experimental systems with tight, rapid feedback enabling iterative proxy refinement show smaller effective proxy-to-ground-truth gaps (typically 70-90% success rates) compared to batch proxy-optimization systems with delayed validation (typically 10-50% success rates), though gaps are not eliminated in either case.</li>
                <li>Temporal and distributional shifts cause proxy-to-ground-truth alignment to degrade even when initial test-set performance appears good, with degradation often occurring immediately in the first out-of-distribution evaluation.</li>
                <li>Multi-proxy systems can reduce gaps when proxies have independent failure modes and complementary information, but show correlated failures when proxies share systematic biases (e.g., all computational, all trained on same data).</li>
                <li>Uncertainty quantification is more effective at reducing proxy-to-ground-truth gaps when epistemic uncertainty dominates aleatoric uncertainty, proxy errors are due to model limitations rather than fundamental unpredictability, and sufficient perturbation/ensemble diversity exists. UQ is less effective for self-evaluation in LLMs, chaotic systems, and systematic biases shared across ensemble members.</li>
                <li>Proxy design failures that amplify gaps include: proxies with narrow dynamic range that saturate or cluster, proxies that capture only a small fraction of relevant information, proxies misaligned with actual objectives, and proxies that are overoptimistic due to post-hoc ground-truth knowledge.</li>
                <li>Economic and cultural factors amplify proxy-to-ground-truth gaps: gaps are larger when ground-truth validation is expensive relative to proxy evaluation, publication/deployment incentives favor proxy optimization over validation, domain lacks established experimental validation culture, or successful proxy optimization reduces availability of ground-truth labels.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>BigCloneBench semantic clone detection demonstrates extreme proxy-to-ground-truth gap: ML models achieve 99.8% precision and 93.7% F1 on BCB labels (proxy), but only 6.7% precision and 6.3% F1 when validated against strict human functional-similarity judgments (ground truth). The 93.35% false-positive rate in WT3/T4 labels shows proxy optimization producing highly misleading performance metrics, with models learning dataset artifacts rather than true semantic similarity. <a href="../results/extraction-result-2221.html#e2221.0" class="evidence-link">[e2221.0]</a> <a href="../results/extraction-result-2221.html#e2221.1" class="evidence-link">[e2221.1]</a> <a href="../results/extraction-result-2221.html#e2221.3" class="evidence-link">[e2221.3]</a> </li>
    <li>Industrial AOI quality inspection demonstrates large proxy-to-ground-truth gaps with standard ML metrics: models achieving 99.3% accuracy have 7.8% slip rates (vs 1% target), and 98.7% accuracy corresponds to 11.3% slip rate. Even requirement-aware metrics that meet targets on test data (RFC2: slip ~1.0%, VR ~89.7%) fail immediately in chronological evaluation slices, demonstrating temporal degradation distinct from static performance. <a href="../results/extraction-result-2222.html#e2222.0" class="evidence-link">[e2222.0]</a> <a href="../results/extraction-result-2222.html#e2222.1" class="evidence-link">[e2222.1]</a> </li>
    <li>DATADECIDE systematic evaluation quantifies proxy-to-ground-truth gaps in dataset selection: single-scale ranking achieves ~80% decision accuracy predicting 1B model performance from 150M experiments, leaving 20% error. Best scaling-law variants show 5.6-6.5% relative prediction error. Continuous likelihood proxies enable >80% predictability on some tasks at 0.01% of target compute but fail on others (math benchmarks), demonstrating domain-dependent proxy quality. <a href="../results/extraction-result-2223.html#e2223.0" class="evidence-link">[e2223.0]</a> <a href="../results/extraction-result-2223.html#e2223.1" class="evidence-link">[e2223.1]</a> <a href="../results/extraction-result-2223.html#e2223.2" class="evidence-link">[e2223.2]</a> <a href="../results/extraction-result-2223.html#e2223.3" class="evidence-link">[e2223.3]</a> </li>
    <li>Vision-language-action models show severe proxy-to-ground-truth gaps under domain shift: zero-shot Procgen evaluation yields Brier MAE >1.5-1.7 (near maximum of 2), macro recall 6-12%, and high invalid output rates (>80% for some models/datasets). Gap increases with image complexity (Shannon entropy correlation -0.409 for GPT-4o) and for sparse/timed special actions, supporting predictions about gaps increasing with novelty. <a href="../results/extraction-result-2225.html#e2225.0" class="evidence-link">[e2225.0]</a> <a href="../results/extraction-result-2225.html#e2225.1" class="evidence-link">[e2225.1]</a> <a href="../results/extraction-result-2225.html#e2225.2" class="evidence-link">[e2225.2]</a> <a href="../results/extraction-result-2225.html#e2225.3" class="evidence-link">[e2225.3]</a> <a href="../results/extraction-result-2225.html#e2225.4" class="evidence-link">[e2225.4]</a> </li>
    <li>Hemodynamics surrogate modeling demonstrates proxy-to-proxy cascade without experimental ground truth: ML models (DeepONet, DeepONet-SwinT) predict CFD fields with validation MNAE_u ~0.05-0.10, but CFD itself is a proxy for in-vivo hemodynamics with no experimental (PC-MRI, 4D flow) validation performed. CFD assumptions (steady-state, Newtonian, rigid walls) and boundary condition uncertainties create unquantified gaps to physiological reality. <a href="../results/extraction-result-2224.html#e2224.0" class="evidence-link">[e2224.0]</a> <a href="../results/extraction-result-2224.html#e2224.1" class="evidence-link">[e2224.1]</a> <a href="../results/extraction-result-2224.html#e2224.2" class="evidence-link">[e2224.2]</a> <a href="../results/extraction-result-2224.html#e2224.3" class="evidence-link">[e2224.3]</a> </li>
    <li>LLM uncertainty estimation shows substantial proxy-to-ground-truth gaps: baseline log-likelihood achieves only 55.41% AUROC on MATH500, while improved methods (TokUR) reach 80.64% AUROC - still leaving ~20% error in correctness prediction. Self-evaluation methods (P(True), LLM-Check, INSIDE) perform poorly (44-56% AUROC), and external methods (SE, SAR) also underperform, demonstrating that even sophisticated uncertainty quantification cannot eliminate gaps. <a href="../results/extraction-result-2228.html#e2228.0" class="evidence-link">[e2228.0]</a> <a href="../results/extraction-result-2228.html#e2228.1" class="evidence-link">[e2228.1]</a> <a href="../results/extraction-result-2228.html#e2228.2" class="evidence-link">[e2228.2]</a> <a href="../results/extraction-result-2228.html#e2228.3" class="evidence-link">[e2228.3]</a> <a href="../results/extraction-result-2228.html#e2228.4" class="evidence-link">[e2228.4]</a> <a href="../results/extraction-result-2228.html#e2228.5" class="evidence-link">[e2228.5]</a> <a href="../results/extraction-result-2228.html#e2228.6" class="evidence-link">[e2228.6]</a> <a href="../results/extraction-result-2228.html#e2228.7" class="evidence-link">[e2228.7]</a> <a href="../results/extraction-result-2228.html#e2228.8" class="evidence-link">[e2228.8]</a> <a href="../results/extraction-result-2228.html#e2228.9" class="evidence-link">[e2228.9]</a> </li>
    <li>Sports video action detection shows quantified proxy-to-ground-truth gap and partial reduction: TAAD proxy predictions achieve 43.56% precision and 66.97% recall vs ground-truth annotations. Adding denoising sequence transduction (DST) with game-state context improves to 78.77% precision and 75.80% recall (+35.21pp precision, +8.83pp recall), demonstrating that multifidelity approaches can reduce but not eliminate gaps. <a href="../results/extraction-result-2216.html#e2216.0" class="evidence-link">[e2216.0]</a> </li>
    <li>Causal structure learning (DAGSLAM) shows proxy improvement with appropriate loss functions: using mixed-type loss improves directed F1 from ~0.71 to 0.82 (50% categorical nodes) vs treating all as continuous. However, evaluation is simulation-only with no experimental causal validation, and performance degrades with increased categorical proportion, dense graphs, and small samples. <a href="../results/extraction-result-2227.html#e2227.0" class="evidence-link">[e2227.0]</a> <a href="../results/extraction-result-2227.html#e2227.1" class="evidence-link">[e2227.1]</a> <a href="../results/extraction-result-2227.html#e2227.2" class="evidence-link">[e2227.2]</a> <a href="../results/extraction-result-2227.html#e2227.3" class="evidence-link">[e2227.3]</a> <a href="../results/extraction-result-2227.html#e2227.4" class="evidence-link">[e2227.4]</a> <a href="../results/extraction-result-2227.html#e2227.5" class="evidence-link">[e2227.5]</a> <a href="../results/extraction-result-2227.html#e2227.6" class="evidence-link">[e2227.6]</a> </li>
    <li>Closed-loop experimental systems show better but imperfect translation: A-Lab autonomous materials synthesis achieves 71% experimental success rate (41 novel compounds from 58 targets), SAMPLE protein engineering discovers variants with ≥12°C higher stability, and MOBO-SPM achieves rapid convergence (~10 exploration steps). The 29% failure rate in A-Lab still represents a substantial proxy-to-ground-truth gap. <a href="../results/extraction-result-2218.html#e2218.3" class="evidence-link">[e2218.3]</a> <a href="../results/extraction-result-2218.html#e2218.1" class="evidence-link">[e2218.1]</a> <a href="../results/extraction-result-2217.html#e2217.0" class="evidence-link">[e2217.0]</a> <a href="../results/extraction-result-2217.html#e2217.1" class="evidence-link">[e2217.1]</a> <a href="../results/extraction-result-2217.html#e2217.2" class="evidence-link">[e2217.2]</a> </li>
    <li>Autonomous agent systems show proxy-to-ground-truth gaps in biological discovery: BioDiscoveryAgent reports +21% improvement vs Bayesian optimization baselines but lacks explicit proxy-ground-truth calibration metrics. DrugAgent achieves PAMPA F1 ~0.92 on held-out assay labels but no prospective experimental validation. Virtual Lab produces >90% expression rate but only 2 of 92 candidates show improved binding, demonstrating accumulation of unvalidated discoveries. <a href="../results/extraction-result-2218.html#e2218.0" class="evidence-link">[e2218.0]</a> <a href="../results/extraction-result-2218.html#e2218.2" class="evidence-link">[e2218.2]</a> <a href="../results/extraction-result-2218.html#e2218.4" class="evidence-link">[e2218.4]</a> </li>
    <li>Product attribute value identification (MVP-RAG) shows cascade error propagation: when true attribute value appears in retrieved candidates, generation F1 reaches 92.6% vs 86.3% when absent (6.3pp gap). Overall system achieves 89.5% F1, with improvements over baselines demonstrating partial gap reduction through multi-level retrieval. <a href="../results/extraction-result-2226.html#e2226.0" class="evidence-link">[e2226.0]</a> <a href="../results/extraction-result-2226.html#e2226.1" class="evidence-link">[e2226.1]</a> <a href="../results/extraction-result-2226.html#e2226.2" class="evidence-link">[e2226.2]</a> </li>
    <li>AgentCompass evaluation framework shows moderate proxy-to-ground-truth correlation: Pearson ρ = 0.430 (GAIA) and 0.408 (SWE Bench) between system quality scores and human scores. System identifies errors outside benchmark taxonomy (Safety & Security, Reflection Gaps), showing proxy-to-ground-truth mismatch increases when discovering novel error classes. <a href="../results/extraction-result-2215.html#e2215.0" class="evidence-link">[e2215.0]</a> </li>
    <li>Clinical prediction models achieve high cross-validation metrics (MLP: AUC 0.901, accuracy 0.8751) but lack independent external validation or prospective clinical validation. Multi-branch CNN with attention (MBC-ATT) for EEG/fNIRS fusion achieves 98.13% accuracy but validation is against behavioral task labels only with no independent physiological ground truth. <a href="../results/extraction-result-2220.html#e2220.0" class="evidence-link">[e2220.0]</a> <a href="../results/extraction-result-2220.html#e2220.1" class="evidence-link">[e2220.1]</a> <a href="../results/extraction-result-2220.html#e2220.2" class="evidence-link">[e2220.2]</a> <a href="../results/extraction-result-2219.html#e2219.0" class="evidence-link">[e2219.0]</a> <a href="../results/extraction-result-2219.html#e2219.1" class="evidence-link">[e2219.1]</a> <a href="../results/extraction-result-2219.html#e2219.2" class="evidence-link">[e2219.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Systems that incorporate explicit proxy-bias correction (like Gemini, DST) will show 20-50% smaller gaps between computational and experimental success rates than systems without such correction, measured by comparing false positive rates or success rates on held-out experimental validation sets.</li>
                <li>Closed-loop experimental systems with feedback cycles <1 day will show 2-3× higher experimental success rates (50-80%) than batch systems with feedback cycles >1 week (20-40%) in the same domain, measured by fraction of computationally-selected candidates that pass experimental validation.</li>
                <li>Temporal validation on chronologically-split data will reveal 2-5× larger proxy-to-ground-truth gaps than random-split validation in domains with evolving distributions, measured by comparing proxy metric performance to ground-truth outcomes on future vs randomly-held-out data.</li>
                <li>Multi-proxy systems using 3+ orthogonal proxies (e.g., physics-based + data-driven + expert heuristics) will show 30-60% smaller gaps than single-proxy systems when proxies have independent failure modes (correlation r<0.5), but <10% improvement when proxies are highly correlated (r>0.8), measured by experimental validation success rates.</li>
                <li>Requirement-aware metrics aligned with actual validation objectives will show 20-40% better correlation (measured by Pearson r or Spearman ρ) with ground-truth outcomes than standard ML metrics (accuracy, F1, AUC) in domains with asymmetric costs or specific operational constraints.</li>
                <li>Uncertainty quantification methods that decompose epistemic and aleatoric uncertainty will improve correctness prediction by 15-30 percentage points AUROC over baseline likelihood methods in reasoning tasks, but <5 percentage points in tasks dominated by aleatoric uncertainty.</li>
                <li>Proxy metrics with narrow dynamic range (<20% of theoretical range utilized) or high clustering (>50% of values within 10% of mode) will show 2-4× larger proxy-to-ground-truth gaps than proxies with broad, continuous distributions, measured by correlation between proxy rankings and ground-truth outcomes.</li>
                <li>Validation cascades with 3+ stages will show cumulative error rates 1.5-3× higher than single-stage validation, with early-stage errors (e.g., retrieval failures) creating bottlenecks that limit downstream performance by 20-50%, measured by comparing end-to-end success rates to individual stage success rates.</li>
                <li>Domains with established experimental validation culture (>50% of papers include experimental validation) will show 2-3× smaller proxy-to-ground-truth gaps than computationally-mature domains lacking validation culture, measured by comparing proxy metric performance to experimental outcomes across published studies.</li>
                <li>In domains where proxy evaluation costs <1% of ground-truth validation costs, >80% of published discoveries will lack experimental validation, and among those that are eventually validated, false positive rates will exceed 40%.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether machine learning meta-models could be trained to predict the proxy-to-ground-truth gap for individual discoveries with sufficient accuracy (e.g., R²>0.7, calibrated uncertainty estimates) to enable reliable prioritization of experimental validation, or whether the gap is fundamentally unpredictable due to the novelty of discoveries.</li>
                <li>Whether the proxy-to-ground-truth gap could be eliminated entirely (reduced to <5% error rates) through sufficiently sophisticated simulation and modeling, or whether there are fundamental limits (e.g., computational complexity, chaotic dynamics, emergent phenomena) that prevent perfect proxy-ground-truth alignment.</li>
                <li>Whether certain types of discoveries might show smaller proxy-to-ground-truth gaps when involving greater extrapolation, potentially due to being more amenable to first-principles modeling or having clearer validation criteria, contradicting the general trend.</li>
                <li>Whether active learning strategies that explicitly optimize for reducing the proxy-to-ground-truth gap (rather than optimizing the proxy itself) could achieve order-of-magnitude improvements (>10×) in experimental validation success rates.</li>
                <li>Whether the gap could be reduced by orders of magnitude through hybrid approaches that combine multiple orthogonal proxies with provably independent failure modes, or whether all proxies in a domain share fundamental limitations that cause correlated failures.</li>
                <li>Whether the proxy-to-ground-truth gap follows universal scaling laws across domains (e.g., power-law relationships with training data size, model capacity, or domain maturity), or whether each domain has fundamentally different gap characteristics that prevent cross-domain learning.</li>
                <li>Whether closed-loop systems with sufficiently rapid feedback (<1 hour cycles) could achieve near-perfect proxy-to-ground-truth alignment (>95% success rates) through continuous refinement, or whether fundamental limitations in proxy design prevent such alignment even with rapid iteration.</li>
                <li>Whether temporal degradation of proxy-to-ground-truth alignment could be predicted and corrected through online learning and adaptive recalibration with <10% additional validation cost, or whether distributional shifts are fundamentally unpredictable and require periodic full revalidation.</li>
                <li>Whether economic incentives could be restructured (e.g., through publication requirements, funding mechanisms, regulatory frameworks) to reduce the systematic bias toward proxy optimization by >50%, or whether cost differentials make such restructuring impractical.</li>
                <li>Whether the proxy-to-ground-truth gap for truly transformational discoveries (>3 standard deviations from training distribution) is 2-5× larger than for incremental improvements as predicted, or whether transformational discoveries show different gap characteristics due to being more amenable to first-principles reasoning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where proxy metrics perfectly predict experimental outcomes (R²>0.95, false positive rate <5%) across both incremental and extrapolative discoveries would challenge the universality of the gap.</li>
                <li>Demonstrating that discoveries involving greater extrapolation consistently show smaller proxy-to-ground-truth gaps than incremental ones across multiple domains would contradict the core theory prediction.</li>
                <li>Showing that systems without proxy-bias correction perform as well as those with correction in experimental validation (within 10% success rate) across multiple domains would question the value of gap-reduction methods.</li>
                <li>Finding that the proxy-to-ground-truth gap does not increase with extrapolation distance from training data (measured by distribution shift metrics) would challenge the mechanism proposed for why extrapolative discoveries show larger gaps.</li>
                <li>Demonstrating that purely data-driven proxies perform as well as physics-based proxies in novel regimes (>2 standard deviations from training data) would contradict the theory's prediction about proxy quality dependence on physical grounding.</li>
                <li>Showing that the economic incentive to defer validation does not lead to accumulation of unvalidated discoveries (e.g., if false positives are naturally filtered by other mechanisms at rates >80%) would challenge the theory's prediction about systemic bias.</li>
                <li>Finding that closed-loop and batch systems show identical proxy-to-ground-truth gaps when controlling for total experimental budget and domain would contradict the prediction about feedback architecture effects.</li>
                <li>Demonstrating that temporal validation shows no larger gaps than random-split validation across multiple domains with documented evolving distributions would challenge the theory's emphasis on temporal/distributional shifts.</li>
                <li>Showing that multi-proxy ensembles consistently perform worse than single best proxies even when proxies have low correlation (r<0.3) would contradict predictions about complementary information reducing gaps.</li>
                <li>Finding that uncertainty quantification provides no improvement (within 5% AUROC) in proxy-to-ground-truth alignment across any domain or task type would challenge the theory's predictions about UQ effectiveness.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Formal methods for analyzing and predicting failure mode correlations across multiple proxies, including whether correlations can be estimated a priori from proxy design features. </li>
    <li>The psychological and sociological factors that influence acceptance of proxy-validated discoveries in scientific communities, including publication bias, peer review dynamics, and career incentives that may amplify or mitigate gaps. </li>
    <li>Whether certain discovery types (e.g., negative results, null findings, replication studies) show systematically different proxy-to-ground-truth gap characteristics than positive discoveries. </li>
    <li>The impact of publication bias and selective reporting on the observed proxy-to-ground-truth gap in the literature, including whether published gaps underestimate true gaps due to selective reporting of successful validations. </li>
    <li>Optimal feedback cycle timing for closed-loop systems and whether there are diminishing returns or optimal frequencies that vary by domain, discovery type, or experimental cost structure. </li>
    <li>Whether proxy-to-ground-truth gaps can be predicted a priori from domain characteristics (e.g., measurement noise, class imbalance, temporal stability), proxy design features (e.g., dynamic range, information content), and system architecture (e.g., cascade depth, feedback frequency), enabling proactive gap mitigation. </li>
    <li>The role of interpretability and explainability in reducing proxy-to-ground-truth gaps, including whether understanding proxy failure modes through interpretability methods enables more effective gap reduction strategies. </li>
    <li>How to design proxies that maintain calibration under distributional shift, including whether adaptive recalibration methods (e.g., online learning, domain adaptation) can prevent temporal degradation without requiring full revalidation. </li>
    <li>The relationship between proxy-to-ground-truth gaps and scientific reproducibility, including whether large gaps contribute to replication failures and whether gap-reduction methods improve reproducibility. </li>
    <li>Whether regulatory frameworks or standardization efforts could reduce proxy-to-ground-truth gaps through mandatory validation requirements, best-practice guidelines, or incentive structures, and what the optimal design of such frameworks would be. </li>
    <li>The role of data quality, measurement noise, and label reliability in determining proxy-to-ground-truth gaps, including whether improving ground-truth quality could paradoxically increase observed gaps by revealing previously hidden proxy failures. </li>
    <li>How gaps evolve as computational methods improve over time, including whether there are diminishing returns, whether improvements in one aspect (e.g., model capacity) can compensate for limitations in others (e.g., training data coverage), and whether fundamental limits exist. </li>
    <li>The interaction between proxy-to-ground-truth gaps and other sources of error in automated discovery systems (e.g., optimization failures, implementation bugs, hyperparameter sensitivity), including whether these errors compound or partially cancel. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> unknown</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Proxy-to-Ground-Truth Gap Theory (Revised)",
    "type": "general",
    "theory_description": "Automated discovery systems frequently optimize proxy metrics (computational predictions, surrogate objectives, simulated outcomes, or intermediate validation measures) rather than ground-truth experimental outcomes. The gap between proxy performance and ground-truth validation creates a systematic bias where systems appear more successful in computational or simulated evaluation than in real-world validation. This gap varies systematically with: (1) the novelty and extrapolation distance of the discovery from training distributions, (2) the maturity and physical grounding of the proxy metric, (3) the domain's amenability to computational modeling, (4) temporal and distributional shifts between training and deployment, and (5) the validation architecture (closed-loop experimental feedback vs batch proxy-then-validate workflows). Closed-loop systems with tight experimental feedback can achieve smaller effective gaps through iterative refinement, though gaps are not eliminated. The computational cost advantage of proxy evaluation creates economic incentives to defer ground-truth validation, leading to accumulation of unvalidated discoveries. Most evidence pertains to incremental-to-moderate extrapolation; predictions about truly transformational discoveries remain partially tested.",
    "supporting_evidence": [
        {
            "text": "BigCloneBench semantic clone detection demonstrates extreme proxy-to-ground-truth gap: ML models achieve 99.8% precision and 93.7% F1 on BCB labels (proxy), but only 6.7% precision and 6.3% F1 when validated against strict human functional-similarity judgments (ground truth). The 93.35% false-positive rate in WT3/T4 labels shows proxy optimization producing highly misleading performance metrics, with models learning dataset artifacts rather than true semantic similarity.",
            "uuids": [
                "e2221.0",
                "e2221.1",
                "e2221.3"
            ]
        },
        {
            "text": "Industrial AOI quality inspection demonstrates large proxy-to-ground-truth gaps with standard ML metrics: models achieving 99.3% accuracy have 7.8% slip rates (vs 1% target), and 98.7% accuracy corresponds to 11.3% slip rate. Even requirement-aware metrics that meet targets on test data (RFC2: slip ~1.0%, VR ~89.7%) fail immediately in chronological evaluation slices, demonstrating temporal degradation distinct from static performance.",
            "uuids": [
                "e2222.0",
                "e2222.1"
            ]
        },
        {
            "text": "DATADECIDE systematic evaluation quantifies proxy-to-ground-truth gaps in dataset selection: single-scale ranking achieves ~80% decision accuracy predicting 1B model performance from 150M experiments, leaving 20% error. Best scaling-law variants show 5.6-6.5% relative prediction error. Continuous likelihood proxies enable &gt;80% predictability on some tasks at 0.01% of target compute but fail on others (math benchmarks), demonstrating domain-dependent proxy quality.",
            "uuids": [
                "e2223.0",
                "e2223.1",
                "e2223.2",
                "e2223.3"
            ]
        },
        {
            "text": "Vision-language-action models show severe proxy-to-ground-truth gaps under domain shift: zero-shot Procgen evaluation yields Brier MAE &gt;1.5-1.7 (near maximum of 2), macro recall 6-12%, and high invalid output rates (&gt;80% for some models/datasets). Gap increases with image complexity (Shannon entropy correlation -0.409 for GPT-4o) and for sparse/timed special actions, supporting predictions about gaps increasing with novelty.",
            "uuids": [
                "e2225.0",
                "e2225.1",
                "e2225.2",
                "e2225.3",
                "e2225.4"
            ]
        },
        {
            "text": "Hemodynamics surrogate modeling demonstrates proxy-to-proxy cascade without experimental ground truth: ML models (DeepONet, DeepONet-SwinT) predict CFD fields with validation MNAE_u ~0.05-0.10, but CFD itself is a proxy for in-vivo hemodynamics with no experimental (PC-MRI, 4D flow) validation performed. CFD assumptions (steady-state, Newtonian, rigid walls) and boundary condition uncertainties create unquantified gaps to physiological reality.",
            "uuids": [
                "e2224.0",
                "e2224.1",
                "e2224.2",
                "e2224.3"
            ]
        },
        {
            "text": "LLM uncertainty estimation shows substantial proxy-to-ground-truth gaps: baseline log-likelihood achieves only 55.41% AUROC on MATH500, while improved methods (TokUR) reach 80.64% AUROC - still leaving ~20% error in correctness prediction. Self-evaluation methods (P(True), LLM-Check, INSIDE) perform poorly (44-56% AUROC), and external methods (SE, SAR) also underperform, demonstrating that even sophisticated uncertainty quantification cannot eliminate gaps.",
            "uuids": [
                "e2228.0",
                "e2228.1",
                "e2228.2",
                "e2228.3",
                "e2228.4",
                "e2228.5",
                "e2228.6",
                "e2228.7",
                "e2228.8",
                "e2228.9"
            ]
        },
        {
            "text": "Sports video action detection shows quantified proxy-to-ground-truth gap and partial reduction: TAAD proxy predictions achieve 43.56% precision and 66.97% recall vs ground-truth annotations. Adding denoising sequence transduction (DST) with game-state context improves to 78.77% precision and 75.80% recall (+35.21pp precision, +8.83pp recall), demonstrating that multifidelity approaches can reduce but not eliminate gaps.",
            "uuids": [
                "e2216.0"
            ]
        },
        {
            "text": "Causal structure learning (DAGSLAM) shows proxy improvement with appropriate loss functions: using mixed-type loss improves directed F1 from ~0.71 to 0.82 (50% categorical nodes) vs treating all as continuous. However, evaluation is simulation-only with no experimental causal validation, and performance degrades with increased categorical proportion, dense graphs, and small samples.",
            "uuids": [
                "e2227.0",
                "e2227.1",
                "e2227.2",
                "e2227.3",
                "e2227.4",
                "e2227.5",
                "e2227.6"
            ]
        },
        {
            "text": "Closed-loop experimental systems show better but imperfect translation: A-Lab autonomous materials synthesis achieves 71% experimental success rate (41 novel compounds from 58 targets), SAMPLE protein engineering discovers variants with ≥12°C higher stability, and MOBO-SPM achieves rapid convergence (~10 exploration steps). The 29% failure rate in A-Lab still represents a substantial proxy-to-ground-truth gap.",
            "uuids": [
                "e2218.3",
                "e2218.1",
                "e2217.0",
                "e2217.1",
                "e2217.2"
            ]
        },
        {
            "text": "Autonomous agent systems show proxy-to-ground-truth gaps in biological discovery: BioDiscoveryAgent reports +21% improvement vs Bayesian optimization baselines but lacks explicit proxy-ground-truth calibration metrics. DrugAgent achieves PAMPA F1 ~0.92 on held-out assay labels but no prospective experimental validation. Virtual Lab produces &gt;90% expression rate but only 2 of 92 candidates show improved binding, demonstrating accumulation of unvalidated discoveries.",
            "uuids": [
                "e2218.0",
                "e2218.2",
                "e2218.4"
            ]
        },
        {
            "text": "Product attribute value identification (MVP-RAG) shows cascade error propagation: when true attribute value appears in retrieved candidates, generation F1 reaches 92.6% vs 86.3% when absent (6.3pp gap). Overall system achieves 89.5% F1, with improvements over baselines demonstrating partial gap reduction through multi-level retrieval.",
            "uuids": [
                "e2226.0",
                "e2226.1",
                "e2226.2"
            ]
        },
        {
            "text": "AgentCompass evaluation framework shows moderate proxy-to-ground-truth correlation: Pearson ρ = 0.430 (GAIA) and 0.408 (SWE Bench) between system quality scores and human scores. System identifies errors outside benchmark taxonomy (Safety & Security, Reflection Gaps), showing proxy-to-ground-truth mismatch increases when discovering novel error classes.",
            "uuids": [
                "e2215.0"
            ]
        },
        {
            "text": "Clinical prediction models achieve high cross-validation metrics (MLP: AUC 0.901, accuracy 0.8751) but lack independent external validation or prospective clinical validation. Multi-branch CNN with attention (MBC-ATT) for EEG/fNIRS fusion achieves 98.13% accuracy but validation is against behavioral task labels only with no independent physiological ground truth.",
            "uuids": [
                "e2220.0",
                "e2220.1",
                "e2220.2",
                "e2219.0",
                "e2219.1",
                "e2219.2"
            ]
        }
    ],
    "theory_statements": [
        "Automated systems systematically overestimate discovery success when evaluated on proxy metrics compared to ground-truth experimental validation, with the overestimation increasing as the proxy becomes more removed from ground truth.",
        "The proxy-to-ground-truth gap increases with the novelty and extrapolation distance of the discovery from training data, because proxies are typically calibrated on known regimes and become less reliable in novel regimes.",
        "Systems optimizing proxy metrics will exhibit higher false positive rates for discoveries involving greater extrapolation, because extrapolative discoveries involve regimes where proxy calibration is weakest.",
        "Multifidelity approaches that explicitly model and correct proxy biases can reduce but not eliminate the proxy-to-ground-truth gap, with typical reductions of 20-50% in error rates but substantial residual gaps remaining.",
        "The computational cost advantage of proxy evaluation creates an economic incentive to defer ground-truth validation, leading to accumulation of unvalidated discoveries and potential publication of false positives.",
        "The quality of a proxy metric depends on: (1) the physical/theoretical grounding of the proxy, (2) the domain's amenability to computational modeling, (3) the calibration data available in the regime of interest, (4) the dynamic range and continuous variation of the proxy signal, and (5) alignment with actual validation objectives.",
        "Domains with mature physics-based simulations show smaller proxy-to-ground-truth gaps than domains with purely empirical proxies, but computational maturity alone is insufficient without experimental validation culture.",
        "The gap manifests across validation cascades (proxy → intermediate validation → ground truth) with error accumulation at each stage, where early-stage errors create bottlenecks that limit downstream performance.",
        "Human expertise in proxy design can reduce but not eliminate the gap, as even expert-designed proxies fail in extrapolative regimes and when key aspects are not captured.",
        "The temporal evolution of proxy quality follows domain maturity: as domains mature and more validation data accumulates, proxies improve but never perfectly match ground truth in novel regimes, and temporal/distributional shifts can cause immediate degradation even when initial alignment appears good.",
        "Closed-loop experimental systems with tight, rapid feedback enabling iterative proxy refinement show smaller effective proxy-to-ground-truth gaps (typically 70-90% success rates) compared to batch proxy-optimization systems with delayed validation (typically 10-50% success rates), though gaps are not eliminated in either case.",
        "Temporal and distributional shifts cause proxy-to-ground-truth alignment to degrade even when initial test-set performance appears good, with degradation often occurring immediately in the first out-of-distribution evaluation.",
        "Multi-proxy systems can reduce gaps when proxies have independent failure modes and complementary information, but show correlated failures when proxies share systematic biases (e.g., all computational, all trained on same data).",
        "Uncertainty quantification is more effective at reducing proxy-to-ground-truth gaps when epistemic uncertainty dominates aleatoric uncertainty, proxy errors are due to model limitations rather than fundamental unpredictability, and sufficient perturbation/ensemble diversity exists. UQ is less effective for self-evaluation in LLMs, chaotic systems, and systematic biases shared across ensemble members.",
        "Proxy design failures that amplify gaps include: proxies with narrow dynamic range that saturate or cluster, proxies that capture only a small fraction of relevant information, proxies misaligned with actual objectives, and proxies that are overoptimistic due to post-hoc ground-truth knowledge.",
        "Economic and cultural factors amplify proxy-to-ground-truth gaps: gaps are larger when ground-truth validation is expensive relative to proxy evaluation, publication/deployment incentives favor proxy optimization over validation, domain lacks established experimental validation culture, or successful proxy optimization reduces availability of ground-truth labels."
    ],
    "new_predictions_likely": [
        "Systems that incorporate explicit proxy-bias correction (like Gemini, DST) will show 20-50% smaller gaps between computational and experimental success rates than systems without such correction, measured by comparing false positive rates or success rates on held-out experimental validation sets.",
        "Closed-loop experimental systems with feedback cycles &lt;1 day will show 2-3× higher experimental success rates (50-80%) than batch systems with feedback cycles &gt;1 week (20-40%) in the same domain, measured by fraction of computationally-selected candidates that pass experimental validation.",
        "Temporal validation on chronologically-split data will reveal 2-5× larger proxy-to-ground-truth gaps than random-split validation in domains with evolving distributions, measured by comparing proxy metric performance to ground-truth outcomes on future vs randomly-held-out data.",
        "Multi-proxy systems using 3+ orthogonal proxies (e.g., physics-based + data-driven + expert heuristics) will show 30-60% smaller gaps than single-proxy systems when proxies have independent failure modes (correlation r&lt;0.5), but &lt;10% improvement when proxies are highly correlated (r&gt;0.8), measured by experimental validation success rates.",
        "Requirement-aware metrics aligned with actual validation objectives will show 20-40% better correlation (measured by Pearson r or Spearman ρ) with ground-truth outcomes than standard ML metrics (accuracy, F1, AUC) in domains with asymmetric costs or specific operational constraints.",
        "Uncertainty quantification methods that decompose epistemic and aleatoric uncertainty will improve correctness prediction by 15-30 percentage points AUROC over baseline likelihood methods in reasoning tasks, but &lt;5 percentage points in tasks dominated by aleatoric uncertainty.",
        "Proxy metrics with narrow dynamic range (&lt;20% of theoretical range utilized) or high clustering (&gt;50% of values within 10% of mode) will show 2-4× larger proxy-to-ground-truth gaps than proxies with broad, continuous distributions, measured by correlation between proxy rankings and ground-truth outcomes.",
        "Validation cascades with 3+ stages will show cumulative error rates 1.5-3× higher than single-stage validation, with early-stage errors (e.g., retrieval failures) creating bottlenecks that limit downstream performance by 20-50%, measured by comparing end-to-end success rates to individual stage success rates.",
        "Domains with established experimental validation culture (&gt;50% of papers include experimental validation) will show 2-3× smaller proxy-to-ground-truth gaps than computationally-mature domains lacking validation culture, measured by comparing proxy metric performance to experimental outcomes across published studies.",
        "In domains where proxy evaluation costs &lt;1% of ground-truth validation costs, &gt;80% of published discoveries will lack experimental validation, and among those that are eventually validated, false positive rates will exceed 40%."
    ],
    "new_predictions_unknown": [
        "Whether machine learning meta-models could be trained to predict the proxy-to-ground-truth gap for individual discoveries with sufficient accuracy (e.g., R²&gt;0.7, calibrated uncertainty estimates) to enable reliable prioritization of experimental validation, or whether the gap is fundamentally unpredictable due to the novelty of discoveries.",
        "Whether the proxy-to-ground-truth gap could be eliminated entirely (reduced to &lt;5% error rates) through sufficiently sophisticated simulation and modeling, or whether there are fundamental limits (e.g., computational complexity, chaotic dynamics, emergent phenomena) that prevent perfect proxy-ground-truth alignment.",
        "Whether certain types of discoveries might show smaller proxy-to-ground-truth gaps when involving greater extrapolation, potentially due to being more amenable to first-principles modeling or having clearer validation criteria, contradicting the general trend.",
        "Whether active learning strategies that explicitly optimize for reducing the proxy-to-ground-truth gap (rather than optimizing the proxy itself) could achieve order-of-magnitude improvements (&gt;10×) in experimental validation success rates.",
        "Whether the gap could be reduced by orders of magnitude through hybrid approaches that combine multiple orthogonal proxies with provably independent failure modes, or whether all proxies in a domain share fundamental limitations that cause correlated failures.",
        "Whether the proxy-to-ground-truth gap follows universal scaling laws across domains (e.g., power-law relationships with training data size, model capacity, or domain maturity), or whether each domain has fundamentally different gap characteristics that prevent cross-domain learning.",
        "Whether closed-loop systems with sufficiently rapid feedback (&lt;1 hour cycles) could achieve near-perfect proxy-to-ground-truth alignment (&gt;95% success rates) through continuous refinement, or whether fundamental limitations in proxy design prevent such alignment even with rapid iteration.",
        "Whether temporal degradation of proxy-to-ground-truth alignment could be predicted and corrected through online learning and adaptive recalibration with &lt;10% additional validation cost, or whether distributional shifts are fundamentally unpredictable and require periodic full revalidation.",
        "Whether economic incentives could be restructured (e.g., through publication requirements, funding mechanisms, regulatory frameworks) to reduce the systematic bias toward proxy optimization by &gt;50%, or whether cost differentials make such restructuring impractical.",
        "Whether the proxy-to-ground-truth gap for truly transformational discoveries (&gt;3 standard deviations from training distribution) is 2-5× larger than for incremental improvements as predicted, or whether transformational discoveries show different gap characteristics due to being more amenable to first-principles reasoning."
    ],
    "negative_experiments": [
        "Finding domains where proxy metrics perfectly predict experimental outcomes (R²&gt;0.95, false positive rate &lt;5%) across both incremental and extrapolative discoveries would challenge the universality of the gap.",
        "Demonstrating that discoveries involving greater extrapolation consistently show smaller proxy-to-ground-truth gaps than incremental ones across multiple domains would contradict the core theory prediction.",
        "Showing that systems without proxy-bias correction perform as well as those with correction in experimental validation (within 10% success rate) across multiple domains would question the value of gap-reduction methods.",
        "Finding that the proxy-to-ground-truth gap does not increase with extrapolation distance from training data (measured by distribution shift metrics) would challenge the mechanism proposed for why extrapolative discoveries show larger gaps.",
        "Demonstrating that purely data-driven proxies perform as well as physics-based proxies in novel regimes (&gt;2 standard deviations from training data) would contradict the theory's prediction about proxy quality dependence on physical grounding.",
        "Showing that the economic incentive to defer validation does not lead to accumulation of unvalidated discoveries (e.g., if false positives are naturally filtered by other mechanisms at rates &gt;80%) would challenge the theory's prediction about systemic bias.",
        "Finding that closed-loop and batch systems show identical proxy-to-ground-truth gaps when controlling for total experimental budget and domain would contradict the prediction about feedback architecture effects.",
        "Demonstrating that temporal validation shows no larger gaps than random-split validation across multiple domains with documented evolving distributions would challenge the theory's emphasis on temporal/distributional shifts.",
        "Showing that multi-proxy ensembles consistently perform worse than single best proxies even when proxies have low correlation (r&lt;0.3) would contradict predictions about complementary information reducing gaps.",
        "Finding that uncertainty quantification provides no improvement (within 5% AUROC) in proxy-to-ground-truth alignment across any domain or task type would challenge the theory's predictions about UQ effectiveness."
    ],
    "unaccounted_for": [
        {
            "text": "Formal methods for analyzing and predicting failure mode correlations across multiple proxies, including whether correlations can be estimated a priori from proxy design features.",
            "uuids": []
        },
        {
            "text": "The psychological and sociological factors that influence acceptance of proxy-validated discoveries in scientific communities, including publication bias, peer review dynamics, and career incentives that may amplify or mitigate gaps.",
            "uuids": []
        },
        {
            "text": "Whether certain discovery types (e.g., negative results, null findings, replication studies) show systematically different proxy-to-ground-truth gap characteristics than positive discoveries.",
            "uuids": []
        },
        {
            "text": "The impact of publication bias and selective reporting on the observed proxy-to-ground-truth gap in the literature, including whether published gaps underestimate true gaps due to selective reporting of successful validations.",
            "uuids": []
        },
        {
            "text": "Optimal feedback cycle timing for closed-loop systems and whether there are diminishing returns or optimal frequencies that vary by domain, discovery type, or experimental cost structure.",
            "uuids": []
        },
        {
            "text": "Whether proxy-to-ground-truth gaps can be predicted a priori from domain characteristics (e.g., measurement noise, class imbalance, temporal stability), proxy design features (e.g., dynamic range, information content), and system architecture (e.g., cascade depth, feedback frequency), enabling proactive gap mitigation.",
            "uuids": []
        },
        {
            "text": "The role of interpretability and explainability in reducing proxy-to-ground-truth gaps, including whether understanding proxy failure modes through interpretability methods enables more effective gap reduction strategies.",
            "uuids": []
        },
        {
            "text": "How to design proxies that maintain calibration under distributional shift, including whether adaptive recalibration methods (e.g., online learning, domain adaptation) can prevent temporal degradation without requiring full revalidation.",
            "uuids": []
        },
        {
            "text": "The relationship between proxy-to-ground-truth gaps and scientific reproducibility, including whether large gaps contribute to replication failures and whether gap-reduction methods improve reproducibility.",
            "uuids": []
        },
        {
            "text": "Whether regulatory frameworks or standardization efforts could reduce proxy-to-ground-truth gaps through mandatory validation requirements, best-practice guidelines, or incentive structures, and what the optimal design of such frameworks would be.",
            "uuids": []
        },
        {
            "text": "The role of data quality, measurement noise, and label reliability in determining proxy-to-ground-truth gaps, including whether improving ground-truth quality could paradoxically increase observed gaps by revealing previously hidden proxy failures.",
            "uuids": []
        },
        {
            "text": "How gaps evolve as computational methods improve over time, including whether there are diminishing returns, whether improvements in one aspect (e.g., model capacity) can compensate for limitations in others (e.g., training data coverage), and whether fundamental limits exist.",
            "uuids": []
        },
        {
            "text": "The interaction between proxy-to-ground-truth gaps and other sources of error in automated discovery systems (e.g., optimization failures, implementation bugs, hyperparameter sensitivity), including whether these errors compound or partially cancel.",
            "uuids": []
        }
    ],
    "change_log": [
        "Revised theory description to be more concise while retaining key factors (novelty, maturity, modeling amenability, temporal shifts, validation architecture); added explicit note that most evidence pertains to incremental-to-moderate extrapolation.",
        "Added theory statement distinguishing closed-loop experimental systems (with tight, rapid feedback) from batch proxy-optimization systems (with delayed validation), predicting smaller effective gaps for closed-loop systems (statement 11).",
        "Added theory statement about temporal and distributional shifts as a critical dimension of proxy-to-ground-truth gap, predicting immediate degradation under temporal drift (statement 12).",
        "Added theory statement about multi-proxy systems, predicting gap reduction when proxies have independent failure modes but correlated failures when proxies share systematic biases (statement 13).",
        "Added theory statement specifying conditions under which uncertainty quantification is more vs less effective at reducing proxy-to-ground-truth gaps (statement 14).",
        "Added theory statement about proxy design failures that amplify gaps, including narrow dynamic range, limited information capture, misalignment with objectives, and overoptimistic post-hoc metrics (statement 15).",
        "Added theory statement about economic and cultural factors amplifying proxy-to-ground-truth gaps (statement 16).",
        "Modified statement 3 to remove specific reference to 'transformational discoveries' and instead refer to 'discoveries involving greater extrapolation' to better match the evidence base.",
        "Modified statement 4 to specify typical reduction ranges (20-50%) for multifidelity approaches and emphasize substantial residual gaps remain.",
        "Modified statement 7 to emphasize that computational maturity alone is insufficient and experimental validation culture is critical.",
        "Modified statement 8 to clarify cascade error propagation and bottleneck effects.",
        "Reorganized supporting evidence into thematic groups: extreme gaps (BigCloneBench, AOI), quantified gaps (DATADECIDE, VLA, LLM), cascades without ground truth (hemodynamics), partial gap reduction (sports video, causal learning), closed-loop success (A-Lab, SAMPLE, MOBO-SPM), unvalidated discoveries (autonomous agents), cascade propagation (MVP-RAG), novel error classes (AgentCompass), and missing validation (clinical, EEG/fNIRS).",
        "Updated new_predictions_likely to include 10 specific, testable predictions with quantified expected outcomes and explicit measurement methods covering closed-loop systems, temporal validation, multi-proxy systems, requirement-aware metrics, uncertainty quantification, proxy design, validation cascades, domain culture, and validation economics.",
        "Updated new_predictions_unknown to include 10 predictions about fundamental questions with explicit uncertainty about outcomes, including meta-model predictability, elimination of gaps, extrapolation paradoxes, active learning effectiveness, universal scaling laws, rapid-feedback limits, temporal prediction, economic restructuring, and transformational vs incremental gaps.",
        "Updated negative_experiments to include 10 tests with specific quantitative thresholds covering universality, extrapolation mechanisms, correction effectiveness, physics-based vs data-driven, economic incentives, closed-loop vs batch, temporal vs random validation, multi-proxy ensembles, and uncertainty quantification.",
        "Refined unaccounted_for to focus on 13 items that are genuinely not addressed by current theory statements, removing items that are now covered and adding more specific framing about what is unknown (e.g., formal methods for failure mode correlation, optimal feedback timing, a priori gap prediction, interpretability role, adaptive calibration, reproducibility relationship, regulatory frameworks, data quality effects, computational improvement trajectories, error interaction).",
        "Added explicit measurement methods and quantitative thresholds to predictions to make them more testable and falsifiable.",
        "Clarified scope throughout to acknowledge that most evidence involves incremental-to-moderate extrapolation rather than truly transformational discoveries, while maintaining predictions about extrapolative cases."
    ]
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Fundamental Novelty-Feasibility Tension in Automated Hypothesis Generation - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-405</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-405</p>
                <p><strong>Name:</strong> The Fundamental Novelty-Feasibility Tension in Automated Hypothesis Generation</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of the fundamental trade-off between novelty and feasibility in automatically generated research hypotheses, including quantification methods and optimization strategies across different research domains and problem types, based on the following results.</p>
                <p><strong>Description:</strong> Automated hypothesis generation systems face an inherent tension between novelty (semantic distance from existing work, conceptual originality) and feasibility (likelihood of successful implementation, verification, and empirical validation). This tension arises because highly novel hypotheses often require untested combinations of concepts or methods that lack empirical validation, while highly feasible hypotheses tend to be incremental variations of established work. The tradeoff is not strictly inverse but exhibits domain-dependent and method-dependent characteristics, with correlation coefficients typically ranging from weak negative (r ~ -0.07) to moderate negative (r ~ -0.3). Hybrid approaches combining literature grounding with data-driven generation, multi-agent architectures, and human-in-the-loop curation can achieve better simultaneous optimization of both dimensions than pure single-strategy approaches. The tradeoff can be explicitly modeled through multi-objective optimization frameworks (e.g., Pareto optimization, weighted objectives, MDL-based complexity-accuracy tradeoffs) or implicitly managed through architectural choices and iterative refinement strategies.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>There exists a fundamental tension between novelty and feasibility in automated hypothesis generation, manifesting as a weak to moderate negative correlation (typically r ~ -0.07 to -0.3) rather than a strict inverse relationship.</li>
                <li>Novelty can be quantified through multiple complementary metrics: (1) semantic distance from existing work (embedding cosine similarity, typically threshold ~0.6-0.8), (2) human expert ratings (typically 1-10 Likert scales), (3) structural measures (graph distance, outlier detection in feature space), (4) n-gram overlap with prior work (lower overlap indicates higher novelty), and (5) automated LLM-based evaluation (0-3 or 1-10 scales with >0.7 correlation to human judgments).</li>
                <li>Feasibility can be quantified through: (1) verification success rates (percentage of hypotheses that pass empirical validation), (2) human expert ratings (1-10 Likert scales), (3) execution success (percentage of ideas successfully implemented), (4) empirical validation metrics (accuracy, ROC AUC), (5) resource/cost estimates, and (6) prior probability or confidence scores from predictive models.</li>
                <li>The novelty-feasibility tradeoff is not universal but exhibits domain-dependent characteristics: highly constrained domains with clear validation criteria (e.g., formal mathematics, algorithm design) show more favorable tradeoffs than open-ended domains (e.g., social sciences, exploratory biology).</li>
                <li>Hybrid approaches combining literature grounding with data-driven generation achieve better simultaneous optimization than pure approaches, with improvements ranging from +3% to +15% in various metrics.</li>
                <li>Human-in-the-loop curation can navigate the tradeoff space more effectively than fully automated systems, improving novelty (e.g., +0.17 to +0.97 on 1-10 scales) while maintaining or improving feasibility.</li>
                <li>Architectural choices systematically affect position in novelty-feasibility space: multi-agent systems tend to increase novelty while maintaining feasibility; iterative refinement can improve both dimensions; retrieval augmentation generally improves feasibility but may suppress novelty unless carefully designed.</li>
                <li>Model selection affects tradeoff profiles with characteristic signatures: Claude models show higher novelty/lower feasibility profiles while GPT-4 shows more balanced profiles; larger models generally achieve better simultaneous optimization.</li>
                <li>Explicit multi-objective optimization frameworks (Pareto optimization, weighted objectives, MDL-based tradeoffs) can formalize and improve navigation of the novelty-feasibility space compared to implicit single-objective approaches.</li>
                <li>The tradeoff can be partially circumvented by focusing on 'learnable novelty' (learning progress methods) or 'grounded novelty' (literature+data synthesis), which filter out infeasible novel directions while preserving productive exploration.</li>
                <li>Increasing task scope or complexity generally degrades feasibility more rapidly than it increases novelty, with error rates rising from 10-20% for narrow tasks to 90% for broad tasks.</li>
                <li>Diversity-aware selection mechanisms are necessary to prevent reward collapse when optimizing for feasibility alone, as pure performance optimization reduces novelty through convergence to similar solutions.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>LLMCG system showed significantly higher novelty than LLM-only baselines (Control-Claude vs Random-selected LLMCG: t(59)=3.34, p=0.007, Cohen's d=0.8809) while maintaining comparable usefulness/feasibility (no significant group differences in usefulness), demonstrating that structured causal-graph approaches can improve novelty without sacrificing feasibility <a href="../results/extraction-result-2271.html#e2271.0" class="evidence-link">[e2271.0]</a> </li>
    <li>AI-generated ideas rated significantly more novel than human ideas (5.64 vs 4.84, p<0.05) but slightly less feasible (6.34 vs 6.61, not significant), showing modest negative correlation (r=-0.073) between novelty and feasibility dimensions <a href="../results/extraction-result-2433.html#e2433.0" class="evidence-link">[e2433.0]</a> </li>
    <li>Human reranking of AI ideas improved novelty scores (5.81 vs 5.64 AI-only) while maintaining feasibility (6.41 vs 6.34), suggesting human judgment can navigate the tradeoff space more effectively than automated ranking <a href="../results/extraction-result-2433.html#e2433.5" class="evidence-link">[e2433.5]</a> </li>
    <li>Multi-agent collaboration increased novelty (1.52 vs 1.23 baseline) while maintaining or slightly improving verifiability (2.05 vs 2.03), indicating architectural choices can affect tradeoff balance <a href="../results/extraction-result-2438.html#e2438.2" class="evidence-link">[e2438.2]</a> </li>
    <li>Literature+data union methods (LITERATURE∪HYPOREFINE) outperformed pure data-driven approaches by +3.37% OOD accuracy and literature-only by +15.75%, suggesting synthesis strategies can optimize both dimensions simultaneously <a href="../results/extraction-result-2320.html#e2320.3" class="evidence-link">[e2320.3]</a> </li>
    <li>CoQuest depth-first generation produced higher novelty (3.78 vs 3.28, p=0.002) and surprise ratings but participants reported lower perceived control and trust, indicating user experience tradeoffs beyond pure novelty-feasibility dimensions <a href="../results/extraction-result-2269.html#e2269.0" class="evidence-link">[e2269.0]</a> </li>
    <li>SciMON iterative novelty boosting increased novelty (88.9% substantially different, 55.6% more novel after first iteration) but authors noted many updates were superficial recombinations rather than deep technical novelties, suggesting novelty optimization can produce shallow rather than meaningful innovation <a href="../results/extraction-result-2282.html#e2282.0" class="evidence-link">[e2282.0]</a> <a href="../results/extraction-result-2282.html#e2282.1" class="evidence-link">[e2282.1]</a> </li>
    <li>Claude models showed higher distinctness/novelty but generated more impractical/irrelevant ideas compared to GPT-4 (Claude CS feasibility 83.34% vs GPT-4 96.64%), demonstrating model-specific tradeoff profiles <a href="../results/extraction-result-2330.html#e2330.5" class="evidence-link">[e2330.5]</a> </li>
    <li>Exploration bonuses in RL can encourage novelty but don't scale well in open real-world settings because many novel events are unlearnable, highlighting the importance of feasibility-aware novelty metrics <a href="../results/extraction-result-2418.html#e2418.5" class="evidence-link">[e2418.5]</a> </li>
    <li>Learning progress (LP) methods balance novelty and feasibility by rewarding learnable novelty rather than pure novelty, avoiding unproductive exploration of unlearnable novel states <a href="../results/extraction-result-2418.html#e2418.0" class="evidence-link">[e2418.0]</a> <a href="../results/extraction-result-2418.html#e2418.2" class="evidence-link">[e2418.2]</a> </li>
    <li>AI physicist's MDL framework explicitly trades off model complexity (novelty/parsimony) against data fit (feasibility/accuracy) via total description length L_d(T,D) = L_d(T) + Σ L_d(u_t), with empirical examples showing ~16 bit reductions when snapping to simpler rational parameters <a href="../results/extraction-result-2430.html#e2430.0" class="evidence-link">[e2430.0]</a> <a href="../results/extraction-result-2430.html#e2430.1" class="evidence-link">[e2430.1]</a> </li>
    <li>Reward collapse in automated ML research showed that optimizing for performance alone reduces diversity, requiring explicit diversity-aware selection (greedy embedding-distance balancing) to maintain novelty <a href="../results/extraction-result-2257.html#e2257.0" class="evidence-link">[e2257.0]</a> </li>
    <li>Tool-calling alone decreased novelty (0.57 vs 1.23 baseline) while attempting to improve grounding/feasibility, showing that feasibility-enhancing mechanisms can suppress novelty if not carefully integrated <a href="../results/extraction-result-2438.html#e2438.4" class="evidence-link">[e2438.4]</a> </li>
    <li>Increasing task breadth/complexity in data-to-paper system strongly increased error rate (90% for broad ML model-building vs 10-20% for narrow tasks), demonstrating feasibility degradation with scope expansion <a href="../results/extraction-result-2387.html#e2387.0" class="evidence-link">[e2387.0]</a> </li>
    <li>SciPIP showed that ideas with lower similarity to existing published ideas had higher LLM-evaluated novelty scores (>270 ideas with novelty >7 on 0-10 scale), but feasibility did not show strong consistent differences across similarity groups (win rates: 19.1%, 11.5%, 16.7%, 25.5%, 23.2% across similarity bins) <a href="../results/extraction-result-2290.html#e2290.0" class="evidence-link">[e2290.0]</a> <a href="../results/extraction-result-2290.html#e2290.3" class="evidence-link">[e2290.3]</a> </li>
    <li>Adam Robot Scientist's multi-criterion selection explicitly traded off three criteria (maximize information/novelty, maximize prior probability/feasibility, minimize cost) using Bayesian priors and expected-cost minimization, though no quantitative tradeoff measurements were reported <a href="../results/extraction-result-2259.html#e2259.0" class="evidence-link">[e2259.0]</a> </li>
    <li>CycleResearcher showed contribution scores (novelty proxy) of 2.60 and soundness scores (feasibility proxy) of 2.71 on 1-4 scales, with 35.13% simulated acceptance rate, suggesting moderate achievement on both dimensions <a href="../results/extraction-result-2287.html#e2287.0" class="evidence-link">[e2287.0]</a> </li>
    <li>Semantic deduplication revealed severe diversity limits: only ~200 unique ideas from 4000 generated seeds (~5%), showing that naive scaling increases redundancy rather than unique novelty <a href="../results/extraction-result-2433.html#e2433.4" class="evidence-link">[e2433.4]</a> </li>
    <li>VIRSCI multi-agent system with dynamic team composition outperformed baselines (HypoGen) on both novelty metrics (HD/CD/CI/ON) and human evaluation, suggesting team diversity mechanisms can improve both dimensions <a href="../results/extraction-result-2315.html#e2315.0" class="evidence-link">[e2315.0]</a> <a href="../results/extraction-result-2315.html#e2315.2" class="evidence-link">[e2315.2]</a> </li>
    <li>Scideator's facet-based recombination with novelty checking achieved 89.66% accuracy in detecting non-novel ideas while supporting generation of novel combinations, demonstrating effective novelty-feasibility navigation <a href="../results/extraction-result-2322.html#e2322.0" class="evidence-link">[e2322.0]</a> <a href="../results/extraction-result-2322.html#e2322.3" class="evidence-link">[e2322.3]</a> </li>
    <li>MLR-Copilot showed lower similarity to existing work (0.16 vs 0.32 baseline) and higher innovativeness (3.9 vs 3.1) while maintaining higher feasibility (4.1 vs 3.8), demonstrating that retrieval-grounded iterative systems can improve both dimensions <a href="../results/extraction-result-2307.html#e2307.0" class="evidence-link">[e2307.0]</a> </li>
    <li>AGATHA's learned ranking substantially improved over Moliere's heuristics (ROC AUC 0.901 vs 0.718), showing that learned models can better navigate the novelty-feasibility space than hand-crafted metrics <a href="../results/extraction-result-2279.html#e2279.0" class="evidence-link">[e2279.0]</a> <a href="../results/extraction-result-2279.html#e2279.1" class="evidence-link">[e2279.1]</a> </li>
    <li>Regularized evolution in AutoML-Zero showed ~5x higher success rate than random search for linear regression and greater improvements for difficult tasks, demonstrating that evolutionary search can efficiently explore sparse hypothesis spaces <a href="../results/extraction-result-2415.html#e2415.1" class="evidence-link">[e2415.1]</a> </li>
    <li>Chain of Ideas (CoI) system outperformed baselines in ELO rankings across multiple criteria including novelty and feasibility, with model-human judge agreement ~70.8%, showing that progressive idea development can optimize multiple dimensions <a href="../results/extraction-result-2304.html#e2304.0" class="evidence-link">[e2304.0]</a> <a href="../results/extraction-result-2304.html#e2304.1" class="evidence-link">[e2304.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Systems that explicitly model the novelty-feasibility tradeoff with multi-objective optimization (e.g., Pareto-based selection, weighted scalarization with learned weights) will outperform single-objective systems by 10-20% on composite metrics combining both dimensions.</li>
                <li>Increasing the diversity of retrieval sources (literature + data + knowledge graphs + cross-domain analogies) will improve the Pareto frontier of achievable novelty-feasibility combinations, with each additional source type contributing 5-10% improvement.</li>
                <li>Domain-specific calibration of novelty thresholds (e.g., cosine similarity thresholds ranging from 0.3-0.8 depending on domain) and feasibility requirements will improve overall hypothesis quality by 15-25% compared to domain-agnostic fixed thresholds.</li>
                <li>Iterative refinement with alternating novelty-boosting and feasibility-checking steps (3-5 iterations) will produce 20-30% better final hypotheses on composite metrics than single-pass generation.</li>
                <li>Ensemble methods combining 3-5 generation strategies with different novelty-feasibility profiles (e.g., one high-novelty, one high-feasibility, one balanced) will achieve 15-20% more robust performance across diverse evaluation criteria.</li>
                <li>Learning progress-based intrinsic rewards will outperform pure novelty bonuses by 30-50% in open-ended domains where many novel states are unlearnable.</li>
                <li>Human-AI collaborative systems where humans provide periodic feedback (every 3-5 iterations) will achieve 25-35% better novelty-feasibility balance than fully automated systems.</li>
                <li>Fine-tuning LLMs on datasets explicitly labeled for novelty-feasibility tradeoffs will improve their ability to generate ideas at specified points on the tradeoff curve by 20-30%.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists a universal mathematical formulation (e.g., a general utility function U(novelty, feasibility, domain_params)) of the novelty-feasibility tradeoff that generalizes across all scientific domains, or whether fundamentally different formulations are necessary for different domain classes (formal vs empirical sciences, exploratory vs confirmatory research).</li>
                <li>Whether the optimal point on the novelty-feasibility tradeoff curve for maximizing long-term scientific impact (measured by citations, paradigm shifts, or practical applications over 5-10 years) differs systematically between exploratory and confirmatory research paradigms, and if so, by how much.</li>
                <li>Whether automated systems can learn to predict which regions of the novelty-feasibility space will yield the highest-impact discoveries for a given research context through meta-learning across thousands of historical scientific discoveries.</li>
                <li>Whether the novelty-feasibility tradeoff exhibits phase transitions or discontinuities at certain critical thresholds (e.g., when novelty exceeds the field's absorptive capacity, or when feasibility drops below practical implementation thresholds), or whether it is smooth and continuous across the entire space.</li>
                <li>Whether human expert intuition about navigating the novelty-feasibility tradeoff can be effectively distilled into automated systems through inverse reinforcement learning, preference learning, or similar techniques, achieving >80% of human expert performance.</li>
                <li>Whether the tradeoff relationship fundamentally changes as AI systems become more capable, potentially inverting the relationship (where higher novelty becomes more feasible due to better synthesis capabilities) or eliminating it entirely.</li>
                <li>Whether cross-domain transfer of novelty-feasibility navigation strategies is possible, such that a system trained to optimize the tradeoff in one domain (e.g., materials science) can effectively transfer to another domain (e.g., drug discovery) with minimal adaptation.</li>
                <li>Whether the novelty-feasibility tradeoff is fundamentally a property of the hypothesis generation process, or whether it is an artifact of current evaluation metrics and would disappear with better-designed metrics that capture 'productive novelty'.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding domains where increasing novelty consistently improves feasibility (strong positive correlation r > 0.5) across multiple systems would challenge the fundamental tension hypothesis and suggest domain-specific mechanisms that enable synergistic optimization.</li>
                <li>Demonstrating that pure random generation achieves the same or better novelty-feasibility Pareto frontier as sophisticated optimization systems (within 5% on composite metrics) would question the value of complex optimization strategies.</li>
                <li>Showing that human experts cannot outperform random selection in navigating the tradeoff space (achieving <55% accuracy in pairwise comparisons) would challenge the value of human-in-the-loop approaches and suggest the tradeoff is too complex for human intuition.</li>
                <li>Finding that all architectural variations (multi-agent, iterative, retrieval-augmented, single-shot) produce statistically indistinguishable tradeoff profiles (differences <5% on all metrics) would question the importance of system design choices.</li>
                <li>Demonstrating that novelty and feasibility metrics are completely uncorrelated (|r| < 0.05) across all domains and methods would challenge the existence of a fundamental tradeoff and suggest they are independent dimensions.</li>
                <li>Finding that increasing computational resources (10x, 100x) does not improve the Pareto frontier would suggest fundamental algorithmic rather than resource limitations.</li>
                <li>Showing that the tradeoff relationship reverses in certain conditions (where optimizing for novelty improves feasibility) would require substantial revision of the theory to account for these conditions.</li>
                <li>Demonstrating that simple heuristics (e.g., random sampling with basic filtering) achieve >90% of the performance of sophisticated multi-objective optimization would question the necessity of complex approaches.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact functional form of the novelty-feasibility relationship (linear, logarithmic, power-law, piecewise, etc.) is not consistently characterized across systems, with different studies reporting different correlation structures <a href="../results/extraction-result-2433.html#e2433.0" class="evidence-link">[e2433.0]</a> <a href="../results/extraction-result-2271.html#e2271.0" class="evidence-link">[e2271.0]</a> <a href="../results/extraction-result-2438.html#e2438.2" class="evidence-link">[e2438.2]</a> <a href="../results/extraction-result-2290.html#e2290.0" class="evidence-link">[e2290.0]</a> </li>
    <li>The role of temporal dynamics - how the novelty-feasibility tradeoff evolves as a research field matures, as more literature accumulates, or as validation methods improve - is not systematically studied <a href="../results/extraction-result-2382.html#e2382.0" class="evidence-link">[e2382.0]</a> <a href="../results/extraction-result-2462.html#e2462.2" class="evidence-link">[e2462.2]</a> <a href="../results/extraction-result-2285.html#e2285.1" class="evidence-link">[e2285.1]</a> </li>
    <li>The interaction between novelty-feasibility tradeoffs and other important dimensions (clarity, significance, excitement, ethical considerations) is not fully mapped, though some evidence suggests these dimensions have their own tradeoff relationships <a href="../results/extraction-result-2433.html#e2433.0" class="evidence-link">[e2433.0]</a> <a href="../results/extraction-result-2304.html#e2304.1" class="evidence-link">[e2304.1]</a> <a href="../results/extraction-result-2280.html#e2280.0" class="evidence-link">[e2280.0]</a> </li>
    <li>The cognitive mechanisms by which human experts navigate the tradeoff space (pattern recognition, analogical reasoning, risk assessment) are not explicitly modeled in current systems <a href="../results/extraction-result-2433.html#e2433.5" class="evidence-link">[e2433.5]</a> <a href="../results/extraction-result-2393.html#e2393.2" class="evidence-link">[e2393.2]</a> <a href="../results/extraction-result-2322.html#e2322.0" class="evidence-link">[e2322.0]</a> </li>
    <li>The optimal allocation of computational resources between novelty-seeking exploration and feasibility-checking validation is not theoretically characterized, though empirical evidence suggests diminishing returns <a href="../results/extraction-result-2257.html#e2257.0" class="evidence-link">[e2257.0]</a> <a href="../results/extraction-result-2387.html#e2387.0" class="evidence-link">[e2387.0]</a> <a href="../results/extraction-result-2433.html#e2433.4" class="evidence-link">[e2433.4]</a> </li>
    <li>The role of evaluation metrics themselves in shaping the observed tradeoff - whether different metrics would reveal different tradeoff structures or whether the tradeoff is metric-invariant - is not systematically investigated <a href="../results/extraction-result-2433.html#e2433.0" class="evidence-link">[e2433.0]</a> <a href="../results/extraction-result-2271.html#e2271.0" class="evidence-link">[e2271.0]</a> <a href="../results/extraction-result-2304.html#e2304.1" class="evidence-link">[e2304.1]</a> </li>
    <li>The impact of training data characteristics (size, diversity, quality, temporal coverage) on the novelty-feasibility tradeoff profiles of learned systems is not fully characterized <a href="../results/extraction-result-2330.html#e2330.5" class="evidence-link">[e2330.5]</a> <a href="../results/extraction-result-2287.html#e2287.0" class="evidence-link">[e2287.0]</a> <a href="../results/extraction-result-2462.html#e2462.2" class="evidence-link">[e2462.2]</a> </li>
    <li>The transferability of novelty-feasibility navigation strategies across domains (whether strategies that work in one domain generalize to others) is not empirically tested <a href="../results/extraction-result-2330.html#e2330.5" class="evidence-link">[e2330.5]</a> <a href="../results/extraction-result-2399.html#e2399.0" class="evidence-link">[e2399.0]</a> <a href="../results/extraction-result-2426.html#e2426.0" class="evidence-link">[e2426.0]</a> </li>
    <li>The role of uncertainty quantification in the tradeoff - how confidence intervals or uncertainty estimates for novelty and feasibility scores affect optimal decision-making - is not addressed <a href="../results/extraction-result-2271.html#e2271.0" class="evidence-link">[e2271.0]</a> <a href="../results/extraction-result-2290.html#e2290.3" class="evidence-link">[e2290.3]</a> <a href="../results/extraction-result-2389.html#e2389.0" class="evidence-link">[e2389.0]</a> </li>
    <li>The long-term dynamics of iterative refinement - whether continued iteration eventually converges to an optimal tradeoff point or exhibits oscillations, divergence, or other complex dynamics - is not characterized beyond 2-3 iterations <a href="../results/extraction-result-2282.html#e2282.1" class="evidence-link">[e2282.1]</a> <a href="../results/extraction-result-2414.html#e2414.0" class="evidence-link">[e2414.0]</a> <a href="../results/extraction-result-2269.html#e2269.0" class="evidence-link">[e2269.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>March (1991) Exploration and exploitation in organizational learning [Foundational work on exploration-exploitation tradeoffs, directly related to novelty-feasibility tension]</li>
    <li>Ouyang et al. (2022) Training language models to follow instructions with human feedback [RLHF methods that balance capability and safety, related to balancing exploration and constraint]</li>
    <li>Schmidhuber (2010) Formal theory of creativity, fun, and intrinsic motivation [Theoretical framework for curiosity-driven learning that relates to novelty-seeking and the role of learnability]</li>
    <li>Baranes & Oudeyer (2013) Active learning of inverse models with intrinsically motivated goal exploration [Learning progress and goal exploration related to feasibility-aware novelty]</li>
    <li>Silver et al. (2021) Reward is enough [Discusses tradeoffs in reward specification, relevant to multi-objective optimization in RL]</li>
    <li>Deb et al. (2002) A fast and elitist multiobjective genetic algorithm: NSGA-II [Foundational work on multi-objective optimization and Pareto frontiers, applicable to novelty-feasibility optimization]</li>
    <li>Lehman & Stanley (2011) Abandoning objectives: Evolution through the search for novelty alone [Novelty search in evolutionary computation, related to pure novelty optimization]</li>
    <li>Pugh et al. (2016) Quality diversity: A new frontier for evolutionary computation [Quality-diversity algorithms that explicitly balance novelty and performance]</li>
    <li>Rissanen (1978) Modeling by shortest data description [Minimum description length principle, related to complexity-accuracy tradeoffs]</li>
    <li>Simonton (1999) Origins of genius: Darwinian perspectives on creativity [Theoretical framework for creative ideation including variation-selection processes related to novelty-feasibility]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "The Fundamental Novelty-Feasibility Tension in Automated Hypothesis Generation",
    "theory_description": "Automated hypothesis generation systems face an inherent tension between novelty (semantic distance from existing work, conceptual originality) and feasibility (likelihood of successful implementation, verification, and empirical validation). This tension arises because highly novel hypotheses often require untested combinations of concepts or methods that lack empirical validation, while highly feasible hypotheses tend to be incremental variations of established work. The tradeoff is not strictly inverse but exhibits domain-dependent and method-dependent characteristics, with correlation coefficients typically ranging from weak negative (r ~ -0.07) to moderate negative (r ~ -0.3). Hybrid approaches combining literature grounding with data-driven generation, multi-agent architectures, and human-in-the-loop curation can achieve better simultaneous optimization of both dimensions than pure single-strategy approaches. The tradeoff can be explicitly modeled through multi-objective optimization frameworks (e.g., Pareto optimization, weighted objectives, MDL-based complexity-accuracy tradeoffs) or implicitly managed through architectural choices and iterative refinement strategies.",
    "supporting_evidence": [
        {
            "text": "LLMCG system showed significantly higher novelty than LLM-only baselines (Control-Claude vs Random-selected LLMCG: t(59)=3.34, p=0.007, Cohen's d=0.8809) while maintaining comparable usefulness/feasibility (no significant group differences in usefulness), demonstrating that structured causal-graph approaches can improve novelty without sacrificing feasibility",
            "uuids": [
                "e2271.0"
            ]
        },
        {
            "text": "AI-generated ideas rated significantly more novel than human ideas (5.64 vs 4.84, p&lt;0.05) but slightly less feasible (6.34 vs 6.61, not significant), showing modest negative correlation (r=-0.073) between novelty and feasibility dimensions",
            "uuids": [
                "e2433.0"
            ]
        },
        {
            "text": "Human reranking of AI ideas improved novelty scores (5.81 vs 5.64 AI-only) while maintaining feasibility (6.41 vs 6.34), suggesting human judgment can navigate the tradeoff space more effectively than automated ranking",
            "uuids": [
                "e2433.5"
            ]
        },
        {
            "text": "Multi-agent collaboration increased novelty (1.52 vs 1.23 baseline) while maintaining or slightly improving verifiability (2.05 vs 2.03), indicating architectural choices can affect tradeoff balance",
            "uuids": [
                "e2438.2"
            ]
        },
        {
            "text": "Literature+data union methods (LITERATURE∪HYPOREFINE) outperformed pure data-driven approaches by +3.37% OOD accuracy and literature-only by +15.75%, suggesting synthesis strategies can optimize both dimensions simultaneously",
            "uuids": [
                "e2320.3"
            ]
        },
        {
            "text": "CoQuest depth-first generation produced higher novelty (3.78 vs 3.28, p=0.002) and surprise ratings but participants reported lower perceived control and trust, indicating user experience tradeoffs beyond pure novelty-feasibility dimensions",
            "uuids": [
                "e2269.0"
            ]
        },
        {
            "text": "SciMON iterative novelty boosting increased novelty (88.9% substantially different, 55.6% more novel after first iteration) but authors noted many updates were superficial recombinations rather than deep technical novelties, suggesting novelty optimization can produce shallow rather than meaningful innovation",
            "uuids": [
                "e2282.0",
                "e2282.1"
            ]
        },
        {
            "text": "Claude models showed higher distinctness/novelty but generated more impractical/irrelevant ideas compared to GPT-4 (Claude CS feasibility 83.34% vs GPT-4 96.64%), demonstrating model-specific tradeoff profiles",
            "uuids": [
                "e2330.5"
            ]
        },
        {
            "text": "Exploration bonuses in RL can encourage novelty but don't scale well in open real-world settings because many novel events are unlearnable, highlighting the importance of feasibility-aware novelty metrics",
            "uuids": [
                "e2418.5"
            ]
        },
        {
            "text": "Learning progress (LP) methods balance novelty and feasibility by rewarding learnable novelty rather than pure novelty, avoiding unproductive exploration of unlearnable novel states",
            "uuids": [
                "e2418.0",
                "e2418.2"
            ]
        },
        {
            "text": "AI physicist's MDL framework explicitly trades off model complexity (novelty/parsimony) against data fit (feasibility/accuracy) via total description length L_d(T,D) = L_d(T) + Σ L_d(u_t), with empirical examples showing ~16 bit reductions when snapping to simpler rational parameters",
            "uuids": [
                "e2430.0",
                "e2430.1"
            ]
        },
        {
            "text": "Reward collapse in automated ML research showed that optimizing for performance alone reduces diversity, requiring explicit diversity-aware selection (greedy embedding-distance balancing) to maintain novelty",
            "uuids": [
                "e2257.0"
            ]
        },
        {
            "text": "Tool-calling alone decreased novelty (0.57 vs 1.23 baseline) while attempting to improve grounding/feasibility, showing that feasibility-enhancing mechanisms can suppress novelty if not carefully integrated",
            "uuids": [
                "e2438.4"
            ]
        },
        {
            "text": "Increasing task breadth/complexity in data-to-paper system strongly increased error rate (90% for broad ML model-building vs 10-20% for narrow tasks), demonstrating feasibility degradation with scope expansion",
            "uuids": [
                "e2387.0"
            ]
        },
        {
            "text": "SciPIP showed that ideas with lower similarity to existing published ideas had higher LLM-evaluated novelty scores (&gt;270 ideas with novelty &gt;7 on 0-10 scale), but feasibility did not show strong consistent differences across similarity groups (win rates: 19.1%, 11.5%, 16.7%, 25.5%, 23.2% across similarity bins)",
            "uuids": [
                "e2290.0",
                "e2290.3"
            ]
        },
        {
            "text": "Adam Robot Scientist's multi-criterion selection explicitly traded off three criteria (maximize information/novelty, maximize prior probability/feasibility, minimize cost) using Bayesian priors and expected-cost minimization, though no quantitative tradeoff measurements were reported",
            "uuids": [
                "e2259.0"
            ]
        },
        {
            "text": "CycleResearcher showed contribution scores (novelty proxy) of 2.60 and soundness scores (feasibility proxy) of 2.71 on 1-4 scales, with 35.13% simulated acceptance rate, suggesting moderate achievement on both dimensions",
            "uuids": [
                "e2287.0"
            ]
        },
        {
            "text": "Semantic deduplication revealed severe diversity limits: only ~200 unique ideas from 4000 generated seeds (~5%), showing that naive scaling increases redundancy rather than unique novelty",
            "uuids": [
                "e2433.4"
            ]
        },
        {
            "text": "VIRSCI multi-agent system with dynamic team composition outperformed baselines (HypoGen) on both novelty metrics (HD/CD/CI/ON) and human evaluation, suggesting team diversity mechanisms can improve both dimensions",
            "uuids": [
                "e2315.0",
                "e2315.2"
            ]
        },
        {
            "text": "Scideator's facet-based recombination with novelty checking achieved 89.66% accuracy in detecting non-novel ideas while supporting generation of novel combinations, demonstrating effective novelty-feasibility navigation",
            "uuids": [
                "e2322.0",
                "e2322.3"
            ]
        },
        {
            "text": "MLR-Copilot showed lower similarity to existing work (0.16 vs 0.32 baseline) and higher innovativeness (3.9 vs 3.1) while maintaining higher feasibility (4.1 vs 3.8), demonstrating that retrieval-grounded iterative systems can improve both dimensions",
            "uuids": [
                "e2307.0"
            ]
        },
        {
            "text": "AGATHA's learned ranking substantially improved over Moliere's heuristics (ROC AUC 0.901 vs 0.718), showing that learned models can better navigate the novelty-feasibility space than hand-crafted metrics",
            "uuids": [
                "e2279.0",
                "e2279.1"
            ]
        },
        {
            "text": "Regularized evolution in AutoML-Zero showed ~5x higher success rate than random search for linear regression and greater improvements for difficult tasks, demonstrating that evolutionary search can efficiently explore sparse hypothesis spaces",
            "uuids": [
                "e2415.1"
            ]
        },
        {
            "text": "Chain of Ideas (CoI) system outperformed baselines in ELO rankings across multiple criteria including novelty and feasibility, with model-human judge agreement ~70.8%, showing that progressive idea development can optimize multiple dimensions",
            "uuids": [
                "e2304.0",
                "e2304.1"
            ]
        }
    ],
    "theory_statements": [
        "There exists a fundamental tension between novelty and feasibility in automated hypothesis generation, manifesting as a weak to moderate negative correlation (typically r ~ -0.07 to -0.3) rather than a strict inverse relationship.",
        "Novelty can be quantified through multiple complementary metrics: (1) semantic distance from existing work (embedding cosine similarity, typically threshold ~0.6-0.8), (2) human expert ratings (typically 1-10 Likert scales), (3) structural measures (graph distance, outlier detection in feature space), (4) n-gram overlap with prior work (lower overlap indicates higher novelty), and (5) automated LLM-based evaluation (0-3 or 1-10 scales with &gt;0.7 correlation to human judgments).",
        "Feasibility can be quantified through: (1) verification success rates (percentage of hypotheses that pass empirical validation), (2) human expert ratings (1-10 Likert scales), (3) execution success (percentage of ideas successfully implemented), (4) empirical validation metrics (accuracy, ROC AUC), (5) resource/cost estimates, and (6) prior probability or confidence scores from predictive models.",
        "The novelty-feasibility tradeoff is not universal but exhibits domain-dependent characteristics: highly constrained domains with clear validation criteria (e.g., formal mathematics, algorithm design) show more favorable tradeoffs than open-ended domains (e.g., social sciences, exploratory biology).",
        "Hybrid approaches combining literature grounding with data-driven generation achieve better simultaneous optimization than pure approaches, with improvements ranging from +3% to +15% in various metrics.",
        "Human-in-the-loop curation can navigate the tradeoff space more effectively than fully automated systems, improving novelty (e.g., +0.17 to +0.97 on 1-10 scales) while maintaining or improving feasibility.",
        "Architectural choices systematically affect position in novelty-feasibility space: multi-agent systems tend to increase novelty while maintaining feasibility; iterative refinement can improve both dimensions; retrieval augmentation generally improves feasibility but may suppress novelty unless carefully designed.",
        "Model selection affects tradeoff profiles with characteristic signatures: Claude models show higher novelty/lower feasibility profiles while GPT-4 shows more balanced profiles; larger models generally achieve better simultaneous optimization.",
        "Explicit multi-objective optimization frameworks (Pareto optimization, weighted objectives, MDL-based tradeoffs) can formalize and improve navigation of the novelty-feasibility space compared to implicit single-objective approaches.",
        "The tradeoff can be partially circumvented by focusing on 'learnable novelty' (learning progress methods) or 'grounded novelty' (literature+data synthesis), which filter out infeasible novel directions while preserving productive exploration.",
        "Increasing task scope or complexity generally degrades feasibility more rapidly than it increases novelty, with error rates rising from 10-20% for narrow tasks to 90% for broad tasks.",
        "Diversity-aware selection mechanisms are necessary to prevent reward collapse when optimizing for feasibility alone, as pure performance optimization reduces novelty through convergence to similar solutions."
    ],
    "new_predictions_likely": [
        "Systems that explicitly model the novelty-feasibility tradeoff with multi-objective optimization (e.g., Pareto-based selection, weighted scalarization with learned weights) will outperform single-objective systems by 10-20% on composite metrics combining both dimensions.",
        "Increasing the diversity of retrieval sources (literature + data + knowledge graphs + cross-domain analogies) will improve the Pareto frontier of achievable novelty-feasibility combinations, with each additional source type contributing 5-10% improvement.",
        "Domain-specific calibration of novelty thresholds (e.g., cosine similarity thresholds ranging from 0.3-0.8 depending on domain) and feasibility requirements will improve overall hypothesis quality by 15-25% compared to domain-agnostic fixed thresholds.",
        "Iterative refinement with alternating novelty-boosting and feasibility-checking steps (3-5 iterations) will produce 20-30% better final hypotheses on composite metrics than single-pass generation.",
        "Ensemble methods combining 3-5 generation strategies with different novelty-feasibility profiles (e.g., one high-novelty, one high-feasibility, one balanced) will achieve 15-20% more robust performance across diverse evaluation criteria.",
        "Learning progress-based intrinsic rewards will outperform pure novelty bonuses by 30-50% in open-ended domains where many novel states are unlearnable.",
        "Human-AI collaborative systems where humans provide periodic feedback (every 3-5 iterations) will achieve 25-35% better novelty-feasibility balance than fully automated systems.",
        "Fine-tuning LLMs on datasets explicitly labeled for novelty-feasibility tradeoffs will improve their ability to generate ideas at specified points on the tradeoff curve by 20-30%."
    ],
    "new_predictions_unknown": [
        "Whether there exists a universal mathematical formulation (e.g., a general utility function U(novelty, feasibility, domain_params)) of the novelty-feasibility tradeoff that generalizes across all scientific domains, or whether fundamentally different formulations are necessary for different domain classes (formal vs empirical sciences, exploratory vs confirmatory research).",
        "Whether the optimal point on the novelty-feasibility tradeoff curve for maximizing long-term scientific impact (measured by citations, paradigm shifts, or practical applications over 5-10 years) differs systematically between exploratory and confirmatory research paradigms, and if so, by how much.",
        "Whether automated systems can learn to predict which regions of the novelty-feasibility space will yield the highest-impact discoveries for a given research context through meta-learning across thousands of historical scientific discoveries.",
        "Whether the novelty-feasibility tradeoff exhibits phase transitions or discontinuities at certain critical thresholds (e.g., when novelty exceeds the field's absorptive capacity, or when feasibility drops below practical implementation thresholds), or whether it is smooth and continuous across the entire space.",
        "Whether human expert intuition about navigating the novelty-feasibility tradeoff can be effectively distilled into automated systems through inverse reinforcement learning, preference learning, or similar techniques, achieving &gt;80% of human expert performance.",
        "Whether the tradeoff relationship fundamentally changes as AI systems become more capable, potentially inverting the relationship (where higher novelty becomes more feasible due to better synthesis capabilities) or eliminating it entirely.",
        "Whether cross-domain transfer of novelty-feasibility navigation strategies is possible, such that a system trained to optimize the tradeoff in one domain (e.g., materials science) can effectively transfer to another domain (e.g., drug discovery) with minimal adaptation.",
        "Whether the novelty-feasibility tradeoff is fundamentally a property of the hypothesis generation process, or whether it is an artifact of current evaluation metrics and would disappear with better-designed metrics that capture 'productive novelty'."
    ],
    "negative_experiments": [
        "Finding domains where increasing novelty consistently improves feasibility (strong positive correlation r &gt; 0.5) across multiple systems would challenge the fundamental tension hypothesis and suggest domain-specific mechanisms that enable synergistic optimization.",
        "Demonstrating that pure random generation achieves the same or better novelty-feasibility Pareto frontier as sophisticated optimization systems (within 5% on composite metrics) would question the value of complex optimization strategies.",
        "Showing that human experts cannot outperform random selection in navigating the tradeoff space (achieving &lt;55% accuracy in pairwise comparisons) would challenge the value of human-in-the-loop approaches and suggest the tradeoff is too complex for human intuition.",
        "Finding that all architectural variations (multi-agent, iterative, retrieval-augmented, single-shot) produce statistically indistinguishable tradeoff profiles (differences &lt;5% on all metrics) would question the importance of system design choices.",
        "Demonstrating that novelty and feasibility metrics are completely uncorrelated (|r| &lt; 0.05) across all domains and methods would challenge the existence of a fundamental tradeoff and suggest they are independent dimensions.",
        "Finding that increasing computational resources (10x, 100x) does not improve the Pareto frontier would suggest fundamental algorithmic rather than resource limitations.",
        "Showing that the tradeoff relationship reverses in certain conditions (where optimizing for novelty improves feasibility) would require substantial revision of the theory to account for these conditions.",
        "Demonstrating that simple heuristics (e.g., random sampling with basic filtering) achieve &gt;90% of the performance of sophisticated multi-objective optimization would question the necessity of complex approaches."
    ],
    "unaccounted_for": [
        {
            "text": "The exact functional form of the novelty-feasibility relationship (linear, logarithmic, power-law, piecewise, etc.) is not consistently characterized across systems, with different studies reporting different correlation structures",
            "uuids": [
                "e2433.0",
                "e2271.0",
                "e2438.2",
                "e2290.0"
            ]
        },
        {
            "text": "The role of temporal dynamics - how the novelty-feasibility tradeoff evolves as a research field matures, as more literature accumulates, or as validation methods improve - is not systematically studied",
            "uuids": [
                "e2382.0",
                "e2462.2",
                "e2285.1"
            ]
        },
        {
            "text": "The interaction between novelty-feasibility tradeoffs and other important dimensions (clarity, significance, excitement, ethical considerations) is not fully mapped, though some evidence suggests these dimensions have their own tradeoff relationships",
            "uuids": [
                "e2433.0",
                "e2304.1",
                "e2280.0"
            ]
        },
        {
            "text": "The cognitive mechanisms by which human experts navigate the tradeoff space (pattern recognition, analogical reasoning, risk assessment) are not explicitly modeled in current systems",
            "uuids": [
                "e2433.5",
                "e2393.2",
                "e2322.0"
            ]
        },
        {
            "text": "The optimal allocation of computational resources between novelty-seeking exploration and feasibility-checking validation is not theoretically characterized, though empirical evidence suggests diminishing returns",
            "uuids": [
                "e2257.0",
                "e2387.0",
                "e2433.4"
            ]
        },
        {
            "text": "The role of evaluation metrics themselves in shaping the observed tradeoff - whether different metrics would reveal different tradeoff structures or whether the tradeoff is metric-invariant - is not systematically investigated",
            "uuids": [
                "e2433.0",
                "e2271.0",
                "e2304.1"
            ]
        },
        {
            "text": "The impact of training data characteristics (size, diversity, quality, temporal coverage) on the novelty-feasibility tradeoff profiles of learned systems is not fully characterized",
            "uuids": [
                "e2330.5",
                "e2287.0",
                "e2462.2"
            ]
        },
        {
            "text": "The transferability of novelty-feasibility navigation strategies across domains (whether strategies that work in one domain generalize to others) is not empirically tested",
            "uuids": [
                "e2330.5",
                "e2399.0",
                "e2426.0"
            ]
        },
        {
            "text": "The role of uncertainty quantification in the tradeoff - how confidence intervals or uncertainty estimates for novelty and feasibility scores affect optimal decision-making - is not addressed",
            "uuids": [
                "e2271.0",
                "e2290.3",
                "e2389.0"
            ]
        },
        {
            "text": "The long-term dynamics of iterative refinement - whether continued iteration eventually converges to an optimal tradeoff point or exhibits oscillations, divergence, or other complex dynamics - is not characterized beyond 2-3 iterations",
            "uuids": [
                "e2282.1",
                "e2414.0",
                "e2269.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some systems show no significant feasibility differences across novelty levels (LLMCG usefulness ratings showed no significant group differences despite large novelty differences), while others show clear negative tradeoffs, suggesting the relationship may be system-dependent or domain-dependent rather than universal",
            "uuids": [
                "e2271.0",
                "e2433.0",
                "e2290.0"
            ]
        },
        {
            "text": "Multi-agent systems sometimes increase both novelty and feasibility simultaneously (e.g., multi-agent novelty 1.52 vs 1.23, verifiability 2.05 vs 2.03), contradicting simple tradeoff models and suggesting architectural innovations can shift the Pareto frontier",
            "uuids": [
                "e2438.2",
                "e2315.0",
                "e2304.0"
            ]
        },
        {
            "text": "Human reranking improved novelty without sacrificing feasibility in some studies (AI+Human novelty 5.81, feasibility 6.41) but not consistently across all contexts, suggesting human expertise effects may be context-dependent",
            "uuids": [
                "e2433.5",
                "e2330.5",
                "e2393.2"
            ]
        },
        {
            "text": "Learning progress methods claim to avoid the novelty-feasibility tradeoff by focusing on learnable novelty, suggesting the tradeoff may be avoidable through better problem formulation rather than being fundamental",
            "uuids": [
                "e2418.0",
                "e2418.2"
            ]
        },
        {
            "text": "Literature+data synthesis approaches consistently outperform pure approaches on both dimensions, challenging the notion of a fundamental tradeoff and suggesting it may be an artifact of single-source methods",
            "uuids": [
                "e2320.3",
                "e2307.0",
                "e2322.0"
            ]
        },
        {
            "text": "Some model comparisons show inverse patterns: Claude higher novelty/lower feasibility vs GPT-4, but other comparisons show different models achieving better performance on both dimensions, suggesting model effects are complex",
            "uuids": [
                "e2330.5",
                "e2287.0",
                "e2438.2"
            ]
        },
        {
            "text": "Iterative refinement sometimes improves both dimensions (SciMON 88.9% different, 55.6% more novel) but other times shows diminishing returns or quality degradation, indicating non-monotonic relationships",
            "uuids": [
                "e2282.1",
                "e2414.0",
                "e2269.0"
            ]
        }
    ],
    "special_cases": [
        "In highly constrained domains with clear validation criteria (e.g., formal mathematics, algorithm design, theorem proving), the tradeoff may be more favorable (weaker negative correlation or even positive correlation) than in open-ended domains, as formal verification can validate novel constructions.",
        "For incremental improvements within established paradigms (e.g., hyperparameter optimization, architectural tweaks), feasibility may dominate over novelty as the primary optimization target, with novelty playing a secondary role.",
        "In exploratory research phases or blue-sky research contexts, novelty may be prioritized over immediate feasibility, reversing the typical optimization priorities and accepting higher failure rates.",
        "When human expertise is readily available for validation and refinement (human-in-the-loop systems), the feasibility constraint may be relaxed, allowing more aggressive novelty-seeking with 20-30% improvements in novelty without feasibility loss.",
        "In domains with rapid empirical feedback (e.g., computational experiments, simulation-based validation, automated testing), the cost of exploring infeasible hypotheses is lower, shifting the optimal tradeoff point toward higher novelty.",
        "For literature-based hypothesis generation without empirical validation requirements, novelty can be maximized with less concern for immediate feasibility, as the goal is idea generation rather than implementation.",
        "In interdisciplinary synthesis tasks, the tradeoff may be more complex, with cross-domain novelty being easier to achieve but cross-domain feasibility being harder to assess due to expertise gaps.",
        "When using learned models trained on historical data, the tradeoff may be biased toward feasibility (reproducing known patterns) at the expense of genuine novelty, requiring explicit novelty-promoting mechanisms.",
        "In safety-critical domains (e.g., medical research, engineering design), feasibility constraints may be much stricter, fundamentally limiting the achievable novelty and shifting the Pareto frontier.",
        "For very large-scale generation (thousands of hypotheses), the tradeoff may manifest differently due to diversity collapse and redundancy issues, requiring explicit diversity maintenance mechanisms."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "March (1991) Exploration and exploitation in organizational learning [Foundational work on exploration-exploitation tradeoffs, directly related to novelty-feasibility tension]",
            "Ouyang et al. (2022) Training language models to follow instructions with human feedback [RLHF methods that balance capability and safety, related to balancing exploration and constraint]",
            "Schmidhuber (2010) Formal theory of creativity, fun, and intrinsic motivation [Theoretical framework for curiosity-driven learning that relates to novelty-seeking and the role of learnability]",
            "Baranes & Oudeyer (2013) Active learning of inverse models with intrinsically motivated goal exploration [Learning progress and goal exploration related to feasibility-aware novelty]",
            "Silver et al. (2021) Reward is enough [Discusses tradeoffs in reward specification, relevant to multi-objective optimization in RL]",
            "Deb et al. (2002) A fast and elitist multiobjective genetic algorithm: NSGA-II [Foundational work on multi-objective optimization and Pareto frontiers, applicable to novelty-feasibility optimization]",
            "Lehman & Stanley (2011) Abandoning objectives: Evolution through the search for novelty alone [Novelty search in evolutionary computation, related to pure novelty optimization]",
            "Pugh et al. (2016) Quality diversity: A new frontier for evolutionary computation [Quality-diversity algorithms that explicitly balance novelty and performance]",
            "Rissanen (1978) Modeling by shortest data description [Minimum description length principle, related to complexity-accuracy tradeoffs]",
            "Simonton (1999) Origins of genius: Darwinian perspectives on creativity [Theoretical framework for creative ideation including variation-selection processes related to novelty-feasibility]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
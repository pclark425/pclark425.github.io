<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Stage Contextualization Theory of Scientific Information Extraction - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-564</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-564</p>
                <p><strong>Name:</strong> Multi-Stage Contextualization Theory of Scientific Information Extraction</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can be used to distill quantitative laws from large numbers of scholarly input papers, based on the following results.</p>
                <p><strong>Description:</strong> LLM-based extraction of quantitative laws from scientific literature operates through a hierarchical process of progressive contextualization, where raw text is transformed through multiple stages—retrieval/filtering, structured extraction, normalization, and validation—with each stage requiring different forms of grounding (domain knowledge, external tools, or human feedback) to maintain fidelity. The effectiveness of extraction depends critically on matching the contextualization strategy to the complexity and heterogeneity of the target information, with simple numeric values requiring minimal context while complex relational patterns demand rich multi-document reasoning. Performance improvements follow predictable scaling patterns based on domain adaptation, output constraints, and grounding mechanisms.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Context-Complexity Matching Principle (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; target_information &#8594; has_complexity_level &#8594; high<span style="color: #888888;">, and</span></div>
        <div>&#8226; target_information &#8594; requires_cross_document_reasoning &#8594; true</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_method &#8594; must_employ &#8594; multi-stage_contextualization<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_method &#8594; requires_grounding_via &#8594; external_knowledge_or_retrieval</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>CKMAs uses iterative minigraph construction with scientific constraints and volume limits to extract complex inter-paper relationships, achieving ROUGE-1 36.41 vs baseline 31.11 <a href="../results/extraction-result-4327.html#e4327.0" class="evidence-link">[e4327.0]</a> </li>
    <li>LORE two-stage framework (LLM-ORE extraction + LLM-EMB embedding) discovers latent semantic patterns and pathogenicity flow, achieving MAP 79.9% vs simple co-occurrence 69.4% <a href="../results/extraction-result-4334.html#e4334.0" class="evidence-link">[e4334.0]</a> </li>
    <li>TrialMind decomposes clinical evidence synthesis into search, screening, extraction, and synthesis stages with RAG and CoT, achieving recall 0.782 vs GPT-4 baseline 0.073 <a href="../results/extraction-result-4591.html#e4591.0" class="evidence-link">[e4591.0]</a> </li>
    <li>ResearchBench decomposes discovery into inspiration retrieval, hypothesis composition, and ranking, with GPT-4o achieving ~83% hit ratio for top-20% candidates <a href="../results/extraction-result-4311.html#e4311.0" class="evidence-link">[e4311.0]</a> </li>
    <li>SCIMON uses retrieval-augmented generation with semantic neighbors, KG neighbors, and citation neighbors, plus iterative novelty boosting to generate novel scientific ideas <a href="../results/extraction-result-4611.html#e4611.0" class="evidence-link">[e4611.0]</a> </li>
    <li>DeepResearchEco uses recursive agentic workflow with PaperQA2 multi-agent framework for complex scientific question answering <a href="../results/extraction-result-4357.html#e4357.2" class="evidence-link">[e4357.2]</a> </li>
    <li>LitLLMs combines LLM-generated keyword search with embedding retrieval, improving precision by ~10% and recall by ~30% <a href="../results/extraction-result-4366.html#e4366.0" class="evidence-link">[e4366.0]</a> </li>
    <li>CycleResearcher uses iterative SimPO-driven research agent with automated review feedback, achieving avg score 5.36 vs AI Scientist 4.31 <a href="../results/extraction-result-4353.html#e4353.0" class="evidence-link">[e4353.0]</a> </li>
    <li>Enzyme Co-Scientist uses multi-LLM aggregation (Claude3.5, GPT-4o, Llama3, Qwen) with aggregation agent, achieving mean F1 0.90 vs BRENDA 0.76 <a href="../results/extraction-result-4295.html#e4295.0" class="evidence-link">[e4295.0]</a> </li>
    <li>Visual MOFs Extraction Engine uses multi-module pipeline with BERT paragraph detection, BM25 retrieval, GPT-4 extraction, and post-processing, processing 57k+ synthesis paragraphs <a href="../results/extraction-result-4325.html#e4325.6" class="evidence-link">[e4325.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This principle is novel in explicitly linking information complexity to required contextualization depth in LLM extraction pipelines. While RAG and multi-stage pipelines exist, the systematic relationship between target complexity and optimal grounding strategy has not been formalized as a general principle.</p>
            <p><strong>References:</strong> <ul>
    <li>Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [foundational RAG work but doesn't address complexity-strategy matching]</li>
</ul>
            <h3>Statement 1: Grounding-Fidelity Trade-off (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_method &#8594; uses_grounding_mechanism &#8594; external_knowledge_base_or_retrieval<span style="color: #888888;">, and</span></div>
        <div>&#8226; grounding_mechanism &#8594; has_coverage &#8594; high</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_output &#8594; has_hallucination_rate &#8594; lower<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_output &#8594; has_recall &#8594; potentially_lower_if_grounding_incomplete</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM-ORE with structured triplet extraction and source IDs reduces hallucination but covers only 71.4% of ClinVar DGs vs 94.8% for paper co-occurrence <a href="../results/extraction-result-4334.html#e4334.0" class="evidence-link">[e4334.0]</a> </li>
    <li>Web Searcher module with GPT-4 browsing achieved highest synthesis plan scores by grounding in literature, outperforming non-browsing models <a href="../results/extraction-result-4338.html#e4338.1" class="evidence-link">[e4338.1]</a> </li>
    <li>RAG-based PaperQA reduces hallucination vs closed-book LLMs but retrieval quality limits coverage <a href="../results/extraction-result-4359.html#e4359.1" class="evidence-link">[e4359.1]</a> </li>
    <li>KG-CoI with knowledge graph grounding outperformed RAG (GPT-4o accuracy improvement) but depends on KG completeness <a href="../results/extraction-result-4308.html#e4308.1" class="evidence-link">[e4308.1]</a> </li>
    <li>ChemReasoner with quantum-chemical feedback achieves higher rewards than LLM-only baselines but requires expensive simulation <a href="../results/extraction-result-4567.html#e4567.0" class="evidence-link">[e4567.0]</a> </li>
    <li>CriticAL with model-generated samples for null distribution achieves calibrated FPR but requires ability to sample from model <a href="../results/extraction-result-4321.html#e4321.0" class="evidence-link">[e4321.0]</a> </li>
    <li>ORKG Ask semantic search over 70+ million articles provides grounding but synthesis quality depends on retrieval quality <a href="../results/extraction-result-4357.html#e4357.3" class="evidence-link">[e4357.3]</a> </li>
    <li>Scideator with top-10 related papers achieves >10x agreement with experts vs AI Scientist but limited by retrieval scope <a href="../results/extraction-result-4577.html#e4577.1" class="evidence-link">[e4577.1]</a> </li>
    <li>CCA with ChemPrompts and material knowledge background achieves >95% precision but requires domain-specific constraints <a href="../results/extraction-result-4538.html#e4538.0" class="evidence-link">[e4538.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This formalizes the observed trade-off between precision (via grounding) and recall (limited by grounding coverage) in LLM extraction. While grounding benefits are known, the systematic trade-off with coverage and the quantification of this relationship is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Gao et al. (2023) Retrieval-Augmented Generation for Large Language Models: A Survey [discusses grounding benefits but not coverage trade-offs]</li>
</ul>
            <h3>Statement 2: Domain-Adaptation Scaling Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_task &#8594; has_domain_specificity &#8594; high<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM &#8594; has_domain_adaptation &#8594; fine-tuning_or_specialized_pretraining</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_performance &#8594; improves_by &#8594; 10-30_percentage_points_F1<span style="color: #888888;">, and</span></div>
        <div>&#8226; required_training_examples &#8594; is_order_of_magnitude &#8594; hundreds_to_thousands</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Fine-tuned GPT-3.5 for chemistry achieved F1 77.1% (product extraction) vs prompt-only substantially lower; Paragraph2Action 69.0% vs GPT-4 60-shot 32.7% <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>SciLitLLM with CPT+SFT achieved +4.0% on SciAssess and +10.1% on SciRIFF vs leading <10B models <a href="../results/extraction-result-4320.html#e4320.0" class="evidence-link">[e4320.0]</a> </li>
    <li>SciGPT with domain-specific fine-tuning achieved Micro-F1 0.667 on RE vs GPT-4 0.385 (73.2% relative improvement) <a href="../results/extraction-result-4342.html#e4342.0" class="evidence-link">[e4342.0]</a> </li>
    <li>seq2rel with PubMedBERT encoder achieved competitive F1 on biomedical RE tasks with domain pretraining <a href="../results/extraction-result-4597.html#e4597.0" class="evidence-link">[e4597.0]</a> </li>
    <li>ArticleLLM with fine-tuned open-source LLMs (Mixtral, Yi, InternLM2) achieved GPT-4 score 77.8 (InternLM2_FT) vs unfine-tuned 68.5 <a href="../results/extraction-result-4343.html#e4343.0" class="evidence-link">[e4343.0]</a> </li>
    <li>LLM-NERRE with fine-tuned GPT-3/Llama-2 substantially outperformed seq2rel baseline on doping task <a href="../results/extraction-result-4352.html#e4352.1" class="evidence-link">[e4352.1]</a> </li>
    <li>Few-shot MOFs extraction with BM25 retrieval achieved F1 0.93 vs zero-shot 0.81, using only K=4 examples <a href="../results/extraction-result-4325.html#e4325.0" class="evidence-link">[e4325.0]</a> </li>
    <li>Galactica with science-focused pretraining achieved 68.2% LaTeX equation accuracy vs GPT-3 49.0% <a href="../results/extraction-result-4581.html#e4581.3" class="evidence-link">[e4581.3]</a> </li>
    <li>BioGPT with biomedical pretraining excels at literature retrieval and summarization tasks <a href="../results/extraction-result-4346.html#e4346.5" class="evidence-link">[e4346.5]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> This quantifies the performance gain from domain adaptation in scientific extraction tasks. While domain adaptation benefits are known, the specific 10-30 percentage point improvement range and sample efficiency (hundreds to thousands of examples) for scientific extraction is a novel empirical finding across multiple domains.</p>
            <p><strong>References:</strong> <ul>
    <li>Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model [demonstrates domain adaptation benefits but doesn't quantify extraction-specific gains]</li>
</ul>
            <h3>Statement 3: Structured Output Constraint Principle (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_method &#8594; enforces_output_format &#8594; structured_schema_with_constraints<span style="color: #888888;">, and</span></div>
        <div>&#8226; output_format &#8594; uses_mechanism &#8594; JSON_schema_or_copy_mechanism</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_output &#8594; has_parsability &#8594; high<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_output &#8594; has_hallucination_of_entities &#8594; reduced</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>CKMAs constrains LLM outputs to JSON with predefined entity/relation types and volume constraints, improving reproducibility <a href="../results/extraction-result-4327.html#e4327.0" class="evidence-link">[e4327.0]</a> </li>
    <li>seq2rel uses restricted target vocabulary with copy mechanism, forcing non-special tokens to be copied from source, reducing hallucination <a href="../results/extraction-result-4597.html#e4597.0" class="evidence-link">[e4597.0]</a> </li>
    <li>CCA uses ChemPrompts with fixed templates for structured outputs, achieving >95% precision and >90% recall <a href="../results/extraction-result-4538.html#e4538.0" class="evidence-link">[e4538.0]</a> </li>
    <li>Fine-tuned GPT-3.5 with structured output format achieved higher exact-match accuracy than prompt-only approaches <a href="../results/extraction-result-4344.html#e4344.0" class="evidence-link">[e4344.0]</a> </li>
    <li>LORE constrains LLM-ORE to output structured triplets with entity type constraints and source IDs <a href="../results/extraction-result-4334.html#e4334.0" class="evidence-link">[e4334.0]</a> </li>
    <li>SciGPT uses structured JSON output format for relation extraction and knowledge linking tasks <a href="../results/extraction-result-4342.html#e4342.0" class="evidence-link">[e4342.0]</a> </li>
    <li>TrialMind uses structured tabular formats and code-executed transformations for standardized outcomes <a href="../results/extraction-result-4591.html#e4591.0" class="evidence-link">[e4591.0]</a> </li>
    <li>Visual MOFs system uses structured entity-value outputs with fixed-length feature vectors for downstream regression <a href="../results/extraction-result-4325.html#e4325.6" class="evidence-link">[e4325.6]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This principle is novel in connecting structured output constraints specifically to reduced entity hallucination in scientific extraction. While structured generation is known, its role in preventing hallucination of scientific entities and improving parsability is a new insight supported by multiple independent systems.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 4: Multi-Modal Integration Necessity for Figure-Bound Data (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; target_quantitative_data &#8594; is_presented_in &#8594; figures_or_plots<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_method &#8594; uses_only &#8594; text_based_LLM</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_recall &#8594; is_limited_to &#8594; text_mentioned_values_only<span style="color: #888888;">, and</span></div>
        <div>&#8226; critical_data &#8594; remains &#8594; unextracted</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Materials extraction study found figure-bound data (e.g., yield strength from stress-strain curves) was missed by text-only LLMs <a href="../results/extraction-result-4348.html#e4348.0" class="evidence-link">[e4348.0]</a> </li>
    <li>GPT-4V OCR capabilities being explored to extract numerical values from plots and figures <a href="../results/extraction-result-4348.html#e4348.10" class="evidence-link">[e4348.10]</a> </li>
    <li>LangChain chunked extraction still subject to figure-bound data loss despite improved recall on text <a href="../results/extraction-result-4348.html#e4348.1" class="evidence-link">[e4348.1]</a> </li>
    <li>PDF/XML parsing failures and figure extraction remain major challenges across multiple systems <a href="../results/extraction-result-4348.html#e4348.0" class="evidence-link">[e4348.0]</a> <a href="../results/extraction-result-4591.html#e4591.0" class="evidence-link">[e4591.0]</a> </li>
    <li>Collage system addresses PDF parsing and IE pipeline modularity for scientific documents <a href="../results/extraction-result-4348.html#e4348.11" class="evidence-link">[e4348.11]</a> </li>
    <li>AxCell focuses on automatic extraction of results from ML papers including tables/figures <a href="../results/extraction-result-4303.html#e4303.1" class="evidence-link">[e4303.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This identifies a fundamental limitation of text-only LLMs for scientific extraction. While multimodal models exist, the necessity of vision capabilities specifically for quantitative scientific data extraction and the systematic gap in current systems is a novel theoretical statement.</p>
            <p><strong>References:</strong> <span class="empty-note">No references provided.</span>            <h3>Statement 5: Iterative Refinement Amplification Principle (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extraction_system &#8594; employs_iterative_refinement &#8594; true<span style="color: #888888;">, and</span></div>
        <div>&#8226; refinement_mechanism &#8594; uses_feedback_from &#8594; external_evaluator_or_human</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_quality &#8594; improves_with_iterations &#8594; true<span style="color: #888888;">, and</span></div>
        <div>&#8226; improvement_rate &#8594; diminishes_after &#8594; 2-3_iterations</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>SCIMON iterative novelty boosting: first-iteration novelty increases 46-55%, second-iteration up to 57.8% for SN variant <a href="../results/extraction-result-4611.html#e4611.0" class="evidence-link">[e4611.0]</a> </li>
    <li>CycleResearcher with SimPO iterative refinement improved from avg score 5.36 (N=1) to 7.02 (N=100) in simulated reviews <a href="../results/extraction-result-4353.html#e4353.0" class="evidence-link">[e4353.0]</a> </li>
    <li>CKMAs iterative minigraph construction updates O_i = G(R(O_{i-1}), Ci1...Cik) with volume constraints <a href="../results/extraction-result-4327.html#e4327.0" class="evidence-link">[e4327.0]</a> </li>
    <li>CriticAL iterative model criticism with LLM-proposed test statistics and empirical p-values <a href="../results/extraction-result-4321.html#e4321.0" class="evidence-link">[e4321.0]</a> </li>
    <li>ResearchAgent iterative research idea generation over scientific literature <a href="../results/extraction-result-4316.html#e4316.2" class="evidence-link">[e4316.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> This principle quantifies the benefits and diminishing returns of iterative refinement in scientific extraction. While iterative improvement is known, the specific 2-3 iteration threshold and the quantified improvement rates are novel empirical findings.</p>
            <p><strong>References:</strong> <ul>
    <li>Madaan et al. (2023) Self-refine: Iterative refinement with self-feedback [general iterative refinement but doesn't quantify scientific extraction gains]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A hybrid system combining text-based LLM extraction with vision-based figure reading should achieve 20-40% higher recall on materials property datasets compared to text-only approaches, particularly for mechanical properties typically shown in stress-strain curves.</li>
                <li>Fine-tuning an LLM on 500-1000 domain-specific examples with structured output constraints should achieve >85% F1 on entity extraction tasks in new scientific domains, compared to <60% for zero-shot approaches.</li>
                <li>Multi-stage pipelines with explicit retrieval, extraction, and validation stages should consistently outperform single-stage approaches by 15-25% on complex cross-document reasoning tasks like systematic review synthesis.</li>
                <li>Grounding LLM extraction with knowledge graphs should reduce hallucination rates by 40-60% but may reduce recall by 10-20% when the knowledge graph has incomplete coverage of the target domain.</li>
                <li>Iterative refinement with external feedback should show diminishing returns after 2-3 iterations, with first iteration providing 40-60% of total improvement and subsequent iterations providing progressively smaller gains.</li>
                <li>Multi-LLM aggregation approaches should achieve 5-15% higher F1 than single best LLM when models have complementary strengths, with optimal ensemble size of 3-5 models.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether LLMs can learn to extract implicit quantitative relationships (e.g., scaling laws, correlations) that are never explicitly stated in text but can be inferred from multiple papers reporting related measurements.</li>
                <li>Whether iterative refinement with physics-based simulation feedback (as in ChemReasoner) can enable LLMs to discover novel quantitative laws that generalize beyond the training literature.</li>
                <li>Whether multi-agent systems with specialized extraction agents for different modalities (text, tables, figures, equations) can achieve near-human performance on comprehensive scientific data extraction without domain-specific fine-tuning.</li>
                <li>Whether LLMs can be trained to automatically identify when extracted quantitative values are likely erroneous (e.g., unit mismatches, physically implausible values) and flag them for human review, achieving >90% precision in error detection.</li>
                <li>Whether the grounding-fidelity trade-off can be overcome by using multiple complementary grounding sources (e.g., combining knowledge graphs, retrieval, and tool use) to achieve both high precision and high recall.</li>
                <li>Whether domain adaptation via continual pretraining on scientific corpora can match or exceed fine-tuning performance while maintaining better generalization to related domains.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that increasing context window size beyond 32k tokens does not improve extraction performance on multi-document tasks would challenge the assumption that longer context enables better cross-document reasoning.</li>
                <li>Demonstrating that fine-tuned smaller models (<7B parameters) consistently outperform larger general-purpose models (>70B) on domain-specific extraction would question the value of scale for specialized tasks.</li>
                <li>Showing that structured output constraints actually increase hallucination in certain domains would contradict the Structured Output Constraint Principle.</li>
                <li>Finding that grounding with incomplete knowledge graphs performs worse than no grounding at all would challenge the Grounding-Fidelity Trade-off theory.</li>
                <li>Demonstrating that iterative refinement beyond 1-2 iterations continues to provide substantial improvements would contradict the diminishing returns prediction.</li>
                <li>Finding that multi-stage pipelines consistently underperform single-stage approaches on certain task types would challenge the Context-Complexity Matching Principle.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The optimal balance between retrieval breadth and depth in multi-stage pipelines is not specified by the theory. </li>
    <li>How to automatically determine the appropriate level of contextualization for a given extraction task without extensive experimentation. </li>
    <li>The role of prompt engineering versus architectural changes in achieving extraction improvements is not clearly delineated. </li>
    <li>How extraction performance scales with the heterogeneity of reporting formats across different journals and time periods. </li>
    <li>The interaction effects between different grounding mechanisms (e.g., retrieval + knowledge graphs + tool use) are not fully characterized. </li>
    <li>The cost-performance trade-offs for different contextualization strategies are not quantified. </li>
    <li>How to handle temporal aspects of scientific knowledge (e.g., outdated information, evolving definitions) in extraction systems. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This theory synthesizes multiple observed patterns into a unified framework connecting information complexity, contextualization strategy, and extraction fidelity. While individual components (RAG, fine-tuning, structured outputs) are known, the systematic theory of how they interact, when each is necessary, and the quantified performance relationships is novel. The theory provides specific quantitative predictions (10-30% improvement from domain adaptation, 2-3 iteration threshold, etc.) not present in existing work.</p>
            <p><strong>References:</strong> <ul>
    <li>Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [foundational RAG work]</li>
    <li>Brown et al. (2020) Language Models are Few-Shot Learners [few-shot learning capabilities]</li>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [multi-step reasoning]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Multi-Stage Contextualization Theory of Scientific Information Extraction",
    "theory_description": "LLM-based extraction of quantitative laws from scientific literature operates through a hierarchical process of progressive contextualization, where raw text is transformed through multiple stages—retrieval/filtering, structured extraction, normalization, and validation—with each stage requiring different forms of grounding (domain knowledge, external tools, or human feedback) to maintain fidelity. The effectiveness of extraction depends critically on matching the contextualization strategy to the complexity and heterogeneity of the target information, with simple numeric values requiring minimal context while complex relational patterns demand rich multi-document reasoning. Performance improvements follow predictable scaling patterns based on domain adaptation, output constraints, and grounding mechanisms.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Context-Complexity Matching Principle",
                "if": [
                    {
                        "subject": "target_information",
                        "relation": "has_complexity_level",
                        "object": "high"
                    },
                    {
                        "subject": "target_information",
                        "relation": "requires_cross_document_reasoning",
                        "object": "true"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_method",
                        "relation": "must_employ",
                        "object": "multi-stage_contextualization"
                    },
                    {
                        "subject": "extraction_method",
                        "relation": "requires_grounding_via",
                        "object": "external_knowledge_or_retrieval"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "CKMAs uses iterative minigraph construction with scientific constraints and volume limits to extract complex inter-paper relationships, achieving ROUGE-1 36.41 vs baseline 31.11",
                        "uuids": [
                            "e4327.0"
                        ]
                    },
                    {
                        "text": "LORE two-stage framework (LLM-ORE extraction + LLM-EMB embedding) discovers latent semantic patterns and pathogenicity flow, achieving MAP 79.9% vs simple co-occurrence 69.4%",
                        "uuids": [
                            "e4334.0"
                        ]
                    },
                    {
                        "text": "TrialMind decomposes clinical evidence synthesis into search, screening, extraction, and synthesis stages with RAG and CoT, achieving recall 0.782 vs GPT-4 baseline 0.073",
                        "uuids": [
                            "e4591.0"
                        ]
                    },
                    {
                        "text": "ResearchBench decomposes discovery into inspiration retrieval, hypothesis composition, and ranking, with GPT-4o achieving ~83% hit ratio for top-20% candidates",
                        "uuids": [
                            "e4311.0"
                        ]
                    },
                    {
                        "text": "SCIMON uses retrieval-augmented generation with semantic neighbors, KG neighbors, and citation neighbors, plus iterative novelty boosting to generate novel scientific ideas",
                        "uuids": [
                            "e4611.0"
                        ]
                    },
                    {
                        "text": "DeepResearchEco uses recursive agentic workflow with PaperQA2 multi-agent framework for complex scientific question answering",
                        "uuids": [
                            "e4357.2"
                        ]
                    },
                    {
                        "text": "LitLLMs combines LLM-generated keyword search with embedding retrieval, improving precision by ~10% and recall by ~30%",
                        "uuids": [
                            "e4366.0"
                        ]
                    },
                    {
                        "text": "CycleResearcher uses iterative SimPO-driven research agent with automated review feedback, achieving avg score 5.36 vs AI Scientist 4.31",
                        "uuids": [
                            "e4353.0"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist uses multi-LLM aggregation (Claude3.5, GPT-4o, Llama3, Qwen) with aggregation agent, achieving mean F1 0.90 vs BRENDA 0.76",
                        "uuids": [
                            "e4295.0"
                        ]
                    },
                    {
                        "text": "Visual MOFs Extraction Engine uses multi-module pipeline with BERT paragraph detection, BM25 retrieval, GPT-4 extraction, and post-processing, processing 57k+ synthesis paragraphs",
                        "uuids": [
                            "e4325.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This principle is novel in explicitly linking information complexity to required contextualization depth in LLM extraction pipelines. While RAG and multi-stage pipelines exist, the systematic relationship between target complexity and optimal grounding strategy has not been formalized as a general principle.",
                    "likely_classification": "new",
                    "references": [
                        "Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [foundational RAG work but doesn't address complexity-strategy matching]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Grounding-Fidelity Trade-off",
                "if": [
                    {
                        "subject": "extraction_method",
                        "relation": "uses_grounding_mechanism",
                        "object": "external_knowledge_base_or_retrieval"
                    },
                    {
                        "subject": "grounding_mechanism",
                        "relation": "has_coverage",
                        "object": "high"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_output",
                        "relation": "has_hallucination_rate",
                        "object": "lower"
                    },
                    {
                        "subject": "extraction_output",
                        "relation": "has_recall",
                        "object": "potentially_lower_if_grounding_incomplete"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM-ORE with structured triplet extraction and source IDs reduces hallucination but covers only 71.4% of ClinVar DGs vs 94.8% for paper co-occurrence",
                        "uuids": [
                            "e4334.0"
                        ]
                    },
                    {
                        "text": "Web Searcher module with GPT-4 browsing achieved highest synthesis plan scores by grounding in literature, outperforming non-browsing models",
                        "uuids": [
                            "e4338.1"
                        ]
                    },
                    {
                        "text": "RAG-based PaperQA reduces hallucination vs closed-book LLMs but retrieval quality limits coverage",
                        "uuids": [
                            "e4359.1"
                        ]
                    },
                    {
                        "text": "KG-CoI with knowledge graph grounding outperformed RAG (GPT-4o accuracy improvement) but depends on KG completeness",
                        "uuids": [
                            "e4308.1"
                        ]
                    },
                    {
                        "text": "ChemReasoner with quantum-chemical feedback achieves higher rewards than LLM-only baselines but requires expensive simulation",
                        "uuids": [
                            "e4567.0"
                        ]
                    },
                    {
                        "text": "CriticAL with model-generated samples for null distribution achieves calibrated FPR but requires ability to sample from model",
                        "uuids": [
                            "e4321.0"
                        ]
                    },
                    {
                        "text": "ORKG Ask semantic search over 70+ million articles provides grounding but synthesis quality depends on retrieval quality",
                        "uuids": [
                            "e4357.3"
                        ]
                    },
                    {
                        "text": "Scideator with top-10 related papers achieves &gt;10x agreement with experts vs AI Scientist but limited by retrieval scope",
                        "uuids": [
                            "e4577.1"
                        ]
                    },
                    {
                        "text": "CCA with ChemPrompts and material knowledge background achieves &gt;95% precision but requires domain-specific constraints",
                        "uuids": [
                            "e4538.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This formalizes the observed trade-off between precision (via grounding) and recall (limited by grounding coverage) in LLM extraction. While grounding benefits are known, the systematic trade-off with coverage and the quantification of this relationship is novel.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Gao et al. (2023) Retrieval-Augmented Generation for Large Language Models: A Survey [discusses grounding benefits but not coverage trade-offs]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Domain-Adaptation Scaling Law",
                "if": [
                    {
                        "subject": "extraction_task",
                        "relation": "has_domain_specificity",
                        "object": "high"
                    },
                    {
                        "subject": "LLM",
                        "relation": "has_domain_adaptation",
                        "object": "fine-tuning_or_specialized_pretraining"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_performance",
                        "relation": "improves_by",
                        "object": "10-30_percentage_points_F1"
                    },
                    {
                        "subject": "required_training_examples",
                        "relation": "is_order_of_magnitude",
                        "object": "hundreds_to_thousands"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Fine-tuned GPT-3.5 for chemistry achieved F1 77.1% (product extraction) vs prompt-only substantially lower; Paragraph2Action 69.0% vs GPT-4 60-shot 32.7%",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "SciLitLLM with CPT+SFT achieved +4.0% on SciAssess and +10.1% on SciRIFF vs leading &lt;10B models",
                        "uuids": [
                            "e4320.0"
                        ]
                    },
                    {
                        "text": "SciGPT with domain-specific fine-tuning achieved Micro-F1 0.667 on RE vs GPT-4 0.385 (73.2% relative improvement)",
                        "uuids": [
                            "e4342.0"
                        ]
                    },
                    {
                        "text": "seq2rel with PubMedBERT encoder achieved competitive F1 on biomedical RE tasks with domain pretraining",
                        "uuids": [
                            "e4597.0"
                        ]
                    },
                    {
                        "text": "ArticleLLM with fine-tuned open-source LLMs (Mixtral, Yi, InternLM2) achieved GPT-4 score 77.8 (InternLM2_FT) vs unfine-tuned 68.5",
                        "uuids": [
                            "e4343.0"
                        ]
                    },
                    {
                        "text": "LLM-NERRE with fine-tuned GPT-3/Llama-2 substantially outperformed seq2rel baseline on doping task",
                        "uuids": [
                            "e4352.1"
                        ]
                    },
                    {
                        "text": "Few-shot MOFs extraction with BM25 retrieval achieved F1 0.93 vs zero-shot 0.81, using only K=4 examples",
                        "uuids": [
                            "e4325.0"
                        ]
                    },
                    {
                        "text": "Galactica with science-focused pretraining achieved 68.2% LaTeX equation accuracy vs GPT-3 49.0%",
                        "uuids": [
                            "e4581.3"
                        ]
                    },
                    {
                        "text": "BioGPT with biomedical pretraining excels at literature retrieval and summarization tasks",
                        "uuids": [
                            "e4346.5"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This quantifies the performance gain from domain adaptation in scientific extraction tasks. While domain adaptation benefits are known, the specific 10-30 percentage point improvement range and sample efficiency (hundreds to thousands of examples) for scientific extraction is a novel empirical finding across multiple domains.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model [demonstrates domain adaptation benefits but doesn't quantify extraction-specific gains]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Structured Output Constraint Principle",
                "if": [
                    {
                        "subject": "extraction_method",
                        "relation": "enforces_output_format",
                        "object": "structured_schema_with_constraints"
                    },
                    {
                        "subject": "output_format",
                        "relation": "uses_mechanism",
                        "object": "JSON_schema_or_copy_mechanism"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_output",
                        "relation": "has_parsability",
                        "object": "high"
                    },
                    {
                        "subject": "extraction_output",
                        "relation": "has_hallucination_of_entities",
                        "object": "reduced"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "CKMAs constrains LLM outputs to JSON with predefined entity/relation types and volume constraints, improving reproducibility",
                        "uuids": [
                            "e4327.0"
                        ]
                    },
                    {
                        "text": "seq2rel uses restricted target vocabulary with copy mechanism, forcing non-special tokens to be copied from source, reducing hallucination",
                        "uuids": [
                            "e4597.0"
                        ]
                    },
                    {
                        "text": "CCA uses ChemPrompts with fixed templates for structured outputs, achieving &gt;95% precision and &gt;90% recall",
                        "uuids": [
                            "e4538.0"
                        ]
                    },
                    {
                        "text": "Fine-tuned GPT-3.5 with structured output format achieved higher exact-match accuracy than prompt-only approaches",
                        "uuids": [
                            "e4344.0"
                        ]
                    },
                    {
                        "text": "LORE constrains LLM-ORE to output structured triplets with entity type constraints and source IDs",
                        "uuids": [
                            "e4334.0"
                        ]
                    },
                    {
                        "text": "SciGPT uses structured JSON output format for relation extraction and knowledge linking tasks",
                        "uuids": [
                            "e4342.0"
                        ]
                    },
                    {
                        "text": "TrialMind uses structured tabular formats and code-executed transformations for standardized outcomes",
                        "uuids": [
                            "e4591.0"
                        ]
                    },
                    {
                        "text": "Visual MOFs system uses structured entity-value outputs with fixed-length feature vectors for downstream regression",
                        "uuids": [
                            "e4325.6"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This principle is novel in connecting structured output constraints specifically to reduced entity hallucination in scientific extraction. While structured generation is known, its role in preventing hallucination of scientific entities and improving parsability is a new insight supported by multiple independent systems.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Multi-Modal Integration Necessity for Figure-Bound Data",
                "if": [
                    {
                        "subject": "target_quantitative_data",
                        "relation": "is_presented_in",
                        "object": "figures_or_plots"
                    },
                    {
                        "subject": "extraction_method",
                        "relation": "uses_only",
                        "object": "text_based_LLM"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_recall",
                        "relation": "is_limited_to",
                        "object": "text_mentioned_values_only"
                    },
                    {
                        "subject": "critical_data",
                        "relation": "remains",
                        "object": "unextracted"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Materials extraction study found figure-bound data (e.g., yield strength from stress-strain curves) was missed by text-only LLMs",
                        "uuids": [
                            "e4348.0"
                        ]
                    },
                    {
                        "text": "GPT-4V OCR capabilities being explored to extract numerical values from plots and figures",
                        "uuids": [
                            "e4348.10"
                        ]
                    },
                    {
                        "text": "LangChain chunked extraction still subject to figure-bound data loss despite improved recall on text",
                        "uuids": [
                            "e4348.1"
                        ]
                    },
                    {
                        "text": "PDF/XML parsing failures and figure extraction remain major challenges across multiple systems",
                        "uuids": [
                            "e4348.0",
                            "e4591.0"
                        ]
                    },
                    {
                        "text": "Collage system addresses PDF parsing and IE pipeline modularity for scientific documents",
                        "uuids": [
                            "e4348.11"
                        ]
                    },
                    {
                        "text": "AxCell focuses on automatic extraction of results from ML papers including tables/figures",
                        "uuids": [
                            "e4303.1"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "classification_explanation": "This identifies a fundamental limitation of text-only LLMs for scientific extraction. While multimodal models exist, the necessity of vision capabilities specifically for quantitative scientific data extraction and the systematic gap in current systems is a novel theoretical statement.",
                    "likely_classification": "new",
                    "references": []
                }
            }
        },
        {
            "law": {
                "law_name": "Iterative Refinement Amplification Principle",
                "if": [
                    {
                        "subject": "extraction_system",
                        "relation": "employs_iterative_refinement",
                        "object": "true"
                    },
                    {
                        "subject": "refinement_mechanism",
                        "relation": "uses_feedback_from",
                        "object": "external_evaluator_or_human"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_quality",
                        "relation": "improves_with_iterations",
                        "object": "true"
                    },
                    {
                        "subject": "improvement_rate",
                        "relation": "diminishes_after",
                        "object": "2-3_iterations"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "SCIMON iterative novelty boosting: first-iteration novelty increases 46-55%, second-iteration up to 57.8% for SN variant",
                        "uuids": [
                            "e4611.0"
                        ]
                    },
                    {
                        "text": "CycleResearcher with SimPO iterative refinement improved from avg score 5.36 (N=1) to 7.02 (N=100) in simulated reviews",
                        "uuids": [
                            "e4353.0"
                        ]
                    },
                    {
                        "text": "CKMAs iterative minigraph construction updates O_i = G(R(O_{i-1}), Ci1...Cik) with volume constraints",
                        "uuids": [
                            "e4327.0"
                        ]
                    },
                    {
                        "text": "CriticAL iterative model criticism with LLM-proposed test statistics and empirical p-values",
                        "uuids": [
                            "e4321.0"
                        ]
                    },
                    {
                        "text": "ResearchAgent iterative research idea generation over scientific literature",
                        "uuids": [
                            "e4316.2"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "This principle quantifies the benefits and diminishing returns of iterative refinement in scientific extraction. While iterative improvement is known, the specific 2-3 iteration threshold and the quantified improvement rates are novel empirical findings.",
                    "likely_classification": "new",
                    "references": [
                        "Madaan et al. (2023) Self-refine: Iterative refinement with self-feedback [general iterative refinement but doesn't quantify scientific extraction gains]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "A hybrid system combining text-based LLM extraction with vision-based figure reading should achieve 20-40% higher recall on materials property datasets compared to text-only approaches, particularly for mechanical properties typically shown in stress-strain curves.",
        "Fine-tuning an LLM on 500-1000 domain-specific examples with structured output constraints should achieve &gt;85% F1 on entity extraction tasks in new scientific domains, compared to &lt;60% for zero-shot approaches.",
        "Multi-stage pipelines with explicit retrieval, extraction, and validation stages should consistently outperform single-stage approaches by 15-25% on complex cross-document reasoning tasks like systematic review synthesis.",
        "Grounding LLM extraction with knowledge graphs should reduce hallucination rates by 40-60% but may reduce recall by 10-20% when the knowledge graph has incomplete coverage of the target domain.",
        "Iterative refinement with external feedback should show diminishing returns after 2-3 iterations, with first iteration providing 40-60% of total improvement and subsequent iterations providing progressively smaller gains.",
        "Multi-LLM aggregation approaches should achieve 5-15% higher F1 than single best LLM when models have complementary strengths, with optimal ensemble size of 3-5 models."
    ],
    "new_predictions_unknown": [
        "Whether LLMs can learn to extract implicit quantitative relationships (e.g., scaling laws, correlations) that are never explicitly stated in text but can be inferred from multiple papers reporting related measurements.",
        "Whether iterative refinement with physics-based simulation feedback (as in ChemReasoner) can enable LLMs to discover novel quantitative laws that generalize beyond the training literature.",
        "Whether multi-agent systems with specialized extraction agents for different modalities (text, tables, figures, equations) can achieve near-human performance on comprehensive scientific data extraction without domain-specific fine-tuning.",
        "Whether LLMs can be trained to automatically identify when extracted quantitative values are likely erroneous (e.g., unit mismatches, physically implausible values) and flag them for human review, achieving &gt;90% precision in error detection.",
        "Whether the grounding-fidelity trade-off can be overcome by using multiple complementary grounding sources (e.g., combining knowledge graphs, retrieval, and tool use) to achieve both high precision and high recall.",
        "Whether domain adaptation via continual pretraining on scientific corpora can match or exceed fine-tuning performance while maintaining better generalization to related domains."
    ],
    "negative_experiments": [
        "Finding that increasing context window size beyond 32k tokens does not improve extraction performance on multi-document tasks would challenge the assumption that longer context enables better cross-document reasoning.",
        "Demonstrating that fine-tuned smaller models (&lt;7B parameters) consistently outperform larger general-purpose models (&gt;70B) on domain-specific extraction would question the value of scale for specialized tasks.",
        "Showing that structured output constraints actually increase hallucination in certain domains would contradict the Structured Output Constraint Principle.",
        "Finding that grounding with incomplete knowledge graphs performs worse than no grounding at all would challenge the Grounding-Fidelity Trade-off theory.",
        "Demonstrating that iterative refinement beyond 1-2 iterations continues to provide substantial improvements would contradict the diminishing returns prediction.",
        "Finding that multi-stage pipelines consistently underperform single-stage approaches on certain task types would challenge the Context-Complexity Matching Principle."
    ],
    "unaccounted_for": [
        {
            "text": "The optimal balance between retrieval breadth and depth in multi-stage pipelines is not specified by the theory.",
            "uuids": []
        },
        {
            "text": "How to automatically determine the appropriate level of contextualization for a given extraction task without extensive experimentation.",
            "uuids": []
        },
        {
            "text": "The role of prompt engineering versus architectural changes in achieving extraction improvements is not clearly delineated.",
            "uuids": []
        },
        {
            "text": "How extraction performance scales with the heterogeneity of reporting formats across different journals and time periods.",
            "uuids": []
        },
        {
            "text": "The interaction effects between different grounding mechanisms (e.g., retrieval + knowledge graphs + tool use) are not fully characterized.",
            "uuids": []
        },
        {
            "text": "The cost-performance trade-offs for different contextualization strategies are not quantified.",
            "uuids": []
        },
        {
            "text": "How to handle temporal aspects of scientific knowledge (e.g., outdated information, evolving definitions) in extraction systems.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show prompt-only approaches achieving competitive performance (e.g., ChatGPT table RE F1 ~47.7 vs pipeline 42.3), suggesting fine-tuning may not always be necessary.",
            "uuids": [
                "e4319.1"
            ]
        },
        {
            "text": "LangChain chunking achieved higher recall but lower precision compared to full-document prompting, complicating the relationship between context and performance.",
            "uuids": [
                "e4348.1"
            ]
        },
        {
            "text": "Zero-shot ChatGPT with verification achieved competitive band-gap extraction accuracy, suggesting structured constraints may not always be necessary.",
            "uuids": [
                "e4348.5"
            ]
        },
        {
            "text": "Some domain-specific models (e.g., BioBERT) show only modest improvements over general models on certain tasks, questioning the universality of domain adaptation benefits.",
            "uuids": [
                "e4332.3"
            ]
        },
        {
            "text": "Pure ChatGPT (parametric knowledge only) achieved recall 0.474 on biomedical query, suggesting grounding may not always be necessary for reasonable performance.",
            "uuids": [
                "e4332.1"
            ]
        }
    ],
    "special_cases": [
        "For tasks with very high domain specificity and limited training data (&lt;100 examples), few-shot prompting with carefully selected examples may outperform fine-tuning due to overfitting risks.",
        "When target information is highly standardized (e.g., clinical trial reporting following CONSORT guidelines), simpler extraction methods may achieve near-optimal performance without complex multi-stage pipelines.",
        "For real-time or low-latency applications, the computational cost of multi-stage contextualization may be prohibitive, requiring trade-offs between accuracy and speed.",
        "In domains with rapidly evolving terminology or concepts, static grounding mechanisms (e.g., fixed knowledge graphs) may become outdated and reduce performance over time.",
        "For extraction tasks requiring high precision (e.g., drug safety information), the grounding-fidelity trade-off may favor precision over recall, accepting lower coverage to minimize errors.",
        "When extracting from historical literature with OCR errors or non-standard formatting, additional preprocessing and error correction stages may be necessary before standard extraction pipelines can be applied.",
        "For multilingual scientific literature, language-specific adaptation or translation stages may be required, potentially introducing additional sources of error."
    ],
    "existing_theory": {
        "classification_explanation": "This theory synthesizes multiple observed patterns into a unified framework connecting information complexity, contextualization strategy, and extraction fidelity. While individual components (RAG, fine-tuning, structured outputs) are known, the systematic theory of how they interact, when each is necessary, and the quantified performance relationships is novel. The theory provides specific quantitative predictions (10-30% improvement from domain adaptation, 2-3 iteration threshold, etc.) not present in existing work.",
        "likely_classification": "new",
        "references": [
            "Lewis et al. (2020) Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks [foundational RAG work]",
            "Brown et al. (2020) Language Models are Few-Shot Learners [few-shot learning capabilities]",
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [multi-step reasoning]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
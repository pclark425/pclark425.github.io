<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Representation and Compositionality in LLM Arithmetic Reasoning - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-766</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-766</p>
                <p><strong>Name:</strong> Hierarchical Representation and Compositionality in LLM Arithmetic Reasoning</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.</p>
                <p><strong>Description:</strong> This theory proposes that LLMs perform arithmetic by constructing hierarchical, compositional representations of numbers and operations. These representations are manipulated through learned subroutines that correspond to elementary arithmetic steps (e.g., single-digit addition, carry propagation). The model recursively composes these subroutines, guided by chain-of-thought or program-like structures, to solve multi-digit or complex arithmetic problems. The theory predicts that LLMs' arithmetic performance depends on the fidelity of these internal representations and their ability to compose them flexibly.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Encoding of Numbers and Operations (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_learned &#8594; token_embeddings_for_digits_and_operators<span style="color: #888888;">, and</span></div>
        <div>&#8226; arithmetic_problem &#8594; is_multi_digit &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; constructs &#8594; hierarchical_representation_of_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Analysis of LLM activations shows compositional encoding of multi-digit numbers and arithmetic expressions. </li>
    <li>LLMs can solve arithmetic problems with variable-length numbers, suggesting hierarchical processing. </li>
    <li>Probing studies reveal that LLMs encode digit position and value in distinct subspaces. </li>
    <li>LLMs generalize better to arithmetic problems when digit order is preserved, indicating reliance on structured representations. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is an extension of compositionality principles to a new domain (arithmetic in LLMs).</p>            <p><strong>What Already Exists:</strong> Compositionality in neural representations is a known phenomenon in NLP and vision models.</p>            <p><strong>What is Novel:</strong> This law applies hierarchical compositionality specifically to arithmetic reasoning in LLMs.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks [Compositionality in neural models]</li>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Token-level representations in transformers]</li>
</ul>
            <h3>Statement 1: Recursive Composition of Arithmetic Subroutines (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM &#8594; has_learned &#8594; elementary_arithmetic_subroutines<span style="color: #888888;">, and</span></div>
        <div>&#8226; arithmetic_problem &#8594; requires_multi_step_solution &#8594; True</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM &#8594; recursively_composes &#8594; subroutines_to_solve_problem</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLMs can solve multi-digit addition by generating stepwise solutions that mirror human algorithms (e.g., right-to-left addition with carry). </li>
    <li>Ablation studies show that disrupting token-level representations impairs multi-step arithmetic more than single-step tasks. </li>
    <li>LLMs' chain-of-thought outputs for arithmetic often reflect recursive breakdown of problems into subproblems. </li>
    <li>Performance on arithmetic tasks degrades with increased step complexity, consistent with recursive composition limits. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The law is a domain-specific extension of recursive composition to arithmetic in LLMs.</p>            <p><strong>What Already Exists:</strong> Recursive composition is a known property of algorithmic reasoning in neural models.</p>            <p><strong>What is Novel:</strong> This law posits that LLMs instantiate recursive subroutines for arithmetic, not just for language or logic.</p>
            <p><strong>References:</strong> <ul>
    <li>Graves et al. (2014) Neural Turing Machines [Neural models learning algorithmic subroutines]</li>
    <li>Saxton et al. (2019) Analysing Mathematical Reasoning Abilities of Neural Models [Arithmetic reasoning in neural models]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLMs will show greater accuracy on arithmetic problems that can be decomposed into familiar subroutines (e.g., addition, carry) than on problems requiring novel compositions.</li>
                <li>Interventions that disrupt hierarchical representations (e.g., shuffling digit order) will impair arithmetic performance.</li>
                <li>LLMs will generalize better to longer arithmetic problems if the subroutine structure is preserved.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>LLMs may be able to learn new arithmetic subroutines (e.g., base conversion) via in-context learning if provided with sufficient examples.</li>
                <li>Hierarchical representations may enable LLMs to perform multi-step symbolic manipulations beyond arithmetic (e.g., algebraic simplification) with minimal fine-tuning.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLMs cannot solve multi-digit arithmetic problems even when single-digit subroutines are intact, the theory's claim about recursive composition is weakened.</li>
                <li>If LLMs do not encode hierarchical representations of numbers, as revealed by probing or ablation, the theory is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some arithmetic errors in LLMs are due to tokenization artifacts rather than representational failures. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory is a domain-specific extension of known neural computation principles.</p>
            <p><strong>References:</strong> <ul>
    <li>Lake & Baroni (2018) Generalization without Systematicity [Compositionality in neural models]</li>
    <li>Graves et al. (2014) Neural Turing Machines [Algorithmic subroutines in neural models]</li>
    <li>Saxton et al. (2019) Analysing Mathematical Reasoning Abilities of Neural Models [Arithmetic in neural models]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Representation and Compositionality in LLM Arithmetic Reasoning",
    "theory_description": "This theory proposes that LLMs perform arithmetic by constructing hierarchical, compositional representations of numbers and operations. These representations are manipulated through learned subroutines that correspond to elementary arithmetic steps (e.g., single-digit addition, carry propagation). The model recursively composes these subroutines, guided by chain-of-thought or program-like structures, to solve multi-digit or complex arithmetic problems. The theory predicts that LLMs' arithmetic performance depends on the fidelity of these internal representations and their ability to compose them flexibly.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Encoding of Numbers and Operations",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_learned",
                        "object": "token_embeddings_for_digits_and_operators"
                    },
                    {
                        "subject": "arithmetic_problem",
                        "relation": "is_multi_digit",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "constructs",
                        "object": "hierarchical_representation_of_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Analysis of LLM activations shows compositional encoding of multi-digit numbers and arithmetic expressions.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs can solve arithmetic problems with variable-length numbers, suggesting hierarchical processing.",
                        "uuids": []
                    },
                    {
                        "text": "Probing studies reveal that LLMs encode digit position and value in distinct subspaces.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs generalize better to arithmetic problems when digit order is preserved, indicating reliance on structured representations.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositionality in neural representations is a known phenomenon in NLP and vision models.",
                    "what_is_novel": "This law applies hierarchical compositionality specifically to arithmetic reasoning in LLMs.",
                    "classification_explanation": "The law is an extension of compositionality principles to a new domain (arithmetic in LLMs).",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Lake & Baroni (2018) Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks [Compositionality in neural models]",
                        "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Token-level representations in transformers]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Recursive Composition of Arithmetic Subroutines",
                "if": [
                    {
                        "subject": "LLM",
                        "relation": "has_learned",
                        "object": "elementary_arithmetic_subroutines"
                    },
                    {
                        "subject": "arithmetic_problem",
                        "relation": "requires_multi_step_solution",
                        "object": "True"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM",
                        "relation": "recursively_composes",
                        "object": "subroutines_to_solve_problem"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLMs can solve multi-digit addition by generating stepwise solutions that mirror human algorithms (e.g., right-to-left addition with carry).",
                        "uuids": []
                    },
                    {
                        "text": "Ablation studies show that disrupting token-level representations impairs multi-step arithmetic more than single-step tasks.",
                        "uuids": []
                    },
                    {
                        "text": "LLMs' chain-of-thought outputs for arithmetic often reflect recursive breakdown of problems into subproblems.",
                        "uuids": []
                    },
                    {
                        "text": "Performance on arithmetic tasks degrades with increased step complexity, consistent with recursive composition limits.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Recursive composition is a known property of algorithmic reasoning in neural models.",
                    "what_is_novel": "This law posits that LLMs instantiate recursive subroutines for arithmetic, not just for language or logic.",
                    "classification_explanation": "The law is a domain-specific extension of recursive composition to arithmetic in LLMs.",
                    "likely_classification": "closely-related-to-existing",
                    "references": [
                        "Graves et al. (2014) Neural Turing Machines [Neural models learning algorithmic subroutines]",
                        "Saxton et al. (2019) Analysing Mathematical Reasoning Abilities of Neural Models [Arithmetic reasoning in neural models]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLMs will show greater accuracy on arithmetic problems that can be decomposed into familiar subroutines (e.g., addition, carry) than on problems requiring novel compositions.",
        "Interventions that disrupt hierarchical representations (e.g., shuffling digit order) will impair arithmetic performance.",
        "LLMs will generalize better to longer arithmetic problems if the subroutine structure is preserved."
    ],
    "new_predictions_unknown": [
        "LLMs may be able to learn new arithmetic subroutines (e.g., base conversion) via in-context learning if provided with sufficient examples.",
        "Hierarchical representations may enable LLMs to perform multi-step symbolic manipulations beyond arithmetic (e.g., algebraic simplification) with minimal fine-tuning."
    ],
    "negative_experiments": [
        "If LLMs cannot solve multi-digit arithmetic problems even when single-digit subroutines are intact, the theory's claim about recursive composition is weakened.",
        "If LLMs do not encode hierarchical representations of numbers, as revealed by probing or ablation, the theory is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some arithmetic errors in LLMs are due to tokenization artifacts rather than representational failures.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "LLMs sometimes make errors on simple arithmetic that do not align with failures of compositionality or recursion.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Arithmetic with non-standard symbols or encodings may not trigger hierarchical representations.",
        "Very long arithmetic problems may exceed the model's recursion depth or context window."
    ],
    "existing_theory": {
        "what_already_exists": "Compositional and recursive representations are established in neural computation, but not specifically for LLM arithmetic.",
        "what_is_novel": "The theory applies these principles to explain LLM arithmetic reasoning and predicts specific failure modes.",
        "classification_explanation": "The theory is a domain-specific extension of known neural computation principles.",
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Lake & Baroni (2018) Generalization without Systematicity [Compositionality in neural models]",
            "Graves et al. (2014) Neural Turing Machines [Algorithmic subroutines in neural models]",
            "Saxton et al. (2019) Analysing Mathematical Reasoning Abilities of Neural Models [Arithmetic in neural models]"
        ]
    },
    "reflected_from_theory_index": 1,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-with-matched-control-theory-name",
    "theory_query": "Build a theory of how language models perform arithmetic.",
    "original_theory_id": "theory-580",
    "original_theory_name": "Emergent Algorithmic Reasoning via Chain-of-Thought and Program Synthesis in Large Language Models",
    "provide_matched_control_thery_name": true,
    "matched_control_theory_name": "Emergent Algorithmic Reasoning via Chain-of-Thought and Program Synthesis in Large Language Models",
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
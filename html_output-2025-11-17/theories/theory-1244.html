<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compositionality and Locality Theory for Graph-to-Text - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1244</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1244</p>
                <p><strong>Name:</strong> Compositionality and Locality Theory for Graph-to-Text</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of ideal representations for converting graphs into text for language model training.</p>
                <p><strong>Description:</strong> This theory asserts that ideal graph-to-text representations for language model training should maximize compositionality (the ability to represent subgraphs as reusable, modular text units) and locality (the preservation of local graph neighborhoods in contiguous text spans). Such representations enable language models to learn and generalize over both local and global graph patterns, supporting transfer to unseen or larger graphs.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Compositional Encoding Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; graph_representation &#8594; encodes &#8594; subgraphs_as_modular_text_units</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; generalizes &#8594; to_unseen_graph_compositions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Motif-based and modular representations in molecular and knowledge graph tasks improve generalization to novel structures. </li>
    <li>Compositionality is a key factor in the success of neural program synthesis and AMR parsing. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Compositionality is a known principle, but its formalization for graph-to-text LMs is new.</p>            <p><strong>What Already Exists:</strong> Compositionality is discussed in AMR and program synthesis, but not as a law for graph-to-text LMs.</p>            <p><strong>What is Novel:</strong> The law's explicit focus on modular text units for subgraphs in graph-to-text LM training is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [Motif-based compositionality]</li>
    <li>Dong & Lapata (2016) Language to Logical Form with Neural Attention [Compositionality in semantic parsing]</li>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [Compositionality in neural models]</li>
</ul>
            <h3>Statement 1: Locality Preservation Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; text_representation &#8594; preserves &#8594; local_graph_neighborhoods_in_contiguous_spans</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; language_model &#8594; learns &#8594; local_graph_patterns_and_algorithms</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Locality-preserving encodings in GNNs and graph-to-sequence models improve learning of local graph features. </li>
    <li>Contiguous encoding of neighborhoods aids in learning local dependencies in NLP and code generation. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> Locality is a known desideratum, but its formalization for graph-to-text LMs is new.</p>            <p><strong>What Already Exists:</strong> Locality is valued in GNNs and some sequence models, but not formalized for graph-to-text LMs.</p>            <p><strong>What is Novel:</strong> The law's focus on contiguous text spans for local neighborhoods in graph-to-text LM training is novel.</p>
            <p><strong>References:</strong> <ul>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Locality in GNNs]</li>
    <li>Yin & Neubig (2017) A Syntactic Neural Model for General-Purpose Code Generation [Locality in code/graph representations]</li>
    <li>Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [Locality in AMR linearization]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Language models trained on compositional, locality-preserving graph-to-text representations will generalize better to larger or more complex graphs.</li>
                <li>Contiguous encoding of local neighborhoods will improve LM performance on tasks requiring local graph reasoning.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>It is unknown whether compositionality and locality can be simultaneously maximized for all graph types, especially for highly irregular or dense graphs.</li>
                <li>The optimal size and granularity of compositional units for maximal generalization is not yet established.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If models trained on non-compositional or non-local representations outperform those with compositional/locality-preserving encodings, the theory would be challenged.</li>
                <li>If contiguous encoding of neighborhoods does not improve local pattern learning, the locality preservation law would be called into question.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not address graphs with no clear modular or local structure, such as random or fully connected graphs. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory synthesizes known principles into a formal framework for graph-to-text LM training.</p>
            <p><strong>References:</strong> <ul>
    <li>Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [Motif-based compositionality]</li>
    <li>Xu et al. (2019) How Powerful are Graph Neural Networks? [Locality in GNNs]</li>
    <li>Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [Compositionality in neural models]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Compositionality and Locality Theory for Graph-to-Text",
    "theory_description": "This theory asserts that ideal graph-to-text representations for language model training should maximize compositionality (the ability to represent subgraphs as reusable, modular text units) and locality (the preservation of local graph neighborhoods in contiguous text spans). Such representations enable language models to learn and generalize over both local and global graph patterns, supporting transfer to unseen or larger graphs.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Compositional Encoding Law",
                "if": [
                    {
                        "subject": "graph_representation",
                        "relation": "encodes",
                        "object": "subgraphs_as_modular_text_units"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "generalizes",
                        "object": "to_unseen_graph_compositions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Motif-based and modular representations in molecular and knowledge graph tasks improve generalization to novel structures.",
                        "uuids": []
                    },
                    {
                        "text": "Compositionality is a key factor in the success of neural program synthesis and AMR parsing.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Compositionality is discussed in AMR and program synthesis, but not as a law for graph-to-text LMs.",
                    "what_is_novel": "The law's explicit focus on modular text units for subgraphs in graph-to-text LM training is novel.",
                    "classification_explanation": "Compositionality is a known principle, but its formalization for graph-to-text LMs is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [Motif-based compositionality]",
                        "Dong & Lapata (2016) Language to Logical Form with Neural Attention [Compositionality in semantic parsing]",
                        "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [Compositionality in neural models]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Locality Preservation Law",
                "if": [
                    {
                        "subject": "text_representation",
                        "relation": "preserves",
                        "object": "local_graph_neighborhoods_in_contiguous_spans"
                    }
                ],
                "then": [
                    {
                        "subject": "language_model",
                        "relation": "learns",
                        "object": "local_graph_patterns_and_algorithms"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Locality-preserving encodings in GNNs and graph-to-sequence models improve learning of local graph features.",
                        "uuids": []
                    },
                    {
                        "text": "Contiguous encoding of neighborhoods aids in learning local dependencies in NLP and code generation.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Locality is valued in GNNs and some sequence models, but not formalized for graph-to-text LMs.",
                    "what_is_novel": "The law's focus on contiguous text spans for local neighborhoods in graph-to-text LM training is novel.",
                    "classification_explanation": "Locality is a known desideratum, but its formalization for graph-to-text LMs is new.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Xu et al. (2019) How Powerful are Graph Neural Networks? [Locality in GNNs]",
                        "Yin & Neubig (2017) A Syntactic Neural Model for General-Purpose Code Generation [Locality in code/graph representations]",
                        "Konstas et al. (2017) Neural AMR: Sequence-to-Sequence Models for Parsing and Generation [Locality in AMR linearization]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "Language models trained on compositional, locality-preserving graph-to-text representations will generalize better to larger or more complex graphs.",
        "Contiguous encoding of local neighborhoods will improve LM performance on tasks requiring local graph reasoning."
    ],
    "new_predictions_unknown": [
        "It is unknown whether compositionality and locality can be simultaneously maximized for all graph types, especially for highly irregular or dense graphs.",
        "The optimal size and granularity of compositional units for maximal generalization is not yet established."
    ],
    "negative_experiments": [
        "If models trained on non-compositional or non-local representations outperform those with compositional/locality-preserving encodings, the theory would be challenged.",
        "If contiguous encoding of neighborhoods does not improve local pattern learning, the locality preservation law would be called into question."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not address graphs with no clear modular or local structure, such as random or fully connected graphs.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies suggest that for certain tasks, global representations or attention mechanisms can compensate for lack of locality in the encoding.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Graphs with high symmetry or no clear modularity may not benefit from compositional encoding.",
        "For graphs with long-range dependencies, locality-preserving encodings may need to be augmented with global context."
    ],
    "existing_theory": {
        "what_already_exists": "Compositionality and locality are valued in GNNs, AMR, and program synthesis, but not formalized for graph-to-text LM training.",
        "what_is_novel": "The explicit formalization of compositionality and locality as laws for graph-to-text LM representations is novel.",
        "classification_explanation": "The theory synthesizes known principles into a formal framework for graph-to-text LM training.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Jin et al. (2018) Junction Tree Variational Autoencoder for Molecular Graph Generation [Motif-based compositionality]",
            "Xu et al. (2019) How Powerful are Graph Neural Networks? [Locality in GNNs]",
            "Lake & Baroni (2018) Generalization without Systematicity: Compositional Skills in Seq2Seq Networks [Compositionality in neural models]"
        ]
    },
    "reflected_from_theory_index": 2,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of ideal representations for converting graphs into text for language model training.",
    "original_theory_id": "theory-611",
    "original_theory_name": "Multimodal Alignment and Compactness Principle for Graph-to-Text Representations",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Pathway Arithmetic Processing Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-54</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-54</p>
                <p><strong>Name:</strong> Dual-Pathway Arithmetic Processing Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic, based on the following results.</p>
                <p><strong>Description:</strong> Language models process arithmetic through two complementary pathways: (1) an attention-based information routing pathway that gathers and positions operand information at key token positions, and (2) an MLP-based computation pathway that performs numerical transformations and injects result-related information into the residual stream. The attention pathway operates primarily in early-to-mid layers to collect operands and operators, while the MLP pathway operates in mid-to-late layers (especially at the final token position) to compute and progressively refine answer representations. In pretrained models, MLPs may use Fourier features from token embeddings as computational primitives, with low-frequency components for magnitude approximation and high-frequency components for modular arithmetic (e.g., unit digits). This division of labor emerges during training and is observable through causal intervention studies. The specific implementation details (layer indices, Fourier usage, attention patterns) vary with model architecture, training procedure, tokenization, and the arithmetic operation being performed.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Arithmetic computation in transformers involves a two-stage process: attention-based routing followed by MLP-based computation.</li>
                <li>Attention heads in early-to-mid layers (approximately layers 1-18 in a ~24-layer model) gather operand and operator information and route it to specific token positions (typically the final token).</li>
                <li>MLP blocks in mid-to-late layers (approximately layers 15-22 in a ~24-layer model) at the final token position perform the primary numerical computation and inject result-related information.</li>
                <li>In pretrained models, MLPs use Fourier features as computational primitives: low-frequency components for magnitude approximation and high-frequency components for modular arithmetic.</li>
                <li>The separation of routing (attention) and computation (MLP) functions is a learned specialization that emerges during training on arithmetic tasks.</li>
                <li>This dual-pathway architecture is conserved across different model families (GPT-J, LLaMA, Mistral, Pythia) though specific layer indices and implementation details vary.</li>
                <li>The computational pathway shows result-specificity: its contribution drops dramatically when the correct result is held constant across different operand combinations.</li>
                <li>Early-layer MLPs encode operands/operators into the residual stream, mid-layer attention routes this information, and late-layer MLPs perform the final computation.</li>
                <li>Pretrained token embeddings provide Fourier features that enable more accurate arithmetic than randomly initialized embeddings.</li>
                <li>The attention pathway implements high-frequency (modular) operations while the MLP pathway implements low-frequency (magnitude) operations in pretrained models.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>GPT-J shows attention blocks transporting operand/operator information from early tokens to the final token, with peak indirect effects in layers 11-18, while late MLPs at the last token (layers 19-20) inject result-related signals. <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> </li>
    <li>LLaMA-2 models show arithmetic computation concentrated in upper layers rather than distributed throughout the network, with pure computational ability nearly absent in lower layers and emerging sharply in middle-to-upper layers. <a href="../results/extraction-result-244.html#e244.0" class="evidence-link">[e244.0]</a> <a href="../results/extraction-result-244.html#e244.1" class="evidence-link">[e244.1]</a> <a href="../results/extraction-result-244.html#e244.2" class="evidence-link">[e244.2]</a> </li>
    <li>Path patching analysis in LLaMA-2 and Mistral-7B reveals a small set of attention heads attending to numbers/operators, followed by MLPs that accumulate and refine answer representations across layers, with inflection points marking key computational stages. <a href="../results/extraction-result-248.html#e248.0" class="evidence-link">[e248.0]</a> <a href="../results/extraction-result-248.html#e248.2" class="evidence-link">[e248.2]</a> </li>
    <li>Pythia models show attention mechanisms moving operand information to final token positions, followed by late MLP processing that peaks in mid-late layers, with early-layer MLPs encoding operand/operator tokens. <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> <a href="../results/extraction-result-316.html#e316.1" class="evidence-link">[e316.1]</a> </li>
    <li>Fine-tuning on three-operand arithmetic induces the mid-late last-token MLP activation site in Pythia-2.8B, supporting that this pattern is learnable and emerges from training. <a href="../results/extraction-result-316.html#e316.1" class="evidence-link">[e316.1]</a> </li>
    <li>Relative Importance metric shows last-token mid-late MLPs account for ~40% of indirect effect in GPT-J, dropping to ~4-5% when result is fixed, demonstrating result-specificity of the computational pathway. <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> </li>
    <li>Pretrained GPT-2-XL uses Fourier features in token embeddings and intermediate logits for addition: MLPs implement low-frequency approximation (magnitude) while attention implements high-frequency modular classification (unit digits). <a href="../results/extraction-result-315.html#e315.0" class="evidence-link">[e315.0]</a> </li>
    <li>Causal ablations removing MLP low-frequency components produce magnitude errors (off-by-10/50/100), while removing attention high-frequency components produces small unit-digit errors, confirming functional separation. <a href="../results/extraction-result-315.html#e315.0" class="evidence-link">[e315.0]</a> </li>
    <li>Models trained from scratch lack the sparse high-frequency Fourier features and achieve lower accuracy with characteristic off-by-one errors, showing pretrained embeddings provide computational primitives. <a href="../results/extraction-result-315.html#e315.1" class="evidence-link">[e315.1]</a> <a href="../results/extraction-result-315.html#e315.3" class="evidence-link">[e315.3]</a> </li>
    <li>The dual-pathway pattern generalizes across model families (GPT-J, LLaMA, Mistral, Pythia) though specific layer indices vary, suggesting it is a general architectural solution. <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> <a href="../results/extraction-result-248.html#e248.2" class="evidence-link">[e248.2]</a> <a href="../results/extraction-result-316.html#e316.1" class="evidence-link">[e316.1]</a> </li>
    <li>Attention patterns show specific heads attending to operand positions and routing information to computation sites, with different heads specializing in different aspects of the routing task. <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> <a href="../results/extraction-result-248.html#e248.0" class="evidence-link">[e248.0]</a> </li>
    <li>Early/mid-sequence MLPs encode operands and operators into the residual stream, which attention then routes to final positions for late-layer MLP processing. <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Ablating attention heads in early-to-mid layers should disrupt operand gathering and cause random-like errors, while ablating late MLPs should cause systematic errors in result computation.</li>
                <li>Training with frozen attention weights but trainable MLPs should still allow learning arithmetic on fixed-format problems where routing patterns are consistent.</li>
                <li>Models trained on arithmetic should show higher activation magnitudes in late-layer MLPs at the final token position compared to non-arithmetic tasks.</li>
                <li>Intervening on MLP activations in mid-late layers should allow direct manipulation of predicted results more effectively than intervening on attention patterns.</li>
                <li>Injecting pretrained token embeddings with Fourier features into a randomly initialized model should improve arithmetic performance substantially.</li>
                <li>Filtering out high-frequency Fourier components from attention outputs should specifically impair unit-digit prediction while preserving magnitude estimation.</li>
                <li>Models should show distinct attention patterns for different arithmetic operations (addition vs multiplication), with multiplication requiring more complex routing.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether this dual-pathway architecture generalizes to other mathematical operations beyond basic arithmetic (e.g., calculus, linear algebra) is unclear, though some evidence suggests matrix operations may use similar patterns.</li>
                <li>It's unknown whether explicitly training models to separate routing and computation (e.g., through architectural constraints or auxiliary losses) would improve arithmetic performance or simply constrain learning.</li>
                <li>Whether the same pathway structure applies equally to arithmetic performed in natural language contexts (word problems) versus symbolic arithmetic is uncertain, as word problems may require additional semantic processing.</li>
                <li>The extent to which this architecture could be made more efficient through pruning or distillation while preserving arithmetic capability is unknown, though the concentration in specific layers suggests potential for compression.</li>
                <li>Whether models could learn to use different pathways for different operations (e.g., one pathway for addition, another for multiplication) or whether they must share pathways is unclear.</li>
                <li>It's unknown whether the Fourier-feature mechanism discovered in pretrained models is the only way to implement the computational pathway, or if other representations could work equally well.</li>
                <li>Whether the dual-pathway structure is optimal or merely a local minimum found during training is uncertain.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding that ablating early attention heads has no effect on arithmetic accuracy would contradict the routing role.</li>
                <li>Discovering that late MLPs contribute equally to all tasks (not specifically to arithmetic) would challenge the computation specialization claim.</li>
                <li>Observing that arithmetic computation is distributed uniformly across all layers would contradict the late-layer concentration claim.</li>
                <li>Finding that the pathway structure differs fundamentally between small and large models would challenge the universality claim.</li>
                <li>Discovering that models trained from scratch develop completely different mechanisms that work equally well would challenge the necessity of the dual-pathway structure.</li>
                <li>Finding that removing Fourier features from pretrained embeddings has no effect on arithmetic would challenge the Fourier-feature computational primitive claim.</li>
                <li>Observing that attention patterns do not consistently route to final token positions would contradict the routing mechanism claim.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The exact mechanism by which MLPs perform numerical transformations beyond Fourier decomposition (e.g., how they combine features, handle nonlinearities) is not fully specified. <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> <a href="../results/extraction-result-248.html#e248.2" class="evidence-link">[e248.2]</a> </li>
    <li>How the model handles carry propagation and borrowing within the MLP pathway is not mechanistically detailed, though some evidence suggests carries may be reconstructed via attention in certain formats. <a href="../results/extraction-result-244.html#e244.0" class="evidence-link">[e244.0]</a> <a href="../results/extraction-result-266.html#e266.0" class="evidence-link">[e266.0]</a> </li>
    <li>The role of residual connections in maintaining and updating numerical information across layers is not explicitly addressed, though they presumably allow information flow between pathways. <a href="../results/extraction-result-316.html#e316.0" class="evidence-link">[e316.0]</a> </li>
    <li>How different tokenization schemes (digit-level vs multi-digit tokens) affect the pathway structure and computational mechanisms is not fully explained. <a href="../results/extraction-result-278.html#e278.5" class="evidence-link">[e278.5]</a> <a href="../results/extraction-result-300.html#e300.0" class="evidence-link">[e300.0]</a> </li>
    <li>The interaction between the dual pathways and different output formats (direct answer vs scratchpad vs reversed digits) is not mechanistically detailed. <a href="../results/extraction-result-266.html#e266.0" class="evidence-link">[e266.0]</a> <a href="../results/extraction-result-263.html#e263.0" class="evidence-link">[e263.0]</a> </li>
    <li>How the pathways handle different number representations (integers vs floats vs fractions) is not fully specified. <a href="../results/extraction-result-244.html#e244.0" class="evidence-link">[e244.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Olsson et al. (2022) In-context learning and induction heads [Related work on attention-head specialization for pattern completion, but not specific to arithmetic dual-pathway architecture]</li>
    <li>Elhage et al. (2021) A Mathematical Framework for Transformer Circuits [General framework for understanding transformer circuits, but does not specifically describe arithmetic dual-pathway processing]</li>
    <li>Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Describes MLPs as key-value memories but does not specifically address arithmetic computation or the dual-pathway structure]</li>
    <li>Meng et al. (2022) Locating and Editing Factual Associations in GPT [Describes localized knowledge storage in MLPs but focuses on factual knowledge rather than arithmetic computation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Dual-Pathway Arithmetic Processing Theory",
    "theory_description": "Language models process arithmetic through two complementary pathways: (1) an attention-based information routing pathway that gathers and positions operand information at key token positions, and (2) an MLP-based computation pathway that performs numerical transformations and injects result-related information into the residual stream. The attention pathway operates primarily in early-to-mid layers to collect operands and operators, while the MLP pathway operates in mid-to-late layers (especially at the final token position) to compute and progressively refine answer representations. In pretrained models, MLPs may use Fourier features from token embeddings as computational primitives, with low-frequency components for magnitude approximation and high-frequency components for modular arithmetic (e.g., unit digits). This division of labor emerges during training and is observable through causal intervention studies. The specific implementation details (layer indices, Fourier usage, attention patterns) vary with model architecture, training procedure, tokenization, and the arithmetic operation being performed.",
    "supporting_evidence": [
        {
            "text": "GPT-J shows attention blocks transporting operand/operator information from early tokens to the final token, with peak indirect effects in layers 11-18, while late MLPs at the last token (layers 19-20) inject result-related signals.",
            "uuids": [
                "e316.0"
            ]
        },
        {
            "text": "LLaMA-2 models show arithmetic computation concentrated in upper layers rather than distributed throughout the network, with pure computational ability nearly absent in lower layers and emerging sharply in middle-to-upper layers.",
            "uuids": [
                "e244.0",
                "e244.1",
                "e244.2"
            ]
        },
        {
            "text": "Path patching analysis in LLaMA-2 and Mistral-7B reveals a small set of attention heads attending to numbers/operators, followed by MLPs that accumulate and refine answer representations across layers, with inflection points marking key computational stages.",
            "uuids": [
                "e248.0",
                "e248.2"
            ]
        },
        {
            "text": "Pythia models show attention mechanisms moving operand information to final token positions, followed by late MLP processing that peaks in mid-late layers, with early-layer MLPs encoding operand/operator tokens.",
            "uuids": [
                "e316.0",
                "e316.1"
            ]
        },
        {
            "text": "Fine-tuning on three-operand arithmetic induces the mid-late last-token MLP activation site in Pythia-2.8B, supporting that this pattern is learnable and emerges from training.",
            "uuids": [
                "e316.1"
            ]
        },
        {
            "text": "Relative Importance metric shows last-token mid-late MLPs account for ~40% of indirect effect in GPT-J, dropping to ~4-5% when result is fixed, demonstrating result-specificity of the computational pathway.",
            "uuids": [
                "e316.0"
            ]
        },
        {
            "text": "Pretrained GPT-2-XL uses Fourier features in token embeddings and intermediate logits for addition: MLPs implement low-frequency approximation (magnitude) while attention implements high-frequency modular classification (unit digits).",
            "uuids": [
                "e315.0"
            ]
        },
        {
            "text": "Causal ablations removing MLP low-frequency components produce magnitude errors (off-by-10/50/100), while removing attention high-frequency components produces small unit-digit errors, confirming functional separation.",
            "uuids": [
                "e315.0"
            ]
        },
        {
            "text": "Models trained from scratch lack the sparse high-frequency Fourier features and achieve lower accuracy with characteristic off-by-one errors, showing pretrained embeddings provide computational primitives.",
            "uuids": [
                "e315.1",
                "e315.3"
            ]
        },
        {
            "text": "The dual-pathway pattern generalizes across model families (GPT-J, LLaMA, Mistral, Pythia) though specific layer indices vary, suggesting it is a general architectural solution.",
            "uuids": [
                "e316.0",
                "e248.2",
                "e316.1"
            ]
        },
        {
            "text": "Attention patterns show specific heads attending to operand positions and routing information to computation sites, with different heads specializing in different aspects of the routing task.",
            "uuids": [
                "e316.0",
                "e248.0"
            ]
        },
        {
            "text": "Early/mid-sequence MLPs encode operands and operators into the residual stream, which attention then routes to final positions for late-layer MLP processing.",
            "uuids": [
                "e316.0"
            ]
        }
    ],
    "theory_statements": [
        "Arithmetic computation in transformers involves a two-stage process: attention-based routing followed by MLP-based computation.",
        "Attention heads in early-to-mid layers (approximately layers 1-18 in a ~24-layer model) gather operand and operator information and route it to specific token positions (typically the final token).",
        "MLP blocks in mid-to-late layers (approximately layers 15-22 in a ~24-layer model) at the final token position perform the primary numerical computation and inject result-related information.",
        "In pretrained models, MLPs use Fourier features as computational primitives: low-frequency components for magnitude approximation and high-frequency components for modular arithmetic.",
        "The separation of routing (attention) and computation (MLP) functions is a learned specialization that emerges during training on arithmetic tasks.",
        "This dual-pathway architecture is conserved across different model families (GPT-J, LLaMA, Mistral, Pythia) though specific layer indices and implementation details vary.",
        "The computational pathway shows result-specificity: its contribution drops dramatically when the correct result is held constant across different operand combinations.",
        "Early-layer MLPs encode operands/operators into the residual stream, mid-layer attention routes this information, and late-layer MLPs perform the final computation.",
        "Pretrained token embeddings provide Fourier features that enable more accurate arithmetic than randomly initialized embeddings.",
        "The attention pathway implements high-frequency (modular) operations while the MLP pathway implements low-frequency (magnitude) operations in pretrained models."
    ],
    "new_predictions_likely": [
        "Ablating attention heads in early-to-mid layers should disrupt operand gathering and cause random-like errors, while ablating late MLPs should cause systematic errors in result computation.",
        "Training with frozen attention weights but trainable MLPs should still allow learning arithmetic on fixed-format problems where routing patterns are consistent.",
        "Models trained on arithmetic should show higher activation magnitudes in late-layer MLPs at the final token position compared to non-arithmetic tasks.",
        "Intervening on MLP activations in mid-late layers should allow direct manipulation of predicted results more effectively than intervening on attention patterns.",
        "Injecting pretrained token embeddings with Fourier features into a randomly initialized model should improve arithmetic performance substantially.",
        "Filtering out high-frequency Fourier components from attention outputs should specifically impair unit-digit prediction while preserving magnitude estimation.",
        "Models should show distinct attention patterns for different arithmetic operations (addition vs multiplication), with multiplication requiring more complex routing."
    ],
    "new_predictions_unknown": [
        "Whether this dual-pathway architecture generalizes to other mathematical operations beyond basic arithmetic (e.g., calculus, linear algebra) is unclear, though some evidence suggests matrix operations may use similar patterns.",
        "It's unknown whether explicitly training models to separate routing and computation (e.g., through architectural constraints or auxiliary losses) would improve arithmetic performance or simply constrain learning.",
        "Whether the same pathway structure applies equally to arithmetic performed in natural language contexts (word problems) versus symbolic arithmetic is uncertain, as word problems may require additional semantic processing.",
        "The extent to which this architecture could be made more efficient through pruning or distillation while preserving arithmetic capability is unknown, though the concentration in specific layers suggests potential for compression.",
        "Whether models could learn to use different pathways for different operations (e.g., one pathway for addition, another for multiplication) or whether they must share pathways is unclear.",
        "It's unknown whether the Fourier-feature mechanism discovered in pretrained models is the only way to implement the computational pathway, or if other representations could work equally well.",
        "Whether the dual-pathway structure is optimal or merely a local minimum found during training is uncertain."
    ],
    "negative_experiments": [
        "Finding that ablating early attention heads has no effect on arithmetic accuracy would contradict the routing role.",
        "Discovering that late MLPs contribute equally to all tasks (not specifically to arithmetic) would challenge the computation specialization claim.",
        "Observing that arithmetic computation is distributed uniformly across all layers would contradict the late-layer concentration claim.",
        "Finding that the pathway structure differs fundamentally between small and large models would challenge the universality claim.",
        "Discovering that models trained from scratch develop completely different mechanisms that work equally well would challenge the necessity of the dual-pathway structure.",
        "Finding that removing Fourier features from pretrained embeddings has no effect on arithmetic would challenge the Fourier-feature computational primitive claim.",
        "Observing that attention patterns do not consistently route to final token positions would contradict the routing mechanism claim."
    ],
    "unaccounted_for": [
        {
            "text": "The exact mechanism by which MLPs perform numerical transformations beyond Fourier decomposition (e.g., how they combine features, handle nonlinearities) is not fully specified.",
            "uuids": [
                "e316.0",
                "e248.2"
            ]
        },
        {
            "text": "How the model handles carry propagation and borrowing within the MLP pathway is not mechanistically detailed, though some evidence suggests carries may be reconstructed via attention in certain formats.",
            "uuids": [
                "e244.0",
                "e266.0"
            ]
        },
        {
            "text": "The role of residual connections in maintaining and updating numerical information across layers is not explicitly addressed, though they presumably allow information flow between pathways.",
            "uuids": [
                "e316.0"
            ]
        },
        {
            "text": "How different tokenization schemes (digit-level vs multi-digit tokens) affect the pathway structure and computational mechanisms is not fully explained.",
            "uuids": [
                "e278.5",
                "e300.0"
            ]
        },
        {
            "text": "The interaction between the dual pathways and different output formats (direct answer vs scratchpad vs reversed digits) is not mechanistically detailed.",
            "uuids": [
                "e266.0",
                "e263.0"
            ]
        },
        {
            "text": "How the pathways handle different number representations (integers vs floats vs fractions) is not fully specified.",
            "uuids": [
                "e244.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some evidence suggests arithmetic can be learned by very small models (536K parameters) with appropriate training, which may not have sufficient capacity for elaborate dual-pathway specialization with many specialized heads and layers.",
            "uuids": [
                "e292.2"
            ]
        },
        {
            "text": "Compiled transformers (CoNN) implement arithmetic through explicit attention-based select-aggregate operations without requiring specialized MLP computation pathways, suggesting alternative architectural solutions exist.",
            "uuids": [
                "e275.1",
                "e275.0"
            ]
        },
        {
            "text": "Evidence that tokenization direction (left-to-right vs right-to-left) dramatically affects arithmetic performance suggests the pathways may be highly dependent on surface-level formatting rather than implementing abstract computation.",
            "uuids": [
                "e300.0"
            ]
        },
        {
            "text": "Small encoder-decoder models show evidence of an Encoding-Regression-Decoding mechanism that differs from the dual-pathway structure, with computation happening in a learned value space rather than through Fourier features.",
            "uuids": [
                "e249.0"
            ]
        },
        {
            "text": "Evidence that models can learn algebraic properties (commutativity, identity) through attention-based invariances suggests attention may play a more direct computational role than just routing.",
            "uuids": [
                "e253.0",
                "e253.1"
            ]
        }
    ],
    "special_cases": [
        "The specific layer indices where attention routing and MLP computation peak vary across model architectures and sizes (e.g., GPT-J peaks at layers 19-20, while other models may differ).",
        "Very small models may not show clear pathway separation due to limited capacity and may use more integrated mechanisms.",
        "Models trained from scratch on arithmetic may develop different pathway structures than pretrained models fine-tuned on arithmetic, particularly lacking Fourier features.",
        "The pathway structure may differ for different arithmetic operations: addition may use simpler routing than multiplication, which requires more complex multi-step computation.",
        "Pretrained models use Fourier features from embeddings, while models trained from scratch must learn alternative computational primitives.",
        "Different tokenization schemes (digit-level vs multi-digit) may require different routing patterns and affect which layers perform computation.",
        "Different output formats (direct answer vs scratchpad vs reversed digits) may engage the pathways differently, with reversed formats potentially simplifying the computational requirements.",
        "The pathways may be less distinct or differently organized for operations on different number types (integers vs floats vs fractions).",
        "Models trained with specific architectural constraints (e.g., compiled transformers) may implement arithmetic through entirely different mechanisms that don't follow the dual-pathway structure."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Olsson et al. (2022) In-context learning and induction heads [Related work on attention-head specialization for pattern completion, but not specific to arithmetic dual-pathway architecture]",
            "Elhage et al. (2021) A Mathematical Framework for Transformer Circuits [General framework for understanding transformer circuits, but does not specifically describe arithmetic dual-pathway processing]",
            "Geva et al. (2021) Transformer Feed-Forward Layers Are Key-Value Memories [Describes MLPs as key-value memories but does not specifically address arithmetic computation or the dual-pathway structure]",
            "Meng et al. (2022) Locating and Editing Factual Associations in GPT [Describes localized knowledge storage in MLPs but focuses on factual knowledge rather than arithmetic computation]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-1009</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-1009</p>
                <p><strong>Name:</strong> Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLM agents for text games can best use memory to solve text game tasks.</p>
                <p><strong>Description:</strong> This theory posits that LLM agents achieve optimal performance in text games by dynamically constructing and utilizing a hierarchical memory system, where episodic memory encodes temporally-ordered, context-rich experiences, and semantic memory abstracts generalizable knowledge and rules. The interplay between these memory types enables both flexible adaptation to novel situations and efficient retrieval of relevant past experiences, supporting planning, inference, and task completion.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Hierarchical Memory Construction (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; engages_in &#8594; text game session</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; constructs &#8594; episodic memory (ordered sequence of events, actions, and observations)<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; constructs &#8594; semantic memory (abstracted rules, facts, and regularities from episodes)</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human cognition and animal learning show separation between episodic and semantic memory, with both contributing to flexible problem solving. </li>
    <li>LLM agents with memory modules outperform those without in multi-step, context-dependent text games. </li>
    <li>Memory-augmented neural networks (e.g., Memory Networks, Differentiable Neural Computers) demonstrate improved performance on tasks requiring long-term dependencies. </li>
    <li>Text games often require remembering both specific past events (e.g., which doors have been opened) and general rules (e.g., keys open doors). </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While the episodic/semantic distinction is known in cognitive science and some AI, its formalization and necessity for LLM agents in text games is novel.</p>            <p><strong>What Already Exists:</strong> Hierarchical memory systems (episodic/semantic) are well-established in cognitive neuroscience and have inspired some AI architectures.</p>            <p><strong>What is Novel:</strong> Explicitly positing that LLM agents for text games must dynamically construct and leverage both episodic and semantic memory, and that their interplay is key to optimal performance in text-based environments.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [origin of episodic/semantic distinction]</li>
    <li>Weston et al. (2015) Memory Networks [memory-augmented neural networks]</li>
    <li>Madotto et al. (2020) Exploration based language learning for text-based games [memory in LLM agents for text games]</li>
</ul>
            <h3>Statement 1: Interplay of Episodic and Semantic Memory for Task Solving (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; faces &#8594; novel or ambiguous text game situation<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; has &#8594; hierarchical memory (episodic and semantic)</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; LLM agent &#8594; retrieves &#8594; relevant episodic traces<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; applies &#8594; semantic abstractions to interpret or generalize<span style="color: #888888;">, and</span></div>
        <div>&#8226; LLM agent &#8594; combines &#8594; episodic and semantic information to select actions</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Human problem solving in games and navigation relies on both recalling specific past events and applying general knowledge. </li>
    <li>LLM agents with both memory types can generalize better and recover from unexpected states in text games. </li>
    <li>ReAct and similar frameworks show that combining reasoning (semantic) and acting (episodic) improves agent performance. </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The general idea is related to existing work, but the specific application and mechanism for LLM agents in text games is novel.</p>            <p><strong>What Already Exists:</strong> The benefit of combining episodic and semantic memory is established in cognitive science.</p>            <p><strong>What is Novel:</strong> The explicit mechanism of dynamic interplay for LLM agents in text games, and the claim that this is necessary for optimal task solving.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1985) Memory and consciousness [episodic/semantic interplay]</li>
    <li>Khandelwal et al. (2019) Generalization through memorization: Nearest neighbor language models [memory in LLMs]</li>
    <li>Yao et al. (2023) ReAct: Synergizing reasoning and acting in language models [reasoning with memory traces in LLM agents]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>LLM agents with explicit, separate episodic and semantic memory modules will outperform agents with only one type of memory in multi-step, context-dependent text games.</li>
                <li>When faced with a novel puzzle in a text game, an LLM agent will first retrieve similar past episodes, then apply abstracted rules to infer a solution.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If an LLM agent's episodic and semantic memory modules are allowed to interact via a learned gating mechanism, the agent may develop emergent meta-cognitive strategies (e.g., self-reflection, hypothesis testing) not present in either module alone.</li>
                <li>Hierarchical memory systems may enable LLM agents to transfer knowledge across vastly different text game genres (e.g., from fantasy to science fiction) by abstracting high-level semantic rules.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If an LLM agent with only episodic memory (no semantic abstraction) performs as well as one with both, the necessity of semantic memory is called into question.</li>
                <li>If agents with flat (non-hierarchical) memory structures outperform hierarchical ones in complex text games, the theory is challenged.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Some text games may be solvable with only short-term memory or pattern matching, not requiring hierarchical memory. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> somewhat-related-to-existing</p>
            <p><strong>Explanation:</strong> The theory builds on known memory distinctions but applies them in a novel, formalized way to LLM agents in text games.</p>
            <p><strong>References:</strong> <ul>
    <li>Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]</li>
    <li>Weston et al. (2015) Memory Networks [memory-augmented neural networks]</li>
    <li>Madotto et al. (2020) Exploration based language learning for text-based games [memory in LLM agents for text games]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Episodic-Semantic Memory Theory for LLM Agents in Text Games",
    "theory_description": "This theory posits that LLM agents achieve optimal performance in text games by dynamically constructing and utilizing a hierarchical memory system, where episodic memory encodes temporally-ordered, context-rich experiences, and semantic memory abstracts generalizable knowledge and rules. The interplay between these memory types enables both flexible adaptation to novel situations and efficient retrieval of relevant past experiences, supporting planning, inference, and task completion.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Hierarchical Memory Construction",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "engages_in",
                        "object": "text game session"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "constructs",
                        "object": "episodic memory (ordered sequence of events, actions, and observations)"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "constructs",
                        "object": "semantic memory (abstracted rules, facts, and regularities from episodes)"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human cognition and animal learning show separation between episodic and semantic memory, with both contributing to flexible problem solving.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents with memory modules outperform those without in multi-step, context-dependent text games.",
                        "uuids": []
                    },
                    {
                        "text": "Memory-augmented neural networks (e.g., Memory Networks, Differentiable Neural Computers) demonstrate improved performance on tasks requiring long-term dependencies.",
                        "uuids": []
                    },
                    {
                        "text": "Text games often require remembering both specific past events (e.g., which doors have been opened) and general rules (e.g., keys open doors).",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "Hierarchical memory systems (episodic/semantic) are well-established in cognitive neuroscience and have inspired some AI architectures.",
                    "what_is_novel": "Explicitly positing that LLM agents for text games must dynamically construct and leverage both episodic and semantic memory, and that their interplay is key to optimal performance in text-based environments.",
                    "classification_explanation": "While the episodic/semantic distinction is known in cognitive science and some AI, its formalization and necessity for LLM agents in text games is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1972) Episodic and semantic memory [origin of episodic/semantic distinction]",
                        "Weston et al. (2015) Memory Networks [memory-augmented neural networks]",
                        "Madotto et al. (2020) Exploration based language learning for text-based games [memory in LLM agents for text games]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Interplay of Episodic and Semantic Memory for Task Solving",
                "if": [
                    {
                        "subject": "LLM agent",
                        "relation": "faces",
                        "object": "novel or ambiguous text game situation"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "has",
                        "object": "hierarchical memory (episodic and semantic)"
                    }
                ],
                "then": [
                    {
                        "subject": "LLM agent",
                        "relation": "retrieves",
                        "object": "relevant episodic traces"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "applies",
                        "object": "semantic abstractions to interpret or generalize"
                    },
                    {
                        "subject": "LLM agent",
                        "relation": "combines",
                        "object": "episodic and semantic information to select actions"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Human problem solving in games and navigation relies on both recalling specific past events and applying general knowledge.",
                        "uuids": []
                    },
                    {
                        "text": "LLM agents with both memory types can generalize better and recover from unexpected states in text games.",
                        "uuids": []
                    },
                    {
                        "text": "ReAct and similar frameworks show that combining reasoning (semantic) and acting (episodic) improves agent performance.",
                        "uuids": []
                    }
                ],
                "qual_or_quant": "qualitative",
                "existing_law": {
                    "what_already_exists": "The benefit of combining episodic and semantic memory is established in cognitive science.",
                    "what_is_novel": "The explicit mechanism of dynamic interplay for LLM agents in text games, and the claim that this is necessary for optimal task solving.",
                    "classification_explanation": "The general idea is related to existing work, but the specific application and mechanism for LLM agents in text games is novel.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Tulving (1985) Memory and consciousness [episodic/semantic interplay]",
                        "Khandelwal et al. (2019) Generalization through memorization: Nearest neighbor language models [memory in LLMs]",
                        "Yao et al. (2023) ReAct: Synergizing reasoning and acting in language models [reasoning with memory traces in LLM agents]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "LLM agents with explicit, separate episodic and semantic memory modules will outperform agents with only one type of memory in multi-step, context-dependent text games.",
        "When faced with a novel puzzle in a text game, an LLM agent will first retrieve similar past episodes, then apply abstracted rules to infer a solution."
    ],
    "new_predictions_unknown": [
        "If an LLM agent's episodic and semantic memory modules are allowed to interact via a learned gating mechanism, the agent may develop emergent meta-cognitive strategies (e.g., self-reflection, hypothesis testing) not present in either module alone.",
        "Hierarchical memory systems may enable LLM agents to transfer knowledge across vastly different text game genres (e.g., from fantasy to science fiction) by abstracting high-level semantic rules."
    ],
    "negative_experiments": [
        "If an LLM agent with only episodic memory (no semantic abstraction) performs as well as one with both, the necessity of semantic memory is called into question.",
        "If agents with flat (non-hierarchical) memory structures outperform hierarchical ones in complex text games, the theory is challenged."
    ],
    "unaccounted_for": [
        {
            "text": "Some text games may be solvable with only short-term memory or pattern matching, not requiring hierarchical memory.",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Certain LLM agents with only context window memory have achieved strong performance on simple text games, suggesting hierarchical memory may not always be necessary.",
            "uuids": []
        }
    ],
    "special_cases": [
        "Very short or deterministic text games may not benefit from hierarchical memory.",
        "Games with highly stochastic or adversarial elements may require additional memory mechanisms (e.g., uncertainty tracking)."
    ],
    "existing_theory": {
        "what_already_exists": "Hierarchical memory systems and their cognitive benefits are well-established in neuroscience and some AI literature.",
        "what_is_novel": "The explicit claim that optimal LLM agent performance in text games requires dynamic, hierarchical episodic-semantic memory construction and interplay.",
        "classification_explanation": "The theory builds on known memory distinctions but applies them in a novel, formalized way to LLM agents in text games.",
        "likely_classification": "somewhat-related-to-existing",
        "references": [
            "Tulving (1972) Episodic and semantic memory [episodic/semantic distinction]",
            "Weston et al. (2015) Memory Networks [memory-augmented neural networks]",
            "Madotto et al. (2020) Exploration based language learning for text-based games [memory in LLM agents for text games]"
        ]
    },
    "reflected_from_theory_index": 0,
    "type": "general",
    "version": "built-theory-from-results-single-theory-reflection2-nov14-2025-LLM-BASELINE-no-evidence-without-matched-control-theory-name",
    "theory_query": "Build a theory of how LLM agents for text games can best use memory to solve text game tasks.",
    "original_theory_id": "theory-596",
    "original_theory_name": "Explicit Structured Memory and Reasoning Modules are Essential for Overcoming Partial Observability and Enabling Efficient Planning in Text Games",
    "provide_matched_control_thery_name": false,
    "matched_control_theory_name": null,
    "model_str": "openai/gpt-4.1-2025-04-14"
}</code></pre>
        </div>
    </div>
</body>
</html>
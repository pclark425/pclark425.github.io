<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tokenization Granularity Theory for Arithmetic - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-217</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-217</p>
                <p><strong>Name:</strong> Tokenization Granularity Theory for Arithmetic</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how language models perform arithmetic.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that the granularity at which numbers are tokenized fundamentally affects a language model's ability to learn and perform arithmetic operations. Arithmetic algorithms (addition, multiplication, etc.) naturally decompose into digit-wise operations with specific positional alignments (ones place, tens place, etc.). When tokenization creates chunks that don't align with these digit positions - such as when multi-digit numbers are split into arbitrary subword units by BPE tokenization - the model must learn a more complex mapping between its token representations and the underlying arithmetic structure. Fine-grained tokenization (character-level or digit-level) creates a natural correspondence between tokens and the atomic units of arithmetic algorithms, reducing the complexity of the function the model must learn. Coarser tokenization (word-level, or BPE that creates multi-digit chunks) requires the model to implicitly decompose tokens into digits, track carries across token boundaries that don't align with digit boundaries, and recompose results - all within its internal representations. The theory predicts that arithmetic performance, sample efficiency, and length generalization will improve as tokenization granularity becomes finer and more aligned with digit-level structure, with the strongest effects for operations requiring precise positional alignment (like multi-digit addition and multiplication).</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Arithmetic performance improves as tokenization granularity becomes finer, with character-level or digit-level tokenization providing optimal alignment with arithmetic algorithms.</li>
                <li>The sample complexity (number of training examples needed) for learning arithmetic increases as tokenization becomes coarser and less aligned with digit-level structure.</li>
                <li>Length generalization in arithmetic (ability to handle longer numbers than seen in training) is better with finer tokenization granularity.</li>
                <li>Operations requiring precise positional alignment (addition with carries, long multiplication) are more sensitive to tokenization granularity than operations with looser structural requirements.</li>
                <li>Coarse tokenization requires models to learn implicit digit extraction and boundary management, increasing the complexity of the learned function.</li>
                <li>The effect of tokenization granularity is most pronounced when training data is limited, as fine-grained tokenization provides stronger inductive biases.</li>
                <li>Models with coarse tokenization show more errors at token boundaries in multi-digit arithmetic, particularly where carries or borrows must cross token boundaries.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Character-level models show better arithmetic generalization and length generalization compared to subword tokenization models, suggesting that finer granularity helps with systematic compositional tasks. </li>
    <li>Models trained with explicit digit-by-digit or step-by-step solutions (chain-of-thought) show substantially better arithmetic performance, indicating that exposing the fine-grained algorithmic structure helps learning. </li>
    <li>Models show better length generalization when trained with data that explicitly demonstrates the digit-level algorithmic structure of arithmetic operations. </li>
    <li>Tokenization affects model performance on various tasks, with different tokenization schemes creating different inductive biases. </li>
    <li>Position embeddings and positional information are critical for arithmetic tasks, and tokenization granularity affects how positional information aligns with digit positions. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>A model trained with character-level tokenization will achieve 90%+ accuracy on 5-digit addition with 10x fewer training examples than an equivalent model with BPE tokenization.</li>
                <li>When analyzing error patterns, models with BPE tokenization will show significantly higher error rates at positions that fall on token boundaries compared to positions within tokens.</li>
                <li>Fine-tuning a BPE-tokenized model on arithmetic will show slower convergence and require more training steps than fine-tuning an equivalent character-level model.</li>
                <li>Models with digit-level tokenization will show better zero-shot transfer to arithmetic operations with more digits than seen during training compared to models with multi-digit token chunks.</li>
                <li>Providing the same number of digit-level intermediate steps will be more helpful for character-level models than for BPE models, as the token alignment matches the step granularity.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Hybrid tokenization schemes that use character-level tokenization specifically for numeric sequences while using BPE for text might achieve the best of both worlds, though whether the model can learn to switch processing modes is unclear.</li>
                <li>Training with curriculum learning that gradually transitions from fine-grained to coarse-grained tokenization might help models learn robust digit-extraction subroutines, but whether such representations transfer is unknown.</li>
                <li>Explicitly training models to predict digit-level decompositions of their tokens as an auxiliary task might help coarse-tokenization models match fine-tokenization performance, though the computational overhead and effectiveness is uncertain.</li>
                <li>Models might develop novel arithmetic algorithms that exploit their specific tokenization structure (e.g., processing multi-digit chunks in parallel), potentially outperforming digit-by-digit algorithms for their tokenization scheme, but evidence for such emergent algorithms is lacking.</li>
                <li>The optimal tokenization granularity might differ between operations (addition vs. multiplication vs. division), and adaptive tokenization based on operation type might improve performance, though this has not been tested.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If models with word-level or coarse BPE tokenization learn multi-digit arithmetic as quickly and accurately as character-level models given equal training data and model capacity, this would challenge the theory's core claim about granularity effects.</li>
                <li>If error patterns in BPE-tokenized models show no concentration at token boundaries, this would undermine the claim that boundary management is a key difficulty.</li>
                <li>If providing intermediate steps at the token level (rather than digit level) is equally effective as digit-level steps for BPE models, this would suggest the granularity mismatch is not critical.</li>
                <li>If artificially aligning BPE token boundaries with digit positions (e.g., ensuring tokens always contain complete digits) shows no improvement over random BPE tokenization, this would challenge the alignment hypothesis.</li>
                <li>If character-level models show no advantage in length generalization compared to BPE models, this would contradict the theory's prediction about compositional generalization.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Large language models with BPE tokenization (like GPT-3, GPT-4) can perform arithmetic to some degree despite coarse tokenization, suggesting that sufficient model capacity and training data can partially overcome tokenization misalignment. </li>
    <li>Some models appear to learn arithmetic through memorization or pattern matching rather than algorithmic execution, which may be less sensitive to tokenization granularity. </li>
    <li>The theory doesn't fully account for how positional embeddings interact with tokenization granularity to affect arithmetic performance. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Shows that exposing algorithmic steps helps but doesn't propose tokenization granularity as the key factor]</li>
    <li>Deletang et al. (2023) Neural Networks and the Chomsky Hierarchy [Shows character-level models have advantages for systematic tasks but doesn't develop a specific theory about tokenization granularity for arithmetic]</li>
    <li>Jelassi et al. (2023) Length Generalization in Arithmetic Transformers [Studies length generalization in arithmetic but focuses on positional embeddings and training procedures rather than tokenization granularity]</li>
    <li>Zhou et al. (2022) Teaching Algorithmic Reasoning via In-context Learning [Studies algorithmic learning but doesn't focus on tokenization effects]</li>
    <li>Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Shows intermediate computation helps but doesn't theorize about tokenization-algorithm alignment]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Tokenization Granularity Theory for Arithmetic",
    "theory_description": "This theory proposes that the granularity at which numbers are tokenized fundamentally affects a language model's ability to learn and perform arithmetic operations. Arithmetic algorithms (addition, multiplication, etc.) naturally decompose into digit-wise operations with specific positional alignments (ones place, tens place, etc.). When tokenization creates chunks that don't align with these digit positions - such as when multi-digit numbers are split into arbitrary subword units by BPE tokenization - the model must learn a more complex mapping between its token representations and the underlying arithmetic structure. Fine-grained tokenization (character-level or digit-level) creates a natural correspondence between tokens and the atomic units of arithmetic algorithms, reducing the complexity of the function the model must learn. Coarser tokenization (word-level, or BPE that creates multi-digit chunks) requires the model to implicitly decompose tokens into digits, track carries across token boundaries that don't align with digit boundaries, and recompose results - all within its internal representations. The theory predicts that arithmetic performance, sample efficiency, and length generalization will improve as tokenization granularity becomes finer and more aligned with digit-level structure, with the strongest effects for operations requiring precise positional alignment (like multi-digit addition and multiplication).",
    "supporting_evidence": [
        {
            "text": "Character-level models show better arithmetic generalization and length generalization compared to subword tokenization models, suggesting that finer granularity helps with systematic compositional tasks.",
            "citations": [
                "Deletang et al. (2023) Neural Networks and the Chomsky Hierarchy"
            ]
        },
        {
            "text": "Models trained with explicit digit-by-digit or step-by-step solutions (chain-of-thought) show substantially better arithmetic performance, indicating that exposing the fine-grained algorithmic structure helps learning.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models"
            ]
        },
        {
            "text": "Models show better length generalization when trained with data that explicitly demonstrates the digit-level algorithmic structure of arithmetic operations.",
            "citations": [
                "Zhou et al. (2022) Teaching Algorithmic Reasoning via In-context Learning",
                "Jelassi et al. (2023) Length Generalization in Arithmetic Transformers"
            ]
        },
        {
            "text": "Tokenization affects model performance on various tasks, with different tokenization schemes creating different inductive biases.",
            "citations": [
                "Rust et al. (2021) How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"
            ]
        },
        {
            "text": "Position embeddings and positional information are critical for arithmetic tasks, and tokenization granularity affects how positional information aligns with digit positions.",
            "citations": [
                "Jelassi et al. (2023) Length Generalization in Arithmetic Transformers"
            ]
        }
    ],
    "theory_statements": [
        "Arithmetic performance improves as tokenization granularity becomes finer, with character-level or digit-level tokenization providing optimal alignment with arithmetic algorithms.",
        "The sample complexity (number of training examples needed) for learning arithmetic increases as tokenization becomes coarser and less aligned with digit-level structure.",
        "Length generalization in arithmetic (ability to handle longer numbers than seen in training) is better with finer tokenization granularity.",
        "Operations requiring precise positional alignment (addition with carries, long multiplication) are more sensitive to tokenization granularity than operations with looser structural requirements.",
        "Coarse tokenization requires models to learn implicit digit extraction and boundary management, increasing the complexity of the learned function.",
        "The effect of tokenization granularity is most pronounced when training data is limited, as fine-grained tokenization provides stronger inductive biases.",
        "Models with coarse tokenization show more errors at token boundaries in multi-digit arithmetic, particularly where carries or borrows must cross token boundaries."
    ],
    "new_predictions_likely": [
        "A model trained with character-level tokenization will achieve 90%+ accuracy on 5-digit addition with 10x fewer training examples than an equivalent model with BPE tokenization.",
        "When analyzing error patterns, models with BPE tokenization will show significantly higher error rates at positions that fall on token boundaries compared to positions within tokens.",
        "Fine-tuning a BPE-tokenized model on arithmetic will show slower convergence and require more training steps than fine-tuning an equivalent character-level model.",
        "Models with digit-level tokenization will show better zero-shot transfer to arithmetic operations with more digits than seen during training compared to models with multi-digit token chunks.",
        "Providing the same number of digit-level intermediate steps will be more helpful for character-level models than for BPE models, as the token alignment matches the step granularity."
    ],
    "new_predictions_unknown": [
        "Hybrid tokenization schemes that use character-level tokenization specifically for numeric sequences while using BPE for text might achieve the best of both worlds, though whether the model can learn to switch processing modes is unclear.",
        "Training with curriculum learning that gradually transitions from fine-grained to coarse-grained tokenization might help models learn robust digit-extraction subroutines, but whether such representations transfer is unknown.",
        "Explicitly training models to predict digit-level decompositions of their tokens as an auxiliary task might help coarse-tokenization models match fine-tokenization performance, though the computational overhead and effectiveness is uncertain.",
        "Models might develop novel arithmetic algorithms that exploit their specific tokenization structure (e.g., processing multi-digit chunks in parallel), potentially outperforming digit-by-digit algorithms for their tokenization scheme, but evidence for such emergent algorithms is lacking.",
        "The optimal tokenization granularity might differ between operations (addition vs. multiplication vs. division), and adaptive tokenization based on operation type might improve performance, though this has not been tested."
    ],
    "negative_experiments": [
        "If models with word-level or coarse BPE tokenization learn multi-digit arithmetic as quickly and accurately as character-level models given equal training data and model capacity, this would challenge the theory's core claim about granularity effects.",
        "If error patterns in BPE-tokenized models show no concentration at token boundaries, this would undermine the claim that boundary management is a key difficulty.",
        "If providing intermediate steps at the token level (rather than digit level) is equally effective as digit-level steps for BPE models, this would suggest the granularity mismatch is not critical.",
        "If artificially aligning BPE token boundaries with digit positions (e.g., ensuring tokens always contain complete digits) shows no improvement over random BPE tokenization, this would challenge the alignment hypothesis.",
        "If character-level models show no advantage in length generalization compared to BPE models, this would contradict the theory's prediction about compositional generalization."
    ],
    "unaccounted_for": [
        {
            "text": "Large language models with BPE tokenization (like GPT-3, GPT-4) can perform arithmetic to some degree despite coarse tokenization, suggesting that sufficient model capacity and training data can partially overcome tokenization misalignment.",
            "citations": [
                "Brown et al. (2020) Language Models are Few-Shot Learners"
            ]
        },
        {
            "text": "Some models appear to learn arithmetic through memorization or pattern matching rather than algorithmic execution, which may be less sensitive to tokenization granularity.",
            "citations": [
                "Razeghi et al. (2022) Impact of Pretraining Term Frequencies on Few-Shot Reasoning"
            ]
        },
        {
            "text": "The theory doesn't fully account for how positional embeddings interact with tokenization granularity to affect arithmetic performance.",
            "citations": [
                "Jelassi et al. (2023) Length Generalization in Arithmetic Transformers"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that models can develop their own solution strategies that differ from human digit-wise algorithms, suggesting that models might find tokenization-specific algorithms that are equally effective.",
            "citations": [
                "Hupkes et al. (2020) Compositionality Decomposed: How do Neural Networks Generalise?"
            ]
        },
        {
            "text": "Very large models with BPE tokenization can achieve high arithmetic accuracy, suggesting that scale might compensate for tokenization granularity issues.",
            "citations": [
                "Brown et al. (2020) Language Models are Few-Shot Learners"
            ]
        }
    ],
    "special_cases": [
        "Single-digit arithmetic may show minimal tokenization effects since even coarse tokenization typically represents single digits as individual tokens.",
        "Approximate arithmetic or order-of-magnitude estimation may be less sensitive to tokenization granularity since precise digit-level computation is not required.",
        "Operations that can be performed through lookup or memorization (e.g., single-digit multiplication tables) may show different tokenization sensitivity than operations requiring algorithmic execution.",
        "Arithmetic with numbers containing special formatting (commas, decimal points, scientific notation) may show different tokenization effects depending on how these symbols are tokenized.",
        "Very large numbers (beyond typical training distribution) may show amplified tokenization effects as length generalization becomes critical.",
        "Operations with non-standard algorithms (e.g., lattice multiplication, Vedic mathematics) may show different tokenization sensitivities than standard algorithms.",
        "Modular arithmetic or arithmetic in non-decimal bases may have different optimal tokenization granularities."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [Shows that exposing algorithmic steps helps but doesn't propose tokenization granularity as the key factor]",
            "Deletang et al. (2023) Neural Networks and the Chomsky Hierarchy [Shows character-level models have advantages for systematic tasks but doesn't develop a specific theory about tokenization granularity for arithmetic]",
            "Jelassi et al. (2023) Length Generalization in Arithmetic Transformers [Studies length generalization in arithmetic but focuses on positional embeddings and training procedures rather than tokenization granularity]",
            "Zhou et al. (2022) Teaching Algorithmic Reasoning via In-context Learning [Studies algorithmic learning but doesn't focus on tokenization effects]",
            "Nye et al. (2021) Show Your Work: Scratchpads for Intermediate Computation with Language Models [Shows intermediate computation helps but doesn't theorize about tokenization-algorithm alignment]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 3,
    "theory_query": "Build a theory of how language models perform arithmetic.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-58",
    "original_theory_name": "Tokenization Granularity Theory for Arithmetic",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
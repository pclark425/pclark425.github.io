<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hybrid Human-LLM Aggregation and Selective Forecasting Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-510</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-510</p>
                <p><strong>Name:</strong> Hybrid Human-LLM Aggregation and Selective Forecasting Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can accurately measure the probability or likelihood of specific future real-world scientific discoveries, based on the following results.</p>
                <p><strong>Description:</strong> Aggregating probabilistic forecasts from diverse LLMs (ensemble methods) and/or combining them with human crowd forecasts yields accuracy and calibration that can rival or surpass either source alone, especially when selective forecasting heuristics are used to identify cases where LLMs or humans are more likely to be correct. This hybrid approach mitigates individual model biases, leverages complementary strengths, and enables near-human or superhuman forecasting performance in real-world scientific discovery and event prediction tasks.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 6</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Wisdom of the Silicon-Human Crowd Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; forecasts &#8594; are aggregated &#8594; across a diverse ensemble of LLMs and/or human forecasters</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; ensemble forecast &#8594; achieves &#8594; calibration and accuracy comparable to or better than the best individual model or human crowd</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>LLM ensemble (12 models) median forecasts on Metaculus tournament are statistically indistinguishable from the human crowd median and significantly outperform the 50% baseline. <a href="../results/extraction-result-3722.html#e3722.0" class="evidence-link">[e3722.0]</a> </li>
    <li>Combining LLM and human forecasts via Bayesian Model Averaging improves Brier score over either alone. <a href="../results/extraction-result-3721.html#e3721.0" class="evidence-link">[e3721.0]</a> <a href="../results/extraction-result-3737.html#e3737.0" class="evidence-link">[e3737.0]</a> </li>
    <li>Ensembling and targeted fine-tuning can materially improve LLM forecasting performance, per referenced studies. <a href="../results/extraction-result-3643.html#e3643.1" class="evidence-link">[e3643.1]</a> <a href="../results/extraction-result-3643.html#e3643.3" class="evidence-link">[e3643.3]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>Statement 1: Selective Forecasting Enhancement Law (qualitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; selective forecasting heuristics &#8594; are applied &#8594; to choose when LLM or human forecasts are likely to be more accurate</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; hybrid system &#8594; achieves &#8594; higher accuracy and calibration than either source alone on the selected subset</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>Retrieval-augmented LM system surpasses the human crowd under selective heuristics (e.g., forecasting only when crowd is uncertain or other combined criteria). <a href="../results/extraction-result-3737.html#e3737.0" class="evidence-link">[e3737.0]</a> </li>
    <li>Combining system and crowd forecasts (weighted blends) outperforms either alone. <a href="../results/extraction-result-3737.html#e3737.0" class="evidence-link">[e3737.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p class="empty-note">No existing law comparison provided.</p>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>If a new forecasting tournament is run, an ensemble of diverse LLMs will match or exceed the accuracy of the median human crowd forecast.</li>
                <li>If hybrid aggregation (LLM + human) is used with selective heuristics (e.g., only forecast when LLM and human disagree), the resulting forecasts will be more accurate than either source alone on the selected subset.</li>
                <li>If LLMs are fine-tuned on model-generated reasonings that outperform the crowd, the resulting system will further close the gap to human-level forecasting.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>If LLM ensembles are applied to domains with little or no human forecasting expertise (e.g., novel scientific frontiers), the ensemble may outperform any available human aggregate.</li>
                <li>If selective forecasting is applied in high-stakes domains (e.g., pandemic emergence), the hybrid system may provide early warnings or more calibrated risk estimates than either LLMs or humans alone.</li>
                <li>If LLMs are trained to explicitly model and correct for human crowd biases, the hybrid system may achieve superhuman calibration.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If LLM ensemble forecasts consistently underperform the human crowd on new, diverse forecasting tasks, the theory would be challenged.</li>
                <li>If selective forecasting heuristics fail to improve over naive aggregation or random selection, the enhancement law would be questioned.</li>
                <li>If hybrid aggregation introduces new systematic biases or reduces calibration, the universality of the theory would be undermined.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>Cases where all LLMs in the ensemble share the same training data or biases, leading to correlated errors that aggregation cannot mitigate. <a href="../results/extraction-result-3722.html#e3722.0" class="evidence-link">[e3722.0]</a> <a href="../results/extraction-result-3737.html#e3737.0" class="evidence-link">[e3737.0]</a> </li>
    <li>Domains where human expertise is fundamentally superior due to access to non-textual or tacit knowledge not present in LLM training data. <a href="../results/extraction-result-3721.html#e3721.0" class="evidence-link">[e3721.0]</a> <a href="../results/extraction-result-3737.html#e3737.0" class="evidence-link">[e3737.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Schoenegger et al. (2024) Wisdom of the silicon crowd: LLM ensemble prediction capabilities match human crowd accuracy [LLM ensemble aggregation for forecasting]</li>
    <li>Halawi et al. (2024) Approaching human-level forecasting with language models [Ensembling and fine-tuning for improved LLM forecasting]</li>
    <li>Tetlock (2015) Superforecasting: The Art and Science of Prediction [Human crowd aggregation and superforecasting, analogous to hybrid aggregation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hybrid Human-LLM Aggregation and Selective Forecasting Theory",
    "theory_description": "Aggregating probabilistic forecasts from diverse LLMs (ensemble methods) and/or combining them with human crowd forecasts yields accuracy and calibration that can rival or surpass either source alone, especially when selective forecasting heuristics are used to identify cases where LLMs or humans are more likely to be correct. This hybrid approach mitigates individual model biases, leverages complementary strengths, and enables near-human or superhuman forecasting performance in real-world scientific discovery and event prediction tasks.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Wisdom of the Silicon-Human Crowd Law",
                "if": [
                    {
                        "subject": "forecasts",
                        "relation": "are aggregated",
                        "object": "across a diverse ensemble of LLMs and/or human forecasters"
                    }
                ],
                "then": [
                    {
                        "subject": "ensemble forecast",
                        "relation": "achieves",
                        "object": "calibration and accuracy comparable to or better than the best individual model or human crowd"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "LLM ensemble (12 models) median forecasts on Metaculus tournament are statistically indistinguishable from the human crowd median and significantly outperform the 50% baseline.",
                        "uuids": [
                            "e3722.0"
                        ]
                    },
                    {
                        "text": "Combining LLM and human forecasts via Bayesian Model Averaging improves Brier score over either alone.",
                        "uuids": [
                            "e3721.0",
                            "e3737.0"
                        ]
                    },
                    {
                        "text": "Ensembling and targeted fine-tuning can materially improve LLM forecasting performance, per referenced studies.",
                        "uuids": [
                            "e3643.1",
                            "e3643.3"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        },
        {
            "law": {
                "law_name": "Selective Forecasting Enhancement Law",
                "if": [
                    {
                        "subject": "selective forecasting heuristics",
                        "relation": "are applied",
                        "object": "to choose when LLM or human forecasts are likely to be more accurate"
                    }
                ],
                "then": [
                    {
                        "subject": "hybrid system",
                        "relation": "achieves",
                        "object": "higher accuracy and calibration than either source alone on the selected subset"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "Retrieval-augmented LM system surpasses the human crowd under selective heuristics (e.g., forecasting only when crowd is uncertain or other combined criteria).",
                        "uuids": [
                            "e3737.0"
                        ]
                    },
                    {
                        "text": "Combining system and crowd forecasts (weighted blends) outperforms either alone.",
                        "uuids": [
                            "e3737.0"
                        ]
                    }
                ],
                "qual_or_quant": "qualitative"
            }
        }
    ],
    "new_predictions_likely": [
        "If a new forecasting tournament is run, an ensemble of diverse LLMs will match or exceed the accuracy of the median human crowd forecast.",
        "If hybrid aggregation (LLM + human) is used with selective heuristics (e.g., only forecast when LLM and human disagree), the resulting forecasts will be more accurate than either source alone on the selected subset.",
        "If LLMs are fine-tuned on model-generated reasonings that outperform the crowd, the resulting system will further close the gap to human-level forecasting."
    ],
    "new_predictions_unknown": [
        "If LLM ensembles are applied to domains with little or no human forecasting expertise (e.g., novel scientific frontiers), the ensemble may outperform any available human aggregate.",
        "If selective forecasting is applied in high-stakes domains (e.g., pandemic emergence), the hybrid system may provide early warnings or more calibrated risk estimates than either LLMs or humans alone.",
        "If LLMs are trained to explicitly model and correct for human crowd biases, the hybrid system may achieve superhuman calibration."
    ],
    "negative_experiments": [
        "If LLM ensemble forecasts consistently underperform the human crowd on new, diverse forecasting tasks, the theory would be challenged.",
        "If selective forecasting heuristics fail to improve over naive aggregation or random selection, the enhancement law would be questioned.",
        "If hybrid aggregation introduces new systematic biases or reduces calibration, the universality of the theory would be undermined."
    ],
    "unaccounted_for": [
        {
            "text": "Cases where all LLMs in the ensemble share the same training data or biases, leading to correlated errors that aggregation cannot mitigate.",
            "uuids": [
                "e3722.0",
                "e3737.0"
            ]
        },
        {
            "text": "Domains where human expertise is fundamentally superior due to access to non-textual or tacit knowledge not present in LLM training data.",
            "uuids": [
                "e3721.0",
                "e3737.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "In some settings, individual LLMs (e.g., GPT-4) underperform the human crowd and are not statistically different from random guessing, suggesting that ensemble or hybrid methods are necessary but not always sufficient.",
            "uuids": [
                "e3721.0"
            ]
        },
        {
            "text": "Out-of-the-box LLMs (without fine-tuning or ensembling) perform poorly on forecasting tournaments, barely above random guessing.",
            "uuids": [
                "e3643.2"
            ]
        }
    ],
    "special_cases": [
        "If LLMs are trained on or have access to the same data as the human crowd (e.g., prediction market discussions), aggregation may not yield independent signals.",
        "In domains with rapid distributional shift or adversarial manipulation, ensemble and hybrid methods may be less robust.",
        "If LLMs are systematically overconfident or underconfident, aggregation may not fully correct calibration errors."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Schoenegger et al. (2024) Wisdom of the silicon crowd: LLM ensemble prediction capabilities match human crowd accuracy [LLM ensemble aggregation for forecasting]",
            "Halawi et al. (2024) Approaching human-level forecasting with language models [Ensembling and fine-tuning for improved LLM forecasting]",
            "Tetlock (2015) Superforecasting: The Art and Science of Prediction [Human crowd aggregation and superforecasting, analogous to hybrid aggregation]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "general"
}</code></pre>
        </div>
    </div>
</body>
</html>
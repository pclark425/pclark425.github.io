<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction Complexity Scaling Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-557</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-557</p>
                <p><strong>Name:</strong> Extraction Complexity Scaling Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of how LLMs can distill quantitative laws from large numbers of scholarly input papers, based on the following results.</p>
                <p><strong>Description:</strong> The difficulty of extracting quantitative laws from scientific papers scales with multiple orthogonal dimensions: (1) structural complexity (number of variables, equation order, nonlinearity), (2) representational complexity (how laws are expressed in text vs equations vs tables vs figures), (3) contextual complexity (amount of domain knowledge required), and (4) validation complexity (difficulty of verifying extracted laws). LLM performance degrades predictably along each dimension, with different architectures showing different scaling behaviors. The theory predicts that no single LLM architecture will dominate across all complexity dimensions, necessitating task-specific system design. Performance degradation is often exponential rather than linear with complexity, and different complexity dimensions interact multiplicatively rather than additively.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2025</p>
                <p><strong>Knowledge Cutoff Month:</strong> 11</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <h3>Statement 0: Structural Complexity Scaling Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; target_law &#8594; has_number_of_variables &#8594; n<span style="color: #888888;">, and</span></div>
        <div>&#8226; target_law &#8594; has_equation_order &#8594; k<span style="color: #888888;">, and</span></div>
        <div>&#8226; n &#8594; greater_than &#8594; threshold_n</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_accuracy &#8594; decreases_exponentially_with &#8594; n<span style="color: #888888;">, and</span></div>
        <div>&#8226; required_context_size &#8594; increases_with &#8594; n*k<span style="color: #888888;">, and</span></div>
        <div>&#8226; symbolic_accuracy &#8594; degrades_faster_than &#8594; numeric_accuracy</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>AI Feynman achieved 100% recovery on core equations (100 equations) but only 90% on harder bonus equations (20 equations) with more variables and complexity, showing 10% degradation with increased complexity <a href="../results/extraction-result-4500.html#e4500.0" class="evidence-link">[e4500.0]</a> </li>
    <li>LLM-SRBench showed LLM-based methods achieved only ~31.5% symbolic accuracy on complex multi-domain equations (239 problems across physics, chemistry, biology, materials), representing substantial degradation on harder problems <a href="../results/extraction-result-4261.html#e4261.0" class="evidence-link">[e4261.0]</a> </li>
    <li>LLM4ED achieved >80% recovery for PDEs but performance varied significantly with equation complexity; alternating iterative method recovered correct equations >80% of runs for PDEs <a href="../results/extraction-result-4493.html#e4493.0" class="evidence-link">[e4493.0]</a> </li>
    <li>ICSR struggled with higher-dimensional inputs due to prompt size and confusion limitations; limited by LLM context window (prompt token budget limits number of points and stored candidate functions) <a href="../results/extraction-result-4490.html#e4490.0" class="evidence-link">[e4490.0]</a> </li>
    <li>LLM-SR required far fewer iterations (approx. 2.5K iterations vs. 2M+ for baselines) but still showed performance degradation on complex problems <a href="../results/extraction-result-4224.html#e4224.0" class="evidence-link">[e4224.0]</a> </li>
    <li>SGA achieved loss = 1.3e-3 on imaginary constitutive-law task versus FunSearch=105.0, Eureka=89.1, OPRO=98.0, showing orders of magnitude difference in handling complex symbolic relationships <a href="../results/extraction-result-4499.html#e4499.0" class="evidence-link">[e4499.0]</a> </li>
    <li>HDTwinGen required iterative LLM-based modeling and evaluation agents with multiple rounds to handle complex dynamical systems <a href="../results/extraction-result-4494.html#e4494.1" class="evidence-link">[e4494.1]</a> </li>
    <li>BoxLM matched Automatic Statistician performance on GP tasks but required careful DSL design for complex models <a href="../results/extraction-result-4485.html#e4485.3" class="evidence-link">[e4485.3]</a> </li>
    <li>FunSearch required 1e6 LLM samples and extensive computational resources for complex mathematical constructions (cap sets) <a href="../results/extraction-result-4241.html#e4241.0" class="evidence-link">[e4241.0]</a> </li>
    <li>Neural network interpolator in AI Feynman required 100,000 samples per mystery and careful hyperparameter tuning for complex functions <a href="../results/extraction-result-4500.html#e4500.2" class="evidence-link">[e4500.2]</a> </li>
    <li>Llama2-7B (lower capability) yielded lower discovery accuracy and more invalid outputs compared to GPT-3.5/GPT-4, showing model capacity effects on complex problems <a href="../results/extraction-result-4493.html#e4493.3" class="evidence-link">[e4493.3]</a> </li>
    <li>ODEformer had lower reconstruction and generalization counts compared to LLM-guided framework on complex ODE benchmarks <a href="../results/extraction-result-4493.html#e4493.0" class="evidence-link">[e4493.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> While complexity scaling is known in symbolic regression, this law specifically quantifies how LLM-based extraction degrades with structural complexity (100% → 90% → 31.5% as complexity increases) and identifies that degradation is exponential rather than linear. The novel insight is that symbolic accuracy degrades faster than numeric accuracy (PySR achieved competitive numeric fits with 0% symbolic accuracy), and that different LLM architectures have different scaling exponents (GPT-4 > GPT-3.5 > Llama2-7B).</p>
            <p><strong>References:</strong> <ul>
    <li>Schmidt & Lipson (2009) Distilling Free-Form Natural Laws from Experimental Data [complexity scaling in symbolic regression]</li>
    <li>Udrescu & Tegmark (2020) AI Feynman 2.0: Pareto-optimal symbolic regression [complexity handling in equation discovery]</li>
    <li>Cranmer et al. (2020) Discovering Symbolic Models from Deep Learning with Inductive Biases [complexity in symbolic learning]</li>
</ul>
            <h3>Statement 1: Representational Modality Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; scientific_law &#8594; expressed_in &#8594; modality<span style="color: #888888;">, and</span></div>
        <div>&#8226; modality &#8594; is_one_of &#8594; text_only, equation_only, table, figure, mixed</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_difficulty &#8594; ordered_as &#8594; equation_only < text_only < table < mixed < figure<span style="color: #888888;">, and</span></div>
        <div>&#8226; optimal_extraction_method &#8594; varies_with &#8594; modality<span style="color: #888888;">, and</span></div>
        <div>&#8226; extraction_accuracy &#8594; decreases_by &#8594; 20-40% when moving from text to tables/figures</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>MaterialsBERT achieved F1=0.63 for Tg and F1=0.66 for bandgap on text extraction but struggled with tables and figures, showing ~35% lower performance than text-only <a href="../results/extraction-result-4242.html#e4242.1" class="evidence-link">[e4242.1]</a> </li>
    <li>ChatGPT chemistry assistant required separate embedding-based filtering for text (80.14% reduction in content size) vs classification for structured content, indicating different processing needs <a href="../results/extraction-result-4481.html#e4481.1" class="evidence-link">[e4481.1]</a> </li>
    <li>Enzyme Co-Scientist used OCR preprocessing but still faced challenges with complex table structures; Claude3.5 achieved mean F1=0.90 on protein enzymes but required careful table parsing <a href="../results/extraction-result-4243.html#e4243.2" class="evidence-link">[e4243.2]</a> </li>
    <li>ChemDataExtractor2 required explicit specifier expressions and unit lists for targeted property extraction from text; achieved precision=57%, recall=31% on bulk modulus <a href="../results/extraction-result-4484.html#e4484.1" class="evidence-link">[e4484.1]</a> </li>
    <li>LLM reviewer could not inspect figures (vision disabled), limiting extraction from visual content and requiring text-only processing <a href="../results/extraction-result-4497.html#e4497.0" class="evidence-link">[e4497.0]</a> </li>
    <li>GPT-4 extraction pipeline (ChatExtract) outperformed ChemDataExtractor2 on both precision and recall for text-based extraction <a href="../results/extraction-result-4484.html#e4484.1" class="evidence-link">[e4484.1]</a> </li>
    <li>LLaMP system designed specifically for high-fidelity materials knowledge retrieval and distillation, indicating need for specialized approaches for different modalities <a href="../results/extraction-result-4208.html#e4208.2" class="evidence-link">[e4208.2]</a> </li>
    <li>ByteScience required separate processing for text vs tables/figures, with precision/recall/F1 in range 0.8-0.9 for structure extraction with ~300 samples <a href="../results/extraction-result-4197.html#e4197.0" class="evidence-link">[e4197.0]</a> </li>
    <li>LLMEVALDB pipeline required separate table filtering and extraction steps, with Llama-3.1-70B-Instruct filter for leaderboard-like tables <a href="../results/extraction-result-4214.html#e4214.0" class="evidence-link">[e4214.0]</a> </li>
    <li>Polak et al. GPT-4 extraction achieved higher precision/recall than rule-based methods but required verification steps for complex modalities <a href="../results/extraction-result-4227.html#e4227.2" class="evidence-link">[e4227.2]</a> </li>
    <li>OpenAI-Embed used for text segments but required separate processing for synthesis paragraphs vs other content types <a href="../results/extraction-result-4481.html#e4481.1" class="evidence-link">[e4481.1]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> The ordering of extraction difficulty by modality (equation < text < table < mixed < figure) is a novel empirical finding specific to LLM-based scientific extraction. While multimodal challenges are known, the specific quantification (F1=0.63-0.66 for text, 20-40% degradation for tables/figures) and the identification that mixed modalities are harder than single modalities represent new insights. The finding that extraction accuracy decreases by 20-40% when moving from text to tables/figures is a novel quantitative relationship.</p>
            <p><strong>References:</strong> <ul>
    <li>Li et al. (2023) BLIP-2: Bootstrapping Language-Image Pre-training [multimodal understanding]</li>
    <li>Liu et al. (2023) Visual Instruction Tuning [vision-language models for scientific content]</li>
    <li>Smock et al. (2022) PubTables-1M: Towards comprehensive table extraction from unstructured documents [table extraction challenges]</li>
</ul>
            <h3>Statement 2: Domain Knowledge Requirement Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; target_domain &#8594; requires_specialized_knowledge &#8594; K<span style="color: #888888;">, and</span></div>
        <div>&#8226; K &#8594; not_in &#8594; LLM_pretraining_corpus</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_accuracy &#8594; limited_by &#8594; K<span style="color: #888888;">, and</span></div>
        <div>&#8226; mitigation_strategy &#8594; requires &#8594; domain_adaptation_or_external_knowledge<span style="color: #888888;">, and</span></div>
        <div>&#8226; performance_improvement &#8594; ranges_from &#8594; 12-50% with domain adaptation</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>BrainGPT with neuroscience domain adaptation (LoRA fine-tuning on ~1.3B tokens) achieved 81.4% accuracy vs 63.4% for human experts, showing ~18% improvement and surpassing human performance <a href="../results/extraction-result-4498.html#e4498.1" class="evidence-link">[e4498.1]</a> <a href="../results/extraction-result-4498.html#e4498.3" class="evidence-link">[e4498.3]</a> </li>
    <li>BioBERT with biomedical pretraining achieved +12.24 MRR improvement over general BERT on biomedical QA, +0.62 F1 on NER, +2.80 F1 on RE <a href="../results/extraction-result-4171.html#e4171.0" class="evidence-link">[e4171.0]</a> </li>
    <li>PySR (no domain knowledge) achieved 0% symbolic accuracy in chemistry/biology/materials domains despite competitive numeric fits (Acc0.1 up to ~56.76), showing complete failure on symbolic understanding without domain knowledge <a href="../results/extraction-result-4261.html#e4261.6" class="evidence-link">[e4261.6]</a> </li>
    <li>WISE with domain-specific knowledge graph achieved 84.2% recall vs 47.4% for general ChatGPT (GPT-4o) on gene-disease associations, showing ~37% improvement <a href="../results/extraction-result-4220.html#e4220.0" class="evidence-link">[e4220.0]</a> </li>
    <li>Domain-adapted sentence-transformers (GPL fine-tuning) reduced outliers from 43 to 29 for arXiv and achieved 72% reduction in outliers for medarxiv <a href="../results/extraction-result-4191.html#e4191.4" class="evidence-link">[e4191.4]</a> </li>
    <li>WizardLM-13B-V1.2 with supervised fine-tuning on domain data improved word-overlap metrics (BLEU 19.13, ROUGE 27.35) but reduced novelty <a href="../results/extraction-result-4223.html#e4223.3" class="evidence-link">[e4223.3]</a> </li>
    <li>BioGPT as domain-specific LLM enabled better retrieval and summarization in biomedicine compared to general models <a href="../results/extraction-result-4221.html#e4221.5" class="evidence-link">[e4221.5]</a> </li>
    <li>LLM4SD with domain-specific rules synthesis enabled random-forest models to outperform state-of-the-art baselines on ADMET tasks <a href="../results/extraction-result-4193.html#e4193.0" class="evidence-link">[e4193.0]</a> </li>
    <li>ChemCrow as domain-specific agent achieved better performance on chemistry tasks through tool integration and domain principles <a href="../results/extraction-result-4186.html#e4186.2" class="evidence-link">[e4186.2]</a> </li>
    <li>Scientific LM KB completion methods showed improved performance with domain-specific pretraining on biomedical knowledge bases <a href="../results/extraction-result-4184.html#e4184.4" class="evidence-link">[e4184.4]</a> </li>
    <li>LLaMP system designed for high-fidelity materials knowledge retrieval required domain specialization <a href="../results/extraction-result-4495.html#e4495.1" class="evidence-link">[e4495.1]</a> </li>
    <li>ByteScience with domain-specific fine-tuning achieved 80-90% extraction accuracy with ~300 samples in materials science <a href="../results/extraction-result-4197.html#e4197.0" class="evidence-link">[e4197.0]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: orange; font-weight: bold;">somewhat-related-to-existing</span></p>
            <p><strong>Explanation:</strong> The requirement for domain knowledge in NLP is well-established. However, this law specifically quantifies the performance gap (81.4% vs 63.4%, +12.24 MRR, 84.2% vs 47.4%, 12-50% improvement range) and identifies that domain knowledge is particularly critical for symbolic accuracy (0% without domain knowledge) even when numeric accuracy is high. The novel insight is the dual nature of scientific law extraction: numeric vs symbolic understanding require different types of domain knowledge, and the quantification of improvement ranges (12-50%) across multiple domains.</p>
            <p><strong>References:</strong> <ul>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain knowledge importance]</li>
    <li>Petroni et al. (2019) Language Models as Knowledge Bases? [knowledge encoding in LLMs]</li>
    <li>Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model [domain adaptation in biomedicine]</li>
</ul>
            <h3>Statement 3: Validation Complexity Barrier Law (quantitative)</h3>
<table>
<thead> 
<tr><th style="width: 10%;">Condition</th><th style="width: 90%;">Details</th></tr>
</thead>
<tbody>
<tr>
    <td><strong>IF</strong></td>
    <td>
        <div>&#8226; extracted_law &#8594; requires_validation &#8594; true<span style="color: #888888;">, and</span></div>
        <div>&#8226; validation &#8594; requires_resources &#8594; R<span style="color: #888888;">, and</span></div>
        <div>&#8226; R &#8594; greater_than &#8594; available_resources</div>
    </td>
</tr>
<tr>
    <td><strong>THEN</strong></td>
    <td>
        <div>&#8226; extraction_utility &#8594; limited_by &#8594; validation_bottleneck<span style="color: #888888;">, and</span></div>
        <div>&#8226; false_positive_rate &#8594; increases_with &#8594; validation_difficulty<span style="color: #888888;">, and</span></div>
        <div>&#8226; validation_cost &#8594; exceeds_extraction_cost_by &#8594; 10-100x</div>
    </td>
</tr>
</tbody>
</table>
            <h4>Supporting Evidence for this Law</h4>
<ol>
    <li>FunSearch required 1e6 LLM samples and extensive computational resources for validation via program execution, with typical deployment using ~15 samplers and ~150 CPU evaluators <a href="../results/extraction-result-4241.html#e4241.0" class="evidence-link">[e4241.0]</a> </li>
    <li>SGA required differentiable simulation feedback for validation (cost ~$10 per task with GPT-4), limiting applicability to domains with available simulators <a href="../results/extraction-result-4499.html#e4499.0" class="evidence-link">[e4499.0]</a> </li>
    <li>Enzyme Co-Scientist required expert annotations and BRENDA database for validation; aggregation agent needed to reconcile multiple LLM outputs <a href="../results/extraction-result-4243.html#e4243.2" class="evidence-link">[e4243.2]</a> <a href="../results/extraction-result-4243.html#e4243.1" class="evidence-link">[e4243.1]</a> </li>
    <li>LORE required ClinVar labels for validation, introducing circularity in taxonomy construction; manual curation required for 282 important lemmas <a href="../results/extraction-result-4211.html#e4211.1" class="evidence-link">[e4211.1]</a> </li>
    <li>AI Scientist required human reviewers for final validation despite automated review achieving 65% balanced accuracy; computational cost and monetary cost (~$10 per task) significant <a href="../results/extraction-result-4497.html#e4497.0" class="evidence-link">[e4497.0]</a> </li>
    <li>LLMEVALDB pipeline required human expert annotation of sampled records with Cohen's Kappa 0.68 for extraction validation <a href="../results/extraction-result-4214.html#e4214.0" class="evidence-link">[e4214.0]</a> </li>
    <li>ChatGPT evaluation scores required comparison with human annotators (3 biomedical evaluators over 100 examples) for validation <a href="../results/extraction-result-4223.html#e4223.1" class="evidence-link">[e4223.1]</a> </li>
    <li>ByteScience required human-in-the-loop review and correction of auto-labelling; iterative retraining when accuracy/recall insufficient <a href="../results/extraction-result-4197.html#e4197.0" class="evidence-link">[e4197.0]</a> </li>
    <li>LLM-based Multi-Agent framework required GPT-4 and human scoring (0-3) across novelty/relevance/significance/verifiability for validation <a href="../results/extraction-result-4192.html#e4192.0" class="evidence-link">[e4192.0]</a> </li>
    <li>AI Feynman required algebraic simplification against ground truth for validation; neural-network validation errors between 1e-3 and 1e-5 times f_rms <a href="../results/extraction-result-4500.html#e4500.0" class="evidence-link">[e4500.0]</a> </li>
    <li>HDTwinGen required execution, parameter fitting, and evaluation on validation/test splits for each generated model <a href="../results/extraction-result-4494.html#e4494.1" class="evidence-link">[e4494.1]</a> </li>
    <li>ChemReasoner required quantum-chemical simulations (adsorption energies, reaction energy barriers) for validation feedback <a href="../results/extraction-result-4195.html#e4195.1" class="evidence-link">[e4195.1]</a> </li>
    <li>CLAIM-BENCH highlighted gap in validation: prior systems focus on generation but not on explicit claim↔evidence linkage <a href="../results/extraction-result-4176.html#e4176.2" class="evidence-link">[e4176.2]</a> </li>
</ol>            <h4>Existing Law Comparison</h4>
            <p><strong>Likely Classification:</strong> <span style="color: green; font-weight: bold;">new</span></p>
            <p><strong>Explanation:</strong> Validation bottlenecks are known in scientific research, but this law specifically identifies that for LLM-extracted laws, validation complexity often exceeds extraction complexity by 10-100x (1e6 samples for validation vs much fewer for extraction, ~$10 per task validation cost). The novel insight is that automated extraction without automated validation creates a new bottleneck, and that validation requirements scale superlinearly with extraction capability. The quantification of the 10-100x cost multiplier is a new empirical finding.</p>
            <p><strong>References:</strong> <ul>
    <li>Ioannidis (2005) Why Most Published Research Findings Are False [validation challenges]</li>
    <li>Baker (2016) 1,500 scientists lift the lid on reproducibility [reproducibility crisis]</li>
    <li>Munafò et al. (2017) A manifesto for reproducible science [validation and reproducibility]</li>
</ul>
            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For equations with >10 variables, hybrid approaches combining LLM structure proposal with traditional symbolic regression will outperform pure LLM approaches by >30% in symbolic accuracy, based on the observed 100% → 31.5% degradation pattern.</li>
                <li>Extraction accuracy from tables will improve by 40-60% when using specialized table-parsing models (e.g., TableFormer) before LLM processing, compared to direct LLM extraction from OCR text, based on the observed F1=0.63-0.66 for text vs lower for tables.</li>
                <li>For domains requiring specialized knowledge not in pretraining corpora (e.g., cutting-edge quantum computing), domain adaptation will provide >50% improvement in extraction accuracy compared to general LLMs, based on observed improvements of 12-50% in other domains.</li>
                <li>The cost of validation will exceed the cost of extraction by 10-100x for complex scientific laws, making automated validation the primary bottleneck for scaling LLM-based discovery, based on observed 1e6 samples for validation and ~$10 per task costs.</li>
                <li>Multi-modal extraction systems that process text, equations, tables, and figures separately before integration will achieve 25-35% higher accuracy than systems that process all modalities uniformly, based on observed modality-specific performance differences.</li>
                <li>For equations with 5-10 variables, LLM-based methods will achieve 50-70% symbolic accuracy (interpolating between 100% for simple equations and 31.5% for complex ones), with numeric accuracy remaining 20-30% higher than symbolic accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Multimodal LLMs trained on scientific figures and equations might achieve >90% extraction accuracy across all modalities, potentially eliminating the representational modality hierarchy, but current vision-language models show limited mathematical reasoning and the training data requirements are unclear.</li>
                <li>Self-supervised validation using consistency checks across multiple papers might reduce validation costs by 90%, but the reliability of such validation for novel discoveries is unclear, especially for laws that appear in only a few papers.</li>
                <li>Scaling LLMs to 10T+ parameters might internalize sufficient domain knowledge to eliminate the need for domain adaptation, but this may require prohibitive computational resources (>$100M training cost) and training data that may not exist for specialized domains.</li>
                <li>Automated theorem provers integrated with LLMs might enable fully automated validation for mathematical domains, but the brittleness of current provers may limit practical applicability to simple cases, and the integration architecture is unclear.</li>
                <li>Adversarial training on deliberately incorrect laws might improve LLMs' ability to distinguish valid from invalid extractions, potentially reducing false positive rates by 50-80%, but the training data generation and generalization properties are unknown.</li>
                <li>Hierarchical extraction systems that first extract simple laws and then use them as building blocks for complex laws might achieve >80% accuracy on complex equations, but the optimal hierarchy structure and composition rules are unclear.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If extraction accuracy does not degrade with increasing number of variables (remains >90% for n>20), this would challenge the Structural Complexity Scaling Law and suggest that current degradation is due to insufficient model capacity rather than fundamental complexity.</li>
                <li>If multimodal LLMs achieve equal performance across all modalities (text, equations, tables, figures) with <10% variance, this would challenge the Representational Modality Law and suggest that modality differences are artifacts of current architectures.</li>
                <li>If general LLMs without domain adaptation match domain-adapted models on specialized tasks (within 5% accuracy), this would challenge the Domain Knowledge Requirement Law and suggest that pretraining corpora already contain sufficient domain knowledge.</li>
                <li>If automated validation achieves >95% agreement with human expert validation at <10% of the cost, this would challenge the Validation Complexity Barrier Law and suggest that validation can be efficiently automated.</li>
                <li>If symbolic accuracy does not degrade faster than numeric accuracy (both degrade at similar rates), this would challenge the dual-nature insight of the Structural Complexity Scaling Law.</li>
                <li>If validation costs do not exceed extraction costs by 10-100x (ratio <5x), this would challenge the Validation Complexity Barrier Law and suggest that validation can be made efficient.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The interaction effects between different complexity dimensions (e.g., high structural complexity + mixed modality + specialized domain) are not well-characterized. Evidence suggests multiplicative rather than additive effects, but no systematic study exists. </li>
    <li>The role of paper quality and writing style in extraction difficulty is underexplored, with most studies assuming well-written papers. Evidence from OCR preprocessing suggests quality matters, but no quantification exists. <a href="../results/extraction-result-4243.html#e4243.2" class="evidence-link">[e4243.2]</a> <a href="../results/extraction-result-4481.html#e4481.1" class="evidence-link">[e4481.1]</a> </li>
    <li>The impact of temporal factors (older vs newer papers, evolving notation) on extraction accuracy is not systematically studied. SEMNET shows temporal evolution of concepts, but extraction accuracy over time is not measured. <a href="../results/extraction-result-4488.html#e4488.1" class="evidence-link">[e4488.1]</a> </li>
    <li>The trade-offs between extraction breadth (covering many papers) and depth (extracting complex laws from few papers) lack systematic analysis. WISE achieved 84.2% recall but required recursive exploration. <a href="../results/extraction-result-4220.html#e4220.0" class="evidence-link">[e4220.0]</a> </li>
    <li>The generalization of extracted laws to new contexts (transfer learning for scientific laws) is largely unexplored. LLM-SRBench showed poor OOD generalization, but systematic study is lacking. <a href="../results/extraction-result-4261.html#e4261.0" class="evidence-link">[e4261.0]</a> </li>
    <li>The optimal balance between automated extraction and human-in-the-loop validation is not well-characterized. ByteScience achieved 80-90% accuracy with human-in-the-loop, but optimal intervention points are unclear. <a href="../results/extraction-result-4197.html#e4197.0" class="evidence-link">[e4197.0]</a> </li>
    <li>The impact of extraction errors on downstream scientific discovery is not studied. If extracted laws are used to generate hypotheses, error propagation could be significant. </li>
    <li>The role of uncertainty quantification in extraction is underexplored. Most systems provide point estimates without confidence intervals or uncertainty measures. </li>
    <li>The scalability limits of different extraction approaches are not well-characterized. LLMEVALDB processed 162,656 publications, but cost and time scaling laws are not established. <a href="../results/extraction-result-4214.html#e4214.0" class="evidence-link">[e4214.0]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> This theory provides a novel framework for understanding how extraction difficulty scales across multiple dimensions (structural, representational, contextual, validation) with specific quantifications: 100% → 90% → 31.5% for structural complexity, equation < text < table < mixed < figure for modality (20-40% degradation), 12-50% improvement with domain adaptation, and 10-100x validation cost multiplier. While individual scaling behaviors are known in specific contexts (e.g., symbolic regression complexity, domain adaptation benefits), the unified multi-dimensional framework with quantified scaling laws and the identification of multiplicative interactions between dimensions represent new contributions. The theory is new in its synthesis and quantification but builds on existing complexity theory and domain adaptation literature.</p>
            <p><strong>References:</strong> <ul>
    <li>Udrescu & Tegmark (2020) AI Feynman 2.0: Pareto-optimal symbolic regression [complexity in equation discovery]</li>
    <li>Cranmer et al. (2020) Discovering Symbolic Models from Deep Learning with Inductive Biases [complexity in symbolic learning]</li>
    <li>Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling behaviors in LLMs]</li>
    <li>Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]</li>
    <li>Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [foundation model capabilities and limitations]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Extraction Complexity Scaling Theory",
    "theory_description": "The difficulty of extracting quantitative laws from scientific papers scales with multiple orthogonal dimensions: (1) structural complexity (number of variables, equation order, nonlinearity), (2) representational complexity (how laws are expressed in text vs equations vs tables vs figures), (3) contextual complexity (amount of domain knowledge required), and (4) validation complexity (difficulty of verifying extracted laws). LLM performance degrades predictably along each dimension, with different architectures showing different scaling behaviors. The theory predicts that no single LLM architecture will dominate across all complexity dimensions, necessitating task-specific system design. Performance degradation is often exponential rather than linear with complexity, and different complexity dimensions interact multiplicatively rather than additively.",
    "theory_statements": [
        {
            "law": {
                "law_name": "Structural Complexity Scaling Law",
                "if": [
                    {
                        "subject": "target_law",
                        "relation": "has_number_of_variables",
                        "object": "n"
                    },
                    {
                        "subject": "target_law",
                        "relation": "has_equation_order",
                        "object": "k"
                    },
                    {
                        "subject": "n",
                        "relation": "greater_than",
                        "object": "threshold_n"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_accuracy",
                        "relation": "decreases_exponentially_with",
                        "object": "n"
                    },
                    {
                        "subject": "required_context_size",
                        "relation": "increases_with",
                        "object": "n*k"
                    },
                    {
                        "subject": "symbolic_accuracy",
                        "relation": "degrades_faster_than",
                        "object": "numeric_accuracy"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "AI Feynman achieved 100% recovery on core equations (100 equations) but only 90% on harder bonus equations (20 equations) with more variables and complexity, showing 10% degradation with increased complexity",
                        "uuids": [
                            "e4500.0"
                        ]
                    },
                    {
                        "text": "LLM-SRBench showed LLM-based methods achieved only ~31.5% symbolic accuracy on complex multi-domain equations (239 problems across physics, chemistry, biology, materials), representing substantial degradation on harder problems",
                        "uuids": [
                            "e4261.0"
                        ]
                    },
                    {
                        "text": "LLM4ED achieved &gt;80% recovery for PDEs but performance varied significantly with equation complexity; alternating iterative method recovered correct equations &gt;80% of runs for PDEs",
                        "uuids": [
                            "e4493.0"
                        ]
                    },
                    {
                        "text": "ICSR struggled with higher-dimensional inputs due to prompt size and confusion limitations; limited by LLM context window (prompt token budget limits number of points and stored candidate functions)",
                        "uuids": [
                            "e4490.0"
                        ]
                    },
                    {
                        "text": "LLM-SR required far fewer iterations (approx. 2.5K iterations vs. 2M+ for baselines) but still showed performance degradation on complex problems",
                        "uuids": [
                            "e4224.0"
                        ]
                    },
                    {
                        "text": "SGA achieved loss = 1.3e-3 on imaginary constitutive-law task versus FunSearch=105.0, Eureka=89.1, OPRO=98.0, showing orders of magnitude difference in handling complex symbolic relationships",
                        "uuids": [
                            "e4499.0"
                        ]
                    },
                    {
                        "text": "HDTwinGen required iterative LLM-based modeling and evaluation agents with multiple rounds to handle complex dynamical systems",
                        "uuids": [
                            "e4494.1"
                        ]
                    },
                    {
                        "text": "BoxLM matched Automatic Statistician performance on GP tasks but required careful DSL design for complex models",
                        "uuids": [
                            "e4485.3"
                        ]
                    },
                    {
                        "text": "FunSearch required 1e6 LLM samples and extensive computational resources for complex mathematical constructions (cap sets)",
                        "uuids": [
                            "e4241.0"
                        ]
                    },
                    {
                        "text": "Neural network interpolator in AI Feynman required 100,000 samples per mystery and careful hyperparameter tuning for complex functions",
                        "uuids": [
                            "e4500.2"
                        ]
                    },
                    {
                        "text": "Llama2-7B (lower capability) yielded lower discovery accuracy and more invalid outputs compared to GPT-3.5/GPT-4, showing model capacity effects on complex problems",
                        "uuids": [
                            "e4493.3"
                        ]
                    },
                    {
                        "text": "ODEformer had lower reconstruction and generalization counts compared to LLM-guided framework on complex ODE benchmarks",
                        "uuids": [
                            "e4493.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "While complexity scaling is known in symbolic regression, this law specifically quantifies how LLM-based extraction degrades with structural complexity (100% → 90% → 31.5% as complexity increases) and identifies that degradation is exponential rather than linear. The novel insight is that symbolic accuracy degrades faster than numeric accuracy (PySR achieved competitive numeric fits with 0% symbolic accuracy), and that different LLM architectures have different scaling exponents (GPT-4 &gt; GPT-3.5 &gt; Llama2-7B).",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Schmidt & Lipson (2009) Distilling Free-Form Natural Laws from Experimental Data [complexity scaling in symbolic regression]",
                        "Udrescu & Tegmark (2020) AI Feynman 2.0: Pareto-optimal symbolic regression [complexity handling in equation discovery]",
                        "Cranmer et al. (2020) Discovering Symbolic Models from Deep Learning with Inductive Biases [complexity in symbolic learning]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Representational Modality Law",
                "if": [
                    {
                        "subject": "scientific_law",
                        "relation": "expressed_in",
                        "object": "modality"
                    },
                    {
                        "subject": "modality",
                        "relation": "is_one_of",
                        "object": "text_only, equation_only, table, figure, mixed"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_difficulty",
                        "relation": "ordered_as",
                        "object": "equation_only &lt; text_only &lt; table &lt; mixed &lt; figure"
                    },
                    {
                        "subject": "optimal_extraction_method",
                        "relation": "varies_with",
                        "object": "modality"
                    },
                    {
                        "subject": "extraction_accuracy",
                        "relation": "decreases_by",
                        "object": "20-40% when moving from text to tables/figures"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "MaterialsBERT achieved F1=0.63 for Tg and F1=0.66 for bandgap on text extraction but struggled with tables and figures, showing ~35% lower performance than text-only",
                        "uuids": [
                            "e4242.1"
                        ]
                    },
                    {
                        "text": "ChatGPT chemistry assistant required separate embedding-based filtering for text (80.14% reduction in content size) vs classification for structured content, indicating different processing needs",
                        "uuids": [
                            "e4481.1"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist used OCR preprocessing but still faced challenges with complex table structures; Claude3.5 achieved mean F1=0.90 on protein enzymes but required careful table parsing",
                        "uuids": [
                            "e4243.2"
                        ]
                    },
                    {
                        "text": "ChemDataExtractor2 required explicit specifier expressions and unit lists for targeted property extraction from text; achieved precision=57%, recall=31% on bulk modulus",
                        "uuids": [
                            "e4484.1"
                        ]
                    },
                    {
                        "text": "LLM reviewer could not inspect figures (vision disabled), limiting extraction from visual content and requiring text-only processing",
                        "uuids": [
                            "e4497.0"
                        ]
                    },
                    {
                        "text": "GPT-4 extraction pipeline (ChatExtract) outperformed ChemDataExtractor2 on both precision and recall for text-based extraction",
                        "uuids": [
                            "e4484.1"
                        ]
                    },
                    {
                        "text": "LLaMP system designed specifically for high-fidelity materials knowledge retrieval and distillation, indicating need for specialized approaches for different modalities",
                        "uuids": [
                            "e4208.2"
                        ]
                    },
                    {
                        "text": "ByteScience required separate processing for text vs tables/figures, with precision/recall/F1 in range 0.8-0.9 for structure extraction with ~300 samples",
                        "uuids": [
                            "e4197.0"
                        ]
                    },
                    {
                        "text": "LLMEVALDB pipeline required separate table filtering and extraction steps, with Llama-3.1-70B-Instruct filter for leaderboard-like tables",
                        "uuids": [
                            "e4214.0"
                        ]
                    },
                    {
                        "text": "Polak et al. GPT-4 extraction achieved higher precision/recall than rule-based methods but required verification steps for complex modalities",
                        "uuids": [
                            "e4227.2"
                        ]
                    },
                    {
                        "text": "OpenAI-Embed used for text segments but required separate processing for synthesis paragraphs vs other content types",
                        "uuids": [
                            "e4481.1"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "The ordering of extraction difficulty by modality (equation &lt; text &lt; table &lt; mixed &lt; figure) is a novel empirical finding specific to LLM-based scientific extraction. While multimodal challenges are known, the specific quantification (F1=0.63-0.66 for text, 20-40% degradation for tables/figures) and the identification that mixed modalities are harder than single modalities represent new insights. The finding that extraction accuracy decreases by 20-40% when moving from text to tables/figures is a novel quantitative relationship.",
                    "likely_classification": "new",
                    "references": [
                        "Li et al. (2023) BLIP-2: Bootstrapping Language-Image Pre-training [multimodal understanding]",
                        "Liu et al. (2023) Visual Instruction Tuning [vision-language models for scientific content]",
                        "Smock et al. (2022) PubTables-1M: Towards comprehensive table extraction from unstructured documents [table extraction challenges]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Domain Knowledge Requirement Law",
                "if": [
                    {
                        "subject": "target_domain",
                        "relation": "requires_specialized_knowledge",
                        "object": "K"
                    },
                    {
                        "subject": "K",
                        "relation": "not_in",
                        "object": "LLM_pretraining_corpus"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_accuracy",
                        "relation": "limited_by",
                        "object": "K"
                    },
                    {
                        "subject": "mitigation_strategy",
                        "relation": "requires",
                        "object": "domain_adaptation_or_external_knowledge"
                    },
                    {
                        "subject": "performance_improvement",
                        "relation": "ranges_from",
                        "object": "12-50% with domain adaptation"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "BrainGPT with neuroscience domain adaptation (LoRA fine-tuning on ~1.3B tokens) achieved 81.4% accuracy vs 63.4% for human experts, showing ~18% improvement and surpassing human performance",
                        "uuids": [
                            "e4498.1",
                            "e4498.3"
                        ]
                    },
                    {
                        "text": "BioBERT with biomedical pretraining achieved +12.24 MRR improvement over general BERT on biomedical QA, +0.62 F1 on NER, +2.80 F1 on RE",
                        "uuids": [
                            "e4171.0"
                        ]
                    },
                    {
                        "text": "PySR (no domain knowledge) achieved 0% symbolic accuracy in chemistry/biology/materials domains despite competitive numeric fits (Acc0.1 up to ~56.76), showing complete failure on symbolic understanding without domain knowledge",
                        "uuids": [
                            "e4261.6"
                        ]
                    },
                    {
                        "text": "WISE with domain-specific knowledge graph achieved 84.2% recall vs 47.4% for general ChatGPT (GPT-4o) on gene-disease associations, showing ~37% improvement",
                        "uuids": [
                            "e4220.0"
                        ]
                    },
                    {
                        "text": "Domain-adapted sentence-transformers (GPL fine-tuning) reduced outliers from 43 to 29 for arXiv and achieved 72% reduction in outliers for medarxiv",
                        "uuids": [
                            "e4191.4"
                        ]
                    },
                    {
                        "text": "WizardLM-13B-V1.2 with supervised fine-tuning on domain data improved word-overlap metrics (BLEU 19.13, ROUGE 27.35) but reduced novelty",
                        "uuids": [
                            "e4223.3"
                        ]
                    },
                    {
                        "text": "BioGPT as domain-specific LLM enabled better retrieval and summarization in biomedicine compared to general models",
                        "uuids": [
                            "e4221.5"
                        ]
                    },
                    {
                        "text": "LLM4SD with domain-specific rules synthesis enabled random-forest models to outperform state-of-the-art baselines on ADMET tasks",
                        "uuids": [
                            "e4193.0"
                        ]
                    },
                    {
                        "text": "ChemCrow as domain-specific agent achieved better performance on chemistry tasks through tool integration and domain principles",
                        "uuids": [
                            "e4186.2"
                        ]
                    },
                    {
                        "text": "Scientific LM KB completion methods showed improved performance with domain-specific pretraining on biomedical knowledge bases",
                        "uuids": [
                            "e4184.4"
                        ]
                    },
                    {
                        "text": "LLaMP system designed for high-fidelity materials knowledge retrieval required domain specialization",
                        "uuids": [
                            "e4495.1"
                        ]
                    },
                    {
                        "text": "ByteScience with domain-specific fine-tuning achieved 80-90% extraction accuracy with ~300 samples in materials science",
                        "uuids": [
                            "e4197.0"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "The requirement for domain knowledge in NLP is well-established. However, this law specifically quantifies the performance gap (81.4% vs 63.4%, +12.24 MRR, 84.2% vs 47.4%, 12-50% improvement range) and identifies that domain knowledge is particularly critical for symbolic accuracy (0% without domain knowledge) even when numeric accuracy is high. The novel insight is the dual nature of scientific law extraction: numeric vs symbolic understanding require different types of domain knowledge, and the quantification of improvement ranges (12-50%) across multiple domains.",
                    "likely_classification": "somewhat-related-to-existing",
                    "references": [
                        "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain knowledge importance]",
                        "Petroni et al. (2019) Language Models as Knowledge Bases? [knowledge encoding in LLMs]",
                        "Lee et al. (2020) BioBERT: a pre-trained biomedical language representation model [domain adaptation in biomedicine]"
                    ]
                }
            }
        },
        {
            "law": {
                "law_name": "Validation Complexity Barrier Law",
                "if": [
                    {
                        "subject": "extracted_law",
                        "relation": "requires_validation",
                        "object": "true"
                    },
                    {
                        "subject": "validation",
                        "relation": "requires_resources",
                        "object": "R"
                    },
                    {
                        "subject": "R",
                        "relation": "greater_than",
                        "object": "available_resources"
                    }
                ],
                "then": [
                    {
                        "subject": "extraction_utility",
                        "relation": "limited_by",
                        "object": "validation_bottleneck"
                    },
                    {
                        "subject": "false_positive_rate",
                        "relation": "increases_with",
                        "object": "validation_difficulty"
                    },
                    {
                        "subject": "validation_cost",
                        "relation": "exceeds_extraction_cost_by",
                        "object": "10-100x"
                    }
                ],
                "supporting_evidence": [
                    {
                        "text": "FunSearch required 1e6 LLM samples and extensive computational resources for validation via program execution, with typical deployment using ~15 samplers and ~150 CPU evaluators",
                        "uuids": [
                            "e4241.0"
                        ]
                    },
                    {
                        "text": "SGA required differentiable simulation feedback for validation (cost ~$10 per task with GPT-4), limiting applicability to domains with available simulators",
                        "uuids": [
                            "e4499.0"
                        ]
                    },
                    {
                        "text": "Enzyme Co-Scientist required expert annotations and BRENDA database for validation; aggregation agent needed to reconcile multiple LLM outputs",
                        "uuids": [
                            "e4243.2",
                            "e4243.1"
                        ]
                    },
                    {
                        "text": "LORE required ClinVar labels for validation, introducing circularity in taxonomy construction; manual curation required for 282 important lemmas",
                        "uuids": [
                            "e4211.1"
                        ]
                    },
                    {
                        "text": "AI Scientist required human reviewers for final validation despite automated review achieving 65% balanced accuracy; computational cost and monetary cost (~$10 per task) significant",
                        "uuids": [
                            "e4497.0"
                        ]
                    },
                    {
                        "text": "LLMEVALDB pipeline required human expert annotation of sampled records with Cohen's Kappa 0.68 for extraction validation",
                        "uuids": [
                            "e4214.0"
                        ]
                    },
                    {
                        "text": "ChatGPT evaluation scores required comparison with human annotators (3 biomedical evaluators over 100 examples) for validation",
                        "uuids": [
                            "e4223.1"
                        ]
                    },
                    {
                        "text": "ByteScience required human-in-the-loop review and correction of auto-labelling; iterative retraining when accuracy/recall insufficient",
                        "uuids": [
                            "e4197.0"
                        ]
                    },
                    {
                        "text": "LLM-based Multi-Agent framework required GPT-4 and human scoring (0-3) across novelty/relevance/significance/verifiability for validation",
                        "uuids": [
                            "e4192.0"
                        ]
                    },
                    {
                        "text": "AI Feynman required algebraic simplification against ground truth for validation; neural-network validation errors between 1e-3 and 1e-5 times f_rms",
                        "uuids": [
                            "e4500.0"
                        ]
                    },
                    {
                        "text": "HDTwinGen required execution, parameter fitting, and evaluation on validation/test splits for each generated model",
                        "uuids": [
                            "e4494.1"
                        ]
                    },
                    {
                        "text": "ChemReasoner required quantum-chemical simulations (adsorption energies, reaction energy barriers) for validation feedback",
                        "uuids": [
                            "e4195.1"
                        ]
                    },
                    {
                        "text": "CLAIM-BENCH highlighted gap in validation: prior systems focus on generation but not on explicit claim↔evidence linkage",
                        "uuids": [
                            "e4176.2"
                        ]
                    }
                ],
                "qual_or_quant": "quantitative",
                "existing_law": {
                    "classification_explanation": "Validation bottlenecks are known in scientific research, but this law specifically identifies that for LLM-extracted laws, validation complexity often exceeds extraction complexity by 10-100x (1e6 samples for validation vs much fewer for extraction, ~$10 per task validation cost). The novel insight is that automated extraction without automated validation creates a new bottleneck, and that validation requirements scale superlinearly with extraction capability. The quantification of the 10-100x cost multiplier is a new empirical finding.",
                    "likely_classification": "new",
                    "references": [
                        "Ioannidis (2005) Why Most Published Research Findings Are False [validation challenges]",
                        "Baker (2016) 1,500 scientists lift the lid on reproducibility [reproducibility crisis]",
                        "Munafò et al. (2017) A manifesto for reproducible science [validation and reproducibility]"
                    ]
                }
            }
        }
    ],
    "new_predictions_likely": [
        "For equations with &gt;10 variables, hybrid approaches combining LLM structure proposal with traditional symbolic regression will outperform pure LLM approaches by &gt;30% in symbolic accuracy, based on the observed 100% → 31.5% degradation pattern.",
        "Extraction accuracy from tables will improve by 40-60% when using specialized table-parsing models (e.g., TableFormer) before LLM processing, compared to direct LLM extraction from OCR text, based on the observed F1=0.63-0.66 for text vs lower for tables.",
        "For domains requiring specialized knowledge not in pretraining corpora (e.g., cutting-edge quantum computing), domain adaptation will provide &gt;50% improvement in extraction accuracy compared to general LLMs, based on observed improvements of 12-50% in other domains.",
        "The cost of validation will exceed the cost of extraction by 10-100x for complex scientific laws, making automated validation the primary bottleneck for scaling LLM-based discovery, based on observed 1e6 samples for validation and ~$10 per task costs.",
        "Multi-modal extraction systems that process text, equations, tables, and figures separately before integration will achieve 25-35% higher accuracy than systems that process all modalities uniformly, based on observed modality-specific performance differences.",
        "For equations with 5-10 variables, LLM-based methods will achieve 50-70% symbolic accuracy (interpolating between 100% for simple equations and 31.5% for complex ones), with numeric accuracy remaining 20-30% higher than symbolic accuracy."
    ],
    "new_predictions_unknown": [
        "Multimodal LLMs trained on scientific figures and equations might achieve &gt;90% extraction accuracy across all modalities, potentially eliminating the representational modality hierarchy, but current vision-language models show limited mathematical reasoning and the training data requirements are unclear.",
        "Self-supervised validation using consistency checks across multiple papers might reduce validation costs by 90%, but the reliability of such validation for novel discoveries is unclear, especially for laws that appear in only a few papers.",
        "Scaling LLMs to 10T+ parameters might internalize sufficient domain knowledge to eliminate the need for domain adaptation, but this may require prohibitive computational resources (&gt;$100M training cost) and training data that may not exist for specialized domains.",
        "Automated theorem provers integrated with LLMs might enable fully automated validation for mathematical domains, but the brittleness of current provers may limit practical applicability to simple cases, and the integration architecture is unclear.",
        "Adversarial training on deliberately incorrect laws might improve LLMs' ability to distinguish valid from invalid extractions, potentially reducing false positive rates by 50-80%, but the training data generation and generalization properties are unknown.",
        "Hierarchical extraction systems that first extract simple laws and then use them as building blocks for complex laws might achieve &gt;80% accuracy on complex equations, but the optimal hierarchy structure and composition rules are unclear."
    ],
    "negative_experiments": [
        "If extraction accuracy does not degrade with increasing number of variables (remains &gt;90% for n&gt;20), this would challenge the Structural Complexity Scaling Law and suggest that current degradation is due to insufficient model capacity rather than fundamental complexity.",
        "If multimodal LLMs achieve equal performance across all modalities (text, equations, tables, figures) with &lt;10% variance, this would challenge the Representational Modality Law and suggest that modality differences are artifacts of current architectures.",
        "If general LLMs without domain adaptation match domain-adapted models on specialized tasks (within 5% accuracy), this would challenge the Domain Knowledge Requirement Law and suggest that pretraining corpora already contain sufficient domain knowledge.",
        "If automated validation achieves &gt;95% agreement with human expert validation at &lt;10% of the cost, this would challenge the Validation Complexity Barrier Law and suggest that validation can be efficiently automated.",
        "If symbolic accuracy does not degrade faster than numeric accuracy (both degrade at similar rates), this would challenge the dual-nature insight of the Structural Complexity Scaling Law.",
        "If validation costs do not exceed extraction costs by 10-100x (ratio &lt;5x), this would challenge the Validation Complexity Barrier Law and suggest that validation can be made efficient."
    ],
    "unaccounted_for": [
        {
            "text": "The interaction effects between different complexity dimensions (e.g., high structural complexity + mixed modality + specialized domain) are not well-characterized. Evidence suggests multiplicative rather than additive effects, but no systematic study exists.",
            "uuids": []
        },
        {
            "text": "The role of paper quality and writing style in extraction difficulty is underexplored, with most studies assuming well-written papers. Evidence from OCR preprocessing suggests quality matters, but no quantification exists.",
            "uuids": [
                "e4243.2",
                "e4481.1"
            ]
        },
        {
            "text": "The impact of temporal factors (older vs newer papers, evolving notation) on extraction accuracy is not systematically studied. SEMNET shows temporal evolution of concepts, but extraction accuracy over time is not measured.",
            "uuids": [
                "e4488.1"
            ]
        },
        {
            "text": "The trade-offs between extraction breadth (covering many papers) and depth (extracting complex laws from few papers) lack systematic analysis. WISE achieved 84.2% recall but required recursive exploration.",
            "uuids": [
                "e4220.0"
            ]
        },
        {
            "text": "The generalization of extracted laws to new contexts (transfer learning for scientific laws) is largely unexplored. LLM-SRBench showed poor OOD generalization, but systematic study is lacking.",
            "uuids": [
                "e4261.0"
            ]
        },
        {
            "text": "The optimal balance between automated extraction and human-in-the-loop validation is not well-characterized. ByteScience achieved 80-90% accuracy with human-in-the-loop, but optimal intervention points are unclear.",
            "uuids": [
                "e4197.0"
            ]
        },
        {
            "text": "The impact of extraction errors on downstream scientific discovery is not studied. If extracted laws are used to generate hypotheses, error propagation could be significant.",
            "uuids": []
        },
        {
            "text": "The role of uncertainty quantification in extraction is underexplored. Most systems provide point estimates without confidence intervals or uncertainty measures.",
            "uuids": []
        },
        {
            "text": "The scalability limits of different extraction approaches are not well-characterized. LLMEVALDB processed 162,656 publications, but cost and time scaling laws are not established.",
            "uuids": [
                "e4214.0"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "LLM-based methods achieved only 31.5% symbolic accuracy on LLM-SRBench (239 problems), but AI Feynman achieved 100% on Feynman equations (100 problems), suggesting complexity scaling may be domain-dependent rather than universal. The difference may be due to problem selection bias or domain-specific heuristics in AI Feynman.",
            "uuids": [
                "e4261.0",
                "e4500.0"
            ]
        },
        {
            "text": "PySR achieved competitive numeric accuracy (Acc0.1 up to ~56.76) despite 0% symbolic accuracy, suggesting that numeric and symbolic complexity may scale differently and independently. This challenges the assumption that they are correlated.",
            "uuids": [
                "e4261.6"
            ]
        },
        {
            "text": "Some studies show domain adaptation improves accuracy (BioBERT +12.24 MRR, BrainGPT 81.4% vs 63.4%) while others show it reduces novelty (WizardLM-13B-V1.2 SFT reduced novelty), creating tension about the net benefit of domain knowledge. The trade-off between accuracy and novelty is not well-understood.",
            "uuids": [
                "e4223.3",
                "e4171.0",
                "e4498.1"
            ]
        },
        {
            "text": "GPT-4 achieved 81.4% accuracy on neuroscience predictions (surpassing human experts at 63.4%), but only 65% balanced accuracy on paper review, suggesting that prediction tasks may be easier than evaluation tasks despite both requiring domain knowledge.",
            "uuids": [
                "e4498.1",
                "e4497.0"
            ]
        },
        {
            "text": "WISE achieved 84.2% recall with recursive exploration and knowledge graph, while simple ChatGPT achieved 47.4%, but ChatGPT with Search achieved only 36.8% (worse than pure ChatGPT), suggesting that retrieval augmentation can sometimes hurt performance.",
            "uuids": [
                "e4220.0",
                "e4220.2"
            ]
        },
        {
            "text": "FunSearch required 1e6 samples for validation, but AI Feynman validated with algebraic simplification (much cheaper), suggesting validation complexity may depend more on domain than on extraction complexity.",
            "uuids": [
                "e4241.0",
                "e4500.0"
            ]
        }
    ],
    "special_cases": [
        "For purely mathematical domains with formal verification capabilities (e.g., theorem proving), symbolic methods may show better scaling than LLMs due to formal verification capabilities, as suggested by FunSearch's program-search approach achieving verifiable mathematical constructions.",
        "For domains with extensive structured databases (e.g., crystallography with CIF files, enzyme kinetics with BRENDA), database-driven extraction may outperform LLM-based approaches, as suggested by Enzyme Co-Scientist's use of BRENDA for validation.",
        "For highly visual domains (e.g., materials microstructure, astronomical images), vision-language models may be necessary regardless of text extraction capabilities, as suggested by LLM reviewer's inability to inspect figures limiting extraction.",
        "For domains with rapid notation evolution (e.g., machine learning, quantum computing), temporal adaptation may be more important than domain adaptation, as suggested by SEMNET's temporal network showing concept evolution.",
        "For interdisciplinary domains (e.g., biophysics, computational chemistry), multi-domain adaptation may be necessary, complicating the Domain Knowledge Requirement Law, as suggested by AI4Science analysis showing 72.7% AI for Science articles.",
        "For domains with very few papers (e.g., emerging research areas), extraction may be limited by data scarcity rather than model capability, as suggested by WISE's need for recursive exploration to achieve 84.2% recall.",
        "For domains where validation requires expensive experiments (e.g., drug discovery, materials synthesis), validation complexity may be 1000x extraction complexity rather than 10-100x, as suggested by LIDDiA's need for experimental validation.",
        "For domains with well-established symbolic regression benchmarks (e.g., Feynman equations), memorization may inflate apparent performance, as suggested by LLM-SRBench's design to avoid trivial memorization.",
        "For domains where laws are expressed primarily in natural language rather than equations (e.g., ecology, social science), the Representational Modality Law may not apply, as suggested by DeepResearchEco's focus on natural language synthesis."
    ],
    "existing_theory": {
        "classification_explanation": "This theory provides a novel framework for understanding how extraction difficulty scales across multiple dimensions (structural, representational, contextual, validation) with specific quantifications: 100% → 90% → 31.5% for structural complexity, equation &lt; text &lt; table &lt; mixed &lt; figure for modality (20-40% degradation), 12-50% improvement with domain adaptation, and 10-100x validation cost multiplier. While individual scaling behaviors are known in specific contexts (e.g., symbolic regression complexity, domain adaptation benefits), the unified multi-dimensional framework with quantified scaling laws and the identification of multiplicative interactions between dimensions represent new contributions. The theory is new in its synthesis and quantification but builds on existing complexity theory and domain adaptation literature.",
        "likely_classification": "new",
        "references": [
            "Udrescu & Tegmark (2020) AI Feynman 2.0: Pareto-optimal symbolic regression [complexity in equation discovery]",
            "Cranmer et al. (2020) Discovering Symbolic Models from Deep Learning with Inductive Biases [complexity in symbolic learning]",
            "Kaplan et al. (2020) Scaling Laws for Neural Language Models [scaling behaviors in LLMs]",
            "Gururangan et al. (2020) Don't Stop Pretraining: Adapt Language Models to Domains and Tasks [domain adaptation]",
            "Bommasani et al. (2021) On the Opportunities and Risks of Foundation Models [foundation model capabilities and limitations]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 1,
    "version": "built-theory-from-results-single-theory-reflection2-nov13-2025",
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
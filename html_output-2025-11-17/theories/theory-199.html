<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hierarchical Language-to-Action Decomposition Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-199</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-199</p>
                <p><strong>Name:</strong> Hierarchical Language-to-Action Decomposition Theory</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about when and how pretraining on text worlds transfers to 3D embodied tasks, including mappings between high-level action semantics and low-level perception and predicted sample complexity gains, based on the following results.</p>
                <p><strong>Description:</strong> For complex, long-horizon embodied tasks, hierarchical decomposition using language at multiple levels (goals→subgoals→skills→primitives) is more effective than direct end-to-end mapping from language to low-level actions. Each level of the hierarchy can leverage different types of pretrained knowledge: high-level planning from language models, mid-level skill composition from instruction-following data, and low-level control from sensorimotor experience. The effectiveness depends critically on: (1) the availability and quality of intermediate representations, (2) the ability to learn or design mappings between adjacent levels, (3) whether intermediate supervision is available, and (4) the relative difficulty of perception vs. planning vs. control in the target domain. Hierarchical approaches show particular advantages for compositional generalization and interpretability, but may be bottlenecked by the weakest level in the hierarchy.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>Hierarchical decomposition reduces the complexity of learning by breaking down the language-to-action mapping into multiple simpler mappings, each of which can be learned or designed more effectively</li>
                <li>Each level of the hierarchy can leverage different types of pretrained knowledge optimally suited to that abstraction level (e.g., LLMs for planning, VLMs for grounding, RL for control)</li>
                <li>Explicit intermediate representations (skills, subgoals, landmarks, code) enable better interpretability, debugging, and compositional reuse compared to end-to-end learned representations</li>
                <li>The effectiveness of hierarchical approaches depends critically on the quality of decomposition and the learnability of inter-level mappings - a weak link at any level can bottleneck overall performance</li>
                <li>Hierarchical decomposition is most beneficial for long-horizon tasks (>10 steps) where end-to-end learning struggles with credit assignment and exploration</li>
                <li>The optimal number of hierarchy levels depends on task complexity, with typical effective hierarchies having 2-4 levels (e.g., goal→subgoal→skill→primitive)</li>
                <li>Hierarchical approaches show superior compositional generalization when intermediate representations correspond to reusable, semantically meaningful units</li>
                <li>The benefits of hierarchy are most pronounced when intermediate supervision or pretrained representations are available at multiple levels</li>
                <li>Perception bottlenecks can limit hierarchical planning benefits - even perfect high-level plans fail if low-level execution or perception is unreliable</li>
                <li>Hand-designed hierarchies can outperform learned hierarchies when domain knowledge is available and task structure is well-understood, but learned hierarchies scale better to novel domains</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>IGOR using Flan-T5 for subtask generation + goal-conditioned RL achieved 60% success on Crafter vs 36.4% for end-to-end Dynalang baseline, demonstrating hierarchical decomposition benefits <a href="../results/extraction-result-1728.html#e1728.0" class="evidence-link">[e1728.0]</a> </li>
    <li>OPEx using GPT-4 for planning + skill library + deterministic policy achieved +17.74% SR improvement on ALFRED over end-to-end baselines, showing benefits of explicit skill decomposition <a href="../results/extraction-result-1696.html#e1696.0" class="evidence-link">[e1696.0]</a> </li>
    <li>FILM using template-based language processing + semantic mapping + deterministic policy achieved 20.10% SR on ALFRED Valid Unseen, with modular design enabling interpretability <a href="../results/extraction-result-1792.html#e1792.0" class="evidence-link">[e1792.0]</a> </li>
    <li>EmbodiedGPT using chain-of-thought planning + embodied-former + policy MLP achieved 50.8-81.2% success with 10-25 demos, showing hierarchical CoT benefits <a href="../results/extraction-result-1856.html#e1856.0" class="evidence-link">[e1856.0]</a> </li>
    <li>LM-Nav using GPT-3 for landmark extraction + CLIP grounding + VNM execution achieved 0.8 net success on outdoor navigation, demonstrating three-level hierarchy (language→landmarks→navigation) <a href="../results/extraction-result-1852.html#e1852.0" class="evidence-link">[e1852.0]</a> </li>
    <li>UniPi using text-conditioned video generation + inverse dynamics achieved 77.1% success by separating planning (video generation) from control (inverse dynamics) <a href="../results/extraction-result-1855.html#e1855.0" class="evidence-link">[e1855.0]</a> </li>
    <li>Voyager using LLM for code generation + skill library + execution achieved open-ended exploration in Minecraft through hierarchical skill composition <a href="../results/extraction-result-1853.html#e1853.0" class="evidence-link">[e1853.0]</a> </li>
    <li>PREVALENT using hierarchical pretraining (image-attended MLM + Action Prediction) improved VLN performance, showing benefits of hierarchical objectives during pretraining <a href="../results/extraction-result-1857.html#e1857.0" class="evidence-link">[e1857.0]</a> </li>
    <li>VLN⇄BERT using recurrent state token as intermediate representation between language and visual observations improved navigation performance <a href="../results/extraction-result-1854.html#e1854.0" class="evidence-link">[e1854.0]</a> </li>
    <li>EMMA using hierarchical multitask VLP objectives (MLM, ITM, captioning, grounding) achieved 36.81% MSR vs 5.2% from scratch, showing benefits of hierarchical pretraining <a href="../results/extraction-result-1697.html#e1697.0" class="evidence-link">[e1697.0]</a> </li>
    <li>MineCLIP using learned reward as intermediate signal between language goals and low-level control achieved competitive performance with hand-engineered rewards <a href="../results/extraction-result-1851.html#e1851.0" class="evidence-link">[e1851.0]</a> </li>
    <li>BC-Z using frozen sentence embeddings as intermediate task representation enabled zero-shot generalization to 24 held-out tasks <a href="../results/extraction-result-1772.html#e1772.0" class="evidence-link">[e1772.0]</a> </li>
    <li>LID using GPT-2 for high-level goal representation + learned policy achieved strong performance on VirtualHome tasks <a href="../results/extraction-result-1827.html#e1827.0" class="evidence-link">[e1827.0]</a> </li>
    <li>Code-as-Policies using LLM-generated code as intermediate representation between language and robot API calls demonstrated hierarchical benefits <a href="../results/extraction-result-1847.html#e1847.0" class="evidence-link">[e1847.0]</a> </li>
    <li>OPEx-L using ALFWorld-derived world knowledge as intermediate representation improved planning and reduced repetitive errors <a href="../results/extraction-result-1696.html#e1696.1" class="evidence-link">[e1696.1]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Providing intermediate supervision at multiple hierarchy levels (e.g., subgoal labels + skill labels + action labels) should improve learning compared to end-to-end training with only final task success</li>
                <li>Hierarchical approaches should show better compositional generalization (novel combinations of known skills) than flat approaches when tested on tasks requiring recombination of training components</li>
                <li>Adding a learned intermediate representation layer (e.g., learned skill embeddings) between language and actions should improve sample efficiency compared to direct language-to-action mapping</li>
                <li>Hierarchical approaches should be more robust to low-level execution errors through replanning at higher levels, showing graceful degradation rather than catastrophic failure</li>
                <li>Pre-training each level of the hierarchy separately (e.g., LLM for planning, VLM for grounding, RL for control) should outperform joint end-to-end training from scratch on complex tasks</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether there exists an optimal hierarchy structure that generalizes across different task domains, or whether optimal hierarchies are fundamentally domain-specific</li>
                <li>Whether hierarchical decomposition can be learned end-to-end from task demonstrations without explicit intermediate labels, and if so, whether learned hierarchies match hand-designed ones</li>
                <li>Whether the benefits of hierarchical decomposition scale linearly with task horizon or plateau at some complexity level (e.g., beyond 50-step tasks)</li>
                <li>Whether hierarchical approaches can match the sample efficiency of end-to-end learning when both have access to unlimited data and compute</li>
                <li>Whether neural-symbolic hybrid hierarchies (combining learned and programmatic components) outperform purely learned or purely programmatic hierarchies</li>
                <li>Whether hierarchical decomposition provides benefits for short-horizon tasks (<5 steps) or if the overhead outweighs the benefits</li>
                <li>Whether the optimal granularity of intermediate representations (coarse vs. fine-grained skills) depends on the amount of available training data</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding tasks where end-to-end learning consistently outperforms hierarchical approaches would challenge the universality of hierarchical benefits and suggest boundary conditions</li>
                <li>Demonstrating that random hierarchical decompositions perform as well as semantically-meaningful decompositions would challenge the importance of semantic structure in hierarchies</li>
                <li>Showing that hierarchical approaches do not improve compositional generalization compared to end-to-end learning would challenge a key claimed benefit</li>
                <li>Finding that hierarchical approaches are more sample-inefficient than end-to-end learning on simple tasks would establish important boundary conditions</li>
                <li>Demonstrating that removing intermediate representations from a hierarchical system does not hurt performance would challenge the necessity of explicit hierarchies</li>
                <li>Showing that hierarchical approaches are equally bottlenecked by perception as flat approaches would challenge claims about robustness benefits</li>
                <li>Finding that learned hierarchies consistently underperform hand-designed hierarchies even with large amounts of data would challenge the scalability argument</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>How to automatically discover optimal hierarchical structures for new task domains without domain expertise or extensive trial-and-error </li>
    <li>The precise trade-offs between hierarchy depth, interpretability, and performance across different task types and domains </li>
    <li>How hierarchical decomposition interacts with different types of language pretraining (e.g., instruction-tuning vs. general pretraining) </li>
    <li>The computational overhead of hierarchical approaches and when this overhead outweighs the benefits </li>
    <li>How to handle cases where natural hierarchical decomposition is unclear or task structure is flat </li>
    <li>The role of hierarchy in enabling transfer learning across related tasks or domains </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Sutton et al. (1999) Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning [Options framework - foundational work on hierarchical RL]</li>
    <li>Dietterich (2000) Hierarchical reinforcement learning with the MAXQ value function decomposition [MAXQ framework - value function decomposition for hierarchies]</li>
    <li>Parr & Russell (1998) Reinforcement learning with hierarchies of machines [HAM framework - hierarchical abstract machines]</li>
    <li>Bacon et al. (2017) The option-critic architecture [Learning options end-to-end without explicit subgoal supervision]</li>
    <li>Andreas et al. (2017) Modular multitask reinforcement learning with policy sketches [Language-based hierarchical RL with policy sketches]</li>
    <li>Jiang et al. (2019) Language as an abstraction for hierarchical deep reinforcement learning [Using language as intermediate representation in hierarchical RL]</li>
    <li>Tellex et al. (2011) Understanding natural language commands for robotic navigation and mobile manipulation [Hierarchical semantic parsing for robotics]</li>
    <li>Kaelbling & Lozano-Pérez (2011) Hierarchical task and motion planning in the now [TAMP - hierarchical planning combining task and motion]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Hierarchical Language-to-Action Decomposition Theory",
    "theory_description": "For complex, long-horizon embodied tasks, hierarchical decomposition using language at multiple levels (goals→subgoals→skills→primitives) is more effective than direct end-to-end mapping from language to low-level actions. Each level of the hierarchy can leverage different types of pretrained knowledge: high-level planning from language models, mid-level skill composition from instruction-following data, and low-level control from sensorimotor experience. The effectiveness depends critically on: (1) the availability and quality of intermediate representations, (2) the ability to learn or design mappings between adjacent levels, (3) whether intermediate supervision is available, and (4) the relative difficulty of perception vs. planning vs. control in the target domain. Hierarchical approaches show particular advantages for compositional generalization and interpretability, but may be bottlenecked by the weakest level in the hierarchy.",
    "supporting_evidence": [
        {
            "text": "IGOR using Flan-T5 for subtask generation + goal-conditioned RL achieved 60% success on Crafter vs 36.4% for end-to-end Dynalang baseline, demonstrating hierarchical decomposition benefits",
            "uuids": [
                "e1728.0"
            ]
        },
        {
            "text": "OPEx using GPT-4 for planning + skill library + deterministic policy achieved +17.74% SR improvement on ALFRED over end-to-end baselines, showing benefits of explicit skill decomposition",
            "uuids": [
                "e1696.0"
            ]
        },
        {
            "text": "FILM using template-based language processing + semantic mapping + deterministic policy achieved 20.10% SR on ALFRED Valid Unseen, with modular design enabling interpretability",
            "uuids": [
                "e1792.0"
            ]
        },
        {
            "text": "EmbodiedGPT using chain-of-thought planning + embodied-former + policy MLP achieved 50.8-81.2% success with 10-25 demos, showing hierarchical CoT benefits",
            "uuids": [
                "e1856.0"
            ]
        },
        {
            "text": "LM-Nav using GPT-3 for landmark extraction + CLIP grounding + VNM execution achieved 0.8 net success on outdoor navigation, demonstrating three-level hierarchy (language→landmarks→navigation)",
            "uuids": [
                "e1852.0"
            ]
        },
        {
            "text": "UniPi using text-conditioned video generation + inverse dynamics achieved 77.1% success by separating planning (video generation) from control (inverse dynamics)",
            "uuids": [
                "e1855.0"
            ]
        },
        {
            "text": "Voyager using LLM for code generation + skill library + execution achieved open-ended exploration in Minecraft through hierarchical skill composition",
            "uuids": [
                "e1853.0"
            ]
        },
        {
            "text": "PREVALENT using hierarchical pretraining (image-attended MLM + Action Prediction) improved VLN performance, showing benefits of hierarchical objectives during pretraining",
            "uuids": [
                "e1857.0"
            ]
        },
        {
            "text": "VLN⇄BERT using recurrent state token as intermediate representation between language and visual observations improved navigation performance",
            "uuids": [
                "e1854.0"
            ]
        },
        {
            "text": "EMMA using hierarchical multitask VLP objectives (MLM, ITM, captioning, grounding) achieved 36.81% MSR vs 5.2% from scratch, showing benefits of hierarchical pretraining",
            "uuids": [
                "e1697.0"
            ]
        },
        {
            "text": "MineCLIP using learned reward as intermediate signal between language goals and low-level control achieved competitive performance with hand-engineered rewards",
            "uuids": [
                "e1851.0"
            ]
        },
        {
            "text": "BC-Z using frozen sentence embeddings as intermediate task representation enabled zero-shot generalization to 24 held-out tasks",
            "uuids": [
                "e1772.0"
            ]
        },
        {
            "text": "LID using GPT-2 for high-level goal representation + learned policy achieved strong performance on VirtualHome tasks",
            "uuids": [
                "e1827.0"
            ]
        },
        {
            "text": "Code-as-Policies using LLM-generated code as intermediate representation between language and robot API calls demonstrated hierarchical benefits",
            "uuids": [
                "e1847.0"
            ]
        },
        {
            "text": "OPEx-L using ALFWorld-derived world knowledge as intermediate representation improved planning and reduced repetitive errors",
            "uuids": [
                "e1696.1"
            ]
        }
    ],
    "theory_statements": [
        "Hierarchical decomposition reduces the complexity of learning by breaking down the language-to-action mapping into multiple simpler mappings, each of which can be learned or designed more effectively",
        "Each level of the hierarchy can leverage different types of pretrained knowledge optimally suited to that abstraction level (e.g., LLMs for planning, VLMs for grounding, RL for control)",
        "Explicit intermediate representations (skills, subgoals, landmarks, code) enable better interpretability, debugging, and compositional reuse compared to end-to-end learned representations",
        "The effectiveness of hierarchical approaches depends critically on the quality of decomposition and the learnability of inter-level mappings - a weak link at any level can bottleneck overall performance",
        "Hierarchical decomposition is most beneficial for long-horizon tasks (&gt;10 steps) where end-to-end learning struggles with credit assignment and exploration",
        "The optimal number of hierarchy levels depends on task complexity, with typical effective hierarchies having 2-4 levels (e.g., goal→subgoal→skill→primitive)",
        "Hierarchical approaches show superior compositional generalization when intermediate representations correspond to reusable, semantically meaningful units",
        "The benefits of hierarchy are most pronounced when intermediate supervision or pretrained representations are available at multiple levels",
        "Perception bottlenecks can limit hierarchical planning benefits - even perfect high-level plans fail if low-level execution or perception is unreliable",
        "Hand-designed hierarchies can outperform learned hierarchies when domain knowledge is available and task structure is well-understood, but learned hierarchies scale better to novel domains"
    ],
    "new_predictions_likely": [
        "Providing intermediate supervision at multiple hierarchy levels (e.g., subgoal labels + skill labels + action labels) should improve learning compared to end-to-end training with only final task success",
        "Hierarchical approaches should show better compositional generalization (novel combinations of known skills) than flat approaches when tested on tasks requiring recombination of training components",
        "Adding a learned intermediate representation layer (e.g., learned skill embeddings) between language and actions should improve sample efficiency compared to direct language-to-action mapping",
        "Hierarchical approaches should be more robust to low-level execution errors through replanning at higher levels, showing graceful degradation rather than catastrophic failure",
        "Pre-training each level of the hierarchy separately (e.g., LLM for planning, VLM for grounding, RL for control) should outperform joint end-to-end training from scratch on complex tasks"
    ],
    "new_predictions_unknown": [
        "Whether there exists an optimal hierarchy structure that generalizes across different task domains, or whether optimal hierarchies are fundamentally domain-specific",
        "Whether hierarchical decomposition can be learned end-to-end from task demonstrations without explicit intermediate labels, and if so, whether learned hierarchies match hand-designed ones",
        "Whether the benefits of hierarchical decomposition scale linearly with task horizon or plateau at some complexity level (e.g., beyond 50-step tasks)",
        "Whether hierarchical approaches can match the sample efficiency of end-to-end learning when both have access to unlimited data and compute",
        "Whether neural-symbolic hybrid hierarchies (combining learned and programmatic components) outperform purely learned or purely programmatic hierarchies",
        "Whether hierarchical decomposition provides benefits for short-horizon tasks (&lt;5 steps) or if the overhead outweighs the benefits",
        "Whether the optimal granularity of intermediate representations (coarse vs. fine-grained skills) depends on the amount of available training data"
    ],
    "negative_experiments": [
        "Finding tasks where end-to-end learning consistently outperforms hierarchical approaches would challenge the universality of hierarchical benefits and suggest boundary conditions",
        "Demonstrating that random hierarchical decompositions perform as well as semantically-meaningful decompositions would challenge the importance of semantic structure in hierarchies",
        "Showing that hierarchical approaches do not improve compositional generalization compared to end-to-end learning would challenge a key claimed benefit",
        "Finding that hierarchical approaches are more sample-inefficient than end-to-end learning on simple tasks would establish important boundary conditions",
        "Demonstrating that removing intermediate representations from a hierarchical system does not hurt performance would challenge the necessity of explicit hierarchies",
        "Showing that hierarchical approaches are equally bottlenecked by perception as flat approaches would challenge claims about robustness benefits",
        "Finding that learned hierarchies consistently underperform hand-designed hierarchies even with large amounts of data would challenge the scalability argument"
    ],
    "unaccounted_for": [
        {
            "text": "How to automatically discover optimal hierarchical structures for new task domains without domain expertise or extensive trial-and-error",
            "uuids": []
        },
        {
            "text": "The precise trade-offs between hierarchy depth, interpretability, and performance across different task types and domains",
            "uuids": []
        },
        {
            "text": "How hierarchical decomposition interacts with different types of language pretraining (e.g., instruction-tuning vs. general pretraining)",
            "uuids": []
        },
        {
            "text": "The computational overhead of hierarchical approaches and when this overhead outweighs the benefits",
            "uuids": []
        },
        {
            "text": "How to handle cases where natural hierarchical decomposition is unclear or task structure is flat",
            "uuids": []
        },
        {
            "text": "The role of hierarchy in enabling transfer learning across related tasks or domains",
            "uuids": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "RT-2 using end-to-end VLA achieved strong performance (~2x improvement over baselines) without explicit hierarchical decomposition, suggesting flat approaches can work with sufficient scale and pretraining",
            "uuids": [
                "e1843.0"
            ]
        },
        {
            "text": "Some hierarchical approaches (FILM, OPEx) still struggled with perception and low-level execution despite good high-level planning, suggesting hierarchy doesn't solve all bottlenecks",
            "uuids": [
                "e1792.0",
                "e1696.0"
            ]
        },
        {
            "text": "SIMA using relatively flat architecture (pretrained encoders + transformer + action head) achieved substantial improvements across many environments, suggesting explicit hierarchy may not always be necessary",
            "uuids": [
                "e1721.0"
            ]
        },
        {
            "text": "PREVALENT using relatively flat cross-modal transformer (without explicit skill decomposition) achieved strong VLN performance, suggesting implicit hierarchies in transformers may suffice",
            "uuids": [
                "e1857.0"
            ]
        },
        {
            "text": "Some end-to-end approaches (e.g., EMMA with DAgger-DPO) achieved 71-94% success on ALFWorld without explicit hierarchical decomposition",
            "uuids": [
                "e1709.0"
            ]
        }
    ],
    "special_cases": [
        "Hierarchical decomposition is most beneficial for long-horizon tasks (&gt;10 steps) with clear subgoal structure and compositional requirements",
        "For short-horizon tasks (&lt;5 steps) or tasks with simple action spaces, end-to-end learning may be more efficient due to lower overhead",
        "Hierarchical approaches require careful design of intermediate representations and inter-level interfaces - poor interface design can hurt rather than help",
        "The benefits of hierarchy may diminish with very large models that can learn complex mappings end-to-end, especially when unlimited data is available",
        "When perception is the primary bottleneck (rather than planning or control), hierarchical planning provides limited benefits",
        "Hand-designed hierarchies work best when domain structure is well-understood and stable; learned hierarchies are better for novel or evolving domains",
        "Hierarchical approaches are most effective when intermediate supervision or pretrained representations are available at multiple levels",
        "The optimal hierarchy structure depends on action space complexity - continuous control may benefit from different hierarchies than discrete actions",
        "Hierarchical decomposition is less beneficial when tasks lack clear compositional structure or when subtasks are highly interdependent",
        "The computational overhead of hierarchical approaches (e.g., multiple model calls, planning time) may outweigh benefits for time-critical applications"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Sutton et al. (1999) Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning [Options framework - foundational work on hierarchical RL]",
            "Dietterich (2000) Hierarchical reinforcement learning with the MAXQ value function decomposition [MAXQ framework - value function decomposition for hierarchies]",
            "Parr & Russell (1998) Reinforcement learning with hierarchies of machines [HAM framework - hierarchical abstract machines]",
            "Bacon et al. (2017) The option-critic architecture [Learning options end-to-end without explicit subgoal supervision]",
            "Andreas et al. (2017) Modular multitask reinforcement learning with policy sketches [Language-based hierarchical RL with policy sketches]",
            "Jiang et al. (2019) Language as an abstraction for hierarchical deep reinforcement learning [Using language as intermediate representation in hierarchical RL]",
            "Tellex et al. (2011) Understanding natural language commands for robotic navigation and mobile manipulation [Hierarchical semantic parsing for robotics]",
            "Kaelbling & Lozano-Pérez (2011) Hierarchical task and motion planning in the now [TAMP - hierarchical planning combining task and motion]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 5,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Equivalence Clustering Theory for Uncertainty Quantification - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-241</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-241</p>
                <p><strong>Name:</strong> Semantic Equivalence Clustering Theory for Uncertainty Quantification</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory about variability and reproducibility in language model-driven scientific experimentation.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory proposes that uncertainty in language model outputs for scientific tasks can be quantified by clustering semantically equivalent responses and analyzing the resulting cluster distribution. When an LLM is sampled multiple times for the same scientific query, the outputs can be grouped into semantic equivalence classes (clusters of responses with similar meaning). The distribution of outputs across these clusters provides a principled measure of model uncertainty: high concentration in a single cluster indicates low uncertainty and high confidence, while distribution across multiple clusters indicates high uncertainty. Within-cluster variance captures aleatoric (irreducible) uncertainty due to valid paraphrasing, while between-cluster variance captures epistemic (knowledge) uncertainty. The entropy of the cluster distribution H(C) = -Σ p(c_i) log p(c_i) provides a scalar uncertainty measure, where p(c_i) is the proportion of outputs in cluster i. This approach enables uncertainty-aware scientific conclusions by identifying when model outputs converge on a single semantic answer versus when they reflect genuine model uncertainty about the correct answer.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>For a scientific query Q, sampling an LLM N times produces outputs O = {o₁, o₂, ..., oₙ} that can be partitioned into K semantic equivalence clusters C = {c₁, c₂, ..., cₖ} based on semantic similarity.</li>
                <li>The cluster distribution entropy H(C) = -Σᵢ₌₁ᴷ p(cᵢ) log p(cᵢ), where p(cᵢ) = |cᵢ|/N, quantifies the overall uncertainty in the model's response to Q.</li>
                <li>Low entropy (H(C) < 0.5 nats for binary case) indicates high confidence with outputs concentrated in one semantic cluster, while high entropy (H(C) > 1.5 nats) indicates high uncertainty with outputs distributed across multiple clusters.</li>
                <li>Within-cluster semantic variance σ²(cᵢ) = (1/|cᵢ|) Σₒ∈cᵢ d(o, centroid(cᵢ))² captures aleatoric uncertainty (paraphrasing variation), while between-cluster variance captures epistemic uncertainty (knowledge gaps).</li>
                <li>The dominant cluster c* = argmaxᵢ p(cᵢ) represents the model's most confident answer, and its probability p(c*) serves as a confidence score for scientific conclusions.</li>
                <li>For scientific applications, conclusions should only be drawn when H(C) is below a threshold (e.g., H(C) < 1.0) and the dominant cluster contains a supermajority of outputs (e.g., p(c*) > 0.7).</li>
                <li>The number of semantic clusters K itself is informative: K=1 suggests consensus, K=2-3 suggests competing hypotheses, and K>5 suggests the model lacks coherent knowledge about Q.</li>
                <li>Semantic equivalence should be determined using embedding-based similarity (e.g., cosine similarity > 0.85 in sentence embedding space) rather than surface-form matching to properly capture paraphrases.</li>
                <li>Uncertainty estimates from semantic clustering are more reliable than token-level probabilities because they account for the many-to-one mapping between surface forms and meanings.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>Language models exhibit stochastic variation in outputs even with the same prompt, requiring methods to aggregate and interpret multiple samples. </li>
    <li>Semantic similarity metrics and clustering methods can effectively group paraphrases and semantically equivalent text. </li>
    <li>Ensemble diversity and output distribution entropy are established methods for uncertainty quantification in machine learning. </li>
    <li>Distinguishing epistemic and aleatoric uncertainty is important for understanding model confidence and improving decision-making. </li>
    <li>Self-consistency and answer clustering have been shown to improve reasoning accuracy in language models. </li>
    <li>Semantic uncertainty and meaning-based confidence estimation outperform token-level probabilities for assessing LLM reliability. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>For well-established scientific facts (e.g., 'What is the chemical formula of water?'), sampling an LLM 20 times will produce outputs with H(C) < 0.3 nats and p(c*) > 0.95, with K ≤ 2 clusters.</li>
                <li>For controversial or frontier scientific questions, H(C) will be 2-3x higher than for established facts, with K ≥ 3 distinct semantic clusters representing different scientific perspectives.</li>
                <li>Filtering scientific QA predictions to only those with H(C) < 1.0 will improve accuracy by 15-25% compared to using all predictions, at the cost of 30-40% coverage.</li>
                <li>Within-cluster variance will be 3-5x smaller than between-cluster variance for scientific questions, confirming that paraphrasing contributes less to uncertainty than knowledge gaps.</li>
                <li>The correlation between H(C) and answer correctness will be negative (r ≈ -0.6 to -0.7), with lower entropy predicting higher accuracy.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether the optimal clustering threshold for semantic equivalence varies by scientific domain (e.g., physics vs. biology) or remains constant - this would inform domain-specific calibration.</li>
                <li>Whether H(C) correlates with the actual scientific controversy or uncertainty in the literature about a question - this would validate the approach as capturing genuine scientific uncertainty rather than just model limitations.</li>
                <li>Whether combining semantic clustering uncertainty with other uncertainty measures (e.g., token probabilities, model ensembles) provides complementary information or redundant signals - this would determine optimal uncertainty quantification strategies.</li>
                <li>Whether the relationship between cluster entropy and accuracy transfers across model scales and architectures - this would determine if uncertainty thresholds need model-specific calibration.</li>
                <li>Whether semantic clustering can distinguish between 'known unknowns' (questions where the model recognizes uncertainty) and 'unknown unknowns' (questions where the model is confidently wrong) - this would be critical for safe scientific applications.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If H(C) does not correlate negatively with answer accuracy, this would challenge the theory's core premise that cluster entropy measures epistemic uncertainty.</li>
                <li>If random or adversarial prompts produce the same H(C) values as carefully designed scientific prompts, this would suggest the measure captures noise rather than meaningful uncertainty.</li>
                <li>If within-cluster variance is comparable to or larger than between-cluster variance, this would challenge the distinction between aleatoric and epistemic uncertainty.</li>
                <li>If the dominant cluster c* is frequently incorrect even when p(c*) > 0.8, this would undermine using cluster probability as a confidence measure.</li>
                <li>If different semantic similarity metrics or clustering algorithms produce uncorrelated H(C) values for the same outputs, this would suggest the measure is not robust.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not specify how to determine the optimal number of samples N needed for reliable cluster distribution estimation. </li>
    <li>How to handle multi-part scientific questions where different parts may have different uncertainty levels is not addressed. </li>
    <li>The computational cost of generating N samples and computing pairwise semantic similarities (O(N²)) may be prohibitive for large N. </li>
    <li>How to calibrate uncertainty thresholds (e.g., H(C) < 1.0) for different scientific domains or risk levels is not fully specified. </li>
    <li>The theory does not address how to handle cases where semantic similarity is ambiguous or where outputs are partially correct. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Kuhn et al. (2023) Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation, ICLR [Proposes semantic uncertainty based on meaning equivalence, but focuses on entailment-based clustering rather than general semantic similarity clustering and does not fully develop the connection to epistemic vs. aleatoric uncertainty]</li>
    <li>Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models, ICLR [Uses answer clustering for aggregation but does not formalize cluster distribution entropy as an uncertainty measure]</li>
    <li>Lakshminarayanan et al. (2017) Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles, NeurIPS [Establishes ensemble-based uncertainty quantification but for classification/regression, not semantic clustering of text]</li>
    <li>Lin et al. (2023) Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models, arXiv [Explores uncertainty quantification for LLMs but uses different approaches than semantic equivalence clustering]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Semantic Equivalence Clustering Theory for Uncertainty Quantification",
    "theory_description": "This theory proposes that uncertainty in language model outputs for scientific tasks can be quantified by clustering semantically equivalent responses and analyzing the resulting cluster distribution. When an LLM is sampled multiple times for the same scientific query, the outputs can be grouped into semantic equivalence classes (clusters of responses with similar meaning). The distribution of outputs across these clusters provides a principled measure of model uncertainty: high concentration in a single cluster indicates low uncertainty and high confidence, while distribution across multiple clusters indicates high uncertainty. Within-cluster variance captures aleatoric (irreducible) uncertainty due to valid paraphrasing, while between-cluster variance captures epistemic (knowledge) uncertainty. The entropy of the cluster distribution H(C) = -Σ p(c_i) log p(c_i) provides a scalar uncertainty measure, where p(c_i) is the proportion of outputs in cluster i. This approach enables uncertainty-aware scientific conclusions by identifying when model outputs converge on a single semantic answer versus when they reflect genuine model uncertainty about the correct answer.",
    "supporting_evidence": [
        {
            "text": "Language models exhibit stochastic variation in outputs even with the same prompt, requiring methods to aggregate and interpret multiple samples.",
            "citations": [
                "Holtzman et al. (2019) The Curious Case of Neural Text Degeneration, ICLR",
                "Wang & Sennrich (2020) On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation, ACL"
            ]
        },
        {
            "text": "Semantic similarity metrics and clustering methods can effectively group paraphrases and semantically equivalent text.",
            "citations": [
                "Reimers & Gurevych (2019) Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, EMNLP",
                "Zhang et al. (2020) BERTScore: Evaluating Text Generation with BERT, ICLR"
            ]
        },
        {
            "text": "Ensemble diversity and output distribution entropy are established methods for uncertainty quantification in machine learning.",
            "citations": [
                "Lakshminarayanan et al. (2017) Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles, NeurIPS",
                "Gal & Ghahramani (2016) Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ICML"
            ]
        },
        {
            "text": "Distinguishing epistemic and aleatoric uncertainty is important for understanding model confidence and improving decision-making.",
            "citations": [
                "Kendall & Gal (2017) What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?, NeurIPS",
                "Malinin & Gales (2018) Predictive Uncertainty Estimation via Prior Networks, NeurIPS"
            ]
        },
        {
            "text": "Self-consistency and answer clustering have been shown to improve reasoning accuracy in language models.",
            "citations": [
                "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models, ICLR"
            ]
        },
        {
            "text": "Semantic uncertainty and meaning-based confidence estimation outperform token-level probabilities for assessing LLM reliability.",
            "citations": [
                "Kuhn et al. (2023) Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation, ICLR"
            ]
        }
    ],
    "theory_statements": [
        "For a scientific query Q, sampling an LLM N times produces outputs O = {o₁, o₂, ..., oₙ} that can be partitioned into K semantic equivalence clusters C = {c₁, c₂, ..., cₖ} based on semantic similarity.",
        "The cluster distribution entropy H(C) = -Σᵢ₌₁ᴷ p(cᵢ) log p(cᵢ), where p(cᵢ) = |cᵢ|/N, quantifies the overall uncertainty in the model's response to Q.",
        "Low entropy (H(C) &lt; 0.5 nats for binary case) indicates high confidence with outputs concentrated in one semantic cluster, while high entropy (H(C) &gt; 1.5 nats) indicates high uncertainty with outputs distributed across multiple clusters.",
        "Within-cluster semantic variance σ²(cᵢ) = (1/|cᵢ|) Σₒ∈cᵢ d(o, centroid(cᵢ))² captures aleatoric uncertainty (paraphrasing variation), while between-cluster variance captures epistemic uncertainty (knowledge gaps).",
        "The dominant cluster c* = argmaxᵢ p(cᵢ) represents the model's most confident answer, and its probability p(c*) serves as a confidence score for scientific conclusions.",
        "For scientific applications, conclusions should only be drawn when H(C) is below a threshold (e.g., H(C) &lt; 1.0) and the dominant cluster contains a supermajority of outputs (e.g., p(c*) &gt; 0.7).",
        "The number of semantic clusters K itself is informative: K=1 suggests consensus, K=2-3 suggests competing hypotheses, and K&gt;5 suggests the model lacks coherent knowledge about Q.",
        "Semantic equivalence should be determined using embedding-based similarity (e.g., cosine similarity &gt; 0.85 in sentence embedding space) rather than surface-form matching to properly capture paraphrases.",
        "Uncertainty estimates from semantic clustering are more reliable than token-level probabilities because they account for the many-to-one mapping between surface forms and meanings."
    ],
    "new_predictions_likely": [
        "For well-established scientific facts (e.g., 'What is the chemical formula of water?'), sampling an LLM 20 times will produce outputs with H(C) &lt; 0.3 nats and p(c*) &gt; 0.95, with K ≤ 2 clusters.",
        "For controversial or frontier scientific questions, H(C) will be 2-3x higher than for established facts, with K ≥ 3 distinct semantic clusters representing different scientific perspectives.",
        "Filtering scientific QA predictions to only those with H(C) &lt; 1.0 will improve accuracy by 15-25% compared to using all predictions, at the cost of 30-40% coverage.",
        "Within-cluster variance will be 3-5x smaller than between-cluster variance for scientific questions, confirming that paraphrasing contributes less to uncertainty than knowledge gaps.",
        "The correlation between H(C) and answer correctness will be negative (r ≈ -0.6 to -0.7), with lower entropy predicting higher accuracy."
    ],
    "new_predictions_unknown": [
        "Whether the optimal clustering threshold for semantic equivalence varies by scientific domain (e.g., physics vs. biology) or remains constant - this would inform domain-specific calibration.",
        "Whether H(C) correlates with the actual scientific controversy or uncertainty in the literature about a question - this would validate the approach as capturing genuine scientific uncertainty rather than just model limitations.",
        "Whether combining semantic clustering uncertainty with other uncertainty measures (e.g., token probabilities, model ensembles) provides complementary information or redundant signals - this would determine optimal uncertainty quantification strategies.",
        "Whether the relationship between cluster entropy and accuracy transfers across model scales and architectures - this would determine if uncertainty thresholds need model-specific calibration.",
        "Whether semantic clustering can distinguish between 'known unknowns' (questions where the model recognizes uncertainty) and 'unknown unknowns' (questions where the model is confidently wrong) - this would be critical for safe scientific applications."
    ],
    "negative_experiments": [
        "If H(C) does not correlate negatively with answer accuracy, this would challenge the theory's core premise that cluster entropy measures epistemic uncertainty.",
        "If random or adversarial prompts produce the same H(C) values as carefully designed scientific prompts, this would suggest the measure captures noise rather than meaningful uncertainty.",
        "If within-cluster variance is comparable to or larger than between-cluster variance, this would challenge the distinction between aleatoric and epistemic uncertainty.",
        "If the dominant cluster c* is frequently incorrect even when p(c*) &gt; 0.8, this would undermine using cluster probability as a confidence measure.",
        "If different semantic similarity metrics or clustering algorithms produce uncorrelated H(C) values for the same outputs, this would suggest the measure is not robust."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not specify how to determine the optimal number of samples N needed for reliable cluster distribution estimation.",
            "citations": []
        },
        {
            "text": "How to handle multi-part scientific questions where different parts may have different uncertainty levels is not addressed.",
            "citations": []
        },
        {
            "text": "The computational cost of generating N samples and computing pairwise semantic similarities (O(N²)) may be prohibitive for large N.",
            "citations": []
        },
        {
            "text": "How to calibrate uncertainty thresholds (e.g., H(C) &lt; 1.0) for different scientific domains or risk levels is not fully specified.",
            "citations": []
        },
        {
            "text": "The theory does not address how to handle cases where semantic similarity is ambiguous or where outputs are partially correct.",
            "citations": []
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some research suggests that language model confidence (including ensemble-based measures) can be poorly calibrated, potentially limiting the reliability of cluster-based uncertainty estimates.",
            "citations": [
                "Jiang et al. (2021) Can We Trust Model Confidence? Analyzing Calibration in Language Models, Findings of EMNLP"
            ]
        },
        {
            "text": "Token-level probabilities from language models sometimes outperform sampling-based uncertainty estimates in certain tasks, suggesting semantic clustering may not always be superior.",
            "citations": [
                "Kadavath et al. (2022) Language Models (Mostly) Know What They Know, arXiv"
            ]
        }
    ],
    "special_cases": [
        "For questions with multiple valid answers (e.g., 'Name a greenhouse gas'), high H(C) may reflect valid diversity rather than uncertainty, requiring different interpretation.",
        "For highly technical questions requiring specialized knowledge, semantic similarity metrics trained on general text may not accurately capture equivalence, requiring domain-specific embeddings.",
        "When outputs include numerical values or structured data, semantic similarity alone may be insufficient and should be combined with value-based similarity metrics.",
        "For very short outputs (e.g., single-word answers), semantic clustering may be less informative than for longer explanatory responses.",
        "When sampling with very low temperature (near 0), outputs may be nearly identical, making clustering trivial and potentially masking underlying uncertainty."
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Kuhn et al. (2023) Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation, ICLR [Proposes semantic uncertainty based on meaning equivalence, but focuses on entailment-based clustering rather than general semantic similarity clustering and does not fully develop the connection to epistemic vs. aleatoric uncertainty]",
            "Wang et al. (2022) Self-Consistency Improves Chain of Thought Reasoning in Language Models, ICLR [Uses answer clustering for aggregation but does not formalize cluster distribution entropy as an uncertainty measure]",
            "Lakshminarayanan et al. (2017) Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles, NeurIPS [Establishes ensemble-based uncertainty quantification but for classification/regression, not semantic clustering of text]",
            "Lin et al. (2023) Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models, arXiv [Explores uncertainty quantification for LLMs but uses different approaches than semantic equivalence clustering]"
        ]
    },
    "theory_type_general_specific": "specific",
    "reflected_from_theory_index": 2,
    "theory_query": "Build a theory about variability and reproducibility in language model-driven scientific experimentation.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-80",
    "original_theory_name": "Semantic Equivalence Clustering Theory for Uncertainty Quantification",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
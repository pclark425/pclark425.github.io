<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training Distribution Mismatch Theory - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-273</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-273</p>
                <p><strong>Name:</strong> Training Distribution Mismatch Theory</p>
                <p><strong>Type:</strong> general</p>
                <p><strong>Theory Query:</strong> Build a theory explaining the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents, and the architectural/training interventions required to close the gap.. Please focus on creating new theories that have not been proposed before in the literature.</p>
                <p><strong>Description:</strong> This theory posits that the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents arises from a fundamental mismatch between the statistical properties of pre-training and fine-tuning data distributions versus the requirements of interactive procedural tasks. Specifically, LLMs are trained predominantly on static, context-complete text where all necessary information is present within a single context window, and where success is measured by single-step prediction accuracy. In contrast, interactive procedural tasks require: (1) temporal credit assignment across long action sequences, (2) state tracking and memory management across episodes, (3) recovery from compounding errors, (4) exploration-exploitation tradeoffs, and (5) learning from sparse, delayed rewards. The training distribution lacks sufficient examples of these interactive dynamics, leading to models that can retrieve and synthesize knowledge (QA) but cannot effectively deploy it in sequential decision-making contexts. This mismatch manifests at multiple levels: token-level (next-token prediction vs. action selection), episode-level (single passages vs. multi-step trajectories), and objective-level (likelihood maximization vs. task completion). The theory acknowledges that scale and certain training methods (like RLHF) may partially compensate for this mismatch by enabling implicit learning of interactive patterns, and that inference-time interventions can partially bridge the gap by better utilizing the model's latent capabilities.</p>
                <p><strong>Knowledge Cutoff Year:</strong> -1</p>
                <p><strong>Knowledge Cutoff Month:</strong> -1</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>The statistical properties of pre-training data (static, context-complete text) fundamentally differ from the requirements of interactive procedural tasks (dynamic, partially observable environments with feedback loops).</li>
                <li>Next-token prediction objectives optimize for local coherence and factual accuracy but do not directly incentivize long-horizon planning, error recovery, or state management capabilities.</li>
                <li>The absence of explicit temporal credit assignment in standard language modeling prevents models from learning which early actions in a sequence lead to successful task completion.</li>
                <li>Error compounding in interactive tasks creates a distribution shift between training (mostly correct contexts) and deployment (contexts containing accumulated errors), which models are not trained to handle.</li>
                <li>The ratio of static text to interactive trajectories in training data predicts the magnitude of the QA-to-procedural performance gap: higher ratios lead to larger gaps.</li>
                <li>Models trained on interactive trajectories with explicit state representations and temporal structure will show reduced dissociation between QA and procedural performance.</li>
                <li>Fine-tuning on static instruction-following examples provides minimal transfer to interactive tasks because it does not expose models to the dynamics of sequential decision-making with feedback.</li>
                <li>The performance gap increases with task horizon length, as longer tasks amplify the effects of training distribution mismatch through compounding errors and increased credit assignment difficulty.</li>
                <li>Architectural components that enable explicit memory, state tracking, and planning (e.g., external memory modules, world models) can partially compensate for training distribution mismatch by providing structure not learned from data.</li>
                <li>Interventions that align training objectives with interactive task requirements (e.g., RL with task completion rewards, trajectory-based training) will close the performance gap more effectively than scaling model size alone, though scale may enable partial implicit learning of interactive patterns.</li>
                <li>Inference-time interventions (e.g., chain-of-thought, ReAct) can partially bridge the gap by better eliciting latent interactive capabilities that exist but are not readily accessible through standard prompting.</li>
                <li>The mismatch is most severe for tasks requiring tight temporal coupling between actions, where each action depends critically on the outcomes of previous actions, as opposed to tasks decomposable into independent subtasks.</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>LLMs demonstrate strong performance on static benchmarks like MMLU, TriviaQA, and Natural Questions, achieving near-human or superhuman accuracy on factual question answering. </li>
    <li>The same LLMs show significant performance degradation on interactive benchmarks like WebShop, ALFWorld, and ScienceWorld, where success rates often fall below 50% even for large models. </li>
    <li>Pre-training corpora consist overwhelmingly of static text (web pages, books, articles) with minimal representation of interactive trajectories or multi-step procedures with feedback. </li>
    <li>Error accumulation and compounding is a major failure mode in interactive tasks, where early mistakes cascade into task failure, but this dynamic is absent in single-turn QA. </li>
    <li>Instruction-tuned models show improved QA performance but limited transfer to interactive tasks, suggesting that supervised fine-tuning on static examples is insufficient. </li>
    <li>Models struggle with state tracking and memory management in long-horizon tasks, often forgetting previous observations or actions. </li>
    <li>Reinforcement learning from task-specific environments shows improvements on interactive benchmarks, suggesting that exposure to interactive trajectories is critical. </li>
    <li>Prompting techniques like chain-of-thought and ReAct can improve interactive task performance without training changes, suggesting that models may have latent interactive capabilities not fully utilized by standard prompting. </li>
    <li>Very large models show emergent interactive capabilities that smaller models lack, suggesting that scale enables some implicit learning of interactive patterns from static text. </li>
    <li>Models fine-tuned with RLHF show improvements on both QA and interactive tasks, suggesting that human feedback on diverse tasks may implicitly capture some interactive dynamics. </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>Pre-training LLMs on a corpus augmented with synthetic interactive trajectories (e.g., simulated dialogues with state changes, procedural walkthroughs with feedback) will reduce the QA-procedural gap by 20-40% compared to standard pre-training, with larger improvements for longer-horizon tasks.</li>
                <li>Fine-tuning on datasets that explicitly include error recovery examples (trajectories showing mistakes and corrections) will improve interactive task performance by 15-30% more than fine-tuning on error-free demonstrations of equal size.</li>
                <li>Models trained with a curriculum that gradually increases task horizon length will show 10-25% better transfer to long-horizon interactive tasks than models trained on mixed-length tasks with the same total training steps.</li>
                <li>Adding a training objective that predicts future states or action consequences alongside next-token prediction will improve procedural task performance by 15-35% without degrading QA performance by more than 5%.</li>
                <li>The performance gap will be 30-50% smaller for tasks that can be decomposed into independent sub-tasks (low temporal dependency) compared to tasks requiring tight integration across steps, when controlling for total task length.</li>
                <li>Measuring the proportion of multi-turn, feedback-containing sequences in training data will show a correlation of r > 0.6 with interactive task performance across different models of similar scale.</li>
                <li>Models that receive explicit training on state tracking (e.g., predicting what information is currently known vs. unknown) will show 20-40% improvement on long-context interactive tasks compared to standard training.</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Training on interactive trajectories from one domain (e.g., text-based games) may or may not transfer to interactive tasks in other domains (e.g., web navigation, robotic control). If transfer occurs with >70% efficiency, it would suggest domain-general sequential decision-making skills; if <30%, it would suggest domain-specific heuristics dominate.</li>
                <li>It is unclear whether the optimal intervention is to modify pre-training data composition, add specialized fine-tuning stages, or change the model architecture—or whether all three are necessary. The relative contribution of each could range from 10% to 80% of the total improvement, with major implications for research priorities.</li>
                <li>The theory predicts that models trained with online RL in interactive environments will close the gap, but it's unknown whether this requires environment-specific training or whether a diverse set of 5-10 interactive environments can produce general procedural competence transferring to novel environments with >60% efficiency.</li>
                <li>Whether the training distribution mismatch can be fully addressed (>90% gap closure) through data augmentation alone, or whether fundamental architectural changes (e.g., separate planning modules, explicit world models) are necessary, remains an open question. If data alone is sufficient, it would suggest current architectures are more capable than commonly believed.</li>
                <li>The theory suggests that human feedback on interactive trajectories would be more valuable than feedback on static QA pairs, but the relative value could range from 2x to 20x per feedback instance, dramatically affecting the cost-effectiveness of different training approaches.</li>
                <li>It is uncertain whether the dissociation is primarily due to lack of exposure to interactive dynamics (>70% of the gap) or whether current Transformer architectures have fundamental limitations in credit assignment and temporal reasoning that cannot be overcome by data alone (<30% addressable by data). This has profound implications for whether architectural innovation is necessary.</li>
                <li>The extent to which very large scale (>1T parameters) can implicitly overcome distribution mismatch is unknown. If models at 10T parameters show <20% remaining gap without specialized training, it would suggest scale is a viable alternative to specialized interactive training; if >60% gap remains, it would confirm the necessity of distributional interventions.</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>If models trained exclusively on interactive trajectories (with no static text) show poor QA performance but strong procedural performance, while models trained on static text show the opposite, this would support the theory. However, if both show poor performance across both task types, it would suggest factors beyond distribution mismatch (e.g., that both types of data are necessary for general competence).</li>
                <li>If augmenting training data with synthetic interactive trajectories (comprising >20% of training tokens) does not improve procedural performance by at least 10%, this would challenge the theory's core premise that exposure to interactive dynamics is a primary bottleneck.</li>
                <li>If the performance gap does not show a monotonic increase with task horizon length (i.e., if 20-step tasks show similar or smaller gaps than 5-step tasks), this would contradict the theory's prediction about temporal credit assignment and error compounding.</li>
                <li>If architectural interventions (e.g., adding explicit memory modules) close >70% of the gap without any changes to training data distribution, this would suggest the issue is primarily architectural rather than distributional.</li>
                <li>If models show equal difficulty with interactive tasks that have immediate feedback versus delayed feedback (controlling for task complexity), this would challenge the theory's emphasis on temporal credit assignment as a key factor.</li>
                <li>If fine-tuning on error-containing trajectories does not improve performance by at least 10% compared to error-free trajectories of equal size, this would question the theory's claim about distribution shift from accumulated errors.</li>
                <li>If the correlation between proportion of interactive sequences in training data and interactive task performance is weak (r < 0.3) across models of similar scale, this would undermine the theory's central distributional claim.</li>
                <li>If inference-time interventions (like ReAct) can close >80% of the performance gap without any training changes, this would suggest the gap is primarily due to inference/prompting factors rather than training distribution, contradicting the theory's core premise.</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory does not fully explain why some procedural tasks (e.g., code generation with execution feedback) show substantially better performance than others (e.g., embodied navigation), despite similar interactive structure. This suggests task-specific factors beyond just interactive dynamics. </li>
    <li>Individual differences in model architecture (e.g., decoder-only vs. encoder-decoder) affect the QA-procedural gap in ways not fully explained by training distribution alone, suggesting architectural factors play an independent role. </li>
    <li>The theory does not fully account for why certain prompting techniques can substantially improve interactive performance without any training changes, suggesting that models may already possess relevant capabilities that are simply not being properly elicited. </li>
    <li>The theory does not explain the specific mechanisms by which scale enables emergent interactive capabilities, or why certain capabilities emerge at specific scale thresholds. </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> new</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Related work on grounding language models in interactive environments, but focuses on affordances and embodiment rather than training distribution mismatch as root cause]</li>
    <li>Ramamurthy et al. (2022) Is Reinforcement Learning (Not) for Natural Language Processing? [Discusses RL for NLP but doesn't specifically theorize about QA-procedural dissociation from a distributional perspective]</li>
    <li>Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models [Proposes architectural/prompting solutions but doesn't provide a distributional theory of the gap]</li>
    <li>Carta et al. (2023) Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning [Demonstrates RL improvements but doesn't theorize about training distribution mismatch as root cause]</li>
    <li>Kenton et al. (2021) Alignment of Language Agents [Discusses alignment challenges but not specifically the QA-procedural gap from a distributional perspective]</li>
    <li>Huang et al. (2022) Language Models as Zero-Shot Planners [Addresses planning capabilities but doesn't provide a theory of why planning fails in interactive settings]</li>
    <li>Shridhar et al. (2021) ALFWorld [Demonstrates the gap empirically but doesn't provide a theoretical framework for why it exists]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Training Distribution Mismatch Theory",
    "theory_description": "This theory posits that the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents arises from a fundamental mismatch between the statistical properties of pre-training and fine-tuning data distributions versus the requirements of interactive procedural tasks. Specifically, LLMs are trained predominantly on static, context-complete text where all necessary information is present within a single context window, and where success is measured by single-step prediction accuracy. In contrast, interactive procedural tasks require: (1) temporal credit assignment across long action sequences, (2) state tracking and memory management across episodes, (3) recovery from compounding errors, (4) exploration-exploitation tradeoffs, and (5) learning from sparse, delayed rewards. The training distribution lacks sufficient examples of these interactive dynamics, leading to models that can retrieve and synthesize knowledge (QA) but cannot effectively deploy it in sequential decision-making contexts. This mismatch manifests at multiple levels: token-level (next-token prediction vs. action selection), episode-level (single passages vs. multi-step trajectories), and objective-level (likelihood maximization vs. task completion). The theory acknowledges that scale and certain training methods (like RLHF) may partially compensate for this mismatch by enabling implicit learning of interactive patterns, and that inference-time interventions can partially bridge the gap by better utilizing the model's latent capabilities.",
    "supporting_evidence": [
        {
            "text": "LLMs demonstrate strong performance on static benchmarks like MMLU, TriviaQA, and Natural Questions, achieving near-human or superhuman accuracy on factual question answering.",
            "citations": [
                "Hendrycks et al. (2021) Measuring Massive Multitask Language Understanding",
                "Brown et al. (2020) Language Models are Few-Shot Learners",
                "Chowdhery et al. (2022) PaLM: Scaling Language Modeling with Pathways"
            ]
        },
        {
            "text": "The same LLMs show significant performance degradation on interactive benchmarks like WebShop, ALFWorld, and ScienceWorld, where success rates often fall below 50% even for large models.",
            "citations": [
                "Yao et al. (2022) WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
                "Shridhar et al. (2021) ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
                "Wang et al. (2022) ScienceWorld: Is your Agent Smarter than a 5th Grader?"
            ]
        },
        {
            "text": "Pre-training corpora consist overwhelmingly of static text (web pages, books, articles) with minimal representation of interactive trajectories or multi-step procedures with feedback.",
            "citations": [
                "Gao et al. (2020) The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
                "Raffel et al. (2020) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "Touvron et al. (2023) LLaMA: Open and Efficient Foundation Language Models"
            ]
        },
        {
            "text": "Error accumulation and compounding is a major failure mode in interactive tasks, where early mistakes cascade into task failure, but this dynamic is absent in single-turn QA.",
            "citations": [
                "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "Venkatesh et al. (2023) Analyzing the Failure Modes of LLM-based Agents in Interactive Environments"
            ]
        },
        {
            "text": "Instruction-tuned models show improved QA performance but limited transfer to interactive tasks, suggesting that supervised fine-tuning on static examples is insufficient.",
            "citations": [
                "Ouyang et al. (2022) Training language models to follow instructions with human feedback",
                "Chung et al. (2022) Scaling Instruction-Finetuned Language Models"
            ]
        },
        {
            "text": "Models struggle with state tracking and memory management in long-horizon tasks, often forgetting previous observations or actions.",
            "citations": [
                "Liu et al. (2023) Lost in the Middle: How Language Models Use Long Contexts",
                "Modarressi et al. (2022) GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers"
            ]
        },
        {
            "text": "Reinforcement learning from task-specific environments shows improvements on interactive benchmarks, suggesting that exposure to interactive trajectories is critical.",
            "citations": [
                "Carta et al. (2023) Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning",
                "Ramamurthy et al. (2022) Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks"
            ]
        },
        {
            "text": "Prompting techniques like chain-of-thought and ReAct can improve interactive task performance without training changes, suggesting that models may have latent interactive capabilities not fully utilized by standard prompting.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models"
            ]
        },
        {
            "text": "Very large models show emergent interactive capabilities that smaller models lack, suggesting that scale enables some implicit learning of interactive patterns from static text.",
            "citations": [
                "OpenAI (2023) GPT-4 Technical Report",
                "Bubeck et al. (2023) Sparks of Artificial General Intelligence: Early experiments with GPT-4"
            ]
        },
        {
            "text": "Models fine-tuned with RLHF show improvements on both QA and interactive tasks, suggesting that human feedback on diverse tasks may implicitly capture some interactive dynamics.",
            "citations": [
                "Ouyang et al. (2022) Training language models to follow instructions with human feedback",
                "Bai et al. (2022) Constitutional AI: Harmlessness from AI Feedback"
            ]
        }
    ],
    "theory_statements": [
        "The statistical properties of pre-training data (static, context-complete text) fundamentally differ from the requirements of interactive procedural tasks (dynamic, partially observable environments with feedback loops).",
        "Next-token prediction objectives optimize for local coherence and factual accuracy but do not directly incentivize long-horizon planning, error recovery, or state management capabilities.",
        "The absence of explicit temporal credit assignment in standard language modeling prevents models from learning which early actions in a sequence lead to successful task completion.",
        "Error compounding in interactive tasks creates a distribution shift between training (mostly correct contexts) and deployment (contexts containing accumulated errors), which models are not trained to handle.",
        "The ratio of static text to interactive trajectories in training data predicts the magnitude of the QA-to-procedural performance gap: higher ratios lead to larger gaps.",
        "Models trained on interactive trajectories with explicit state representations and temporal structure will show reduced dissociation between QA and procedural performance.",
        "Fine-tuning on static instruction-following examples provides minimal transfer to interactive tasks because it does not expose models to the dynamics of sequential decision-making with feedback.",
        "The performance gap increases with task horizon length, as longer tasks amplify the effects of training distribution mismatch through compounding errors and increased credit assignment difficulty.",
        "Architectural components that enable explicit memory, state tracking, and planning (e.g., external memory modules, world models) can partially compensate for training distribution mismatch by providing structure not learned from data.",
        "Interventions that align training objectives with interactive task requirements (e.g., RL with task completion rewards, trajectory-based training) will close the performance gap more effectively than scaling model size alone, though scale may enable partial implicit learning of interactive patterns.",
        "Inference-time interventions (e.g., chain-of-thought, ReAct) can partially bridge the gap by better eliciting latent interactive capabilities that exist but are not readily accessible through standard prompting.",
        "The mismatch is most severe for tasks requiring tight temporal coupling between actions, where each action depends critically on the outcomes of previous actions, as opposed to tasks decomposable into independent subtasks."
    ],
    "new_predictions_likely": [
        "Pre-training LLMs on a corpus augmented with synthetic interactive trajectories (e.g., simulated dialogues with state changes, procedural walkthroughs with feedback) will reduce the QA-procedural gap by 20-40% compared to standard pre-training, with larger improvements for longer-horizon tasks.",
        "Fine-tuning on datasets that explicitly include error recovery examples (trajectories showing mistakes and corrections) will improve interactive task performance by 15-30% more than fine-tuning on error-free demonstrations of equal size.",
        "Models trained with a curriculum that gradually increases task horizon length will show 10-25% better transfer to long-horizon interactive tasks than models trained on mixed-length tasks with the same total training steps.",
        "Adding a training objective that predicts future states or action consequences alongside next-token prediction will improve procedural task performance by 15-35% without degrading QA performance by more than 5%.",
        "The performance gap will be 30-50% smaller for tasks that can be decomposed into independent sub-tasks (low temporal dependency) compared to tasks requiring tight integration across steps, when controlling for total task length.",
        "Measuring the proportion of multi-turn, feedback-containing sequences in training data will show a correlation of r &gt; 0.6 with interactive task performance across different models of similar scale.",
        "Models that receive explicit training on state tracking (e.g., predicting what information is currently known vs. unknown) will show 20-40% improvement on long-context interactive tasks compared to standard training."
    ],
    "new_predictions_unknown": [
        "Training on interactive trajectories from one domain (e.g., text-based games) may or may not transfer to interactive tasks in other domains (e.g., web navigation, robotic control). If transfer occurs with &gt;70% efficiency, it would suggest domain-general sequential decision-making skills; if &lt;30%, it would suggest domain-specific heuristics dominate.",
        "It is unclear whether the optimal intervention is to modify pre-training data composition, add specialized fine-tuning stages, or change the model architecture—or whether all three are necessary. The relative contribution of each could range from 10% to 80% of the total improvement, with major implications for research priorities.",
        "The theory predicts that models trained with online RL in interactive environments will close the gap, but it's unknown whether this requires environment-specific training or whether a diverse set of 5-10 interactive environments can produce general procedural competence transferring to novel environments with &gt;60% efficiency.",
        "Whether the training distribution mismatch can be fully addressed (&gt;90% gap closure) through data augmentation alone, or whether fundamental architectural changes (e.g., separate planning modules, explicit world models) are necessary, remains an open question. If data alone is sufficient, it would suggest current architectures are more capable than commonly believed.",
        "The theory suggests that human feedback on interactive trajectories would be more valuable than feedback on static QA pairs, but the relative value could range from 2x to 20x per feedback instance, dramatically affecting the cost-effectiveness of different training approaches.",
        "It is uncertain whether the dissociation is primarily due to lack of exposure to interactive dynamics (&gt;70% of the gap) or whether current Transformer architectures have fundamental limitations in credit assignment and temporal reasoning that cannot be overcome by data alone (&lt;30% addressable by data). This has profound implications for whether architectural innovation is necessary.",
        "The extent to which very large scale (&gt;1T parameters) can implicitly overcome distribution mismatch is unknown. If models at 10T parameters show &lt;20% remaining gap without specialized training, it would suggest scale is a viable alternative to specialized interactive training; if &gt;60% gap remains, it would confirm the necessity of distributional interventions."
    ],
    "negative_experiments": [
        "If models trained exclusively on interactive trajectories (with no static text) show poor QA performance but strong procedural performance, while models trained on static text show the opposite, this would support the theory. However, if both show poor performance across both task types, it would suggest factors beyond distribution mismatch (e.g., that both types of data are necessary for general competence).",
        "If augmenting training data with synthetic interactive trajectories (comprising &gt;20% of training tokens) does not improve procedural performance by at least 10%, this would challenge the theory's core premise that exposure to interactive dynamics is a primary bottleneck.",
        "If the performance gap does not show a monotonic increase with task horizon length (i.e., if 20-step tasks show similar or smaller gaps than 5-step tasks), this would contradict the theory's prediction about temporal credit assignment and error compounding.",
        "If architectural interventions (e.g., adding explicit memory modules) close &gt;70% of the gap without any changes to training data distribution, this would suggest the issue is primarily architectural rather than distributional.",
        "If models show equal difficulty with interactive tasks that have immediate feedback versus delayed feedback (controlling for task complexity), this would challenge the theory's emphasis on temporal credit assignment as a key factor.",
        "If fine-tuning on error-containing trajectories does not improve performance by at least 10% compared to error-free trajectories of equal size, this would question the theory's claim about distribution shift from accumulated errors.",
        "If the correlation between proportion of interactive sequences in training data and interactive task performance is weak (r &lt; 0.3) across models of similar scale, this would undermine the theory's central distributional claim.",
        "If inference-time interventions (like ReAct) can close &gt;80% of the performance gap without any training changes, this would suggest the gap is primarily due to inference/prompting factors rather than training distribution, contradicting the theory's core premise."
    ],
    "unaccounted_for": [
        {
            "text": "The theory does not fully explain why some procedural tasks (e.g., code generation with execution feedback) show substantially better performance than others (e.g., embodied navigation), despite similar interactive structure. This suggests task-specific factors beyond just interactive dynamics.",
            "citations": [
                "Chen et al. (2023) Teaching Large Language Models to Self-Debug",
                "Shridhar et al. (2021) ALFWorld: Aligning Text and Embodied Environments for Interactive Learning"
            ]
        },
        {
            "text": "Individual differences in model architecture (e.g., decoder-only vs. encoder-decoder) affect the QA-procedural gap in ways not fully explained by training distribution alone, suggesting architectural factors play an independent role.",
            "citations": [
                "Tay et al. (2022) UL2: Unifying Language Learning Paradigms",
                "Raffel et al. (2020) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
            ]
        },
        {
            "text": "The theory does not fully account for why certain prompting techniques can substantially improve interactive performance without any training changes, suggesting that models may already possess relevant capabilities that are simply not being properly elicited.",
            "citations": [
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models"
            ]
        },
        {
            "text": "The theory does not explain the specific mechanisms by which scale enables emergent interactive capabilities, or why certain capabilities emerge at specific scale thresholds.",
            "citations": [
                "OpenAI (2023) GPT-4 Technical Report",
                "Bubeck et al. (2023) Sparks of Artificial General Intelligence: Early experiments with GPT-4",
                "Wei et al. (2022) Emergent Abilities of Large Language Models"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "Some studies show that very large models (e.g., GPT-4) demonstrate emergent interactive capabilities without explicit training on interactive trajectories, suggesting that scale alone may partially overcome distribution mismatch, possibly through implicit learning of interactive patterns from static text descriptions of interactive processes.",
            "citations": [
                "OpenAI (2023) GPT-4 Technical Report",
                "Bubeck et al. (2023) Sparks of Artificial General Intelligence: Early experiments with GPT-4"
            ]
        },
        {
            "text": "Models fine-tuned with RLHF show improvements on both QA and interactive tasks, but the theory would predict RLHF on static preferences should not help with interactive dynamics. This suggests RLHF may implicitly capture interactive patterns or that human preferences encode interactive competence.",
            "citations": [
                "Ouyang et al. (2022) Training language models to follow instructions with human feedback",
                "Bai et al. (2022) Constitutional AI: Harmlessness from AI Feedback"
            ]
        },
        {
            "text": "Prompting techniques like ReAct can substantially improve interactive task performance without any training changes, suggesting the gap may be partially due to inference-time factors (how capabilities are elicited) rather than training distribution alone.",
            "citations": [
                "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models",
                "Wei et al. (2022) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
            ]
        }
    ],
    "special_cases": [
        "The theory applies most strongly to tasks requiring long-horizon planning (&gt;10 steps). For very short interactive tasks (2-3 steps), the gap may be minimal because they resemble multi-turn QA and don't require substantial temporal credit assignment.",
        "Tasks with dense, immediate feedback may show 40-60% smaller gaps than tasks with sparse, delayed rewards, as the former more closely resemble the turn-by-turn structure of conversational data in training corpora.",
        "Domains where extensive procedural text exists in training data (e.g., cooking recipes, programming tutorials) may show 30-50% smaller gaps than domains with minimal procedural coverage (e.g., novel game environments, specialized robotic tasks).",
        "The theory's predictions may not hold for models that incorporate explicit search or planning algorithms at inference time (e.g., tree search, Monte Carlo methods), as these architectural additions change the computational model beyond what training distribution affects.",
        "Interactive tasks that can be solved through retrieval and single-step application of knowledge (e.g., 'find the capital of France and navigate to its Wikipedia page') may not show the predicted gap, as they don't truly require sequential decision-making with temporal dependencies.",
        "For models above a certain scale threshold (possibly &gt;100B parameters), the theory's predictions may be attenuated as emergent capabilities partially compensate for distributional mismatch, though the exact threshold and degree of compensation remain empirically uncertain.",
        "Tasks where the action space is highly constrained or where valid actions are easily identifiable may show smaller gaps, as the challenge of action selection is reduced even without interactive training."
    ],
    "existing_theory": {
        "likely_classification": "new",
        "references": [
            "Ahn et al. (2022) Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [Related work on grounding language models in interactive environments, but focuses on affordances and embodiment rather than training distribution mismatch as root cause]",
            "Ramamurthy et al. (2022) Is Reinforcement Learning (Not) for Natural Language Processing? [Discusses RL for NLP but doesn't specifically theorize about QA-procedural dissociation from a distributional perspective]",
            "Yao et al. (2023) ReAct: Synergizing Reasoning and Acting in Language Models [Proposes architectural/prompting solutions but doesn't provide a distributional theory of the gap]",
            "Carta et al. (2023) Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning [Demonstrates RL improvements but doesn't theorize about training distribution mismatch as root cause]",
            "Kenton et al. (2021) Alignment of Language Agents [Discusses alignment challenges but not specifically the QA-procedural gap from a distributional perspective]",
            "Huang et al. (2022) Language Models as Zero-Shot Planners [Addresses planning capabilities but doesn't provide a theory of why planning fails in interactive settings]",
            "Shridhar et al. (2021) ALFWorld [Demonstrates the gap empirically but doesn't provide a theoretical framework for why it exists]"
        ]
    },
    "theory_type_general_specific": "general",
    "reflected_from_theory_index": 0,
    "theory_query": "Build a theory explaining the dissociation between strong QA performance and weak interactive procedural performance in LLM-based agents, and the architectural/training interventions required to close the gap.. Please focus on creating new theories that have not been proposed before in the literature.",
    "generation_mode": "llm_baseline_no_evidence",
    "original_theory_id": "theory-106",
    "original_theory_name": "Training Distribution Mismatch Theory",
    "model_str": "claude-sonnet-4-5-20250929"
}</code></pre>
        </div>
    </div>
</body>
</html>
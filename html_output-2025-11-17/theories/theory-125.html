<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Latent Confounder Detection via Independent Subspace Analysis in Linear Non-Gaussian Models - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Theory Details for theory-125</h1>

        <div class="section">
            <h2>Theory (General Information)</h2>
            <div class="info-section">
                <p><strong>ID:</strong> theory-125</p>
                <p><strong>Name:</strong> Latent Confounder Detection via Independent Subspace Analysis in Linear Non-Gaussian Models</p>
                <p><strong>Type:</strong> specific</p>
                <p><strong>Theory Query:</strong> Build a theory of distractor-robust causal discovery in open-ended virtual labs, including methods to detect, downweight, and refute spurious signals during inquiry, based on the following results.</p>
                <p><strong>Description:</strong> In linear non-Gaussian models (LiNGLaM) with latent confounders, spurious correlations induced by hidden common causes can be detected and characterized through Independent Subspace Analysis (ISA) combined with the Generalized Independent Noise (GIN) condition. The fundamental insight is that the inverse principal submatrix A^{-1}_{S,S} of the mixing matrix is an ISA solution, where observed variables sharing latent parents form irreducible multi-dimensional subspaces while unconfounded variables form 1-D independent components. By constructing surrogate variables that null cross-covariance with other observed sets (ω^T Y where ω^T E[YZ^T] = 0) and testing their independence using nonparametric tests (HSIC), one can: (1) identify which observed variables share latent causes, (2) determine the dimensionality of latent confounders behind each group (Dim(L(P)) = Dim(P) - 1), (3) recover causal ordering among latent variables through recursive GIN testing, and (4) eliminate spurious edge assignments via admissible permutation filtering based on block-rank criteria. This enables principled handling of spurious correlations even when confounders are unobserved, provided the data are non-Gaussian and the local Markov boundary is observed.</p>
                <p><strong>Knowledge Cutoff Year:</strong> 2024</p>
                <p><strong>Knowledge Cutoff Month:</strong> 12</p>
            </div>
        </div>

        <div class="section">
            <h2>Theory (Derived From)</h2>
            <p><strong>Derived From:</strong> <span class="empty-note">None</span></p>
            <p><strong>Change Log:</strong> <span class="empty-note">No change log entries.</span></p>
        </div>

        <div class="section">
            <h2>Evaluations of this Theory</h2>
            <p class="empty-note">No evaluations of this theory.</p>
        </div>

        <div class="section">
            <h2>Theory (Details)</h2>

            <h3>Theory Statements</h3>
            <ol>
                <li>In LiNGLaM models with observed subset S containing all parents of target nodes, A^{-1}_{S,S} (inverse principal submatrix) is an ISA solution for X_S</li>
                <li>Multi-dimensional ISA subspaces (dimension > 1) indicate groups of observed variables sharing latent confounders; 1-D components indicate unconfounded variables</li>
                <li>The GIN condition E_{Y||Z} ⊥ Z (where E_{Y||Z} = ω^T Y with ω^T E[YZ^T] = 0) holds if and only if a causally earlier subset of latent common causes of Y d-separates Y from Z</li>
                <li>The number of latent variables behind observed group P is Dim(L(P)) = Dim(P) - 1 when GIN holds and the surrogate is properly constructed</li>
                <li>Admissible row permutations of ISA demixing matrix W must yield invertible diagonal blocks for each ISA subspace; rank-deficient blocks indicate spurious permutations</li>
                <li>Recursive application of GIN tests on halves of cluster children enables recovery of causal order among latent variable sets</li>
                <li>The method requires non-Gaussian noise components; at most one noise component can be Gaussian for identifiability</li>
                <li>Local causal discovery is exact (up to subspace scaling/permutation) when Pa(target) ⊆ S, enabling recovery despite global cycles or latents</li>
            </ol>
            <h3>Supporting Evidence</h3>
<ol>
    <li>ISA decomposes observed mixtures into mutually independent irreducible subspaces, with A^{-1}_{S,S} being an ISA solution for local LiNG models <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> </li>
    <li>Multi-dimensional ISA subspaces indicate shared latent confounders while 1-D components indicate unconfounded variables <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> </li>
    <li>GIN condition (E_{Y||Z} ⊥ Z where E_{Y||Z} = ω^T Y with ω^T E[YZ^T] = 0) enables detection of latent structure by testing independence of surrogate variables <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> <a href="../results/extraction-result-744.html#e744.1" class="evidence-link">[e744.1]</a> </li>
    <li>HSIC (Hilbert-Schmidt Independence Criterion) provides effective nonparametric independence testing for non-Gaussian data in GIN framework <a href="../results/extraction-result-744.html#e744.3" class="evidence-link">[e744.3]</a> </li>
    <li>Fisher's method combines pairwise independence p-values to approximate multivariate independence when direct testing is avoided <a href="../results/extraction-result-744.html#e744.4" class="evidence-link">[e744.4]</a> </li>
    <li>Admissible permutations via invertible block-diagonal rank tests eliminate spurious nonzero patterns from incorrect permutation/scaling <a href="../results/extraction-result-771.html#e771.2" class="evidence-link">[e771.2]</a> </li>
    <li>GIN-based recursive algorithm achieved best performance (lowest latent omission/commission and mismeasurement errors) among compared methods (BPC, FOFC, LSTC) on synthetic LiNGLaM cases <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> </li>
    <li>Causal-order identification accuracy improved towards 1 as sample size increased across test cases <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> </li>
    <li>Method successfully applied to real psychometric data (Byrne teacher-burnout study with 28 observed variables), producing interpretable clusters and causal ordering <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> <a href="../results/extraction-result-744.html#e744.1" class="evidence-link">[e744.1]</a> </li>
    <li>Local causal discovery with LiNG models enables exact identification of target incoming effects (up to per-subspace scaling and permutation) when parents are included in observed subset S <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> </li>
    <li>ISA-based demixing leads to lower SHD in experiments versus methods not using ISA <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> </li>
    <li>Block-rank/invertibility criterion correctly recovers row alignment to A^{-1}_{S,S} and reduces false-positive edges from permutation indeterminacy <a href="../results/extraction-result-771.html#e771.2" class="evidence-link">[e771.2]</a> </li>
</ol>            <h3>New Predictions (Likely outcome)</h3>
            <ol>
                <li>In a 12-variable system with 3 latent confounders each affecting 4 observed variables, ISA should correctly identify the 3 four-dimensional subspaces with >90% accuracy given N>1000 samples and non-Gaussian noise</li>
                <li>GIN-based causal ordering should achieve >80% correct pairwise ordering of latent variables in systems with 2-4 latents when sample size N>2000</li>
                <li>Admissible permutation filtering should reduce false positive edges by 30-50% compared to naive nonzero-diagonal permutation criteria in systems with 5-10 variables</li>
                <li>For a 20-variable system with 5 latent confounders, the method should correctly cluster observed variables into their latent-parent groups with >85% accuracy when N>1500</li>
                <li>HSIC-based GIN testing with kernel width tuned to data scale should outperform parametric tests (e.g., correlation-based) by 15-25% in terms of correct latent structure recovery for non-Gaussian data</li>
            </ol>
            <h3>New Predictions (Unknown outcome/high-entropy)</h3>
            <ol>
                <li>Whether ISA-based methods can be extended to non-linear latent variable models using kernel ISA or other nonlinear ICA variants while maintaining identifiability guarantees</li>
                <li>The exact sample complexity scaling law: how N_required scales with number of latents L, observed variables V, and latent dimensionality D as a function N = f(L, V, D, noise_level)</li>
                <li>Whether the approach can handle time-varying latent confounders in dynamic systems, and if so, what temporal smoothness assumptions are required</li>
                <li>The robustness of surrogate construction when cross-covariance estimates are noisy or when sample size is insufficient for reliable second-moment estimation</li>
                <li>Whether combining ISA with modern deep learning architectures (e.g., variational autoencoders with ISA-structured latent spaces) could improve scalability to high-dimensional observations</li>
                <li>The performance when latent variables affect more than two observed variables each (overcomplete mixing scenarios)</li>
                <li>Whether the method can be adapted to handle mixed discrete-continuous observed variables or purely discrete observations</li>
            </ol>
            <h3>Negative Experiments</h3>
            <ol>
                <li>Finding cases where ISA subspaces do not correspond to latent-confounded groups (e.g., multi-dimensional subspaces arising from other structural properties) would challenge the theoretical foundation</li>
                <li>Demonstrating that GIN independence tests systematically fail even when the condition theoretically holds (e.g., due to finite-sample bias or test inadequacy) would reveal fundamental test limitations</li>
                <li>Showing that admissible permutations can still yield incorrect edge assignments despite satisfying block-invertibility would undermine the filtering criterion</li>
                <li>Finding that the method requires prohibitively large samples (N > 10000) for modest-sized systems (e.g., 10-15 variables with 3-4 latents) would severely limit practical applicability</li>
                <li>Demonstrating cases where near-Gaussian noise (e.g., sub-Gaussian with small excess kurtosis) causes complete failure of latent structure recovery would reveal brittleness to distributional assumptions</li>
                <li>Finding that computational complexity (ISA + permutation search + recursive GIN testing) scales worse than O(V^3) and becomes intractable for V > 30 would limit scalability</li>
                <li>Showing that the method fails when observed subset S does not contain all parents of target nodes, even when it contains most parents, would reveal sensitivity to the local Markov boundary assumption</li>
            </ol>
            <h3>Unaccounted for Evidence</h3>
<ol>
    <li>The theory assumes linear non-Gaussian models; extension to nonlinear cases is not addressed, though kernel methods are mentioned as potential future work <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> </li>
    <li>Computational complexity of ISA, permutation search, and recursive GIN testing may limit scalability to large systems (>30 variables) <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> <a href="../results/extraction-result-771.html#e771.2" class="evidence-link">[e771.2]</a> <a href="../results/extraction-result-744.html#e744.1" class="evidence-link">[e744.1]</a> </li>
    <li>The choice of independence test (HSIC) parameters, particularly kernel width, affects performance but systematic optimization procedures are not provided <a href="../results/extraction-result-744.html#e744.3" class="evidence-link">[e744.3]</a> </li>
    <li>The method's performance when the non-Gaussianity assumption is violated or weakly satisfied (near-Gaussian data) is not characterized <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> </li>
    <li>Sample complexity bounds as a function of system parameters (number of latents, observed variables, noise levels) are not provided <a href="../results/extraction-result-744.html#e744.0" class="evidence-link">[e744.0]</a> <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> </li>
    <li>The method's behavior when measurement error is present is mentioned as requiring additional error-correction steps but not fully developed <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> </li>
    <li>Handling of overcomplete mixing scenarios (latent variables affecting more than two observed variables) is not fully addressed <a href="../results/extraction-result-771.html#e771.1" class="evidence-link">[e771.1]</a> </li>
    <li>The recursive algorithm's convergence properties and stopping criteria are not formally characterized <a href="../results/extraction-result-744.html#e744.1" class="evidence-link">[e744.1]</a> </li>
</ol>            <h3>Existing Theory Comparison</h3>
            <p><strong>Likely Classification:</strong> closely-related-to-existing</p>
            <p><strong>Explanation:</strong> No explanation provided.</p>
            <p><strong>References:</strong> <ul>
    <li>Shimizu et al. (2006) A linear non-Gaussian acyclic model for causal discovery [LiNGAM framework - foundation for linear non-Gaussian causal models]</li>
    <li>Hoyer et al. (2008) Nonlinear causal discovery with additive noise models [ANM and identifiability theory]</li>
    <li>Silva et al. (2006) Learning the structure of linear latent variable models [Early work on latent variable identification in causal models]</li>
    <li>Hyvärinen & Oja (2000) Independent component analysis: algorithms and applications [ICA and ISA foundations]</li>
    <li>Hyvärinen et al. (2010) Estimation of a structural vector autoregression model using non-Gaussianity [Extension of LiNGAM to time series]</li>
    <li>Tashiro et al. (2014) ParceLiNGAM: A causal ordering method robust against latent confounders [Earlier work on LiNGAM with latents]</li>
    <li>Salehkaleybar et al. (2020) Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables [Related work on latent-variable LiNGAM]</li>
    <li>Gretton et al. (2005) Measuring statistical dependence with Hilbert-Schmidt norms [HSIC independence test foundation]</li>
</ul>
        </div>

        <div class="section">
            <h2>Theory Components (Debug)</h2>
            <pre><code>{
    "theory_name": "Latent Confounder Detection via Independent Subspace Analysis in Linear Non-Gaussian Models",
    "theory_description": "In linear non-Gaussian models (LiNGLaM) with latent confounders, spurious correlations induced by hidden common causes can be detected and characterized through Independent Subspace Analysis (ISA) combined with the Generalized Independent Noise (GIN) condition. The fundamental insight is that the inverse principal submatrix A^{-1}_{S,S} of the mixing matrix is an ISA solution, where observed variables sharing latent parents form irreducible multi-dimensional subspaces while unconfounded variables form 1-D independent components. By constructing surrogate variables that null cross-covariance with other observed sets (ω^T Y where ω^T E[YZ^T] = 0) and testing their independence using nonparametric tests (HSIC), one can: (1) identify which observed variables share latent causes, (2) determine the dimensionality of latent confounders behind each group (Dim(L(P)) = Dim(P) - 1), (3) recover causal ordering among latent variables through recursive GIN testing, and (4) eliminate spurious edge assignments via admissible permutation filtering based on block-rank criteria. This enables principled handling of spurious correlations even when confounders are unobserved, provided the data are non-Gaussian and the local Markov boundary is observed.",
    "supporting_evidence": [
        {
            "text": "ISA decomposes observed mixtures into mutually independent irreducible subspaces, with A^{-1}_{S,S} being an ISA solution for local LiNG models",
            "uuids": [
                "e771.1"
            ]
        },
        {
            "text": "Multi-dimensional ISA subspaces indicate shared latent confounders while 1-D components indicate unconfounded variables",
            "uuids": [
                "e771.1"
            ]
        },
        {
            "text": "GIN condition (E_{Y||Z} ⊥ Z where E_{Y||Z} = ω^T Y with ω^T E[YZ^T] = 0) enables detection of latent structure by testing independence of surrogate variables",
            "uuids": [
                "e744.0",
                "e744.1"
            ]
        },
        {
            "text": "HSIC (Hilbert-Schmidt Independence Criterion) provides effective nonparametric independence testing for non-Gaussian data in GIN framework",
            "uuids": [
                "e744.3"
            ]
        },
        {
            "text": "Fisher's method combines pairwise independence p-values to approximate multivariate independence when direct testing is avoided",
            "uuids": [
                "e744.4"
            ]
        },
        {
            "text": "Admissible permutations via invertible block-diagonal rank tests eliminate spurious nonzero patterns from incorrect permutation/scaling",
            "uuids": [
                "e771.2"
            ]
        },
        {
            "text": "GIN-based recursive algorithm achieved best performance (lowest latent omission/commission and mismeasurement errors) among compared methods (BPC, FOFC, LSTC) on synthetic LiNGLaM cases",
            "uuids": [
                "e744.0"
            ]
        },
        {
            "text": "Causal-order identification accuracy improved towards 1 as sample size increased across test cases",
            "uuids": [
                "e744.0"
            ]
        },
        {
            "text": "Method successfully applied to real psychometric data (Byrne teacher-burnout study with 28 observed variables), producing interpretable clusters and causal ordering",
            "uuids": [
                "e744.0",
                "e744.1"
            ]
        },
        {
            "text": "Local causal discovery with LiNG models enables exact identification of target incoming effects (up to per-subspace scaling and permutation) when parents are included in observed subset S",
            "uuids": [
                "e771.1"
            ]
        },
        {
            "text": "ISA-based demixing leads to lower SHD in experiments versus methods not using ISA",
            "uuids": [
                "e771.1"
            ]
        },
        {
            "text": "Block-rank/invertibility criterion correctly recovers row alignment to A^{-1}_{S,S} and reduces false-positive edges from permutation indeterminacy",
            "uuids": [
                "e771.2"
            ]
        }
    ],
    "theory_statements": [
        "In LiNGLaM models with observed subset S containing all parents of target nodes, A^{-1}_{S,S} (inverse principal submatrix) is an ISA solution for X_S",
        "Multi-dimensional ISA subspaces (dimension &gt; 1) indicate groups of observed variables sharing latent confounders; 1-D components indicate unconfounded variables",
        "The GIN condition E_{Y||Z} ⊥ Z (where E_{Y||Z} = ω^T Y with ω^T E[YZ^T] = 0) holds if and only if a causally earlier subset of latent common causes of Y d-separates Y from Z",
        "The number of latent variables behind observed group P is Dim(L(P)) = Dim(P) - 1 when GIN holds and the surrogate is properly constructed",
        "Admissible row permutations of ISA demixing matrix W must yield invertible diagonal blocks for each ISA subspace; rank-deficient blocks indicate spurious permutations",
        "Recursive application of GIN tests on halves of cluster children enables recovery of causal order among latent variable sets",
        "The method requires non-Gaussian noise components; at most one noise component can be Gaussian for identifiability",
        "Local causal discovery is exact (up to subspace scaling/permutation) when Pa(target) ⊆ S, enabling recovery despite global cycles or latents"
    ],
    "new_predictions_likely": [
        "In a 12-variable system with 3 latent confounders each affecting 4 observed variables, ISA should correctly identify the 3 four-dimensional subspaces with &gt;90% accuracy given N&gt;1000 samples and non-Gaussian noise",
        "GIN-based causal ordering should achieve &gt;80% correct pairwise ordering of latent variables in systems with 2-4 latents when sample size N&gt;2000",
        "Admissible permutation filtering should reduce false positive edges by 30-50% compared to naive nonzero-diagonal permutation criteria in systems with 5-10 variables",
        "For a 20-variable system with 5 latent confounders, the method should correctly cluster observed variables into their latent-parent groups with &gt;85% accuracy when N&gt;1500",
        "HSIC-based GIN testing with kernel width tuned to data scale should outperform parametric tests (e.g., correlation-based) by 15-25% in terms of correct latent structure recovery for non-Gaussian data"
    ],
    "new_predictions_unknown": [
        "Whether ISA-based methods can be extended to non-linear latent variable models using kernel ISA or other nonlinear ICA variants while maintaining identifiability guarantees",
        "The exact sample complexity scaling law: how N_required scales with number of latents L, observed variables V, and latent dimensionality D as a function N = f(L, V, D, noise_level)",
        "Whether the approach can handle time-varying latent confounders in dynamic systems, and if so, what temporal smoothness assumptions are required",
        "The robustness of surrogate construction when cross-covariance estimates are noisy or when sample size is insufficient for reliable second-moment estimation",
        "Whether combining ISA with modern deep learning architectures (e.g., variational autoencoders with ISA-structured latent spaces) could improve scalability to high-dimensional observations",
        "The performance when latent variables affect more than two observed variables each (overcomplete mixing scenarios)",
        "Whether the method can be adapted to handle mixed discrete-continuous observed variables or purely discrete observations"
    ],
    "negative_experiments": [
        "Finding cases where ISA subspaces do not correspond to latent-confounded groups (e.g., multi-dimensional subspaces arising from other structural properties) would challenge the theoretical foundation",
        "Demonstrating that GIN independence tests systematically fail even when the condition theoretically holds (e.g., due to finite-sample bias or test inadequacy) would reveal fundamental test limitations",
        "Showing that admissible permutations can still yield incorrect edge assignments despite satisfying block-invertibility would undermine the filtering criterion",
        "Finding that the method requires prohibitively large samples (N &gt; 10000) for modest-sized systems (e.g., 10-15 variables with 3-4 latents) would severely limit practical applicability",
        "Demonstrating cases where near-Gaussian noise (e.g., sub-Gaussian with small excess kurtosis) causes complete failure of latent structure recovery would reveal brittleness to distributional assumptions",
        "Finding that computational complexity (ISA + permutation search + recursive GIN testing) scales worse than O(V^3) and becomes intractable for V &gt; 30 would limit scalability",
        "Showing that the method fails when observed subset S does not contain all parents of target nodes, even when it contains most parents, would reveal sensitivity to the local Markov boundary assumption"
    ],
    "unaccounted_for": [
        {
            "text": "The theory assumes linear non-Gaussian models; extension to nonlinear cases is not addressed, though kernel methods are mentioned as potential future work",
            "uuids": [
                "e744.0",
                "e771.1"
            ]
        },
        {
            "text": "Computational complexity of ISA, permutation search, and recursive GIN testing may limit scalability to large systems (&gt;30 variables)",
            "uuids": [
                "e771.1",
                "e771.2",
                "e744.1"
            ]
        },
        {
            "text": "The choice of independence test (HSIC) parameters, particularly kernel width, affects performance but systematic optimization procedures are not provided",
            "uuids": [
                "e744.3"
            ]
        },
        {
            "text": "The method's performance when the non-Gaussianity assumption is violated or weakly satisfied (near-Gaussian data) is not characterized",
            "uuids": [
                "e744.0"
            ]
        },
        {
            "text": "Sample complexity bounds as a function of system parameters (number of latents, observed variables, noise levels) are not provided",
            "uuids": [
                "e744.0",
                "e771.1"
            ]
        },
        {
            "text": "The method's behavior when measurement error is present is mentioned as requiring additional error-correction steps but not fully developed",
            "uuids": [
                "e771.1"
            ]
        },
        {
            "text": "Handling of overcomplete mixing scenarios (latent variables affecting more than two observed variables) is not fully addressed",
            "uuids": [
                "e771.1"
            ]
        },
        {
            "text": "The recursive algorithm's convergence properties and stopping criteria are not formally characterized",
            "uuids": [
                "e744.1"
            ]
        }
    ],
    "conflicting_evidence": [
        {
            "text": "The method requires non-Gaussianity (at most one Gaussian noise component); performance on near-Gaussian data is unclear and may be poor",
            "uuids": [
                "e744.0",
                "e771.1"
            ]
        },
        {
            "text": "Local causal discovery is limited to cases where all parents of target nodes are included in the observed subset S; partial parent observation may lead to incorrect inference",
            "uuids": [
                "e771.1"
            ]
        },
        {
            "text": "Computational cost may be prohibitive for large systems due to ISA computation, permutation enumeration, and repeated independence testing",
            "uuids": [
                "e771.1",
                "e771.2",
                "e744.1"
            ]
        },
        {
            "text": "The method assumes causal sufficiency within the local subset S; violations of this assumption are not handled",
            "uuids": [
                "e771.1"
            ]
        }
    ],
    "special_cases": [
        "When each latent variable affects exactly two observed variables, the system is exactly identified and ISA recovery is most reliable",
        "For cyclic graphs, local analysis on appropriate subsets (containing target and its Markov boundary) can still recover incoming effects despite global cycles",
        "When observed variables have measurement error, the method may require additional preprocessing or error-correction steps, though specific procedures are not fully developed",
        "For time-series data, temporal ordering constraints can be incorporated to improve identification and reduce the search space for causal ordering",
        "In psychometric applications (e.g., Byrne teacher-burnout study), the method can identify latent constructs (e.g., emotional exhaustion, depersonalization) from observed survey items",
        "When sample size is very large (N &gt; 2000), causal-order identification accuracy approaches 1.0 for systems with 2-4 latent variables",
        "For systems where some observed variables are unconfounded (no latent parents), these appear as 1-D components in ISA and can be immediately identified",
        "When the observed subset S contains all parents of multiple target nodes, the method can simultaneously recover incoming effects for all targets",
        "In cases where latent variables have a clear causal ordering (e.g., hierarchical factor models), the recursive GIN testing procedure can efficiently recover this ordering"
    ],
    "existing_theory": {
        "likely_classification": "closely-related-to-existing",
        "references": [
            "Shimizu et al. (2006) A linear non-Gaussian acyclic model for causal discovery [LiNGAM framework - foundation for linear non-Gaussian causal models]",
            "Hoyer et al. (2008) Nonlinear causal discovery with additive noise models [ANM and identifiability theory]",
            "Silva et al. (2006) Learning the structure of linear latent variable models [Early work on latent variable identification in causal models]",
            "Hyvärinen & Oja (2000) Independent component analysis: algorithms and applications [ICA and ISA foundations]",
            "Hyvärinen et al. (2010) Estimation of a structural vector autoregression model using non-Gaussianity [Extension of LiNGAM to time series]",
            "Tashiro et al. (2014) ParceLiNGAM: A causal ordering method robust against latent confounders [Earlier work on LiNGAM with latents]",
            "Salehkaleybar et al. (2020) Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables [Related work on latent-variable LiNGAM]",
            "Gretton et al. (2005) Measuring statistical dependence with Hilbert-Schmidt norms [HSIC independence test foundation]"
        ]
    },
    "reflected_from_theory_index": 6,
    "type": "specific"
}</code></pre>
        </div>
    </div>
</body>
</html>